{
  "https://papers.nips.cc/paper_files/paper/2023/hash/0001ca33ba34ce0351e4612b744b3936-Abstract-Conference.html": {
    "title": "Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder",
    "volume": "main",
    "abstract": "Generative models of observations under interventions have been a vibrant topic of interest across machine learning and the sciences in recent years. For example, in drug discovery, there is a need to model the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action. We propose the Sparse Additive Mechanism Shift Variational Autoencoder, SAMS-VAE, to combine compositionality, disentanglement, and interpretability for perturbation models. SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects. Crucially, SAMS-VAE sparsifies these global latent variables for individual perturbations to identify disentangled, perturbation-specific latent subspaces that are flexibly composable. We evaluate SAMS-VAE both quantitatively and qualitatively on a range of tasks using two popular single cell sequencing datasets.In order to measure perturbation-specific model-properties, we also introduce a framework for evaluation of perturbation models based on average treatment effects with links to posterior predictive checks. SAMS-VAE outperforms comparable models in terms of generalization across in-distribution and out-of-distribution tasks, including a combinatorial reasoning task under resource paucity, and yields interpretable latent structures which correlate strongly to known biological mechanisms. Our results suggest SAMS-VAE is an interesting addition to the modeling toolkit for machine learning-driven scientific discovery",
    "keywords": [],
    "checked": true,
    "id": "6f4b1caaec3342214fc92c5e6161daa75ffd9ea8",
    "semantic_title": "modelling cellular perturbations with the sparse additive mechanism shift variational autoencoder",
    "citation_count": 0,
    "authors": [
      "Michael Bereket",
      "Theofanis Karaletsos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/001608167bb652337af5df0129aeaabd-Abstract-Conference.html": {
    "title": "Cross-Episodic Curriculum for Transformer Agents",
    "volume": "main",
    "abstract": "We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the learning efficiency and generalization of Transformer agents. Central to CEC is the placement of cross-episodic experiences into a Transformer's context, which forms the basis of a curriculum. By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes. Such synergy combined with the potent pattern recognition capabilities of Transformer models delivers a powerful cross-episodic attention mechanism. The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning with discrete control, such as in DeepMind Lab, where the curriculum captures the learning progression in both individual and progressively complex settings; and the other involving imitation learning with mixed-quality data for continuous control, as seen in RoboMimic, where the curriculum captures the improvement in demonstrators' expertise. In all instances, policies resulting from CEC exhibit superior performance and strong generalization. Code is open-sourced on the project website https://cec-agent.github.io/ to facilitate research on Transformer agent learning",
    "keywords": [],
    "checked": true,
    "id": "a8282d1378cf872817d3291f5e7edb6fbdf0121a",
    "semantic_title": "cross-episodic curriculum for transformer agents",
    "citation_count": 2,
    "authors": [
      "Lucy Xiaoyang Shi",
      "Yunfan Jiang",
      "Jake Grigsby",
      "Linxi Fan",
      "Yuke Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0021c2cb1b9b6a71ac478ea52a93b25a-Abstract-Conference.html": {
    "title": "PaintSeg: Painting Pixels for Training-free Segmentation",
    "volume": "main",
    "abstract": "The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation. Code: https://github.com/lxa9867/PaintSeg",
    "keywords": [],
    "checked": false,
    "id": "dc157eba8bdb4cfe6ee65566d8295939ac5b4b37",
    "semantic_title": "paintseg: training-free segmentation via painting",
    "citation_count": 2,
    "authors": [
      "Xiang Li",
      "Chung-Ching Lin",
      "Yinpeng Chen",
      "Zicheng Liu",
      "Jinglu Wang",
      "Rita Singh",
      "Bhiksha Raj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/002262941c9edfd472a79298b2ac5e17-Abstract-Conference.html": {
    "title": "Bootstrapping Vision-Language Learning with Decoupled Language Pre-training",
    "volume": "main",
    "abstract": "We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importantly, our framework is modality-agnostic and flexible in terms of architectural design, as validated by its successful application in a video learning task using varied base modules. The code will be made available at https://github.com/yiren-jian/BLIText",
    "keywords": [],
    "checked": true,
    "id": "98f8793a18eaced0ce93f5202065496cc5a84943",
    "semantic_title": "bootstrapping vision-language learning with decoupled language pre-training",
    "citation_count": 3,
    "authors": [
      "Yiren Jian",
      "Chongyang Gao",
      "Soroush Vosoughi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/00296c0e10cd24d415c2db63ea2a2c68-Abstract-Conference.html": {
    "title": "Path following algorithms for $\\ell_2$-regularized $M$-estimation with approximation guarantee",
    "volume": "main",
    "abstract": "Many modern machine learning algorithms are formulated as regularized M-estimation problems, in which a regularization (tuning) parameter controls a trade-off between model fit to the training data and model complexity. To select the ``best'' tuning parameter value that achieves a good trade-off, an approximated solution path needs to be computed. In practice, this is often done through selecting a grid of tuning parameter values and solving the regularized problem at the selected grid points. However, given any desired level of accuracy, it is often not clear how to choose the grid points and also how accurately one should solve the regularized problems at the selected gird points, both of which can greatly impact the overall amount of computation. In the context of $\\ell_2$-regularized $M$-estimation problem, we propose a novel grid point selection scheme and an adaptive stopping criterion for any given optimization algorithm that produces an approximated solution path with approximation error guarantee. Theoretically, we prove that the proposed solution path can approximate the exact solution path to arbitrary level of accuracy, while saving the overall computation as much as possible. Numerical results also corroborate with our theoretical analysis",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhang Zhu",
      "Renxiong Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0073cc73e1873b35345209b50a3dab66-Abstract-Conference.html": {
    "title": "PDF: Point Diffusion Implicit Function for Large-scale Scene Neural Representation",
    "volume": "main",
    "abstract": "Recent advances in implicit neural representations have achieved impressive results by sampling and fusing individual points along sampling rays in the sampling space. However, due to the explosively growing sampling space, finely representing and synthesizing detailed textures remains a challenge for unbounded large-scale outdoor scenes. To alleviate the dilemma of using individual points to perceive the entire colossal space, we explore learning the surface distribution of the scene to provide structural priors and reduce the samplable space and propose a Point Diffusion implicit Function, PDF, for large-scale scene neural representation. The core of our method is a large-scale point cloud super-resolution diffusion module that enhances the sparse point cloud reconstructed from several training images into a dense point cloud as an explicit prior. Then in the rendering stage, only sampling points with prior points within the sampling radius are retained. That is, the sampling space is reduced from the unbounded space to the scene surface. Meanwhile, to fill in the background of the scene that cannot be provided by point clouds, the region sampling based on Mip-NeRF 360 is employed to model the background representation. Expensive experiments have demonstrated the effectiveness of our method for large-scale scene novel view synthesis, which outperforms relevant state-of-the-art baselines",
    "keywords": [],
    "checked": true,
    "id": "8ad5cbcd1e44c1ee17fbb4ed70b8aa3959ecab98",
    "semantic_title": "pdf: point diffusion implicit function for large-scale scene neural representation",
    "citation_count": 0,
    "authors": [
      "Yuhan Ding",
      "Fukun Yin",
      "Jiayuan Fan",
      "Hui Li",
      "Xin Chen",
      "Wen Liu",
      "Chongshan Lu",
      "Gang Yu",
      "Tao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/007f4927e60699392425f267d43f0940-Abstract-Conference.html": {
    "title": "Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation",
    "volume": "main",
    "abstract": "We study robust reinforcement learning (RL) with the goal of determining a well-performing policy that is robust against model mismatch between the training simulator and the testing environment. Previous policy-based robust RL algorithms mainly focus on the tabular setting under uncertainty sets that facilitate robust policy evaluation, but are no longer tractable when the number of states scales up. To this end, we propose two novel uncertainty set formulations, one based on double sampling and the other on an integral probability metric. Both make large-scale robust RL tractable even when one only has access to a simulator. We propose a robust natural actor-critic (RNAC) approach that incorporates the new uncertainty sets and employs function approximation. We provide finite-time convergence guarantees for the proposed RNAC algorithm to the optimal robust policy within the function approximation error. Finally, we demonstrate the robust performance of the policy learned by our proposed RNAC approach in multiple MuJoCo environments and a real-world TurtleBot navigation task",
    "keywords": [],
    "checked": true,
    "id": "b284d0899d8a1dd31461c81074deadcbcd577be4",
    "semantic_title": "natural actor-critic for robust reinforcement learning with function approximation",
    "citation_count": 2,
    "authors": [
      "Ruida Zhou",
      "Tao Liu",
      "Min Cheng",
      "Dileep Kalathil",
      "P. R. Kumar",
      "Chao Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/00b67df24009747e8bbed4c2c6f9c825-Abstract-Conference.html": {
    "title": "Adaptive Selective Sampling for Online Prediction with Experts",
    "volume": "main",
    "abstract": "We consider online prediction of a binary sequence with expert advice. For this setting, we devise label-efficient forecasting algorithms, which use a selective sampling scheme that enables collecting much fewer labels than standard procedures. For the general case without a perfect expert, we prove best-of-both-worlds guarantees, demonstrating that the proposed forecasting algorithm always queries sufficiently many labels in the worst case to obtain optimal regret guarantees, while simultaneously querying much fewer labels in more benign settings. Specifically, for a scenario where one expert is strictly better than the others in expectation, we show that the label complexity of the label-efficient forecaster is roughly upper-bounded by the square root of the number of rounds. Finally, we present numerical experiments empirically showing that the normalized regret of the label-efficient forecaster can asymptotically match known minimax rates for pool-based active learning, suggesting it can optimally adapt to benign settings",
    "keywords": [],
    "checked": true,
    "id": "309dc5a8736c5c0a883a3d8e8fb5ca445114a193",
    "semantic_title": "adaptive selective sampling for online prediction with experts",
    "citation_count": 0,
    "authors": [
      "Rui Castro",
      "Fredrik Hellström",
      "Tim van Erven"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/00bb4e415ef117f2dee2fc3b778d806d-Abstract-Conference.html": {
    "title": "Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect",
    "volume": "main",
    "abstract": "We consider the problem of reconstructing coupled networks (e.g., biological neural networks) connecting large numbers of variables (e.g.,nerve cells), of which state evolution is governed by dissipative dynamics consisting of strong self-drive (dominants the evolution) and weak coupling-drive. The core difficulty is sparseness of coupling effect that emerges (the coupling force is significant) only momentarily and otherwise remains quiescent in time series (e.g., neuronal activity sequence). Here we learn the idea from attention mechanism to guide the classifier to make inference focusing on the critical regions of time series data where coupling effect may manifest. Specifically, attention coefficients are assigned autonomously by artificial neural networks trained to maximise the Attentive Transfer Entropy (ATEn), which is a novel generalization of the iconic transfer entropy metric. Our results show that, without any prior knowledge of dynamics, ATEn explicitly identifies areas where the strength of coupling-drive is distinctly greater than zero. This innovation substantially improves reconstruction performance for both synthetic and real directed coupling networks using data generated by neuronal models widely used in neuroscience",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolei Ru",
      "XINYA ZHANG",
      "Zijia Liu",
      "Jack Murdoch Moore",
      "Gang Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/00db17c36b5435195760520efa96d99c-Abstract-Conference.html": {
    "title": "(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More",
    "volume": "main",
    "abstract": "A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning",
    "keywords": [],
    "checked": true,
    "id": "29bcba8ec89c97d19cb7fbf9720ddfedf8d5ade3",
    "semantic_title": "(provable) adversarial robustness for group equivariant tasks: graphs, point clouds, molecules, and more",
    "citation_count": 3,
    "authors": [
      "Jan Schuchardt",
      "Yan Scholten",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/00ed9ab006311be67879ecef8f80d7c5-Abstract-Conference.html": {
    "title": "Self-Supervised Motion Magnification by Backpropagating Through Optical Flow",
    "volume": "main",
    "abstract": "This paper presents a simple, self-supervised method for magnifying subtle motions in video: given an input video and a magnification factor, we manipulate the video such that its new optical flow is scaled by the desired amount. To train our model, we propose a loss function that estimates the optical flow of the generated video and penalizes how far if deviates from the given magnification factor. Thus, training involves differentiating through a pretrained optical flow network. Since our model is self-supervised, we can further improve its performance through test-time adaptation, by finetuning it on the input video. It can also be easily extended to magnify the motions of only user-selected objects. Our approach avoids the need for synthetic magnification datasets that have been used to train prior learning-based approaches. Instead, it leverages the existing capabilities of off-the-shelf motion estimators. We demonstrate the effectiveness of our method through evaluations of both visual quality and quantitative metrics on a range of real-world and synthetic videos, and we show our method works for both supervised and unsupervised optical flow methods",
    "keywords": [],
    "checked": true,
    "id": "9b08ff0993b11121b32de551cb042b93cc71379c",
    "semantic_title": "self-supervised motion magnification by backpropagating through optical flow",
    "citation_count": 1,
    "authors": [
      "Zhaoying Pan",
      "Daniel Geng",
      "Andrew Owens"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0113ef4642264adc2e6924a3cbbdf532-Abstract-Conference.html": {
    "title": "TexQ: Zero-shot Network Quantization with Texture Feature Distribution Calibration",
    "volume": "main",
    "abstract": "Quantization is an effective way to compress neural networks. By reducing the bit width of the parameters, the processing efficiency of neural network models at edge devices can be notably improved. Most conventional quantization methods utilize real datasets to optimize quantization parameters and fine-tune. Due to the inevitable privacy and security issues of real samples, the existing real-data-driven methods are no longer applicable. Thus, a natural method is to introduce synthetic samples for zero-shot quantization (ZSQ). However, the conventional synthetic samples fail to retain the detailed texture feature distributions, which severely limits the knowledge transfer and performance of the quantized model. In this paper, a novel ZSQ method, TexQ is proposed to address this issue. We first synthesize a calibration image and extract its calibration center for each class with a texture feature energy distribution calibration method. Then, the calibration centers are used to guide the generator to synthesize samples. Finally, we introduce the mixup knowledge distillation module to diversify synthetic samples for fine-tuning. Extensive experiments on CIFAR10/100 and ImageNet show that TexQ is observed to perform state-of-the-art in ultra-low bit width quantization. For example, when ResNet-18 is quantized to 3-bit, TexQ achieves a 12.18% top-1 accuracy increase on ImageNet compared to state-of-the-art methods. Code at https://github.com/dangsingrue/TexQ",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinrui Chen",
      "Yizhi Wang",
      "Renao YAN",
      "Yiqing Liu",
      "Tian Guan",
      "Yonghong He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/012af729c5d14d279581fc8a5db975a1-Abstract-Conference.html": {
    "title": "Ambient Diffusion: Learning Clean Distributions from Corrupted Data",
    "volume": "main",
    "abstract": "We present the first diffusion-based framework that can learn an unknown distribution using only highly-corrupted samples. This problem arises in scientific applications where access to uncorrupted samples is impossible or expensive to acquire. Another benefit of our approach is the ability to train generative models that are less likely to memorize any individual training sample, since they never observe clean training data. Our main idea is to introduce additional measurement distortion during the diffusion process and require the model to predict the original corrupted image from the further corrupted image. We prove that our method leads to models that learn the conditional expectation of the full uncorrupted image given this additional measurement corruption. This holds for any corruption process that satisfies some technical conditions (and in particular includes inpainting and compressed sensing). We train models on standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learn the distribution even when all the training samples have 90\\% of their pixels missing. We also show that we can finetune foundation models on small corrupted datasets (e.g. MRI scans with block corruptions) and learn the clean distribution without memorizing the training set",
    "keywords": [],
    "checked": true,
    "id": "fdc04fe7c4a3fb4038d1bbd5f4e2fdf2599566e3",
    "semantic_title": "ambient diffusion: learning clean distributions from corrupted data",
    "citation_count": 4,
    "authors": [
      "Giannis Daras",
      "Kulin Shah",
      "Yuval Dagan",
      "Aravind Gollakota",
      "Alex Dimakis",
      "Adam Klivans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/01328d0767830e73a612f9073e9ff15f-Abstract-Conference.html": {
    "title": "Scalable Membership Inference Attacks via Quantile Regression",
    "volume": "main",
    "abstract": "Membership inference attacks are designed to determine, using black box access to trained models, whether a particular example was used in training or not. Membership inference can be formalized as a hypothesis testing problem. The most effective existing attacks estimate the distribution of some test statistic (usually the model's confidence on the true label) on points that were (and were not) used in training by training many \\emph{shadow models}---i.e. models of the same architecture as the model being attacked, trained on a random subsample of data. While effective, these attacks are extremely computationally expensive, especially when the model under attack is large. \\footnotetext[0]{Martin and Shuai are the lead authors, and other authors are ordered alphabetically. {maberlop,shuat}@amazon.com}We introduce a new class of attacks based on performing quantile regression on the distribution of confidence scores induced by the model under attack on points that are not used in training. We show that our method is competitive with state-of-the-art shadow model attacks, while requiring substantially less compute because our attack requires training only a single model. Moreover, unlike shadow model attacks, our proposed attack does not require any knowledge of the architecture of the model under attack and is therefore truly ``black-box\". We show the efficacy of this approach in an extensive series of experiments on various datasets and model architectures. Our code is available at \\href{https://github.com/amazon-science/quantile-mia}{github.com/amazon-science/quantile-mia.}",
    "keywords": [],
    "checked": true,
    "id": "85b6d229b5ee21840116d9b0520ede1e73c25174",
    "semantic_title": "scalable membership inference attacks via quantile regression",
    "citation_count": 9,
    "authors": [
      "Martin Bertran",
      "Shuai Tang",
      "Aaron Roth",
      "Michael Kearns",
      "Jamie H. Morgenstern",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0147d967a5db3b8dde08d2a327b24568-Abstract-Conference.html": {
    "title": "ESSEN: Improving Evolution State Estimation for Temporal Networks using Von Neumann Entropy",
    "volume": "main",
    "abstract": "Temporal networks are widely used as abstract graph representations for real-world dynamic systems. Indeed, recognizing the network evolution states is crucial in understanding and analyzing temporal networks. For instance, social networks will generate the clustering and formation of tightly-knit groups or communities over time, relying on the triadic closure theory. However, the existing methods often struggle to account for the time-varying nature of these network structures, hindering their performance when applied to networks with complex evolution states. To mitigate this problem, we propose a novel framework called ESSEN, an Evolution StateS awarE Network, to measure temporal network evolution using von Neumann entropy and thermodynamic temperature. The developed framework utilizes a von Neumann entropy aware attention mechanism and network evolution state contrastive learning in the graph encoding. In addition, it employs a unique decoder the so-called Mixture of Thermodynamic Experts (MoTE) for decoding. ESSEN extracts local and global network evolution information using thermodynamic features and adaptively recognizes the network evolution states. Moreover, the proposed method is evaluated on link prediction tasks under both transductive and inductive settings, with the corresponding results demonstrating its effectiveness compared to various state-of-the-art baselines",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyao Huang",
      "Yingyue Zhang",
      "Zhihong Zhang",
      "Edwin Hancock"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/015a8c69bedcb0a7b2ed2e1678f34399-Abstract-Conference.html": {
    "title": "Label Correction of Crowdsourced Noisy Annotations with an Instance-Dependent Noise Transition Model",
    "volume": "main",
    "abstract": "The predictive ability of supervised learning algorithms hinges on the quality of annotated examples, whose labels often come from multiple crowdsourced annotators with diverse expertise. To aggregate noisy crowdsourced annotations, many existing methods employ an annotator-specific instance-independent noise transition matrix to characterize the labeling skills of each annotator. Learning an instance-dependent noise transition model, however, is challenging and remains relatively less explored. To address this problem, in this paper, we formulate the noise transition model in a Bayesian framework and subsequently design a new label correction algorithm. Specifically, we approximate the instance-dependent noise transition matrices using a Bayesian network with a hierarchical spike and slab prior. To theoretically characterize the distance between the noise transition model and the true instance-dependent noise transition matrix, we provide a posterior-concentration theorem that ensures the posterior consistency in terms of the Hellinger distance. We further formulate the label correction process as a hypothesis testing problem and propose a novel algorithm to infer the true label from the noisy annotations based on the pairwise likelihood ratio test. Moreover, we establish an information-theoretic bound on the Bayes error for the proposed method. We validate the effectiveness of our approach through experiments on benchmark and real-world datasets",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui GUO",
      "Boyu Wang",
      "Grace Yi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0163ca1c69f848e766cfb0b7bb7e17f4-Abstract-Conference.html": {
    "title": "Diffused Task-Agnostic Milestone Planner",
    "volume": "main",
    "abstract": "Addressing decision-making problems using sequence modeling to predict future trajectories shows promising results in recent years.In this paper, we take a step further to leverage the sequence predictive method in wider areas such as long-term planning, vision-based control, and multi-task decision-making.To this end, we propose a method to utilize a diffusion-based generative sequence model to plan a series of milestones in a latent space and to have an agent to follow the milestones to accomplish a given task.The proposed method can learn control-relevant, low-dimensional latent representations of milestones, which makes it possible to efficiently perform long-term planning and vision-based control.Furthermore, our approach exploits generation flexibility of the diffusion model, which makes it possible to plan diverse trajectories for multi-task decision-making.We demonstrate the proposed method across offline reinforcement learning (RL) benchmarks and an visual manipulation environment.The results show that our approach outperforms offline RL methods in solving long-horizon, sparse-reward tasks and multi-task problems,while also achieving the state-of-the-art performance on the most challenging vision-based manipulation benchmark",
    "keywords": [],
    "checked": true,
    "id": "c9c04748167bb2dd39272a008f84d720a292b0d2",
    "semantic_title": "diffused task-agnostic milestone planner",
    "citation_count": 1,
    "authors": [
      "Mineui Hong",
      "Minjae Kang",
      "Songhwai Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/016c63403370d81c24c1ca0123de6cfa-Abstract-Conference.html": {
    "title": "Task-aware Distributed Source Coding under Dynamic Bandwidth",
    "volume": "main",
    "abstract": "Efficient compression of correlated data is essential to minimize communication overload in multi-sensor networks. In such networks, each sensor independently compresses the data and transmits them to a central node. A decoder at the central node decompresses and passes the data to a pre-trained machine learning-based task model to generate the final output. Due to limited communication bandwidth, it is important for the compressor to learn only the features that are relevant to the task. Additionally, the final performance depends heavily on the total available bandwidth. In practice, it is common to encounter varying availability in bandwidth. Since higher bandwidth results in better performance, it is essential for the compressor to dynamically take advantage of the maximum available bandwidth at any instant. In this work, we propose a novel distributed compression framework composed of independent encoders and a joint decoder, which we call neural distributed principal component analysis (NDPCA). NDPCA flexibly compresses data from multiple sources to any available bandwidth with a single model, reducing compute and storage overhead. NDPCA achieves this by learning low-rank task representations and efficiently distributing bandwidth among sensors, thus providing a graceful trade-off between performance and bandwidth. Experiments show that NDPCA improves the success rate of multi-view robotic arm manipulation by 9% and the accuracy of object detection tasks on satellite imagery by 14% compared to an autoencoder with uniform bandwidth allocation",
    "keywords": [],
    "checked": true,
    "id": "42d5586f73bc8d55ba9057a1bad2ae93cf008524",
    "semantic_title": "task-aware distributed source coding under dynamic bandwidth",
    "citation_count": 1,
    "authors": [
      "Po-han Li",
      "Sravan Kumar Ankireddy",
      "Ruihan (Philip) Zhao",
      "Hossein Nourkhiz Mahjoub",
      "Ehsan Moradi Pari",
      "Ufuk Topcu",
      "Sandeep Chinchali",
      "Hyeji Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/01772a8b0420baec00c4d59fe2fbace6-Abstract-Conference.html": {
    "title": "ANTN: Bridging Autoregressive Neural Networks and Tensor Networks for Quantum Many-Body Simulation",
    "volume": "main",
    "abstract": "Quantum many-body physics simulation has important impacts on understanding fundamental science and has applications to quantum materials design and quantum technology. However, due to the exponentially growing size of the Hilbert space with respect to the particle number, a direct simulation is intractable. While representing quantum states with tensor networks and neural networks are the two state-of-the-art methods for approximate simulations, each has its own limitations in terms of expressivity and inductive bias. To address these challenges, we develop a novel architecture, Autoregressive Neural TensorNet (ANTN), which bridges tensor networks and autoregressive neural networks. We show that Autoregressive Neural TensorNet parameterizes normalized wavefunctions, allows for exact sampling, generalizes the expressivity of tensor networks and autoregressive neural networks, and inherits a variety of symmetries from autoregressive neural networks. We demonstrate our approach on quantum state learning as well as finding the ground state of the challenging 2D $J_1$-$J_2$ Heisenberg model with different systems sizes and coupling parameters, outperforming both tensor networks and autoregressive neural networks. Our work opens up new opportunities for quantum many-body physics simulation, quantum technology design, and generative modeling in artificial intelligence",
    "keywords": [],
    "checked": true,
    "id": "7ac57c5cd8b00f48226f3f2a018a1fa78ff48676",
    "semantic_title": "antn: bridging autoregressive neural networks and tensor networks for quantum many-body simulation",
    "citation_count": 1,
    "authors": [
      "Zhuo Chen",
      "Laker Newhouse",
      "Eddie Chen",
      "Di Luo",
      "Marin Soljacic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/017c897b4d85a744f345ccbf9d71e501-Abstract-Conference.html": {
    "title": "Causal Effect Identification in Uncertain Causal Networks",
    "volume": "main",
    "abstract": "Causal identification is at the core of the causal inference literature, where complete algorithms have been proposed to identify causal queries of interest. The validity of these algorithms hinges on the restrictive assumption of having access to a correctly specified causal structure. In this work, we study the setting where a probabilistic model of the causal structure is available. Specifically, the edges in a causal graph exist with uncertainties which may, for example, represent degree of belief from domain experts. Alternatively, the uncertainty about an edge may reflect the confidence of a particular statistical test. The question that naturally arises in this setting is: Given such a probabilistic graph and a specific causal effect of interest, what is the subgraph which has the highest plausibility and for which the causal effect is identifiable? We show that answering this question reduces to solving an NP-hard combinatorial optimization problem which we call the edge ID problem. We propose efficient algorithms to approximate this problem and evaluate them against both real-world networks and randomly generated graphs",
    "keywords": [],
    "checked": true,
    "id": "672aada2df911bf47db62da52466564f66e9ee84",
    "semantic_title": "causal effect identification in uncertain causal networks",
    "citation_count": 1,
    "authors": [
      "Sina Akbari",
      "Fateme Jamshidi",
      "Ehsan Mokhtarian",
      "Matthew Vowels",
      "Jalal Etesami",
      "Negar Kiyavash"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/01830c92c6558179fa6d7fb1edff692c-Abstract-Conference.html": {
    "title": "FAST: a Fused and Accurate Shrinkage Tree for Heterogeneous Treatment Effects Estimation",
    "volume": "main",
    "abstract": "This paper proposes a novel strategy for estimating the heterogeneous treatment effect called the Fused and Accurate Shrinkage Tree ($\\mathrm{FAST}$). Our approach utilizes both trial and observational data to improve the accuracy and robustness of the estimator. Inspired by the concept of shrinkage estimation in statistics, we develop an optimal weighting scheme and a corresponding estimator that balances the unbiased estimator based on the trial data with the potentially biased estimator based on the observational data. Specifically, combined with tree-based techniques, we introduce a new split criterion that utilizes both trial data and observational data to more accurately estimate the treatment effect. Furthermore, we confirm the consistency of our proposed tree-based estimator and demonstrate the effectiveness of our criterion in reducing prediction error through theoretical analysis. The advantageous finite sample performance of the $\\mathrm{FAST}$ and its ensemble version over existing methods is demonstrated via simulations and real data analysis",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Gu",
      "Caizhi Tang",
      "Han Yan",
      "Qing Cui",
      "Longfei Li",
      "Jun Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/01b681025fdbda8e935a66cc5bb6e9de-Abstract-Conference.html": {
    "title": "Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond",
    "volume": "main",
    "abstract": "Homophily is a graph property describing the tendency of edges to connect similar nodes; the opposite is called heterophily. It is often believed that heterophilous graphs are challenging for standard message-passing graph neural networks (GNNs), and much effort has been put into developing efficient methods for this setting. However, there is no universally agreed-upon measure of homophily in the literature. In this work, we show that commonly used homophily measures have critical drawbacks preventing the comparison of homophily levels across different datasets. For this, we formalize desirable properties for a proper homophily measure and verify which measures satisfy which properties. In particular, we show that a measure that we call adjusted homophily satisfies more desirable properties than other popular homophily measures while being rarely used in graph machine learning literature. Then, we go beyond the homophily-heterophily dichotomy and propose a new characteristic that allows one to further distinguish different sorts of heterophily. The proposed label informativeness (LI) characterizes how much information a neighbor's label provides about a node's label. We prove that this measure satisfies important desirable properties. We also observe empirically that LI better agrees with GNN performance compared to homophily measures, which confirms that it is a useful characteristic of the graph structure",
    "keywords": [],
    "checked": true,
    "id": "0be89c19da818232684355a558fed214b0ee0f4b",
    "semantic_title": "characterizing graph datasets for node classification: homophily-heterophily dichotomy and beyond",
    "citation_count": 3,
    "authors": [
      "Oleg Platonov",
      "Denis Kuznedelev",
      "Artem Babenko",
      "Liudmila Prokhorenkova"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/01d64478381c33e29ed611f1719f5a37-Abstract-Conference.html": {
    "title": "Equivariant Flow Matching with Hybrid Probability Transport for 3D Molecule Generation",
    "volume": "main",
    "abstract": "The generation of 3D molecules requires simultaneously deciding the categorical features (atom types) and continuous features (atom coordinates). Deep generative models, especially Diffusion Models (DMs), have demonstrated effectiveness in generating feature-rich geometries. However, existing DMs typically suffer from unstable probability dynamics with inefficient sampling speed. In this paper, we introduce geometric flow matching, which enjoys the advantages of both equivariant modeling and stabilized probability dynamics. More specifically, we propose a hybrid probability path where the coordinates probability path is regularized by an equivariant optimal transport, and the information between different modalities is aligned. Experimentally, the proposed method could consistently achieve better performance on multiple molecule generation benchmarks with 4.75$\\times$ speed up of sampling on average",
    "keywords": [],
    "checked": false,
    "id": "6d77d4a044ee4a1eb928a65f722cf218e2fddbfa",
    "semantic_title": "equivariant flow matching with hybrid probability transport",
    "citation_count": 2,
    "authors": [
      "Yuxuan Song",
      "Jingjing Gong",
      "Minkai Xu",
      "Ziyao Cao",
      "Yanyan Lan",
      "Stefano Ermon",
      "Hao Zhou",
      "Wei-Ying Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/01ecd39ca49ddecc5729ca996304781b-Abstract-Conference.html": {
    "title": "Hyperbolic VAE via Latent Gaussian Distributions",
    "volume": "main",
    "abstract": "We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent space consists of a set of Gaussian distributions. It is known that the set of the univariate Gaussian distributions with the Fisher information metric form a hyperbolic space, which we call a Gaussian manifold. To learn the VAE endowed with the Gaussian manifolds, we propose a pseudo-Gaussian manifold normal distribution based on the Kullback-Leibler divergence, a local approximation of the squared Fisher-Rao distance, to define a density over the latent space. We demonstrate the efficacy of GM-VAE on two different tasks: density estimation of image datasets and state representation learning for model-based reinforcement learning. GM-VAE outperforms the other variants of hyperbolic- and Euclidean-VAEs on density estimation tasks and shows competitive performance in model-based reinforcement learning. We observe that our model provides strong numerical stability, addressing a common limitation reported in previous hyperbolic-VAEs. The implementation is available at https://github.com/ml-postech/GM-VAE",
    "keywords": [],
    "checked": true,
    "id": "1878cb3f590ecb47076c47cade53db18f724e878",
    "semantic_title": "hyperbolic vae via latent gaussian distributions",
    "citation_count": 0,
    "authors": [
      "Seunghyuk Cho",
      "Juyong Lee",
      "Dongwoo Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0203f489345567b4a048c38f507cdbfa-Abstract-Conference.html": {
    "title": "A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories",
    "volume": "main",
    "abstract": "Offline imitation from observations aims to solve MDPs where only task-specific expert states and task-agnostic non-expert state-action pairs are available. Offline imitation is useful in real-world scenarios where arbitrary interactions are costly and expert actions are unavailable. The state-of-the-art ‘DIstribution Correction Estimation' (DICE) methods minimize divergence of state occupancy between expert and learner policies and retrieve a policy with weighted behavior cloning; however, their results are unstable when learning from incomplete trajectories, due to a non-robust optimization in the dual domain. To address the issue, in this paper, we propose Trajectory-Aware Imitation Learning from Observations (TAILO). TAILO uses a discounted sum along the future trajectory as the weight for weighted behavior cloning. The terms for the sum are scaled by the output of a discriminator, which aims to identify expert states. Despite simplicity, TAILO works well if there exist trajectories or segments of expert behavior in the task-agnostic data, a common assumption in prior work. In experiments across multiple testbeds, we find TAILO to be more robust and effective, particularly with incomplete trajectories",
    "keywords": [],
    "checked": true,
    "id": "6adce69f20e284e91534f2ec922b5c231ff551e6",
    "semantic_title": "a simple solution for offline imitation from observations and examples with possibly incomplete trajectories",
    "citation_count": 0,
    "authors": [
      "Kai Yan",
      "Alex Schwing",
      "Yu-Xiong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0207c9ea9faf66c6e892c3fa3c167b75-Abstract-Conference.html": {
    "title": "Defending against Data-Free Model Extraction by Distributionally Robust Defensive Training",
    "volume": "main",
    "abstract": "Data-Free Model Extraction (DFME) aims to clone a black-box model without knowing its original training data distribution, making it much easier for attackers to steal commercial models. Defense against DFME faces several challenges: (i) effectiveness; (ii) efficiency; (iii) no prior on the attacker's query data distribution and strategy. However, existing defense methods: (1) are highly computation and memory inefficient; or (2) need strong assumptions about attack data distribution; or (3) can only delay the attack or prove a model theft after the model stealing has happened. In this work, we propose a Memory and Computation efficient defense approach, named MeCo, to prevent DFME from happening while maintaining the model utility simultaneously by distributionally robust defensive training on the target victim model. Specifically, we randomize the input so that it: (1) causes a mismatch of the knowledge distillation loss for attackers; (2) disturbs the zeroth-order gradient estimation; (3) changes the label prediction for the attack query data. Therefore, the attacker can only extract misleading information from the black-box model. Extensive experiments on defending against both decision-based and score-based DFME demonstrate that MeCo can significantly reduce the effectiveness of existing DFME methods and substantially improve running efficiency",
    "keywords": [],
    "checked": false,
    "id": "73a70eb1b3c914e87c2f7c0516a66d655841bdf7",
    "semantic_title": "removing batch normalization boosts adversarial training",
    "citation_count": 23,
    "authors": [
      "Zhenyi Wang",
      "Li Shen",
      "Tongliang Liu",
      "Tiehang Duan",
      "Yanjun Zhu",
      "Donglin Zhan",
      "DAVID DOERMANN",
      "Mingchen Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/020ad0ac6a1974e6748e4a5a48110a07-Abstract-Conference.html": {
    "title": "Large language models transition from integrating across position-yoked, exponential windows to structure-yoked, power-law windows",
    "volume": "main",
    "abstract": "Modern language models excel at integrating across long temporal scales needed to encode linguistic meaning and show non-trivial similarities to biological neural systems. Prior work suggests that human brain responses to language exhibit hierarchically organized \"integration windows\" that substantially constrain the overall influence of an input token (e.g., a word) on the neural response. However, little prior work has attempted to use integration windows to characterize computations in large language models (LLMs). We developed a simple word-swap procedure for estimating integration windows from black-box language models that does not depend on access to gradients or knowledge of the model architecture (e.g., attention weights). Using this method, we show that trained LLMs exhibit stereotyped integration windows that are well-fit by a convex combination of an exponential and a power-law function, with a partial transition from exponential to power-law dynamics across network layers. We then introduce a metric for quantifying the extent to which these integration windows vary with structural boundaries (e.g., sentence boundaries), and using this metric, we show that integration windows become increasingly yoked to structure at later network layers. None of these findings were observed in an untrained model, which as expected integrated uniformly across its input. These results suggest that LLMs learn to integrate information in natural language using a stereotyped pattern: integrating across position-yoked, exponential windows at early layers, followed by structure-yoked, power-law windows at later layers. The methods we describe in this paper provide a general-purpose toolkit for understanding temporal integration in language models, facilitating cross-disciplinary research at the intersection of biological and artificial intelligence",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Skrill",
      "Samuel Norman-Haignere"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/022ca1bed6b574b962c48a2856eb207b-Abstract-Conference.html": {
    "title": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?",
    "volume": "main",
    "abstract": "We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual ‘foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community",
    "keywords": [],
    "checked": true,
    "id": "326f6a8011e43322c433751b9cc31fd56564621c",
    "semantic_title": "where are we in the search for an artificial visual cortex for embodied intelligence?",
    "citation_count": 53,
    "authors": [
      "Arjun Majumdar",
      "Karmesh Yadav",
      "Sergio Arnaud",
      "Jason Ma",
      "Claire Chen",
      "Sneha Silwal",
      "Aryan Jain",
      "Vincent-Pierre Berges",
      "Tingfan Wu",
      "Jay Vakil",
      "Pieter Abbeel",
      "Jitendra Malik",
      "Dhruv Batra",
      "Yixin Lin",
      "Oleksandr Maksymets",
      "Aravind Rajeswaran",
      "Franziska Meier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0252a434b18962c94910c07cd9a7fecc-Abstract-Conference.html": {
    "title": "Belief Projection-Based Reinforcement Learning for Environments with Delayed Feedback",
    "volume": "main",
    "abstract": "We present a novel actor-critic algorithm for an environment with delayed feedback, which addresses the state-space explosion problem of conventional approaches. Conventional approaches use an augmented state constructed from the last observed state and actions executed since visiting the last observed state. Using the augmented state space, the correct Markov decision process for delayed environments can be constructed; however, this causes the state space to explode as the number of delayed timesteps increases, leading to slow convergence. Our proposed algorithm, called Belief-Projection-Based Q-learning (BPQL), addresses the state-space explosion problem by evaluating the values of the critic for which the input state size is equal to the original state-space size rather than that of the augmented one. We compare BPQL to traditional approaches in continuous control tasks and demonstrate that it significantly outperforms other algorithms in terms of asymptotic performance and sample efficiency. We also show that BPQL solves long-delayed environments, which conventional approaches are unable to do",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jangwon Kim",
      "Hangyeol Kim",
      "Jiwook Kang",
      "Jongchan Baek",
      "Soohee Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0266d95023740481d22d437aa8aba0e9-Abstract-Conference.html": {
    "title": "Batchnorm Allows Unsupervised Radial Attacks",
    "volume": "main",
    "abstract": "The construction of adversarial examples usually requires the existence of soft or hard labels for each instance, with respect to which a loss gradient provides the signal for construction of the example. We show that for batch normalized deep image recognition architectures, intermediate latents that are produced after a batch normalization step by themselves suffice to produce adversarial examples using an intermediate loss solely utilizing angular deviations, without relying on any label. We motivate our loss through the geometry of batch normed representations and their concentration of norm on a hypersphere and distributional proximity to Gaussians. Our losses expand intermediate latent based attacks that usually require labels. The success of our method implies that leakage of intermediate representations may create a security breach for deployed models, which persists even when the model is transferred to downstream usage. Removal of batch norm weakens our attack, indicating it contributes to this vulnerability. Our attacks also succeed against LayerNorm empirically, thus being relevant for transformer architectures, most notably vision transformers which we analyze",
    "keywords": [],
    "checked": true,
    "id": "c57ab32f4f844cbf11b243b44df94d02138ebd22",
    "semantic_title": "batchnorm allows unsupervised radial attacks",
    "citation_count": 0,
    "authors": [
      "Amur Ghose",
      "Apurv Gupta",
      "Yaoliang Yu",
      "Pascal Poupart"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/02687e7b22abc64e651be8da74ec610e-Abstract-Conference.html": {
    "title": "Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models",
    "volume": "main",
    "abstract": "Human-object interaction (HOI) detection aims to comprehend the intricate relationships between humans and objects, predicting triplets, and serving as the foundation for numerous computer vision tasks. The complexity and diversity of human-object interactions in the real world, however, pose significant challenges for both annotation and recognition, particularly in recognizing interactions within an open world context. This study explores the universal interaction recognition in an open-world setting through the use of Vision-Language (VL) foundation models and large language models (LLMs). The proposed method is dubbed as UniHOI. We conduct a deep analysis of the three hierarchical features inherent in visual HOI detectors and propose a method for high-level relation extraction aimed at VL foundation models, which we call HO prompt-based learning. Our design includes an HO Prompt-guided Decoder (HOPD), facilitates the association of high-level relation representations in the foundation model with various HO pairs within the image. Furthermore, we utilize a LLM (i.e. GPT) for interaction interpretation, generating a richer linguistic understanding for complex HOIs. For open-category interaction recognition, our method supports either of two input types: interaction phrase or interpretive sentence. Our efficient architecture design and learning methods effectively unleash the potential of the VL foundation models and LLMs, allowing UniHOI to surpass all existing methods with a substantial margin, under both supervised and zero-shot settings. The code and pre-trained weights will be made publicly available",
    "keywords": [],
    "checked": true,
    "id": "c5ab539ab10d85a784487324360029fd5cf96539",
    "semantic_title": "detecting any human-object interaction relationship: universal hoi detector with spatial prompt learning on foundation models",
    "citation_count": 0,
    "authors": [
      "Yichao Cao",
      "Qingfei Tang",
      "Xiu Su",
      "Song Chen",
      "Shan You",
      "Xiaobo Lu",
      "Chang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/02763667a5761ff92bb15d8751bcd223-Abstract-Conference.html": {
    "title": "Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models",
    "volume": "main",
    "abstract": "We focus on the task of learning a single index model $\\sigma(w^\\star \\cdot x)$ with respect to the isotropic Gaussian distribution in $d$ dimensions. Prior work has shown that the sample complexity of learning $w^\\star$ is governed by the information exponent $k^\\star$ of the link function $\\sigma$, which is defined as the index of the first nonzero Hermite coefficient of $\\sigma$. Ben Arous et al. (2021) showed that $n \\gtrsim d^{k^\\star-1}$ samples suffice for learning $w^\\star$ and that this is tight for online SGD. However, the CSQ lower bound for gradient based methods only shows that $n \\gtrsim d^{k^\\star/2}$ samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns $w^\\star$ with $n \\gtrsim d^{k^\\star/2}$ samples. We also draw connections to statistical analyses of tensor PCA and to the implicit regularization effects of minibatch SGD on empirical losses",
    "keywords": [],
    "checked": true,
    "id": "df29153629c3f00bdbbe1ea28f32ce379e35a144",
    "semantic_title": "smoothing the landscape boosts the signal for sgd: optimal sample complexity for learning single index models",
    "citation_count": 6,
    "authors": [
      "Alex Damian",
      "Eshaan Nichani",
      "Rong Ge",
      "Jason D. Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/027e86facfe7c1ea52ca1fca7bc1402b-Abstract-Conference.html": {
    "title": "A Scale-Invariant Sorting Criterion to Find a Causal Order in Additive Noise Models",
    "volume": "main",
    "abstract": "Additive Noise Models (ANMs) are a common model class for causal discovery from observational data. Due to a lack of real-world data for which an underlying ANM is known, ANMs with randomly sampled parameters are commonly used to simulate data for the evaluation of causal discovery algorithms. While some parameters may be fixed by explicit assumptions, fully specifying an ANM requires choosing all parameters. Reisach et al. (2021) show that, for many ANM parameter choices, sorting the variables by increasing variance yields an ordering close to a causal order and introduce ‘var-sortability' to quantify this alignment. Since increasing variances may be unrealistic and cannot be exploited when data scales are arbitrary, ANM data are often rescaled to unit variance in causal discovery benchmarking.We show that synthetic ANM data are characterized by another pattern that is scale-invariant and thus persists even after standardization: the explainable fraction of a variable's variance, as captured by the coefficient of determination $R^2$, tends to increase along the causal order. The result is high ‘$R^2$-sortability', meaning that sorting the variables by increasing $R^2$ yields an ordering close to a causal order. We propose a computationally efficient baseline algorithm termed ‘$R^2$-SortnRegress' that exploits high $R^2$-sortability and that can match and exceed the performance of established causal discovery algorithms. We show analytically that sufficiently high edge weights lead to a relative decrease of the noise contributions along causal chains, resulting in increasingly deterministic relationships and high $R^2$. We characterize $R^2$-sortability on synthetic data with different simulation parameters and find high values in common settings. Our findings reveal high $R^2$-sortability as an assumption about the data generating process relevant to causal discovery and implicit in many ANM sampling schemes. It should be made explicit, as its prevalence in real-world data is an open question. For causal discovery benchmarking, we provide implementations of $R^2$-sortability, the $R^2$-SortnRegress algorithm, and ANM simulation procedures in our library CausalDisco at https://causaldisco.github.io/CausalDisco/",
    "keywords": [],
    "checked": true,
    "id": "3b227285d0a4c8033d2da2b2e0bd085cf5d37e69",
    "semantic_title": "a scale-invariant sorting criterion to find a causal order in additive noise models",
    "citation_count": 4,
    "authors": [
      "Alexander Reisach",
      "Myriam Tami",
      "Christof Seiler",
      "Antoine Chambaz",
      "Sebastian Weichwald"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/028957869e560af14243ac37663a471e-Abstract-Conference.html": {
    "title": "PROTES: Probabilistic Optimization with Tensor Sampling",
    "volume": "main",
    "abstract": "We developed a new method PROTES for black-box optimization, which is based on the probabilistic sampling from a probability density function given in the low-parametric tensor train format. We tested it on complex multidimensional arrays and discretized multivariable functions taken, among others, from real-world applications, including unconstrained binary optimization and optimal control problems, for which the possible number of elements is up to $2^{1000}$. In numerical experiments, both on analytic model functions and on complex problems, PROTES outperforms popular discrete optimization methods (Particle Swarm Optimization, Covariance Matrix Adaptation, Differential Evolution, and others)",
    "keywords": [],
    "checked": true,
    "id": "c399f4724bde401589e206064e93e9663d93c675",
    "semantic_title": "protes: probabilistic optimization with tensor sampling",
    "citation_count": 5,
    "authors": [
      "Anastasiia Batsheva",
      "Andrei Chertkov",
      "Gleb Ryzhakov",
      "Ivan Oseledets"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/028fcbcf85435d39a40c4d61b42c99a4-Abstract-Conference.html": {
    "title": "Perturbation Towards Easy Samples Improves Targeted Adversarial Transferability",
    "volume": "main",
    "abstract": "The transferability of adversarial perturbations provides an effective shortcut for black-box attacks. Targeted perturbations have greater practicality but are more difficult to transfer between models. In this paper, we experimentally and theoretically demonstrated that neural networks trained on the same dataset have more consistent performance in High-Sample-Density-Regions (HSDR) of each class instead of low sample density regions. Therefore, in the target setting, adding perturbations towards HSDR of the target class is more effective in improving transferability. However, density estimation is challenging in high-dimensional scenarios. Further theoretical and experimental verification demonstrates that easy samples with low loss are more likely to be located in HSDR. Perturbations towards such easy samples in the target class can avoid density estimation for HSDR location. Based on the above facts, we verified that adding perturbations to easy samples in the target class improves targeted adversarial transferability of existing attack methods. A generative targeted attack strategy named Easy Sample Matching Attack (ESMA) is proposed, which has a higher success rate for targeted attacks and outperforms the SOTA generative method. Moreover, ESMA requires only $5\\%$ of the storage space and much less computation time comparing to the current SOTA, as ESMA attacks all classes with only one model instead of seperate models for each class. Our code is available at https://github.com/gjq100/ESMA",
    "keywords": [],
    "checked": false,
    "id": "96ea0658b9813706834966433107f9343e8389db",
    "semantic_title": "danaa: towards transferable attacks with double adversarial neuron attribution",
    "citation_count": 3,
    "authors": [
      "Junqi Gao",
      "Biqing Qi",
      "Yao Li",
      "Zhichang Guo",
      "Dong Li",
      "Yuming Xing",
      "Dazhi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/029df12a9363313c3e41047844ecad94-Abstract-Conference.html": {
    "title": "AVIS: Autonomous Visual Information Seeking with Large Language Model Agent",
    "volume": "main",
    "abstract": "In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs via tree search, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as \"What event is commemorated by the building depicted in this image?\", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-based visual question answering benchmarks such as Infoseek and OK-VQA",
    "keywords": [],
    "checked": false,
    "id": "d98536f24272e258b1d399074b64284d64786099",
    "semantic_title": "avis: autonomous visual information seeking with large language models",
    "citation_count": 9,
    "authors": [
      "Ziniu Hu",
      "Ahmet Iscen",
      "Chen Sun",
      "Kai-Wei Chang",
      "Yizhou Sun",
      "David Ross",
      "Cordelia Schmid",
      "Alireza Fathi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/029f699912bf3db747fe110948cc6169-Abstract-Conference.html": {
    "title": "Conformal Prediction Sets for Ordinal Classification",
    "volume": "main",
    "abstract": "Ordinal classification (OC), i.e., labeling instances along classes with a natural ordering, is common in multiple applications such as size or budget based recommendations and disease severity labeling. Often in practical scenarios, it is desirable to obtain a small set of likely classes with a guaranteed high chance of including the true class. Recent works on conformal prediction (CP) address this problem for the classification setting with non-ordered labels but the resulting prediction sets (PS) are often non-contiguous and unsuitable for ordinal classification. In this work, we propose a framework to adapt existing CP methods to generate contiguous sets with guaranteed coverage and minimal cardinality. Our framework employs a novel non-parametric approach for modeling unimodal distributions. Empirical results on both synthetic and real-world datasets demonstrate our method outperforms SOTA baselines by 4% on Accuracy@K and 8% on PS size",
    "keywords": [],
    "checked": false,
    "id": "65b650c866c6c21a06a80dd9d942568edb098f1c",
    "semantic_title": "conformal risk control for ordinal classification",
    "citation_count": 0,
    "authors": [
      "Prasenjit Dey",
      "Srujana Merugu",
      "Sivaramakrishnan R Kaveri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/02a589ef9a4f6f1e2dcc1cfb3b978a51-Abstract-Conference.html": {
    "title": "Minimax-Optimal Location Estimation",
    "volume": "main",
    "abstract": "Location estimation is one of the most basic questions in parametric statistics. Suppose we have a known distribution density $f$, and we get $n$ i.i.d. samples from $f(x-\\mu)$ for some unknown shift $\\mu$.The task is to estimate $\\mu$ to high accuracy with high probability.The maximum likelihood estimator (MLE) is known to be asymptotically optimal as $n \\to \\infty$, but what is possible for finite $n$?In this paper, we give two location estimators that are optimal under different criteria: 1) an estimator that has minimax-optimal estimation error subject to succeeding with probability $1-\\delta$ and 2) a confidence interval estimator which, subject to its output interval containing $\\mu$ with probability at least $1-\\delta$, has the minimum expected squared interval width among all shift-invariant estimators.The latter construction can be generalized to minimizing the expectation of any loss function on the interval width",
    "keywords": [],
    "checked": true,
    "id": "0ff10d013be0c1fa159d83935a82a4d4140c4e36",
    "semantic_title": "minimax-optimal location estimation",
    "citation_count": 1,
    "authors": [
      "Shivam Gupta",
      "Jasper Lee",
      "Eric Price",
      "Paul Valiant"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/02a92b52670752daf17b53f04f1ab405-Abstract-Conference.html": {
    "title": "Tight Bounds for Volumetric Spanners and Applications",
    "volume": "main",
    "abstract": "Given a set of points of interest, a volumetric spanner is a subset of the points using which all the points can be expressed using \"small\" coefficients (measured in an appropriate norm). Formally, given a set of vectors $X = [v_1, v_2, \\dots, v_n]$, the goal is to find $T \\subseteq [n]$ such that every $v \\in X$ can be expressed as $\\sum_{i\\in T} \\alpha_i v_i$, with $\\Vert \\alpha \\Vert$ being small. This notion, which has also been referred to as a well-conditioned basis, has found several applications, including bandit linear optimization, determinant maximization, and matrix low rank approximation. In this paper, we give almost optimal bounds on the size of volumetric spanners for all $\\ell_p$ norms, and show that they can be constructed using a simple local search procedure. We then show the applications of our result to other tasks and in particular the problem of finding coresets for the Minimum Volume Enclosing Ellipsoid (MVEE) problem",
    "keywords": [],
    "checked": true,
    "id": "1b6648ee99533c59e888d78a77972de16c26f105",
    "semantic_title": "tight bounds for volumetric spanners and applications",
    "citation_count": 0,
    "authors": [
      "Aditya Bhaskara",
      "Sepideh Mahabadi",
      "Ali Vakilian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/02dd0db10c40092de3d9ec2508d12f60-Abstract-Conference.html": {
    "title": "Learning better with Dale's Law: A Spectral Perspective",
    "volume": "main",
    "abstract": "Most recurrent neural networks (RNNs) do not include a fundamental constraint of real neural circuits: Dale's Law, which implies that neurons must be excitatory (E) or inhibitory (I). Dale's Law is generally absent from RNNs because simply partitioning a standard network's units into E and I populations impairs learning. However, here we extend a recent feedforward bio-inspired EI network architecture, named Dale's ANNs, to recurrent networks, and demonstrate that good performance is possible while respecting Dale's Law. This begs the question: What makes some forms of EI network learn poorly and others learn well? And, why does the simple approach of incorporating Dale's Law impair learning? Historically the answer was thought to be the sign constraints on EI network parameters, and this was a motivation behind Dale's ANNs. However, here we show the spectral properties of the recurrent weight matrix at initialisation are more impactful on network performance than sign constraints. We find that simple EI partitioning results in a singular value distribution that is multimodal and dispersed, whereas standard RNNs have an unimodal, more clustered singular value distribution, as do recurrent Dale's ANNs. We also show that the spectral properties and performance of partitioned EI networks are worse for small networks with fewer I units, and we present normalised SVD entropy as a measure of spectrum pathology that correlates with performance. Overall, this work sheds light on a long-standing mystery in neuroscience-inspired AI and computational neuroscience, paving the way for greater alignment between neural networks and biology",
    "keywords": [],
    "checked": true,
    "id": "c7c58af22c4b387379f49993b53ccece7b8ddd73",
    "semantic_title": "learning better with dale's law: a spectral perspective",
    "citation_count": 2,
    "authors": [
      "Pingsheng Li",
      "Jonathan Cornford",
      "Arna Ghosh",
      "Blake Richards"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/02dec8877fb7c6aa9a79f81661baca7c-Abstract-Conference.html": {
    "title": "Dense-Exponential Random Features: Sharp Positive Estimators of the Gaussian Kernel",
    "volume": "main",
    "abstract": "The problem of efficient approximation of a linear operator induced by the Gaussian or softmax kernel is often addressed using random features (RFs) which yield an unbiased approximation of the operator's result. Such operators emerge in important applications ranging from kernel methods to efficient Transformers. We propose parameterized, positive, non-trigonometric RFs which approximate Gaussian and softmax-kernels. In contrast to traditional RF approximations, parameters of these new methods can be optimized to reduce the variance of the approximation, and the optimum can be expressed in closed form. We show that our methods lead to variance reduction in practice (e^{10}-times smaller variance and beyond) and outperform previous methods in a kernel regression task. Using our proposed mechanism, we also present FAVOR#, a method for self-attention approximation in Transformers. We show that FAVOR# outperforms other random feature methods in speech modelling and natural language processing",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valerii Likhosherstov",
      "Krzysztof M Choromanski",
      "Kumar Avinava Dubey",
      "Frederick Liu",
      "Tamas Sarlos",
      "Adrian Weller"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/03261886741f1f21f52f2a2d570616a2-Abstract-Conference.html": {
    "title": "Projection-Free Online Convex Optimization via Efficient Newton Iterations",
    "volume": "main",
    "abstract": "This paper presents new projection-free algorithms for Online Convex Optimization (OCO) over a convex domain $\\mathcal{K} \\subset \\mathbb{R}^d$. Classical OCO algorithms (such as Online Gradient Descent) typically need to perform Euclidean projections onto the convex set $\\mathcal{K}$ to ensure feasibility of their iterates. Alternative algorithms, such as those based on the Frank-Wolfe method, swap potentially-expensive Euclidean projections onto $\\mathcal{K}$ for linear optimization over $\\mathcal{K}$. However, such algorithms have a sub-optimal regret in OCO compared to projection-based algorithms. In this paper, we look at a third type of algorithms that output approximate Newton iterates using a self-concordant barrier for the set of interest. The use of a self-concordant barrier automatically ensures feasibility without the need of projections. However, the computation of the Newton iterates requires a matrix inverse, which can still be expensive. As our main contribution, we show how the stability of the Newton iterates can be leveraged to only compute the inverse Hessian a vanishing fractions of the rounds, leading to a new efficient projection-free OCO algorithm with a state-of-the-art regret bound",
    "keywords": [],
    "checked": true,
    "id": "d67dc1d5259e2846767cdd7c42221c5a22775d32",
    "semantic_title": "projection-free online convex optimization via efficient newton iterations",
    "citation_count": 2,
    "authors": [
      "Khashayar Gatmiry",
      "Zak Mhammedi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/034d7bfeace2a9a258648b16fc626298-Abstract-Conference.html": {
    "title": "Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals",
    "volume": "main",
    "abstract": "High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. Experimentally, various RL algorithms obtain significant improvement in performance and training speed when assisted by our design. Code at github.com/Holmeswww/RnR",
    "keywords": [],
    "checked": true,
    "id": "61678a9f1d8291bb0f3d704a439ac8cd64fa6482",
    "semantic_title": "read and reap the rewards: learning to play atari with the help of instruction manuals",
    "citation_count": 15,
    "authors": [
      "Yue Wu",
      "Yewen Fan",
      "Paul Pu Liang",
      "Amos Azaria",
      "Yuanzhi Li",
      "Tom M. Mitchell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0354767c6386386be17cabe4fc59711b-Abstract-Conference.html": {
    "title": "Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization",
    "volume": "main",
    "abstract": "Despite extensive studies, the underlying reason as to why overparameterizedneural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thusa natural potential explanation is that flatness implies generalization. This workcritically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1)flatness provably implies generalization; (2) there exist non-generalizing flattestmodels and sharpness minimization algorithms fail to generalize poorly, and (3)perhaps most strikingly, there exist non-generalizing flattest models, but sharpnessminimization algorithms still generalize. Our results suggest that the relationshipbetween sharpness and generalization subtly depends on the data distributionsand the model architectures and sharpness minimization algorithms do not onlyminimize sharpness to achieve better generalization. This calls for the search forother explanations for the generalization of over-parameterized neural networks",
    "keywords": [],
    "checked": true,
    "id": "0c311d188f2708e2f3a90ac03cfdd68c5523009b",
    "semantic_title": "sharpness minimization algorithms do not only minimize sharpness to achieve better generalization",
    "citation_count": 7,
    "authors": [
      "Kaiyue Wen",
      "Zhiyuan Li",
      "Tengyu Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/03600ae6c3392fd65ad7c3a90c6f7ce8-Abstract-Conference.html": {
    "title": "Feature-Learning Networks Are Consistent Across Widths At Realistic Scales",
    "volume": "main",
    "abstract": "We study the effect of width on the dynamics of feature-learning neural networks across a variety of architectures and datasets. Early in training, wide neural networks trained on online data have not only identical loss curves but also agree in their point-wise test predictions throughout training. For simple tasks such as CIFAR-5m this holds throughout training for networks of realistic widths. We also show that structural properties of the models, including internal representations, preactivation distributions, edge of stability phenomena, and large learning rate effects are consistent across large widths. This motivates the hypothesis that phenomena seen in realistic models can be captured by infinite-width, feature-learning limits. For harder tasks (such as ImageNet and language modeling), and later training times, finite-width deviations grow systematically. Two distinct effects cause these deviations across widths. First, the network output has an initialization-dependent variance scaling inversely with width, which can be removed by ensembling networks. We observe, however, that ensembles of narrower networks perform worse than a single wide network. We call this the bias of narrower width. We conclude with a spectral perspective on the origin of this finite-width bias",
    "keywords": [],
    "checked": true,
    "id": "5875e7cc6d8941f078e32b51b8d36fc08f9c1774",
    "semantic_title": "feature-learning networks are consistent across widths at realistic scales",
    "citation_count": 7,
    "authors": [
      "Nikhil Vyas",
      "Alexander Atanasov",
      "Blake Bordelon",
      "Depen Morwani",
      "Sabarish Sainathan",
      "Cengiz Pehlevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/036912a83bdbb1fd792baf6532f102d8-Abstract-Conference.html": {
    "title": "Taylor TD-learning",
    "volume": "main",
    "abstract": "Many reinforcement learning approaches rely on temporal-difference (TD) learning to learn a critic.However, TD-learning updates can be high variance.Here, we introduce a model-based RL framework, Taylor TD, which reduces this variance in continuous state-action settings. Taylor TD uses a first-order Taylor series expansion of TD updates.This expansion allows Taylor TD to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution for the initial state and action of each TD update.We include theoretical and empirical evidence that Taylor TD updates are indeed lower variance than standard TD updates. Additionally, we show Taylor TD has the same stable learning guarantees as standard TD-learning with linear function approximation under a reasonable assumption.Next, we combine Taylor TD with the TD3 algorithm, forming TaTD3.We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based baseline algorithms on a set of standard benchmark tasks",
    "keywords": [],
    "checked": true,
    "id": "9b8a0da395016bb27da95acd5402511e6857e456",
    "semantic_title": "taylor td-learning",
    "citation_count": 0,
    "authors": [
      "Michele Garibbo",
      "Maxime Robeyns",
      "Laurence Aitchison"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/03a9a9c1e15850439653bb971a4ad4b3-Abstract-Conference.html": {
    "title": "Calibrating Neural Simulation-Based Inference with Differentiable Coverage Probability",
    "volume": "main",
    "abstract": "Bayesian inference allows expressing the uncertainty of posterior belief under a probabilistic model given prior information and the likelihood of the evidence. Predominantly, the likelihood function is only implicitly established by a simulator posing the need for simulation-based inference (SBI). However, the existing algorithms can yield overconfident posteriors (Hermans et al., 2022) defeating the whole purpose of credibility if the uncertainty quantification is inaccurate. We propose to include a calibration term directly into the training objective of the neural model in selected amortized SBI techniques. By introducing a relaxation of the classical formulation of calibration error we enable end-to-end backpropagation. The proposed method is not tied to any particular neural model and brings moderate computational overhead compared to the profits it introduces. It is directly applicable to existing computational pipelines allowing reliable black-box posterior inference. We empirically show on six benchmark problems that the proposed method achieves competitive or better results in terms of coverage and expected posterior density than the previously existing approaches",
    "keywords": [],
    "checked": true,
    "id": "f52ed85c1f20a6befb1b798cbb43fe2705e749fc",
    "semantic_title": "calibrating neural simulation-based inference with differentiable coverage probability",
    "citation_count": 0,
    "authors": [
      "Maciej Falkiewicz",
      "Naoya Takeishi",
      "Imahn Shekhzadeh",
      "Antoine Wehenkel",
      "Arnaud Delaunoy",
      "Gilles Louppe",
      "Alexandros Kalousis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/03b1043052700b1a471996b0baf309d4-Abstract-Conference.html": {
    "title": "Agnostic Multi-Group Active Learning",
    "volume": "main",
    "abstract": "Inspired by the problem of improving classification accuracy on rare or hard subsets of a population, there has been recent interest in models of learning where the goal is to generalize to a collection of distributions, each representing a ``group''. We consider a variant of this problem from the perspective of active learning, where the learner is endowed with the power to decide which examples are labeled from each distribution in the collection, and the goal is to minimize the number of label queries while maintaining PAC-learning guarantees. Our main challenge is that standard active learning techniques such as disagreement-based active learning do not directly apply to the multi-group learning objective. We modify existing algorithms to provide a consistent active learning algorithm for an agnostic formulation of multi-group learning, which given a collection of $G$ distributions and a hypothesis class $\\mathcal{H}$ with VC-dimension $d$, outputs an $\\epsilon$-optimal hypothesis using $\\tilde{O}\\left( (\\nu^2/\\epsilon^2) G d \\theta_{\\mathcal{G}}^2 \\log^2(1/\\epsilon) + G\\log(1/\\epsilon)/\\epsilon^2 \\right)$ label queries, where $\\theta_{\\mathcal{G}}$ is the worst-case disagreement coefficient over the collection. Roughly speaking, this guarantee improves upon the label complexity of standard multi-group learning in regimes where disagreement-based active learning algorithms may be expected to succeed, and the number of groups is not too large. We also consider the special case where each distribution in the collection is individually realizable with respect to $\\mathcal{H}$, and demonstrate $\\tilde{O}\\left( G d \\theta_{\\mathcal{G}} \\log(1/\\epsilon) \\right)$ label queries are sufficient for learning in this case. We further give an approximation result for the full agnostic case inspired by the group realizable strategy",
    "keywords": [],
    "checked": true,
    "id": "0c981a9995f95a17e2fed4d3a08e2926a65bea1e",
    "semantic_title": "agnostic multi-group active learning",
    "citation_count": 0,
    "authors": [
      "Nicholas Rittler",
      "Kamalika Chaudhuri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/03b13b0db740b95cb741e007178ef5e5-Abstract-Conference.html": {
    "title": "Self-Weighted Contrastive Learning among Multiple Views for Mitigating Representation Degeneration",
    "volume": "main",
    "abstract": "Recently, numerous studies have demonstrated the effectiveness of contrastive learning (CL), which learns feature representations by pulling in positive samples while pushing away negative samples. Many successes of CL lie in that there exists semantic consistency between data augmentations of the same instance. In multi-view scenarios, however, CL might cause representation degeneration when the collected multiple views inherently have inconsistent semantic information or their representations subsequently do not capture sufficient discriminative information. To address this issue, we propose a novel framework called SEM: SElf-weighted Multi-view contrastive learning with reconstruction regularization. Specifically, SEM is a general framework where we propose to first measure the discrepancy between pairwise representations and then minimize the corresponding self-weighted contrastive loss, and thus making SEM adaptively strengthen the useful pairwise views and also weaken the unreliable pairwise views. Meanwhile, we impose a self-supervised reconstruction term to regularize the hidden features of encoders, to assist CL in accessing sufficient discriminative information of data. Experiments on public multi-view datasets verified that SEM can mitigate representation degeneration in existing CL methods and help them achieve significant performance improvements. Ablation studies also demonstrated the effectiveness of SEM with different options of weighting strategies and reconstruction terms",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Xu",
      "Shuo Chen",
      "Yazhou Ren",
      "Xiaoshuang Shi",
      "Hengtao Shen",
      "Gang Niu",
      "Xiaofeng Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/03df5246cc78af497940338dd3eacbaa-Abstract-Conference.html": {
    "title": "Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features",
    "volume": "main",
    "abstract": "Recent studies have demonstrated the susceptibility of deep neural networks to backdoor attacks. Given a backdoored model, its prediction of a poisoned sample with trigger will be dominated by the trigger information, though trigger information and benign information coexist. Inspired by the mechanism of the optical polarizer that a polarizer could pass light waves with particular polarizations while filtering light waves with other polarizations, we propose a novel backdoor defense method by inserting a learnable neural polarizer into the backdoored model as an intermediate layer, in order to purify the poisoned sample via filtering trigger information while maintaining benign information. The neural polarizer is instantiated as one lightweight linear transformation layer, which is learned through solving a well designed bi-level optimization problem, based on a limited clean dataset. Compared to other fine-tuning-based defense methods which often adjust all parameters of the backdoored model, the proposed method only needs to learn one additional layer, such that it is more efficient and requires less clean data. Extensive experiments demonstrate the effectiveness and efficiency of our method in removing backdoors across various neural network architectures and datasets, especially in the case of very limited clean data. Codes are available at \\href{https://github.com/SCLBD/BackdoorBench}{https://github.com/SCLBD/BackdoorBench} (PyTorch) and \\href{https://github.com/JulieCarlon/NPD-MindSpore}{https://github.com/JulieCarlon/NPD-MindSpore} (MindSpore)",
    "keywords": [],
    "checked": true,
    "id": "efbc16949c62985b46290d50495c62ac0ef21e51",
    "semantic_title": "neural polarizer: a lightweight and effective backdoor defense via purifying poisoned features",
    "citation_count": 8,
    "authors": [
      "Mingli Zhu",
      "Shaokui Wei",
      "Hongyuan Zha",
      "Baoyuan Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/03e33e1f62e3302b47fe1d38a235921e-Abstract-Conference.html": {
    "title": "Tools for Verifying Neural Models' Training Data",
    "volume": "main",
    "abstract": "It is important that consumers and regulators can verify the provenance of large neural models to evaluate their capabilities and risks. We introduce the concept of a \"Proof-of-Training-Data\": any protocol that allows a model trainer to convince a Verifier of the training data that produced a set of model weights. Such protocols could verify the amount and kind of data and compute used to train the model, including whether it was trained on specific harmful or beneficial data sources. We explore efficient verification strategies for Proof-of-Training-Data that are compatible with most current large-model training procedures. These include a method for the model-trainer to verifiably pre-commit to a random seed used in training, and a method that exploits models' tendency to temporarily overfit to training data in order to detect whether a given data-point was included in training. We show experimentally that our verification procedures can catch a wide variety of attacks, including all known attacks from the Proof-of-Learning literature",
    "keywords": [],
    "checked": true,
    "id": "c12e232657d38c47116f9e7295ed6083f2b2c2b5",
    "semantic_title": "tools for verifying neural models' training data",
    "citation_count": 2,
    "authors": [
      "Dami Choi",
      "Yonadav Shavit",
      "David K. Duvenaud"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/040ace837dd270a87055bb10dd7c0392-Abstract-Conference.html": {
    "title": "Towards Higher Ranks via Adversarial Weight Pruning",
    "volume": "main",
    "abstract": "Convolutional Neural Networks (CNNs) are hard to deploy on edge devices due to its high computation and storage complexities. As a common practice for model compression, network pruning consists of two major categories: unstructured and structured pruning, where unstructured pruning constantly performs better. However, unstructured pruning presents a structured pattern at high pruning rates, which limits its performance. To this end, we propose a Rank-based PruninG (RPG) method to maintain the ranks of sparse weights in an adversarial manner. In each step, we minimize the low-rank approximation error for the weight matrices using singular value decomposition, and maximize their distance by pushing the weight matrices away from its low rank approximation. This rank-based optimization objective guides sparse weights towards a high-rank topology. The proposed method is conducted in a gradual pruning fashion to stabilize the change of rank during training. Experimental results on various datasets and different tasks demonstrate the effectiveness of our algorithm in high sparsity. The proposed RPG outperforms the state-of-the-art performance by 1.13\\% top-1 accuracy on ImageNet in ResNet-50 with 98\\% sparsity. The codes are available at https://github.com/huawei-noah/Efficient-Computing/tree/master/Pruning/RPG and https://gitee.com/mindspore/models/tree/master/research/cv/RPG",
    "keywords": [],
    "checked": true,
    "id": "132c3c726f3704c90097928880ecafd0f8a8978f",
    "semantic_title": "towards higher ranks via adversarial weight pruning",
    "citation_count": 0,
    "authors": [
      "Yuchuan Tian",
      "Hanting Chen",
      "Tianyu Guo",
      "Chao Xu",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/040d3b6af368bf71f952c18da5713b48-Abstract-Conference.html": {
    "title": "On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective",
    "volume": "main",
    "abstract": "Weight decay is a simple yet powerful regularization technique that has been very widely used in training of deep neural networks (DNNs). While weight decay has attracted much attention, previous studies fail to discover some overlooked pitfalls on large gradient norms resulted by weight decay. In this paper, we discover that, weight decay can unfortunately lead to large gradient norms at the final phase (or the terminated solution) of training, which often indicates bad convergence and poor generalization. To mitigate the gradient-norm-centered pitfalls, we present the first practical scheduler for weight decay, called the Scheduled Weight Decay (SWD) method that can dynamically adjust the weight decay strength according to the gradient norm and significantly penalize large gradient norms during training. Our experiments also support that SWD indeed mitigates large gradient norms and often significantly outperforms the conventional constant weight decay strategy for Adaptive Moment Estimation (Adam)",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeke Xie",
      "Zhiqiang Xu",
      "Jingzhao Zhang",
      "Issei Sato",
      "Masashi Sugiyama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/04261fce1705c4f02f062866717d592a-Abstract-Conference.html": {
    "title": "Leveraging Early-Stage Robustness in Diffusion Models for Efficient and High-Quality Image Synthesis",
    "volume": "main",
    "abstract": "While diffusion models have demonstrated exceptional image generation capabilities, the iterative noise estimation process required for these models is compute-intensive and their practical implementation is limited by slow sampling speeds. In this paper, we propose a novel approach to speed up the noise estimation network by leveraging the robustness of early-stage diffusion models. Our findings indicate that inaccurate computation during the early-stage of the reverse diffusion process has minimal impact on the quality of generated images, as this stage primarily outlines the image while later stages handle the finer details that require more sensitive information. To improve computational efficiency, we combine our findings with post-training quantization (PTQ) to introduce a method that utilizes low-bit activation for the early reverse diffusion process while maintaining high-bit activation for the later stages. Experimental results show that the proposed method can accelerate the early-stage computation without sacrificing the quality of the generated images",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulhwa Kim",
      "Dongwon Jo",
      "Hyesung Jeon",
      "Taesu Kim",
      "Daehyun Ahn",
      "Hyungjun Kim",
      "jae-joon kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0429ececfb199efc93182990169e73bb-Abstract-Conference.html": {
    "title": "Adversarial Model for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "We propose a novel model-based offline Reinforcement Learning (RL) framework, called Adversarial Model for Offline Reinforcement Learning (ARMOR), which can robustly learn policies to improve upon an arbitrary reference policy regardless of data coverage. ARMOR is designed to optimize policies for the worst-case performance relative to the reference policy through adversarially training a Markov decision process model. In theory, we prove that ARMOR, with a well-tuned hyperparameter, can compete with the best policy within data coverage when the reference policy is supported by the data. At the same time, ARMOR is robust to hyperparameter choices: the policy learned by ARMOR, with any admissible hyperparameter, would never degrade the performance of the reference policy, even when the reference policy is not covered by the dataset. To validate these properties in practice, we design a scalable implementation of ARMOR, which by adversarial training, can optimize policies without using model ensembles in contrast to typical model-based methods. We show that ARMOR achieves competent performance with both state-of-the-art offline model-free and model-based RL algorithms and can robustly improve the reference policy over various hyperparameter choices",
    "keywords": [],
    "checked": true,
    "id": "f1398927e7f78295f277ee2e0b437e26a04ec1c6",
    "semantic_title": "adversarial model for offline reinforcement learning",
    "citation_count": 10,
    "authors": [
      "Mohak Bhardwaj",
      "Tengyang Xie",
      "Byron Boots",
      "Nan Jiang",
      "Ching-An Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/043f0503c4f652c737add3690aa5d12c-Abstract-Conference.html": {
    "title": "Training Your Image Restoration Network Better with Random Weight Network as Optimization Function",
    "volume": "main",
    "abstract": "The blooming progress made in deep learning-based image restoration has been largely attributed to the availability of high-quality, large-scale datasets and advanced network structures. However, optimization functions such as L1 and L2 are still de facto. In this study, we propose to investigate new optimization functions to improve image restoration performance. Our key insight is that ``random weight network can be acted as a constraint for training better image restoration networks''. However, not all random weight networks are suitable as constraints. We draw inspiration from Functional theory and show that alternative random weight networks should be represented in the form of a strict mathematical manifold. We explore the potential of our random weight network prototypes that satisfy this requirement: Taylor's unfolding network, invertible neural network, central difference convolution, and zero-order filtering. We investigate these prototypes from four aspects: 1) random weight strategies, 2) network architectures, 3) network depths, and 4) combinations of random weight networks. Furthermore, we devise the random weight in two variants: the weights are randomly initialized only once during the entire training procedure, and the weights are randomly initialized in each training epoch. Our approach can be directly integrated into existing networks without incurring additional training and testing computational costs. We perform extensive experiments across multiple image restoration tasks, including image denoising, low-light image enhancement, and guided image super-resolution to demonstrate the consistent performance gains achieved by our method. Upon acceptance of this paper, we will release the code",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "man zhou",
      "Naishan Zheng",
      "Yuan Xu",
      "Chun-Le Guo",
      "Chongyi Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/045c87def0c02e3ad0d3d849766d7f1e-Abstract-Conference.html": {
    "title": "Passive learning of active causal strategies in agents and language models",
    "volume": "main",
    "abstract": "What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training.We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and have implications for understanding the behaviors and capabilities of language models",
    "keywords": [],
    "checked": true,
    "id": "ce0154d9251f67c262512b6e598f3aa3ba9fe9a4",
    "semantic_title": "passive learning of active causal strategies in agents and language models",
    "citation_count": 4,
    "authors": [
      "Andrew Lampinen",
      "Stephanie Chan",
      "Ishita Dasgupta",
      "Andrew Nam",
      "Jane Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/047397849f63b4fcfced4ff720159f3d-Abstract-Conference.html": {
    "title": "Zero-Regret Performative Prediction Under Inequality Constraints",
    "volume": "main",
    "abstract": "Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained problems, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradient is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stationary stochastic primal-dual algorithm without performativity. Based on this framework, we then propose an adaptive primal-dual algorithm for location families. Our analysis demonstrates that the proposed adaptive primal-dual algorithm attains $\\mathcal{O}(\\sqrt{T})$ regret and constraint violations, using only $\\sqrt{T} + 2T$ samples, where $T$ is the time horizon. To our best knowledge, this is the first study and analysis on the optimality of the performative prediction problem under inequality constraints. Finally, we validate the effectiveness of our algorithm and theoretical results through numerical simulations",
    "keywords": [],
    "checked": true,
    "id": "fd8dd3d442a3c95e1b05b9dd8fd6c4e5cd237cb5",
    "semantic_title": "zero-regret performative prediction under inequality constraints",
    "citation_count": 1,
    "authors": [
      "Wenjing YAN",
      "Xuanyu Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/047682108c3b053c61ad2da5a6057b4e-Abstract-Conference.html": {
    "title": "Towards Free Data Selection with General-Purpose Models",
    "volume": "main",
    "abstract": "A desirable data selection algorithm can efficiently choose the most informative samples to maximize the utility of limited annotation budgets. However, current approaches, represented by active learning methods, typically follow a cumbersome pipeline that iterates the time-consuming model training and batch data selection repeatedly. In this paper, we challenge this status quo by designing a distinct data selection pipeline that utilizes existing general-purpose models to select data from various datasets with a single-pass inference without the need for additional training or supervision. A novel free data selection (FreeSel) method is proposed following this new pipeline. Specifically, we define semantic patterns extracted from inter-mediate features of the general-purpose model to capture subtle local information in each image. We then enable the selection of all data samples in a single pass through distance-based sampling at the fine-grained semantic pattern level. FreeSel bypasses the heavy batch selection process, achieving a significant improvement in efficiency and being 530x faster than existing active learning methods. Extensive experiments verify the effectiveness of FreeSel on various computer vision tasks",
    "keywords": [],
    "checked": true,
    "id": "7fbdcd5fa2f246dcc8c4b789063372b710642f66",
    "semantic_title": "towards free data selection with general-purpose models",
    "citation_count": 2,
    "authors": [
      "Yichen Xie",
      "Mingyu Ding",
      "Masayoshi TOMIZUKA",
      "Wei Zhan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/04bd683d5428d91c5fbb5a7d2c27064d-Abstract-Conference.html": {
    "title": "Communication-Efficient Federated Bilevel Optimization with Global and Local Lower Level Problems",
    "volume": "main",
    "abstract": "Bilevel Optimization has witnessed notable progress recently with new emerging efficient algorithms. However, its application in the Federated Learning setting remains relatively underexplored, and the impact of Federated Learning's inherent challenges on the convergence of bilevel algorithms remain obscure.In this work, we investigate Federated Bilevel Optimization problems and propose a communication-efficient algorithm, named FedBiOAcc. The algorithm leverages an efficient estimation of the hyper-gradient in the distributed setting and utilizes the momentum-based variance-reduction acceleration. Remarkably, FedBiOAcc achieves a communication complexity $O(\\epsilon^{-1})$, a sample complexity $O(\\epsilon^{-1.5})$ and the linear speed up with respect to the number of clients. We also analyze a special case of the Federated Bilevel Optimization problems, where lower level problems are locally managed by clients. We prove that FedBiOAcc-Local, a modified version of FedBiOAcc, converges at the same rate for this type of problems. Finally, we validate the proposed algorithms through two real-world tasks: Federated Data-cleaning and Federated Hyper-representation Learning. Empirical results show superior performance of our algorithms",
    "keywords": [],
    "checked": false,
    "id": "7192ae0cce10ed34865f806dbbd812e90f9dd56e",
    "semantic_title": "communication-efficient federated bilevel optimization with local and global lower level problems",
    "citation_count": 3,
    "authors": [
      "Junyi Li",
      "Feihu Huang",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/04e05ba5cbc36044f6499d1edf15247e-Abstract-Conference.html": {
    "title": "Partial Multi-Label Learning with Probabilistic Graphical Disambiguation",
    "volume": "main",
    "abstract": "In partial multi-label learning (PML), each training example is associated with a set of candidate labels, among which only some labels are valid. As a common strategy to tackle PML problem, disambiguation aims to recover the ground-truth labeling information from such inaccurate annotations. However, existing approaches mainly rely on heuristics or ad-hoc rules to disambiguate candidate labels, which may not be universal enough in complicated real-world scenarios. To provide a principled way for disambiguation, we make a first attempt to explore the probabilistic graphical model for PML problem, where a directed graph is tailored to infer latent ground-truth labeling information from the generative process of partial multi-label data. Under the framework of stochastic gradient variational Bayes, a unified variational lower bound is derived for this graphical model, which is further relaxed probabilistically so that the desired prediction model can be induced with simultaneously identified ground-truth labeling information. Comprehensive experiments on multiple synthetic and real-world data sets show that our approach outperforms the state-of-the-art counterparts",
    "keywords": [],
    "checked": false,
    "id": "f9750a5a58af08e4b1e1672ac37926589f973445",
    "semantic_title": "understanding partial multi-label learning via mutual information",
    "citation_count": 7,
    "authors": [
      "Jun-Yi Hang",
      "Min-Ling Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/04f61ec02d1b3a025a59d978269ce437-Abstract-Conference.html": {
    "title": "Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks",
    "volume": "main",
    "abstract": "Most reinforcement learning methods rely heavily on dense, well-normalized environment rewards. DreamerV3 recently introduced a model-based method with a number of tricks that mitigate these limitations, achieving state-of-the-art on a wide range of benchmarks with a single set of hyperparameters. This result sparked discussion about the generality of the tricks, since they appear to be applicable to other reinforcement learning algorithms. Our work applies DreamerV3's tricks to PPO and is the first such empirical study outside of the original work. Surprisingly, we find that the tricks presented do not transfer as general improvements to PPO. We use a high quality PPO reference implementation and present extensive ablation studies totaling over 10,000 A100 hours on the Arcade Learning Environment and the DeepMind Control Suite. Though our experiments demonstrate that these tricks do not generally outperform PPO, we identify cases where they succeed and offer insight into the relationship between the implementation tricks. In particular, PPO with these tricks performs comparably to PPO on Atari games with reward clipping and significantly outperforms PPO without reward clipping",
    "keywords": [],
    "checked": true,
    "id": "ee73ed1ee6af52d44e26f02fe2f9188e0dad511a",
    "semantic_title": "reward scale robustness for proximal policy optimization via dreamerv3 tricks",
    "citation_count": 0,
    "authors": [
      "Ryan Sullivan",
      "Akarsh Kumar",
      "Shengyi Huang",
      "John Dickerson",
      "Joseph Suarez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0503f5dce343a1d06d16ba103dd52db1-Abstract-Conference.html": {
    "title": "Emergent Correspondence from Image Diffusion",
    "volume": "main",
    "abstract": "Finding correspondences between images is a fundamental problem in computer vision. In this paper, we show that correspondence emerges in image diffusion models without any explicit supervision. We propose a simple strategy to extract this implicit knowledge out of diffusion networks as image features, namely DIffusion FeaTures (DIFT), and use them to establish correspondences between real images. Without any additional fine-tuning or supervision on the task-specific data or annotations, DIFT is able to outperform both weakly-supervised methods and competitive off-the-shelf features in identifying semantic, geometric, and temporal correspondences. Particularly for semantic correspondence, DIFT from Stable Diffusion is able to outperform DINO and OpenCLIP by 19 and 14 accuracy points respectively on the challenging SPair-71k benchmark. It even outperforms the state-of-the-art supervised methods on 9 out of 18 categories while remaining on par for the overall performance. Project page: https://diffusionfeatures.github.io",
    "keywords": [],
    "checked": true,
    "id": "f421b314aaff48e463507034691cfdd3f93cd4c2",
    "semantic_title": "emergent correspondence from image diffusion",
    "citation_count": 48,
    "authors": [
      "Luming Tang",
      "Menglin Jia",
      "Qianqian Wang",
      "Cheng Perng Phoo",
      "Bharath Hariharan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0506ad3d1bcc8398a920db9340f27fe4-Abstract-Conference.html": {
    "title": "Robust Learning with Progressive Data Expansion Against Spurious Correlation",
    "volume": "main",
    "abstract": "While deep learning models have shown remarkable performance in various tasks, they are susceptible to learning non-generalizable _spurious features_ rather than the core features that are genuinely correlated to the true label. In this paper, beyond existing analyses of linear models, we theoretically examine the learning process of a two-layer nonlinear convolutional neural network in the presence of spurious features. Our analysis suggests that imbalanced data groups and easily learnable spurious features can lead to the dominance of spurious features during the learning process. In light of this, we propose a new training algorithm called **PDE** that efficiently enhances the model's robustness for a better worst-group performance. PDE begins with a group-balanced subset of training data and progressively expands it to facilitate the learning of the core features. Experiments on synthetic and real-world benchmark datasets confirm the superior performance of our method on models such as ResNets and Transformers. On average, our method achieves a $2.8$ \\% improvement in worst-group accuracy compared with the state-of-the-art method, while enjoying up to $10\\times$ faster training efficiency",
    "keywords": [],
    "checked": true,
    "id": "ea68c705715b610b5f4750217a934f8d1666d30d",
    "semantic_title": "robust learning with progressive data expansion against spurious correlation",
    "citation_count": 2,
    "authors": [
      "Yihe Deng",
      "Yu Yang",
      "Baharan Mirzasoleiman",
      "Quanquan Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/050f8591be3874b52fdac4e1060eeb29-Abstract-Conference.html": {
    "title": "Multiclass Boosting: Simple and Intuitive Weak Learning Criteria",
    "volume": "main",
    "abstract": "We study a generalization of boosting to the multiclass setting.We introduce a weak learning condition for multiclass classification that captures the original notion of weak learnability as being \"slightly better than random guessing\". We give a simple and efficient boosting algorithm, that does not require realizability assumptions and its sample and oracle complexity bounds are independent of the number of classes. In addition, we utilize our new boosting technique in several theoretical applications within the context of List PAC Learning. First, we establish an equivalence to weak PAC learning. Furthermore, we present a new result on boosting for list learners, as well as provide a novel proof for the characterization of multiclass PAC learning and List PAC learning. Notably, our technique gives rise to simplified algorithms and analysis compared to previous works",
    "keywords": [],
    "checked": true,
    "id": "9dabf2ab96e242975b5a6c76b870071c39df12fa",
    "semantic_title": "multiclass boosting: simple and intuitive weak learning criteria",
    "citation_count": 0,
    "authors": [
      "Nataly Brukhim",
      "Amit Daniely",
      "Yishay Mansour",
      "Shay Moran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0525a72df7fb2cd943c780d059b94774-Abstract-Conference.html": {
    "title": "Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "A recent line of empirical studies has demonstrated that SGD might exhibit a heavy-tailed behavior in practical settings, and the heaviness of the tails might correlate with the overall performance. In this paper, we investigate the emergence of such heavy tails. Previous works on this problem only considered, up to our knowledge, online (also called single-pass) SGD, in which the emergence of heavy tails in theoretical findings is contingent upon access to an infinite amount of data. Hence, the underlying mechanism generating the reported heavy-tailed behavior in practical settings, where the amount of training data is finite, is still not well-understood. Our contribution aims to fill this gap. In particular, we show that the stationary distribution of offline (also called multi-pass) SGD exhibits ‘approximate' power-law tails and the approximation error is controlled by how fast the empirical distribution of the training data converges to the true underlying data distribution in the Wasserstein metric. Our main takeaway is that, as the number of data points increases, offline SGD will behave increasingly ‘power-law-like'. To achieve this result, we first prove nonasymptotic Wasserstein convergence bounds for offline SGD to online SGD as the number of data points increases, which can be interesting on their own. Finally, we illustrate our theory on various experiments conducted on synthetic data and neural networks",
    "keywords": [],
    "checked": true,
    "id": "3de7b17b3fca785cfcba234473c82e99d5408675",
    "semantic_title": "approximate heavy tails in offline (multi-pass) stochastic gradient descent",
    "citation_count": 1,
    "authors": [
      "Kruno Lehman",
      "Alain Durmus",
      "Umut Simsekli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0534abc9e6db91683d82186ef0d68202-Abstract-Conference.html": {
    "title": "FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow",
    "volume": "main",
    "abstract": "Reconstruction of 3D neural fields from posed images has emerged as a promising method for self-supervised representation learning. The key challenge preventing the deployment of these 3D scene learners on large-scale video data is their dependence on precise camera poses from structure-from-motion, which is prohibitively expensive to run at scale. We propose a method that jointly reconstructs camera poses and 3D neural scene representations online and in a single forward pass. We estimate poses by first lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, preserving locality and shift-equivariance of the image processing backbone. SE(3) camera pose estimation is then performed via a weighted least-squares fit to the scene flow field. This formulation enables us to jointly supervise pose estimation and a generalizable neural scene representation via re-rendering the input video, and thus, train end-to-end and fully self-supervised on real-world video datasets. We demonstrate that our method performs robustly on diverse, real-world video, notably on sequences traditionally challenging to optimization-based pose estimation techniques",
    "keywords": [],
    "checked": true,
    "id": "fe0c14242aa048354b1e08de068f64200531caf1",
    "semantic_title": "flowcam: training generalizable 3d radiance fields without camera poses via pixel-aligned scene flow",
    "citation_count": 11,
    "authors": [
      "Cameron Smith",
      "Yilun Du",
      "Ayush Tewari",
      "Vincent Sitzmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/054e9f9a286671ababa3213d6e59c1c2-Abstract-Conference.html": {
    "title": "Minimum Description Length and Generalization Guarantees for Representation Learning",
    "volume": "main",
    "abstract": "A major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data. While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees. For example, the information bottleneck method seeks a good generalization by finding a minimal description of the input that is maximally informative about the label variable, where minimality and informativeness are both measured by Shannon's mutual information. In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the \"Minimum Description Length'' (MDL) of the labels or the latent variables (representations). Rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the \"multi-letter\" relative entropy between the distribution of the representations (or labels) of the training and test sets and a fixed prior. In particular, these new bounds reflect the structure of the encoder and are not vacuous for deterministic algorithms. Our compressibility approach, which is information-theoretic in nature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two essential ingredients: block-coding and lossy-compression. The latter allows our approach to subsume the so-called geometrical compressibility as a special case. To the best knowledge of the authors, the established generalization bounds are the first of their kind for Information Bottleneck type encoders and representation learning. Finally, we partly exploit the theoretical results by introducing a new data-dependent prior. Numerical simulations illustrate the advantages of well-chosen such priors over classical priors used in IB",
    "keywords": [],
    "checked": false,
    "id": "41bba26e8b607d1c0be9fb494c545c233d51a887",
    "semantic_title": "minimum description length control",
    "citation_count": 1,
    "authors": [
      "Milad Sefidgaran",
      "Abdellatif Zaidi",
      "Piotr Krasnowski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/054f771d614df12fe8def8ecdbe4e8e1-Abstract-Conference.html": {
    "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
    "volume": "main",
    "abstract": "Deep generative models can generate high-fidelity audio conditioned on varioustypes of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients(MFCC)). Recently, such models have been used to synthesize audiowaveforms conditioned on highly compressed representations. Although suchmethods produce impressive results, they are prone to generate audible artifactswhen the conditioning is flawed or imperfect. An alternative modeling approach isto use diffusion models. However, these have mainly been used as speech vocoders(i.e., conditioned on mel-spectrograms) or generating relatively low samplingrate signals. In this work, we propose a high-fidelity multi-band diffusion-basedframework that generates any type of audio modality (e.g., speech, music, environmentalsounds) from low-bitrate discrete representations. At equal bit rate,the proposed approach outperforms state-of-the-art generative techniques in termsof perceptual quality. Training and evaluation code are available on the facebookresearch/audiocraft github project. Samples are available on the followinglink (https://ai.honu.io/papers/mbd/)",
    "keywords": [],
    "checked": true,
    "id": "bca9507a2f6bd3379db34ab2ef4295dbdf503a67",
    "semantic_title": "from discrete tokens to high-fidelity audio using multi-band diffusion",
    "citation_count": 1,
    "authors": [
      "Robin San Roman",
      "Yossi Adi",
      "Antoine Deleforge",
      "Romain Serizel",
      "Gabriel Synnaeve",
      "Alexandre Defossez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/055fc19a3ce780b96cff15ffe738c1f1-Abstract-Conference.html": {
    "title": "Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs",
    "volume": "main",
    "abstract": "Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the training data. A consequence of this lack of dependence on the targets is that the NTK cannot perform better than the optimal MKL kernel on the training set. By using iterative reweighting, we improve the weights induced by the NTK to obtain the optimal MKL kernel which is equivalent to the solution of the exact convex reformulation of the gated ReLU network. We also provide several numerical simulations corroborating our theory. Additionally, we provide an analysis of the prediction error of the resulting optimal kernel via consistency results for the group lasso",
    "keywords": [],
    "checked": true,
    "id": "646090314325c23059daa217c0be9f1efb8b14c0",
    "semantic_title": "fixing the ntk: from neural network linearizations to exact convex programs",
    "citation_count": 0,
    "authors": [
      "Rajat Vadiraj Dwaraknath",
      "Tolga Ergen",
      "Mert Pilanci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0561738a239a995c8cd2ef0e50cfa4fd-Abstract-Conference.html": {
    "title": "Birth of a Transformer: A Memory Viewpoint",
    "volume": "main",
    "abstract": "Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an \"induction head\" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties",
    "keywords": [],
    "checked": true,
    "id": "11ae58636a5daf0ea1297f1c4ee94042fcebefa8",
    "semantic_title": "birth of a transformer: a memory viewpoint",
    "citation_count": 16,
    "authors": [
      "Alberto Bietti",
      "Vivien Cabannes",
      "Diane Bouchacourt",
      "Herve Jegou",
      "Leon Bottou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0569458210c88d8db2985799da830d27-Abstract-Conference.html": {
    "title": "A Variational Perspective on High-Resolution ODEs",
    "volume": "main",
    "abstract": "We consider unconstrained minimization of smooth convex functions. We propose a novel variational perspective using forced Euler-Lagrange equation that allows for studying high-resolution ODEs. Through this, we obtain a faster convergence rate for gradient norm minimization using Nesterov's accelerated gradient method. Additionally, we show that Nesterov's method can be interpreted as a rate-matching discretization of an appropriately chosen high-resolution ODE. Finally, using the results from the new variational perspective, we propose a stochastic method for noisy gradients. Several numerical experiments compare and illustrate our stochastic algorithm with state of the art methods",
    "keywords": [],
    "checked": true,
    "id": "990ab230ef148700681b6462d1ff2cebec848364",
    "semantic_title": "a variational perspective on high-resolution odes",
    "citation_count": 0,
    "authors": [
      "Hoomaan Maskan",
      "Konstantinos Zygalakis",
      "Alp Yurtsever"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/056e8e9c8ca9929cb6cf198952bf1dbb-Abstract-Conference.html": {
    "title": "What You See is What You Read? Improving Text-Image Alignment Evaluation",
    "volume": "main",
    "abstract": "Automatically determining whether a text and a corresponding image are semantically aligned is a significant challenge for vision-language models, with applications in generative text-to-image and image-to-text tasks. In this work, we study methods for automatic text-image alignment evaluation. We first introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets from both text-to-image and image-to-text generation tasks, with human judgements for whether a given text-image pair is semantically aligned. We then describe two automatic methods to determine alignment: the first involving a pipeline based on question generation and visual question answering models, and the second employing an end-to-end classification approach by finetuning multimodal pretrained models. Both methods surpass prior approaches in various text-image alignment tasks, with significant improvements in challenging cases that involve complex composition or unnatural images. Finally, we demonstrate how our approaches can localize specific misalignments between an image and a given text, and how they can be used to automatically re-rank candidates in text-to-image generation",
    "keywords": [],
    "checked": true,
    "id": "b1ef91c5541f88297c7551e1adf15fcee7987197",
    "semantic_title": "what you see is what you read? improving text-image alignment evaluation",
    "citation_count": 17,
    "authors": [
      "Michal Yarom",
      "Yonatan Bitton",
      "Soravit Changpinyo",
      "Roee Aharoni",
      "Jonathan Herzig",
      "Oran Lang",
      "Eran Ofek",
      "Idan Szpektor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/058983528186511a74968e88a6d0ad63-Abstract-Conference.html": {
    "title": "On the Robustness of Mechanism Design under Total Variation Distance",
    "volume": "main",
    "abstract": "We study the problem of designing mechanisms when agents' valuation functions are drawn from unknown and correlated prior distributions. In particular, we are given a prior distribution $D$, and we are interested in designing a (truthful) mechanism that has good performance for all \"true distributions\" that are close to $D$ in Total Variation (TV) distance. We show that DSIC and BIC mechanisms in this setting are strongly robust with respect to TV distance, for any bounded objective function $\\mathcal{O}$, extending a recent result of Brustle et al. ([BCD20], EC 2020). At the heart of our result is a fundamental duality property of total variation distance. As direct applications of our result, we (i) demonstrate how to find approximately revenue-optimal and approximately BIC mechanisms for weakly dependent prior distributions; (ii) show how to find correlation-robust mechanisms when only ``noisy'' versions of marginals are accessible, extending recent results of Bei et. al. ([BGLT19], SODA 2019); (iii) prove that prophet-inequality type guarantees are preserved for correlated priors, recovering a variant of a result of D{\\\"u}tting and Kesselheim ([DK19], EC 2019) as a special case; (iv) give a new necessary condition for a correlated distribution to witness an infinite separation in revenue between simple and optimal mechanisms, complementing recent results of Psomas et al. ([PSCW22], NeurIPS 2022); (v) give a new condition for simple mechanisms to approximate revenue-optimal mechanisms for the case of a single agent whose type is drawn from a correlated distribution that can be captured by a Markov Random Field, complementing recent results of Cai and Oikonomou ([CO21], EC 2021)",
    "keywords": [],
    "checked": true,
    "id": "547535539b74ba97e8de7a1e1d665f72a8c3a88d",
    "semantic_title": "on the robustness of mechanism design under total variation distance",
    "citation_count": 0,
    "authors": [
      "Anuran Makur",
      "Marios Mertzanidis",
      "Alexandros Psomas",
      "Athina Terzoglou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05ab457c7b769f01c2973e2a5ab66ad9-Abstract-Conference.html": {
    "title": "A generative model of the hippocampal formation trained with theta driven local learning rules",
    "volume": "main",
    "abstract": "Advances in generative models have recently revolutionised machine learning. Meanwhile, in neuroscience, generative models have long been thought fundamental to animal intelligence. Understanding the biological mechanisms that support these processes promises to shed light on the relationship between biological and artificial intelligence. In animals, the hippocampal formation is thought to learn and use a generative model to support its role in spatial and non-spatial memory. Here we introduce a biologically plausible model of the hippocampal formation tantamount to a Helmholtz machine that we apply to a temporal stream of inputs. A novel component of our model is that fast theta-band oscillations (5-10 Hz) gate the direction of information flow throughout the network, training it akin to a high-frequency wake-sleep algorithm. Our model accurately infers the latent state of high-dimensional sensory environments and generates realistic sensory predictions. Furthermore, it can learn to path integrate by developing a ring attractor connectivity structure matching previous theoretical proposals and flexibly transfer this structure between environments. Whereas many models trade-off biological plausibility with generality, our model captures a variety of hippocampal cognitive functions under one biologically plausible local learning rule",
    "keywords": [],
    "checked": true,
    "id": "cd98854d3482e276b7124712c02e20ed48f56a0f",
    "semantic_title": "a generative model of the hippocampal formation trained with theta driven local learning rules",
    "citation_count": 2,
    "authors": [
      "Tom M George",
      "Kimberly L. Stachenfeld",
      "Caswell Barry",
      "Claudia Clopath",
      "Tomoki Fukai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05b63fa06784b71aab3939004e0f0a0d-Abstract-Conference.html": {
    "title": "Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test environments",
    "keywords": [],
    "checked": true,
    "id": "67ce63f8399c3b63da8f8746f312a3f57232edc4",
    "semantic_title": "risk-averse model uncertainty for distributionally robust safe reinforcement learning",
    "citation_count": 1,
    "authors": [
      "James Queeney",
      "Mouhacine Benosman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05b69cc4c8ff6e24c5de1ecd27223d37-Abstract-Conference.html": {
    "title": "Optimal approximation using complex-valued neural networks",
    "volume": "main",
    "abstract": "Complex-valued neural networks (CVNNs) have recently shown promising empirical success, for instance for increasing the stability of recurrent neural networks and for improving the performance in tasks with complex-valued inputs, such as MRI fingerprinting. While the overwhelming success of Deep Learning in the real-valued case is supported by a growing mathematical foundation, such a foundation is still largely lacking in the complex-valued case. We thus analyze the expressivity of CVNNs by studying their approximation properties. Our results yield the first quantitative approximation bounds for CVNNs that apply to a wide class of activation functions including the popular modReLU and complex cardioid activation functions. Precisely, our results apply to any activation function that is smooth but not polyharmonic on some non-empty open set; this is the natural generalization of the class of smooth and non-polynomial activation functions to the complex setting. Our main result shows that the approximation error scales as $m^{-k/(2n)}$ for $m \\to \\infty$ where $m$ is the number of neurons, $k$ the smoothness of the target function and $n$ is the (complex) input dimension. Under a natural continuity assumption, we show that this rate is optimal; we further discuss the optimality when dropping this assumption. Moreover, we prove that the problem of approximating $C^k$-functions using continuous approximation methods unavoidably suffers from the curse of dimensionality",
    "keywords": [],
    "checked": true,
    "id": "cb4e0788b9e95c445fe580d8b17931ea8c35ae4b",
    "semantic_title": "optimal approximation using complex-valued neural networks",
    "citation_count": 1,
    "authors": [
      "Paul Geuchen",
      "Felix Voigtlaender"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05cf28e3d3c9a179d789c55270fe6f72-Abstract-Conference.html": {
    "title": "BayesDAG: Gradient-Based Posterior Inference for Causal Discovery",
    "volume": "main",
    "abstract": "Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on a combination of stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and Variational Inference (VI) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and is applicable to both linear and nonlinear causal models. To enable our approach, we derive a novel equivalence to the permutation-based DAG learning, which opens up possibilities of using any relaxed gradient estimator defined over permutations. To our knowledge, this is the first framework applying gradient-based MCMC sampling for causal discovery. Empirical evaluation on synthetic and real-world datasets demonstrate our approach's effectiveness compared to state-of-the-art baselines",
    "keywords": [],
    "checked": true,
    "id": "16a44d0782699b43057151225773f3ebe2cd5918",
    "semantic_title": "bayesdag: gradient-based posterior inference for causal discovery",
    "citation_count": 1,
    "authors": [
      "Yashas Annadani",
      "Nick Pawlowski",
      "Joel Jennings",
      "Stefan Bauer",
      "Cheng Zhang",
      "Wenbo Gong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05d2175de7ee637588d1b5ced8b15b32-Abstract-Conference.html": {
    "title": "Bounce: Reliable High-Dimensional Bayesian Optimization for Combinatorial and Mixed Spaces",
    "volume": "main",
    "abstract": "Impactful applications such as materials discovery, hardware design, neural architecture search, or portfolio optimization require optimizing high-dimensional black-box functions with mixed and combinatorial input spaces.While Bayesian optimization has recently made significant progress in solving such problems, an in-depth analysis reveals that the current state-of-the-art methods are not reliable. Their performances degrade substantially when the unknown optima of the function do not have a certain structure. To fill the need for a reliable algorithm for combinatorial and mixed spaces, this paper proposes Bounce that relies on a novel map of various variable types into nested embeddings of increasing dimensionality.Comprehensive experiments show that Bounce reliably achieves and often even improves upon state-of-the-art performance on a variety of high-dimensional problems",
    "keywords": [],
    "checked": false,
    "id": "b886db66e0b1a138923434657b82f0fd8a17275d",
    "semantic_title": "bounce: a reliable bayesian optimization algorithm for combinatorial and mixed spaces",
    "citation_count": 2,
    "authors": [
      "Leonard Papenmeier",
      "Luigi Nardi",
      "Matthias Poloczek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05d6b5b6901fb57d2c287e1d3ce6d63c-Abstract-Conference.html": {
    "title": "Uniform-in-Time Wasserstein Stability Bounds for (Noisy) Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "Algorithmic stability is an important notion that has proven powerful for deriving generalization bounds for practical algorithms. The last decade has witnessed an increasing number of stability bounds for different algorithms applied on different classes of loss functions. While these bounds have illuminated various properties of optimization algorithms, the analysis of each case typically required a different proof technique with significantly different mathematical tools. In this study, we make a novel connection between learning theory and applied probability and introduce a unified guideline for proving Wasserstein stability bounds for stochastic optimization algorithms. We illustrate our approach on stochastic gradient descent (SGD) and we obtain time-uniform stability bounds (i.e., the bound does not increase with the number of iterations) for strongly convex losses and non-convex losses with additive noise, where we recover similar results to the prior art or extend them to more general cases by using a single proof technique. Our approach is flexible and can be generalizable to other popular optimizers, as it mainly requires developing Lyapunov functions, which are often readily available in the literature. It also illustrates that ergodicity is an important component for obtaining time-uniform bounds -- which might not be achieved for convex or non-convex losses unless additional noise is injected to the iterates. Finally, we slightly stretch our analysis technique and prove time-uniform bounds for SGD under convex and non-convex losses (without additional additive noise), which, to our knowledge, is novel",
    "keywords": [],
    "checked": true,
    "id": "a05ed14804f09bdc2df7f005978a0543fb5174f4",
    "semantic_title": "uniform-in-time wasserstein stability bounds for (noisy) stochastic gradient descent",
    "citation_count": 2,
    "authors": [
      "Lingjiong  Zhu",
      "Mert Gurbuzbalaban",
      "Anant Raj",
      "Umut Simsekli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05dc08730e32441edff52b0fa6caab5f-Abstract-Conference.html": {
    "title": "Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation",
    "volume": "main",
    "abstract": "Volume-wise labeling in 3D medical images is a time-consuming task that requires expertise. As a result, there is growing interest in using semi-supervised learning (SSL) techniques to train models with limited labeled data. However, the challenges and practical applications extend beyond SSL to settings such as unsupervised domain adaptation (UDA) and semi-supervised domain generalization (SemiDG). This work aims to develop a generic SSL framework that can handle all three settings. We identify two main obstacles to achieving this goal in the existing SSL framework: 1) the weakness of capturing distribution-invariant features; and 2) the tendency for unlabeled data to be overwhelmed by labeled data, leading to over-fitting to the labeled data during training. To address these issues, we propose an Aggregating & Decoupling framework. The aggregating part consists of a Diffusion encoder that constructs a \"common knowledge set\" by extracting distribution-invariant features from aggregated information from multiple distributions/domains. The decoupling part consists of three decoders that decouple the training process with labeled and unlabeled data, thus avoiding over-fitting to labeled data, specific domains and classes. We evaluate our proposed framework on four benchmark datasets for SSL, Class-imbalanced SSL, UDA and SemiDG. The results showcase notable improvements compared to state-of-the-art methods across all four settings, indicating the potential of our framework to tackle more challenging SSL scenarios. Code and models are available at: https://github.com/xmed-lab/GenericSSL",
    "keywords": [],
    "checked": true,
    "id": "9a768e60bd56c3d8ac2c4a76aac58561d056a786",
    "semantic_title": "towards generic semi-supervised framework for volumetric medical image segmentation",
    "citation_count": 2,
    "authors": [
      "Haonan Wang",
      "Xiaomeng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05e552739c2629f3324c1063a382b4bd-Abstract-Conference.html": {
    "title": "Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis",
    "volume": "main",
    "abstract": "We study finite-sum distributed optimization problems involving a master node and $n-1$ local nodes under the popular $\\delta$-similarity and $\\mu$-strong convexity conditions. We propose two new algorithms, SVRS and AccSVRS, motivated by previous works. The non-accelerated SVRS method combines the techniques of gradient sliding and variance reduction and achieves a better communication complexity of $\\tilde{\\mathcal{O}}(n {+} \\sqrt{n}\\delta/\\mu)$ compared to existing non-accelerated algorithms. Applying the framework proposed in Katyusha X, we also develop a directly accelerated version named AccSVRS with the $\\tilde{\\mathcal{O}}(n {+} n^{3/4}\\sqrt{\\delta/\\mu})$ communication complexity. In contrast to existing results, our complexity bounds are entirely smoothness-free and exhibit superiority in ill-conditioned cases. Furthermore, we establish a nearly matched lower bound to verify the tightness of our AccSVRS method",
    "keywords": [],
    "checked": true,
    "id": "4e665b172ba607e87856e3561bfee448b4713244",
    "semantic_title": "stochastic distributed optimization under average second-order similarity: algorithms and analysis",
    "citation_count": 0,
    "authors": [
      "Dachao Lin",
      "Yuze Han",
      "Haishan Ye",
      "Zhihua Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05f0e2fa003602db2d98ca72b79dec51-Abstract-Conference.html": {
    "title": "PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Models",
    "volume": "main",
    "abstract": "This paper presents \\textit{PolyDiffuse}, a novel structured reconstruction algorithm that transforms visual sensor data into polygonal shapes with Diffusion Models (DM), an emerging machinery amid exploding generative AI, while formulating reconstruction as a generation process conditioned on sensor data. The task of structured reconstruction poses two fundamental challenges to DM: 1) A structured geometry is a ''set'' (e.g., a set of polygons for a floorplan geometry), where a sample of $N$ elements has $N!$ different but equivalent representations, making the denoising highly ambiguous; and 2) A ''reconstruction'' task has a single solution, where an initial noise needs to be chosen carefully, while any initial noise works for a generation task.Our technical contribution is the introduction of a Guided Set Diffusion Model where 1) the forward diffusion process learns \\textit{guidance networks} to control noise injection so that one representation of a sample remains distinct from its other permutation variants, thus resolving denoising ambiguity; and 2) the reverse denoising process reconstructs polygonal shapes, initialized and directed by the guidance networks, as a conditional generation process subject to the sensor data.We have evaluated our approach for reconstructing two types of polygonal shapes: floorplan as a set of polygons and HD map for autonomous cars as a set of polylines.Through extensive experiments on standard benchmarks, we demonstrate that PolyDiffuse significantly advances the current state of the art and enables broader practical applications",
    "keywords": [],
    "checked": true,
    "id": "b14bdd699cce2a7a7d894c3a020f0c1b1c798758",
    "semantic_title": "polydiffuse: polygonal shape reconstruction via guided set diffusion models",
    "citation_count": 5,
    "authors": [
      "Jiacheng Chen",
      "Ruizhi Deng",
      "Yasutaka Furukawa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05fb0f4e645cad23e0ab59d6b9901428-Abstract-Conference.html": {
    "title": "Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data",
    "volume": "main",
    "abstract": "Evaluating the performance of machine learning models on diverse and underrepresented subgroups is essential for ensuring fairness and reliability in real-world applications. However, accurately assessing model performance becomes challenging due to two main issues: (1) a scarcity of test data, especially for small subgroups, and (2) possible distributional shifts in the model's deployment setting, which may not align with the available test data. In this work, we introduce 3S Testing, a deep generative modeling framework to facilitate model evaluation by generating synthetic test sets for small subgroups and simulating distributional shifts. Our experiments demonstrate that 3S-Testing outperforms traditional baselines---including real test data alone---in estimating model performance on minority subgroups and under plausible distributional shifts. In addition, 3S offers intervals around its performance estimates, exhibiting superior coverage of the ground truth compared to existing approaches. Overall, these results raise the question of whether we need a paradigm shift away from limited real test data towards synthetic test data",
    "keywords": [],
    "checked": true,
    "id": "56274dc93093a4cebf327f68285234537e9699e0",
    "semantic_title": "can you rely on your model evaluation? improving model evaluation with synthetic test data",
    "citation_count": 4,
    "authors": [
      "Boris van Breugel",
      "Nabeel Seedat",
      "Fergus Imrie",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05fe0c633ae41756540dba2a99a36306-Abstract-Conference.html": {
    "title": "Rethinking the Backward Propagation for Adversarial Transferability",
    "volume": "main",
    "abstract": "Transfer-based attacks generate adversarial examples on the surrogate model, which can mislead other black-box models without access, making it promising to attack real-world applications. Recently, several works have been proposed to boost adversarial transferability, in which the surrogate model is usually overlooked. In this work, we identify that non-linear layers (e.g., ReLU, max-pooling, etc.) truncate the gradient during backward propagation, making the gradient w.r.t. input image imprecise to the loss function. We hypothesize and empirically validate that such truncation undermines the transferability of adversarial examples. Based on these findings, we propose a novel method called Backward Propagation Attack (BPA) to increase the relevance between the gradient w.r.t. input image and loss function so as to generate adversarial examples with higher transferability. Specifically, BPA adopts a non-monotonic function as the derivative of ReLU and incorporates softmax with temperature to smooth the derivative of max-pooling, thereby mitigating the information loss during the backward propagation of gradients. Empirical results on the ImageNet dataset demonstrate that not only does our method substantially boost the adversarial transferability, but it is also general to existing transfer-based attacks. Code is available at https://github.com/Trustworthy-AI-Group/RPA",
    "keywords": [],
    "checked": true,
    "id": "89347e7b42e00e949fd64060136cc64497ab2ef3",
    "semantic_title": "rethinking the backward propagation for adversarial transferability",
    "citation_count": 8,
    "authors": [
      "Wang Xiaosen",
      "Kangheng Tong",
      "Kun He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/060b2af0081a460f7f466f7f174d9052-Abstract-Conference.html": {
    "title": "Compression with Bayesian Implicit Neural Representations",
    "volume": "main",
    "abstract": "Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a progressive refinement process for the variational posterior that significantly enhances performance. Experiments show that our method achieves strong performance on image and audio compression while retaining simplicity",
    "keywords": [],
    "checked": true,
    "id": "e772d88c3f20bf965872d108bd0bf262f26bf5f7",
    "semantic_title": "compression with bayesian implicit neural representations",
    "citation_count": 3,
    "authors": [
      "Zongyu Guo",
      "Gergely Flamich",
      "Jiajun He",
      "Zhibo Chen",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/062d711fb777322e2152435459e6e9d9-Abstract-Conference.html": {
    "title": "Towards Unbounded Machine Unlearning",
    "volume": "main",
    "abstract": "Deep machine unlearning is the problem of 'removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their 'right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for 'forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure model utility (i.e. accuracy on retained data and generalization), and is more efficient than previous work. The above are substantiated through a comprehensive empirical evaluation against previous state-of-the-art",
    "keywords": [],
    "checked": true,
    "id": "570a341a8fd511cf0e05687110f053aaac646010",
    "semantic_title": "towards unbounded machine unlearning",
    "citation_count": 17,
    "authors": [
      "Meghdad Kurmanji",
      "Peter Triantafillou",
      "Jamie Hayes",
      "Eleni Triantafillou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/065e259a1d2d955e63b99aac6a3a3081-Abstract-Conference.html": {
    "title": "Collaborative Learning via Prediction Consensus",
    "volume": "main",
    "abstract": "We consider a collaborative learning setting where the goal of each agent is to improve their own model by leveraging the expertise of collaborators, in addition to their own training data. To facilitate the exchange of expertise among agents, we propose a distillation-based method leveraging shared unlabeled auxiliary data, which is pseudo-labeled by the collective. Central to our method is a trust weighting scheme that serves to adaptively weigh the influence of each collaborator on the pseudo-labels until a consensus on how to label the auxiliary data is reached. We demonstrate empirically that our collaboration scheme is able to significantly boost individual models' performance in the target domain from which the auxiliary data is sampled. At the same time, it can provably mitigate the negative impact of bad models on the collective. By design, our method adeptly accommodates heterogeneity in model architectures and substantially reduces communication overhead compared to typical collaborative learning methods",
    "keywords": [],
    "checked": true,
    "id": "bc32f76b6c3ccb8b26e1ca6bdd0a487ac01ea676",
    "semantic_title": "collaborative learning via prediction consensus",
    "citation_count": 0,
    "authors": [
      "Dongyang Fan",
      "Celestine Mendler-Dünner",
      "Martin Jaggi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/065ef23a944b3995de7dd4a3e203d133-Abstract-Conference.html": {
    "title": "Identification of Nonlinear Latent Hierarchical Models",
    "volume": "main",
    "abstract": "Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of causal structures and latent variables (up to invertible transformations) can be achieved under mild assumptions: on causal structures, we allow for multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we permit general nonlinearity and multi-dimensional continuous variables, alleviating existing work's parametric assumptions. Specifically, we first develop an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model. Leveraging this criterion, we show that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure. To the best of our knowledge, our work is the first to establish identifiability guarantees for both causal structures and latent variables in nonlinear latent hierarchical models",
    "keywords": [],
    "checked": true,
    "id": "075b751201f549daeba9840f78768f4ceb507e17",
    "semantic_title": "identification of nonlinear latent hierarchical models",
    "citation_count": 5,
    "authors": [
      "Lingjing Kong",
      "Biwei Huang",
      "Feng Xie",
      "Eric Xing",
      "Yuejie Chi",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0663a39baab211328fc865f91abc75ab-Abstract-Conference.html": {
    "title": "Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks",
    "volume": "main",
    "abstract": "This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic {\\em given} the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including Manufacturing systems, communication networks, and queueing networks. We propose a sample-efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven (model-free), but it learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the optimality gap decreases as $O\\left(\\sqrt{\\frac{1}{n}}+\\sqrt{\\frac{1}{m}}\\right),$ where $n$ represents the number of real samples, and $m$ is the number of augmented samples per real sample. It is important to note that without augmented samples, the optimality gap is $O(1)$ due to the insufficient data coverage of the pseudo-stochastic states. Our experimental results on multiple queueing network applications confirm that the proposed method indeed significantly accelerates both deep Q-learning and deep policy gradient",
    "keywords": [],
    "checked": true,
    "id": "cc563dcbacdb5474de52535b1de384aebb30b597",
    "semantic_title": "sample efficient reinforcement learning in mixed systems through augmented samples and its applications to queueing networks",
    "citation_count": 3,
    "authors": [
      "Honghao Wei",
      "Xin Liu",
      "Weina Wang",
      "Lei Ying"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/066e4dbfeccb5dc2851acd5eca584937-Abstract-Conference.html": {
    "title": "Navigating Data Heterogeneity in Federated Learning: A Semi-Supervised Approach for Object Detection",
    "volume": "main",
    "abstract": "Federated Learning (FL) has emerged as a potent framework for training models across distributed data sources while maintaining data privacy. Nevertheless, it faces challenges with limited high-quality labels and non-IID client data, particularly in applications like autonomous driving. To address these hurdles, we navigate the uncharted waters of Semi-Supervised Federated Object Detection (SSFOD). We present a pioneering SSFOD framework, designed for scenarios where labeled data reside only at the server while clients possess unlabeled data. Notably, our method represents the inaugural implementation of SSFOD for clients with 0% labeled non-IID data, a stark contrast to previous studies that maintain some subset of labels at each client. We propose FedSTO, a two-stage strategy encompassing Selective Training followed by Orthogonally enhanced full-parameter training, to effectively address data shift (e.g. weather conditions) between server and clients. Our contributions include selectively refining the backbone of the detector to avert overfitting, orthogonality regularization to boost representation divergence, and local EMA-driven pseudo label assignment to yield high-quality pseudo labels. Extensive validation on prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M) attests to the efficacy of our approach, demonstrating state-of-the-art results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as well as fully-supervised centralized training methods",
    "keywords": [],
    "checked": false,
    "id": "490b98e44d2e69f1aaabe4dba2c39d170c348b75",
    "semantic_title": "navigating data heterogeneity in federated learning a semi-supervised approach for object detection",
    "citation_count": 1,
    "authors": [
      "Taehyeon Kim",
      "Eric Lin",
      "Junu Lee",
      "Christian Lau",
      "Vaikkunth Mugunthan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/06abed94583030dd50abe6767bd643b1-Abstract-Conference.html": {
    "title": "On the Generalization Properties of Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish the theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., independent of the data dimension) when *early-stopped*. Furthermore, we extend our quantitative analysis to a *data-dependent* scenario, wherein target distributions are portrayed as a succession of densities with progressively increasing distances between modes. This precisely elucidates the *adverse* effect of \"*modes shift*'' in ground truths on the model generalization. Furthermore, these estimates are not solely theoretical constructs but have also been confirmed through numerical simulations. Our findings contribute to the rigorous understanding of diffusion models' generalization properties and provide insights that may guide practical applications",
    "keywords": [],
    "checked": true,
    "id": "a3eff696eef2d9cf155a4433f8a4e77c78229373",
    "semantic_title": "on the generalization properties of diffusion models",
    "citation_count": 5,
    "authors": [
      "Puheng Li",
      "Zhong Li",
      "Huishuai Zhang",
      "Jiang Bian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/06b71ad997f7e3e4b2e2f2ea12e5a759-Abstract-Conference.html": {
    "title": "Regularized Behavior Cloning for Blocking the Leakage of Past Action Information",
    "volume": "main",
    "abstract": "For partially observable environments, imitation learning with observation histories (ILOH) assumes that control-relevant information is sufficiently captured in the observation histories for imitating the expert actions. In the offline setting wherethe agent is required to learn to imitate without interaction with the environment, behavior cloning (BC) has been shown to be a simple yet effective method for imitation learning. However, when the information about the actions executed in the past timesteps leaks into the observation histories, ILOH via BC often ends up imitating its own past actions. In this paper, we address this catastrophic failure by proposing a principled regularization for BC, which we name Past Action Leakage Regularization (PALR). The main idea behind our approach is to leverage the classical notion of conditional independence to mitigate the leakage. We compare different instances of our framework with natural choices of conditional independence metric and its estimator. The result of our comparison advocates the use of a particular kernel-based estimator for the conditional independence metric. We conduct an extensive set of experiments on benchmark datasets in order to assess the effectiveness of our regularization method. The experimental results show that our method significantly outperforms prior related approaches, highlighting its potential to successfully imitate expert actions when the past action information leaks into the observation histories",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokin Seo",
      "HyeongJoo Hwang",
      "Hongseok Yang",
      "Kee-Eung Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/06cb881ec90a657a8f949a62f1b4ee5f-Abstract-Conference.html": {
    "title": "The Distortion of Binomial Voting Defies Expectation",
    "volume": "main",
    "abstract": "In computational social choice, the distortion of a voting rule quantifies the degree to which the rule overcomes limited preference information to select a socially desirable outcome. This concept has been investigated extensively, but only through a worst-case lens. Instead, we study the expected distortion of voting rules with respect to an underlying distribution over voter utilities. Our main contribution is the design and analysis of a novel and intuitive rule, binomial voting, which provides strong distribution-independent guarantees for both expected distortion and expected welfare",
    "keywords": [],
    "checked": true,
    "id": "436681b801da68922569138f7243d1f8cf663716",
    "semantic_title": "the distortion of binomial voting defies expectation",
    "citation_count": 1,
    "authors": [
      "Yannai A. Gonczarowski",
      "Gregory Kehne",
      "Ariel D. Procaccia",
      "Ben Schiffer",
      "Shirley Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/06d5f1fe6509b001e6d4e0ec1afd83dd-Abstract-Conference.html": {
    "title": "UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models",
    "volume": "main",
    "abstract": "In this study, we investigate the task of data pre-selection, which aims to select instances for labeling from an unlabeled dataset through a single pass, thereby optimizing performance for undefined downstream tasks with a limited annotation budget. Previous approaches to data pre-selection relied solely on visual features extracted from foundation models, such as CLIP and BLIP-2, but largely ignored the powerfulness of text features. In this work, we argue that, with proper design, the joint feature space of both vision and text can yield a better representation for data pre-selection. To this end, we introduce UP-DP, a simple yet effective unsupervised prompt learning approach that adapts vision-language models, like BLIP-2, for data pre-selection. Specifically, with the BLIP-2 parameters frozen, we train text prompts to extract the joint features with improved representation, ensuring a diverse cluster structure that covers the entire dataset. We extensively compare our method with the state-of-the-art using seven benchmark datasets in different settings, achieving up to a performance gain of 20\\%. Interestingly, the prompts learned from one dataset demonstrate significant generalizability and can be applied directly to enhance the feature extraction of BLIP-2 from other datasets. To the best of our knowledge, UP-DP is the first work to incorporate unsupervised prompt learning in a vision-language model for data pre-selection",
    "keywords": [],
    "checked": true,
    "id": "a679c736d26dbcab41b483f2dbbc417da62c7a16",
    "semantic_title": "up-dp: unsupervised prompt learning for data pre-selection with vision-language models",
    "citation_count": 0,
    "authors": [
      "Xin Li",
      "Sima Behpour",
      "Thang Long Doan",
      "Wenbin He",
      "Liang Gou",
      "Liu Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/06e3c330d140f3a25671acf2dc2d6357-Abstract-Conference.html": {
    "title": "Optimistic Rates for Multi-Task Representation Learning",
    "volume": "main",
    "abstract": "We study the problem of transfer learning via Multi-Task Representation Learning (MTRL), wherein multiple source tasks are used to learn a good common representation, and a predictor is trained on top of it for the target task. Under standard regularity assumptions on the loss function and task diversity, we provide new statistical rates on the excess risk of the target task, which demonstrate the benefit of representation learning. Importantly, our rates are optimistic, i.e., they interpolate between the standard $O(m^{-1/2})$ rate and the fast $O(m^{-1})$ rate, depending on the difficulty of the learning task, where $m$ is the number of samples for the target task. Besides the main result, we make several new contributions, including giving optimistic rates for excess risk of source tasks (multi-task learning (MTL)), a local Rademacher complexity theorem for MTRL and MTL, as well as a chain rule for local Rademacher complexity for composite predictor classes",
    "keywords": [],
    "checked": false,
    "id": "7aea82e390f6c293c1c1cf02c6593c0e062377f8",
    "semantic_title": "active multi-task representation learning",
    "citation_count": 6,
    "authors": [
      "Austin Watkins",
      "Enayat Ullah",
      "Thanh Nguyen-Tang",
      "Raman Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/06ea400b9b7cfce6428ec27a371632eb-Abstract-Conference.html": {
    "title": "Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution",
    "volume": "main",
    "abstract": "The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining.NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViTmarks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs",
    "keywords": [],
    "checked": true,
    "id": "918617dbc02fa4df1999599bcf967acd2ea84d71",
    "semantic_title": "patch n' pack: navit, a vision transformer for any aspect ratio and resolution",
    "citation_count": 10,
    "authors": [
      "Mostafa Dehghani",
      "Basil Mustafa",
      "Josip Djolonga",
      "Jonathan Heek",
      "Matthias Minderer",
      "Mathilde Caron",
      "Andreas Steiner",
      "Joan Puigcerver",
      "Robert Geirhos",
      "Ibrahim M. Alabdulmohsin",
      "Avital Oliver",
      "Piotr Padlewski",
      "Alexey Gritsenko",
      "Mario Lucic",
      "Neil Houlsby"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/06fc38f5c21ae66ef955e28b7a78ece5-Abstract-Conference.html": {
    "title": "The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning",
    "volume": "main",
    "abstract": "While distributional reinforcement learning (DistRL) has been empirically effective, the question of when and why it is better than vanilla, non-distributional RL has remained unanswered.This paper explains the benefits of DistRL through the lens of small-loss bounds, which are instance-dependent bounds that scale with optimal achievable cost.Particularly, our bounds converge much faster than those from non-distributional approaches if the optimal cost is small.As warmup, we propose a distributional contextual bandit (DistCB) algorithm, which we show enjoys small-loss regret bounds and empirically outperforms the state-of-the-art on three real-world tasks.In online RL, we propose a DistRL algorithm that constructs confidence sets using maximum likelihood estimation. We prove that our algorithm enjoys novel small-loss PAC bounds in low-rank MDPs.As part of our analysis, we introduce the $\\ell_1$ distributional eluder dimension which may be of independent interest. Then, in offline RL, we show that pessimistic DistRL enjoys small-loss PAC bounds that are novel to the offline setting and are more robust to bad single-policy coverage",
    "keywords": [],
    "checked": true,
    "id": "8d7154a1714f4a076ff211b9a4e0b95429e7a9c1",
    "semantic_title": "the benefits of being distributional: small-loss bounds for reinforcement learning",
    "citation_count": 5,
    "authors": [
      "Kaiwen Wang",
      "Kevin Zhou",
      "Runzhe Wu",
      "Nathan Kallus",
      "Wen Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/06fc7ae4a11a7eb5e20fe018db6c036f-Abstract-Conference.html": {
    "title": "Honesty Is the Best Policy: Defining and Mitigating AI Deception",
    "volume": "main",
    "abstract": "Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We focus on the problem that agents might deceive in order to achieve their goals (for instance, in our experiments with language models, the goal of being evaluated as truthful).There are a number of existing definitions of deception in the literature on game theory and symbolic AI, but there is no overarching theory of deception for learning agents in games. We introduce a formaldefinition of deception in structural causal games, grounded in the philosophyliterature, and applicable to real-world machine learning systems.Several examples and results illustrate that our formal definition aligns with the philosophical and commonsense meaning of deception.Our main technical result is to provide graphical criteria for deception. We show, experimentally, that these results can be used to mitigate deception in reinforcement learning agents and language models",
    "keywords": [],
    "checked": true,
    "id": "569d6641ff9b67c2bd399ea0519940a171a48829",
    "semantic_title": "honesty is the best policy: defining and mitigating ai deception",
    "citation_count": 6,
    "authors": [
      "Francis Ward",
      "Francesca Toni",
      "Francesco Belardinelli",
      "Tom Everitt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/070a57c5ef1e58cc90201b11d369b3c2-Abstract-Conference.html": {
    "title": "Improving *day-ahead* Solar Irradiance Time Series Forecasting by Leveraging Spatio-Temporal Context",
    "volume": "main",
    "abstract": "Solar power harbors immense potential in mitigating climate change by substantially reducing CO$_{2}$ emissions. Nonetheless, the inherent variability of solar irradiance poses a significant challenge for seamlessly integrating solar power into the electrical grid. While the majority of prior research has centered on employing purely time series-based methodologies for solar forecasting, only a limited number of studies have taken into account factors such as cloud cover or the surrounding physical context.In this paper, we put forth a deep learning architecture designed to harness spatio-temporal context using satellite data, to attain highly accurate day-ahead time-series forecasting for any given station, with a particular emphasis on forecasting Global Horizontal Irradiance (GHI). We also suggest a methodology to extract a distribution for each time step prediction, which can serve as a very valuable measure of uncertainty attached to the forecast. When evaluating models, we propose a testing scheme in which we separate particularly difficult examples from easy ones, in order to capture the model performances in crucial situations, which in the case of this study are the days suffering from varying cloudy conditions. Furthermore, we present a new multi-modal dataset gathering satellite imagery over a large zone and time series for solar irradiance and other related physical variables from multiple geographically diverse solar stations. Our approach exhibits robust performance in solar irradiance forecasting, including zero-shot generalization tests at unobserved solar stations, and holds great promise in promoting the effective integration of solar power into the grid",
    "keywords": [],
    "checked": false,
    "id": "4a6efc258088dd2c2245592e5366bf475f92e674",
    "semantic_title": "improving day-ahead solar irradiance time series forecasting by leveraging spatio-temporal context",
    "citation_count": 1,
    "authors": [
      "Oussama Boussif",
      "Ghait Boukachab",
      "Dan Assouline",
      "Stefano Massaroli",
      "Tianle Yuan",
      "Loubna Benabbou",
      "Yoshua Bengio"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/071a637d41ea290ac4360818a8323f33-Abstract-Conference.html": {
    "title": "Uncovering and Quantifying Social Biases in Code Generation",
    "volume": "main",
    "abstract": "With the popularity of automatic code generation tools, such as Copilot, the study of the potential hazards of these tools is gaining importance. In this work, we explore the social bias problem in pre-trained code generation models. We propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. To quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. Experimental results on three pre-trained code generation models (Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases. Moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias",
    "keywords": [],
    "checked": true,
    "id": "e43c46f2c00df489e3a68b58bfcd3f87a84a3f59",
    "semantic_title": "uncovering and quantifying social biases in code generation",
    "citation_count": 2,
    "authors": [
      "Yan Liu",
      "Xiaokang Chen",
      "Yan Gao",
      "Zhe Su",
      "Fengji Zhang",
      "Daoguang Zan",
      "Jian-Guang Lou",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0730b81dbc16cce7e85b519cb7fe5a8d-Abstract-Conference.html": {
    "title": "A Bounded Ability Estimation for Computerized Adaptive Testing",
    "volume": "main",
    "abstract": "Computerized adaptive testing (CAT), as a tool that can efficiently measure student's ability, has been widely used in various standardized tests (e.g., GMAT and GRE). The adaptivity of CAT refers to the selection of the most informative questions for each student, reducing test length. Existing CAT methods do not explicitly target ability estimation accuracy since there is no student's true ability as ground truth; therefore, these methods cannot be guaranteed to make the estimate converge to the true with such limited responses. In this paper, we analyze the statistical properties of estimation and find a theoretical approximation of the true ability: the ability estimated by full responses to question bank. Based on this, a Bounded Ability Estimation framework for CAT (BECAT) is proposed in a data-summary manner, which selects a question subset that closely matches the gradient of the full responses. Thus, we develop an expected gradient difference approximation to design a simple greedy selection algorithm, and show the rigorous theoretical and error upper-bound guarantees of its ability estimate. Experiments on both real-world and synthetic datasets, show that it can reach the same estimation accuracy using 15\\% less questions on average, significantly reducing test length",
    "keywords": [],
    "checked": true,
    "id": "130d1464cc19b02c3a09146e8878ae29e208a1df",
    "semantic_title": "a bounded ability estimation for computerized adaptive testing",
    "citation_count": 0,
    "authors": [
      "Yan Zhuang",
      "Qi Liu",
      "Guanhao Zhao",
      "Zhenya Huang",
      "Weizhe Huang",
      "Zachary Pardos",
      "Enhong Chen",
      "Jinze Wu",
      "Xin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0731f0e65559059eb9cd9d6f44ce2dd8-Abstract-Conference.html": {
    "title": "ForecastPFN: Synthetically-Trained Zero-Shot Forecasting",
    "volume": "main",
    "abstract": "The vast majority of time-series forecasting approaches require a substantial training dataset. However, many real-life forecasting applications have very little initial observations, sometimes just 40 or fewer. Thus, the applicability of most forecasting methods is restricted in data-sparse commercial applications. While there is recent work in the setting of very limited initial data (so-called `zero-shot' forecasting), its performance is inconsistent depending on the data used for pretraining. In this work, we take a different approach and devise ForecastPFN, the first zero-shot forecasting model trained purely on a novel synthetic data distribution. ForecastPFN is a prior-data fitted network, trained to approximate Bayesian inference, which can make predictions on a new time series dataset in a single forward pass. Through extensive experiments, we show that zero-shot predictions made by ForecastPFN are more accurate and faster compared to state-of-the-art forecasting methods, even when the other methods are allowed to train on hundreds of additional in-distribution data points",
    "keywords": [],
    "checked": true,
    "id": "2d2bfb068e3441aaa9743043603d00f860dd0308",
    "semantic_title": "forecastpfn: synthetically-trained zero-shot forecasting",
    "citation_count": 6,
    "authors": [
      "Samuel Dooley",
      "Gurnoor Singh Khurana",
      "Chirag Mohapatra",
      "Siddartha V Naidu",
      "Colin White"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0747af6f877c0cb555fea595f01b0e83-Abstract-Conference.html": {
    "title": "Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach",
    "volume": "main",
    "abstract": "We present an exact Bayesian inference method for discrete statistical models, which can find exact solutions to a large class of discrete inference problems, even with infinite support and continuous priors.To express such models, we introduce a probabilistic programming language that supports discrete and continuous sampling, discrete observations, affine functions, (stochastic) branching, and conditioning on discrete events.Our key tool is probability generating functions:they provide a compact closed-form representation of distributions that are definable by programs, thus enabling the exact computation of posterior probabilities, expectation, variance, and higher moments.Our inference method is provably correct and fully automated in a tool called Genfer, which uses automatic differentiation (specifically, Taylor polynomials), but does not require computer algebra.Our experiments show that Genfer is often faster than the existing exact inference tools PSI, Dice, and Prodigy.On a range of real-world inference problems that none of these exact tools can solve, Genfer's performance is competitive with approximate Monte Carlo methods, while avoiding approximation errors",
    "keywords": [],
    "checked": true,
    "id": "659d2997af18f6f40802de2d146a4488692ee1b0",
    "semantic_title": "exact bayesian inference on discrete models via probability generating functions: a probabilistic programming approach",
    "citation_count": 2,
    "authors": [
      "Fabian Zaiser",
      "Andrzej Murawski",
      "Chih-Hao Luke Ong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/075b2875e2b671ddd74aeec0ac9f0357-Abstract-Conference.html": {
    "title": "$SE(3)$ Equivariant Convolution and Transformer in Ray Space",
    "volume": "main",
    "abstract": "3D reconstruction and novel view rendering can greatly benefit from geometric priors when the input views are not sufficient in terms of coverage and inter-view baselines. Deep learning of geometric priors from 2D images requires each image to be represented in a $2D$ canonical frame and the prior to be learned in a given or learned $3D$ canonical frame. In this paper, given only the relative poses of the cameras, we show how to learn priors from multiple views equivariant to coordinate frame transformations by proposing an $SE(3)$-equivariant convolution and transformer in the space of rays in 3D. We model the ray space as a homogeneous space of $SE(3)$ and introduce the $SE(3)$-equivariant convolution in ray space. Depending on the output domain of the convolution, we present convolution-based $SE(3)$-equivariant maps from ray space to ray space and to $\\mathbb{R}^3$. Our mathematical framework allows us to go beyond convolution to $SE(3)$-equivariant attention in the ray space. We showcase how to tailor and adapt the equivariant convolution and transformer in the tasks of equivariant $3D$ reconstruction and equivariant neural rendering from multiple views. We demonstrate $SE(3)$-equivariance by obtaining robust results in roto-translated datasets without performing transformation augmentation",
    "keywords": [],
    "checked": false,
    "id": "763efe38d5a8ee011b98cbd326834104b8943bae",
    "semantic_title": "equivariant light field convolution and transformer",
    "citation_count": 0,
    "authors": [
      "Yinshuang Xu",
      "Jiahui Lei",
      "Kostas Daniilidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0764db1151b936aca59249e2c1386101-Abstract-Conference.html": {
    "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
    "volume": "main",
    "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including < 200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings",
    "keywords": [],
    "checked": true,
    "id": "e01515c6138bc525f7aec30fc85f2adf028d4156",
    "semantic_title": "principle-driven self-alignment of language models from scratch with minimal human supervision",
    "citation_count": 109,
    "authors": [
      "Zhiqing Sun",
      "Yikang Shen",
      "Qinhong Zhou",
      "Hongxin Zhang",
      "Zhenfang Chen",
      "David Cox",
      "Yiming Yang",
      "Chuang Gan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/076a93fd42aa85f5ccee921a01d77dd5-Abstract-Conference.html": {
    "title": "Prototypical Variational Autoencoder for 3D Few-shot Object Detection",
    "volume": "main",
    "abstract": "Few-Shot 3D Point Cloud Object Detection (FS3D) is a challenging task, aiming to detect 3D objects of novel classes using only limited annotated samples for training. Considering that the detection performance highly relies on the quality of the latent features, we design a VAE-based prototype learning scheme, named prototypical VAE (P-VAE), to learn a probabilistic latent space for enhancing the diversity and distinctiveness of the sampled features. The network encodes a multi-center GMM-like posterior, in which each distribution centers at a prototype. For regularization, P-VAE incorporates a reconstruction task to preserve geometric information. To adopt P-VAE for the detection framework, we formulate Geometric-informative Prototypical VAE (GP-VAE) to handle varying geometric components and Class-specific Prototypical VAE (CP-VAE) to handle varying object categories. In the first stage, we harness GP-VAE to aid feature extraction from the input scene. In the second stage, we cluster the geometric-informative features into per-instance features and use CP-VAE to refine each instance feature with category-level guidance. Experimental results show the top performance of our approach over the state of the arts on two FS3D benchmarks. Quantitative ablations and qualitative prototype analysis further demonstrate that our probabilistic modeling can significantly boost prototype learning for FS3D",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiliang Tang",
      "Biqi YANG",
      "Xianzhi Li",
      "Yun-Hui Liu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/07956d40074d6523bad11112b3225c6e-Abstract-Conference.html": {
    "title": "Double Gumbel Q-Learning",
    "volume": "main",
    "abstract": "We show that Deep Neural Networks introduce two heteroscedastic Gumbel noise sources into Q-Learning. To account for these noise sources, we propose Double Gumbel Q-Learning, a Deep Q-Learning algorithm applicable for both discrete and continuous control. In discrete control, we derive a closed-form expression for the loss function of our algorithm. In continuous control, this loss function is intractable and we therefore derive an approximation with a hyperparameter whose value regulates pessimism in Q-Learning. We present a default value for our pessimism hyperparameter that enables DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D and show that tuning this hyperparameter may further improve sample efficiency",
    "keywords": [],
    "checked": true,
    "id": "a9b16e7729436387b1e51bd4095434ea6ffd9f43",
    "semantic_title": "double gumbel q-learning",
    "citation_count": 0,
    "authors": [
      "David Yu-Tung Hui",
      "Aaron C. Courville",
      "Pierre-Luc Bacon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0799492e7be38b66d10ead5e8809616d-Abstract-Conference.html": {
    "title": "Mutual-Information Regularized Multi-Agent Policy Iteration",
    "volume": "main",
    "abstract": "Despite the success of cooperative multi-agent reinforcement learning algorithms, most of them focus on a single team composition, which prevents them from being used in more realistic scenarios where dynamic team composition is possible. While some studies attempt to solve this problem via multi-task learning in a fixed set of team compositions, there is still a risk of overfitting to the training set, which may lead to catastrophic performance when facing dramatically varying team compositions during execution. To address this problem, we propose to use mutual information (MI) as an augmented reward to prevent individual policies from relying too much on team-related information and encourage agents to learn policies that are robust in different team compositions. Optimizing this MI-augmented objective in an off-policy manner can be intractable due to the existence of dynamic marginal distribution. To alleviate this problem, we first propose a multi-agent policy iteration algorithm with a fixed marginal distribution and prove its convergence and optimality. Then, we propose to employ the Blahut–Arimoto algorithm and an imaginary team composition distribution for optimization with approximate marginal distribution as the practical implementation. Empirically, our method demonstrates strong zero-shot generalization to dynamic team compositions in complex cooperative tasks",
    "keywords": [],
    "checked": false,
    "id": "2b23a0e5c1709d4a82e1bd09a83e2cef83d8dfe8",
    "semantic_title": "mutual information regularized offline reinforcement learning",
    "citation_count": 2,
    "authors": [
      " Wang",
      "Deheng Ye",
      "Zongqing Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/07a363fd2263091c2063998e0034999c-Abstract-Conference.html": {
    "title": "An Efficient End-to-End Training Approach for Zero-Shot Human-AI Coordination",
    "volume": "main",
    "abstract": "The goal of zero-shot human-AI coordination is to develop an agent that can collaborate with humans without relying on human data. Prevailing two-stage population-based methods require a diverse population of mutually distinct policies to simulate diverse human behaviors. The necessity of such populations severely limits their computational efficiency. To address this issue, we propose E3T, an Efficient End-to-End Training approach for zero-shot human-AI coordination. E3T employs a mixture of ego policy and random policy to construct the partner policy, making it both coordination-skilled and diverse. In this way, the ego agent is end-to-end trained with this mixture policy without the need of a pre-trained population, thus significantly improving the training efficiency. In addition, a partner modeling module is proposed to predict the partner's action from historical information. With the predicted partner's action, the ego policy is able to adapt its policy and take actions accordingly when collaborating with humans of different behavior patterns. Empirical results on the Overcooked environment show that our method significantly improves the training efficiency while preserving comparable or superior performance than the population-based baselines. Demo videos are available at https://sites.google.com/view/e3t-overcooked",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xue Yan",
      "Jiaxian Guo",
      "Xingzhou Lou",
      "Jun Wang",
      "Haifeng Zhang",
      "Yali Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/07be1a0850e58ca29e2b6ce31fc0c791-Abstract-Conference.html": {
    "title": "Computing Optimal Equilibria and Mechanisms via Learning in Zero-Sum Extensive-Form Games",
    "volume": "main",
    "abstract": "We introduce a new approach for computing optimal equilibria via learning in games. It applies to extensive-form settings with any number of players, including mechanism design, information design, and solution concepts such as correlated, communication, and certification equilibria. We observe that optimal equilibria are minimax equilibrium strategies of a player in an extensive-form zero-sum game. This reformulation allows to apply techniques for learning in zero-sum games, yielding the first learning dynamics that converge to optimal equilibria, not only in empirical averages, but also in iterates. We demonstrate the practical scalability and flexibility of our approach by attaining state-of-the-art performance in benchmark tabular games, and by computing an optimal mechanism for a sequential auction design problem using deep reinforcement learning",
    "keywords": [],
    "checked": true,
    "id": "2075f2b11c6f6b72ec968dee74a9cd98266e648c",
    "semantic_title": "computing optimal equilibria and mechanisms via learning in zero-sum extensive-form games",
    "citation_count": 6,
    "authors": [
      "Brian Zhang",
      "Gabriele Farina",
      "Ioannis Anagnostides",
      "Federico Cacciamani",
      "Stephen McAleer",
      "Andreas Haupt",
      "Andrea Celli",
      "Nicola Gatti",
      "Vincent Conitzer",
      "Tuomas Sandholm"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/07cf32cf61224da628157b7ed0ce994a-Abstract-Conference.html": {
    "title": "Parts of Speech–Grounded Subspaces in Vision-Language Models",
    "volume": "main",
    "abstract": "Latent image representations arising from vision-language models have proved immensely useful for a variety of downstream tasks. However, their utility is limited by their entanglement with respect to different visual attributes. For instance, recent work has shown that CLIP image representations are often biased toward specific visual properties (such as objects or actions) in an unpredictable manner. In this paper, we propose to separate representations of the different visual modalities in CLIP's joint vision-language space by leveraging the association between parts of speech and specific visual modes of variation (e.g. nouns relate to objects, adjectives describe appearance). This is achieved by formulating an appropriate component analysis model that learns subspaces capturing variability corresponding to a specific part of speech, while jointly minimising variability to the rest. Such a subspace yields disentangled representations of the different visual properties of an image or text in closed form while respecting the underlying geometry of the manifold on which the representations lie. What's more, we show the proposed model additionally facilitates learning subspaces corresponding to specific visual appearances (e.g. artists' painting styles), which enables the selective removal of entire visual themes from CLIP-based text-to-image synthesis. We validate the model both qualitatively, by visualising the subspace projections with a text-to-image model and by preventing the imitation of artists' styles, and quantitatively, through class invariance metrics and improvements to baseline zero-shot classification",
    "keywords": [],
    "checked": false,
    "id": "470e12281fde486f2872f825528198384ff76702",
    "semantic_title": "parts of speech-grounded subspaces in vision-language models",
    "citation_count": 2,
    "authors": [
      "James Oldfield",
      "Christos Tzelepis",
      "Yannis Panagakis",
      "Mihalis Nicolaou",
      "Ioannis Patras"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/07e436cdeb48e2a67618274f5d5eff85-Abstract-Conference.html": {
    "title": "Searching for Optimal Per-Coordinate Step-sizes with Multidimensional Backtracking",
    "volume": "main",
    "abstract": "The backtracking line-search is an effective technique to automatically tune the step-size in smooth optimization. It guarantees similar performance to using the theoretically optimal step-size. Many approaches have been developed to instead tune per-coordinate step-sizes, also known as diagonal preconditioners, but none of the existing methods are provably competitive with the optimal per-coordinate step-sizes. We propose multidimensional backtracking, an extension of the backtracking line-search to find good diagonal preconditioners for smooth convex problems. Our key insight is that the gradient with respect to the step-sizes, also known as hyper-gradients, yields separating hyperplanes that let us search for good preconditioners using cutting-plane methods. As black-box cutting-plane approaches like the ellipsoid method are computationally prohibitive, we develop an efficient algorithm tailored to our setting. Multidimensional backtracking is provably competitive with the best diagonal preconditioner and requires no manual tuning",
    "keywords": [],
    "checked": true,
    "id": "e9ab52f657ec4b0775c5b17b670a2b15792c4a4c",
    "semantic_title": "searching for optimal per-coordinate step-sizes with multidimensional backtracking",
    "citation_count": 3,
    "authors": [
      "Frederik Kunstner",
      "Victor Sanches Portella",
      "Mark Schmidt",
      "Nicholas Harvey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/07eea3fb833c905c5edf46f914231f15-Abstract-Conference.html": {
    "title": "Estimating the Rate-Distortion Function by Wasserstein Gradient Descent",
    "volume": "main",
    "abstract": "In the theory of lossy compression, the rate-distortion (R-D) function $R(D)$ describes how much a data source can be compressed (in bit-rate) at any given level of fidelity (distortion). Obtaining $R(D)$ for a given data source establishes the fundamental performance limit for all compression algorithms. We propose a new method to estimate $R(D)$ from the perspective of optimal transport. Unlike the classic Blahut--Arimoto algorithm which fixes the support of the reproduction distribution in advance, our Wasserstein gradient descent algorithm learns the support of the optimal reproduction distribution by moving particles. We prove its local convergence and analyze the sample complexity of our R-D estimator based on a connection to entropic optimal transport. Experimentally, we obtain comparable or tighter bounds than state-of-the-art neural network methods on low-rate sources while requiring considerably less tuning and computation effort. We also highlight a connection to maximum-likelihood deconvolution and introduce a new class of sources that can be used as test cases with known solutions to the R-D problem",
    "keywords": [],
    "checked": true,
    "id": "25ee53d80c48e56bdab4a4f4989f0772d931b09b",
    "semantic_title": "estimating the rate-distortion function by wasserstein gradient descent",
    "citation_count": 1,
    "authors": [
      "Yibo Yang",
      "Stephan Eckstein",
      "Marcel Nutz",
      "Stephan Mandt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/07fbde96bee50f4e09303fd4f877c2f3-Abstract-Conference.html": {
    "title": "Epistemic Neural Networks",
    "volume": "main",
    "abstract": "Intelligence relies on an agent's knowledge of what it does not know.This capability can be assessed based on the quality of joint predictions of labels across multiple inputs.In principle, ensemble-based approaches can produce effective joint predictions, but the computational costs of large ensembles become prohibitive.We introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty.With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation.The epinet does not fit the traditional framework of Bayesian neural networks.To accommodate development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as a general interface for models that produce joint predictions",
    "keywords": [],
    "checked": true,
    "id": "1106f88502c8681c774a63fd1553fb98525fe2fa",
    "semantic_title": "epistemic neural networks",
    "citation_count": 60,
    "authors": [
      "Ian Osband",
      "Zheng Wen",
      "Seyed Mohammad Asghari",
      "Vikranth Dwaracherla",
      "MORTEZA IBRAHIMI",
      "Xiuyuan Lu",
      "Benjamin Van Roy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/081b08068e4733ae3e7ad019fe8d172f-Abstract-Conference.html": {
    "title": "HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception",
    "volume": "main",
    "abstract": "The bird's-eye-view (BEV) perception plays a critical role in autonomous driving systems, involving the accurate and efficient detection and tracking of objects from a top-down perspective. To achieve real-time decision-making in self-driving scenarios, low-latency computation is essential. While recent approaches to BEV detection have focused on improving detection precision using Lift-Splat-Shoot (LSS)-based or transformer-based schemas, the substantial computational and memory burden of these approaches increases the risk of system crashes when multiple on-vehicle tasks run simultaneously. Unfortunately, there is a dearth of literature on efficient BEV detector paradigms, let alone achieving realistic speedups.Unlike existing works that focus on reducing computation costs, this paper focuses on developing an efficient model design that prioritizes actual on-device latency.To achieve this goal, we propose a latency-aware design methodology that considers key hardware properties, such as memory access cost and degree of parallelism.Given the prevalence of GPUs as the main computation platform for autonomous driving systems, we develop a theoretical latency prediction model and introduce efficient building operators.By leveraging these operators and following an effective local-to-global visual modeling process, we propose a hardware-oriented backbone that is also optimized for strong feature capturing and fusing.Using these insights, we present a new hardware-oriented framework for efficient yet accurate camera-view BEV detectors.Experiments show that HotBEV achieves a 2\\%$\\sim$23\\% NDS gain, and 2\\%$\\sim$7.8\\% mAP gain with a 1.1$\\times$$\\sim$3.4$\\times$ speedups compared to existing works on V100;On multiple GPU devices such as GPU GTX 2080 and the low-end GTX 1080, HotBEV achieves 1.1$\\times$$\\sim$6.3$\\times$ faster than others",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiyan Dong",
      "Zhenglun Kong",
      "Xin Meng",
      "Pinrui Yu",
      "Yifan Gong",
      "Geng Yuan",
      "Hao Tang",
      "Yanzhi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/082d3d795520c43214da5123e56a3a34-Abstract-Conference.html": {
    "title": "Mip-Grid: Anti-aliased Grid Representations for Neural Radiance Fields",
    "volume": "main",
    "abstract": "Despite the remarkable achievements of neural radiance fields (NeRF) in representing 3D scenes and generating novel view images, the aliasing issue, rendering 'jaggies' or 'blurry' images at varying camera distances, remains unresolved in most existing approaches. The recently proposed mip-NeRF has effectively addressed this challenge by introducing integrated positional encodings (IPE). However, it relies on MLP architecture to represent the radiance fields, missing out on the fast training speed offered by the latest grid-based methods. In this work, we present mip-Grid, a novel approach that integrates anti-aliasing techniques into grid-based representations for radiance fields, mitigating the aliasing artifacts while enjoying fast training time. Notably, the proposed method uses a single-scale shared grid representation and a single-sampling approach, which only introduces minimal additions to the model parameters and computational costs. To handle scale ambiguity, mip-Grid generates multiple grids by applying simple convolution operations over the shared grid and uses the scale-aware coordinate to retrieve the appropriate features from the generated multiple grids. To test the effectiveness, we incorporated the proposed approach into the two recent representative grid-based methods, TensoRF and K-Planes. The experimental results demonstrated that mip-Grid greatly improved the rendering performance of both methods and showed comparable performance to mip-NeRF on multi-scale datasets while achieving significantly faster training time",
    "keywords": [],
    "checked": false,
    "id": "9eafa581e0268d471b9c61c87db79710dab9274e",
    "semantic_title": "zip-nerf: anti-aliased grid-based neural radiance fields",
    "citation_count": 102,
    "authors": [
      "Seungtae Nam",
      "Daniel Rho",
      "Jong Hwan Ko",
      "Eunbyung Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/08309150af77fc7c79ade0bf8bb6a562-Abstract-Conference.html": {
    "title": "Theoretically Guaranteed Bidirectional Data Rectification for Robust Sequential Recommendation",
    "volume": "main",
    "abstract": "Sequential recommender systems (SRSs) are typically trained to predict the next item as the target given its preceding (and succeeding) items as the input. Such a paradigm assumes that every input-target pair is reliable for training. However, users can be induced to click on items that are inconsistent with their true preferences, resulting in unreliable instances, i.e., mismatched input-target pairs. Current studies on mitigating this issue suffer from two limitations: (i) they discriminate instance reliability according to models trained with unreliable data, yet without theoretical guarantees that such a seemingly contradictory solution can be effective; and (ii) most methods can only tackle either unreliable input or targets but fail to handle both simultaneously. To fill the gap, we theoretically unveil the relationship between SRS predictions and instance reliability, whereby two error-bounded strategies are proposed to rectify unreliable targets and input, respectively. On this basis, we devise a model-agnostic Bidirectional Data Rectification (BirDRec) framework, which can be flexibly implemented with most existing SRSs for robust training against unreliable data. Additionally, a rectification sampling strategy is devised and a self-ensemble mechanism is adopted to reduce the (time and space) complexity of BirDRec. Extensive experiments on four real-world datasets verify the generality, effectiveness, and efficiency of our proposed BirDRec",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yatong Sun",
      "Bin Wang",
      "Zhu Sun",
      "Xiaochun Yang",
      "Yan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/08342dc6ab69f23167b4123086ad4d38-Abstract-Conference.html": {
    "title": "Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards",
    "volume": "main",
    "abstract": "As the capabilities of artificial agents improve, they are being increasingly deployed to service multiple diverse objectives and stakeholders. However, the composition of these objectives is often performed ad hoc, with no clear justification. This paper takes a normative approach to multi-objective agency: from a set of intuitively appealing axioms, it is shown that Markovian aggregation of Markovian reward functions is not possible when the time preference (discount factor) for each objective may vary. It follows that optimal multi-objective agents must admit rewards that are non-Markovian with respect to the individual objectives. To this end, a practical non-Markovian aggregation scheme is proposed, which overcomes the impossibility with only one additional parameter for each objective. This work offers new insights into sequential, multi-objective agency and intertemporal choice, and has practical implications for the design of AI systems deployed to serve multiple generations of principals with varying time preference",
    "keywords": [],
    "checked": true,
    "id": "d6f452265914d8f5ecf185f999170f6a106602e8",
    "semantic_title": "consistent aggregation of objectives with diverse time preferences requires non-markovian rewards",
    "citation_count": 1,
    "authors": [
      "Silviu Pitis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/088463cd3126aef2002ffc69da42ec59-Abstract-Conference.html": {
    "title": "Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability",
    "volume": "main",
    "abstract": "Neural networks are known to be susceptible to adversarial samples: small variations of natural examples crafted to deliberatelymislead the models. While they can be easily generated using gradient-based techniques in digital and physical scenarios, they often differ greatly from the actual data distribution of natural images, resulting in a trade-off between strength and stealthiness. In this paper, we propose a novel framework dubbed Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic adversarial samples. By exploiting a gradient guided by a diffusion model, Diff-PGD ensures that adversarial samples remain close to the original data distribution while maintaining their effectiveness. Moreover, our framework can be easily customized for specific tasks such as digital attacks, physical-world attacks, and style-based attacks. Compared with existing methods for generating natural-style adversarial samples, our framework enables the separation of optimizing adversarial loss from other surrogate losses (e.g. content/smoothness/style loss), making it more stable and controllable. Finally, we demonstrate that the samples generated using Diff-PGD have better transferability and anti-purification power than traditional gradient-based methods",
    "keywords": [],
    "checked": true,
    "id": "fcd9a82decb020e946e9adde4fab9ba055e8396b",
    "semantic_title": "diffusion-based adversarial sample generation for improved stealthiness and controllability",
    "citation_count": 8,
    "authors": [
      "Haotian Xue",
      "Alexandre Araujo",
      "Bin Hu",
      "Yongxin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/088d99765bc121c6df215da7d45bc4e9-Abstract-Conference.html": {
    "title": "InstanT: Semi-supervised Learning with Instance-dependent Thresholds",
    "volume": "main",
    "abstract": "Semi-supervised learning (SSL) has been a fundamental challenge in machine learning for decades. The primary family of SSL algorithms, known as pseudo-labeling, involves assigning pseudo-labels to confident unlabeled instances and incorporating them into the training set. Therefore, the selection criteria of confident instances are crucial to the success of SSL. Recently, there has been growing interest in the development of SSL methods that use dynamic or adaptive thresholds. Yet, these methods typically apply the same threshold to all samples, or use class-dependent thresholds for instances belonging to a certain class, while neglecting instance-level information. In this paper, we propose the study of instance-dependent thresholds, which has the highest degree of freedom compared with existing methods. Specifically, we devise a novel instance-dependent threshold function for all unlabeled instances by utilizing their instance-level ambiguity and the instance-dependent error rates of pseudo-labels, so instances that are more likely to have incorrect pseudo-labels will have higher thresholds. Furthermore, we demonstrate that our instance-dependent threshold function provides a bounded probabilistic guarantee for the correctness of the pseudo-labels it assigns",
    "keywords": [],
    "checked": true,
    "id": "e9ca3a5f33ab79bf96ad347b4162d24a0484389d",
    "semantic_title": "instant: semi-supervised learning with instance-dependent thresholds",
    "citation_count": 0,
    "authors": [
      "Muyang Li",
      "Runze Wu",
      "Haoyu Liu",
      "Jun Yu",
      "Xun Yang",
      "Bo Han",
      "Tongliang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/08bf1773e94763b6cc366ee7c6582f27-Abstract-Conference.html": {
    "title": "Neural Lyapunov Control for Discrete-Time Systems",
    "volume": "main",
    "abstract": "While ensuring stability for linear systems is well understood, it remains a major challenge for nonlinear systems. A general approach in such cases is to compute a combination of a Lyapunov function and an associated control policy. However, finding Lyapunov functions for general nonlinear systems is a challenging task. To address this challenge, several methods have been proposed that represent Lyapunov functions using neural networks. However, such approaches either focus on continuous-time systems, or highly restricted classes of nonlinear dynamics. We propose the first approach for learning neural Lyapunov control in a broad class of discrete-time systems. Three key ingredients enable us to effectively learn provably stable control policies. The first is a novel mixed-integer linear programming approach for verifying the discrete-time Lyapunov stability conditions, leveraging the particular structure of these conditions. The second is a novel approach for computing verified sublevel sets. The third is a heuristic gradient-based method for quickly finding counterexamples to significantly speed up Lyapunov function learning. Our experiments on four standard benchmarks demonstrate that our approach significantly outperforms state-of-the-art baselines. For example, on the path tracking benchmark, we outperform recent neural Lyapunov control baselines by an order of magnitude in both running time and the size of the region of attraction, and on two of the four benchmarks (cartpole and PVTOL), ours is the first automated approach to return a provably stable controller. Our code is available at: https://github.com/jlwu002/nlc_discrete",
    "keywords": [],
    "checked": true,
    "id": "9d621d0f878f90229eb5eed47e7082f549002195",
    "semantic_title": "neural lyapunov control for discrete-time systems",
    "citation_count": 1,
    "authors": [
      "Junlin Wu",
      "Andrew Clark",
      "Yiannis Kantaros",
      "Yevgeniy Vorobeychik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/08eac13583b310ec55d755f99c549be3-Abstract-Conference.html": {
    "title": "Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable AI",
    "volume": "main",
    "abstract": "Information Pursuit (IP) is a classical active testing algorithm for predicting an output by sequentially and greedily querying the input in order of information gain. However, IP is computationally intensive since it involves estimating mutual information in high-dimensional spaces. This paper explores Orthogonal Matching Pursuit (OMP) as an alternative to IP for greedily selecting the queries. OMP is a classical signal processing algorithm for sequentially encoding a signal in terms of dictionary atoms chosen in order of correlation gain. In each iteration, OMP selects the atom that is most correlated with the signal residual (the signal minus its reconstruction thus far). Our first contribution is to establish a fundamental connection between IP and OMP, where we prove that IP with random projections of dictionary atoms as queries ``almost'' reduces to OMP, with the difference being that IP selects atoms in order of normalized correlation gain. We call this version IP-OMP and present simulations indicating that this difference does not have any appreciable effect on the sparse code recovery rate of IP-OMP compared to that of OMP for random Gaussian dictionaries. Inspired by this connection, our second contribution is to explore the utility of IP-OMP for generating explainable predictions, an area in which IP has recently gained traction. More specifically, we propose a simple explainable AI algorithm which encodes an image as a sparse combination of semantically meaningful dictionary atoms that are defined as text embeddings of interpretable concepts. The final prediction is made using the weights of this sparse combination, which serve as an explanation. Empirically, our proposed algorithm is not only competitive with existing explainability methods but also computationally less expensive",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Chattopadhyay",
      "Ryan Pilgrim",
      "Rene Vidal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/08f9de0232c0b485110237f6e6cf88f1-Abstract-Conference.html": {
    "title": "Evolving Connectivity for Recurrent Spiking Neural Networks",
    "volume": "main",
    "abstract": "Recurrent spiking neural networks (RSNNs) hold great potential for advancing artificial general intelligence, as they draw inspiration from the biological nervous system and show promise in modeling complex dynamics.However, the widely-used surrogate gradient-based training methods for RSNNs are inherently inaccurate and unfriendly to neuromorphic hardware.To address these limitations, we propose the evolving connectivity (EC) framework, an inference-only method for training RSNNs.The EC framework reformulates weight-tuning as a search into parameterized connection probability distributions, and employs Natural Evolution Strategies (NES) for optimizing these distributions.Our EC framework circumvents the need for gradients and features hardware-friendly characteristics, including sparse boolean connections and high scalability.We evaluate EC on a series of standard robotic locomotion tasks, where it achieves comparable performance with deep neural networks and outperforms gradient-trained RSNNs, even solving the complex 17-DoF humanoid task.Additionally, the EC framework demonstrates a two to three fold speedup in efficiency compared to directly evolving parameters.By providing a performant and hardware-friendly alternative, the EC framework lays the groundwork for further energy-efficient applications of RSNNs and advances the development of neuromorphic devices.Our code is publicly available at https://github.com/imoneoi/EvolvingConnectivity",
    "keywords": [],
    "checked": true,
    "id": "517393c7b3a0730fc73000b9b575fa5756ed589c",
    "semantic_title": "evolving connectivity for recurrent spiking neural networks",
    "citation_count": 1,
    "authors": [
      "Guan Wang",
      "Yuhao Sun",
      "Sijie Cheng",
      "Sen Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/090b23d52bc2722eef2fbf79c5ebf9ec-Abstract-Conference.html": {
    "title": "Bayesian Optimization with Cost-varying Variable Subsets",
    "volume": "main",
    "abstract": "We introduce the problem of Bayesian optimization with cost-varying variable subsets (BOCVS) where in each iteration, the learner chooses a subset of query variables and specifies their values while the rest are randomly sampled. Each chosen subset has an associated cost. This presents the learner with the novel challenge of balancing between choosing more informative subsets for more directed learning versus leaving some variables to be randomly sampled to reduce incurred costs. This paper presents a novel Gaussian process upper confidence bound-based algorithm for solving the BOCVS problem that is provably no-regret. We analyze how the availability of cheaper control sets helps in exploration and reduces overall regret. We empirically show that our proposed algorithm can find significantly better solutions than comparable baselines with the same budget",
    "keywords": [],
    "checked": false,
    "id": "1da76ea0444047a98c50bd5077edbf489f58414e",
    "semantic_title": "personalized bayesian optimization for noisy problems",
    "citation_count": 0,
    "authors": [
      "Sebastian Tay",
      "Chuan Sheng Foo",
      "Daisuke Urano",
      "Richalynn Leong",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/092c2d45005ea2db40fc24c470663416-Abstract-Conference.html": {
    "title": "Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks",
    "volume": "main",
    "abstract": "Multi-channel learning has gained significant attention in recent applications, where neural networks with t-product layers (t-NNs) have shown promising performance through novel feature mapping in the transformed domain. However, despite the practical success of t-NNs, the theoretical analysis of their generalization remains unexplored. We address this gap by deriving upper bounds on the generalization error of t-NNs in both standard and adversarial settings. Notably, it reveals that t-NNs compressed with exact transformed low-rank parameterization can achieve tighter adversarial generalization bounds compared to non-compressed models. While exact transformed low-rank weights are rare in practice, the analysis demonstrates that through adversarial training with gradient flow, highly over-parameterized t-NNs with the ReLU activation can be implicitly regularized towards a transformed low-rank parameterization under certain conditions. Moreover, this paper establishes sharp adversarial generalization bounds for t-NNs with approximately transformed low-rank weights. Our analysis highlights the potential of transformed low-rank parameterization in enhancing the robust generalization of t-NNs, offering valuable insights for further research and development",
    "keywords": [],
    "checked": true,
    "id": "173d52d4483387b5d8ea0e9aab3a529ade78104c",
    "semantic_title": "transformed low-rank parameterization can help robust generalization for tensor neural networks",
    "citation_count": 1,
    "authors": [
      "Andong Wang",
      "Chao Li",
      "Mingyuan Bai",
      "Zhong Jin",
      "Guoxu Zhou",
      "Qibin Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/09425891e393e64b0535194a81ba15b7-Abstract-Conference.html": {
    "title": "Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples",
    "volume": "main",
    "abstract": "Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction",
    "keywords": [],
    "checked": true,
    "id": "c58325547156a70cb27c148e5b57738ca9ce79aa",
    "semantic_title": "testing the general deductive reasoning capacity of large language models using ood examples",
    "citation_count": 19,
    "authors": [
      "Abulhair Saparov",
      "Richard Yuanzhe Pang",
      "Vishakh Padmakumar",
      "Nitish Joshi",
      "Mehran Kazemi",
      "Najoung Kim",
      "He He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/095a6917768712b7ccc61acbeecad1d8-Abstract-Conference.html": {
    "title": "MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining",
    "volume": "main",
    "abstract": "Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pretrained from scratch on the C4 dataset, this base model achieves a downstream average GLUE score of 79.6 in 1.13 hours on 8 A100 80 GB GPUs at a cost of roughly $20. We plot extensive accuracy vs. pretraining speed Pareto curves and show that MosaicBERT base and large are consistently Pareto optimal when compared to a competitive BERT base and large. This empirical speed up in pretraining enables researchers and engineers to pretrain custom BERT-style models at low cost instead of finetune on existing generic models. We open source our model weights and code",
    "keywords": [],
    "checked": true,
    "id": "a4715887bd5a4c328ebf1d6ebb18ab94e71ee8d8",
    "semantic_title": "mosaicbert: a bidirectional encoder optimized for fast pretraining",
    "citation_count": 1,
    "authors": [
      "Jacob Portes",
      "Alexander Trott",
      "Sam Havens",
      "DANIEL KING",
      "Abhinav Venigalla",
      "Moin Nadeem",
      "Nikhil Sardana",
      "Daya Khudia",
      "Jonathan Frankle"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/096961cae3c3423c44ea045aeb584e05-Abstract-Conference.html": {
    "title": "GraphMP: Graph Neural Network-based Motion Planning with Efficient Graph Search",
    "volume": "main",
    "abstract": "Motion planning, which aims to find a high-quality collision-free path in the configuration space, is a fundamental task in robotic systems. Recently, learning-based motion planners, especially the graph neural network-powered, have shown promising planning performance. However, though the state-of-the-art GNN planner can efficiently extract and learn graph information, its inherent mechanism is not well suited for graph search process, hindering its further performance improvement. To address this challenge and fully unleash the potential of GNN in motion planning, this paper proposes GraphMP, a neural motion planner for both low and high-dimensional planning tasks. With the customized model architecture and training mechanism design, GraphMP can simultaneously perform efficient graph pattern extraction and graph search processing, leading to strong planning performance. Experiments on a variety of environments, ranging from 2D Maze to 14D dual KUKA robotic arm, show that our proposed GraphMP achieves significant improvement on path quality and planning speed over the state-of-the-art learning-based and classical planners; while preserving the competitive success rate",
    "keywords": [],
    "checked": false,
    "id": "3c3de90b055c7e015f7dc4c837a6f5d15ea74bd1",
    "semantic_title": "dyngmp: graph neural network-based motion planning in unpredictable dynamic environments",
    "citation_count": 0,
    "authors": [
      "Xiao Zang",
      "Miao Yin",
      "Jinqi Xiao",
      "Saman Zonouz",
      "Bo Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/096b1019463f34eb241e87cfce8dfe16-Abstract-Conference.html": {
    "title": "Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples",
    "volume": "main",
    "abstract": "Learning controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature.This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. AOC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability.We assess AOC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability",
    "keywords": [],
    "checked": true,
    "id": "9e3079df985f3464bd3a49cce10550ed49fcbf78",
    "semantic_title": "accountability in offline reinforcement learning: explaining decisions with a corpus of examples",
    "citation_count": 1,
    "authors": [
      "Hao Sun",
      "Alihan Hüyük",
      "Daniel Jarrett",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0973524e02a712af33325d0688ae6f49-Abstract-Conference.html": {
    "title": "SOAR: Improved Indexing for Approximate Nearest Neighbor Search",
    "volume": "main",
    "abstract": "This paper introduces SOAR: Spilling with Orthogonality-Amplified Residuals, a novel data indexing technique for approximate nearest neighbor (ANN) search. SOAR extends upon previous approaches to ANN search, such as spill trees, that utilize multiple redundant representations while partitioning the data to reduce the probability of missing a nearest neighbor during search. Rather than training and computing these redundant representations independently, however, SOAR uses an orthogonality-amplified residual loss, which optimizes each representation to compensate for cases where other representations perform poorly. This drastically improves the overall index quality, resulting in state-of-the-art ANN benchmark performance while maintaining fast indexing times and low memory consumption",
    "keywords": [],
    "checked": false,
    "id": "c96b300bca15b7acb9b138ccb916f99276bd714b",
    "semantic_title": "a meta-indexing method for fast probably approximately correct nearest neighbor searches",
    "citation_count": 1,
    "authors": [
      "Philip Sun",
      "David Simcha",
      "Dave Dopson",
      "Ruiqi Guo",
      "Sanjiv Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/098491b37deebbe6c007e69815729e09-Abstract-Conference.html": {
    "title": "Type-to-Track: Retrieve Any Object via Prompt-based Tracking",
    "volume": "main",
    "abstract": "One of the recent trends in vision problems is to use natural language captions to describe the objects of interest. This approach can overcome some limitations of traditional methods that rely on bounding boxes or category annotations. This paper introduces a novel paradigm for Multiple Object Tracking called Type-to-Track, which allows users to track objects in videos by typing natural language descriptions. We present a new dataset for that Grounded Multiple Object Tracking task, called GroOT, that contains videos with various types of objects and their corresponding textual captions describing their appearance and action in detail. Additionally, we introduce two new evaluation protocols and formulate evaluation metrics specifically for this task. We develop a new efficient method that models a transformer-based eMbed-ENcoDE-extRact framework (MENDER) using the third-order tensor decomposition. The experiments in five scenarios show that our MENDER approach outperforms another two-stage design in terms of accuracy and efficiency, up to 14.7\\% accuracy and $4\\times$ speed faster",
    "keywords": [],
    "checked": true,
    "id": "334377132b587261d12734a43c94cdbe72d32899",
    "semantic_title": "type-to-track: retrieve any object via prompt-based tracking",
    "citation_count": 1,
    "authors": [
      "Pha Nguyen",
      "Kha Gia Quach",
      "Kris Kitani",
      "Khoa Luu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/09ae6beae5f1ff38f05c05979097ea0f-Abstract-Conference.html": {
    "title": "Finding Counterfactually Optimal Action Sequences in Continuous State Spaces",
    "volume": "main",
    "abstract": "Whenever a clinician reflects on the efficacy of a sequence of treatment decisions for a patient, they may try to identify critical time steps where, had they made different decisions, the patient's health would have improved. While recent methods at the intersection of causal inference and reinforcement learning promise to aid human experts, as the clinician above, to retrospectively analyze sequential decision making processes, they have focused on environments with finitely many discrete states. However, in many practical applications, the state of the environment is inherently continuous in nature. In this paper, we aim to fill this gap. We start by formally characterizing a sequence of discrete actions and continuous states using finite horizon Markov decision processes and a broad class of bijective structural causal models. Building upon this characterization, we formalize the problem of finding counterfactually optimal action sequences and show that, in general, we cannot expect to solve it in polynomial time. Then, we develop a search method based on the A* algorithm that, under a natural form of Lipschitz continuity of the environment's dynamics, is guaranteed to return the optimal solution to the problem. Experiments on real clinical data show that our method is very efficient in practice, and it has the potential to offer interesting insights for sequential decision making tasks",
    "keywords": [],
    "checked": true,
    "id": "ba24491c52f10ba705ae88dece4be7be058de0d4",
    "semantic_title": "finding counterfactually optimal action sequences in continuous state spaces",
    "citation_count": 2,
    "authors": [
      "Stratis Tsirtsis",
      "Manuel Rodriguez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/09d9a13f7018110cfb439c06b07940a2-Abstract-Conference.html": {
    "title": "Reusing Pretrained Models by Multi-linear Operators for Efficient Training",
    "volume": "main",
    "abstract": "Training large models from scratch usually costs a substantial amount of resources. Towards this problem, recent studies such as bert2BERT and LiGO have reused small pretrained models to initialize a large model (termed the ``target model''), leading to a considerable acceleration in training. Despite the successes of these previous studies, they grew pretrained models by mapping partial weights only, ignoring potential correlations across the entire model. As we show in this paper, there are inter- and intra-interactions among the weights of both the pretrained and the target models. As a result, the partial mapping may not capture the complete information and lead to inadequate growth. In this paper, we propose a method that linearly correlates each weight of the target model to all the weights of the pretrained model to further enhance acceleration ability. We utilize multi-linear operators to reduce computational and spacial complexity, enabling acceptable resource requirements. Experiments demonstrate that our method can save 76\\% computational costs on DeiT-base transferred from DeiT-small, which outperforms bert2BERT by +12\\% and LiGO by +21\\%, respectively",
    "keywords": [],
    "checked": true,
    "id": "317bad11d7207183c1538db511c3592297e364e5",
    "semantic_title": "reusing pretrained models by multi-linear operators for efficient training",
    "citation_count": 2,
    "authors": [
      "Yu Pan",
      "Ye Yuan",
      "Yichun Yin",
      "Zenglin Xu",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0a003511b09274348b8117f5f3b94c93-Abstract-Conference.html": {
    "title": "DreamSparse: Escaping from Plato's Cave with 2D Diffusion Model Given Sparse Views",
    "volume": "main",
    "abstract": "Synthesizing novel view images from a few views is a challenging but practical problem. Existing methods often struggle with producing high-quality results or necessitate per-object optimization in such few-view settings due to the insufficient information provided. In this work, we explore leveraging the strong 2D priors in pre-trained diffusion models for synthesizing novel view images. 2D diffusion models, nevertheless, lack 3D awareness, leading to distorted image synthesis and compromising the identity. To address these problems, we propose $\\textit{DreamSparse}$, a framework that enables the frozen pre-trained diffusion model to generate geometry and identity-consistent novel view images. Specifically, DreamSparse incorporates a geometry module designed to capture features about spatial information from sparse views as a 3D prior. Subsequently, a spatial guidance model is introduced to convert rendered feature maps as spatial information for the generative process. This information is then used to guide the pre-trained diffusion model toencourage the synthesis of geometrically consistent images without further tuning. Leveraging the strong image priors in the pre-trained diffusion models, DreamSparse is capable of synthesizing high-quality novel views for both object and object-centric scene-level images and generalising to open-set images.Experimental results demonstrate that our framework can effectively synthesize novel view images from sparse views and outperforms baselines in both trained and open-set category images. More results can be found on our project page: https://sites.google.com/view/dreamsparse-webpage",
    "keywords": [],
    "checked": false,
    "id": "48dc3d9acdc624add5b68984d6419a10883a7b7e",
    "semantic_title": "dreamsparse: escaping from plato's cave with 2d frozen diffusion model given sparse views",
    "citation_count": 11,
    "authors": [
      "Paul Yoo",
      "Jiaxian Guo",
      "Yutaka Matsuo",
      "Shixiang (Shane) Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0a3dc35a2391cabcb59a6b123544e3db-Abstract-Conference.html": {
    "title": "Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling",
    "volume": "main",
    "abstract": "This paper provides statistical sample complexity bounds for score-matching and its applications in causal discovery. We demonstrate that accurate estimation of the score function is achievable by training a standard deep ReLU neural network using stochastic gradient descent. We establish bounds on the error rate of recovering causal relationships using the score-matching-based causal discovery method of Rolland et al. [2022], assuming a sufficiently good estimation of the score function. Finally, we analyze the upper bound of score-matching estimation within the score-based generative modeling, which has been applied for causal discovery but is also of independent interest within the domain of generative models",
    "keywords": [],
    "checked": true,
    "id": "fed787fae9bdc09f78e87f99f1758a3a8b22ec8d",
    "semantic_title": "sample complexity bounds for score-matching: causal discovery and generative modeling",
    "citation_count": 0,
    "authors": [
      "Zhenyu Zhu",
      "Francesco Locatello",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0a443a000e1cb2281480b3bac395b3b8-Abstract-Conference.html": {
    "title": "Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experiments is available at \\url{https://github.com/zknus/NeurIPS-2023-HANG-Robustness}",
    "keywords": [],
    "checked": true,
    "id": "72ce1f2b6c2f66f548607faf53ffccf1f5a2acd5",
    "semantic_title": "adversarial robustness in graph neural networks: a hamiltonian approach",
    "citation_count": 3,
    "authors": [
      "Kai Zhao",
      "Qiyu Kang",
      "Yang Song",
      "Rui She",
      "Sijie Wang",
      "Wee Peng Tay"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0a49935d2b3d3342ca08d6db0adcfa34-Abstract-Conference.html": {
    "title": "A Path to Simpler Models Starts With Noise",
    "volume": "main",
    "abstract": "The Rashomon set is the set of models that perform approximately equally well on a given dataset, and the Rashomon ratio is the fraction of all models in a given hypothesis space that are in the Rashomon set. Rashomon ratios are often large for tabular datasets in criminal justice, healthcare, lending, education, and in other areas, which has practical implications about whether simpler models can attain the same level of accuracy as more complex models. An open question is why Rashomon ratios often tend to be large. In this work, we propose and study a mechanism of the data generation process, coupled with choices usually made by the analyst during the learning process, that determines the size of the Rashomon ratio. Specifically, we demonstrate that noisier datasets lead to larger Rashomon ratios through the way that practitioners train models. Additionally, we introduce a measure called pattern diversity, which captures the average difference in predictions between distinct classification patterns in the Rashomon set, and motivate why it tends to increase with label noise. Our results explain a key aspect of why simpler models often tend to perform as well as black box models on complex, noisier datasets",
    "keywords": [],
    "checked": true,
    "id": "c371e4fff5c5be5f6abd2faf549001c47dddccca",
    "semantic_title": "a path to simpler models starts with noise",
    "citation_count": 0,
    "authors": [
      "Lesia Semenova",
      "Harry Chen",
      "Ronald Parr",
      "Cynthia Rudin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0a6059857ae5c82ea9726ee9282a7145-Abstract-Conference.html": {
    "title": "Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model",
    "volume": "main",
    "abstract": "As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. Previous works usually focus on reducing the number of trainable parameters in the network. While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. Notably, machine learning models are typically trained using stochastic gradient descent.We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.Following this motivation, we propose a new family of unbiased estimators called \\sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\\times$ larger batch size.Under the same hardware, \\sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/",
    "keywords": [],
    "checked": true,
    "id": "6d31db7c53853de62cacec26facdb4300d6b5092",
    "semantic_title": "winner-take-all column row sampling for memory efficient adaptation of language model",
    "citation_count": 8,
    "authors": [
      "Zirui Liu",
      "Guanchu Wang",
      "Shaochen (Henry) Zhong",
      "Zhaozhuo Xu",
      "Daochen Zha",
      "Ruixiang (Ryan) Tang",
      "Zhimeng (Stephen) Jiang",
      "Kaixiong Zhou",
      "Vipin Chaudhary",
      "Shuai Xu",
      "Xia Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0a70c9cd8179fe6f8f6135fafa2a8798-Abstract-Conference.html": {
    "title": "Zeroth-Order Methods for Nondifferentiable, Nonconvex, and Hierarchical Federated Optimization",
    "volume": "main",
    "abstract": "Federated learning (FL) has emerged as an enabling framework for communication-efficient decentralized training. We study three broadly applicable problem classes in FL: (i) Nondifferentiable nonconvex optimization; (ii) Federated bilevel optimization; (iii) Federated minimax problems. Notably, in an implicit sense, both (ii) and (iii) are instances of (i). However, these hierarchical problems are often complicated by the absence of a closed-form expression for the implicit objective function. Unfortunately, research on these problems has been limited and afflicted by reliance on strong assumptions, including the need for differentiability and L-smoothness of the implicit function. We address this shortcoming by making the following contributions. In (i), by leveraging convolution-based smoothing and Clarke's subdifferential calculus, we devise a randomized smoothing-enabled zeroth-order FL method and derive communication and iteration complexity guarantees for computing an approximate Clarke stationary point. To contend with (ii) and (iii), we devise a unifying randomized implicit zeroth-order FL framework, equipped with explicit communication and iteration complexities. Importantly, our method utilizes delays during local steps to skip calling the inexact lower-level FL oracle. This results in significant reduction in communication overhead when addressing hierarchical problems. We empirically validate the theory on nonsmooth and hierarchical ML problems",
    "keywords": [],
    "checked": true,
    "id": "a92567fea82cae650bf21fc9986641b94244947a",
    "semantic_title": "zeroth-order methods for nondifferentiable, nonconvex, and hierarchical federated optimization",
    "citation_count": 0,
    "authors": [
      "Yuyang Qiu",
      "Uday Shanbhag",
      "Farzad Yousefian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0a980183c520446f6b8afb6fa2a2c70e-Abstract-Conference.html": {
    "title": "Language Model Alignment with Elastic Reset",
    "volume": "main",
    "abstract": "Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. But optimizing against a reward model can improve on reward while degrading performance in other areas, a phenomenon known as reward hacking, alignment tax, or language drift. First, we argue that commonly-used test metrics are insufficient and instead measure how different algorithms tradeoff between reward and drift. The standard method modified the reward with a Kullback-Lieber (KL) penalty between the online and initial model. We propose Elastic Reset, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model. Through the use of an EMA, our model recovers quickly after resets and achieves higher reward with less drift in the same number of steps. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant and more aligned technical QA chatbot with LLaMA-7B. Code available https://github.com/mnoukhov/elastic-reset",
    "keywords": [],
    "checked": true,
    "id": "15b8b6a8028b2b6e75b67dfb6aebaede36826cf8",
    "semantic_title": "language model alignment with elastic reset",
    "citation_count": 2,
    "authors": [
      "Michael Noukhovitch",
      "Samuel Lavoie",
      "Florian Strub",
      "Aaron C. Courville"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0aa800df4298539770b57824afc77a89-Abstract-Conference.html": {
    "title": "Resolving the Tug-of-War: A Separation of Communication and Learning in Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) is a promising privacy-preserving machine learning paradigm over distributed data. In this paradigm, each client trains the parameter of a model locally and the server aggregates the parameter from clients periodically. Therefore, we perform the learning and communication over the same set of parameters. However, we find that learning and communication have fundamentally divergent requirements for parameter selection, akin to two opposite teams in a tug-of-war game. To mitigate this discrepancy, we introduce FedSep, a novel two-layer federated learning framework. FedSep consists of separated communication and learning layers for each client and the two layers are connected through decode/encode operations. In particular, the decoding operation is formulated as a minimization problem. We view FedSep as a federated bilevel optimization problem and propose an efficient algorithm to solve it. Theoretically, we demonstrate that its convergence matches that of the standard FL algorithms. The separation of communication and learning in FedSep offers innovative solutions to various challenging problems in FL, such as Communication-Efficient FL and Heterogeneous-Model FL. Empirical validation shows the superior performance of FedSep over various baselines in these tasks",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Li",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ab51646ca369140c3c3ece011b66587-Abstract-Conference.html": {
    "title": "GlucoSynth: Generating Differentially-Private Synthetic Glucose Traces",
    "volume": "main",
    "abstract": "We focus on the problem of generating high-quality, private synthetic glucose traces, a task generalizable to many other time series sources. Existing methods for time series data synthesis, such as those using Generative Adversarial Networks (GANs), are not able to capture the innate characteristics of glucose data and cannot provide any formal privacy guarantees without severely degrading the utility of the synthetic data. In this paper we present GlucoSynth, a novel privacy-preserving GAN framework to generate synthetic glucose traces. The core intuition behind our approach is to conserve relationships amongst motifs (glucose events) within the traces, in addition to temporal dynamics. Our framework incorporates differential privacy mechanisms to provide strong formal privacy guarantees. We provide a comprehensive evaluation on the real-world utility of the data using 1.2 million glucose traces; GlucoSynth outperforms all previous methods in its ability to generate high-quality synthetic glucose traces with strong privacy guarantees",
    "keywords": [],
    "checked": true,
    "id": "3ed9c6c7f5ee8bc8386a59f034321ed6974020b1",
    "semantic_title": "glucosynth: generating differentially-private synthetic glucose traces",
    "citation_count": 0,
    "authors": [
      "Josephine Lamp",
      "Mark Derdzinski",
      "Christopher Hannemann",
      "Joost van der Linden",
      "Lu Feng",
      "Tianhao Wang",
      "David Evans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0b0153a91f827b14e8bfea4e211362f3-Abstract-Conference.html": {
    "title": "OBJECT 3DIT: Language-guided 3D-aware Image Editing",
    "volume": "main",
    "abstract": "Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is projected. As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the foundation of the image formation process; such edits break the portrayal of a coherent 3D world. 3D-aware generative models are a promising solution, but currently only succeed on small datasets or at the level of a single object. In this work, we formulate the new task of language-guided 3D-aware editing, where objects in an image should be edited according to a language instruction while remaining consistent with the underlying 3D scene. To promote progress towards this goal, we release OBJect: a benchmark dataset of 400K editing examples created from procedurally generated 3D scenes. Each example consists of an input image, editing instruction in language, and the edited image. We also introduce 3DIT: single and multi-task models for four editing tasks. Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects, surfaces, lighting conditions, shadows, and physically-plausible object configurations. Surprisingly, training on only synthetic scenes from \\dataset, editing capabilities of 3DIT generalize to real-world images",
    "keywords": [],
    "checked": true,
    "id": "f3ba7153e16a7a56bd0861f58a6bee0244bf5dab",
    "semantic_title": "object 3dit: language-guided 3d-aware image editing",
    "citation_count": 4,
    "authors": [
      "Oscar Michel",
      "Anand Bhattad",
      "Eli VanderBilt",
      "Ranjay Krishna",
      "Aniruddha Kembhavi",
      "Tanmay Gupta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0b06c8673ebb453e5e468f7743d8f54e-Abstract-Conference.html": {
    "title": "Learning Rule-Induced Subgraph Representations for Inductive Relation Prediction",
    "volume": "main",
    "abstract": "Inductive relation prediction (IRP)---where entities can be different during training and inference---has shown great power for completing evolving knowledge graphs. Existing works mainly focus on using graph neural networks (GNNs) to learn the representation of the subgraph induced from the target link, which can be seen as an implicit rule-mining process to measure the plausibility of the target link. However, these methods are not able to differentiate the target link and other links during message passing, hence the final subgraph representation will contain irrelevant rule information to the target link, which reduces the reasoning performance and severely hinders the applications for real-world scenarios. To tackle this problem, we propose a novel $\\textit{single-source edge-wise}$ GNN model to learn the $\\textbf{R}$ule-induc$\\textbf{E}$d $\\textbf{S}$ubgraph represen$\\textbf{T}$ations $(\\textbf{REST}$), which encodes relevant rules and eliminates irrelevant rules within the subgraph. Specifically, we propose a $\\textit{single-source}$ initialization approach to initialize edge features only for the target link, which guarantees the relevance of mined rules and target link. Then we propose several RNN-based functions for $\\textit{edge-wise}$ message passing to model the sequential property of mined rules. REST is a simple and effective approach with theoretical support to learn the $\\textit{rule-induced subgraph representation}$. Moreover, REST does not need node labeling, which significantly accelerates the subgraph preprocessing time by up to $\\textbf{11.66}\\times$. Experiments on inductive relation prediction benchmarks demonstrate the effectiveness of our REST",
    "keywords": [],
    "checked": false,
    "id": "8884aaa287595ee6fdfff66d2c36594c3f3b7516",
    "semantic_title": "subgraph representation learning with hard negative samples for inductive link prediction",
    "citation_count": 3,
    "authors": [
      "Tianyu Liu",
      "Qitan Lv",
      "Jie Wang",
      "Shuling Yang",
      "Hanzhu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0b08d733a5d45a547344c4e9d88bb8bc-Abstract-Conference.html": {
    "title": "Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment",
    "volume": "main",
    "abstract": "Text-conditioned image generation models often generate incorrect associations between entities and their visual attributes. This reflects an impaired mapping between linguistic binding of entities and modifiers in the prompt and visual binding of the corresponding elements in the generated image. As one example, a query like ``a pink sunflower and a yellow flamingo'' may incorrectly produce an image of a yellow sunflower and a pink flamingo. To remedy this issue, we propose SynGen, an approach which first syntactically analyses the prompt to identify entities and their modifiers, and then uses a novel loss function that encourages the cross-attention maps to agree with the linguistic binding reflected by the syntax. Specifically, we encourage large overlap between attention maps of entities and their modifiers, and small overlap with other entities and modifier words. The loss is optimized during inference, without retraining or fine-tuning the model. Human evaluation on three datasets, including one new and challenging set, demonstrate significant improvements of SynGen compared with current state of the art methods. This work highlights how making use of sentence structure during inference can efficiently and substantially improve the faithfulness of text-to-image generation",
    "keywords": [],
    "checked": true,
    "id": "9e8648550fbec45dc712eb084251807b6c44e1e4",
    "semantic_title": "linguistic binding in diffusion models: enhancing attribute correspondence through attention map alignment",
    "citation_count": 14,
    "authors": [
      "Royi Rassin",
      "Eran Hirsch",
      "Daniel Glickman",
      "Shauli Ravfogel",
      "Yoav Goldberg",
      "Gal Chechik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0b13c22ca208bc08f3fd13793292f25f-Abstract-Conference.html": {
    "title": "Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL",
    "volume": "main",
    "abstract": "While policy optimization algorithms have played an important role in recent empirical success of Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains rather limited---they are either restricted to tabular MDPs or suffer from highly suboptimal sample complexity, especial in online RL where exploration is necessary. This paper proposes a simple efficient policy optimization framework---Optimistic NPG for online RL. Optimistic NPG can be viewed as simply combining of the classic natural policy gradient (NPG) algorithm [Kakade, 2001] with optimistic policy evaluation subroutines to encourage exploration. For $d$-dimensional linear MDPs, Optimistic NPG is computationally efficient, and learns an $\\epsilon$-optimal policy within $\\tilde{\\mathcal{O}}(d^2/\\epsilon^3)$ samples, which is the first computationally efficient algorithm whose sample complexity has the optimal dimension dependence $\\tilde{\\Theta}(d^2)$. It also improves over state-of-the-art results of policy optimization algorithms [Zanette et al., 2021] by a factor of $d$. For general function approximation that subsumes linear MDPs, Optimistic NPG, to our best knowledge, is also the first policy optimization algorithm that achieves the polynomial sample complexity for learning near-optimal policies",
    "keywords": [],
    "checked": true,
    "id": "b05e44becc97d6d78db9411242f1f34463923ceb",
    "semantic_title": "optimistic natural policy gradient: a simple efficient policy optimization framework for online rl",
    "citation_count": 3,
    "authors": [
      "Qinghua Liu",
      "Gellert Weisz",
      "András György",
      "Chi Jin",
      "Csaba Szepesvari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0b17d256cf1fe1cc084922a8c6b565b7-Abstract-Conference.html": {
    "title": "Two-Stage Learning to Defer with Multiple Experts",
    "volume": "main",
    "abstract": "We study a two-stage scenario for learning to defer with multiple experts, which is crucial in practice for many applications. In this scenario, a predictor is derived in a first stage by training with a common loss function such as cross-entropy. In the second stage, a deferral function is learned to assign the most suitable expert to each input. We design a new family of surrogate loss functions for this scenario both in the score-based and the predictor-rejector settings and prove that they are supported by $H$-consistency bounds, which implies their Bayes-consistency. Moreover, we show that, for a constant cost function, our two-stage surrogate losses are realizable $H$-consistent. While the main focus of this work is a theoretical analysis, we also report the results of several experiments on CIFAR-10 and SVHN datasets",
    "keywords": [],
    "checked": true,
    "id": "a8ce83368dafc5494a8a5294eaf67f12940ace51",
    "semantic_title": "two-stage learning to defer with multiple experts",
    "citation_count": 5,
    "authors": [
      "Anqi Mao",
      "Christopher Mohri",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0b43289db08ed60edc6451cb2132e203-Abstract-Conference.html": {
    "title": "A Computationally Efficient Sparsified Online Newton Method",
    "volume": "main",
    "abstract": "Second-order methods hold significant promise for enhancing the convergence of deep neural network training; however, their large memory and computational demands have limited their practicality. Thus there is a need for scalable second-order methods that can efficiently train large models. In this paper, we introduce the Sparsified Online Newton~(SONew) method, a memory-efficient second-order algorithm that yields a sparsified yet effective preconditioner. The algorithm emerges from a novel use of the LogDet matrix divergence measure; we combine it with sparsity constraints to minimize regret in the online convex optimization framework. Empirically, we test our method on large scale benchmarks of up to 1B parameters. We achieve up to $30\\\\%$ faster convergence, $3.4\\\\%$ relative improvement in validation performance, and $80\\\\%$ relative improvement in training loss, in comparison to memory efficient optimizers including first order methods. Powering the method is a surprising fact -- imposing structured sparsity patterns, like tridiagonal and banded structure, requires little to no overhead, making it as efficient and parallelizable as first-order methods. In wall-clock time, tridiagonal SONew is only about $3\\\\%$ slower per step than first-order methods but gives overall gains due to much faster convergence. In contrast, one of the state-of-the-art (SOTA) memory-intensive second-order methods, Shampoo, is unable to scale to large benchmarks. Additionally, while Shampoo necessitates significant engineering efforts to scale to large benchmarks, SONew offers a more straightforward implementation, increasing its practical appeal. SONew code is available at: https://github.com/devvrit/SONew",
    "keywords": [],
    "checked": true,
    "id": "d1041b5cfb05bfd404025804199517f37b5765a8",
    "semantic_title": "a computationally efficient sparsified online newton method",
    "citation_count": 0,
    "authors": [
      "Fnu Devvrit",
      "Sai Surya Duvvuri",
      "Rohan Anil",
      "Vineet Gupta",
      "Cho-Jui Hsieh",
      "Inderjit Dhillon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0b443d358a391166d1fbf551fb53de02-Abstract-Conference.html": {
    "title": "SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNNs) are biologically-inspired models that are capable of processing information in streams of action potentials. However, simulating and training SNNs is computationally expensive due to the need to solve large systems of coupled differential equations. In this paper, we propose a novel event-based algorithm called SparseProp for simulating and training sparse SNNs. Our algorithm reduces the computational cost of both forward pass and backward pass operations from O(N) to O(log(N)) per network spike, enabling numerically exact simulations of large spiking networks and their efficient training using backpropagation through time. By exploiting the sparsity of the network, SparseProp avoids iterating through all neurons at every spike and uses efficient state updates. We demonstrate the effectiveness of SparseProp for several classical integrate-and-fire neuron models, including simulating a sparse SNN with one million LIF neurons, which is sped up by more than four orders of magnitude compared to previous implementations. Our work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models",
    "keywords": [],
    "checked": true,
    "id": "763822c53ae52b25ff261f3338f9800830082dc2",
    "semantic_title": "sparseprop: efficient event-based simulation and training of sparse recurrent spiking neural networks",
    "citation_count": 0,
    "authors": [
      "Rainer Engelken"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0b68d474baf8dff30f3280c199a32089-Abstract-Conference.html": {
    "title": "ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image",
    "volume": "main",
    "abstract": "We present a novel method for reconstructing 3D objects from a single RGB image. Our method leverages the latest image generation models to infer the hidden 3D structure while remaining faithful to the input image. While existing methods obtain impressive results in generating 3D models from text prompts, they do not provide an easy approach for conditioning on input RGB data. Naive extensions of these methods often lead to improper alignment in appearance between the input image and the 3D reconstructions. We address these challenges by introducing Image Constrained Radiance Fields (ConRad), a novel variant of neural radiance fields. ConRad is an efficient 3D representation that explicitly captures the appearance of an input image in one viewpoint. We propose a training algorithm that leverages the single RGB image in conjunction with pretrained Diffusion Models to optimize the parameters of a ConRad representation. Extensive experiments show that ConRad representations can simplify preservation of image details while producing a realistic 3D reconstruction. Compared to existing state-of-the-art baselines, we show that our 3D reconstructions remain more faithful to the input and produce more consistent 3D models while demonstrating significantly improved quantitative performance on a ShapeNet object benchmark",
    "keywords": [],
    "checked": true,
    "id": "f7b841060aac8b9f1c39597686325103a2266921",
    "semantic_title": "conrad: image constrained radiance fields for 3d generation from a single image",
    "citation_count": 4,
    "authors": [
      "Senthil Purushwalkam",
      "Nikhil Naik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0b8e4c8468273ee3bafb288229c0acbc-Abstract-Conference.html": {
    "title": "Fair Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "This paper investigates fairness and bias in Canonical Correlation Analysis (CCA), a widely used statistical technique for examining the relationship between two sets of variables. We present a framework that alleviates unfairness by minimizing the correlation disparity error associated with protected attributes. Our approach enables the CCA model to learn global projection matrices from all data points while ensuring that these matrices yield comparable correlation levels to group-specific projection matrices. Experimental evaluation on both synthetic and real-world datasets demonstrates the efficacy of our method in reducing unfairness without compromising CCA model accuracy. These findings emphasize the importance of considering fairness in CCA applications to real-world problems",
    "keywords": [],
    "checked": true,
    "id": "bd807a83f27d6e280ce794db32bc79612cd3dc64",
    "semantic_title": "fair canonical correlation analysis",
    "citation_count": 0,
    "authors": [
      "Zhuoping Zhou",
      "Davoud Ataee Tarzanagh",
      "Bojian Hou",
      "Boning Tong",
      "Jia Xu",
      "Yanbo Feng",
      "Qi Long",
      "Li Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ba520d93c3df592c83a611961314c98-Abstract-Conference.html": {
    "title": "DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization",
    "volume": "main",
    "abstract": "Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. It formulates NPC problems into a discrete {0, 1}-vector space and uses graph-based denoising diffusion models to generate high-quality solutions. Specifically, we explore diffusion models with Gaussian and Bernoulli noise, respectively, and also introduce an effective inference schedule to improve the generation quality. We evaluate our methods on two well-studied combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP-10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark. Our code is available at this url",
    "keywords": [],
    "checked": true,
    "id": "5f90d43e6ece5c6ee6e8186e4b57d46c85377713",
    "semantic_title": "difusco: graph-based diffusion solvers for combinatorial optimization",
    "citation_count": 17,
    "authors": [
      "Zhiqing Sun",
      "Yiming Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0bc795afae289ed465a65a3b4b1f4eb7-Abstract-Conference.html": {
    "title": "Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models",
    "volume": "main",
    "abstract": "We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them.Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations.Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3.We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at https://github.com/layer6ai-labs/dgm-eval",
    "keywords": [],
    "checked": true,
    "id": "caf6ca4ccfabf903003cdf927fb7e883342fdcad",
    "semantic_title": "exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models",
    "citation_count": 13,
    "authors": [
      "George Stein",
      "Jesse Cresswell",
      "Rasa Hosseinzadeh",
      "Yi Sui",
      "Brendan Ross",
      "Valentin Villecroze",
      "Zhaoyan Liu",
      "Anthony L. Caterini",
      "Eric Taylor",
      "Gabriel Loaiza-Ganem"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0bcd8d153b8c548629eca53f4ebdeb42-Abstract-Conference.html": {
    "title": "Online Clustering of Bandits with Misspecified User Models",
    "volume": "main",
    "abstract": "The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, respectively), that can accommodate the inaccurate user preference estimations and erroneous clustering caused by model misspecifications. We prove regret upper bounds of $O(\\epsilon_*T\\sqrt{md\\log T} + d\\sqrt{mT}\\log T)$ for our algorithms under milder assumptions than previous CB works, which match the lower bound asymptotically in $T$ up to logarithmic factors, and also match the state-of-the-art results in several degenerate cases. Our regret analysis is novel and different from the typical proof flow of previous CB works. The techniques in proving the regret caused by misclustering users are quite general and may be of independent interest. Experiments on both synthetic and real-world data show our outperformance over previous algorithms",
    "keywords": [],
    "checked": true,
    "id": "ccd9f022813ae1547dffa2cec78a4096741e2045",
    "semantic_title": "online clustering of bandits with misspecified user models",
    "citation_count": 1,
    "authors": [
      "Zhiyong Wang",
      "Jize Xie",
      "Xutong Liu",
      "Shuai Li",
      "John C.S. Lui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0bcf9cf6ffe26bba3af99e18be0e1d8d-Abstract-Conference.html": {
    "title": "Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes",
    "volume": "main",
    "abstract": "Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing paradigm. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce more realistic spike activities and accurately fit spike statistics than powerful alternatives. Additionally, learned TeCoS-LVM models can generalize well to longer time scales. Overall, while remaining computationally tractable, our model effectively captures key features of neural coding systems. It thus provides a useful tool for building accurate predictive computational accounts for various sensory perception circuits",
    "keywords": [],
    "checked": true,
    "id": "e686191dc766d473d3c981dca9715440888303f5",
    "semantic_title": "temporal conditioning spiking latent variable models of the neural response to natural visual scenes",
    "citation_count": 0,
    "authors": [
      "Gehua Ma",
      "Runhao Jiang",
      "Rui Yan",
      "Huajin Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0bcfb525c8f8f07ae10a93d0b2a40e00-Abstract-Conference.html": {
    "title": "Double Auctions with Two-sided Bandit Feedback",
    "volume": "main",
    "abstract": "Double Auction enables decentralized transfer of goods between multiple buyers and sellers, thus underpinning functioning of many online marketplaces. Buyers and sellers compete in these markets through bidding, but do not often know their own valuation a-priori. As the allocation and pricing happens through bids, the profitability of participants, hence sustainability of such markets, depends crucially on learning respective valuations through repeated interactions. We initiate the study of Double Auction markets under bandit feedback on both buyers' and sellers' side. We show with confidence bound based bidding, and `Average Pricing' there is an efficient price discovery among the participants. In particular, the regret on combined valuation of the buyers and the sellers -- a.k.a. the social regret -- is $O(\\log(T)/\\Delta)$ in $T$ rounds, where $\\Delta$ is the minimum price gap. Moreover, the buyers and sellers exchanging goods attain $O(\\sqrt{T})$ regret, individually. The buyers and sellers who do not benefit from exchange in turn only experience $O(\\log{T}/ \\Delta)$ regret individually in $T$ rounds. We augment our upper bound by showing that $\\omega(\\sqrt{T})$ individual regret, and $\\omega(\\log{T})$ social regret is unattainable in certain Double Auction markets. Our paper is the first to provide decentralized learning algorithms in a two-sided market where \\emph{both sides have uncertain preference} that need to be learned",
    "keywords": [],
    "checked": true,
    "id": "849b5efd0df5e6fffc7f256951cda893d24c23ea",
    "semantic_title": "double auctions with two-sided bandit feedback",
    "citation_count": 0,
    "authors": [
      "Soumya Basu",
      "Abishek Sankararaman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0c1e94af650f5c74b1f3da467c2308c2-Abstract-Conference.html": {
    "title": "Enhancing Robot Program Synthesis Through Environmental Context",
    "volume": "main",
    "abstract": "Program synthesis aims to automatically generate an executable program that conforms to the given specification. Recent advancements have demonstrated that deep neural methodologies and large-scale pretrained language models are highly proficient in capturing program semantics.For robot programming, prior works have facilitated program synthesis by incorporating global environments. However, the assumption of acquiring a comprehensive understanding of the entire environment is often excessively challenging to achieve.In this work, we present a framework that learns to synthesize a program by rectifying potentially erroneous code segments, with the aid of partially observed environments. To tackle the issue of inadequate attention to partial observations, we propose to first learn an environment embedding space that can implicitly evaluate the impacts of each program token based on the precondition. Furthermore, by employing a graph structure, the model can aggregate both environmental and syntactic information flow and furnish smooth program rectification guidance.Extensive experimental evaluations and ablation studies on the partially observed VizDoom domain authenticate that our method offers superior generalization capability across various tasks and greater robustness when encountering noises",
    "keywords": [],
    "checked": true,
    "id": "08d9019e3084fb4d5d21b342485a1ae4c1dd8129",
    "semantic_title": "enhancing robot program synthesis through environmental context",
    "citation_count": 0,
    "authors": [
      "Tianyi Chen",
      "Qidi Wang",
      "Zhen Dong",
      "Liwei Shen",
      "Xin Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0c4dd7e3d9f528f0b4f2aca9fbcdca8d-Abstract-Conference.html": {
    "title": "Understanding Deep Gradient Leakage via Inversion Influence Functions",
    "volume": "main",
    "abstract": "Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, attack implementations, and noise-based defenses. With this novel tool, we provide insights into effective gradient perturbation directions, the unfairness of privacy protection, and privacy-preferred model initialization. Our codes are provided in https://github.com/illidanlab/inversion-influence-function",
    "keywords": [],
    "checked": true,
    "id": "70050b54438794a77ced84dd8e28a06a44f6fb2a",
    "semantic_title": "understanding deep gradient leakage via inversion influence functions",
    "citation_count": 0,
    "authors": [
      "Haobo Zhang",
      "Junyuan Hong",
      "Yuyang Deng",
      "Mehrdad Mahdavi",
      "Jiayu Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0c6c92a0c5237761168eafd4549f1584-Abstract-Conference.html": {
    "title": "Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization",
    "keywords": [],
    "checked": true,
    "id": "46423fd6c5d0e53c436cc8c1848c815e779937f6",
    "semantic_title": "joint learning of label and environment causal independence for graph out-of-distribution generalization",
    "citation_count": 5,
    "authors": [
      "Shurui Gui",
      "Meng Liu",
      "Xiner Li",
      "Youzhi Luo",
      "Shuiwang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0c79d6ed1788653643a1ac67b6ea32a7-Abstract-Conference.html": {
    "title": "Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space",
    "volume": "main",
    "abstract": "Models of many real-life applications, such as queueing models of communication networks or computing systems, have a countably infinite state-space. Algorithmic and learning procedures that have been developed to produce optimal policies mainly focus on finite state settings, and do not directly apply to these models. To overcome this lacuna, in this work we study the problem of optimal control of a family of discrete-time countable state-space Markov Decision Processes (MDPs) governed by an unknown parameter $\\theta\\in\\Theta$, and defined on a countably-infinite state-space $\\mathcal X=\\mathbb{Z}_+^d$, with finite action space $\\mathcal A$, and an unbounded cost function. We take a Bayesian perspective with the random unknown parameter $\\boldsymbol{\\theta}^*$ generated via a given fixed prior distribution on $\\Theta$. To optimally control the unknown MDP, we propose an algorithm based on Thompson sampling with dynamically-sized episodes: at the beginning of each episode, the posterior distribution formed via Bayes' rule is used to produce a parameter estimate, which then decides the policy applied during the episode. To ensure the stability of the Markov chain obtained by following the policy chosen for each parameter, we impose ergodicity assumptions. From this condition and using the solution of the average cost Bellman equation, we establish an $\\tilde O(dh^d\\sqrt{|\\mathcal A|T})$ upper bound on the Bayesian regret of our algorithm, where $T$ is the time-horizon. Finally, to elucidate the applicability of our algorithm, we consider two different queueing models with unknown dynamics, and show that our algorithm can be applied to develop approximately optimal control algorithms",
    "keywords": [],
    "checked": true,
    "id": "c6f1a8b5df9572ac43d0b70247918bd7bfa6a495",
    "semantic_title": "bayesian learning of optimal policies in markov decision processes with countably infinite state-space",
    "citation_count": 2,
    "authors": [
      "Saghar Adler",
      "Vijay Subramanian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0c7ca207a051228f978971447a56464a-Abstract-Conference.html": {
    "title": "CARE: Modeling Interacting Dynamics Under Temporal Environmental Variation",
    "volume": "main",
    "abstract": "Modeling interacting dynamical systems, such as fluid dynamics and intermolecular interactions, is a fundamental research problem for understanding and simulating complex real-world systems. Many of these systems can be naturally represented by dynamic graphs, and graph neural network-based approaches have been proposed and shown promising performance. However, most of these approaches assume the underlying dynamics does not change over time, which is unfortunately untrue. For example, a molecular dynamics can be affected by the environment temperature over the time. In this paper, we take an attempt to provide a probabilistic view for time-varying dynamics and propose a model Context-attended Graph ODE (CARE) for modeling time-varying interacting dynamical systems. In our CARE, we explicitly use a context variable to model time-varying environment and construct an encoder to initialize the context variable from historical trajectories. Furthermore, we employ a neural ODE model to depict the dynamic evolution of the context variable inferred from system states. This context variable is incorporated into a coupled ODE to simultaneously drive the evolution of systems. Comprehensive experiments on four datasets demonstrate the effectiveness of our proposed CARE compared with several state-of-the-art approaches",
    "keywords": [],
    "checked": false,
    "id": "bda43615d49ee935e3dd56bb99eef7a0bff76c6c",
    "semantic_title": "evolutionary rescue under demographic and environmental stochasticity",
    "citation_count": 0,
    "authors": [
      "Xiao Luo",
      "Haixin Wang",
      "Zijie Huang",
      "Huiyu Jiang",
      "Abhijeet Gangan",
      "Song Jiang",
      "Yizhou Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0c86142265c5e2c900613dd1d031cb90-Abstract-Conference.html": {
    "title": "Diffused Redundancy in Pre-trained Representations",
    "volume": "main",
    "abstract": "Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\\%$ of randomly picked neurons from the penultimate layer of a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstream tasks taken from the VTAB benchmark. We find that the loss \\& dataset used during pre-training largely govern the degree of diffuse redundancy and the \"critical mass\" of neurons needed often depends on the downstream task, suggesting that there is a task-inherent redundancy-performance Pareto frontier. Our findings shed light on the nature of representations learned by pre-trained deep neural networks and suggest that entire layers might not be necessary to perform many downstream tasks. We investigate the potential for exploiting this redundancy to achieve efficient generalization for downstream tasks and also draw caution to certain possible unintended consequences. Our code is available at \\url{https://github.com/nvedant07/diffused-redundancy}",
    "keywords": [],
    "checked": true,
    "id": "32658cbc2e3435b3df667cb494e527a5955d0cde",
    "semantic_title": "diffused redundancy in pre-trained representations",
    "citation_count": 1,
    "authors": [
      "Vedant Nanda",
      "Till Speicher",
      "John Dickerson",
      "Krishna Gummadi",
      "Soheil Feizi",
      "Adrian Weller"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ca70969597da7166128f7755c64ffd5-Abstract-Conference.html": {
    "title": "AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning",
    "volume": "main",
    "abstract": "Deep learning-based reaction predictors have undergone significant architectural evolution. However, their reliance on reactions from the US Patent Office results in a lack of interpretable predictions and limited generalizability to other chemistry domains, such as radical and atmospheric chemistry. To address these challenges, we introduce a new reaction predictor system, RMechRP, that leverages contrastive learning in conjunction with mechanistic pathways, the most interpretable representation of chemical reactions. Specifically designed for radical reactions, RMechRP provides different levels of interpretation of chemical reactions. We develop and train multiple deep-learning models using RMechDB, a public database of radical reactions, to establish the first benchmark for predicting radical reactions. Our results demonstrate the effectiveness of RMechRP in providing accurate and interpretable predictions of radical reactions, and its potential for various applications in atmospheric chemistry",
    "keywords": [],
    "checked": true,
    "id": "37de2fe0e44dc337332c92dd94b9e70b208126b2",
    "semantic_title": "ai for interpretable chemistry: predicting radical mechanistic pathways via contrastive learning",
    "citation_count": 0,
    "authors": [
      "Mohammadamin Tavakoli",
      "Pierre Baldi",
      "Ann Marie Carlton",
      "Yin Ting Chiu",
      "Alexander Shmakov",
      "David Van Vranken"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0cb310ed8121549488fea8e8c2056096-Abstract-Conference.html": {
    "title": "Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks",
    "volume": "main",
    "abstract": "Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties; however, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time. This work introduces Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. The randomization avoids overfitting locally in time and so helps prevent the error from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step. In numerical experiments with a wide range of evolution equations, the proposed scheme with randomized sparse updates is up to two orders of magnitude more accurate at a fixed computational budget and up to two orders of magnitude faster at a fixed accuracy than schemes with dense updates",
    "keywords": [],
    "checked": true,
    "id": "c05c72e0f180e7c13809138550205578eb084cfb",
    "semantic_title": "randomized sparse neural galerkin schemes for solving evolution equations with deep networks",
    "citation_count": 2,
    "authors": [
      "Jules Berman",
      "Benjamin Peherstorfer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ccd06ff26fd6a7829293ce90e0e7f7d-Abstract-Conference.html": {
    "title": "Handling Data Heterogeneity via Architectural Design for Federated Visual Recognition",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a promising research paradigm that enables the collaborative training of machine learning models among various parties without the need for sensitive information exchange. Nonetheless, retaining data in individual clients introduces fundamental challenges to achieving performance on par with centrally trained models. Our study provides an extensive review of federated learning applied to visual recognition. It underscores the critical role of thoughtful architectural design choices in achieving optimal performance, a factor often neglected in the FL literature. Many existing FL solutions are tested on shallow or simple networks, which may not accurately reflect real-world applications. This practice restricts the transferability of research findings to large-scale visual recognition models. Through an in-depth analysis of diverse cutting-edge architectures such as convolutional neural networks, transformers, and MLP-mixers, we experimentally demonstrate that architectural choices can substantially enhance FL systems' performance, particularly when handling heterogeneous data. We study visual recognition models from five different architectural families on four challenging FL datasets. We also re-investigate the inferior performance convolution-based architectures in the FL setting and analyze the influence of normalization layers on the FL performance. Our findings emphasize the importance of architectural design for computer vision tasks in practical scenarios, effectively narrowing the performance gap between federated and centralized learning",
    "keywords": [],
    "checked": true,
    "id": "d78642d28e439537e08f449754fa9a55d1efe56d",
    "semantic_title": "handling data heterogeneity via architectural design for federated visual recognition",
    "citation_count": 2,
    "authors": [
      "Sara Pieri",
      "Jose Restom",
      "Samuel Horváth",
      "Hisham Cholakkal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0cdc1e85736d9c01d366cbf9b4b81672-Abstract-Conference.html": {
    "title": "Spatial-frequency channels, shape bias, and adversarial robustness",
    "volume": "main",
    "abstract": "What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or \"channel\") that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. Unlike humans, the neural network channel is very broad, 2-4 times wider than the human channel. This means that the network channel extends to frequencies higher and lower than those that humans are sensitive to. Thus, noise at those frequencies will impair network performance and spare human performance. Adversarial and augmented-image training are commonly used to increase network robustness and shape bias. Does this training align network and human object recognition channels? Three network channel properties (bandwidth, center frequency, peak noise sensitivity) correlate strongly with shape bias (51% variance explained) and robustness of adversarially-trained networks (66% variance explained). Adversarial training increases robustness but expands the channel bandwidth even further beyond the human bandwidth. Thus, critical band masking reveals that the network channel is more than twice as wide as the human channel, and that adversarial training only makes it worse. Networks with narrower channels might be more robust",
    "keywords": [],
    "checked": true,
    "id": "83991e8da386c581396aed9134fe8a470cec3f8a",
    "semantic_title": "spatial-frequency channels, shape bias, and adversarial robustness",
    "citation_count": 3,
    "authors": [
      "Ajay Subramanian",
      "Elena Sizikova",
      "Najib Majaj",
      "Denis Pelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0cddb777d3441326544e21b67f41bdc8-Abstract-Conference.html": {
    "title": "Optimality in Mean Estimation: Beyond Worst-Case, Beyond Sub-Gaussian, and Beyond $1+\\alpha$ Moments",
    "volume": "main",
    "abstract": "There is growing interest in improving our algorithmic understanding of fundamental statistical problems such as mean estimation, driven by the goal of understanding the fundamental limits of what we can extract from limited and valuable data.The state of the art results for mean estimation in $\\mathbb{R}$ are 1) the optimal sub-Gaussian mean estimator by [Lee and Valiant, 2022], attaining the optimal sub-Gaussian error constant for all distributions with finite but unknown variance, and 2) the analysis of the median-of-means algorithm by [Bubeck, Cesa-Bianchi and Lugosi, 2013] and a matching lower bound by [Devroye, Lerasle, Lugosi, and Oliveira, 2016], characterizing the big-O optimal errors for distributions that have tails heavy enough that only a $1+\\alpha$ moment exists for some $\\alpha \\in (0,1)$.Both of these results, however, are optimal only in the worst case.Motivated by the recent effort in the community to go \"beyond the worst-case analysis\" of algorithms, we initiate the fine-grained study of the mean estimation problem:Is it possible for algorithms to leverage *beneficial* features/quirks of their input distribution to *beat* the sub-Gaussian rate, without explicit knowledge of these features?We resolve this question, finding an unexpectedly nuanced answer: \"Yes in limited regimes, but in general no\".Given a distribution $p$, assuming *only* that it has a finite mean and absent any additional assumptions,we show how to construct a distribution $q_{n,\\delta}$ such that the means of $p$ and $q$ are well-separated, yet $p$ and $q$ are impossible to distinguish with $n$ samples with probability $1-\\delta$, and $q$ further preserves the finiteness of moments of $p$.Moreover, the variance of $q$ is at most twice the variance of $p$ if it exists.The main consequence of our result is that, no reasonable estimator can asymptotically achieve better than the sub-Gaussian error rate for any distribution, up to constant factors, which matches the worst-case result of [Lee and Valiant, 2022].More generally, we introduce a new definitional framework to analyze the fine-grained optimality of algorithms, which we call \"neighborhood optimality\", interpolating between the unattainably strong \"instance optimality\" and the trivially weak admissibility/Pareto optimality definitions.As an application of the new framework, we show that the median-of-means algorithm is neighborhood optimal, up to constant factors.It is an open question to find a neighborhood-optimal estimator *without* constant factor slackness",
    "keywords": [],
    "checked": false,
    "id": "7837e5f0f1d663b48b448d131712738008519504",
    "semantic_title": "optimality in mean estimation: beyond worst-case, beyond sub-gaussian, and beyond 1+α moments",
    "citation_count": 1,
    "authors": [
      "Trung Dang",
      "Jasper Lee",
      "Maoyuan 'Raymond' Song",
      "Paul Valiant"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0cfc9404f89400c5ed897035e0d3748c-Abstract-Conference.html": {
    "title": "Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability",
    "volume": "main",
    "abstract": "Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $\\tilde{O}(\\text{poly}(1/\\epsilon))$ sample complexity (where $\\epsilon$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the objective functions. We only require nearly minimal assumptions on the dataset (single-policy concentrability) and the function class (realizability). Moreover, this algorithm consists of two uninterleaved optimization steps, which we refer to as $V$-learning and policy learning, and is computationally stable since it does not involve minimax optimization. We also empirically validate our theory by showing that the modified algorithm outperforms the previous algorithm in various real-world environments.To the best of our knowledge, this is the first algorithm that is both provably efficient with general function approximation and single-policy concentrability, and empirically successful without requiring solving minimax optimization problems",
    "keywords": [],
    "checked": true,
    "id": "cf8626c23d1d22405b9cc8061044ce7d8f8adf77",
    "semantic_title": "provably efficient offline goal-conditioned reinforcement learning with general function approximation and single-policy concentrability",
    "citation_count": 2,
    "authors": [
      "Hanlin Zhu",
      "Amy Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d00a699f60e642b310eb04b76cc7731-Abstract-Conference.html": {
    "title": "SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions",
    "volume": "main",
    "abstract": "We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model.Prior work developed a methodology to prove SQ lower bounds for NGCA that have been applicable to a wide range of contexts.In particular, it was known that for any univariate distribution $A$ satisfying certain conditions,distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard.The required conditions were that (1) $A$ matches many low-order moments with a standard Gaussian,and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite.While the moment-matching condition is clearly necessary for hardness, the chi-squared condition was only required for technical reasons.In this work, we establish that the latter condition is indeed not necessary.In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching condition only",
    "keywords": [],
    "checked": true,
    "id": "3e1cb24198b715eb1d942df92d67d759e4591bd2",
    "semantic_title": "sq lower bounds for non-gaussian component analysis with weaker assumptions",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Daniel Kane",
      "Lisheng Ren",
      "Yuxin Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d02892a0055c94584f6394f8d069c8e-Abstract-Conference.html": {
    "title": "Efficient Equivariant Transfer Learning from Pretrained Models",
    "volume": "main",
    "abstract": "Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of Basu et al. (2023) and Kaba et al. (2022) propose group averaging (equitune) and optimization-based methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While Kaba et al. (2022) are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose λ-equitune that averages the features using importance weights, λs. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned results that outperform equitune. Further, we prove that λ-equitune is equivariant and a universal approximator of equivariant functions. Additionally, we show that the method of Kaba et al. (2022) used with appropriate loss functions, which we call equizero, also gives excellent zero-shot and finetuned performance. Both equitune and equizero are special cases of λ- equitune. To show the simplicity and generality of our method, we validate on a wide range of diverse applications and models such as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in natural language generation (NLG), 4) compositional generalization in languages, and 5) image classification using pretrained CNNs such as Resnet and Alexnet",
    "keywords": [],
    "checked": true,
    "id": "fa69d1a623579e3591045e4615410176e0f55204",
    "semantic_title": "efficient equivariant transfer learning from pretrained models",
    "citation_count": 1,
    "authors": [
      "Sourya Basu",
      "Pulkit Katdare",
      "Prasanna Sattigeri",
      "Vijil Chenthamarakshan",
      "Katherine Driggs-Campbell",
      "Payel Das",
      "Lav R. Varshney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d17d033059bacd127f25ab28784f829-Abstract-Conference.html": {
    "title": "Kernelized Reinforcement Learning with Order Optimal Regret Bounds",
    "volume": "main",
    "abstract": "Modern reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\\pi$-KRVI, an optimistic modification of least-squares value iteration, when the action-value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Matérn kernels) the existing results lead to trivial (superlinear in the number of episodes) regret bounds. We show a sublinear regret bound that is order optimal in the cases where a lower bound on regret is known (which includes the kernels mentioned above)",
    "keywords": [],
    "checked": true,
    "id": "ed63e41f4ebfd1736e8c9e75c0c75150dc46d627",
    "semantic_title": "kernelized reinforcement learning with order optimal regret bounds",
    "citation_count": 1,
    "authors": [
      "Sattar Vakili",
      "Julia Olkhovskaya"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d18ab3b5fabfa6fe47c62e711af02f0-Abstract-Conference.html": {
    "title": "Learning Domain-Aware Detection Head with Prompt Tuning",
    "volume": "main",
    "abstract": "Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. However, existing methods focus on reducing the domain bias of the detection backbone by inferring a discriminative visual encoder, while ignoring the domain bias in the detection head. Inspired by the high generalization of vision-language models (VLMs), applying a VLM as the robust detection backbone following a domain-aware detection head is a reasonable way to learn the discriminative detector for each domain, rather than reducing the domain bias in traditional methods. To achieve the above issue, we thus propose a novel DAOD framework named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies the learnable domain-adaptive prompt to generate the dynamic detection head for each domain. Formally, the domain-adaptive prompt consists of the domain-invariant tokens, domain-specific tokens, and the domain-related textual description along with the class label. Furthermore, two constraints between the source and target domains are applied to ensure that the domain-adaptive prompt can capture the domains-shared and domain-specific knowledge. A prompt ensemble strategy is also proposed to reduce the effect of prompt disturbance. Comprehensive experiments over multiple cross-domain adaptation tasks demonstrate that using the domain-adaptive prompt can produce an effectively domain-related detection head for boosting domain-adaptive object detection. Our code is available at https://github.com/Therock90421/DA-Pro",
    "keywords": [],
    "checked": true,
    "id": "b7e42f7bc9c3ef6bf9315736fa149dff82fccf8f",
    "semantic_title": "learning domain-aware detection head with prompt tuning",
    "citation_count": 0,
    "authors": [
      "Haochen Li",
      "Rui Zhang",
      "Hantao Yao",
      "Xinkai Song",
      "Yifan Hao",
      "Yongwei Zhao",
      "Ling Li",
      "Yunji Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d1986a61e30e5fa408c81216a616e20-Abstract-Conference.html": {
    "title": "Parallel Sampling of Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we improve sampling speed by 2-4x across a range of robotics and image generation models, giving state-of-the-art sampling speeds of 0.2s on 100-step DiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable degradation of task reward, FID score, or CLIP score",
    "keywords": [],
    "checked": true,
    "id": "ba3e9e45f91257c9af6dfe74ed91a03c53086f07",
    "semantic_title": "parallel sampling of diffusion models",
    "citation_count": 7,
    "authors": [
      "Andy Shih",
      "Suneel Belkhale",
      "Stefano Ermon",
      "Dorsa Sadigh",
      "Nima Anari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d21f257b5288385cb6cb8e0ff2ce82e-Abstract-Conference.html": {
    "title": "Fractal Landscapes in Policy Optimization",
    "volume": "main",
    "abstract": "Policy gradient lies at the core of deep reinforcement learning (RL) in continuous domains. Despite much success, it is often observed in practice that RL training with policy gradient can fail for many reasons, even on standard control problems with known solutions. We propose a framework for understanding one inherent limitation of the policy gradient approach: the optimization landscape in the policy space can be extremely non-smooth or fractal for certain classes of MDPs, such that there does not exist gradient to be estimated in the first place. We draw on techniques from chaos theory and non-smooth analysis, and analyze the maximal Lyapunov exponents and H\\\"older exponents of the policy optimization objectives. Moreover, we develop a practical method that can estimate the local smoothness of objective function from samples to identify when the training process has encountered fractal landscapes. We show experiments to illustrate how some failure cases of policy optimization can be explained by such fractal landscapes",
    "keywords": [],
    "checked": true,
    "id": "cab5dac1ba43bc36b12a4f629ef998d3247534f8",
    "semantic_title": "fractal landscapes in policy optimization",
    "citation_count": 0,
    "authors": [
      "Tao Wang",
      "Sylvia Herbert",
      "Sicun Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d5b7fd8c669fac58d6702188ed63afa-Abstract-Conference.html": {
    "title": "Moral Responsibility for AI Systems",
    "volume": "main",
    "abstract": "As more and more decisions that have a significant ethical dimension are being outsourced to AI systems, it is important to have a definition of moral responsibility that can be applied to AI systems. Moral responsibility for an outcome of an agent who performs some action is commonly taken to involve both a causal condition and an epistemic condition: the action should cause the outcome, and the agent should have been aware - in some form or other - of the possible moral consequences of their action. This paper presents a formal definition of both conditions within the framework of causal models. I compare my approach to the existing approaches of Braham and van Hees (BvH) and of Halpern and Kleiman-Weiner (HK). I then generalize my definition into a degree of responsibility",
    "keywords": [],
    "checked": true,
    "id": "dbe09be3d8721bd0bd62150b3c8d9c6765f678d3",
    "semantic_title": "moral responsibility for ai systems",
    "citation_count": 0,
    "authors": [
      "Sander Beckers"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d6270381e018b3d83eb9be7d0b06036-Abstract-Conference.html": {
    "title": "Characterizing the Impacts of Semi-supervised Learning for Weak Supervision",
    "volume": "main",
    "abstract": "Labeling training data is a critical and expensive step in producing high accuracy ML models, whether training from scratch or fine-tuning. To make labeling more efficient, two major approaches are programmatic weak supervision (WS) and semi-supervised learning (SSL). More recent works have either explicitly or implicitly used techniques at their intersection, but in various complex and ad hoc ways. In this work, we define a simple, modular design space to study the use of SSL techniques for WS more systematically. Surprisingly, we find that fairly simple methods from our design space match the performance of more complex state-of-the-art methods, averaging a 3 p.p. increase in accuracy/F1-score across 8 standard WS benchmarks. Further, we provide practical guidance on when different components are worth their added complexity and training costs. Contrary to current understanding, we find using SSL is not necessary to obtain the best performance on most WS benchmarks but is more effective when: (1) end models are smaller, and (2) WS provides labels for only a small portion of training examples",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeffrey Li",
      "Jieyu Zhang",
      "Ludwig Schmidt",
      "Alexander J. Ratner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d9057d84a9fc37523bf826232ea6820-Abstract-Conference.html": {
    "title": "Logarithmic Bayes Regret Bounds",
    "volume": "main",
    "abstract": "We derive the first finite-time logarithmic Bayes regret upper bounds for Bayesian bandits. In Gaussian bandits, we obtain $O(c_\\Delta \\log n)$ and $O(c_h \\log^2 n)$ bounds for an upper confidence bound algorithm, where $c_h$ and $c_\\Delta$ are constants depending on the prior distribution and the gaps of random bandit instances sampled from it, respectively. The latter bound asymptotically matches the lower bound of Lai (1987). Our proofs are a major technical departure from prior works, while being simple and general. To show the generality of our techniques, we apply them to linear bandits. Our results provide insights on the value of prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve upon existing $\\tilde{O}(\\sqrt{n})$ bounds, which have become standard in the literature despite the existing lower bounds",
    "keywords": [],
    "checked": true,
    "id": "bee752f8f18ad764d84da3d841d77d42f465a458",
    "semantic_title": "logarithmic bayes regret bounds",
    "citation_count": 0,
    "authors": [
      "Alexia Atsidakou",
      "Branislav Kveton",
      "Sumeet Katariya",
      "Constantine Caramanis",
      "Sujay Sanghavi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0d9e08f247ca7fbbfd5e50b7ff9cf357-Abstract-Conference.html": {
    "title": "Frequency-Enhanced Data Augmentation for Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "Vision-and-Language Navigation (VLN) is a challenging task that requires an agent to navigate through complex environments based on natural language instructions. In contrast to conventional approaches, which primarily focus on the spatial domain exploration, we propose a paradigm shift toward the Fourier domain. This alternative perspective aims to enhance visual-textual matching, ultimately improving the agent's ability to understand and execute navigation tasks based on the given instructions. In this study, we first explore the significance of high-frequency information in VLN and provide evidence that it is instrumental in bolstering visual-textual matching processes. Building upon this insight, we further propose a sophisticated and versatile Frequency-enhanced Data Augmentation (FDA) technique to improve the VLN model's capability of capturing critical high-frequency information. Specifically, this approach requires the agent to navigate in environments where only a subset of high-frequency visual information corresponds with the provided textual instructions, ultimately fostering the agent's ability to selectively discern and capture pertinent high-frequency features according to the given instructions. Promising results on R2R, RxR, CVDN and REVERIE demonstrate that our FDA can be readily integrated with existing VLN approaches, improving performance without adding extra parameters, and keeping models simple and efficient. The code is available at https://github.com/hekj/FDA",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keji He",
      "Chenyang Si",
      "Zhihe Lu",
      "Yan Huang",
      "Liang Wang",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0df1738319f8c6e15b58cb16ea3cfa57-Abstract-Conference.html": {
    "title": "Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment",
    "volume": "main",
    "abstract": "Recent progress in scaling up large language models has shown impressive capabilities in performing few-shot learning across a wide range of natural language tasks. However, a key limitation is that these language models fundamentally lack grounding to visual perception - a crucial attribute needed to extend to real world tasks such as in visual-question answering and robotics. While prior works have largely connected image to text through pretraining or fine-tuning, learning such alignments are generally costly due to a combination of curating massive datasets and large computational burdens. In order to resolve these limitations, we propose a simple yet effective approach called Language-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to align text-image data in an unsupervised manner by leveraging pretrained language model denoisers (e.g., BERT). Our main idea is to encode images as sequences of text tokens by directly quantizing image embeddings using a pretrained language codebook. We then feed a masked version of the quantized embeddings into a BERT to reconstruct the original input. By doing so, LQAE learns to represent similar images with similar clusters of text tokens, thereby aligning these two modalities without the use of aligned text-image pairs. We show LQAE learns text-aligned image tokens that enable few-shot multi-modal learning with large language models, outperforming baseline methods in tasks such as image classification and VQA while requiring as few as 1-10 image-text pairs",
    "keywords": [],
    "checked": true,
    "id": "d3bc7ba19e274bb6fb5e055a3f1b62924c731432",
    "semantic_title": "language quantized autoencoders: towards unsupervised text-image alignment",
    "citation_count": 8,
    "authors": [
      "Hao Liu",
      "Wilson Yan",
      "Pieter Abbeel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0df38cd13520747e1e64e5b123a78ef8-Abstract-Conference.html": {
    "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees",
    "volume": "main",
    "abstract": "This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/jerry-chee/QuIP",
    "keywords": [],
    "checked": true,
    "id": "56b828717f32251a5e0f0be9c0113077f23c8429",
    "semantic_title": "quip: 2-bit quantization of large language models with guarantees",
    "citation_count": 25,
    "authors": [
      "Jerry Chee",
      "Yaohui Cai",
      "Volodymyr Kuleshov",
      "Christopher M. De Sa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0e0157ce5ea15831072be4744cbd5334-Abstract-Conference.html": {
    "title": "Exploiting Correlated Auxiliary Feedback in Parameterized Bandits",
    "volume": "main",
    "abstract": "We study a novel variant of the parameterized bandits problem in which the learner can observe additional auxiliary feedback that is correlated with the observed reward. The auxiliary feedback is readily available in many real-life applications, e.g., an online platform that wants to recommend the best-rated services to its users can observe the user's rating of service (rewards) and collect additional information like service delivery time (auxiliary feedback). In this paper, we first develop a method that exploits auxiliary feedback to build a reward estimator with tight confidence bounds, leading to a smaller regret. We then characterize the regret reduction in terms of the correlation coefficient between reward and its auxiliary feedback. Experimental results in different settings also verify the performance gain achieved by our proposed method",
    "keywords": [],
    "checked": true,
    "id": "14e348995c9557bb5bab7ee30a977e51e3099c36",
    "semantic_title": "exploiting correlated auxiliary feedback in parameterized bandits",
    "citation_count": 0,
    "authors": [
      "Arun Verma",
      "Zhongxiang Dai",
      "YAO SHU",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0e3af444e7d82d29871804de476d1fbe-Abstract-Conference.html": {
    "title": "Multi-modal Queried Object Detection in the Wild",
    "volume": "main",
    "abstract": "We introduce MQ-Det, an efficient architecture and pre-training strategy design to utilize both textual description with open-set generalization and visual exemplars with rich description granularity as category queries, namely, Multi-modal Queried object Detection, for real-world detection with both open-vocabulary categories and various granularity. MQ-Det incorporates vision queries into existing well-established language-queried-only detectors. A plug-and-play gated class-scalable perceiver module upon the frozen detector is proposed to augment category text with class-wise visual information. To address the learning inertia problem brought by the frozen detector, a vision conditioned masked language prediction strategy is proposed. MQ-Det's simple yet effective architecture and training strategy design is compatible with most language-queried object detectors, thus yielding versatile applications. Experimental results demonstrate that multi-modal queries largely boost open-world detection. For instance, MQ-Det significantly improves the state-of-the-art open-set detector GLIP by +7.8% AP on the LVIS benchmark via multi-modal queries without any downstream finetuning, and averagely +6.3% AP on 13 few-shot downstream tasks, with merely additional 3% modulating time required by GLIP. Code is available at https://github.com/YifanXu74/MQ-Det",
    "keywords": [],
    "checked": true,
    "id": "dd8f69a4c298d66df844c648a6fd001aef43938f",
    "semantic_title": "multi-modal queried object detection in the wild",
    "citation_count": 7,
    "authors": [
      "Yifan Xu",
      "Mengdan Zhang",
      "Chaoyou Fu",
      "Peixian Chen",
      "Xiaoshan Yang",
      "Ke Li",
      "Changsheng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0e441913d4fa486c3eec967d79750b13-Abstract-Conference.html": {
    "title": "$H$-Consistency Bounds: Characterization and Extensions",
    "volume": "main",
    "abstract": "A series of recent publications by Awasthi et al. have introduced the key notion of *$H$-consistency bounds* for surrogate loss functions. These are upper bounds on the zero-one estimation error of any predictor in a hypothesis set, expressed in terms of its surrogate loss estimation error. They are both non-asymptotic and hypothesis set-specific and thus stronger and more informative than Bayes-consistency. However, determining if they hold and deriving these bounds have required a specific proof and analysis for each surrogate loss. Can we derive more general tools and characterizations? This paper provides both a general characterization and an extension of $H$-consistency bounds for multi-class classification. We present new and tight $H$-consistency bounds for both the family of constrained losses and that of comp-sum losses, which covers the familiar cross-entropy, or logistic loss applied to the outputs of a neural network. We further extend our analysis beyond the completeness assumptions adopted in previous studies and cover more realistic bounded hypothesis sets. Our characterizations are based on error transformations, which are explicitly defined for each formulation. We illustrate the application of our general results through several special examples. A by-product of our analysis is the observation that a recently derived multi-class $H$-consistency bound for cross-entropy reduces to an excess bound and is not significant. Instead, we prove a much stronger and more significant guarantee",
    "keywords": [],
    "checked": false,
    "id": "6da5c2f32c2597ce66c66dc579a16b284737fc19",
    "semantic_title": "h -consistency bounds: characterization and extensions",
    "citation_count": 2,
    "authors": [
      "Anqi Mao",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0e5b96f97c1813bb75f6c28532c2ecc7-Abstract-Conference.html": {
    "title": "Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms",
    "volume": "main",
    "abstract": "Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective formulation by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL or a weighted loss that places higher emphasis on some tasks than the others. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling. We develop a comprehensive convergence analysis for the proposed methods with different loop sizes and regularization coefficients. We show that both SDMGrad and SDMGrad-OS achieve improved sample complexities to find an $\\epsilon$-accurate Pareto stationary point while achieving a small $\\epsilon$-level distance toward a conflict-avoidant (CA) direction. For a constant-level CA distance, their sample complexities match the best known $\\mathcal{O}(\\epsilon^{-2})$ without bounded function value assumption. Extensive experiments show that our methods achieve competitive or improved performance compared to existing gradient manipulation approaches in a series of tasks on multi-task supervised learning and reinforcement learning. Code is available at https://github.com/ml-opt-lab/sdmgrad",
    "keywords": [],
    "checked": true,
    "id": "60ad0e8febb6930f2c0b94955514ea0cebbf58b4",
    "semantic_title": "direction-oriented multi-objective learning: simple and provable stochastic algorithms",
    "citation_count": 1,
    "authors": [
      "Peiyao Xiao",
      "Hao Ban",
      "Kaiyi Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0e769ec2c2cd99b6ad69c9d75113e386-Abstract-Conference.html": {
    "title": "DreamWaltz: Make a Scene with Complex 3D Animatable Avatars",
    "volume": "main",
    "abstract": "We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It provides view-aligned supervision via 3D-aware skeleton conditioning which enables complex avatar generation without artifacts and multiple faces. For animation, our method learns an animatable 3D avatar representation from abundant image priors of diffusion model conditioned on various poses, which could animate complex non-rigged avatars given arbitrary poses without retraining. Extensive evaluations demonstrate that DreamWaltz is an effective and robust approach for creating 3D avatars that can take on complex shapes and appearances as well as novel poses for animation. The proposed framework further enables the creation of complex scenes with diverse compositions, including avatar-avatar, avatar-object and avatar-scene interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and animation results",
    "keywords": [],
    "checked": true,
    "id": "7316596b1f02f288e3b76546d90646524e35fd40",
    "semantic_title": "dreamwaltz: make a scene with complex 3d animatable avatars",
    "citation_count": 25,
    "authors": [
      "Yukun Huang",
      "Jianan Wang",
      "Ailing Zeng",
      "He CAO",
      "Xianbiao Qi",
      "Yukai Shi",
      "Zheng-Jun Zha",
      "Lei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0e7e2af2e5ba822c9ad35a37b31b5dd4-Abstract-Conference.html": {
    "title": "Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects",
    "volume": "main",
    "abstract": "Articulated object manipulation is a fundamental yet challenging task in robotics. Due to significant geometric and semantic variations across object categories, previous manipulation models struggle to generalize to novel categories. Few-shot learning is a promising solution for alleviating this issue by allowing robots to perform a few interactions with unseen objects. However, extant approaches often necessitate costly and inefficient test-time interactions with each unseen instance. Recognizing this limitation, we observe that despite their distinct shapes, different categories often share similar local geometries essential for manipulation, such as pullable handles and graspable edges - a factor typically underutilized in previous few-shot learning works. To harness this commonality, we introduce 'Where2Explore', an affordance learning framework that effectively explores novel categories with minimal interactions on a limited number of instances. Our framework explicitly estimates the geometric similarity across different categories, identifying local areas that differ from shapes in the training categories for efficient exploration while concurrently transferring affordance knowledge to similar parts of the objects. Extensive experiments in simulated and real-world environments demonstrate our framework's capacity for efficient few-shot exploration and generalization",
    "keywords": [],
    "checked": true,
    "id": "8240d0f26984b74e6c50fcdec3ff30829adb962f",
    "semantic_title": "where2explore: few-shot affordance learning for unseen novel categories of articulated objects",
    "citation_count": 4,
    "authors": [
      "Chuanruo Ning",
      "Ruihai Wu",
      "Haoran Lu",
      "Kaichun Mo",
      "Hao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ef1afa0daa888d695dcd5e9513bafa3-Abstract-Conference.html": {
    "title": "Counting Distinct Elements in the Turnstile Model with Differential Privacy under Continual Observation",
    "volume": "main",
    "abstract": "Privacy is a central challenge for systems that learn from sensitive data sets, especially when a system's outputs must be continuously updated to reflect changing data. We consider the achievable error for differentially private continual release of a basic statistic---the number of distinct items---in a stream where items may be both inserted and deleted (the turnstile model). With only insertions, existing algorithms have additive error just polylogarithmic in the length of the stream $T$. We uncover a much richer landscape in the turnstile model, even without considering memory restrictions. We show that every differentially private mechanism that handles insertions and deletions has worst-case additive error at least $T^{1/4}$ even under a relatively weak, event-level privacy definition. Then, we identify a parameter of the input stream, its maximum flippancy, that is low for natural data streams and for which we give tight parameterized error guarantees. Specifically, the maximum flippancy is the largest number of times that the contribution of a single item to the distinct elements count changes over the course of the stream. We present an item-level differentially private mechanism that, for all turnstile streams with maximum flippancy $w$, continually outputs the number of distinct elements with an $O(\\sqrt{w} \\cdot \\mathsf{poly}\\log T)$ additive error, without requiring prior knowledge of $w$. We prove that this is the best achievable error bound that depends only on $w$, for a large range of values of $w$. When $w$ is small, the error of our mechanism is similar to the polylogarithmic in $T$ error in the insertion-only setting, bypassing the hardness in the turnstile model",
    "keywords": [],
    "checked": true,
    "id": "61febc71dfeeed80f57d6d4c581750cbb083bb4e",
    "semantic_title": "counting distinct elements in the turnstile model with differential privacy under continual observation",
    "citation_count": 3,
    "authors": [
      "Palak Jain",
      "Iden Kalemaj",
      "Sofya Raskhodnikova",
      "Satchit Sivakumar",
      "Adam Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ef6ffcb85a2d238fc4761860c31ded4-Abstract-Conference.html": {
    "title": "Demystifying Softmax Gating Function in Gaussian Mixture of Experts",
    "volume": "main",
    "abstract": "Understanding the parameter estimation of softmax gating Gaussian mixture of experts has remained a long-standing open problem in the literature. It is mainly due to three fundamental theoretical challenges associated with the softmax gating function: (i) the identifiability only up to the translation of parameters; (ii) the intrinsic interaction via partial differential equations between the softmax gating and the expert functions in the Gaussian density; (iii) the complex dependence between the numerator and denominator of the conditional density of softmax gating Gaussian mixture of experts. We resolve these challenges by proposing novel Voronoi loss functions among parameters and establishing the convergence rates of maximum likelihood estimator (MLE) for solving parameter estimation in these models. When the true number of experts is unknown and over-specified, our findings show a connection between the convergence rate of the MLE and a solvability problem of a system of polynomial equations",
    "keywords": [],
    "checked": true,
    "id": "f02d9cd70f3e6d53796a47c0b4efefd0a81f826f",
    "semantic_title": "demystifying softmax gating function in gaussian mixture of experts",
    "citation_count": 5,
    "authors": [
      "Huy Nguyen",
      "TrungTin Nguyen",
      "Nhat Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0f0a30c7b46be23a83317c5cb721fc43-Abstract-Conference.html": {
    "title": "Hybrid Policy Optimization from Imperfect Demonstrations",
    "volume": "main",
    "abstract": "Exploration is one of the main challenges in Reinforcement Learning (RL), especially in environments with sparse rewards. Learning from Demonstrations (LfD) is a promising approach to solving this problem by leveraging expert demonstrations. However, expert demonstrations of high quality are usually costly or even impossible to collect in real-world applications. In this work, we propose a novel RL algorithm called HYbrid Policy Optimization (HYPO), which uses a small number of imperfect demonstrations to accelerate an agent's online learning process. The key idea is to train an offline guider policy using imitation learning in order to instruct an online agent policy to explore efficiently. Through mutual update of the guider policy and the agent policy, the agent can leverage suboptimal demonstrations for efficient exploration while avoiding the conservative policy caused by imperfect demonstrations. Empirical results show that HYPO significantly outperforms several baselines in various challenging tasks, such as MuJoCo with sparse rewards, Google Research Football, and the AirSim drone simulation",
    "keywords": [],
    "checked": false,
    "id": "c28c30a846d97f7e9c0722fbc35bb88aba3c6b04",
    "semantic_title": "guarded policy optimization with imperfect online demonstrations",
    "citation_count": 3,
    "authors": [
      "Hanlin Yang",
      "Chao Yu",
      "peng sun",
      "Siji Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0f0c4f3d83c58df58380af3b0729354c-Abstract-Conference.html": {
    "title": "What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization",
    "volume": "main",
    "abstract": "Uncertainty quantification (UQ) is essential for creating trustworthy machine learning models. Recent years have seen a steep rise in UQ methods that can flag suspicious examples, however, it is often unclear what exactly these methods identify. In this work, we propose a framework for categorizing uncertain examples flagged by UQ methods. We introduce the confusion density matrix---a kernel-based approximation of the misclassification density---and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark",
    "keywords": [],
    "checked": true,
    "id": "85e27aec760b3d16ab779e335fc195bc578d0840",
    "semantic_title": "what is flagged in uncertainty quantification? latent density models for uncertainty categorization",
    "citation_count": 2,
    "authors": [
      "Hao Sun",
      "Boris van Breugel",
      "Jonathan Crabbé",
      "Nabeel Seedat",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0f25eb6e9dc26c933a5d7516abf1eb8c-Abstract-Conference.html": {
    "title": "Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks",
    "volume": "main",
    "abstract": "Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches, which lack generalizability--- for each new model, the algorithm has to be executed from the beginning. Therefore, for an unseen architecture, one cannot use the subset chosen for a different model. In this work, we propose $\\texttt{SubSelNet}$, a non-adaptive subset selection framework, which tackles these problems. Here, we first introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This naturally provides us two variants of $\\texttt{SubSelNet}$. The first variant is transductive (called Transductive-$\\texttt{SubSelNet}$), which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called Inductive-$\\texttt{SubSelNet}$), which computes the subset using a trained subset selector, without any optimization. Our experiments show that our model outperforms several methods across several real datasets",
    "keywords": [],
    "checked": true,
    "id": "33762e1c423ece2e466b58ef4a9d6faf6681e9b7",
    "semantic_title": "efficient data subset selection to generalize training across models: transductive and inductive networks",
    "citation_count": 0,
    "authors": [
      "Eeshaan Jain",
      "Tushar Nandy",
      "Gaurav Aggarwal",
      "Ashish Tendulkar",
      "Rishabh Iyer",
      "Abir De"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0f53ecc0d36a5d5d3d3e94d42c4b23ca-Abstract-Conference.html": {
    "title": "HiBug: On Human-Interpretable Model Debug",
    "volume": "main",
    "abstract": "Machine learning models can frequently produce systematic errors on critical subsets (or slices) of data that share common attributes. Discovering and explaining such model bugs is crucial for reliable model deployment. However, existing bug discovery and interpretation methods usually involve heavy human intervention and annotation, which can be cumbersome and have low bug coverage.In this paper, we propose HiBug, an automated framework for interpretable model debugging. Our approach utilizes large pre-trained models, such as chatGPT, to suggest human-understandable attributes that are related to the targeted computer vision tasks. By leveraging pre-trained vision-language models, we can efficiently identify common visual attributes of underperforming data slices using human-understandable terms. This enables us to uncover rare cases in the training data, identify spurious correlations in the model, and use the interpretable debug results to select or generate new training data for model improvement. Experimental results demonstrate the efficacy of the HiBug framework",
    "keywords": [],
    "checked": false,
    "id": "28e81f96eab94e99febcaaee00637825c8a3e664",
    "semantic_title": "interpretable machine learning",
    "citation_count": 13,
    "authors": [
      "Muxi Chen",
      "YU LI",
      "Qiang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0f580c1ace3b857a390575ca42de7938-Abstract-Conference.html": {
    "title": "A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression",
    "volume": "main",
    "abstract": "Existing statistical learning guarantees for general kernel regressors often yield loose bounds when used with finite-rank kernels. Yet, finite-rank kernels naturally appear in a number of machine learning problems, e.g. when fine-tuning a pre-trained deep neural network's last layer to adapt it to a novel task when performing transfer learning. We address this gap for finite-rank kernel ridge regression (KRR) by deriving sharp non-asymptotic upper and lower bounds for the KRR test error of any finite-rank KRR. Our bounds are tighter than previously derived bounds on finite-rank KRR and, unlike comparable results, they also remain valid for any regularization parameters",
    "keywords": [],
    "checked": true,
    "id": "9922019edddf0a0eeed09779c55bd8081d125b72",
    "semantic_title": "a theoretical analysis of the test error of finite-rank kernel ridge regression",
    "citation_count": 4,
    "authors": [
      "Tin Sum Cheng",
      "Aurelien Lucchi",
      "Anastasis Kratsios",
      "Ivan Dokmanić",
      "David Belius"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0f6931a9e339a012a9909306d7c758b4-Abstract-Conference.html": {
    "title": "Learning Invariant Representations with a Nonparametric Nadaraya-Watson Head",
    "volume": "main",
    "abstract": "Machine learning models will often fail when deployed in an environment with a data distribution that is different than the training distribution. When multiple environments are available during training, many methods exist that learn representations which are invariant across the different distributions, with the hope that these representations will be transportable to unseen domains. In this work, we present a nonparametric strategy for learning invariant representations based on the recently-proposed Nadaraya-Watson (NW) head. The NW head makes a prediction by comparing the learned representations of the query to the elements of a support set that consists of labeled data. We demonstrate that by manipulating the support set, one can encode different causal assumptions. In particular, restricting the support set to a single environment encourages the model to learn invariant features that do not depend on the environment. We present a causally-motivated setup for our modeling and training strategy and validate on three challenging real-world domain generalization tasks in computer vision",
    "keywords": [],
    "checked": true,
    "id": "701b1e8ead3ec93b886802decea333001b63a7a7",
    "semantic_title": "learning invariant representations with a nonparametric nadaraya-watson head",
    "citation_count": 0,
    "authors": [
      "Alan Wang",
      "Minh Nguyen",
      "Mert Sabuncu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0f7e4bb7a35dd4cb426203c91a4bfa10-Abstract-Conference.html": {
    "title": "Conformalized matrix completion",
    "volume": "main",
    "abstract": "Matrix completion aims to estimate missing entries in a data matrix, using the assumption of a low-complexity structure (e.g., low-rankness) so that imputation is possible. While many effective estimation algorithms exist in the literature, uncertainty quantification for this problem has proved to be challenging, and existing methods are extremely sensitive to model misspecification. In this work, we propose a distribution-free method for predictive inference in the matrix completion problem. Our method adapts the framework of conformal prediction, which provides prediction intervals with guaranteed distribution-free validity in the setting of regression, to the problem of matrix completion. Our resulting method, conformalized matrix completion (cmc), offers provable predictive coverage regardless of the accuracy of the low-rank model. Empirical results on simulated and real data demonstrate that cmc is robust to model misspecification while matching the performance of existing model-based methods when the model is correct",
    "keywords": [],
    "checked": true,
    "id": "d85419474b26df4392c7483bec225323f3231248",
    "semantic_title": "conformalized matrix completion",
    "citation_count": 2,
    "authors": [
      "Yu Gui",
      "Rina Barber",
      "Cong Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0fa81c3f0d57f95b8776de3a248ef0ed-Abstract-Conference.html": {
    "title": "Mixture Weight Estimation and Model Prediction in Multi-source Multi-target Domain Adaptation",
    "volume": "main",
    "abstract": "We consider a problem of learning a model from multiple sources with the goal to performwell on a new target distribution. Such problem arises inlearning with data collected from multiple sources (e.g. crowdsourcing) orlearning in distributed systems, where the data can be highly heterogeneous. Thegoal of learner is to mix these data sources in a target-distribution aware way andsimultaneously minimize the empirical risk on the mixed source. The literature has made some tangible advancements in establishingtheory of learning on mixture domain. However, there are still two unsolved problems. Firstly, how to estimate the optimal mixture of sources, given a target domain; Secondly, when there are numerous target domains, we have to solve empirical risk minimization for each target on possibly unique mixed source data , which is computationally expensive. In this paper we address both problems efficiently and with guarantees.We cast the first problem, mixture weight estimation as convex-nonconcave compositional minimax, and propose an efficient stochasticalgorithm with provable stationarity guarantees.Next, for the second problem, we identify that for certain regime,solving ERM for each target domain individually can be avoided, and instead parameters for a target optimalmodel can be viewed as a non-linear function ona space of the mixture coefficients.To this end, we show that in offline setting, a GD-trained overparameterized neural network can provably learn such function.Finally, we also consider an online setting and propose an label efficient online algorithm, which predicts parameters for new models given arbitrary sequence of mixing coefficients, while enjoying optimal regret",
    "keywords": [],
    "checked": true,
    "id": "30104cbea097437807d31b34eff4ebbabdce98c6",
    "semantic_title": "mixture weight estimation and model prediction in multi-source multi-target domain adaptation",
    "citation_count": 0,
    "authors": [
      "Yuyang Deng",
      "Ilja Kuzborskij",
      "Mehrdad Mahdavi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0fb7c02d420c993385c7de44c2b5bf01-Abstract-Conference.html": {
    "title": "CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer",
    "volume": "main",
    "abstract": "We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/",
    "keywords": [],
    "checked": false,
    "id": "f6d79996a9d24daa572e4ffc91abd8420cd53fad",
    "semantic_title": "cell-e 2: translating proteins to pictures and back with a bidirectional text-to-image transformer",
    "citation_count": 0,
    "authors": [
      "Emaad Khwaja",
      "Yun Song",
      "Aaron Agarunov",
      "Bo Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0fb98d483fa580e0354bcdd3a003a3f3-Abstract-Conference.html": {
    "title": "HeadSculpt: Crafting 3D Head Avatars with Text",
    "volume": "main",
    "abstract": "Recently, text-guided 3D generative methods have made remarkable advancements in producing high-quality textures and geometry, capitalizing on the proliferation of large vision-language and image diffusion models. However, existing methods still struggle to create high-fidelity 3D head avatars in two aspects: (1) They rely mostly on a pre-trained text-to-image diffusion model whilst missing the necessary 3D awareness and head priors. This makes them prone to inconsistency and geometric distortions in the generated avatars. (2) They fall short in fine-grained editing. This is primarily due to the inherited limitations from the pre-trained 2D image diffusion models, which become more pronounced when it comes to 3D head avatars. In this work, we address these challenges by introducing a versatile coarse-to-fine pipeline dubbed HeadSculpt for crafting (i.e., generating and editing) 3D head avatars from textual prompts. Specifically, we first equip the diffusion model with 3D awareness by leveraging landmark-based control and a learned textual embedding representing the back view appearance of heads, enabling 3D-consistent head avatar generations. We further propose a novel identity-aware editing score distillation strategy to optimize a textured mesh with a high-resolution differentiable rendering technique. This enables identity preservation while following the editing instruction.We showcase HeadSculpt's superior fidelity and editing capabilities through comprehensive experiments and comparisons with existing methods",
    "keywords": [],
    "checked": true,
    "id": "4e8cf9602d4ef714dcdb8580de40e1a2a717ab11",
    "semantic_title": "headsculpt: crafting 3d head avatars with text",
    "citation_count": 17,
    "authors": [
      "Xiao Han",
      "Yukang Cao",
      "Kai Han",
      "Xiatian Zhu",
      "Jiankang Deng",
      "Yi-Zhe Song",
      "Tao Xiang",
      "Kwan-Yee K. Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0fbf046448d7eea18b982001320b9a10-Abstract-Conference.html": {
    "title": "CBD: A Certified Backdoor Detector Based on Local Dominant Probability",
    "volume": "main",
    "abstract": "Backdoor attack is a common threat to deep neural networks. During testing, samples embedded with a backdoor trigger will be misclassified as an adversarial target by a backdoored model, while samples without the backdoor trigger will be correctly classified. In this paper, we present the first certified backdoor detector (CBD), which is based on a novel, adjustable conformal prediction scheme based on our proposed statistic local dominant probability. For any classifier under inspection, CBD provides 1) a detection inference, 2) the condition under which the attacks are guaranteed to be detectable for the same classification domain, and 3) a probabilistic upper bound for the false positive rate. Our theoretical results show that attacks with triggers that are more resilient to test-time noise and have smaller perturbation magnitudes are more likely to be detected with guarantees. Moreover, we conduct extensive experiments on four benchmark datasets considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves comparable or even higher detection accuracy than state-of-the-art detectors, and it in addition provides detection certification. Notably, for backdoor attacks with random perturbation triggers bounded by $\\ell_2\\leq0.75$ which achieves more than 90\\% attack success rate, CBD achieves 100\\% (98\\%), 100\\% (84\\%), 98\\% (98\\%), and 72\\% (40\\%) empirical (certified) detection true positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and TinyImageNet, respectively, with low false positive rates",
    "keywords": [],
    "checked": true,
    "id": "7f02e286cc15bf58ed305cd9c5ad558b30d0a47f",
    "semantic_title": "cbd: a certified backdoor detector based on local dominant probability",
    "citation_count": 1,
    "authors": [
      "Zhen Xiang",
      "Zidi Xiong",
      "Bo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ff30c4bf31db0119a6219e0d250e037-Abstract-Conference.html": {
    "title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models",
    "volume": "main",
    "abstract": "Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page: https://sheetcopilot.github.io/",
    "keywords": [],
    "checked": true,
    "id": "e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7",
    "semantic_title": "sheetcopilot: bringing software productivity to the next level through large language models",
    "citation_count": 10,
    "authors": [
      "Hongxin Li",
      "Jingran Su",
      "Yuntao Chen",
      "Qing Li",
      "ZHAO-XIANG ZHANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ff3502bb29570b219967278db150a50-Abstract-Conference.html": {
    "title": "Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) enables learning a decision-making policy without interaction with the environment. This makes it particularly beneficial in situations where such interactions are costly. However, a known challenge for offline RL algorithms is the distributional mismatch between the state-action distributions of the learned policy and the dataset, which can significantly impact performance. State-of-the-art algorithms address it by constraining the policy to align with the state-action pairs in the dataset. However, this strategy struggles on datasets that predominantly consist of trajectories collected by low-performing policies and only a few trajectories from high-performing ones. Indeed, the constraint to align with the data leads the policy to imitate low-performing behaviors predominating the dataset. Our key insight to address this issue is to constrain the policy to the policy that collected the good parts of the dataset rather than all data. To this end, we optimize the importance sampling weights to emulate sampling data from a data distribution generated by a nearly optimal policy. Our method exhibits considerable performance gains (up to five times better) over the existing approaches in state-of-the-art offline RL algorithms over 72 imbalanced datasets with varying types of imbalance",
    "keywords": [],
    "checked": true,
    "id": "4ff5aa25088ad899a23e6ec0385ff5bd0e5efc68",
    "semantic_title": "beyond uniform sampling: offline reinforcement learning with imbalanced datasets",
    "citation_count": 1,
    "authors": [
      "Zhang-Wei Hong",
      "Aviral Kumar",
      "Sathwik Karnik",
      "Abhishek Bhandwaldar",
      "Akash Srivastava",
      "Joni Pajarinen",
      "Romain Laroche",
      "Abhishek Gupta",
      "Pulkit Agrawal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ff54b4ec4f70b3ae12c8621ca8a49f4-Abstract-Conference.html": {
    "title": "Variational Weighting for Kernel Density Ratios",
    "volume": "main",
    "abstract": "Kernel density estimation (KDE) is integral to a range of generative and discriminative tasks in machine learning. Drawing upon tools from the multidimensional calculus of variations, we derive an optimal weight function that reduces bias in standard kernel density estimates for density ratios, leading to improved estimates of prediction posteriors and information-theoretic measures. In the process, we shed light on some fundamental aspects of density estimation, particularly from the perspective of algorithms that employ KDEs as their main building blocks",
    "keywords": [],
    "checked": true,
    "id": "483c4f2069e60d86ad18393987b8d75cf2034fdd",
    "semantic_title": "variational weighting for kernel density ratios",
    "citation_count": 0,
    "authors": [
      "Sangwoong Yoon",
      "Frank Park",
      "Gunsu YUN",
      "Iljung Kim",
      "Yung-Kyun Noh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0ffd11b5bce666816802b86c77b54cf7-Abstract-Conference.html": {
    "title": "Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Linear Subspaces",
    "volume": "main",
    "abstract": "Despite a great deal of research, it is still not well-understood why trained neural networks are highly vulnerable to adversarial examples.In this work we focus on two-layer neural networks trained using data which lie on a low dimensional linear subspace.We show that standard gradient methods lead to non-robust neural networks, namely, networks which have large gradients in directions orthogonal to the data subspace, and are susceptible to small adversarial $L_2$-perturbations in these directions.Moreover, we show that decreasing the initialization scale of the training algorithm, or adding $L_2$ regularization, can make the trained network more robust to adversarial perturbations orthogonal to the data",
    "keywords": [],
    "checked": true,
    "id": "73a2d8255631a62ef56de0eced5ac82ffde1aa57",
    "semantic_title": "adversarial examples exist in two-layer relu networks for low dimensional linear subspaces",
    "citation_count": 0,
    "authors": [
      "Odelia Melamed",
      "Gilad Yehudai",
      "Gal Vardi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1052b823a161aa2c808dd51c0f58dc37-Abstract-Conference.html": {
    "title": "Complexity of Derivative-Free Policy Optimization for Structured $\\mathcal{H}_\\infty$ Control",
    "volume": "main",
    "abstract": "The applications of direct policy search in reinforcement learning and continuous control have received increasing attention.In this work, we present novel theoretical results on the complexity of derivative-free policy optimization on an important class of robust control tasks, namely the structured $H_\\infty$ synthesis with static output feedback. Optimal $H_\\infty$ synthesis under structural constraints leads to a constrained nonconvex nonsmooth problem and is typicallyaddressed using subgradient-based policy search techniques that are built upon the concept of Goldstein subdifferential or other notions of enlarged subdifferential. In this paper, we study the complexity of finding $(\\delta,\\epsilon)$-stationary points for such nonsmooth robust control design tasks using policy optimization methods which can only access the zeroth-order oracle (i.e. the $H_\\infty$ norm of the closed-loop system). First, we study the exact oracle setting and identify the coerciveness of the cost function to prove high-probability feasibility/complexity bounds for derivative-free policy optimization on this problem. Next, we derive a sample complexity result for the multi-input multi-output (MIMO) $H_\\infty$-norm estimation. We combine this with our analysis to obtain the first sample complexity of model-free, trajectory-based, zeroth-order policy optimization on finding $(\\delta,\\epsilon)$-stationary points for structured $H_\\infty$ control. Numerical results are also provided to demonstrate our theory",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingang Guo",
      "Darioush Keivan",
      "Geir Dullerud",
      "Peter Seiler",
      "Bin Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/105fdc31cc9eb927cc5a0110f4031287-Abstract-Conference.html": {
    "title": "Meet in the Middle: A New Pre-training Paradigm",
    "volume": "main",
    "abstract": "Most language models (LMs) are trained and applied in an autoregressive left-to-right fashion, predicting the next token from the preceding ones. However, this ignores that the full sequence is available during training. In this paper, we introduce ``Meet in the Middle'' (MIM) a new pre-training paradigm that improves data efficiency by training in two directions, left-to-right and right-to-left, and encouraging the respective modelsto agree on their token distribution for each position. While the primary outcome is an improved left-to-right LM,we also obtain secondary benefits in the infilling task. There, we leverage the two pre-trained directions to propose an infilling procedure that builds the completion simultaneously from both sides. We conduct extensive experiments on both programming and natural languages and show that MIM significantly surpasses existing pre-training paradigms, in both left-to-right generation as well as infilling.Code and models available at https://github.com/microsoft/Meet-in-the-Middle",
    "keywords": [],
    "checked": true,
    "id": "fbdd496c421e050a47c4fb2e0019635d2f4b97e7",
    "semantic_title": "meet in the middle: a new pre-training paradigm",
    "citation_count": 8,
    "authors": [
      "Anh Nguyen",
      "Nikos Karampatziakis",
      "Weizhu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/106b2434b8d496c6aed9235d478678af-Abstract-Conference.html": {
    "title": "Score-based Source Separation with Applications to Digital Communication Signals",
    "volume": "main",
    "abstract": "We propose a new method for separating superimposed sources using diffusion-based generative models. Our method relies only on separately trained statistical priors of independent sources to establish a new objective function guided by $\\textit{maximum a posteriori}$ estimation with an $\\textit{$\\alpha$-posterior}$, across multiple levels of Gaussian smoothing. Motivated by applications in radio-frequency (RF) systems, we are interested in sources with underlying discrete nature and the recovery of encoded bits from a signal of interest, as measured by the bit error rate (BER). Experimental results with RF mixtures demonstrate that our method results in a BER reduction of 95\\% over classical and existing learning-based methods. Our analysis demonstrates that our proposed method yields solutions that asymptotically approach the modes of an underlying discrete distribution. Furthermore, our method can be viewed as a multi-source extension to the recently proposed score distillation sampling scheme, shedding additional light on its use beyond conditional sampling. The project webpage is available at https://alpha-rgs.github.io",
    "keywords": [],
    "checked": true,
    "id": "760e904afb36e281fe300911e0befeba21a3eef8",
    "semantic_title": "score-based source separation with applications to digital communication signals",
    "citation_count": 1,
    "authors": [
      "Tejas Jayashankar",
      "Gary C.F. Lee",
      "Alejandro Lancho",
      "Amir Weiss",
      "Yury Polyanskiy",
      "Gregory Wornell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1074541383db5ef12d6ac66d2f8e8d34-Abstract-Conference.html": {
    "title": "Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint",
    "volume": "main",
    "abstract": "Fair Principal Component Analysis (PCA) is a problem setting where we aim to perform PCA while making the resulting representation fair in that the projected distributions, conditional on the sensitive attributes, match one another. However, existing approaches to fair PCA have two main problems: theoretically, there has been no statistical foundation of fair PCA in terms of learnability; practically, limited memory prevents us from using existing approaches, as they explicitly rely on full access to the entire data. On the theoretical side, we rigorously formulate fair PCA using a new notion called probably approximately fair and optimal (PAFO) learnability. On the practical side, motivated by recent advances in streaming algorithms for addressing memory limitation, we propose a new setting called fair streaming PCA along with a memory-efficient algorithm, fair noisy power method (FNPM). We then provide its statistical guarantee in terms of PAFO-learnability, which is the first of its kind in fair PCA literature. We verify our algorithm in the CelebA dataset without any pre-processing; while the existing approaches are inapplicable due to memory limitations, by turning it into a streaming setting, we show that our algorithm performs fair PCA efficiently and effectively",
    "keywords": [],
    "checked": true,
    "id": "28dbb804bf2a358a1ae30756272a39ddfa039604",
    "semantic_title": "fair streaming principal component analysis: statistical and algorithmic viewpoint",
    "citation_count": 0,
    "authors": [
      "Junghyun Lee",
      "Hanseul Cho",
      "Se-Young Yun",
      "Chulhee Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/108030643e640ac050e0ed5e6aace48f-Abstract-Conference.html": {
    "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
    "volume": "main",
    "abstract": "A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights: \"keeping critical thinking\" and \"letting everyone do their jobs\" in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability",
    "keywords": [],
    "checked": true,
    "id": "f8b8f926bbfa327c86c40796131fe2695db81126",
    "semantic_title": "ddcot: duty-distinct chain-of-thought prompting for multimodal reasoning in language models",
    "citation_count": 7,
    "authors": [
      "Ge Zheng",
      "Bin Yang",
      "Jiajin Tang",
      "Hong-Yu Zhou",
      "Sibei Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1097a0aeaf00cacfa8f6aced24f3a8bd-Abstract-Conference.html": {
    "title": "Adversarially Robust Learning with Uncertain Perturbation Sets",
    "volume": "main",
    "abstract": "In many real-world settings exact perturbation sets to be used by an adversary are not plausibly available to a learner. While prior literature has studied both scenarios with completely known and completely unknown perturbation sets, we propose an in-between setting of learning with respect to a class of perturbation sets. We show that in this setting we can improve on previous results with completely unknown perturbation sets, while still addressing the concerns of not having perfect knowledge of these sets in real life. In particular, we give the first positive results for the learnability of infinite Littlestone classes when having access to a perfect-attack oracle. We also consider a setting of learning with abstention, where predictions are considered robustness violations, only when the wrong prediction is made within the perturbation set. We show there are classes for which perturbation-set unaware learning without query access is possible, but abstention is required",
    "keywords": [],
    "checked": false,
    "id": "cce64f803053fa2fb7bf27194ad8b86f747f25c2",
    "semantic_title": "adversarially robust learning with unknown perturbation sets",
    "citation_count": 17,
    "authors": [
      "Tosca Lechner",
      "Vinayak Pathak",
      "Ruth Urner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/10b7e27c8eb9571fbbd2ae6a9f8c3855-Abstract-Conference.html": {
    "title": "Common Ground in Cooperative Communication",
    "volume": "main",
    "abstract": "Cooperative communication plays a fundamental role in theories of human-human interaction--cognition, culture, development, language, etc.--as well as human-robot interaction. The core challenge in cooperative communication is the problem of common ground: having enough shared knowledge and understanding to successfully communicate. Prior models of cooperative communication, however, uniformly assume the strongest form of common ground, perfect and complete knowledge sharing, and, therefore, fail to capture the core challenge of cooperative communication. We propose a general theory of cooperative communication that is mathematically principled and explicitly defines a spectrum of common ground possibilities, going well beyond that of perfect and complete knowledge sharing, on spaces that permit arbitrary representations of data and hypotheses. Our framework is a strict generalization of prior models of cooperative communication. After considering a parametric form of common ground and viewing the data selection and hypothesis inference processes of communication as encoding and decoding, we establish a connection to variational autoencoding, a powerful model in modern machine learning. Finally, we carry out a series of empirical simulations to support and elaborate on our theoretical results",
    "keywords": [],
    "checked": true,
    "id": "480feff7ef39e85f493ddcbfb06a1834ed051090",
    "semantic_title": "common ground in cooperative communication",
    "citation_count": 0,
    "authors": [
      "Xiaoran Hao",
      "Yash Jhaveri",
      "Patrick Shafto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/10cb15f4559b3d578b7f24966d48a137-Abstract-Conference.html": {
    "title": "Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control",
    "volume": "main",
    "abstract": "The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \\textbf{T}rajectories-awar\\textbf{E} \\textbf{E}nsemble exploratio\\textbf{N} (TEEN). The primary goal of TEEN is to maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble policy compared to using sub-policies alone but also improves the performance over ensemble RL algorithms. On average, TEEN outperforms the baseline ensemble DRL algorithms by 41\\% in performance on the tested representative environments",
    "keywords": [],
    "checked": true,
    "id": "b697f7c019911dcf433c66266a113365d6e6d6bd",
    "semantic_title": "keep various trajectories: promoting exploration of ensemble policies in continuous control",
    "citation_count": 0,
    "authors": [
      "Chao Li",
      "Chen GONG",
      "Qiang He",
      "Xinwen Hou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/10e9204f14c4daa08041343455435308-Abstract-Conference.html": {
    "title": "ReSync: Riemannian Subgradient-based Robust Rotation Synchronization",
    "volume": "main",
    "abstract": "This work presents ReSync, a Riemannian subgradient-based algorithm for solving the robust rotation synchronization problem, which arises in various engineering applications. ReSync solves a least-unsquared minimization formulation over the rotation group, which is nonsmooth and nonconvex, and aims at recovering the underlying rotations directly. We provide strong theoretical guarantees for ReSync under the random corruption setting. Specifically, we first show that the initialization procedure of ReSync yields a proper initial point that lies in a local region around the ground-truth rotations. We next establish the weak sharpness property of the aforementioned formulation and then utilize this property to derive the local linear convergence of ReSync to the ground-truth rotations. By combining these guarantees, we conclude that ReSync converges linearly to the ground-truth rotations under appropriate conditions. Experiment results demonstrate the effectiveness of ReSync",
    "keywords": [],
    "checked": true,
    "id": "8694454973a684535aa43d1b7d8114865468627a",
    "semantic_title": "resync: riemannian subgradient-based robust rotation synchronization",
    "citation_count": 0,
    "authors": [
      "Huikang Liu",
      "Xiao Li",
      "Anthony Man-Cho So"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/10fc83943b4540a9524af6fc67a23fef-Abstract-Conference.html": {
    "title": "On the Exploration of Local Significant Differences For Two-Sample Test",
    "volume": "main",
    "abstract": "Recent years have witnessed increasing attentions on two-sample test with diverse real applications, while this work takes one more step on the exploration of local significant differences for two-sample test. We propose the ME$_\\text{MaBiD}$, an effective test for two-sample testing, and the basic idea is to exploit local information by multiple Mahalanobis kernels and introduce bi-directional hypothesis for testing. On the exploration of local significant differences, we first partition the embedding space into several rectangle regions via a new splitting criterion, which is relevant to test power and data correlation. We then explore local significant differences based on our bi-directional masked $p$-value together with the ME$_\\text{MaBiD}$ test. Theoretically, we present the asymptotic distribution and lower bounds of test power for our ME$_\\text{MaBiD}$ test, and control the familywise error rate on the exploration of local significant differences. We finally conduct extensive experiments to validate the effectiveness of our proposed methods on two-sample test and the exploration of local significant differences",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Zhou",
      "Jie Ni",
      "Jia-He Yao",
      "Wei Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/112d8e0c7563de6e3408b49a09b4d8a3-Abstract-Conference.html": {
    "title": "Fine-Grained Cross-View Geo-Localization Using a Correlation-Aware Homography Estimator",
    "volume": "main",
    "abstract": "In this paper, we introduce a novel approach to fine-grained cross-view geo-localization. Our method aligns a warped ground image with a corresponding GPS-tagged satellite image covering the same area using homography estimation. We first employ a differentiable spherical transform, adhering to geometric principles, to accurately align the perspective of the ground image with the satellite map. This transformation effectively places ground and aerial images in the same view and on the same plane, reducing the task to an image alignment problem. To address challenges such as occlusion, small overlapping range, and seasonal variations, we propose a robust correlation-aware homography estimator to align similar parts of the transformed ground image with the satellite image. Our method achieves sub-pixel resolution and meter-level GPS accuracy by mapping the center point of the transformed ground image to the satellite image using a homography matrix and determining the orientation of the ground camera using a point above the central axis. Operating at a speed of 30 FPS, our method outperforms state-of-the-art techniques, reducing the mean metric localization error by 21.3\\% and 32.4\\% in same-area and cross-area generalization tasks on the VIGOR benchmark, respectively, and by 34.4\\% on the KITTI benchmark in same-area evaluation",
    "keywords": [],
    "checked": true,
    "id": "42518af29f4430908353d21d731a7eb458daa8ce",
    "semantic_title": "fine-grained cross-view geo-localization using a correlation-aware homography estimator",
    "citation_count": 0,
    "authors": [
      "Xiaolong Wang",
      "Runsen Xu",
      "Zhuofan Cui",
      "Zeyu Wan",
      "Yu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1160792eab11de2bbaf9e71fce191e8c-Abstract-Conference.html": {
    "title": "Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization",
    "volume": "main",
    "abstract": "This paper investigates new families of compositional optimization problems, called non-smooth weakly-convex finite-sum coupled compositional optimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\\epsilon$-stationary point of the Moreau envelop of the objective function. Additionally, we also extend the algorithm for solving novel non-smooth weakly-convex tri-level finite-sum coupled compositional optimization problems, which feature a nested arrangement of three functions. Lastly, we explore the applications of our algorithms in deep learning for two-way partial AUC maximization and multi-instance two-way partial AUC maximization, using empirical studies to showcase the effectiveness of the proposed algorithms",
    "keywords": [],
    "checked": true,
    "id": "e9d879698e4cd17d3fac9af98a9d343096897048",
    "semantic_title": "non-smooth weakly-convex finite-sum coupled compositional optimization",
    "citation_count": 1,
    "authors": [
      "Quanqi Hu",
      "Dixian Zhu",
      "Tianbao Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1160e7f31d0a74abbbe1bbf7924b949c-Abstract-Conference.html": {
    "title": "Optimal Transport for Treatment Effect Estimation",
    "volume": "main",
    "abstract": "Estimating individual treatment effects from observational data is challenging due to treatment selection bias. Prevalent methods mainly mitigate this issue by aligning different treatment groups in the latent space, the core of which is the calculation of distribution discrepancy. However, two issues that are often overlooked can render these methods invalid:(1) mini-batch sampling effects (MSE), where the calculated discrepancy is erroneous in non-ideal mini-batches with outcome imbalance and outliers;(2) unobserved confounder effects (UCE), where the unobserved confounders are not considered in the discrepancy calculation.Both of these issues invalidate the calculated discrepancy, mislead the training of estimators, and thus impede the handling of treatment selection bias.To tackle these issues, we propose Entire Space CounterFactual Regression (ESCFR), which is a new take on optimal transport technology in the context of causality.Specifically, based on the canonical optimal transport framework, we propose a relaxed mass-preserving regularizer to address the MSE issue and design a proximal factual outcome regularizer to handle the UCE issue.Extensive experiments demonstrate that ESCFR estimates distribution discrepancy accurately, handles the treatment selection bias effectively, and outperforms prevalent competitors significantly",
    "keywords": [],
    "checked": true,
    "id": "5e9a01144dd1508207a5209cf8854096202757d9",
    "semantic_title": "optimal transport for treatment effect estimation",
    "citation_count": 1,
    "authors": [
      "Hao Wang",
      "Jiajun Fan",
      "Zhichao Chen",
      "Haoxuan Li",
      "Weiming Liu",
      "Tianqiao Liu",
      "Quanyu Dai",
      "Yichao Wang",
      "Zhenhua Dong",
      "Ruiming Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1165af8b913fb836c6280b42d6e0084f-Abstract-Conference.html": {
    "title": "Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks",
    "volume": "main",
    "abstract": "We analytically investigate how over-parameterization of models in randomized machine learning algorithms impacts the information leakage about their training data. Specifically, we prove a privacy bound for the KL divergence between model distributions on worst-case neighboring datasets, and explore its dependence on the initialization, width, and depth of fully connected neural networks. We find that this KL privacy bound is largely determined by the expected squared gradient norm relative to model parameters during training. Notably, for the special setting of linearized network, our analysis indicates that the squared gradient norm (and therefore the escalation of privacy loss) is tied directly to the per-layer variance of the initialization distribution. By using this analysis, we demonstrate that privacy bound improves with increasing depth under certain initializations (LeCun and Xavier), while degrades with increasing depth under other initializations (He and NTK). Our work reveals a complex interplay between privacy and depth that depends on the chosen initialization distribution. We further prove excess empirical risk bounds under a fixed KL privacy budget, and show that the interplay between privacy utility trade-off and depth is similarly affected by the initialization",
    "keywords": [],
    "checked": true,
    "id": "f494f0563fe1c7f6f3947b601e0f2344f500bb1a",
    "semantic_title": "initialization matters: privacy-utility analysis of overparameterized neural networks",
    "citation_count": 2,
    "authors": [
      "Jiayuan Ye",
      "Zhenyu Zhu",
      "Fanghui Liu",
      "Reza Shokri",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/11715d433f6f8b9106baae0df023deb3-Abstract-Conference.html": {
    "title": "Cause-Effect Inference in Location-Scale Noise Models: Maximum Likelihood vs. Independence Testing",
    "volume": "main",
    "abstract": "A fundamental problem of causal discovery is cause-effect inference, to learn the correct causal direction between two random variables. Significant progress has been made through modelling the effect as a function of its cause and a noise term, which allows us to leverage assumptions about the generating function class. The recently introduced heteroscedastic location-scale noise functional models (LSNMs) combine expressive power with identifiability guarantees. LSNM model selection based on maximizing likelihood achieves state-of-the-art accuracy, when the noise distributions are correctly specified. However, through an extensive empirical evaluation, we demonstrate that the accuracy deteriorates sharply when the form of the noise distribution is misspecified by the user. Our analysis shows that the failure occurs mainly when the conditional variance in the anti-causal direction is smaller than that in the causal direction. As an alternative, we find that causal model selection through residual independence testing is much more robust to noise misspecification and misleading conditional variance",
    "keywords": [],
    "checked": true,
    "id": "d458cf6f9305a990db1920264f91b43fba028355",
    "semantic_title": "cause-effect inference in location-scale noise models: maximum likelihood vs. independence testing",
    "citation_count": 2,
    "authors": [
      "Xiangyu Sun",
      "Oliver Schulte"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/11822e84689e631615199db3b75cd0e4-Abstract-Conference.html": {
    "title": "CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders",
    "volume": "main",
    "abstract": "A vital and rapidly growing application, remote sensing offers vast yet sparsely labeled, spatially aligned multimodal data; this makes self-supervised learning algorithms invaluable. We present CROMA: a framework that combines contrastive and reconstruction self-supervised objectives to learn rich unimodal and multimodal representations. Our method separately encodes masked-out multispectral optical and synthetic aperture radar samples—aligned in space and time—and performs cross-modal contrastive learning. Another encoder fuses these sensors, producing joint multimodal encodings that are used to predict the masked patches via a lightweight decoder. We show that these objectives are complementary when leveraged on spatially aligned multimodal data. We also introduce X- and 2D-ALiBi, which spatially biases our cross- and self-attention matrices. These strategies improve representations and allow our models to effectively extrapolate to images up to $17.6\\times$ larger at test-time. CROMA outperforms the current SoTA multispectral model, evaluated on: four classification benchmarks—finetuning (avg.$\\uparrow$ 1.8%), linear (avg.$\\uparrow$ 2.4%) and nonlinear (avg.$\\uparrow$ 1.4%) probing, $k$NN classification (avg.$\\uparrow$ 3.5%), and $K$-means clustering (avg.$\\uparrow$ 8.4%); and three segmentation benchmarks (avg.$\\uparrow$ 6.4%). CROMA's rich, optionally multimodal representations can be widely leveraged across remote sensing applications",
    "keywords": [],
    "checked": true,
    "id": "ce73bca81ed4ff96efad709c2fbcd451d610bd10",
    "semantic_title": "croma: remote sensing representations with contrastive radar-optical masked autoencoders",
    "citation_count": 3,
    "authors": [
      "Anthony Fuller",
      "Koreen Millard",
      "James Green"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/11a7f429d75f9f8c6e9c630aeb6524b5-Abstract-Conference.html": {
    "title": "Neural Frailty Machine: Beyond proportional hazard assumption in neural survival regressions",
    "volume": "main",
    "abstract": "We present neural frailty machine (NFM), a powerful and flexible neural modeling framework for survival regressions. The NFM framework utilizes the classical idea of multiplicative frailty in survival analysis as a principled way of extending the proportional hazard assumption, at the same time being able to leverage the strong approximation power of neural architectures for handling nonlinear covariate dependence. Two concrete models are derived under the framework that extends neural proportional hazard models and nonparametric hazard regression models. Both models allow efficient training under the likelihood objective. Theoretically, for both proposed models, we establish statistical guarantees of neural function approximation with respect to nonparametric components via characterizing their rate of convergence. Empirically, we provide synthetic experiments that verify our theoretical statements. We also conduct experimental evaluations over $6$ benchmark datasets of different scales, showing that the proposed NFM models achieve predictive performance comparable to or sometimes surpassing state-of-the-art survival models. Our code is publicly availabel at https://github.com/Rorschach1989/nfm",
    "keywords": [],
    "checked": true,
    "id": "b076fd8ce72e718ed0173014f952807e87929db3",
    "semantic_title": "neural frailty machine: beyond proportional hazard assumption in neural survival regressions",
    "citation_count": 0,
    "authors": [
      "Ruofan Wu",
      "Jiawei Qiao",
      "Mingzhe Wu",
      "Wen Yu",
      "Ming Zheng",
      "Tengfei LIU",
      "Tianyi Zhang",
      "Weiqiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/11c7f1dd168439884b6dfb43a7891432-Abstract-Conference.html": {
    "title": "Non-autoregressive Machine Translation with Probabilistic Context-free Grammar",
    "volume": "main",
    "abstract": "Non-autoregressive Transformer(NAT) significantly accelerates the inference of neural machine translation. However, conventional NAT models suffer from limited expression power and performance degradation compared to autoregressive (AT) models due to the assumption of conditional independence among target tokens. To address these limitations, we propose a novel approach called PCFG-NAT, which leverages a specially designed Probabilistic Context-Free Grammar (PCFG) to enhance the ability of NAT models to capture complex dependencies among output tokens. Experimental results on major machine translation benchmarks demonstrate that PCFG-NAT further narrows the gap in translation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a deeper understanding of the generated sentences, addressing the lack of satisfactory explainability in neural machine translation. Code is publicly available at https://github.com/ictnlp/PCFG-NAT",
    "keywords": [],
    "checked": true,
    "id": "1373e59da4c89c44773173b124cc460f680e9089",
    "semantic_title": "non-autoregressive machine translation with probabilistic context-free grammar",
    "citation_count": 1,
    "authors": [
      "Shangtong Gui",
      "Chenze Shao",
      "Zhengrui Ma",
      "xishan zhang",
      "Yunji Chen",
      "Yang Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/11e1900e680f5fe1893a8e27362dbe2c-Abstract-Conference.html": {
    "title": "Constrained Policy Optimization with Explicit Behavior Density For Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Due to the inability to interact with the environment, offline reinforcement learning (RL) methods face the challenge of estimating the Out-of-Distribution (OOD) points. Existing methods for addressing this issue either control policy to exclude the OOD action or make the $Q$ function pessimistic. However, these methods can be overly conservative or fail to identify OOD areas accurately. To overcome this problem, we propose a Constrained Policy optimization with Explicit Behavior density (CPED) method that utilizes a flow-GAN model to explicitly estimate the density of behavior policy. By estimating the explicit density, CPED can accurately identify the safe region and enable exploration within the region, resulting in less conservative learning policies. We further provide theoretical results for both the flow-GAN estimator and performance guarantee for CPED by showing that CPED can find the optimal $Q$-function value. Empirically, CPED outperforms existing alternatives on various standard offline reinforcement learning tasks, yielding higher expected returns",
    "keywords": [],
    "checked": false,
    "id": "d373059e20daa0b4d91ef2a5fdd09d56692e7ca5",
    "semantic_title": "supported policy optimization for offline reinforcement learning",
    "citation_count": 23,
    "authors": [
      "Jing Zhang",
      "Chi Zhang",
      "Wenjia Wang",
      "Bingyi Jing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/120339238f293d4ae53a7167403abc4b-Abstract-Conference.html": {
    "title": "Formalizing locality for normative synaptic plasticity models",
    "volume": "main",
    "abstract": "In recent years, many researchers have proposed new models for synaptic plasticity in the brain based on principles of machine learning. The central motivation has been the development of learning algorithms that are able to learn difficult tasks while qualifying as \"biologically plausible\". However, the concept of a biologically plausible learning algorithm is only heuristically defined as an algorithm that is potentially implementable by biological neural networks. Further, claims that neural circuits could implement any given algorithm typically rest on an amorphous concept of \"locality\" (both in space and time). As a result, it is unclear what many proposed local learning algorithms actually predict biologically, and which of these are consequently good candidates for experimental investigation. Here, we address this lack of clarity by proposing formal and operational definitions of locality. Specifically, we define different classes of locality, each of which makes clear what quantities cannot be included in a learning rule if an algorithm is to qualify as local with respect to a given (biological) constraint. We subsequently use this framework to distill testable predictions from various classes of biologically plausible synaptic plasticity models that are robust to arbitrary choices about neural network architecture. Therefore, our framework can be used to guide claims of biological plausibility and to identify potential means of experimentally falsifying a proposed learning algorithm for the brain",
    "keywords": [],
    "checked": true,
    "id": "0459f71fca3b443eff059118f7d35f80c9a160e1",
    "semantic_title": "formalizing locality for normative synaptic plasticity models",
    "citation_count": 0,
    "authors": [
      "Colin Bredenberg",
      "Ezekiel Williams",
      "Cristina Savin",
      "Blake Richards",
      "Guillaume Lajoie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/120ed726cf129dbeb8375b6f8a0686f8-Abstract-Conference.html": {
    "title": "Exact Verification of ReLU Neural Control Barrier Functions",
    "volume": "main",
    "abstract": "Control Barrier Functions (CBFs) are a popular approach for safe control of nonlinear systems. In CBF-based control, the desired safety properties of the system are mapped to nonnegativity of a CBF, and the control input is chosen to ensure that the CBF remains nonnegative for all time. Recently, machine learning methods that represent CBFs as neural networks (neural control barrier functions, or NCBFs) have shown great promise due to the universal representability of neural networks. However, verifying that a learned CBF guarantees safety remains a challenging research problem. This paper presents novel exact conditions and algorithms for verifying safety of feedforward NCBFs with ReLU activation functions. The key challenge in doing so is that, due to the piecewise linearity of the ReLU function, the NCBF will be nondifferentiable at certain points, thus invalidating traditional safety verification methods that assume a smooth barrier function. We resolve this issue by leveraging a generalization of Nagumo's theorem for proving invariance of sets with nonsmooth boundaries to derive necessary and sufficient conditions for safety. Based on this condition, we propose an algorithm for safety verification of NCBFs that first decomposes the NCBF into piecewise linear segments and then solves a nonlinear program to verify safety of each segment as well as the intersections of the linear segments. We mitigate the complexity by only considering the boundary of the safe region and by pruning the segments with Interval Bound Propagation (IBP) and linear relaxation. We evaluate our approach through numerical studies with comparison to state-of-the-art SMT-based methods. Our code is available at https://github.com/HongchaoZhang-HZ/exactverif-reluncbf-nips23",
    "keywords": [],
    "checked": true,
    "id": "a10de1c39b73f3cc9f9f4990505c08e8897f4eb6",
    "semantic_title": "exact verification of relu neural control barrier functions",
    "citation_count": 1,
    "authors": [
      "Hongchao Zhang",
      "Junlin Wu",
      "Yevgeniy Vorobeychik",
      "Andrew Clark"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/12143893d9d37c3569dda800b95cabd9-Abstract-Conference.html": {
    "title": "Normalization-Equivariant Neural Networks with Application to Image Denoising",
    "volume": "main",
    "abstract": "In many information processing systems, it may be desirable to ensure that any change of the input, whether by shifting or scaling, results in a corresponding change in the system response. While deep neural networks are gradually replacing all traditional automatic processing methods, they surprisingly do not guarantee such normalization-equivariance (scale + shift) property, which can be detrimental in many applications. To address this issue, we propose a methodology for adapting existing neural networks so that normalization-equivariance holds by design. Our main claim is that not only ordinary convolutional layers, but also all activation functions, including the ReLU (rectified linear unit), which are applied element-wise to the pre-activated neurons, should be completely removed from neural networks and replaced by better conditioned alternatives. To this end, we introduce affine-constrained convolutions and channel-wise sort pooling layers as surrogates and show that these two architectural modifications do preserve normalization-equivariance without loss of performance. Experimental results in image denoising show that normalization-equivariant neural networks, in addition to their better conditioning, also provide much better generalization across noise levels",
    "keywords": [],
    "checked": true,
    "id": "46c00e57639980ef23ab70508e24cb384bfb2b1b",
    "semantic_title": "normalization-equivariant neural networks with application to image denoising",
    "citation_count": 0,
    "authors": [
      "Sébastien Herbreteau",
      "Emmanuel Moebel",
      "Charles Kervrann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/121db870b0470dd63bb5bc59c724275a-Abstract-Conference.html": {
    "title": "Budgeting Counterfactual for Offline RL",
    "volume": "main",
    "abstract": "The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action? These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon. Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation. Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training. Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy. It balances between the potential for improvement from taking out-of-distribution actions and the risk of making errors due to extrapolation. Theoretically, we justify our method by the constrained optimality of the fixed point solution to our $Q$ updating rules. Empirically, we show that the overall performance of our method is better than the state-of-the-art offline RL methods on tasks in the widely-used D4RL benchmarks",
    "keywords": [],
    "checked": true,
    "id": "c7d37504b150d59c7d7193be7560df442fecca74",
    "semantic_title": "budgeting counterfactual for offline rl",
    "citation_count": 0,
    "authors": [
      "Yao Liu",
      "Pratik Chaudhari",
      "Rasool Fakoor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1229eaae5bf1db93e1e4c539258eb472-Abstract-Conference.html": {
    "title": "Federated Conditional Stochastic Optimization",
    "volume": "main",
    "abstract": "Conditional stochastic optimization has found applications in a wide range of machine learning tasks, such as invariant learning, AUPRC maximization, and meta-learning. As the demand for training models with large-scale distributed data grows in these applications, there is an increasing need for communication-efficient distributed optimization algorithms, such as federated learning algorithms. This paper considers the nonconvex conditional stochastic optimization in federated learning and proposes the first federated conditional stochastic optimization algorithm (FCSG) with a conditional stochastic gradient estimator and a momentum-based algorithm (\\emph{i.e.}, FCSG-M). To match the lower bound complexity in the single-machine setting, we design an accelerated algorithm (Acc-FCSG-M) via the variance reduction to achieve the best sample and communication complexity. Compared with the existing optimization analysis for Meta-Learning in FL, federated conditional stochastic optimization considers the sample of tasks. Extensive experimental results on various tasks validate the efficiency of these algorithms",
    "keywords": [],
    "checked": true,
    "id": "6feebec35cf95d3d5b94b7e5efb97818f1f6d1f3",
    "semantic_title": "federated conditional stochastic optimization",
    "citation_count": 3,
    "authors": [
      "Xidong Wu",
      "Jianhui Sun",
      "Zhengmian Hu",
      "Junyi Li",
      "Aidong Zhang",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/123a18dfd821c8b440f42a00a27648d6-Abstract-Conference.html": {
    "title": "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections",
    "volume": "main",
    "abstract": "Recently, large-scale pre-trained Vision and Language (VL) models have set a new state-of-the-art (SOTA) in zero-shot visual classification enabling open-vocabulary recognition of potentially unlimited set of categories defined as simple language prompts. However, despite these great advances, the performance of these zero-shot classifiers still falls short of the results of dedicated (closed category set) classifiers trained with supervised fine-tuning. In this paper we show, for the first time, how to reduce this gap without any labels and without any paired VL data, using an unlabeled image collection and a set of texts auto-generated using a Large Language Model (LLM) describing the categories of interest and effectively substituting labeled visual instances of those categories. Using our label-free approach, we are able to attain significant performance improvements over the zero-shot performance of the base VL model and other contemporary methods and baselines on a wide variety of datasets, demonstrating absolute improvement of up to $11.7\\%$ ($3.8\\%$ on average) in the label-free setting. Moreover, despite our approach being label-free, we observe $1.3\\%$ average gains over leading few-shot prompting baselines that do use 5-shot supervision",
    "keywords": [],
    "checked": true,
    "id": "a04883d1d780b438de6c127caf7ebe3d9233e193",
    "semantic_title": "lafter: label-free tuning of zero-shot classifier using language and unlabeled image collections",
    "citation_count": 5,
    "authors": [
      "Muhammad Jehanzeb Mirza",
      "Leonid Karlinsky",
      "Wei Lin",
      "Horst Possegger",
      "Mateusz Kozinski",
      "Rogerio Feris",
      "Horst Bischof"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/123cfe7d8b7702ac97aaf4468fc05fa5-Abstract-Conference.html": {
    "title": "Contextually Affinitive Neighborhood Refinery for Deep Clustering",
    "volume": "main",
    "abstract": "Previous endeavors in self-supervised learning have enlightened the research of deep clustering from an instance discrimination perspective. Built upon this foundation, recent studies further highlight the importance of grouping semantically similar instances. One effective method to achieve this is by promoting the semantic structure preserved by neighborhood consistency. However, the samples in the local neighborhood may be limited due to their close proximity to each other, which may not provide substantial and diverse supervision signals. Inspired by the versatile re-ranking methods in the context of image retrieval, we propose to employ an efficient online re-ranking process to mine more informative neighbors in a Contextually Affinitive (ConAff) Neighborhood, and then encourage the cross-view neighborhood consistency. To further mitigate the intrinsic neighborhood noises near cluster boundaries, we propose a progressively relaxed boundary filtering strategy to circumvent the issues brought by noisy neighbors. Our method can be easily integrated into the generic self-supervised frameworks and outperforms the state-of-the-art methods on several popular benchmarks",
    "keywords": [],
    "checked": true,
    "id": "dc8e1ee026489de583c28d8bf040e17ec9e91960",
    "semantic_title": "contextually affinitive neighborhood refinery for deep clustering",
    "citation_count": 0,
    "authors": [
      "Chunlin Yu",
      "Ye Shi",
      "Jingya Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/123fd8a56501194823c8e0dca00733df-Abstract-Conference.html": {
    "title": "Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives",
    "volume": "main",
    "abstract": "Given a set of calibrated images of a scene, we present an approach that produces a simple, compact, and actionable 3D world representation by means of 3D primitives. While many approaches focus on recovering high-fidelity 3D scenes, we focus on parsing a scene into mid-level 3D representations made of a small set of textured primitives. Such representations are interpretable, easy to manipulate and suited for physics-based simulations. Moreover, unlike existing primitive decomposition methods that rely on 3D input data, our approach operates directly on images through differentiable rendering. Specifically, we model primitives as textured superquadric meshes and optimize their parameters from scratch with an image rendering loss. We highlight the importance of modeling transparency for each primitive, which is critical for optimization and also enables handling varying numbers of primitives. We show that the resulting textured primitives faithfully reconstruct the input images and accurately model the visible 3D points, while providing amodal shape completions of unseen object regions. We compare our approach to the state of the art on diverse scenes from DTU, and demonstrate its robustness on real-life captures from BlendedMVS and Nerfstudio. We also showcase how our results can be used to effortlessly edit a scene or perform physical simulations. Code and video results are available at https://www.tmonnier.com/DBW",
    "keywords": [],
    "checked": true,
    "id": "c03ca17b633f26c003b4f9c73a0c7fcbf8f6fff4",
    "semantic_title": "differentiable blocks world: qualitative 3d decomposition by rendering primitives",
    "citation_count": 3,
    "authors": [
      "Tom Monnier",
      "Jake Austin",
      "Angjoo Kanazawa",
      "Alexei Efros",
      "Mathieu Aubry"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/124dde499d62b58e97e42a45b26d7369-Abstract-Conference.html": {
    "title": "Learning Shared Safety Constraints from Multi-task Demonstrations",
    "volume": "main",
    "abstract": "Regardless of the particular task we want to perform in an environment, there are often shared safety constraints we want our agents to respect. For example, regardless of whether it is making a sandwich or clearing the table, a kitchen robot should not break a plate. Manually specifying such a constraint can be both time-consuming and error-prone. We show how to learn constraints from expert demonstrations of safe task completion by extending inverse reinforcement learning (IRL) techniques to the space of constraints. Intuitively, we learn constraints that forbid highly rewarding behavior that the expert could have taken but chose not to. Unfortunately, the constraint learning problem is rather ill-posed and typically leads to overly conservative constraints that forbid all behavior that the expert did not take. We counter this by leveraging diverse demonstrations that naturally occur in multi-task setting to learn a tighter set of constraints. We validate our method with simulation experiments on high-dimensional continuous control tasks",
    "keywords": [],
    "checked": true,
    "id": "a22b88c046bd84d29ae592ba1e247e7891476920",
    "semantic_title": "learning shared safety constraints from multi-task demonstrations",
    "citation_count": 2,
    "authors": [
      "Konwoo Kim",
      "Gokul Swamy",
      "ZUXIN LIU",
      "DING ZHAO",
      "Sanjiban Choudhury",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1289f9195d2ef8cfdfe5f50930c4a7c4-Abstract-Conference.html": {
    "title": "Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner",
    "volume": "main",
    "abstract": "Language models (LMs) trained on vast quantities of unlabelled data have greatly advanced the field of natural language processing (NLP). In this study, we re-visit the widely accepted notion in NLP that continued pre-training LMs on task-related texts improves the performance of fine-tuning (FT) in downstream tasks. Through experiments on eight single-sentence tasks and eight sentence-pair tasks in both semi-supervised and fully-supervised settings, we find that conventional continued pre-training does not consistently provide benefits and can even be detrimental for sentence-pair tasks or when prompt-based FT is used. To tackle these issues, we propose Prompt-based Continued Pre-training (PCP), which combines the idea of instruction tuning with conventional continued pre-training. Our approach aims to improve the performance of prompt-based FT by presenting both task-related texts and prompt templates to LMs through unsupervised pre-training objectives before fine-tuning for the target task. Our empirical evaluations on 21 benchmarks demonstrate that the PCP consistently improves the performance of state-of-the-art prompt-based FT approaches (up to 20.1% absolute) in both semi-supervised and fully-supervised settings, even with only hundreds of unlabelled examples. Additionally, prompt-based FT with PCP outperforms state-of-the-art semi-supervised approaches with greater simplicity, eliminating the need for an iterative process and extra data augmentation. Our further analysis explores the performance lower bound of the PCP and reveals that the advantages of PCP persist across different sizes of models and datasets",
    "keywords": [],
    "checked": true,
    "id": "c79852e9c9cc6734c9150847deb5449e489354ea",
    "semantic_title": "don't stop pretraining? make prompt-based fine-tuning powerful learner",
    "citation_count": 11,
    "authors": [
      "Zhengxiang Shi",
      "Aldo Lipani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/129033c7c08be683059559e8d6bfd460-Abstract-Conference.html": {
    "title": "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning",
    "volume": "main",
    "abstract": "Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset consisting of more than two thousand molecule tasks with corresponding instructions derived from task descriptions. We pretrain GIMLET on the molecule tasks along with instructions, enabling the model to transfer effectively to a broad range of tasks. Experimental results demonstrate that GIMLET significantly outperforms molecule-text baselines in instruction-based zero-shot learning, even achieving closed results to supervised GNN models on tasks such as toxcast and muv",
    "keywords": [],
    "checked": true,
    "id": "119a3ed0898499fce0ce6af6958d566d82390ba5",
    "semantic_title": "gimlet: a unified graph-text model for instruction-based molecule zero-shot learning",
    "citation_count": 15,
    "authors": [
      "Haiteng Zhao",
      "Shengchao Liu",
      "Ma Chang",
      "Hannan Xu",
      "Jie Fu",
      "Zhihong Deng",
      "Lingpeng Kong",
      "Qi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1297ca5c906f4bada8f5f6f4e80f9dd2-Abstract-Conference.html": {
    "title": "GEX: A flexible method for approximating influence via Geometric Ensemble",
    "volume": "main",
    "abstract": "Through a deeper understanding of predictions of neural networks, Influence Function (IF) has been applied to various tasks such as detecting and relabeling mislabeled samples, dataset pruning, and separation of data sources in practice. However, we found standard approximations of IF suffer from performance degradation due to oversimplified influence distributions caused by their bilinear approximation, suppressing the expressive power of samples with a relatively strong influence. To address this issue, we propose a new interpretation of existing IF approximations as an average relationship between two linearized losses over parameters sampled from the Laplace approximation (LA). In doing so, we highlight two significant limitations of current IF approximations: the linearity of gradients and the singularity of Hessian. Accordingly, by improving each point, we introduce a new IF approximation method with the following features: i) the removal of linearization to alleviate the bilinear constraint and ii) the utilization of Geometric Ensemble (GE) tailored for non-linear losses. Empirically, our approach outperforms existing IF approximations for downstream tasks with lighter computation, thereby providing new feasibility of low-complexity/nonlinear-based IF design",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SungYub Kim",
      "Kyungsu Kim",
      "Eunho Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/12bcf58a1c09a0fcb5310f3589291ab4-Abstract-Conference.html": {
    "title": "Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) has shown great promise for developing agents for dialogue management (DM) that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite the advancements in RL and language models (LMs), employing RL to drive conversational chatbots still poses significant challenges. A primary issue stems from RL's dependency on online exploration for effective learning, a process that can be costly. Moreover, engaging in online interactions with humans during the training phase can raise safety concerns, as the LM can potentially generate unwanted outputs. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop various RL algorithms, specialized in dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs)---models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting the MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM. We evaluate our methods in open-domain dialogue to demonstrate their effectiveness with respect to the diversity of intent in generated utterances and overall DM performance",
    "keywords": [],
    "checked": true,
    "id": "d7b704e886fd0f6bd4a0604eb2496f2c25ca5bcf",
    "semantic_title": "offline reinforcement learning for mixture-of-expert dialogue management",
    "citation_count": 0,
    "authors": [
      "Dhawal Gupta",
      "Yinlam Chow",
      "Azamat Tulepbergenov",
      "Mohammad Ghavamzadeh",
      "Craig Boutilier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/12c118ef87fde56a10bd858842781b34-Abstract-Conference.html": {
    "title": "Binary Classification with Confidence Difference",
    "volume": "main",
    "abstract": "Recently, learning with soft labels has been shown to achieve better performance than learning with hard labels in terms of model generalization, calibration, and robustness. However, collecting pointwise labeling confidence for all training examples can be challenging and time-consuming in real-world scenarios. This paper delves into a novel weakly supervised binary classification problem called confidence-difference (ConfDiff) classification. Instead of pointwise labeling confidence, we are given only unlabeled data pairs with confidence difference that specifies the difference in the probabilities of being positive. We propose a risk-consistent approach to tackle this problem and show that the estimation error bound achieves the optimal convergence rate. We also introduce a risk correction approach to mitigate overfitting problems, whose consistency and convergence rate are also proven. Extensive experiments on benchmark data sets and a real-world recommender system data set validate the effectiveness of our proposed approaches in exploiting the supervision information of the confidence difference",
    "keywords": [],
    "checked": true,
    "id": "c71cf5051e91e0bd2602369e2d29aa465ccea9df",
    "semantic_title": "binary classification with confidence difference",
    "citation_count": 1,
    "authors": [
      "Wei Wang",
      "Lei Feng",
      "Yuchen Jiang",
      "Gang Niu",
      "Min-Ling Zhang",
      "Masashi Sugiyama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/12d286282e1be5431ea05262a21f415c-Abstract-Conference.html": {
    "title": "On student-teacher deviations in distillation: does it pay to disobey?",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) has been widely used to improve the test accuracy of a \"student\" network, by training it to mimic the soft probabilities of a trained \"teacher\" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student may not only significantly deviate from the teacher probabilities, but may also outdo than the teacher in performance. Our work aims to reconcile this seemingly paradoxical observation. Specifically, we characterize the precise nature of the student-teacher deviations, and argue how they can co-occur with better generalization. First, through experiments on image and language data, we identify that these probability deviations correspond to the student systematically exaggerating the confidence levels of the teacher.Next, we theoretically and empirically establish another form of exaggeration in some simple settings: KD exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, we tie these two observations together: we demonstrate that the exaggerated bias of KD can simultaneously result in both (a) the exaggeration of confidence and (b) the improved generalization of the student, thus offering a resolution to the apparent paradox. Our analysis brings existing theory and practice closer by considering the role of gradient descent in KD and by demonstrating the exaggerated bias effect in both theoretical and empirical settings",
    "keywords": [],
    "checked": true,
    "id": "f6a1cfc2c802d15db8e118c63dd299ca9b9b3b91",
    "semantic_title": "on student-teacher deviations in distillation: does it pay to disobey?",
    "citation_count": 0,
    "authors": [
      "Vaishnavh Nagarajan",
      "Aditya K. Menon",
      "Srinadh Bhojanapalli",
      "Hossein Mobahi",
      "Sanjiv Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/12d7ba753894ed348904df1bf0ce02ec-Abstract-Conference.html": {
    "title": "Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis",
    "volume": "main",
    "abstract": "We introduce Resilient Multiple Choice Learning (rMCL), an extension of the MCL approach for conditional distribution estimation in regression settings where multiple targets may be sampled for each training input.Multiple Choice Learning is a simple framework to tackle multimodal density estimation, using the Winner-Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing MCL variants focus on merging the hypotheses, thereby eventually sacrificing the diversity of the predictions. In contrast, our method relies on a novel learned scoring scheme underpinned by a mathematical framework based on Voronoi tessellations of the output space, from which we can derive a probabilistic interpretation.After empirically validating rMCL with experiments on synthetic data, we further assess its merits on the sound source localization problem, demonstrating its practical usefulness and the relevance of its interpretation",
    "keywords": [],
    "checked": true,
    "id": "b99e58229dd7f2a5c48eefa0508c629d9c4c2b91",
    "semantic_title": "resilient multiple choice learning: a learned scoring scheme with application to audio scene analysis",
    "citation_count": 1,
    "authors": [
      "Victor Letzelter",
      "Mathieu Fontaine",
      "Mickael Chen",
      "Patrick Pérez",
      "Slim Essid",
      "Gaël Richard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/12da92b7c64176eb6eb6ad0ae31554fd-Abstract-Conference.html": {
    "title": "Graph of Circuits with GNN for Exploring the Optimal Design Space",
    "volume": "main",
    "abstract": "The design automation of analog circuits poses significant challenges in terms of the large design space, complex interdependencies between circuit specifications, and resource-intensive simulations. To address these challenges, this paper presents an innovative framework called the Graph of Circuits Explorer (GCX). Leveraging graph structure learning along with graph neural networks, GCX enables the creation of a surrogate model that facilitates efficient exploration of the optimal design space within a semi-supervised learning framework which reduces the need for large labelled datasets. The proposed approach comprises three key stages. First, we learn the geometric representation of circuits and enrich it with technology information to create a comprehensive feature vector. Subsequently, integrating feature-based graph learning with few-shot and zero-shot learning enhances the generalizability in predictions for unseen circuits. Finally, we introduce two algorithms namely, EASCO and ASTROG which upon integration with GCX optimize the available samples to yield the optimal circuit configuration meeting the designer's criteria. The effectiveness of the proposed approach is demonstrated through simulated performance evaluation of various circuits, using derived parameters in 180nm CMOS technology. Furthermore, the generalizability of the approach is extended to higher-order topologies and different technology nodes such as 65nm and 45nm CMOS process nodes",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Shahane",
      "Saripilli Swapna Manjiri",
      "Ankesh Jain",
      "Sandeep Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/13183a224208671a6fc33ba1aa661ec4-Abstract-Conference.html": {
    "title": "Structure-free Graph Condensation: From Large-scale Graphs to Condensed Graph-free Data",
    "volume": "main",
    "abstract": "Graph condensation, which reduces the size of a large-scale graph by synthesizing a small-scale condensed graph as its substitution, has immediate benefits for various graph learning tasks.However, existing graph condensation methods rely on the joint optimization of nodes and structures in the condensed graph, and overlook critical issues in effectiveness and generalization ability.In this paper, we advocate a new Structure-Free Graph Condensation paradigm, named SFGC, to distill a large-scale graph into a small-scale graph node set without explicit graph structures, i.e., graph-free data.Our idea is to implicitly encode topology structure information into the node attributes in the synthesized graph-free data, whose topology is reduced to an identity matrix.Specifically, SFGC contains two collaborative components: (1) a training trajectory meta-matching scheme for effectively synthesizing small-scale graph-free data;(2) a graph neural feature score metric for dynamically evaluating the quality of the condensed data. Through training trajectory meta-matching, SFGC aligns the long-term GNN learning behaviors between the large-scale graph and the condensed small-scale graph-free data, ensuring comprehensive and compact transfer of informative knowledge to the graph-free data.Afterward, the underlying condensed graph-free data would be dynamically evaluated with the graph neural feature score, which is a closed-form metric for ensuring the excellent expressiveness of the condensed graph-free data.Extensive experiments verify the superiority of SFGC across different condensation ratios",
    "keywords": [],
    "checked": true,
    "id": "7975b4a71236acb1e48714e8dd7a2a770bf3c0a3",
    "semantic_title": "structure-free graph condensation: from large-scale graphs to condensed graph-free data",
    "citation_count": 18,
    "authors": [
      "Xin Zheng",
      "Miao Zhang",
      "Chunyang Chen",
      "Quoc Viet Hung Nguyen",
      "Xingquan Zhu",
      "Shirui Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/13250eb13871b3c2c0a0667b54bad165-Abstract-Conference.html": {
    "title": "Visual Programming for Step-by-Step Text-to-Image Generation and Evaluation",
    "volume": "main",
    "abstract": "As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can only handle predefined object classes. We demonstrate that our VPGen has improved control in counts/spatial relations/scales of objects than state-of-the-art T2I generation models. Second, we introduce VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming. Unlike previous T2I evaluations with a single scoring model that is accurate in some skills but unreliable in others, VPEval produces evaluation programs that invoke a set of visual modules that are experts in different skills, and also provides visual+textual explanations of the evaluation results. Our analysis shows that VPEval provides a more human-correlated evaluation for skill-specific and open-ended prompts than widely used single model-based evaluation. We hope that our work encourages future progress on interpretable/explainable generation and evaluation for T2I models",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemin Cho",
      "Abhay Zala",
      "Mohit Bansal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1338c277525011f20166cf740952bb47-Abstract-Conference.html": {
    "title": "Auditing Fairness by Betting",
    "volume": "main",
    "abstract": "We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics---the ``testing by betting'' framework in particular. These connections ensure that our methods are interpretable, fast, and easy to implement. We demonstrate the efficacy of our approach on three benchmark fairness datasets",
    "keywords": [],
    "checked": true,
    "id": "40eabf1cc9a1216f941847b21c3c98244d77e703",
    "semantic_title": "auditing fairness by betting",
    "citation_count": 1,
    "authors": [
      "Ben Chugg",
      "Santiago Cortes-Gomez",
      "Bryan Wilder",
      "Aaditya Ramdas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1343edb2739a61a6e20bd8764e814b50-Abstract-Conference.html": {
    "title": "Truly Scale-Equivariant Deep Nets with Fourier Layers",
    "volume": "main",
    "abstract": "In computer vision, models must be able to adapt to changes in image resolution to effectively carry out tasks such as image segmentation; This is known as scale-equivariance. Recent works have made progress in developing scale-equivariant convolutional neural networks, e.g., through weight-sharing and kernel resizing. However, these networks are not truly scale-equivariant in practice. Specifically, they do not consider anti-aliasing as they formulate the down-scaling operation in the continuous domain. To address this shortcoming, we directly formulate down-scaling in the discrete domain with consideration of anti-aliasing. We then propose a novel architecture based on Fourier layers to achieve truly scale-equivariant deep nets, i.e., absolute zero equivariance-error. Following prior works, we test this model on MNIST-scale and STL-10 datasets. Our proposed model achieves competitive classification performance while maintaining zero equivariance-error",
    "keywords": [],
    "checked": true,
    "id": "7ce22447e68e5ff598c667e21e349d995cf159e0",
    "semantic_title": "truly scale-equivariant deep nets with fourier layers",
    "citation_count": 0,
    "authors": [
      "Md Ashiqur Rahman",
      "Raymond A. Yeh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/136729ae4b0fee25a0d28077442506da-Abstract-Conference.html": {
    "title": "Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem",
    "volume": "main",
    "abstract": "In this paper, we study a class of stochastic bilevel optimization problems, also known as stochastic simple bilevel optimization, where we minimize a smooth stochastic objective function over the optimal solution set of another stochastic convex optimization problem. We introduce novel stochastic bilevel optimization methods that locally approximate the solution set of the lower-level problem via a stochastic cutting plane, and then run a conditional gradient update with variance reduction techniques to control the error induced by using stochastic gradients. For the case that the upper-level function is convex, our method requires $\\mathcal{O}(\\max\\\\{1/\\epsilon_f^{2},1/\\epsilon_g^{2}\\\\}) $ stochastic oracle queries to obtain a solution that is $\\epsilon_f$-optimal for the upper-level and $\\epsilon_g$-optimal for the lower-level. This guarantee improves the previous best-known complexity of $\\mathcal{O}(\\max\\\\{1/\\epsilon_f^{4},1/\\epsilon_g^{4}\\\\})$. Moreover, for the case that the upper-level function is non-convex, our method requires at most $\\mathcal{O}(\\max\\\\{1/\\epsilon_f^{3},1/\\epsilon_g^{3}\\\\}) $ stochastic oracle queries to find an $(\\epsilon_f, \\epsilon_g)$-stationary point. In the finite-sum setting, we show that the number of stochastic oracle calls required by our method are $\\mathcal{O}(\\sqrt{n}/\\epsilon)$ and $\\mathcal{O}(\\sqrt{n}/\\epsilon^{2})$ for the convex and non-convex settings, respectively, where $\\epsilon=\\min \\\\{\\epsilon_f,\\epsilon_g\\\\}$",
    "keywords": [],
    "checked": true,
    "id": "d5536f4090cedfe2d89d1b39ee39f923815dff00",
    "semantic_title": "projection-free methods for stochastic simple bilevel optimization with convex lower-level problem",
    "citation_count": 4,
    "authors": [
      "Jincheng Cao",
      "Ruichen Jiang",
      "Nazanin Abolfazli",
      "Erfan Yazdandoost Hamedani",
      "Aryan Mokhtari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/136a45cd9b841bf785625709a19c6508-Abstract-Conference.html": {
    "title": "On the Implicit Bias of Linear Equivariant Steerable Networks",
    "volume": "main",
    "abstract": "We study the implicit bias of gradient flow on linear equivariant steerable networks in group-invariant binary classification. Our findings reveal that the parameterized predictor converges in direction to the unique group-invariant classifier with a maximum margin defined by the input group action. Under a unitary assumption on the input representation, we establish the equivalence between steerable networks and data augmentation. Furthermore, we demonstrate the improved margin and generalization bound of steerable networks over their non-invariant counterparts",
    "keywords": [],
    "checked": false,
    "id": "d6a971480bc2b4b39c4039080545bd8ae3eadb3c",
    "semantic_title": "on the implicit bias of linear equivariant steerable networks: margin, generalization, and their equivalence to data augmentation",
    "citation_count": 1,
    "authors": [
      "Ziyu Chen",
      "Wei Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1395b425d06a50e42fafe91cf04f3a98-Abstract-Conference.html": {
    "title": "Memory-Constrained Algorithms for Convex Optimization",
    "volume": "main",
    "abstract": "We propose a family of recursive cutting-plane algorithms to solve feasibility problems with constrained memory, which can also be used for first-order convex optimization. Precisely, in order to find a point within a ball of radius $\\epsilon$ with a separation oracle in dimension $d$---or to minimize $1$-Lipschitz convex functions to accuracy $\\epsilon$ over the unit ball---our algorithms use $\\mathcal O(\\frac{d^2}{p}\\ln \\frac{1}{\\epsilon})$ bits of memory, and make $\\mathcal O((C\\frac{d}{p}\\ln \\frac{1}{\\epsilon})^p)$ oracle calls. The family is parametrized by $p\\in[d]$ and provides an oracle-complexity/memory trade-off in the sub-polynomial regime $\\ln\\frac{1}{\\epsilon}\\gg\\ln d$. While several works gave lower-bound trade-offs (impossibility results)---we explicit here their dependence with $\\ln\\frac{1}{\\epsilon}$, showing that these also hold in any sub-polynomial regime---to the best of our knowledge this is the first class of algorithms that provides a positive trade-off between gradient descent and cutting-plane methods in any regime with $\\epsilon\\leq 1/\\sqrt d$. The algorithms divide the $d$ variables into $p$ blocks and optimize over blocks sequentially, with approximate separation vectors constructed using a variant of Vaidya's method. In the regime $\\epsilon \\leq d^{-\\Omega(d)}$, our algorithm with $p=d$ achieves the information-theoretic optimal memory usage and improves the oracle-complexity of gradient descent",
    "keywords": [],
    "checked": false,
    "id": "7684c442498dd4bf1c7ff1e2df6c92c3c4743501",
    "semantic_title": "memory-constrained algorithms for convex optimization via recursive cutting-planes",
    "citation_count": 1,
    "authors": [
      "Moise Blanchard",
      "Junhui Zhang",
      "Patrick Jaillet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/13aef57cf532e88c476a10ff372e44e5-Abstract-Conference.html": {
    "title": "Nonparametric Boundary Geometry in Physics Informed Deep Learning",
    "volume": "main",
    "abstract": "Engineering design problems frequently require solving systems ofpartial differential equations with boundary conditions specified onobject geometries in the form of a triangular mesh. These boundarygeometries are provided by a designer and are problem dependent.The efficiency of the design process greatly benefits from fast turnaroundtimes when repeatedly solving PDEs on various geometries. However,most current work that uses machine learning to speed up the solutionprocess relies heavily on a fixed parameterization of the geometry, whichcannot be changed after training. This severely limits the possibility ofreusing a trained model across a variety of design problems.In this work, we propose a novel neural operator architecture which acceptsboundary geometry, in the form of triangular meshes, as input and produces anapproximate solution to a given PDE as output. Once trained, the model can beused to rapidly estimate the PDE solution over a new geometry, without the need forretraining or representation of the geometry to a pre-specified parameterization",
    "keywords": [],
    "checked": false,
    "id": "512335b665b8733b2e8783cb76f49d9765264fbe",
    "semantic_title": "response of buried pipelines under permanent ground movements: physics-informed deep neural network approach",
    "citation_count": 0,
    "authors": [
      "Scott Cameron",
      "Arnu Pretorius",
      "S Roberts"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/13b501c58ae3bfe9635a259f4414e943-Abstract-Conference.html": {
    "title": "Tracking Most Significant Shifts in Nonparametric Contextual Bandits",
    "volume": "main",
    "abstract": "We study nonparametric contextual bandits where Lipschitz mean reward functions may change over time.We first establish the minimax dynamic regret rate in this less understood setting in terms of number of changes $L$ and total-variation $V$, both capturing all changes in distribution over context space, and argue that state-of-the-art procedures are suboptimal in this setting.Next, we tend to the question of an _adaptivity_ for this setting, i.e. achieving the minimax rate without knowledge of $L$ or $V$. Quite importantly, we posit that the bandit problem, viewed locally at a given context $X_t$, should not be affected by reward changes in other parts of context space $\\cal X$. We therefore propose a notion of _change_, which we term _experienced significant shifts_, that better accounts for locality, and thus counts considerably less changes than $L$ and $V$. Furthermore, similar to recent work on non-stationary MAB (Suk & Kpotufe, 2022), _experienced significant shifts_ only count the most _significant_ changes in mean rewards, e.g., severe best-arm changes relevant to observed contexts.Our main result is to show that this more tolerant notion of change can in fact be adapted to",
    "keywords": [],
    "checked": true,
    "id": "1c0fa22e1f505e3e21bcdbb16bcaad2e90a873b1",
    "semantic_title": "tracking most significant shifts in nonparametric contextual bandits",
    "citation_count": 0,
    "authors": [
      "Joe Suk",
      "Samory Kpotufe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/13f1750b825659394a6499399e7637fc-Abstract-Conference.html": {
    "title": "Empowering Collaborative Filtering with Principled Adversarial Contrastive Loss",
    "volume": "main",
    "abstract": "Contrastive Learning (CL) has achieved impressive performance in self-supervised learning tasks, showing superior generalization ability. Inspired by the success, adopting CL into collaborative filtering (CF) is prevailing in semi-supervised topK recommendations. The basic idea is to routinely conduct heuristic-based data augmentation and apply contrastive losses (e.g., InfoNCE) on the augmented views. Yet, some CF-tailored challenges make this adoption suboptimal, such as the issue of out-of-distribution, the risk of false negatives, and the nature of top-K evaluation. They necessitate the CL-based CF scheme to focus more on mining hard negatives and distinguishing false negatives from the vast unlabeled user-item interactions, for informative contrast signals. Worse still, there is limited understanding of contrastive loss in CF methods, especially w.r.t. its generalization ability. To bridge the gap, we delve into the reasons underpinning the success of contrastive loss in CF, and propose a principled Adversarial InfoNCE loss (AdvInfoNCE), which is a variant of InfoNCE, specially tailored for CF methods. AdvInfoNCE adaptively explores and assigns hardness to each negative instance in an adversarial fashion and further utilizes a fine-grained hardness-aware ranking criterion to empower the recommender's generalization ability. Training CF models with AdvInfoNCE, we validate the effectiveness of AdvInfoNCE on both synthetic and real-world benchmark datasets, thus showing its generalization ability to mitigate out-of-distribution problems. Given the theoretical guarantees and empirical superiority of AdvInfoNCE over most contrastive loss functions, we advocate its adoption as a standard loss in recommender systems, particularly for the out-of-distribution tasks. Codes are available at https://github.com/LehengTHU/AdvInfoNCE",
    "keywords": [],
    "checked": true,
    "id": "96d47d8e9bdbd88161ef4d807f5add0b42c58a41",
    "semantic_title": "empowering collaborative filtering with principled adversarial contrastive loss",
    "citation_count": 2,
    "authors": [
      "An Zhang",
      "Leheng Sheng",
      "Zhibo Cai",
      "Xiang Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1403ab1a427050538ec59c7f570aec8b-Abstract-Conference.html": {
    "title": "The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance",
    "volume": "main",
    "abstract": "Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We demonstrate through experiments that our framework recovers variable importance rankings for complex simulation setups where other methods fail. Further, we show that our framework accurately estimates the true importance of a variable for the underlying data distribution. We provide theoretical guarantees on the consistency and finite sample error rates for our estimator. Finally, we demonstrate its utility with a real-world case study exploring which genes are important for predicting HIV load in persons with HIV, highlighting an important gene that has not previously been studied in connection with HIV",
    "keywords": [],
    "checked": true,
    "id": "a0ab2a87324bd3df4c7dd89d8b372b90e232934b",
    "semantic_title": "the rashomon importance distribution: getting rid of unstable, single model-based variable importance",
    "citation_count": 1,
    "authors": [
      "Jon Donnelly",
      "Srikar Katta",
      "Cynthia Rudin",
      "Edward Browne"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/142cdba4b8d1e03f9ee131ac86bb0afc-Abstract-Conference.html": {
    "title": "Model-Based Control with Sparse Neural Dynamics",
    "volume": "main",
    "abstract": "Learning predictive models from observations using deep neural networks (DNNs) is a promising new approach to many real-world planning and control problems. However, common DNNs are too unstructured for effective planning, and current control methods typically rely on extensive sampling or local gradient descent. In this paper, we propose a new framework for integrated model learning and predictive control that is amenable to efficient optimization algorithms. Specifically, we start with a ReLU neural model of the system dynamics and, with minimal losses in prediction accuracy, we gradually sparsify it by removing redundant neurons. This discrete sparsification process is approximated as a continuous problem, enabling an end-to-end optimization of both the model architecture and the weight parameters. The sparsified model is subsequently used by a mixed-integer predictive controller, which represents the neuron activations as binary variables and employs efficient branch-and-bound algorithms. Our framework is applicable to a wide variety of DNNs, from simple multilayer perceptrons to complex graph neural dynamics. It can efficiently handle tasks involving complicated contact dynamics, such as object pushing, compositional object sorting, and manipulation of deformable objects. Numerical and hardware experiments show that, despite the aggressive sparsification, our framework can deliver better closed-loop performance than existing state-of-the-art methods",
    "keywords": [],
    "checked": true,
    "id": "3b701564d0d24402268176c34b03da125d56f7c9",
    "semantic_title": "model-based control with sparse neural dynamics",
    "citation_count": 1,
    "authors": [
      "Ziang Liu",
      "Genggeng Zhou",
      "Jeff He",
      "Tobia Marcucci",
      "Fei-Fei Li",
      "Jiajun Wu",
      "Yunzhu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1456560769bbc38e4f8c5055048ea712-Abstract-Conference.html": {
    "title": "AmadeusGPT: a natural language interface for interactive animal behavioral analysis",
    "volume": "main",
    "abstract": "The process of quantifying and analyzing animal behavior involves translating the naturally occurring descriptive language of their actions into machine-readable code. Yet, codifying behavior analysis is often challenging without deep understanding of animal behavior and technical machine learning knowledge. To limit this gap, we introduce AmadeusGPT: a natural language interface that turns natural language descriptions of behaviors into machine-executable code. Large-language models (LLMs) such as GPT3.5 and GPT4 allow for interactive language-based queries that are potentially well suited for making interactive behavior analysis. However, the comprehension capability of these LLMs is limited by the context window size, which prevents it from remembering distant conversations. To overcome the context window limitation, we implement a novel dual-memory mechanism to allow communication between short-term and long-term memory using symbols as context pointers for retrieval and saving. Concretely, users directly use language-based definitions of behavior and our augmented GPT develops code based on the core AmadeusGPT API, which contains machine learning, computer vision, spatio-temporal reasoning, and visualization modules. Users then can interactively refine results, and seamlessly add new behavioral modules as needed. We used the MABe 2022 behavior challenge tasks to benchmark AmadeusGPT and show excellent performance. Note, an end-user would not need to write any code to achieve this. Thus, collectively AmadeusGPT presents a novel way to merge deep biological knowledge, large-language models, and core computer vision modules into a more naturally intelligent system. Code and demos can be found at: https://github.com/AdaptiveMotorControlLab/AmadeusGPT",
    "keywords": [],
    "checked": true,
    "id": "ec592e12f45e20819afe203164bbbd0de8990510",
    "semantic_title": "amadeusgpt: a natural language interface for interactive animal behavioral analysis",
    "citation_count": 3,
    "authors": [
      "Shaokai Ye",
      "Jessy Lauer",
      "Mu Zhou",
      "Alexander Mathis",
      "Mackenzie Mathis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/145c28cd4b1df9b426990fd68045f4f7-Abstract-Conference.html": {
    "title": "Provably Efficient Algorithm for Nonstationary Low-Rank MDPs",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) under changing environment models many real-world applications via nonstationary Markov Decision Processes (MDPs), and hence gains considerable interest. However, theoretical studies on nonstationary MDPs in the literature have mainly focused on tabular and linear (mixture) MDPs, which do not capture the nature of unknown representation in deep RL. In this paper, we make the first effort to investigate nonstationary RL under episodic low-rank MDPs, where both transition kernels and rewards may vary over time, and the low-rank model contains unknown representation in addition to the linear state embedding function. We first propose a parameter-dependent policy optimization algorithm called PORTAL,and further improve PORTAL to its parameter-free version of Ada-PORTAL, which is able to tune its hyper-parameters adaptively without any prior knowledge of nonstationarity. For both algorithms, we provide upper bounds on the average dynamic suboptimality gap, which show that as long as the nonstationarity is not significantly large, PORTAL and Ada-PORTAL are sample-efficient and can achieve arbitrarily small average dynamic suboptimality gap with polynomial sample complexity",
    "keywords": [],
    "checked": true,
    "id": "6c58a3bfa9b7f882a166cd2ab246285a4e8c6823",
    "semantic_title": "provably efficient algorithm for nonstationary low-rank mdps",
    "citation_count": 1,
    "authors": [
      "Yuan Cheng",
      "Jing Yang",
      "Yingbin Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/148bbc25b934211d80435b5cad5a7198-Abstract-Conference.html": {
    "title": "Time-uniform confidence bands for the CDF under nonstationarity",
    "volume": "main",
    "abstract": "Estimation of a complete univariate distribution from a sequence of observations is a useful primitive for both manual and automated decision making. This problem has received extensive attention in the i.i.d. setting, but the arbitrary data dependent setting remains largely unaddressed. We present computationally felicitous time-uniform and value-uniform bounds on the CDF of the running averaged conditional distribution of a sequence of real-valued random variables. Consistent with known impossibility results, our CDF bounds are always valid but sometimes trivial when the instance is too hard, and we give an instance-dependent convergence guarantee. The importance-weighted extension is appropriate for estimating complete counterfactual distributions of rewards given data from a randomized experiment, e.g., from an A/B test or a contextual bandit",
    "keywords": [],
    "checked": true,
    "id": "56d3323927bfbb7dcd4742f24f5980d8b95099d6",
    "semantic_title": "time-uniform confidence bands for the cdf under nonstationarity",
    "citation_count": 0,
    "authors": [
      "Paul Mineiro",
      "Steven Howard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1498a03a04f9bcd3a7d44058fc5dc639-Abstract-Conference.html": {
    "title": "Risk-Averse Active Sensing for Timely Outcome Prediction under Cost Pressure",
    "volume": "main",
    "abstract": "Timely outcome prediction is essential in healthcare to enable early detection and intervention of adverse events. However, in longitudinal follow-ups to patients' health status, cost-efficient acquisition of patient covariates is usually necessary due to the significant expense involved in screening and lab tests. To balance the timely and accurate outcome predictions with acquisition costs, an effective active sensing strategy is crucial. In this paper, we propose a novel risk-averse active sensing approach RAS that addresses the composite decision problem of when to conduct the acquisition and which measurements to make. Our approach decomposes the policy into two sub-policies: acquisition scheduler and feature selector, respectively. Moreover, we introduce a novel risk-aversion training strategy to focus on the underrepresented subgroup of high-risk patients for whom timely and accurate prediction of disease progression is of greater value. Our method outperforms baseline active sensing approaches in experiments with both synthetic and real-world datasets, and we illustrate the significance of our policy decomposition and the necessity of a risk-averse sensing policy through case studies",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchao Qin",
      "Mihaela van der Schaar",
      "Changhee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/149ad6e32c08b73a3ecc3d11977fcc47-Abstract-Conference.html": {
    "title": "Single-Pass Pivot Algorithm for Correlation Clustering. Keep it simple!",
    "volume": "main",
    "abstract": "We show that a simple single-pass semi-streaming variant of the Pivot algorithm for Correlation Clustering gives a (3+eps)-approximation using O(n/eps) words of memory. This is a slight improvement over the recent results of Cambus, Kuhn, Lindy, Pai, and Uitto, who gave a (3+eps)-approximation using O(n log n) words of memory, and Behnezhad, Charikar, Ma, and Tan, who gave a 5-approximation using O(n) words of memory. One of the main contributions of our paper is that the algorithm and its analysis are simple and easy to understand",
    "keywords": [],
    "checked": true,
    "id": "cc25365115caace3df7491ff1706b767a70bca67",
    "semantic_title": "single-pass pivot algorithm for correlation clustering. keep it simple!",
    "citation_count": 4,
    "authors": [
      "Konstantin Makarychev",
      "Sayak Chakrabarty"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/14a812fa4b6bf244d055e37a7cd2f557-Abstract-Conference.html": {
    "title": "SPACE: Single-round Participant Amalgamation for Contribution Evaluation in Federated Learning",
    "volume": "main",
    "abstract": "The evaluation of participant contribution in federated learning (FL) has recently gained significant attention due to its applicability in various domains, such as incentive mechanisms, robustness enhancement, and client selection. Previous approaches have predominantly relied on the widely adopted Shapley value for participant evaluation. However, the computation of the Shapley value is expensive, despite using techniques like gradient-based model reconstruction and truncating unnecessary evaluations. Therefore, we present an efficient approach called Single-round Participants Amalgamation for Contribution Evaluation (SPACE). SPACE incorporates two novel components, namely Federated Knowledge Amalgamation and Prototype-based Model Evaluation to reduce the evaluation effort by eliminating the dependence on the size of the validation set and enabling participant evaluation within a single communication round. Experimental results demonstrate that SPACE outperforms state-of-the-art methods in terms of both running time and Pearson's Correlation Coefficient (PCC). Furthermore, extensive experiments conducted on applications, client reweighting, and client selection highlight the effectiveness of SPACE. The code is available at https://github.com/culiver/SPACE",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Chung Chen",
      "Hsi-Wen Chen",
      "Shun-Gui Wang",
      "Ming-syan Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/14cdc9013d80338bf81483a7736ea05c-Abstract-Conference.html": {
    "title": "SAME: Uncovering GNN Black Box with Structure-aware Shapley-based Multipiece Explanations",
    "volume": "main",
    "abstract": "Post-hoc explanation techniques on graph neural networks (GNNs) provide economical solutions for opening the black-box graph models without model retraining. Many GNN explanation variants have achieved state-of-the-art explaining results on a diverse set of benchmarks, while they rarely provide theoretical analysis for their inherent properties and explanatory capability. In this work, we propose $\\underline{\\text{S}}$tructure-$\\underline{\\text{A}}$ware Shapley-based $\\underline{\\text{M}}$ultipiece $\\underline{\\text{E}}$xplanation (SAME) method to address the structure-aware feature interactions challenges for GNNs explanation. Specifically, SAME leverages an expansion-based Monte Carlo tree search to explore the multi-grained structure-aware connected substructure. Afterward, the explanation results are encouraged to be informative of the graph properties by optimizing the combination of distinct single substructures. With the consideration of fair feature interactions in the process of investigating multiple connected important substructures, the explanation provided by SAME has the potential to be as explainable as the theoretically optimal explanation obtained by the Shapley value within polynomial time. Extensive experiments on real-world and synthetic benchmarks show that SAME improves the previous state-of-the-art fidelity performance by 12.9\\% on BBBP, 7.01\\% on MUTAG, 42.3\\% on Graph-SST2, 38.9\\% on Graph-SST5, 11.3\\% on BA-2Motifs and 18.2\\% on BA-Shapes under the same testing condition. Code is available at https://github.com/same2023neurips/same",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyuan Ye",
      "Rihan Huang",
      "Qilin Wu",
      "Quanying Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/14ecbfb2216bab76195b60bfac7efb1f-Abstract-Conference.html": {
    "title": "Federated Learning with Client Subsampling, Data Heterogeneity, and Unbounded Smoothness: A New Algorithm and Lower Bounds",
    "volume": "main",
    "abstract": "We study the problem of Federated Learning (FL) under client subsampling and data heterogeneity with an objective function that has potentially unbounded smoothness. This problem is motivated by empirical evidence that the class of relaxed smooth functions, where the Lipschitz constant of the gradient scales linearly with the gradient norm, closely resembles the loss functions of certain neural networks such as recurrent neural networks (RNNs) with possibly exploding gradient. We introduce EPISODE++, the first algorithm to solve this problem. It maintains historical statistics for each client to construct control variates and decide clipping behavior for sampled clients in the current round. We prove that EPISODE++ achieves linear speedup in the number of participating clients, reduced communication rounds, and resilience to data heterogeneity. Our upper bound proof relies on novel techniques of recursively bounding the client updates under unbounded smoothness and client subsampling, together with a refined high probability analysis. In addition, we prove a lower bound showing that the convergence rate of a special case of clipped minibatch SGD (without randomness in the stochastic gradient and with randomness in client subsampling) suffers from an explicit dependence on the maximum gradient norm of the objective in a sublevel set, which may be large. This effectively demonstrates that applying gradient clipping to minibatch SGD in our setting does not eliminate the problem of exploding gradients. Our lower bound is based on new constructions of hard instances tailored to client subsampling and a novel analysis of the trajectory of the algorithm in the presence of clipping. Lastly, we provide an experimental evaluation of EPISODE++ when training RNNs on federated text classification tasks, demonstrating that EPISODE++ outperforms strong baselines in FL. The code is available at https://github.com/MingruiLiu-ML-Lab/episode_plusplus",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Crawshaw",
      "Yajie Bao",
      "Mingrui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1502957929fc4257dd1b6daf7d869c2f-Abstract-Conference.html": {
    "title": "Quantifying the Cost of Learning in Queueing Systems",
    "volume": "main",
    "abstract": "Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of the system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms. In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty.We characterize the CLQ of a single-queue multi-server system, and then extend these results to multi-queue multi-server systems and networks of queues. In establishing our results, we propose a unified analysis framework for CLQ that bridges Lyapunov and bandit analysis, provides guarantees for a wide range of algorithms, and could be of independent interest",
    "keywords": [],
    "checked": true,
    "id": "34986b23a6f96992231c5a0a8545ffce1c68d685",
    "semantic_title": "quantifying the cost of learning in queueing systems",
    "citation_count": 1,
    "authors": [
      "Daniel Freund",
      "Thodoris Lykouris",
      "Wentao Weng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1516a7f7507d5550db5c7f29e995ec8c-Abstract-Conference.html": {
    "title": "One-Line-of-Code Data Mollification Improves Optimization of Likelihood-based Generative Models",
    "volume": "main",
    "abstract": "Generative Models (GMs) have attracted considerable attention due to their tremendous success in various domains, such as computer vision where they are capable to generate impressive realistic-looking images. Likelihood-based GMs are attractive due to the possibility to generate new data by a single model evaluation. However, they typically achieve lower sample quality compared to state-of-the-art score-based Diffusion Models (DMs). This paper provides a significant step in the direction of addressing this limitation. The idea is to borrow one of the strengths of score-based DMs, which is the ability to perform accurate density estimation in low-density regions and to address manifold overfitting by means of data mollification. We propose a view of data mollification within likelihood-based GMs as a continuation method, whereby the optimization objective smoothly transitions from simple-to-optimize to the original target. Crucially, data mollification can be implemented by adding one line of code in the optimization loop, and we demonstrate that this provides a boost in generation quality of likelihood-based GMs, without computational overheads. We report results on real-world image data sets and UCI benchmarks with popular likelihood-based GMs, including variants of variational autoencoders and normalizing flows, showing large improvements in FID score and density estimation",
    "keywords": [],
    "checked": true,
    "id": "7323db3dcdcadd23df7322329b99e2392d4216b5",
    "semantic_title": "one-line-of-code data mollification improves optimization of likelihood-based generative models",
    "citation_count": 0,
    "authors": [
      "Ba-Hien Tran",
      "Giulio Franzese",
      "Pietro Michiardi",
      "Maurizio Filippone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15212bd2265c4a3ab0dbc1b1982c1b69-Abstract-Conference.html": {
    "title": "FLSL: Feature-level Self-supervised Learning",
    "volume": "main",
    "abstract": "Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a bi-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017. We conclude by presenting visualization and various ablation studies to better understand the success of FLSL. The source code is available at https://github.com/ISL-CV/FLSL",
    "keywords": [],
    "checked": true,
    "id": "df2117d3f35fe264bd68ea2f805023c888551b03",
    "semantic_title": "flsl: feature-level self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Qing Su",
      "Anton Netchaev",
      "Hai Li",
      "Shihao Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15294ba2dcfb4521274f7aa1c26f4dd4-Abstract-Conference.html": {
    "title": "FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning",
    "volume": "main",
    "abstract": "Exemplar-free class-incremental learning (CIL) poses several challenges since it prohibits the rehearsal of data from previous tasks and thus suffers from catastrophic forgetting. Recent approaches to incrementally learning the classifier by freezing the feature extractor after the first task have gained much attention. In this paper, we explore prototypical networks for CIL, which generate new class prototypes using the frozen feature extractor and classify the features based on the Euclidean distance to the prototypes. In an analysis of the feature distributions of classes, we show that classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous. To address this challenge, we revisit the anisotropic Mahalanobis distance for CIL. In addition, we empirically show that modeling the feature covariance relations is better than previous attempts at sampling features from normal distributions and training a linear classifier. Unlike existing methods, our approach generalizes to both many- and few-shot CIL settings, as well as to domain-incremental settings. Interestingly, without updating the backbone network, our method obtains state-of-the-art results on several standard continual learning benchmarks. Code is available at https://github.com/dipamgoswami/FeCAM",
    "keywords": [],
    "checked": true,
    "id": "4646aa53625f24d3f158c9f371fb0c10b712895c",
    "semantic_title": "fecam: exploiting the heterogeneity of class distributions in exemplar-free continual learning",
    "citation_count": 2,
    "authors": [
      "Dipam Goswami",
      "Yuyang Liu",
      "Bartłomiej Twardowski",
      "Joost van de Weijer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/154926e0b66e2b2a8c1120852f31a12d-Abstract-Conference.html": {
    "title": "Learning non-Markovian Decision-Making from State-only Sequences",
    "volume": "main",
    "abstract": "Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables $\\textit{decision-making as inference}$: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Markovian constraints and show that the learned model exhibits strong performances in challenging domains from the MuJoCo suite",
    "keywords": [],
    "checked": true,
    "id": "383b522e4c0584a04775bc258b334b89eb5301ad",
    "semantic_title": "learning non-markovian decision-making from state-only sequences",
    "citation_count": 0,
    "authors": [
      "Aoyang Qin",
      "Feng Gao",
      "Qing Li",
      "Song-Chun Zhu",
      "Sirui Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/154b90fcc9ba3dee96779c05c3108908-Abstract-Conference.html": {
    "title": "Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts",
    "volume": "main",
    "abstract": "Dynamic graph neural networks (DyGNNs) currently struggle with handling distribution shifts that are inherent in dynamic graphs.Existing work on DyGNNs with out-of-distribution settings only focuses on the time domain, failing to handle cases involving distribution shifts in the spectral domain. In this paper, we discover that there exist cases with distribution shifts unobservable in the time domain while observable in the spectral domain, and propose to study distribution shifts on dynamic graphs in the spectral domain for the first time.However, this investigation poses two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain; and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns. To address these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD), which can handle distribution shifts on dynamic graphs by capturing and utilizing invariant and variant spectral patterns. Specifically, we first design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums, allowing the mixed dynamic graph patterns to be transformed into separate frequency components. We then develop a disentangled spectrum mask to filter graph dynamics from various frequency components and discover the invariant and variant spectral patterns. Finally, we propose invariant spectral filtering, which encourages the model to rely on invariant patterns for generalization under distribution shifts. Experimental results on synthetic and real-world dynamic graph datasets demonstrate the superiority of our method for both node classification and link prediction tasks under distribution shifts",
    "keywords": [],
    "checked": false,
    "id": "405dfedb03258f36a4a9cdfd467dd6379616d346",
    "semantic_title": "environment-aware dynamic graph learning for out-of-distribution generalization",
    "citation_count": 1,
    "authors": [
      "Zeyang Zhang",
      "Xin Wang",
      "Ziwei Zhang",
      "Zhou Qin",
      "Weigao Wen",
      "Hui Xue'",
      "Haoyang Li",
      "Wenwu Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/154d63285d3ed7826e7f026c0b350d69-Abstract-Conference.html": {
    "title": "Efficient Activation Function Optimization through Surrogate Modeling",
    "volume": "main",
    "abstract": "Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activation functions in several real-world tasks, with a surprising finding: a sigmoidal design that outperformed all other activation functions was discovered, challenging the status quo of always using rectifier nonlinearities in deep learning. Each of these steps is a contribution in its own right; together they serve as a practical and theoretical foundation for further research on activation function optimization",
    "keywords": [],
    "checked": true,
    "id": "d71a2905f3546065cd3d7e767647e13ad3da56aa",
    "semantic_title": "efficient activation function optimization through surrogate modeling",
    "citation_count": 0,
    "authors": [
      "Garrett Bingham",
      "Risto Miikkulainen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1577ea3eaf8dacb99f64e4496c3ecddf-Abstract-Conference.html": {
    "title": "Data Market Design through Deep Learning",
    "volume": "main",
    "abstract": "The data market design problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle obedience constraints — these arising from modeling the downstream actions of buyers — in addition to incentive constraints on bids. Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs",
    "keywords": [],
    "checked": true,
    "id": "954a96cf14fa918fccf357730359d04847eed5ba",
    "semantic_title": "data market design through deep learning",
    "citation_count": 1,
    "authors": [
      "Sai Srivatsa Ravindranath",
      "Yanchen Jiang",
      "David C. Parkes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/157c30da6a988e1cbef2095f7b9521db-Abstract-Conference.html": {
    "title": "When Visual Prompt Tuning Meets Source-Free Domain Adaptive Semantic Segmentation",
    "volume": "main",
    "abstract": "Source-free domain adaptive semantic segmentation aims to adapt a pre-trained source model to the unlabeled target domain without accessing the private source data. Previous methods usually fine-tune the entire network, which suffers from expensive parameter tuning. To avoid this problem, we propose to utilize visual prompt tuning for parameter-efficient adaptation. However, the existing visual prompt tuning methods are unsuitable for source-free domain adaptive semantic segmentation due to the following two reasons: (1) Commonly used visual prompts like input tokens or pixel-level perturbations cannot reliably learn informative knowledge beneficial for semantic segmentation. (2) Visual prompts require sufficient labeled data to fill the gap between the pre-trained model and downstream tasks. To alleviate these problems, we propose a universal unsupervised visual prompt tuning (Uni-UVPT) framework, which is applicable to various transformer-based backbones. Specifically, we first divide the source pre-trained backbone with frozen parameters into multiple stages, and propose a lightweight prompt adapter for progressively encoding informative knowledge into prompts and enhancing the generalization of target features between adjacent backbone stages. Cooperatively, a novel adaptive pseudo-label correction strategy with a multiscale consistency loss is designed to alleviate the negative effect of target samples with noisy pseudo labels and raise the capacity of visual prompts to spatial perturbations. Extensive experiments demonstrate that Uni-UVPT achieves state-of-the-art performance on GTA5 $\\to$ Cityscapes and SYNTHIA $\\to$ Cityscapes tasks and can serve as a universal and parameter-efficient framework for large-model unsupervised knowledge transfer. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/uni-uvpt and https://github.com/huawei-noah/noah-research/tree/master/uni-uvpt",
    "keywords": [],
    "checked": false,
    "id": "a2f12e7c4b6bedd8cf758efb5b587cf988435092",
    "semantic_title": "fvp: fourier visual prompting for source-free unsupervised domain adaptation of medical image segmentation",
    "citation_count": 4,
    "authors": [
      "Xinhong Ma",
      "Yiming Wang",
      "Hao Liu",
      "Tianyu Guo",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15ce36d35622f126f38e90167de1a350-Abstract-Conference.html": {
    "title": "DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method",
    "volume": "main",
    "abstract": "This paper proposes a new easy-to-implement parameter-free gradient-based optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is efficient---matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and universal---automatically adapting to both smooth and nonsmooth problems. While popular algorithms following the AdaGrad framework compute a running average of the squared gradients, DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. To complement our theory, we also show empirically that DoWG trains at the edge of stability, and validate its effectiveness on practical machine learning tasks",
    "keywords": [],
    "checked": true,
    "id": "1c5829b0ca8d1aa46d6ace0a915218d8af24877e",
    "semantic_title": "dowg unleashed: an efficient universal parameter-free gradient descent method",
    "citation_count": 6,
    "authors": [
      "Ahmed Khaled",
      "Konstantin Mishchenko",
      "Chi Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15d15045f93b44d933a260b249608d43-Abstract-Conference.html": {
    "title": "Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning",
    "volume": "main",
    "abstract": "Multitask learning is a powerful framework that enables one to simultaneously learn multiple related tasks by sharing information between them. Quantifying uncertainty in the estimated tasks is of pivotal importance for many downstream applications, such as online or active learning. In this work, we provide novel confidence intervals for multitask regression in the challenging agnostic setting, i.e., when neither the similarity between tasks nor the tasks' features are available to the learner. The obtained intervals do not require i.i.d. data and can be directly applied to bound the regret in online learning. Through a refined analysis of the multitask information gain, we obtain new regret guarantees that, depending on a task similarity parameter, can significantly improve over treating tasks independently. We further propose a novel online learning algorithm that achieves such improved regret without knowing this parameter in advance, i.e., automatically adapting to task similarity. As a second key application of our results, we introduce a novel multitask active learning setup where several tasks must be simultaneously optimized, but only one of them can be queried for feedback by the learner at each round. For this problem, we design a no-regret algorithm that uses our confidence intervals to decide which task should be queried. Finally, we empirically validate our bounds and algorithms on synthetic and real-world (drug discovery) data",
    "keywords": [],
    "checked": true,
    "id": "b1271b0e0bb342f6a373bbe84d1f2396a7ebda06",
    "semantic_title": "multitask learning with no regret: from improved confidence bounds to active learning",
    "citation_count": 0,
    "authors": [
      "Pier Giuseppe Sessa",
      "Pierre Laforgue",
      "Nicolò Cesa-Bianchi",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15d3d4a4bd808605e3a3c1ea0fd0eba4-Abstract-Conference.html": {
    "title": "Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation",
    "volume": "main",
    "abstract": "Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce \\textit{Delayed-PSVI}, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves $\\widetilde{O}(\\sqrt{d^3H^3 T} + d^2H^2 \\mathbb{E}[\\tau])$ worst-case regret in the presence of unknown stochastic delays. Here $\\mathbb{E}[\\tau]$ is the expected delay. To further improve its computational efficiency and to expand its applicability in high-dimensional RL problems, we incorporate a gradient-based approximate sampling scheme via Langevin dynamics for \\textit{Delayed-LPSVI}, which maintains the same order-optimal regret guarantee with $\\widetilde{O}(dHK)$ computational cost. Empirical evaluations are performed to demonstrate the statistical and computational efficacy of our algorithms",
    "keywords": [],
    "checked": true,
    "id": "ec2aecde467988f67b6d190479cf4b7ae7806b8c",
    "semantic_title": "posterior sampling with delayed feedback for reinforcement learning with linear function approximation",
    "citation_count": 0,
    "authors": [
      "Nikki Lijing Kuang",
      "Ming Yin",
      "Mengdi Wang",
      "Yu-Xiang Wang",
      "Yian Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15d6717f8bb33b3a74df26ce1eee0b9a-Abstract-Conference.html": {
    "title": "Macro Placement by Wire-Mask-Guided Black-Box Optimization",
    "volume": "main",
    "abstract": "The development of very large-scale integration (VLSI) technology has posed new challenges for electronic design automation (EDA) techniques in chip floorplanning. During this process, macro placement is an important subproblem, which tries to determine the positions of all macros with the aim of minimizing half-perimeter wirelength (HPWL) and avoiding overlapping. Previous methods include packing-based, analytical and reinforcement learning methods. In this paper, we propose a new black-box optimization (BBO) framework (called WireMask-BBO) for macro placement, by using a wire-mask-guided greedy procedure for objective evaluation. Equipped with different BBO algorithms, WireMask-BBO empirically achieves significant improvements over previous methods, i.e., achieves significantly shorter HPWL by using much less time. Furthermore, it can fine-tune existing placements by treating them as initial solutions, which can bring up to 50% improvement in HPWL. WireMask-BBO has the potential to significantly improve the quality and efficiency of chip floorplanning, which makes it appealing to researchers and practitioners in EDA and will also promote the application of BBO. Our code is available at https://github.com/lamda-bbo/WireMask-BBO",
    "keywords": [],
    "checked": true,
    "id": "ebdaf870202fa1d3aa8c1b0a911bc8f4721b28a7",
    "semantic_title": "macro placement by wire-mask-guided black-box optimization",
    "citation_count": 0,
    "authors": [
      "Yunqi Shi",
      "Ke Xue",
      "Song Lei",
      "Chao Qian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15dc2344ea9bdc01ffb8bb2d692e4018-Abstract-Conference.html": {
    "title": "Reconciling Competing Sampling Strategies of Network Embedding",
    "volume": "main",
    "abstract": "Network embedding plays a significant role in a variety of applications. To capture the topology of the network, most of the existing network embedding algorithms follow a sampling training procedure, which maximizes the similarity (e.g., embedding vectors' dot product) between positively sampled node pairs and minimizes the similarity between negatively sampled node pairs in the embedding space. Typically, close node pairs function as positive samples while distant node pairs are usually considered as negative samples. However, under different or even competing sampling strategies, some methods champion sampling distant node pairs as positive samples to encapsulate longer distance information in link prediction, whereas others advocate adding close nodes into the negative sample set to boost the performance of node recommendation. In this paper, we seek to understand the intrinsic relationships between these competing strategies. To this end, we identify two properties (discrimination and monotonicity) that given any node pair proximity distribution, node embeddings should embrace.Moreover, we quantify the empirical error of the trained similarity score w.r.t. the sampling strategy, which leads to an important finding that the discrimination property and the monotonicity property for all node pairs can not be satisfied simultaneously in real-world applications. Guided by such analysis, a simple yet novel model (SENSEI) is proposed, which seamlessly fulfills the discrimination property and the partial monotonicity within the top-$K$ ranking list. Extensive experiments show that SENSEI outperforms the state-of-the-arts in plain network embedding",
    "keywords": [],
    "checked": false,
    "id": "e1968076e618e9d7b9a0e689d177eb01fc01859f",
    "semantic_title": "operationalizing the just city",
    "citation_count": 0,
    "authors": [
      "Yuchen Yan",
      "Baoyu Jing",
      "Lihui Liu",
      "Ruijie Wang",
      "Jinning Li",
      "Tarek Abdelzaher",
      "Hanghang Tong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15ddb1773510075ef44981cdb204330b-Abstract-Conference.html": {
    "title": "Zero-shot causal learning",
    "volume": "main",
    "abstract": "Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (e.g., a newly invented drug), which these methods do not address.Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, its recipients, and its nonrecipients. By leveraging both intervention information (e.g., a drug's attributes) and individual features (e.g., a patient's history), CaML is able to predict the personalized effects of novel interventions that do not exist at the time of training. Experimental results on real world datasets in large-scale medical claims and cell-line perturbations demonstrate the effectiveness of our approach. Most strikingly, CaML's zero-shot predictions outperform even strong baselines trained directly on data from the test interventions",
    "keywords": [],
    "checked": true,
    "id": "64c1ba56a32ed9a42f2cac010de56002380c2408",
    "semantic_title": "zero-shot causal learning",
    "citation_count": 2,
    "authors": [
      "Hamed Nilforoshan",
      "Michael Moor",
      "Yusuf Roohani",
      "Yining Chen",
      "Anja Šurina",
      "Michihiro Yasunaga",
      "Sara Oblak",
      "Jure Leskovec"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15f1dbc086bfd94d8c32557b573cbe18-Abstract-Conference.html": {
    "title": "Learning Modulated Transformation in GANs",
    "volume": "main",
    "abstract": "The success of style-based generators largely benefits from style modulation,which helps take care of the cross-instance variation within data. However, theinstance-wise stochasticity is typically introduced via regular convolution, wherekernels interact with features at some fixed locations, limiting its capacity formodeling geometric variation. To alleviate this problem, we equip the generatorin generative adversarial networks (GANs) with a plug-and-play module, termedas modulated transformation module (MTM). This module predicts spatial offsetsunder the control of latent codes, based on which the convolution operation canbe applied at variable locations for different instances, and hence offers the modelan additional degree of freedom to handle geometry deformation. Extensiveexperiments suggest that our approach can be faithfully generalized to variousgenerative tasks, including image generation, 3D-aware image synthesis, andvideo generation, and get compatible with state-of-the-art frameworks withoutany hyper-parameter tuning. It is noteworthy that, towards human generation onthe challenging TaiChi dataset, we improve the FID of StyleGAN3 from 21.36 to13.60, demonstrating the efficacy of learning modulated geometry transformation.Code and models are available at https://github.com/limbo0000/mtm",
    "keywords": [],
    "checked": true,
    "id": "c9f39cc087f49be1e71e9a0dd5ac37039368a72a",
    "semantic_title": "learning modulated transformation in gans",
    "citation_count": 1,
    "authors": [
      "Ceyuan Yang",
      "Qihang Zhang",
      "Yinghao Xu",
      "Jiapeng Zhu",
      "Yujun Shen",
      "Bo Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15f4cefb0e143c7ad9d40e879b0a9d0c-Abstract-Conference.html": {
    "title": "Active Negative Loss Functions for Learning with Noisy Labels",
    "volume": "main",
    "abstract": "Robust loss functions are essential for training deep neural networks in the presence of noisy labels. Some robust loss functions use Mean Absolute Error (MAE) as its necessary component. For example, the recently proposed Active Passive Loss (APL) uses MAE as its passive loss function. However, MAE treats every sample equally, slows down the convergence and can make training difficult. In this work, we propose a new class of theoretically robust passive loss functions different from MAE, namely Normalized Negative Loss Functions (NNLFs), which focus more on memorized clean samples. By replacing the MAE in APL with our proposed NNLFs, we improve APL and propose a new framework called Active Negative Loss (ANL). Experimental results on benchmark and real-world datasets demonstrate that the new set of loss functions created by our ANL framework can outperform state-of-the-art methods. The code is available athttps://github.com/Virusdoll/Active-Negative-Loss",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xichen Ye",
      "Xiaoqiang Li",
      "songmin dai",
      "Tong Liu",
      "Yan Sun",
      "Weiqin Tong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15f6a10899f557ce53fe39939af6f930-Abstract-Conference.html": {
    "title": "Compositional Generalization from First Principles",
    "volume": "main",
    "abstract": "Leveraging the compositional nature of our world to expedite learning and facilitate generalization is a hallmark of human perception. In machine learning, on the other hand, achieving compositional generalization has proven to be an elusive goal, even for models with explicit compositional priors. To get a better handle on compositional generalization, we here approach it from the bottom up: Inspired by identifiable representation learning, we investigate compositionality as a property of the data-generating process rather than the data itself. This reformulation enables us to derive mild conditions on only the support of the training distribution and the model architecture, which are sufficient for compositional generalization. We further demonstrate how our theoretical framework applies to real-world scenarios and validate our findings empirically. Our results set the stage for a principled theoretical study of compositional generalization",
    "keywords": [],
    "checked": true,
    "id": "8cb0b1047de0bfd50f52bfb6a9f8daca4f243ec7",
    "semantic_title": "compositional generalization from first principles",
    "citation_count": 5,
    "authors": [
      "Thaddäus Wiedemer",
      "Prasanna Mayilvahanan",
      "Matthias Bethge",
      "Wieland Brendel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/16049e0c3f47899091ac46f8b3afb178-Abstract-Conference.html": {
    "title": "PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline Panoramas",
    "volume": "main",
    "abstract": "Achieving an immersive experience enabling users to explore virtual environments with six degrees of freedom (6DoF) is essential for various applications such as virtual reality (VR). Wide-baseline panoramas are commonly used in these applications to reduce network bandwidth and storage requirements. However, synthesizing novel views from these panoramas remains a key challenge. Although existing neural radiance field methods can produce photorealistic views under narrow-baseline and dense image captures, they tend to overfit the training views when dealing with wide-baseline panoramas due to the difficulty in learning accurate geometry from sparse $360^{\\circ}$ views. To address this problem, we propose PanoGRF, Generalizable Spherical Radiance Fields for Wide-baseline Panoramas, which construct spherical radiance fields incorporating $360^{\\circ}$ scene priors. Unlike generalizable radiance fields trained on perspective images, PanoGRF avoids the information loss from panorama-to-perspective conversion and directly aggregates geometry and appearance features of 3D sample points from each panoramic view based on spherical projection. Moreover, as some regions of the panorama are only visible from one view while invisible from others under wide baseline settings, PanoGRF incorporates $360^{\\circ}$ monocular depth priors into spherical depth estimation to improve the geometry features. Experimental results on multiple panoramic datasets demonstrate that PanoGRF significantly outperforms state-of-the-art generalizable view synthesis methods for wide-baseline panoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay)",
    "keywords": [],
    "checked": true,
    "id": "2f4b63e3205b10cb8eacfc6836cebbd32b335fdc",
    "semantic_title": "panogrf: generalizable spherical radiance fields for wide-baseline panoramas",
    "citation_count": 0,
    "authors": [
      "Zheng Chen",
      "Yan-Pei Cao",
      "Yuan-Chen Guo",
      "Chen Wang",
      "Ying Shan",
      "Song-Hai Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/16063a1c0f0cddd4894585cf44cebb2c-Abstract-Conference.html": {
    "title": "A Heat Diffusion Perspective on Geodesic Preserving Dimensionality Reduction",
    "volume": "main",
    "abstract": "Diffusion-based manifold learning methods have proven useful in representation learning and dimensionality reduction of modern high dimensional, high throughput, noisy datasets. Such datasets are especially present in fields like biology and physics. While it is thought that these methods preserve underlying manifold structure of data by learning a proxy for geodesic distances, no specific theoretical links have been established. Here, we establish such a link via results in Riemannian geometry explicitly connecting heat diffusion to manifold distances. In this process, we also formulate a more general heat kernel based manifold embedding method that we call heat geodesic embeddings. This novel perspective makes clearer the choices available in manifold learning and denoising. Results show that our method outperforms existing state of the art in preserving ground truth manifold distances, and preserving cluster structure in toy datasets. We also showcase our method on single cell RNA-sequencing datasets with both continuum and cluster structure, where our method enables interpolation of withheld timepoints of data. Finally, we show that parameters of our more general method can be configured to give results similar to PHATE (a state-of-the-art diffusion based manifold learning method) as well as SNE (an attraction/repulsion neighborhood based method that forms the basis of t-SNE)",
    "keywords": [],
    "checked": true,
    "id": "b101e52c741f69ff7ee872526745c27bd0a399b0",
    "semantic_title": "a heat diffusion perspective on geodesic preserving dimensionality reduction",
    "citation_count": 1,
    "authors": [
      "Guillaume Huguet",
      "Alexander Tong",
      "Edward De Brouwer",
      "Yanlei Zhang",
      "Guy Wolf",
      "Ian Adelstein",
      "Smita Krishnaswamy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/160adf2dc118a920e7858484b92a37d8-Abstract-Conference.html": {
    "title": "Finite-Time Analysis of Single-Timescale Actor-Critic",
    "volume": "main",
    "abstract": "Actor-critic methods have achieved significant success in many challenging applications. However, its finite-time convergence is still poorly understood in the most practical single-timescale form. Existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. We investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single Markovian sample per actor step. Previous analysis has been unable to establish the convergence for such a challenging scenario. We demonstrate that the online single-timescale actor-critic method provably finds an $\\epsilon$-approximate stationary point with $\\widetilde{\\mathcal{O}}(\\epsilon^{-2})$ sample complexity under standard assumptions, which can be further improved to $\\mathcal{O}(\\epsilon^{-2})$ under the i.i.d. sampling. Our novel framework systematically evaluates and controls the error propagation between the actor and critic. It offers a promising approach for analyzing other single-timescale reinforcement learning algorithms as well",
    "keywords": [],
    "checked": true,
    "id": "9a5290cf0a31b26fe2e9387bc44a851c2ebe8b50",
    "semantic_title": "finite-time analysis of single-timescale actor-critic",
    "citation_count": 4,
    "authors": [
      "Xuyang Chen",
      "Lin Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/16336d94a5ffca8de019087ab7fe403f-Abstract-Conference.html": {
    "title": "VanillaNet: the Power of Minimalism in Deep Learning",
    "volume": "main",
    "abstract": "At the heart of foundation models is the philosophy of \"more is different\", exemplified by the astonishing success in computer vision and natural language processing. However, the challenges of optimization and inherent complexity of transformer models call for a paradigm shift towards simplicity. In this study, we introduce VanillaNet, a neural network architecture that embraces elegance in design. By avoiding high depth, shortcuts, and intricate operations like self-attention, VanillaNet is refreshingly concise yet remarkably powerful. Each layer is carefully crafted to be compact and straightforward, with nonlinear activation functions pruned after training to restore the original architecture. VanillaNet overcomes the challenges of inherent complexity, making it ideal for resource-constrained environments. Its easy-to-understand and highly simplified architecture opens new possibilities for efficient deployment. Extensive experimentation demonstrates that VanillaNet delivers performance on par with renowned deep neural networks and vision transformers, showcasing the power of minimalism in deep learning. This visionary journey of VanillaNet has significant potential to redefine the landscape and challenge the status quo of foundation model, setting a new path for elegant and effective model design. Pre-trained models and codes are available at https://github.com/huawei-noah/VanillaNet and https://gitee.com/mindspore/models/tree/master/research/cv/vanillanet",
    "keywords": [],
    "checked": true,
    "id": "aca65ea2730e3f49c0ff6fb7761e66756dc82255",
    "semantic_title": "vanillanet: the power of minimalism in deep learning",
    "citation_count": 10,
    "authors": [
      "Hanting Chen",
      "Yunhe Wang",
      "Jianyuan Guo",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/16347f6e665376fd9a9a290dbfe0db5b-Abstract-Conference.html": {
    "title": "Probabilistic inverse optimal control for non-linear partially observable systems disentangles perceptual uncertainty and behavioral costs",
    "volume": "main",
    "abstract": "Inverse optimal control can be used to characterize behavior in sequential decision-making tasks. Most existing work, however, is limited to fully observable or linear systems, or requires the action signals to be known. Here, we introduce a probabilistic approach to inverse optimal control for partially observable stochastic non-linear systems with unobserved action signals, which unifies previous approaches to inverse optimal control with maximum causal entropy formulations. Using an explicit model of the noise characteristics of the sensory and motor systems of the agent in conjunction with local linearization techniques, we derive an approximate likelihood function for the model parameters, which can be computed within a single forward pass. We present quantitative evaluations on stochastic and partially observable versions of two classic control tasks and two human behavioral tasks. Importantly, we show that our method can disentangle perceptual factors and behavioral costs despite the fact that epistemic and pragmatic actions are intertwined in sequential decision-making under uncertainty, such as in active sensing and active learning. The proposed method has broad applicability, ranging from imitation learning to sensorimotor neuroscience",
    "keywords": [],
    "checked": true,
    "id": "23a7b62a8a85d26f0e3dc1140aec0adfa539c556",
    "semantic_title": "probabilistic inverse optimal control for non-linear partially observable systems disentangles perceptual uncertainty and behavioral costs",
    "citation_count": 0,
    "authors": [
      "Dominik Straub",
      "Matthias Schultheis",
      "Heinz Koeppl",
      "Constantin A. Rothkopf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1644c9af28ab7916874f6fd6228a9bcf-Abstract-Conference.html": {
    "title": "TIES-Merging: Resolving Interference When Merging Models",
    "volume": "main",
    "abstract": "Transfer learning – i.e., further fine-tuning a pre-trained model on a downstream task – can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TrIm, Elect Sign & Merge (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, highlight the importance of signs, and show that estimating the signs using the validation data could further improve performance",
    "keywords": [],
    "checked": true,
    "id": "2651f0179874bd010f58d2c9fa7d118807c80977",
    "semantic_title": "ties-merging: resolving interference when merging models",
    "citation_count": 8,
    "authors": [
      "Prateek Yadav",
      "Derek Tam",
      "Leshem Choshen",
      "Colin A. Raffel",
      "Mohit Bansal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/164687cb815daae754d33364716e65e6-Abstract-Conference.html": {
    "title": "3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes",
    "volume": "main",
    "abstract": "Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models from videos of complex scenes with fluids. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, using which we can impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks acquired using color prior. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We generate datasets including three challenging scenarios involving fluid, granular materials, and rigid objects in the simulation. The datasets do not include any dense particle information so most previous 3D-based intuitive physics pipelines can barely deal with that. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that once trained, our model can achieve strong generalization in complex scenarios under extrapolate settings",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Xue",
      "Antonio Torralba",
      "Josh Tenenbaum",
      "Dan Yamins",
      "Yunzhu Li",
      "Hsiao-Yu Tung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1646e34971facbcda3727d1dc28ab635-Abstract-Conference.html": {
    "title": "Entropy-based Training Methods for Scalable Neural Implicit Samplers",
    "volume": "main",
    "abstract": "Efficiently sampling from un-normalized target distributions is a fundamental problem in scientific computing and machine learning. Traditional approaches such as Markov Chain Monte Carlo (MCMC) guarantee asymptotically unbiased samples from such distributions but suffer from computational inefficiency, particularly when dealing with high-dimensional targets, as they require numerous iterations to generate a batch of samples. In this paper, we introduce an efficient and scalable neural implicit sampler that overcomes these limitations. The implicit sampler can generate large batches of samples with low computational costs by leveraging a neural transformation that directly maps easily sampled latent vectors to target samples without the need for iterative procedures. To train the neural implicit samplers, we introduce two novel methods: the KL training method and the Fisher training method. The former method minimizes the Kullback-Leibler divergence, while the latter minimizes the Fisher divergence between the sampler and the target distributions. By employing the two training methods, we effectively optimize the neural implicit samplers to learn and generate from the desired target distribution. To demonstrate the effectiveness, efficiency, and scalability of our proposed samplers, we evaluate them on three sampling benchmarks with different scales. These benchmarks include sampling from 2D targets, Bayesian inference, and sampling from high-dimensional energy-based models (EBMs). Notably, in the experiment involving high-dimensional EBMs, our sampler produces samples that are comparable to those generated by MCMC-based methods while being more than 100 times more efficient, showcasing the efficiency of our neural sampler. Besides the theoretical contributions and strong empirical performances, the proposed neural samplers and corresponding training methods will shed light on further research on developing efficient samplers for various applications beyond the ones explored in this study",
    "keywords": [],
    "checked": false,
    "id": "5834ecf432389445812c318ce20d1f9095ab647c",
    "semantic_title": "entropy-based training methods for scalable neural implicit sampler",
    "citation_count": 0,
    "authors": [
      "Weijian Luo",
      "Boya Zhang",
      "Zhihua Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/165b0e600b1721bd59526131eb061092-Abstract-Conference.html": {
    "title": "Direct Diffusion Bridge using Data Consistency for Inverse Problems",
    "volume": "main",
    "abstract": "Diffusion model-based inverse problem solvers have shown impressive performance, but are limited in speed, mostly as they require reverse diffusion sampling starting from noise. Several recent works have tried to alleviate this problem by building a diffusion process, directly bridging the clean and the corrupted for specific inverse problems. In this paper, we first unify these existing works under the name Direct Diffusion Bridges (DDB), showing that while motivated by different theories, the resulting algorithms only differ in the choice of parameters. Then, we highlight a critical limitation of the current DDB framework, namely that it does not ensure data consistency. To address this problem, we propose a modified inference procedure that imposes data consistency without the need for fine-tuning. We term the resulting method data Consistent DDB (CDDB), which outperforms its inconsistent counterpart in terms of both perception and distortion metrics, thereby effectively pushing the Pareto-frontier toward the optimum. Our proposed method achieves state-of-the-art results on both evaluation criteria, showcasing its superiority over existing methods. Code is open-sourced here",
    "keywords": [],
    "checked": true,
    "id": "403d5853984300565714ef446bc5da0b80479b37",
    "semantic_title": "direct diffusion bridge using data consistency for inverse problems",
    "citation_count": 6,
    "authors": [
      "Hyungjin Chung",
      "Jeongsol Kim",
      "Jong Chul Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/167bcf2af2cd08fcf75b932022db0311-Abstract-Conference.html": {
    "title": "Mask Propagation for Efficient Video Semantic Segmentation",
    "volume": "main",
    "abstract": "Video Semantic Segmentation (VSS) involves assigning a semantic label to each pixel in a video sequence. Prior work in this field has demonstrated promising results by extending image semantic segmentation models to exploit temporal relationships across video frames; however, these approaches often incur significant computational costs. In this paper, we propose an efficient mask propagation framework for VSS, called MPVSS. Our approach first employs a strong query-based image segmentor on sparse key frames to generate accurate binary masks and class predictions. We then design a flow estimation module utilizing the learned queries to generate a set of segment-aware flow maps, each associated with a mask prediction from the key frame. Finally, the mask-flow pairs are warped to serve as the mask predictions for the non-key frames. By reusing predictions from key frames, we circumvent the need to process a large volume of video frames individually with resource-intensive segmentors, alleviating temporal redundancy and significantly reducing computational costs. Extensive experiments on VSPW and Cityscapes demonstrate that our mask propagation framework achieves SOTA accuracy and efficiency trade-offs. For instance, our best model with Swin-L backbone outperforms the SOTA MRCFA using MiT-B5 by 4.0% mIoU, requiring only 26% FLOPs on the VSPW dataset. Moreover, our framework reduces up to 4× FLOPs compared to the per-frame Mask2Former baseline with only up to 2% mIoU degradation on the Cityscapes validation set. Code is available at https://github.com/ziplab/MPVSS",
    "keywords": [],
    "checked": true,
    "id": "fdc2c8da623008528a5b21fc18442a614135d73e",
    "semantic_title": "mask propagation for efficient video semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Yuetian Weng",
      "Mingfei Han",
      "Haoyu He",
      "Mingjie Li",
      "Lina Yao",
      "Xiaojun Chang",
      "Bohan Zhuang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1687466683649e8bdcdec0e3f5c8de64-Abstract-Conference.html": {
    "title": "Private Distribution Learning with Public Data: The View from Sample Compression",
    "volume": "main",
    "abstract": "We study the problem of private distribution learning with access to public data. In this setup, which we refer to as *public-private learning*, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples. We show that the public-private learnability of a class $\\mathcal Q$ is connected to the existence of a sample compression scheme for $\\mathcal Q$, as well as to an intermediate notion we refer to as \\emph{list learning}. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability under taking mixtures and products of distributions. Finally, via the connection to list learning, we show that for Gaussians in $\\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples",
    "keywords": [],
    "checked": true,
    "id": "f821b9a0557bfce8b5cab784e257ae003dee2f9e",
    "semantic_title": "private distribution learning with public data: the view from sample compression",
    "citation_count": 4,
    "authors": [
      "Shai Ben-David",
      "Alex Bie",
      "Clément L Canonne",
      "Gautam Kamath",
      "Vikrant Singhal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/16bce4070c4e23434451b180348e3814-Abstract-Conference.html": {
    "title": "Fitting trees to $\\ell_1$-hyperbolic distances",
    "volume": "main",
    "abstract": "Building trees to represent or to fit distances is a critical component of phylogenetic analysis, metric embeddings, approximation algorithms, geometric graph neural nets, and the analysis of hierarchical data. Much of the previous algorithmic work, however, has focused on generic metric spaces (i.e., those with no \\emph{a priori} constraints). Leveraging several ideas from the mathematical analysis of hyperbolic geometry and geometric group theory, we study the tree fitting problem as finding the relation between the hyperbolicity (ultrametricity) vector and the error of tree (ultrametric) embedding. That is, we define a vector of hyperbolicity (ultrametric) values over all triples of points and compare the $\\ell_p$ norms of this vector with the $\\ell_q$ norm of the distortion of the best tree fit to the distances. This formulation allows us to define the average hyperbolicity (ultrametricity) in terms of a normalized $\\ell_1$ norm of the hyperbolicity vector. Furthermore, we can interpret the classical tree fitting result of Gromov as a $p = q = \\infty$ result. We present an algorithm \\textsc{HCCRootedTreeFit} such that the $\\ell_1$ error of the output embedding is analytically bounded in terms of the $\\ell_1$-norm of the hyperbolicity vector (i.e., $p = q = 1$) and that this result is tight. Furthermore, this algorithm has significantly different theoretical and empirical performance as compared to Gromov's result and related algorithms. Finally, we show using \\textsc{HCCRootedTreeFit} and related tree fitting algorithms, that supposedly standard data sets for hierarchical data analysis and geometric graph neural networks have radically different tree fits than those of synthetic, truly tree-like data sets, suggesting that a much more refined analysis of these standard data sets is called for",
    "keywords": [],
    "checked": false,
    "id": "ebde5d6d3862ffe5ecbd91e5d4bdc51e28dd895b",
    "semantic_title": "hyperaid: denoising in hyperbolic spaces for tree-fitting and hierarchical clustering",
    "citation_count": 3,
    "authors": [
      "Joon-Hyeok Yim",
      "Anna Gilbert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/16c5b4102a6b6eb061e502ce6736ad8a-Abstract-Conference.html": {
    "title": "Learning Robust Statistics for Simulation-based Inference under Model Misspecification",
    "volume": "main",
    "abstract": "Simulation-based inference (SBI) methods such as approximate Bayesian computation (ABC), synthetic likelihood, and neural posterior estimation (NPE) rely on simulating statistics to infer parameters of intractable likelihood models. However, such methods are known to yield untrustworthy and misleading inference outcomes under model misspecification, thus hindering their widespread applicability. In this work, we propose the first general approach to handle model misspecification that works across different classes of SBI methods. Leveraging the fact that the choice of statistics determines the degree of misspecification in SBI, we introduce a regularized loss function that penalizes those statistics that increase the mismatch between the data and the model. Taking NPE and ABC as use cases, we demonstrate the superior performance of our method on high-dimensional time-series models that are artificially misspecified. We also apply our method to real data from the field of radio propagation where the model is known to be misspecified. We show empirically that the method yields robust inference in misspecified scenarios, whilst still being accurate when the model is well-specified",
    "keywords": [],
    "checked": true,
    "id": "98d37a7af4fca9a01c58e429c31f3ac8ba93d266",
    "semantic_title": "learning robust statistics for simulation-based inference under model misspecification",
    "citation_count": 5,
    "authors": [
      "Daolang Huang",
      "Ayush Bharti",
      "Amauri Souza",
      "Luigi Acerbi",
      "Samuel Kaski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/16ccd203e9e3696a7ab0dcf568316379-Abstract-Conference.html": {
    "title": "Block-State Transformers",
    "volume": "main",
    "abstract": "State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity.Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks.In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences.We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention.We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates a more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed",
    "keywords": [],
    "checked": true,
    "id": "0a067fab18c67d4a386efa846c080f8afff5e8f3",
    "semantic_title": "block-state transformers",
    "citation_count": 2,
    "authors": [
      "Jonathan Pilault",
      "Mahan Fathi",
      "Orhan Firat",
      "Chris Pal",
      "Pierre-Luc Bacon",
      "Ross Goroshin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/16e4be78e61a3897665fa01504e9f452-Abstract-Conference.html": {
    "title": "Explaining Predictive Uncertainty with Information Theoretic Shapley Values",
    "volume": "main",
    "abstract": "Researchers in explainable artificial intelligence have developed numerous methods for helping users understand the predictions of complex supervised learning models. By contrast, explaining the $\\textit{uncertainty}$ of model outputs has received relatively little attention. We adapt the popular Shapley value framework to explain various types of predictive uncertainty, quantifying each feature's contribution to the conditional entropy of individual model outputs. We consider games with modified characteristic functions and find deep connections between the resulting Shapley values and fundamental quantities from information theory and conditional independence testing. We outline inference procedures for finite sample error rate control with provable guarantees, and implement efficient algorithms that perform well in a range of experiments on real and simulated data. Our method has applications to covariate shift detection, active learning, feature selection, and active feature-value acquisition",
    "keywords": [],
    "checked": true,
    "id": "31aaa0dc9dd433e9e33f799b143cb8d3ee53b27e",
    "semantic_title": "explaining predictive uncertainty with information theoretic shapley values",
    "citation_count": 6,
    "authors": [
      "David Watson",
      "Joshua O'Hara",
      "Niek Tax",
      "Richard Mudd",
      "Ido Guy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1700ad4e6252e8f2955909f96367b34d-Abstract-Conference.html": {
    "title": "CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning",
    "volume": "main",
    "abstract": "Handling out-of-distribution (OOD) samples has become a major stake in the real-world deployment of machine learning systems. This work explores the use of self-supervised contrastive learning to the simultaneous detection of two types of OOD samples: unseen classes and adversarial perturbations. First, we pair self-supervised contrastive learning with the maximum mean discrepancy (MMD) two-sample test. This approach enables us to robustly test whether two independent sets of samples originate from the same distribution, and we demonstrate its effectiveness by discriminating between CIFAR-10 and CIFAR-10.1 with higher confidence than previous work. Motivated by this success, we introduce CADet (Contrastive Anomaly Detection), a novel method for OOD detection of single samples. CADet draws inspiration from MMD, but leverages the similarity between contrastive transformations of a same sample. CADet outperforms existing adversarial detection methods in identifying adversarially perturbed samples on ImageNet and achieves comparable performance to unseen label detection methods on two challenging benchmarks: ImageNet-O and iNaturalist. Significantly, CADet is fully self-supervised and requires neither labels for in-distribution samples nor access to OOD examples",
    "keywords": [],
    "checked": true,
    "id": "c9a345151cf666a6d78329012daf1833333a3a5d",
    "semantic_title": "cadet: fully self-supervised out-of-distribution detection with contrastive learning",
    "citation_count": 0,
    "authors": [
      "Charles Guille-Escuret",
      "Pau Rodriguez",
      "David Vazquez",
      "Ioannis Mitliagkas",
      "Joao Monteiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1704fe7aaff33a54802b83a016050ab8-Abstract-Conference.html": {
    "title": "PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning",
    "volume": "main",
    "abstract": "Hyperparameters of Deep Learning (DL) pipelines are crucial for their downstream performance. While a large number of methods for Hyperparameter Optimization (HPO) have been developed, their incurred costs are often untenable for modern DL.Consequently, manual experimentation is still the most prevalent approach to optimize hyperparameters, relying on the researcher's intuition, domain knowledge, and cheap preliminary explorations.To resolve this misalignment between HPO algorithms and DL researchers, we propose PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency across a range of DL benchmarks and show its gains under informative expert input and robustness against poor expert beliefs",
    "keywords": [],
    "checked": true,
    "id": "74dba11830c7521825ff008f394e096efb355d98",
    "semantic_title": "priorband: practical hyperparameter optimization in the age of deep learning",
    "citation_count": 5,
    "authors": [
      "Neeratyoy Mallik",
      "Edward Bergman",
      "Carl Hvarfner",
      "Danny Stoll",
      "Maciej Janowski",
      "Marius Lindauer",
      "Luigi Nardi",
      "Frank Hutter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/170dc3e41f2d03e327e04dbab0fccbfb-Abstract-Conference.html": {
    "title": "Towards Efficient Image Compression Without Autoregressive Models",
    "volume": "main",
    "abstract": "Recently, learned image compression (LIC) has garnered increasing interest with its rapidly improving performance surpassing conventional codecs. A key ingredient of LIC is a hyperprior-based entropy model, where the underlying joint probability of the latent image features is modeled as a product of Gaussian distributions from each latent element. Since latents from the actual images are not spatially independent, autoregressive (AR) context based entropy models were proposed to handle the discrepancy between the assumed distribution and the actual distribution. Though the AR-based models have proven effective, the computational complexity is significantly increased due to the inherent sequential nature of the algorithm. In this paper, we present a novel alternative to the AR-based approach that can provide a significantly better trade-off between performance and complexity. To minimize the discrepancy, we introduce a correlation loss that forces the latents to be spatially decorrelated and better fitted to the independent probability model. Our correlation loss is proved to act as a general plug-in for the hyperprior (HP) based learned image compression methods. The performance gain from our correlation loss is ‘free' in terms of computation complexity for both inference time and decoding time. To our knowledge, our method gives the best trade-off between the complexity and performance: combined with the Checkerboard-CM, it attains 90% and when combined with ChARM-CM, it attains 98% of the AR-based BD-Rate gains yet is around 50 times and 30 times faster than AR-based methods respectively",
    "keywords": [],
    "checked": false,
    "id": "de510b4bcb62b6006d31f74ecd6a445818100949",
    "semantic_title": "towards learning-based image compression for storage on dna support",
    "citation_count": 0,
    "authors": [
      "Muhammad Salman Ali",
      "Yeongwoong Kim",
      "Maryam Qamar",
      "Sung-Chang Lim",
      "Donghyun Kim",
      "Chaoning Zhang",
      "Sung-Ho Bae",
      "Hui Yong Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1737656c4dc65027939e47e4587ce95e-Abstract-Conference.html": {
    "title": "De novo Drug Design using Reinforcement Learning with Multiple GPT Agents",
    "volume": "main",
    "abstract": "De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT",
    "keywords": [],
    "checked": true,
    "id": "3e5c942d0c9463c6244699d84264bfb3d893ebb2",
    "semantic_title": "de novo drug design using reinforcement learning with multiple gpt agents",
    "citation_count": 1,
    "authors": [
      "Xiuyuan Hu",
      "Guoqing Liu",
      "Yang Zhao",
      "Hao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/176a579942089c4cdc70136c567932ab-Abstract-Conference.html": {
    "title": "Pointwise uncertainty quantification for sparse variational Gaussian process regression with a Brownian motion prior",
    "volume": "main",
    "abstract": "We study pointwise estimation and uncertainty quantification for a sparse variational Gaussian process method with eigenvector inducing variables. For a rescaled Brownian motion prior, we derive theoretical guarantees and limitations for the frequentist size and coverage of pointwise credible sets. For sufficiently many inducing variables, we precisely characterize the asymptotic frequentist coverage, deducing when credible sets from this variational method are conservative and when overconfident/misleading. We numerically illustrate the applicability of our results and discuss connections with other common Gaussian process priors",
    "keywords": [],
    "checked": true,
    "id": "3e45babeadf3184f221dd5203d8e3c192a079455",
    "semantic_title": "pointwise uncertainty quantification for sparse variational gaussian process regression with a brownian motion prior",
    "citation_count": 2,
    "authors": [
      "Luke Travis",
      "Kolyan Ray"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/17826a22eb8b58494dfdfca61e772c39-Abstract-Conference.html": {
    "title": "Few-shot Generation via Recalling Brain-Inspired Episodic-Semantic Memory",
    "volume": "main",
    "abstract": "Aimed at adapting a generative model to a novel generation task with only a few given data samples, the capability of few-shot generation is crucial for many real-world applications with limited data, \\emph{e.g.}, artistic domains.Instead of training from scratch, recent works tend to leverage the prior knowledge stored in previous datasets, which is quite similar to the memory mechanism of human intelligence, but few of these works directly imitate the memory-recall mechanism that humans make good use of in accomplishing creative tasks, \\emph{e.g.}, painting and writing.Inspired by the memory mechanism of human brain, in this work, we carefully design a variational structured memory module (VSM), which can simultaneously store both episodic and semantic memories to assist existing generative models efficiently recall these memories during sample generation.Meanwhile, we introduce a bionic memory updating strategy for the conversion between episodic and semantic memories, which can also model the uncertainty during conversion.Then, we combine the developed VSM with various generative models under the Bayesian framework, and evaluate these memory-augmented generative models with few-shot generation tasks, demonstrating the effectiveness of our methods",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibin Duan",
      "Zhiyi Lv",
      "Chaojie Wang",
      "Bo Chen",
      "Bo An",
      "Mingyuan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/17a234c91f746d9625a75cf8a8731ee2-Abstract-Conference.html": {
    "title": "Balancing memorization and generalization in RNNs for high performance brain-machine Interfaces",
    "volume": "main",
    "abstract": "Brain-machine interfaces (BMIs) can restore motor function to people with paralysis but are currently limited by the accuracy of real-time decoding algorithms. Recurrent neural networks (RNNs) using modern training techniques have shown promise in accurately predicting movements from neural signals but have yet to be rigorously evaluated against other decoding algorithms in a closed-loop setting. Here we compared RNNs to other neural network architectures in real-time, continuous decoding of finger movements using intracortical signals from nonhuman primates. Across one and two finger online tasks, LSTMs (a type of RNN) outperformed convolutional and transformer-based neural networks, averaging 18% higher throughput than the convolution network. On simplified tasks with a reduced movement set, RNN decoders were allowed to memorize movement patterns and matched able-bodied control. Performance gradually dropped as the number of distinct movements increased but did not go below fully continuous decoder performance. Finally, in a two-finger task where one degree-of-freedom had poor input signals, we recovered functional control using RNNs trained to act both like a movement classifier and continuous decoder. Our results suggest that RNNs can enable functional real-time BMI control by learning and generating accurate movement patterns",
    "keywords": [],
    "checked": true,
    "id": "5fcb56559dd55ec34583f08ab92c102e3d8e1637",
    "semantic_title": "balancing memorization and generalization in rnns for high performance brain-machine interfaces",
    "citation_count": 2,
    "authors": [
      "Joseph Costello",
      "Hisham Temmar",
      "Luis Cubillos",
      "Matthew Mender",
      "Dylan Wallace",
      "Matt Willsey",
      "Parag Patil",
      "Cynthia Chestek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/17a9ab4190289f0e1504bbb98d1d111a-Abstract-Conference.html": {
    "title": "Saddle-to-Saddle Dynamics in Diagonal Linear Networks",
    "volume": "main",
    "abstract": "In this paper we fully describe the trajectory of gradient flow over $2$-layer diagonal linear networks for the regression setting in the limit of vanishing initialisation. We show that the limiting flow successively jumps from a saddle of the training loss to another until reaching the minimum $\\ell_1$-norm solution. We explicitly characterise the visited saddles as well as the jump times through a recursive algorithm reminiscent of the LARS algorithm used for computing the Lasso path. Starting from the zero vector, coordinates are successively activated until the minimum $\\ell_1$-norm solution is recovered, revealing an incremental learning. Our proof leverages a convenient arc-length time-reparametrisation which enables to keep track of the transitions between the jumps. Our analysis requires negligible assumptions on the data, applies to both under and overparametrised settings and covers complex cases where there is no monotonicity of the number of active coordinates. We provide numerical experiments to support our findings",
    "keywords": [],
    "checked": true,
    "id": "b4729916f1249853a56e446d12500528b56f4e83",
    "semantic_title": "saddle-to-saddle dynamics in diagonal linear networks",
    "citation_count": 14,
    "authors": [
      "Scott Pesme",
      "Nicolas Flammarion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/17d0a21da4ec2c12b4f07fa2e34e4d6c-Abstract-Conference.html": {
    "title": "Encoding Human Behavior in Information Design through Deep Learning",
    "volume": "main",
    "abstract": "We initiate the study of $\\textit{behavioral information design}$ through deep learning. In information design, a $\\textit{sender}$ aims to persuade a $\\textit{receiver}$ to take certain actions by strategically revealing information. We address scenarios in which the receiver might exhibit different behavior patterns other than the standard Bayesian rational assumption. We propose HAIDNet, a neural-network-based optimization framework for information design that can adapt to multiple representations of human behavior. Through extensive simulation, we show that HAIDNet can not only recover information policies that are near-optimal compared with known analytical solutions, but also can extend to designing information policies for settings that are computationally challenging (e.g., when there are multiple receivers) or for settings where there are no known solutions in general (e.g., when the receiver behavior does not follow the Bayesian rational assumption). We also conduct real-world human-subject experiments and demonstrate that our framework can capture human behavior from data and lead to more effective information policy for real-world human receivers",
    "keywords": [],
    "checked": true,
    "id": "31a79e50875f3c5b3d19bd35882197165132913d",
    "semantic_title": "encoding human behavior in information design through deep learning",
    "citation_count": 0,
    "authors": [
      "Guanghui Yu",
      "Wei Tang",
      "Saumik Narayanan",
      "Chien-Ju Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/17f158c25b08758cf650130f7f173e51-Abstract-Conference.html": {
    "title": "Collaboratively Learning Linear Models with Structured Missing Data",
    "volume": "main",
    "abstract": "We study the problem of collaboratively learning least squares estimates for $m$ agents. Each agent observes a different subset of the features---e.g., containing data collected from sensors of varying resolution. Our goal is to determine how to coordinate the agents in order to produce the best estimator for each agent. We propose a distributed, semi-supervised algorithm Collab, consisting of three steps: local training, aggregation, and distribution. Our procedure does not require communicating the labeled data, making it communication efficient and useful in settings where the labeled data is inaccessible. Despite this handicap, our procedure is nearly asymptotically, local-minimax optimal---even among estimators allowed to communicate the labeled data such as imputation methods. We test our method on US Census data. We also discuss generalizations of our method to non-Gaussian feature settings, non-linear settings, and Federated Learning",
    "keywords": [],
    "checked": true,
    "id": "b664be3f59679bc5750283ccac7a25f78de84bb1",
    "semantic_title": "collaboratively learning linear models with structured missing data",
    "citation_count": 0,
    "authors": [
      "Chen Cheng",
      "Gary Cheng",
      "John C. Duchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/180d4373aca26bd86bf45fc50d1a709f-Abstract-Conference.html": {
    "title": "Generating Behaviorally Diverse Policies with Latent Diffusion Models",
    "volume": "main",
    "abstract": "Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of theoriginal collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original humanoid archive coverage. Further, the conditioning mechanism of diffusion models allowsfor flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home",
    "keywords": [],
    "checked": true,
    "id": "569648a84a6fa3b9e3659c60047d63ed04d5c175",
    "semantic_title": "generating behaviorally diverse policies with latent diffusion models",
    "citation_count": 4,
    "authors": [
      "Shashank Hegde",
      "Sumeet Batra",
      "K.R. Zentner",
      "Gaurav Sukhatme"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/180f1a1de4244c009ff0848c55ae54a5-Abstract-Conference.html": {
    "title": "Incentives in Private Collaborative Machine Learning",
    "volume": "main",
    "abstract": "Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved. To address this, we introduce differential privacy (DP) as an incentive. Each party can select its required DP guarantee and perturb its sufficient statistic (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a privacy-valuation trade-off, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to the grand coalition's posterior. We empirically demonstrate the effectiveness and practicality of our approach on synthetic and real-world datasets",
    "keywords": [],
    "checked": true,
    "id": "79c2dd7545e68b7c4510ac10d5eba4c44a49b2da",
    "semantic_title": "incentives in private collaborative machine learning",
    "citation_count": 0,
    "authors": [
      "Rachael Sim",
      "Yehong Zhang",
      "Nghia Hoang",
      "Xinyi Xu",
      "Bryan Kian Hsiang Low",
      "Patrick Jaillet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/180f6184a3458fa19c28c5483bc61877-Abstract-Conference.html": {
    "title": "VideoComposer: Compositional Video Synthesis with Motion Controllability",
    "volume": "main",
    "abstract": "The pursuit of controllability as a higher standard of visual content creation has yielded remarkable progress in customizable image synthesis. However, achieving controllable video synthesis remains challenging due to the large variation of temporal dynamics and the requirement of cross-frame temporal consistency. Based on the paradigm of compositional generation, this work presents VideoComposer that allows users to flexibly compose a video with textual conditions, spatial conditions, and more importantly temporal conditions. Specifically, considering the characteristic of video data, we introduce the motion vector from compressed videos as an explicit control signal to provide guidance regarding temporal dynamics. In addition, we develop a Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs, with which the model could make better use of temporal conditions and hence achieve higher inter-frame consistency. Extensive experimental results suggest that VideoComposer is able to control the spatial and temporal patterns simultaneously within a synthesized video in various forms, such as text description, sketch sequence, reference video, or even simply hand-crafted motions. The code and models are publicly available athttps://videocomposer.github.io",
    "keywords": [],
    "checked": true,
    "id": "f02ea7a18f00859d9ea1b321e3385ae7d0170639",
    "semantic_title": "videocomposer: compositional video synthesis with motion controllability",
    "citation_count": 83,
    "authors": [
      "Xiang Wang",
      "Hangjie Yuan",
      "Shiwei Zhang",
      "Dayou Chen",
      "Jiuniu Wang",
      "Yingya Zhang",
      "Yujun Shen",
      "Deli Zhao",
      "Jingren Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/181a027913d36bc0a8857c0da661d621-Abstract-Conference.html": {
    "title": "Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. These can be readily used to construct a new offline RL algorithm (TSRL) with less conservative policy constraints and a reliable latent space data augmentation procedure. Based on extensive experiments, we find TSRL achieves great performance on small benchmark datasets with as few as 1% of the original samples, which significantly outperforms the recent offline RL algorithms in terms of data efficiency and generalizability. Code is available at:https://github.com/pcheng2/TSRL",
    "keywords": [],
    "checked": true,
    "id": "157f71d01fbd1799e30e1b1bf68d53a60c2ad107",
    "semantic_title": "look beneath the surface: exploiting fundamental symmetry for sample-efficient offline rl",
    "citation_count": 2,
    "authors": [
      "Peng Cheng",
      "Xianyuan Zhan",
      "zhihao wu",
      "Wenjia Zhang",
      "Youfang Lin",
      "Shou cheng Song",
      "Han Wang",
      "Li Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/18210aa6209b9adfc97b8c17c3741d95-Abstract-Conference.html": {
    "title": "Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks",
    "volume": "main",
    "abstract": "We provide several new results on the sample complexity of vector-valued linear predictors (parameterized by a matrix), and more generally neural networks. Focusing on size-independent bounds, where only the Frobenius norm distance of the parameters from some fixed reference matrix $W_0$ is controlled, we show that the sample complexity behavior can be surprisingly different than what we may expect considering the well-studied setting of scalar-valued linear predictors. This also leads to new sample complexity bounds for feed-forward neural networks, tackling some open questions in the literature, and establishing a new convex linear prediction problem that is provably learnable without uniform convergence",
    "keywords": [],
    "checked": true,
    "id": "0d3452a58d29a8bdc380ba29cd87ade8b4dc14e3",
    "semantic_title": "initialization-dependent sample complexity of linear predictors and neural networks",
    "citation_count": 0,
    "authors": [
      "Roey Magen",
      "Ohad Shamir"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/182b39a4458fb4a9a8d6871a6671ff3e-Abstract-Conference.html": {
    "title": "Incentivizing Honesty among Competitors in Collaborative Learning and Optimization",
    "volume": "main",
    "abstract": "Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the effectiveness of our incentive scheme on a standard non-convex federated learning benchmark. Our work shows that explicitly modeling the incentives and actions of dishonest clients, rather than assuming them malicious, can enable strong robustness guarantees for collaborative learning",
    "keywords": [],
    "checked": true,
    "id": "4d6be45a51f2ab33329fe5c08939591e52dc6e6d",
    "semantic_title": "incentivizing honesty among competitors in collaborative learning and optimization",
    "citation_count": 0,
    "authors": [
      "Florian E. Dorner",
      "Nikola Konstantinov",
      "Georgi Pashaliev",
      "Martin Vechev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/182c433412b33c14e32a7c4fc2c3e290-Abstract-Conference.html": {
    "title": "SNAP: Self-Supervised Neural Maps for Visual Positioning and Semantic Understanding",
    "volume": "main",
    "abstract": "Semantic 2D maps are commonly used by humans and machines for navigation purposes, whether it's walking or driving. However, these maps have limitations: they lack detail, often contain inaccuracies, and are difficult to create and maintain, especially in an automated fashion. Can we use raw imagery to automatically create better maps that can be easily interpreted by both humans and machines? We introduce SNAP, a deep network that learns rich 2D neural maps from ground-level and overhead images. We train our model to align neural maps estimated from different inputs, supervised only with camera poses over tens of millions of StreetView images. SNAP can resolve the location of challenging image queries beyond the reach of traditional methods, outperforming the state of the art in localization by a large margin. Moreover, our neural maps encode not only geometry and appearance but also high-level semantics, discovered without explicit supervision. This enables effective pre-training for data-efficient semantic scene understanding, with the potential to unlock cost-efficient creation of more detailed maps",
    "keywords": [],
    "checked": true,
    "id": "c33e9fd62c2dc7dd9f4780e8d55a495cf094e34a",
    "semantic_title": "snap: self-supervised neural maps for visual positioning and semantic understanding",
    "citation_count": 2,
    "authors": [
      "Paul-Edouard Sarlin",
      "Eduard Trulls",
      "Marc Pollefeys",
      "Jan Hosang",
      "Simon Lynen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1849b94ed817ae7043a6b6934ef410c1-Abstract-Conference.html": {
    "title": "Equal Opportunity of Coverage in Fair Regression",
    "volume": "main",
    "abstract": "We study fair machine learning (ML) under predictive uncertainty to enable reliable and trustworthy decision-making. The seminal work of 'equalized coverage' proposed an uncertainty-aware fairness notion. However, it does not guarantee equal coverage rates across more fine-grained groups (e.g., low-income females) conditioning on the true label and is biased in the assessment of uncertainty. To tackle these limitations, we propose a new uncertainty-aware fairness -- Equal Opportunity of Coverage (EOC) -- that aims to achieve two properties: (1) coverage rates for different groups with similar outcomes are close, and (2) the coverage rate for the entire population remains at a predetermined level. Further, the prediction intervals should be narrow to be informative. We propose Binned Fair Quantile Regression (BFQR), a distribution-free post-processing method to improve EOC with reasonable width for any trained ML models. It first calibrates a hold-out set to bound deviation from EOC, then leverages conformal prediction to maintain EOC on a test set, meanwhile optimizing prediction interval width. Experimental results demonstrate the effectiveness of our method in improving EOC",
    "keywords": [],
    "checked": true,
    "id": "36bc8de7d8408a68bdbd40e20820f8c71befb328",
    "semantic_title": "equal opportunity of coverage in fair regression",
    "citation_count": 3,
    "authors": [
      "Fangxin Wang",
      "Lu Cheng",
      "Ruocheng Guo",
      "Kay Liu",
      "Philip S Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/184a03a3ad07e8897c62461c02634b02-Abstract-Conference.html": {
    "title": "Nonparametric Teaching for Multiple Learners",
    "volume": "main",
    "abstract": "We study the problem of teaching multiple learners simultaneously in the nonparametric iterative teaching setting, where the teacher iteratively provides examples to the learner for accelerating the acquisition of a target concept. This problem is motivated by the gap between current single-learner teaching setting and the real-world scenario of human instruction where a teacher typically imparts knowledge to multiple students. Under the new problem formulation, we introduce a novel framework -- Multi-learner Nonparametric Teaching (MINT). In MINT, the teacher aims to instruct multiple learners, with each learner focusing on learning a scalar-valued target model. To achieve this, we frame the problem as teaching a vector-valued target model and extend the target model space from a scalar-valued reproducing kernel Hilbert space used in single-learner scenarios to a vector-valued space. Furthermore, we demonstrate that MINT offers significant teaching speed-up over repeated single-learner teaching, particularly when the multiple learners can communicate with each other. Lastly, we conduct extensive experiments to validate the practicality and efficiency of MINT",
    "keywords": [],
    "checked": true,
    "id": "5781ddcd249cbee5032824d86e36dda506c62c3e",
    "semantic_title": "nonparametric teaching for multiple learners",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Xiaofeng Cao",
      "Weiyang Liu",
      "Ivor Tsang",
      "James Kwok"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/184c1e18d00d7752805324da48ad25be-Abstract-Conference.html": {
    "title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search",
    "volume": "main",
    "abstract": "Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as general adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm.While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design",
    "keywords": [],
    "checked": true,
    "id": "411b16add23976ffcdf6422f932453f6ebcca119",
    "semantic_title": "evoprompting: language models for code-level neural architecture search",
    "citation_count": 25,
    "authors": [
      "Angelica Chen",
      "David Dohan",
      "David So"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1857d2e8f51ed219ca0c2663239b38e5-Abstract-Conference.html": {
    "title": "Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction",
    "volume": "main",
    "abstract": "Reconstructing 3D clothed human avatars from single images is a challenging task, especially when encountering complex poses and loose clothing. Current methods exhibit limitations in performance, largely attributable to their dependence on insufficient 2D image features and inconsistent query methods. Owing to this, we present the Global-correlated 3D-decoupling Transformer for clothed Avatar reconstruction (GTA), a novel transformer-based architecture that reconstructs clothed human avatars from monocular images. Our approach leverages transformer architectures by utilizing a Vision Transformer model as an encoder for capturing global-correlated image features. Subsequently, our innovative 3D-decoupling decoder employs cross-attention to decouple tri-plane features, using learnable embeddings as queries for cross-plane generation. To effectively enhance feature fusion with the tri-plane 3D feature and human body prior, we propose a hybrid prior fusion strategy combining spatial and prior-enhanced queries, leveraging the benefits of spatial localization and human body prior knowledge. Comprehensive experiments on CAPE and THuman2.0 datasets illustrate that our method outperforms state-of-the-art approaches in both geometry and texture reconstruction, exhibiting high robustness to challenging poses and loose clothing, and producing higher-resolution textures. Codes are available at https://github.com/River-Zhang/GTA",
    "keywords": [],
    "checked": true,
    "id": "18240c1d699a1717de28a9804908123dc93717a2",
    "semantic_title": "global-correlated 3d-decoupling transformer for clothed avatar reconstruction",
    "citation_count": 4,
    "authors": [
      "Zechuan Zhang",
      "Li Sun",
      "Zongxin Yang",
      "Ling Chen",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/185969291540b3cd86e70c51e8af5d08-Abstract-Conference.html": {
    "title": "TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models",
    "volume": "main",
    "abstract": "We propose a robust and reliable evaluation metric for generative models called Topological Precision and Recall (TopP&R, pronounced \"topper\"), which systematically estimates supports by retaining only topologically and statistically significant features with a certain level of confidence. Existing metrics, such as Inception Score (IS), Frechet Inception Distance (FID), and various Precision and Recall (P&R) variants, rely heavily on support estimates derived from sample features. However, the reliability of these estimates has been overlooked, even though the quality of the evaluation hinges entirely on their accuracy. In this paper, we demonstrate that current methods not only fail to accurately assess sample quality when support estimation is unreliable, but also yield inconsistent results. In contrast, TopP&R reliably evaluates the sample quality and ensures statistical consistency in its results. Our theoretical and experimental findings reveal that TopP&R provides a robust evaluation, accurately capturing the true trend of change in samples, even in the presence of outliers and non-independent and identically distributed (Non-IID) perturbations where other methods result in inaccurate support estimations. To our knowledge, TopP&R is the first evaluation metric specifically focused on the robust estimation of supports, offering statistical consistency under noise conditions",
    "keywords": [],
    "checked": true,
    "id": "b0f6c926398f6046187270c9f9ab11656dd84a1e",
    "semantic_title": "topp&r: robust support estimation approach for evaluating fidelity and diversity in generative models",
    "citation_count": 2,
    "authors": [
      "Pum Jun Kim",
      "Yoojin Jang",
      "Jisu Kim",
      "Jaejun Yoo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1868a3c73d0d2a44c42458575fa8514c-Abstract-Conference.html": {
    "title": "A Unified Detection Framework for Inference-Stage Backdoor Defenses",
    "volume": "main",
    "abstract": "Backdoor attacks involve inserting poisoned samples during training, resulting in a model containing a hidden backdoor that can trigger specific behaviors without impacting performance on normal samples. These attacks are challenging to detect, as the backdoored model appears normal until activated by the backdoor trigger, rendering them particularly stealthy. In this study, we devise a unified inference-stage detection framework to defend against backdoor attacks. We first rigorously formulate the inference-stage backdoor detection problem, encompassing various existing methods, and discuss several challenges and limitations. We then propose a framework with provable guarantees on the false positive rate or the probability of misclassifying a clean sample. Further, we derive the most powerful detection rule to maximize the detection power, namely the rate of accurately identifying a backdoor sample, given a false positive rate under classical learning scenarios. Based on the theoretically optimal detection rule, we suggest a practical and effective approach for real-world applications based on the latent representations of backdoored deep nets. We extensively evaluate our method on 14 different backdoor attacks using Computer Vision (CV) and Natural Language Processing (NLP) benchmark datasets. The experimental findings align with our theoretical results. We significantly surpass the state-of-the-art methods, e.g., up to 300\\% improvement on the detection power as evaluated by AUCROC, over the state-of-the-art defense against advanced adaptive backdoor attacks",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Xian",
      "Ganghua Wang",
      "Jayanth Srinivasa",
      "Ashish Kundu",
      "Xuan Bi",
      "Mingyi Hong",
      "Jie Ding"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/186a213d720568b31f9b59c085a23e5a-Abstract-Conference.html": {
    "title": "Non-Stationary Bandits with Auto-Regressive Temporal Dependency",
    "volume": "main",
    "abstract": "Traditional multi-armed bandit (MAB) frameworks, predominantly examined under stochastic or adversarial settings, often overlook the temporal dynamics inherent in many real-world applications such as recommendation systems and online advertising. This paper introduces a novel non-stationary MAB framework that captures the temporal structure of these real-world dynamics through an auto-regressive (AR) reward structure. We propose an algorithm that integrates two key mechanisms: (i) an alternation mechanism adept at leveraging temporal dependencies to dynamically balance exploration and exploitation, and (ii) a restarting mechanism designed to discard out-of-date information. Our algorithm achieves a regret upper bound that nearly matches the lower bound, with regret measured against a robust dynamic benchmark. Finally, via a real-world case study on tourism demand prediction, we demonstrate both the efficacy of our algorithm and the broader applicability of our techniques to more complex, rapidly evolving time series",
    "keywords": [],
    "checked": true,
    "id": "7d3db808edff0d14df5533c1a52066807bc12f26",
    "semantic_title": "non-stationary bandits with auto-regressive temporal dependency",
    "citation_count": 5,
    "authors": [
      "Qinyi Chen",
      "Negin Golrezaei",
      "Djallel Bouneffouf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/188409d2ad91db4fb13644d024d99074-Abstract-Conference.html": {
    "title": "Globally solving the Gromov-Wasserstein problem for point clouds in low dimensional Euclidean spaces",
    "volume": "main",
    "abstract": "This paper presents a framework for computing the Gromov-Wasserstein problem between two sets of points in low dimensional spaces, where the discrepancy is the squared Euclidean norm.The Gromov-Wasserstein problem is a generalization of the optimal transport problem that finds the assignment between two sets preserving pairwise distances as much as possible. This can be used to quantify the similarity between two formations or shapes, a common problem in AI and machine learning.The problem can be formulated as a Quadratic Assignment Problem (QAP), which is in general computationally intractable even for small problems. Our framework addresses this challenge by reformulating the QAP as an optimization problem with a low-dimensional domain, leveraging the fact that the problem can be expressed as a concave quadratic optimization problem with low rank. The method scales well with the number of points, and it can be used to find the global solution for large-scale problems with thousands of points.We compare the computational complexity of our approach with state-of-the-art methods on synthetic problems and apply it to a near-symmetrical problem which is of particular interest in computational biology",
    "keywords": [],
    "checked": true,
    "id": "c46429f405d0626aa99a3bc23e53b5ef3156b22f",
    "semantic_title": "globally solving the gromov-wasserstein problem for point clouds in low dimensional euclidean spaces",
    "citation_count": 3,
    "authors": [
      "Martin Ryner",
      "Jan Kronqvist",
      "Johan Karlsson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/18d3a2f3068d6c669dcae19ceca1bc24-Abstract-Conference.html": {
    "title": "Combinatorial Optimization with Policy Adaptation using Latent Space Search",
    "volume": "main",
    "abstract": "Combinatorial Optimization underpins many real-world applications and yet, designing performant algorithms to solve these complex, typically NP-hard, problems remains a significant research challenge. Reinforcement Learning (RL) provides a versatile framework for designing heuristics across a broad spectrum of problem domains. However, despite notable progress, RL has not yet supplanted industrial solvers as the go-to solution. Current approaches emphasize pre-training heuristics that construct solutions, but often rely on search procedures with limited variance, such as stochastically sampling numerous solutions from a single policy, or employing computationally expensive fine-tuning of the policy on individual problem instances. Building on the intuition that performant search at inference time should be anticipated during pre-training, we propose COMPASS, a novel RL approach that parameterizes a distribution of diverse and specialized policies conditioned on a continuous latent space. We evaluate COMPASS across three canonical problems - Travelling Salesman, Capacitated Vehicle Routing, and Job-Shop Scheduling - and demonstrate that our search strategy (i) outperforms state-of-the-art approaches in 9 out of 11 standard benchmarking tasks and (ii) generalizes better, surpassing all other approaches on a set of 18 procedurally transformed instance distributions",
    "keywords": [],
    "checked": true,
    "id": "112b25f0116b461810ae042c9995b495470e3324",
    "semantic_title": "combinatorial optimization with policy adaptation using latent space search",
    "citation_count": 1,
    "authors": [
      "Felix Chalumeau",
      "Shikha Surana",
      "Clément Bonnet",
      "Nathan Grinsztajn",
      "Arnu Pretorius",
      "Alexandre Laterre",
      "Tom Barrett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1967f962c7c2083618236d80eeb9d1ac-Abstract-Conference.html": {
    "title": "Adversarial Resilience in Sequential Prediction via Abstention",
    "volume": "main",
    "abstract": "We study the problem of sequential prediction in the stochastic setting with an adversary that is allowed to inject clean-label adversarial (or out-of-distribution) examples. Algorithms designed to handle purely stochastic data tend to fail in the presence of such adversarial examples, often leading to erroneous predictions. This is undesirable in many high-stakes applications such as medical recommendations, where abstaining from predictions on adversarial examples is preferable to misclassification. On the other hand, assuming fully adversarial data leads to very pessimistic bounds that are often vacuous in practice. To move away from these pessimistic guarantees, we propose a new model of sequential prediction that sits between the purely stochastic and fully adversarial settings by allowing the learner to abstain from making a prediction at no cost on adversarial examples, thereby asking the learner to make predictions with certainty. Assuming access to the marginal distribution on the non-adversarial examples, we design a learner whose error scales with the VC dimension (mirroring the stochastic setting) of the hypothesis class, as opposed to the Littlestone dimension which characterizes the fully adversarial setting. Furthermore, we design learners for VC dimension~1 classes and the class of axis-aligned rectangles, which work even in the absence of access to the marginal distribution. Our key technical contribution is a novel measure for quantifying uncertainty for learning VC classes, which may be of independent interest",
    "keywords": [],
    "checked": true,
    "id": "ef2f861526d68f7773826be32c61ef3104fac251",
    "semantic_title": "adversarial resilience in sequential prediction via abstention",
    "citation_count": 0,
    "authors": [
      "Surbhi Goel",
      "Steve Hanneke",
      "Shay Moran",
      "Abhishek Shetty"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/196c4e02b7464c554f0f5646af5d502e-Abstract-Conference.html": {
    "title": "Simplicity Bias in 1-Hidden Layer Neural Networks",
    "volume": "main",
    "abstract": "Recent works have demonstrated that neural networks exhibit extreme *simplicity bias* (SB). That is, they learn *only the simplest* features to solve a task at hand, even in the presence of other, more robust but more complex features. Due to the lack of a general and rigorous definition of *features*, these works showcase SB on *semi-synthetic* datasets such as Color-MNIST , MNIST-CIFAR where defining features is relatively easier. In this work, we rigorously define as well as thoroughly establish SB for *one hidden layer* neural networks in the infinite width regime. More concretely, (i) we define SB as the network essentially being a function of a low dimensional projection of the inputs (ii) theoretically, we show that when the data is linearly separable, the network primarily depends on only the linearly separable ($1$-dimensional) subspace even in the presence of an arbitrarily large number of other, more complex features which could have led to a significantly more robust classifier, (iii) empirically, we show that models trained on *real* datasets such as Imagenet and Waterbirds-Landbirds indeed depend on a low dimensional projection of the inputs, thereby demonstrating SB on these datasets, iv) finally, we present a natural ensemble approach that encourages diversity in models by training successive models on features not used by earlier models, and demonstrate that it yields models that are significantly more robust to Gaussian noise",
    "keywords": [],
    "checked": true,
    "id": "2b7bec81a6ece230ae9361c22336f8a0c70ada5d",
    "semantic_title": "simplicity bias in 1-hidden layer neural networks",
    "citation_count": 6,
    "authors": [
      "Depen Morwani",
      "Jatin Batra",
      "Prateek Jain",
      "Praneeth Netrapalli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/19a567abaec3990cb40d7a013556fecd-Abstract-Conference.html": {
    "title": "Temporally Disentangled Representation Learning under Unknown Nonstationarity",
    "volume": "main",
    "abstract": "In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure.However, in nonstationary setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios.In this study, we further explored the Markov Assumption under time-delayed causally related process in nonstationary setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, without the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to reconstruct time-delayed latent causal variables and identify their relations from measured sequential data only.Empirical evaluations demonstrated the reliable identification of time-delayed latent causal influences, with our methodology substantially outperforming existing baselines that fail to exploit the nonstationarity adequately and then, consequently, cannot distinguish distribution shifts",
    "keywords": [],
    "checked": true,
    "id": "99dadc987c90626e3c0b0adcf694b6a4b5bd3901",
    "semantic_title": "temporally disentangled representation learning under unknown nonstationarity",
    "citation_count": 1,
    "authors": [
      "Xiangchen Song",
      "Weiran Yao",
      "Yewen Fan",
      "Xinshuai Dong",
      "Guangyi Chen",
      "Juan Carlos Niebles",
      "Eric Xing",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/19c9708f31ec44b5b1cbd67f91d05d95-Abstract-Conference.html": {
    "title": "Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization",
    "volume": "main",
    "abstract": "In this paper, we propose an accelerated quasi-Newton proximal extragradient method for solving unconstrained smooth convex optimization problems. With access only to the gradients of the objective, we prove that our method can achieve a convergence rate of $\\mathcal{O}\\bigl(\\min\\\\{\\frac{1}{k^2}, \\frac{\\sqrt{d\\log k}}{k^{2.5}}\\\\}\\bigr)$, where $d$ is the problem dimension and $k$ is the number of iterations. In particular, in the regime where $k = \\mathcal{O}(d)$, our method matches the _optimal rate_ of $\\mathcal{O}(\\frac{1}{k^2})$ by Nesterov's accelerated gradient (NAG). Moreover, in the the regime where $k = \\Omega(d \\log d)$, it outperforms NAG and converges at a _faster rate_ of $\\mathcal{O}\\bigl(\\frac{\\sqrt{d\\log k}}{k^{2.5}}\\bigr)$. To the best of our knowledge, this result is the first to demonstrate a provable gain for a quasi-Newton-type method over NAG in the convex setting. To achieve such results, we build our method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an online learning perspective to update the Hessian approximation matrices, in which we relate the convergence rate of our method to the dynamic regret of a specific online convex optimization problem in the space of matrices",
    "keywords": [],
    "checked": true,
    "id": "2daaba077a441522b00b8431e0964c94b0d0636b",
    "semantic_title": "accelerated quasi-newton proximal extragradient: faster rate for smooth convex optimization",
    "citation_count": 2,
    "authors": [
      "Ruichen Jiang",
      "Aryan Mokhtari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/19d7204af519eae9993f7f72377a0ec0-Abstract-Conference.html": {
    "title": "Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference",
    "volume": "main",
    "abstract": "We propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves inference efficiency. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation.Starting with an existing dense pretrained model, CoDA adds sparse activation together with a small number of new parameters and a light-weight training phase.Our experiments demonstrate that the CoDA approach provides an unexpectedly efficient way to transfer knowledge.Across a variety of language, vision, and speech tasks, CoDA achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter approaches with moderate to no accuracy loss and the same parameter efficiency",
    "keywords": [],
    "checked": true,
    "id": "148644bf4ccef7e022b965304e8b3178be8af0fa",
    "semantic_title": "conditional adapters: parameter-efficient transfer learning with fast inference",
    "citation_count": 8,
    "authors": [
      "Tao Lei",
      "Junwen Bai",
      "Siddhartha Brahma",
      "Joshua Ainslie",
      "Kenton Lee",
      "Yanqi Zhou",
      "Nan Du",
      "Vincent Zhao",
      "Yuexin Wu",
      "Bo Li",
      "Yu Zhang",
      "Ming-Wei Chang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/19dbb86f771ddbf9986cf0c9b1c61c17-Abstract-Conference.html": {
    "title": "Time-Independent Information-Theoretic Generalization Bounds for SGLD",
    "volume": "main",
    "abstract": "We provide novel information-theoretic generalization bounds for stochastic gradient Langevin dynamics (SGLD) under the assumptions of smoothness and dissipativity, which are widely used in sampling and non-convex optimization studies.Our bounds are time-independent and decay to zero as the sample size increases, regardless of the number of iterations and whether the step size is fixed.Unlike previous studies, we derive the generalization error bounds by focusing on the time evolution of the Kullback--Leibler divergence, which is related to the stability of datasets and is the upper bound of the mutual information between output parameters and an input dataset.Additionally, we establish the first information-theoretic generalization bound when the training and test loss are the same by showing that a loss function of SGLD is sub-exponential.This bound is also time-independent and removes the problematic step size dependence in existing work, leading to an improved excess risk bound by combining our analysis with the existing non-convex optimization error bounds",
    "keywords": [],
    "checked": true,
    "id": "b5c2731e22892e627bee940b47cd02e0e1c3487b",
    "semantic_title": "time-independent information-theoretic generalization bounds for sgld",
    "citation_count": 0,
    "authors": [
      "Futoshi Futami",
      "Masahiro Fujisawa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/19ded4cfc36a7feb7fce975393d378fd-Abstract-Conference.html": {
    "title": "Topology-Aware Uncertainty for Image Segmentation",
    "volume": "main",
    "abstract": "Segmentation of curvilinear structures such as vasculature and road networks is challenging due to relatively weak signals and complex geometry/topology. To facilitate and accelerate large scale annotation, one has to adopt semi-automatic approaches such as proofreading by experts. In this work, we focus on uncertainty estimation for such tasks, so that highly uncertain, and thus error-prone structures can be identified for human annotators to verify. Unlike most existing works, which provide pixel-wise uncertainty maps, we stipulate it is crucial to estimate uncertainty in the units of topological structures, e.g., small pieces of connections and branches. To achieve this, we leverage tools from topological data analysis, specifically discrete Morse theory (DMT), to first capture the structures, and then reason about their uncertainties. To model the uncertainty, we (1) propose a joint prediction model that estimates the uncertainty of a structure while taking the neighboring structures into consideration (inter-structural uncertainty); (2) propose a novel Probabilistic DMT to model the inherent uncertainty within each structure (intra-structural uncertainty) by sampling its representations via a perturb-and-walk scheme. On various 2D and 3D datasets, our method produces better structure-wise uncertainty maps compared to existing works. Code available at: https://github.com/Saumya-Gupta-26/struct-uncertainty",
    "keywords": [],
    "checked": true,
    "id": "77d1739b5433c3b3dd49de64407954a3e80c101f",
    "semantic_title": "topology-aware uncertainty for image segmentation",
    "citation_count": 4,
    "authors": [
      "Saumya Gupta",
      "Yikai Zhang",
      "Xiaoling Hu",
      "Prateek Prasanna",
      "Chao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/19df21cd4931bd0caaa4d8480e9a59cd-Abstract-Conference.html": {
    "title": "Multiplication-Free Transformer Training via Piecewise Affine Operations",
    "volume": "main",
    "abstract": "Multiplications are responsible for most of the computational cost involved in neural network training and inference. Recent research has thus looked for ways to reduce the cost associated with them. Inspired by Mogami 2020, we replace multiplication with a cheap piecewise affine approximation that is achieved by adding the bit representation of the floating point numbers together as integers. We show that transformers can be trained with the resulting modified matrix multiplications on both vision and language tasks with little to no performance impact, and without changes to the training hyperparameters. We further replace all non-linearities in the networks making them fully and jointly piecewise affine in both inputs and weights. Finally, we show that we can eliminate all multiplications in the entire training process, including operations in the forward pass, backward pass and optimizer update, demonstrating the first successful training of modern neural network architectures in a fully multiplication-free fashion",
    "keywords": [],
    "checked": true,
    "id": "174993c91bc096802d34d589d4cff78fbd753132",
    "semantic_title": "multiplication-free transformer training via piecewise affine operations",
    "citation_count": 0,
    "authors": [
      "Atli Kosson",
      "Martin Jaggi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1a04df6a405210aab4986994b873db9b-Abstract-Conference.html": {
    "title": "A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing",
    "volume": "main",
    "abstract": "In generative compressed sensing (GCS), we want to recover a signal $\\mathbf{x^*}\\in\\mathbb{R}^n$ from $m$ measurements ($m\\ll n$) using a generative prior $\\mathbf{x^*}\\in G(\\mathbb{B}_2^k(r))$, where $G$ is typically an $L$-Lipschitz continuous generative model and $\\mathbb{B}_2^k(r)$ represents the radius-$r$ $\\ell_2$-ball in $\\mathbb{R}^k$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $\\mathbf{x^*}$ rather than for all $\\mathbf{x^*}$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index model as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, all $\\mathbf{x^*}\\in G(\\mathbb{B}_2^k(r))$ can be recovered up to an $\\ell_2$-error at most $\\epsilon$ using roughly $\\tilde{O}({k}/{\\epsilon^2})$ samples, with omitted logarithmic factors typically being dominated by $\\log L$. Notably, this almost coincides with existing non-uniform guarantees up to logarithmic factors, hence the uniformity costs very little. As part of our technical contributions, we introduce Lipschitz approximation to handle discontinuous observation models. We also develop a concentration inequality that produces tighter bound for product process whose index sets have low metric entropy. Experimental results are presented to corroborate our theory",
    "keywords": [],
    "checked": true,
    "id": "97e2213e594322fcc6c54f6ff943842d39d1e67a",
    "semantic_title": "a unified framework for uniform signal recovery in nonlinear generative compressed sensing",
    "citation_count": 1,
    "authors": [
      "Junren Chen",
      "Jonathan Scarlett",
      "Michael Ng",
      "Zhaoqiang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1a0672689a693e0764f93f900488b3d9-Abstract-Conference.html": {
    "title": "Tempo Adaptation in Non-stationary Reinforcement Learning",
    "volume": "main",
    "abstract": "We first raise and tackle a ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($t$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $t \\in [0, T]$. In existing works, at episode $k$, the agent rolls a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $t_{k}$ allocates $\\Delta t$ for trajectory generation and training, subsequently moves to the next episode at $t_{k+1}=t_{k}+\\Delta t$. Despite a fixed total number of episodes ($K$), the agent accumulates different trajectories influenced by the choice of interaction times ($t_1,t_2,...,t_K$), significantly impacting the suboptimality gap of the policy. We propose a Proactively Synchronizing Tempo ($\\texttt{ProST}$) framework that computes a suboptimal sequence {$t_1,t_2,...,t_K$} (= { $t_{1:K}$}) by minimizing an upper bound on its performance measure, i.e., the dynamic regret. Our main contribution is that we show that a suboptimal {$t_{1:K}$} trades-off between the policy training time (agent tempo) and how fast the environment changes (environment tempo). Theoretically, this work develops a suboptimal {$t_{1:K}$} as a function of the degree of the environment's non-stationarity while also achieving a sublinear dynamic regret. Our experimental evaluation on various high-dimensional non-stationary environments shows that the $\\texttt{ProST}$ framework achieves a higher online return at suboptimal {$t_{1:K}$} than the existing methods",
    "keywords": [],
    "checked": true,
    "id": "b8e843d199ab6807eb8d7758fc44e430a08e9837",
    "semantic_title": "tempo adaptation in non-stationary reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Hyunin Lee",
      "Yuhao Ding",
      "Jongmin Lee",
      "Ming Jin",
      "Javad Lavaei",
      "Somayeh Sojoudi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1a074a28c3a6f2056562d00649ae6416-Abstract-Conference.html": {
    "title": "Unsupervised Semantic Correspondence Using Stable Diffusion",
    "volume": "main",
    "abstract": "Text-to-image diffusion models are now capable of generating images that are often indistinguishable from real images. To generate such images, these models must understand the semantics of the objects they are asked to generate. In this work we show that, without any training, one can leverage this semantic knowledge within diffusion models to find semantic correspondences – locations in multiple images that have the same semantic meaning. Specifically, given an image, we optimize the prompt embeddings of these models for maximum attention on the regions of interest. These optimized embeddings capture semantic information about the location, which can then be transferred to another image. By doing so we obtain results on par with the strongly supervised state of the art on the PF-Willow dataset and significantly outperform (20.9% relative for the SPair-71k dataset) any existing weakly- or unsupervised method on PF-Willow, CUB-200 and SPair-71k datasets",
    "keywords": [],
    "checked": true,
    "id": "e49b1b6227afbe16f01174a72dbf2868915f5aac",
    "semantic_title": "unsupervised semantic correspondence using stable diffusion",
    "citation_count": 17,
    "authors": [
      "Eric Hedlin",
      "Gopal Sharma",
      "Shweta Mahajan",
      "Hossam Isack",
      "Abhishek Kar",
      "Andrea Tagliasacchi",
      "Kwang Moo Yi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1a2b4aba905a16733ff199888ac8eec4-Abstract-Conference.html": {
    "title": "Efficient Subgame Refinement for Extensive-form Games",
    "volume": "main",
    "abstract": "Subgame solving is an essential technique in addressing large imperfect information games, with various approaches developed to enhance the performance of refined strategies in the abstraction of the target subgame. However, directly applying existing subgame solving techniques may be difficult, due to the intricate nature and substantial size of many real-world games. To overcome this issue, recent subgame solving methods allow for subgame solving on limited knowledge order subgames, increasing their applicability in large games; yet this may still face obstacles due to extensive information set sizes. To address this challenge, we propose a generative subgame solving (GS2) framework, which utilizes a generation function to identify a subset of the earliest-reached nodes, reducing the size of the subgame. Our method is supported by a theoretical analysis and employs a diversity-based generation function to enhance safety. Experiments conducted on medium-sized games as well as the challenging large game of GuanDan demonstrate a significant improvement over the blueprint",
    "keywords": [],
    "checked": false,
    "id": "e74d0a1ba493420145f641f438d303952c78a978",
    "semantic_title": "safe subgame resolving for extensive form correlated equilibrium",
    "citation_count": 0,
    "authors": [
      "Zhenxing Ge",
      "Zheng Xu",
      "Tianyu Ding",
      "Wenbin Li",
      "Yang Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1a57081f257da7b440b8eda72a0b12d4-Abstract-Conference.html": {
    "title": "NeRF-IBVS: Visual Servo Based on NeRF for Visual Localization and Navigation",
    "volume": "main",
    "abstract": "Visual localization is a fundamental task in computer vision and robotics. Training existing visual localization methods requires a large number of posed images to generalize to novel views, while state-of-the-art methods generally require dense ground truth 3D labels for supervision. However, acquiring a large number of posed images and dense 3D labels in the real world is challenging and costly. In this paper, we present a novel visual localization method that achieves accurate localization while using only a few posed images compared to other localization methods. To achieve this, we first use a few posed images with coarse pseudo-3D labels provided by NeRF to train a coordinate regression network. Then a coarse pose is estimated from the regression network with PNP. Finally, we use the image-based visual servo (IBVS) with the scene prior provided by NeRF for pose optimization. Furthermore, our method can provide effective navigation prior, which enable navigation based on IBVS without using custom markers and depth sensor. Extensive experiments on 7-Scenes and 12-Scenes datasets demonstrate that our method outperforms state-of-the-art methods under the same setting, with only 5\\% to 25\\% training data. Furthermore, our framework can be naturally extended to the visual navigation task based on IBVS, and its effectiveness is verified in simulation experiments",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanze Wang",
      "Yichao Yan",
      "Dianxi Shi",
      "Wenhan Zhu",
      "Jianqiang Xia",
      "Tan Jeff",
      "Songchang Jin",
      "KE GAO",
      "XIAOBO LI",
      "Xiaokang Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1a5e6d0441a8e1eda9a50717b0870f94-Abstract-Conference.html": {
    "title": "How Does Adaptive Optimization Impact Local Neural Network Geometry?",
    "volume": "main",
    "abstract": "Adaptive optimization methods are well known to achieve superior convergence relative to vanilla gradient methods. The traditional viewpoint in optimization, particularly in convex optimization, explains this improved performance by arguing that, unlike vanilla gradient schemes, adaptive algorithms mimic the behavior of a second-order method by adapting to the *global* geometry of the loss function. We argue that in the context of neural network optimization, this traditional viewpoint is insufficient. Instead, we advocate for a *local* trajectory analysis. For iterate trajectories produced by running a generic optimization algorithm OPT, we introduce $R^{\\text{OPT}}\\_{\\text{med}}$, a statistic that is analogous to the condition number of the loss Hessian evaluated at the iterates. Through extensive experiments on language models where adaptive algorithms converge faster than vanilla gradient methods like SGD, we show that adaptive methods such as Adam bias the trajectories towards regions where $R^{\\text{Adam}}_{\\text{med}}$ is small, where one might expect faster optimization. By contrast, SGD (with momentum) biases the trajectories towards regions where $R^{\\text{SGD}}\\_{\\text{med}}$ is comparatively large. We complement these empirical observations with a theoretical result that provably demonstrates this phenomenon in the simplified setting of a two-layer linear network. We view our findings as evidence for the need of a new explanation of the success of adaptive methods, one that is different than the conventional wisdom",
    "keywords": [],
    "checked": true,
    "id": "3b823616b94475a07fb646c9387cb0bb924edd91",
    "semantic_title": "how does adaptive optimization impact local neural network geometry?",
    "citation_count": 6,
    "authors": [
      "Kaiqi Jiang",
      "Dhruv Malik",
      "Yuanzhi Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1a675d804f50509b8e21d0d3ca709d03-Abstract-Conference.html": {
    "title": "Are Diffusion Models Vision-And-Language Reasoners?",
    "volume": "main",
    "abstract": "Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality.Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM.Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis.We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground.We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining generative capabilities. We also measure the stereotypical bias in diffusion models, and find that Stable Diffusion 2.1 is, for the most part, less biased than Stable Diffusion 1.5.Overall, our results point in an exciting direction bringing discriminative and generative model evaluation closer. We will release code and benchmark setup soon",
    "keywords": [],
    "checked": true,
    "id": "14cb918ee6b9aab981c7b01c2c4db3ee4016abae",
    "semantic_title": "are diffusion models vision-and-language reasoners?",
    "citation_count": 5,
    "authors": [
      "Benno Krojer",
      "Elinor Poole-Dayan",
      "Vikram Voleti",
      "Chris Pal",
      "Siva Reddy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1a87980b9853e84dfb295855b425c262-Abstract-Conference.html": {
    "title": "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation",
    "volume": "main",
    "abstract": "Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present *variational score distillation* (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., 7.5). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed *ProlificDreamer*, can generate high rendering resolution (i.e., 512$\\times$512) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic",
    "keywords": [],
    "checked": true,
    "id": "c5e9fd131cde68c218d0ea69cd617a67c7f35d42",
    "semantic_title": "prolificdreamer: high-fidelity and diverse text-to-3d generation with variational score distillation",
    "citation_count": 204,
    "authors": [
      "Zhengyi Wang",
      "Cheng Lu",
      "Yikai Wang",
      "Fan Bao",
      "Chongxuan LI",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1a8d295871250443f9747d239925b89d-Abstract-Conference.html": {
    "title": "SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise",
    "volume": "main",
    "abstract": "The well-established practice of time series analysis involves estimating deterministic, non-stationary trend and seasonality components followed by learning the residual stochastic, stationary components. Recently, it has been shown that one can learn the deterministic non-stationary components accurately using multivariate Singular Spectrum Analysis (mSSA) in the absence of a correlated stationary component; meanwhile, in the absence of deterministic non-stationary components, the Autoregressive (AR) stationary component can also be learnt readily, e.g. via Ordinary Least Squares (OLS). However, a theoretical underpinning of multi-stage learning algorithms involving both deterministic and stationary components has been absent in the literature despite its pervasiveness. We resolve this open question by establishing desirable theoretical guarantees for a natural two-stage algorithm, where mSSA is first applied to estimate the non-stationary components despite the presence of a correlated stationary AR component, which is subsequently learned from the residual time series. We provide a finite-sample forecasting consistency bound for the proposed algorithm, SAMoSSA, which is data-driven and thus requires minimal parameter tuning. To establish theoretical guarantees, we overcome three hurdles: (i) we characterize the spectra of Page matrices of stable AR processes, thus extending the analysis of mSSA; (ii) we extend the analysis of AR process identification in the presence of arbitrary bounded perturbations; (iii) we characterize the out-of-sample or forecasting error, as opposed to solely considering model identification. Through representative empirical studies, we validate the superior performance of SAMoSSA compared to existing baselines. Notably, SAMoSSA's ability to account for AR noise structure yields improvements ranging from 5% to 37% across various benchmark datasets",
    "keywords": [],
    "checked": true,
    "id": "af0348256beec50cb4cca32a66c359ae68945430",
    "semantic_title": "samossa: multivariate singular spectrum analysis with stochastic autoregressive noise",
    "citation_count": 0,
    "authors": [
      "Abdullah Alomar",
      "Munther Dahleh",
      "Sean Mann",
      "Devavrat Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1abc87c67cc400a67b869358e627fe37-Abstract-Conference.html": {
    "title": "Hierarchical Vector Quantized Transformer for Multi-class Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "Unsupervised image Anomaly Detection (UAD) aims to learn robust and discriminative representations of normal samples. While separate solutions per class endow expensive computation and limited generalizability, this paper focuses on building a unified framework for multiple classes. Under such a challenging setting, popular reconstruction-based networks with continuous latent representation assumption always suffer from the \"identical shortcut\" issue, where both normal and abnormal samples can be well recovered and difficult to distinguish. To address this pivotal issue, we propose a hierarchical vector quantized prototype-oriented Transformer under a probabilistic framework. First, instead of learning the continuous representations, we preserve the typical normal patterns as discrete iconic prototypes, and confirm the importance of Vector Quantization in preventing the model from falling into the shortcut. The vector quantized iconic prototypes are integrated into the Transformer for reconstruction, such that the abnormal data point is flipped to a normal data point. Second, we investigate an exquisite hierarchical framework to relieve the codebook collapse issue and replenish frail normal patterns. Third, a prototype-oriented optimal transport method is proposed to better regulate the prototypes and hierarchically evaluate the abnormal score. By evaluating on MVTec-AD and VisA datasets, our model surpasses the state-of-the-art alternatives and possesses good interpretability. The code is available at https://github.com/RuiyingLu/HVQ-Trans",
    "keywords": [],
    "checked": true,
    "id": "d2ad908b75063cbfde78e99c3b1f44c60a1ae311",
    "semantic_title": "hierarchical vector quantized transformer for multi-class unsupervised anomaly detection",
    "citation_count": 0,
    "authors": [
      "Ruiying Lu",
      "YuJie Wu",
      "Long Tian",
      "Dongsheng Wang",
      "Bo Chen",
      "Xiyang Liu",
      "Ruimin Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1ae4999aefb509d75d8608e07280922c-Abstract-Conference.html": {
    "title": "MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited Memory",
    "volume": "main",
    "abstract": "Due to the high price and heavy energy consumption of GPUs, deploying deep models on IoT devices such as microcontrollers makes significant contributions for ecological AI. Conventional methods successfully enable convolutional neural network inference of high resolution images on microcontrollers, while the framework for vision transformers that achieve the state-of-the-art performance in many vision applications still remains unexplored. In this paper, we propose a hardware-algorithm co-optimizations method called MCUFormer to deploy vision transformers on microcontrollers with extremely limited memory, where we jointly design transformer architecture and construct the inference operator library to fit the memory resource constraint. More specifically, we generalize the one-shot network architecture search (NAS) to discover the optimal architecture with highest task performance given the memory budget from the microcontrollers, where we enlarge the existing search space of vision transformers by considering the low-rank decomposition dimensions and patch resolution for memory reduction. For the construction of the inference operator library of vision transformers, we schedule the memory buffer during inference through operator integration, patch embedding decomposition, and token overwriting, allowing the memory buffer to be fully utilized to adapt to the forward pass of the vision transformer. Experimental results demonstrate that our MCUFormer achieves 73.62\\% top-1 accuracy on ImageNet for image classification with 320KB memory on STM32F746 microcontroller. Code is available at https://github.com/liangyn22/MCUFormer",
    "keywords": [],
    "checked": true,
    "id": "c6eeb8cd7fcf4637d7278e82f7220d1a5daee654",
    "semantic_title": "mcuformer: deploying vision tranformers on microcontrollers with limited memory",
    "citation_count": 0,
    "authors": [
      "Yinan Liang",
      "Ziwei Wang",
      "Xiuwei Xu",
      "Yansong Tang",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1af3e0bf5905e33789979f666c31192d-Abstract-Conference.html": {
    "title": "Towards Accelerated Model Training via Bayesian Data Selection",
    "volume": "main",
    "abstract": "Mislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy to implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can achieve similar predictive performance with significantly fewer training iterations than leading data selection methods",
    "keywords": [],
    "checked": true,
    "id": "8bf0045e1807d418c6924476b9643f32d2c715f8",
    "semantic_title": "towards accelerated model training via bayesian data selection",
    "citation_count": 0,
    "authors": [
      "Zhijie Deng",
      "Peng Cui",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b0da24d136f46bfaee78e8da907127e-Abstract-Conference.html": {
    "title": "CSOT: Curriculum and Structure-Aware Optimal Transport for Learning with Noisy Labels",
    "volume": "main",
    "abstract": "Learning with noisy labels (LNL) poses a significant challenge in training a well-generalized model while avoiding overfitting to corrupted labels.Recent advances have achieved impressive performance by identifying clean labels and correcting corrupted labels for training.However, the current approaches rely heavily on the model's predictions and evaluate each sample independently without considering either the global or local structure of the sample distribution.These limitations typically result in a suboptimal solution for the identification and correction processes, which eventually leads to models overfitting to incorrect labels.In this paper, we propose a novel optimal transport (OT) formulation, called Curriculum and Structure-aware Optimal Transport (CSOT). CSOT concurrently considers the inter- and intra-distribution structure of the samples to construct a robust denoising and relabeling allocator.During the training process, the allocator incrementally assigns reliable labels to a fraction of the samples with the highest confidence. These labels have both global discriminability and local coherence.Notably, CSOT is a new OT formulation with a nonconvex objective function and curriculum constraints, so it is not directly compatible with classical OT solvers. Here, we develop a lightspeed computational method that involves a scaling iteration within a generalized conditional gradient framework to solve CSOT efficiently.Extensive experiments demonstrate the superiority of our method over the current state-of-the-arts in LNL",
    "keywords": [],
    "checked": true,
    "id": "8b7d6bee53b30df13694eba44cb2ae6af85a4e80",
    "semantic_title": "csot: curriculum and structure-aware optimal transport for learning with noisy labels",
    "citation_count": 1,
    "authors": [
      "Wanxing Chang",
      "Ye Shi",
      "Jingya Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b3750390ca8b931fb9ca988647940cb-Abstract-Conference.html": {
    "title": "In-Context Learning Unlocked for Diffusion Models",
    "volume": "main",
    "abstract": "We present Prompt Diffusion, a framework for enabling in-context learning in diffusion-based generative models. Given a pair of task-specific example images, such as depth from/to image and scribble from/to image, and a text guidance, our model automatically understands the underlying task and performs the same task on a new query image following the text guidance. To achieve this, we propose a vision-language prompt that can model a wide range of vision-language tasks and a diffusion model that takes it as input. The diffusion model is trained jointly on six different tasks using these prompts. The resulting Prompt Diffusion model becomes the first diffusion-based vision-language foundation model capable of in-context learning. It demonstrates high-quality in-context generation for the trained tasks and effectively generalizes to new, unseen vision tasks using their respective prompts. Our model also shows compelling text-guided image editing results. Our framework aims to facilitate research into in-context learning for computer vision. We share our code and pre-trained models at https://github.com/Zhendong-Wang/Prompt-Diffusion",
    "keywords": [],
    "checked": true,
    "id": "57be0448d168e8d6d0b6e0d1a4405fb5fbaa1b56",
    "semantic_title": "in-context learning unlocked for diffusion models",
    "citation_count": 20,
    "authors": [
      "Zhendong Wang",
      "Yifan Jiang",
      "Yadong Lu",
      "yelong shen",
      "Pengcheng He",
      "Weizhu Chen",
      "Zhangyang \"Atlas\" Wang",
      "Mingyuan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b3ceb8a495a63ced4a48f8429ccdcd8-Abstract-Conference.html": {
    "title": "Object-Centric Slot Diffusion",
    "volume": "main",
    "abstract": "The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised compositional generation quality. In addition, we conduct a preliminary investigation into the integration of pre-trained diffusion models in LSD and demonstrate its effectiveness in real-world image segmentation and generation. Project page is available at https://latentslotdiffusion.github.io",
    "keywords": [],
    "checked": true,
    "id": "b298f65455dd4acbbc96e8aa43bd3673af480bcb",
    "semantic_title": "object-centric slot diffusion",
    "citation_count": 13,
    "authors": [
      "Jindong Jiang",
      "Fei Deng",
      "Gautam Singh",
      "Sungjin Ahn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b3d005a2cb0e71e698e0b13ac657473-Abstract-Conference.html": {
    "title": "NAS-X: Neural Adaptive Smoothing via Twisting",
    "volume": "main",
    "abstract": "Sequential latent variable models (SLVMs) are essential tools in statistics and machine learning, with applications ranging from healthcare to neuroscience. As their flexibility increases, analytic inference and model learning can become challenging, necessitating approximate methods. Here we introduce neural adaptive smoothing via twisting (NAS-X), a method that extends reweighted wake-sleep (RWS) to the sequential setting by using smoothing sequential Monte Carlo (SMC) to estimate intractable posterior expectations. Combining RWS and smoothing SMC allows NAS-X to provide low-bias and low-variance gradient estimates, and fit both discrete and continuous latent variable models. We illustrate the theoretical advantages of NAS-X over previous methods and explore these advantages empirically in a variety of tasks, including a challenging application to mechanistic models of neuronal dynamics. These experiments show that NAS-X substantially outperforms previous VI- and RWS-based methods in inference and model learning, achieving lower parameter error and tighter likelihood bounds",
    "keywords": [],
    "checked": true,
    "id": "c74de5197a2ccebb294cecafca21c1388b276b00",
    "semantic_title": "nas-x: neural adaptive smoothing via twisting",
    "citation_count": 1,
    "authors": [
      "Dieterich Lawson",
      "Michael Li",
      "Scott Linderman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html": {
    "title": "Reflexion: language agents with verbal reinforcement learning",
    "volume": "main",
    "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose \\emph{Reflexion}, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance. We release all code, demos, and datasets at \\url{https://github.com/noahshinn024/reflexion}",
    "keywords": [],
    "checked": true,
    "id": "0671fd553dd670a4e820553a974bc48040ba0819",
    "semantic_title": "reflexion: language agents with verbal reinforcement learning",
    "citation_count": 231,
    "authors": [
      "Noah Shinn",
      "Federico Cassano",
      "Ashwin Gopinath",
      "Karthik Narasimhan",
      "Shunyu Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b4acad19cc425a7352a71d4e4468393-Abstract-Conference.html": {
    "title": "Demographic Parity Constrained Minimax Optimal Regression under Linear Model",
    "volume": "main",
    "abstract": "We explore the minimax optimal error associated with a demographic parity-constrained regression problem within the context of a linear model. Our proposed model encompasses a broader range of discriminatory bias sources compared to the model presented by Chzhen and Schreuder. Our analysis reveals that the minimax optimal error for the demographic parity-constrained regression problem under our model is characterized by $\\Theta(\\frac{dM}{n})$, where $n$ denotes the sample size, $d$ represents the dimensionality, and $M$ signifies the number of demographic groups arising from sensitive attributes. Moreover, we demonstrate that the minimax error increases in conjunction with a larger bias present in the model",
    "keywords": [],
    "checked": true,
    "id": "f8545b45a64945f56ed879d078852a9f1e973bf2",
    "semantic_title": "demographic parity constrained minimax optimal regression under linear model",
    "citation_count": 1,
    "authors": [
      "Kazuto Fukuchi",
      "Jun Sakuma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b57aaddf85ab01a2445a79c9edc1f4b-Abstract-Conference.html": {
    "title": "GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization",
    "volume": "main",
    "abstract": "Worldwide Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth. This task has considerable challenges due to the immense variation in geographic landscapes. The image-to-image retrieval-based approaches fail to solve this problem on a global scale as it is not feasible to construct a large gallery of images covering the entire world. Instead, existing approaches divide the globe into discrete geographic cells, transforming the problem into a classification task. However, their performance is limited by the predefined classes and often results in inaccurate localizations when an image's location significantly deviates from its class center. To overcome these limitations, we propose GeoCLIP, a novel CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between the image and its corresponding GPS locations. GeoCLIP's location encoder models the Earth as a continuous function by employing positional encoding through random Fourier features and constructing a hierarchical representation that captures information at varying resolutions to yield a semantically rich high-dimensional feature suitable to use even beyond geo-localization. To the best of our knowledge, this is the first work employing GPS encoding for geo-localization. We demonstrate the efficacy of our method via extensive experiments and ablations on benchmark datasets. We achieve competitive performance with just 20% of training data, highlighting its effectiveness even in limited-data settings. Furthermore, we qualitatively demonstrate geo-localization using a text query by leveraging the CLIP backbone of our image encoder. The project webpage is available at: https://vicentevivan.github.io/GeoCLIP",
    "keywords": [],
    "checked": true,
    "id": "01fc730e06c4d3d02840af2ebae083634998b566",
    "semantic_title": "geoclip: clip-inspired alignment between locations and images for effective worldwide geo-localization",
    "citation_count": 3,
    "authors": [
      "Vicente Vivanco Cepeda",
      "Gaurav Kumar Nayak",
      "Mubarak Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b80fe066fdbceb3a2960117bac33917-Abstract-Conference.html": {
    "title": "RECESS Vaccine for Federated Learning: Proactive Defense Against Model Poisoning Attacks",
    "volume": "main",
    "abstract": "Model poisoning attacks greatly jeopardize the application of federated learning (FL). The effectiveness of existing defenses is susceptible to the latest model poisoning attacks, leading to a decrease in prediction accuracy. Besides, these defenses are intractable to distinguish benign outliers from malicious gradients, which further compromises the model generalization. In this work, we propose a novel defense including detection and aggregation, named RECESS, to serve as a \"vaccine\" for FL against model poisoning attacks. Different from the passive analysis in previous defenses, RECESS proactively queries each participating client with a delicately constructed aggregation gradient, accompanied by the detection of malicious clients according to their responses with higher accuracy. Further, RECESS adopts a newly proposed trust scoring based mechanism to robustly aggregate gradients. Rather than previous methods of scoring in each iteration, RECESS takes into account the correlation of clients' performance over multiple iterations to estimate the trust score, bringing in a significant increase in detection fault tolerance. Finally, we extensively evaluate RECESS on typical model architectures and four datasets under various settings including white/black-box, cross-silo/device FL, etc. Experimental results show the superiority of RECESS in terms of reducing accuracy loss caused by the latest model poisoning attacks over five classic and two state-of-the-art defenses",
    "keywords": [],
    "checked": true,
    "id": "947573bcd83fe6985749e08e15402f37f5d69136",
    "semantic_title": "recess vaccine for federated learning: proactive defense against model poisoning attacks",
    "citation_count": 0,
    "authors": [
      "Haonan Yan",
      "Wenjing Zhang",
      "Qian Chen",
      "Xiaoguang Li",
      "Wenhai Sun",
      "HUI LI",
      "Xiaodong Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b8612e11c75456c90963fd408d75c4d-Abstract-Conference.html": {
    "title": "Minimum norm interpolation by perceptra: Explicit regularization and implicit bias",
    "volume": "main",
    "abstract": "We investigate how shallow ReLU networks interpolate between known regions. Our analysis shows that empirical risk minimizers converge to a minimum norm interpolant as the number of data points and parameters tends to infinity when a weight decay regularizer is penalized with a coefficient which vanishes at a precise rate as the network width and the number of data points grow. With and without explicit regularization, we numerically study the implicit bias of common optimization algorithms towards known minimum norm interpolants",
    "keywords": [],
    "checked": true,
    "id": "b1c94d76c8772c656bc8fa2944747e277c4b1a2e",
    "semantic_title": "minimum norm interpolation by perceptra: explicit regularization and implicit bias",
    "citation_count": 0,
    "authors": [
      "Jiyoung Park",
      "Ian Pelakh",
      "Stephan Wojtowytsch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b86cf4b15cd83b6520d851eb7298228-Abstract-Conference.html": {
    "title": "Spectral Co-Distillation for Personalized Federated Learning",
    "volume": "main",
    "abstract": "Personalized federated learning (PFL) has been widely investigated to address the challenge of data heterogeneity, especially when a single generic model is inadequate in satisfying the diverse performance requirements of local clients simultaneously. Existing PFL methods are inherently based on the idea that the relations between the generic global and personalized local models are captured by the similarity of model weights. Such a similarity is primarily based on either partitioning the model architecture into generic versus personalized components or modeling client relationships via model weights. To better capture similar (yet distinct) generic versus personalized model representations, we propose $\\textit{spectral distillation}$, a novel distillation method based on model spectrum information. Building upon spectral distillation, we also introduce a co-distillation framework that establishes a two-way bridge between generic and personalized model training. Moreover, to utilize the local idle time in conventional PFL, we propose a wait-free local training protocol. Through extensive experiments on multiple datasets over diverse heterogeneous data settings, we demonstrate the outperformance and efficacy of our proposed spectral co-distillation method, as well as our wait-free training protocol",
    "keywords": [],
    "checked": false,
    "id": "101bbe43da72cea2be46ba5a118b103a17aacb74",
    "semantic_title": "feddtg: federated data-free knowledge distillation via three-player generative adversarial networks",
    "citation_count": 8,
    "authors": [
      "Zihan Chen",
      "Howard Yang",
      "Tony Quek",
      "Kai Fong Ernest Chong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1bd8cfc0e4c53869b7f1d0ed4b1e78e1-Abstract-Conference.html": {
    "title": "Gradient Informed Proximal Policy Optimization",
    "volume": "main",
    "abstract": "We introduce a novel policy learning method that integrates analytical gradients from differentiable environments with the Proximal Policy Optimization (PPO) algorithm. To incorporate analytical gradients into the PPO framework, we introduce the concept of an α-policy that stands as a locally superior policy. By adaptively modifying the α value, we can effectively manage the influence of analytical policy gradients during learning. To this end, we suggest metrics for assessing the variance and bias of analytical gradients, reducing dependence on these gradients when high variance or bias is detected. Our proposed approach outperforms baseline algorithms in various scenarios, such as function optimization, physics simulations, and traffic control environments. Our code can be found online: https://github.com/SonSang/gippo",
    "keywords": [],
    "checked": true,
    "id": "914876732247677f8f4c6a07ad7d222102dc8b0e",
    "semantic_title": "gradient informed proximal policy optimization",
    "citation_count": 0,
    "authors": [
      "Sanghyun Son",
      "Laura Zheng",
      "Ryan Sullivan",
      "Yi-Ling Qiao",
      "Ming Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1bfd87d2d92f0556819467dc08034f76-Abstract-Conference.html": {
    "title": "Blockwise Parallel Transformers for Large Context Models",
    "volume": "main",
    "abstract": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance",
    "keywords": [],
    "checked": true,
    "id": "adc8b62fd2bd644c140c7c42275a9d2d913ad8a8",
    "semantic_title": "blockwise parallel transformers for large context models",
    "citation_count": 1,
    "authors": [
      "Hao Liu",
      "Pieter Abbeel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c10d0c087c14689628124bbc8fa69f6-Abstract-Conference.html": {
    "title": "Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization",
    "volume": "main",
    "abstract": "Neural combinatorial optimization (NCO) is a promising learning-based approach for solving challenging combinatorial optimization problems without specialized algorithm design by experts. However, most constructive NCO methods cannot solve problems with large-scale instance sizes, which significantly diminishes their usefulness for real-world applications. In this work, we propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong generalization ability to address this critical issue. The LEHD model can learn to dynamically capture the relationships between all available nodes of varying sizes, which is beneficial for model generalization to problems of various scales. Moreover, we develop a data-efficient training scheme and a flexible solution construction mechanism for the proposed LEHD model. By training on small-scale problem instances, the LEHD model can generate nearly optimal solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 1000 nodes, and also generalizes well to solve real-world TSPLib and CVRPLib problems. These results confirm our proposed LEHD model can significantly improve the state-of-the-art performance for constructive NCO",
    "keywords": [],
    "checked": true,
    "id": "3c2f14a28ba5e826c63c294f4cca01a8217a217e",
    "semantic_title": "neural combinatorial optimization with heavy decoder: toward large scale generalization",
    "citation_count": 6,
    "authors": [
      "Fu Luo",
      "Xi Lin",
      "Fei Liu",
      "Qingfu Zhang",
      "Zhenkun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c12ccfc7720f6b680edea17300bfc2b-Abstract-Conference.html": {
    "title": "Topological Obstructions and How to Avoid Them",
    "volume": "main",
    "abstract": "Incorporating geometric inductive biases into models can aid interpretability and generalization, but encoding to a specific geometric structure can be challenging due to the imposed topological constraints. In this paper, we theoretically and empirically characterize obstructions to training encoders with geometric latent spaces. We show that local optima can arise due to singularities (e.g. self-intersection) or due to an incorrect degree or winding number. We then discuss how normalizing flows can potentially circumvent these obstructions by defining multimodal variational distributions. Inspired by this observation, we propose a new flow-based model that maps data points to multimodal distributions over geometric spaces and empirically evaluate our model on 2 domains. We observe improved stability during training and a higher chance of converging to a homeomorphic encoder",
    "keywords": [],
    "checked": true,
    "id": "ba3a342180aaec6e0f624b93b5441e00f2bab3a7",
    "semantic_title": "topological obstructions and how to avoid them",
    "citation_count": 0,
    "authors": [
      "Babak Esmaeili",
      "Robin Walters",
      "Heiko Zimmermann",
      "Jan-Willem van de Meent"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c26c389d60ec419fd24b5fee5b35796-Abstract-Conference.html": {
    "title": "The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks",
    "volume": "main",
    "abstract": "In this work, we study the implications of the implicit bias of gradient flow on generalization and adversarial robustness in ReLU networks. We focus on a setting where the data consists of clusters and the correlations between cluster means are small, and show that in two-layer ReLU networks gradient flow is biased towards solutions that generalize well, but are vulnerable to adversarial examples. Our results hold even in cases where the network is highly overparameterized. Despite the potential for harmful overfitting in such settings, we prove that the implicit bias of gradient flow prevents it. However, the implicit bias also leads to non-robust solutions (susceptible to small adversarial $\\ell_2$-perturbations), even though robust networks that fit the data exist",
    "keywords": [],
    "checked": true,
    "id": "11a596869959f3a20d68f29dfa27992a6586b620",
    "semantic_title": "the double-edged sword of implicit bias: generalization vs. robustness in relu networks",
    "citation_count": 4,
    "authors": [
      "Spencer Frei",
      "Gal Vardi",
      "Peter Bartlett",
      "Nati Srebro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c364d98a5cdc426fd8c76fbb2c10e34-Abstract-Conference.html": {
    "title": "PromptRestorer: A Prompting Image Restoration Method with Degradation Perception",
    "volume": "main",
    "abstract": "We show that raw degradation features can effectively guide deep restoration models, providing accurate degradation priors to facilitate better restoration. While networks that do not consider them for restoration forget gradually degradation during the learning process, model capacity is severely hindered. To address this, we propose a Prompt}ing image Restorer, termed as PromptRestorer. Specifically, PromptRestorer contains two branches: a restoration branch and a prompting branch. The former is used to restore images, while the latter perceives degradation priors to prompt the restoration branch with reliable perceived content to guide the restoration process for better recovery. To better perceive the degradation which is extracted by a pre-trained model from given degradation observations, we propose a prompting degradation perception modulator, which adequately considers the characters of the self-attention mechanism and pixel-wise modulation, to better perceive the degradation priors from global and local perspectives. To control the propagation of the perceived content for the restoration branch, we propose gated degradation perception propagation, enabling the restoration branch to adaptively learn more useful features for better recovery. Extensive experimental results show that our PromptRestorer achieves state-of-the-art results on 4 image restoration tasks, including image deraining, deblurring, dehazing, and desnowing",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wang",
      "Jinshan Pan",
      "Wei Wang",
      "Jiangxin Dong",
      "Mengzhu Wang",
      "Yakun Ju",
      "Junyang Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c3d419b754cb4de0a67a453cb28d959-Abstract-Conference.html": {
    "title": "Beyond MLE: Convex Learning for Text Generation",
    "volume": "main",
    "abstract": "Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the optimal distribution, thereby enabling the model to better capture outputs with high probabilities. Experiments on various text generation tasks and models show the effectiveness of our approach. It enables autoregressive models to bridge the gap between greedy and beam search, and facilitates the learning of non-autoregressive models with a maximum improvement of 9+ BLEU points. Moreover, our approach also exhibits significant impact on large language models (LLMs), substantially enhancing their generative capability on various tasks. Source code is available at \\url{https://github.com/ictnlp/Convex-Learning}",
    "keywords": [],
    "checked": true,
    "id": "02929c69092989315fd4afca9670f98137648878",
    "semantic_title": "beyond mle: convex learning for text generation",
    "citation_count": 0,
    "authors": [
      "Chenze Shao",
      "Zhengrui Ma",
      "Min Zhang",
      "Yang Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c5ee7343f396954377c2c16dda33a96-Abstract-Conference.html": {
    "title": "Bandit Task Assignment with Unknown Processing Time",
    "volume": "main",
    "abstract": "This study considers a novel problem setting, referred to as \\textit{bandit task assignment}, that incorporates the processing time of each task in the bandit setting. In this problem setting, a player sequentially chooses a set of tasks to start so that the set of processing tasks satisfies a given combinatorial constraint. The reward and processing time for each task follow unknown distributions, values of which are revealed only after the task has been completed. The problem generalizes the stochastic combinatorial semi-bandit problem and the budget-constrained bandit problem. For this problem setting, we propose an algorithm based on upper confidence bounds~(UCB) combined with a phased-update approach. The proposed algorithm admits a gap-dependent regret upper bound of $O(MN(1/\\Delta){\\log T})$ and a gap-free regret upper bound of $\\tilde{O}( \\sqrt{MNT} )$, where $N$ is the number of the tasks, $M$ is the maximum number of tasks run at the same time, $T$ is the time horizon, and $\\Delta$ is the gap between expected per-round rewards of the optimal and best suboptimal sets of tasks. These regret bounds nearly match lower bounds",
    "keywords": [],
    "checked": false,
    "id": "41ae33f875d9cf34133e8c295391f031f0b9698d",
    "semantic_title": "experience replay is associated with efficient nonlocal learning",
    "citation_count": 79,
    "authors": [
      "Shinji Ito",
      "Daisuke Hatano",
      "Hanna Sumita",
      "Kei Takemura",
      "Takuro Fukunaga",
      "Naonori Kakimura",
      "Ken-Ichi Kawarabayashi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c6f06863df46de009a7a41b41c95cad-Abstract-Conference.html": {
    "title": "Towards Self-Interpretable Graph-Level Anomaly Detection",
    "volume": "main",
    "abstract": "Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, explainable GLAD, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not only measure the abnormality of each graph based on cross-view mutual information but also provide informative graph rationales by extracting bottleneck subgraphs from the input graph and its dual hypergraph in a self-supervised way. Extensive experiments on 16 datasets demonstrate the anomaly detection capability and self-interpretability of SIGNET",
    "keywords": [],
    "checked": true,
    "id": "db27a18d04e22ddc91ff74204e074c1aaa6d239b",
    "semantic_title": "towards self-interpretable graph-level anomaly detection",
    "citation_count": 2,
    "authors": [
      "Yixin Liu",
      "Kaize Ding",
      "Qinghua Lu",
      "Fuyi Li",
      "Leo Yu Zhang",
      "Shirui Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c70ba3591d0694a535089e1c25888d7-Abstract-Conference.html": {
    "title": "AMAG: Additive, Multiplicative and Adaptive Graph Neural Network For Forecasting Neuron Activity",
    "volume": "main",
    "abstract": "Latent Variable Models (LVMs) propose to model the dynamics of neural populations by capturing low-dimensional structures that represent features involved in neural activity. Recent LVMs are based on deep learning methodology where a deep neural network is trained to reconstruct the same neural activity given as input and as a result to build the latent representation. Without taking past or future activity into account such a task is non-causal. In contrast, the task of forecasting neural activity based on given input extends the reconstruction task. LVMs that are trained on such a task could potentially capture temporal causality constraints within its latent representation. Forecasting has received less attention than reconstruction due to recording challenges such as limited neural measurements and trials. In this work, we address modeling neural population dynamics via the forecasting task and improve forecasting performance by including a prior, which consists of pairwise neural unit interaction as a multivariate dynamic system. Our proposed model---Additive, Multiplicative, and Adaptive Graph Neural Network (AMAG)---leverages additive and multiplicative message-passing operations analogous to the interactions in neuronal systems and adaptively learns the interaction among neural units to forecast their future activity. We demonstrate the advantage of AMAG compared to non-GNN based methods on synthetic data and multiple modalities of neural recordings (field potentials from penetrating electrodes or surface-level micro-electrocorticography) from four rhesus macaques. Our results show the ability of AMAG to recover ground truth spatial interactions and yield estimation for future dynamics of the neural population",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyuan Li",
      "Leo Scholl",
      "Trung Le",
      "Pavithra Rajeswaran",
      "Amy Orsborn",
      "Eli Shlizerman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c92edb990a05f2269f0cc3afbb4c952-Abstract-Conference.html": {
    "title": "PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile",
    "volume": "main",
    "abstract": "While Vision Transformers (ViTs) have undoubtedly made impressive strides in computer vision (CV), their intricate network structures necessitate substantial computation and memory resources. A decision-making process for CV tasks typically entails performing computations with low latency, which is a tricky problem for ViT models.Model quantization is a widely-used technique to optimize the hardware efficiency of deep neural networks.Full quantization under Sub-8-bit precision, in particular, is a promising solution to reduce inference latency significantly. Unfortunately, current commodity hardware, such as CPUs and GPUs, still struggles to efficiently execute these sub-8-bit quantized networks, as their SIMD instructions only support a granularity of 8 bits or wider.Also, there is a scarcity of literature that presents a full quantization paradigm for ViTs.In this paper, we propose an activation-aware fully sub-8-bit quantization-aware training (QAT) framework called PackQViT for efficient yet accurate ViT acceleration on mobile devices to facilitate real-time AI-powered decision-making.Specifically, in revisiting data activation within the ViT dataflow, two characteristics are relevant to quantization strategy and precision: the long-tailed distribution and systematic channel-wise outliers.In response, we employ either log2 quantization or clipping to address the long-tailed distribution and incorporate outlier-aware training for residual link quantization to regulate the various channel-wise outliers more consistently.Notably, due to the systematic fixed pattern, outlier-aware training approach can predict the channel indices and regularized scales of outliers in advance, thus avoiding the runtime data-adaptive selection during inference.Furthermore, we employ Int-$2^{n}$-Softmax, Int-LayerNorm, and Integer GELU to enable integer-only computation flow. Finally, we develop a SIMD-based 4-bit packed multiplier to achieve end-to-end ViT acceleration on mobile phones.Compared to prior studies on ViT quantization using 8-bit precision, PackQViT surpasses other works by an improved accuracy ranging from 0.4\\% to 17.9\\% for various widely used ViTs on ImageNet dataset; under 4-bit precision, PackQViT demonstrates 0.4%$\\sim$2.8% higher accuracy. Compared to the baseline multiplier, our implementations on the Realme GT Android smartphone with Snapdragon 870 SoC CPU achieve 2.6x$\\sim$3.7x speedup under 8-bit scenario and 3.8x$\\sim$5.9x speedup under 4-bit which ensures practical real-time performance",
    "keywords": [],
    "checked": true,
    "id": "5fd1b67482e4b9fe70b480c0fd0467900d6db436",
    "semantic_title": "packqvit: faster sub-8-bit vision transformers via full and packed quantization on the mobile",
    "citation_count": 1,
    "authors": [
      "Peiyan Dong",
      "LEI LU",
      "Chao Wu",
      "Cheng Lyu",
      "Geng Yuan",
      "Hao Tang",
      "Yanzhi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1cac8326ce3fbe79171db9754211530c-Abstract-Conference.html": {
    "title": "Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman",
    "volume": "main",
    "abstract": "Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k, t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^k)$ (for any $k \\geq 2$) in $(k, t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors instead of all nodes, thereby greatly expanding the design space of $k$-FWL. Combining these two modifications results in a flexible and powerful framework $(k, t)$-FWL+. We demonstrate $(k, t)$-FWL+ can implement most existing models with matching expressiveness. We then introduce an instance of $(k,t)$-FWL+ called Neighborhood$^2$-FWL (N$^2$-FWL), which is practically and theoretically sound. We prove that N$^2$-FWL is no less powerful than 3-WL, and can encode many substructures while only requiring $O(n^2)$ space. Finally, we design its neural version named **N$^2$-GNN** and evaluate its performance on various tasks. N$^2$-GNN achieves record-breaking results on ZINC-Subset (**0.059**) and ZINC-Full (**0.013**), outperforming previous SOTA results by 10.6\\% and 40.9\\%, respectively. Moreover, N$^2$-GNN achieves new SOTA results on the BREC dataset (**71.8\\%**) among all existing high-expressive GNN methods",
    "keywords": [],
    "checked": true,
    "id": "2c2a0534d7da4f7a5d6e009ae73fe352f4ebfe22",
    "semantic_title": "extending the design space of graph neural networks by rethinking folklore weisfeiler-lehman",
    "citation_count": 1,
    "authors": [
      "Jiarui Feng",
      "Lecheng Kong",
      "Hao Liu",
      "Dacheng Tao",
      "Fuhai Li",
      "Muhan Zhang",
      "Yixin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1cb57fcf7ff3f6d37eebae5becc9ea6d-Abstract-Conference.html": {
    "title": "Off-Policy Evaluation for Human Feedback",
    "volume": "main",
    "abstract": "Off-policy evaluation (OPE) is important for closing the gap between offline training and evaluation of reinforcement learning (RL), by estimating performance and/or rank of target (evaluation) policies using offline trajectories only. It can improve the safety and efficiency of data collection and policy testing procedures in situations where online deployments are expensive, such as healthcare. However, existing OPE methods fall short in estimating human feedback (HF) signals, as HF may be conditioned over multiple underlying factors and are only sparsely available; as opposed to the agent-defined environmental rewards (used in policy optimization), which are usually determined over parametric functions or distributions. Consequently, the nature of HF signals makes extrapolating accurate OPE estimations to be challenging. To resolve this, we introduce an OPE for HF (OPEHF) framework that revives existing OPE methods in order to accurately evaluate the HF signals. Specifically, we develop an immediate human reward (IHR) reconstruction approach, regularized by environmental knowledge distilled in a latent space that captures the underlying dynamics of state transitions as well as issuing HF signals. Our approach has been tested over two real-world experiments, adaptive in-vivo neurostimulation and intelligent tutoring, and a simulation environment (visual Q&A). Results show that our approach significantly improves the performance toward estimating HF signals accurately, compared to directly applying (variants of) existing OPE methods",
    "keywords": [],
    "checked": true,
    "id": "48d3014ff96babdfb7973a4537195c9e1c756240",
    "semantic_title": "off-policy evaluation for human feedback",
    "citation_count": 0,
    "authors": [
      "Qitong Gao",
      "Ge Gao",
      "Juncheng Dong",
      "Vahid Tarokh",
      "Min Chi",
      "Miroslav Pajic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1cb5b3d64bdf3c6642c8d9a8fbecd019-Abstract-Conference.html": {
    "title": "Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion",
    "volume": "main",
    "abstract": "Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as on our newly created Messy Rooms dataset, demonstrating the effectiveness and scalability of our slow-fast clustering method",
    "keywords": [],
    "checked": true,
    "id": "fc2570c8307c2ac64f1d64b993dded39320b85bd",
    "semantic_title": "contrastive lift: 3d object instance segmentation by slow-fast contrastive fusion",
    "citation_count": 8,
    "authors": [
      "Yash Bhalgat",
      "Iro Laina",
      "João F. Henriques",
      "Andrea Vedaldi",
      "Andrew Zisserman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1d35af80e775e342f4cd3792e4405837-Abstract-Conference.html": {
    "title": "GALOPA: Graph Transport Learning with Optimal Plan Alignment",
    "volume": "main",
    "abstract": "Self-supervised learning on graph aims to learn graph representations in an unsupervised manner. While graph contrastive learning (GCL - relying on graph augmentation for creating perturbation views of anchor graphs and maximizing/minimizing similarity for positive/negative pairs) is a popular self-supervised method, it faces challenges in finding label-invariant augmented graphs and determining the exact extent of similarity between sample pairs to be achieved. In this work, we propose an alternative self-supervised solution that (i) goes beyond the label invariance assumption without distinguishing between positive/negative samples, (ii) can calibrate the encoder for preserving not only the structural information inside the graph, but the matching information between different graphs, (iii) learns isometric embeddings that preserve the distance between graphs, a by-product of our objective. Motivated by optimal transport theory, this scheme relays on an observation that the optimal transport plans between node representations at the output space, which measure the matching probability between two distributions, should be consistent to the plans between the corresponding graphs at the input space. The experimental findings include: (i) The plan alignment strategy significantly outperforms the counterpart using the transport distance; (ii) The proposed model shows superior performance using only node attributes as calibration signals, without relying on edge information; (iii) Our model maintains robust results even under high perturbation rates; (iv) Extensive experiments on various benchmarks validate the effectiveness of the proposed method",
    "keywords": [],
    "checked": false,
    "id": "296721f71a3aabac90f673c9271ade89d00c092f",
    "semantic_title": "investigating graph structure information for entity alignment with dangling cases",
    "citation_count": 1,
    "authors": [
      "Yejiang Wang",
      "Yuhai Zhao",
      "Daniel Zhengkui Wang",
      "Ling Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1d49235669869ab737c1da9d64b7c769-Abstract-Conference.html": {
    "title": "Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds",
    "volume": "main",
    "abstract": "Machine learning for point clouds has been attracting much attention, with many applications in various fields, such as shape recognition and material science. For enhancing the accuracy of such machine learning methods, it is often effective to incorporate global topological features, which are typically extracted by persistent homology. In the calculation of persistent homology for a point cloud, we choose a filtration for the point cloud, an increasing sequence of spaces. Since the performance of machine learning methods combined with persistent homology is highly affected by the choice of a filtration, we need to tune it depending on data and tasks. In this paper, we propose a framework that learns a filtration adaptively with the use of neural networks. In order to make the resulting persistent homology isometry-invariant, we develop a neural network architecture with such invariance. Additionally, we show a theoretical result on a finite-dimensional approximation of filtration functions, which justifies the proposed network architecture. Experimental results demonstrated the efficacy of our framework in several classification tasks",
    "keywords": [],
    "checked": true,
    "id": "cb29ebbdb0375df4b4228453b3c7d2eee0d3829a",
    "semantic_title": "adaptive topological feature via persistent homology: filtration learning for point clouds",
    "citation_count": 0,
    "authors": [
      "Naoki Nishikawa",
      "Yuichi Ike",
      "Kenji Yamanishi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1d5a92867cf463fad136cfa23395840b-Abstract-Conference.html": {
    "title": "Accurate Interpolation for Scattered Data through Hierarchical Residual Refinement",
    "volume": "main",
    "abstract": "Accurate interpolation algorithms are highly desired in various theoretical and engineering scenarios. Unlike the traditional numerical algorithms that have exact zero-residual constraints on observed points, the neural network-based interpolation methods exhibit non-zero residuals at these points. These residuals, which provide observations of an underlying residual function, can guide predicting interpolation functions, but have not been exploited by the existing approaches. To fill this gap, we propose Hierarchical INTerpolation Network (HINT), which utilizes the residuals on observed points to guide target function estimation in a hierarchical fashion. HINT consists of several sequentially arranged lightweight interpolation blocks. The first interpolation block estimates the main component of the target function, while subsequent blocks predict the residual components using observed points residuals of the preceding blocks. The main component and residual components are accumulated to form the final interpolation results. Furthermore, under the assumption that finer residual prediction requires a more focused attention range on observed points, we utilize hierarchical local constraints in correlation modeling between observed and target points. Extensive experiments demonstrate that HINT outperforms existing interpolation algorithms significantly in terms of interpolation accuracy across a wide variety of datasets, which underscores its potential for practical scenarios",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhe Ding",
      "Boyang Xia",
      "Dongbo Bu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1d5b9233ad716a43be5c0d3023cb82d0-Abstract-Conference.html": {
    "title": "Learning Universal Policies via Text-Guided Video Generation",
    "volume": "main",
    "abstract": "A goal of artificial intelligence is to construct an agent that can solve a wide variety of tasks. Recent progress in text-guided image synthesis has yielded models with an impressive ability to generate complex novel images, exhibiting combinatorial generalization across domains. Motivated by this success, we investigate whether such tools can be used to construct more general-purpose agents. Specifically, we cast the sequential decision making problem as a text-conditioned video generation problem, where, given a text-encoded specification of a desired goal, a planner synthesizes a set of future frames depicting its planned actions in the future, after which control actions are extracted from the generated video. By leveraging text as the underlying goal specification, we are able to naturally and combinatorially generalize to novel goals. The proposed policy-as-video formulation can further represent environments with different state and action spaces in a unified space of images, which, for example, enables learning and generalization across a variety of robot manipulation tasks. Finally, by leveraging pretrained language embeddings and widely available videos from the internet, the approach enables knowledge transfer through predicting highly realistic video plans for real robots",
    "keywords": [],
    "checked": true,
    "id": "da2fe6cd385194b0274d04d04ee72e8caf3854d4",
    "semantic_title": "learning universal policies via text-guided video generation",
    "citation_count": 58,
    "authors": [
      "Yilun Du",
      "Sherry Yang",
      "Bo Dai",
      "Hanjun Dai",
      "Ofir Nachum",
      "Josh Tenenbaum",
      "Dale Schuurmans",
      "Pieter Abbeel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1d5fce9627e15c84db572a66e029b1fc-Abstract-Conference.html": {
    "title": "Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming",
    "volume": "main",
    "abstract": "Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue.Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees.We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints.Experiments on five application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin",
    "keywords": [],
    "checked": false,
    "id": "b284ee247d440b669451bf42daf16d9d47430ca7",
    "semantic_title": "optimal decision trees for separable objectives: pushing the limits of dynamic programming",
    "citation_count": 0,
    "authors": [
      "Jacobus van der Linden",
      "Mathijs de Weerdt",
      "Emir Demirović"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1d83ad88759cef8192451543e5d59bf6-Abstract-Conference.html": {
    "title": "Polyhedron Attention Module: Learning Adaptive-order Interactions",
    "volume": "main",
    "abstract": "Learning feature interactions can be the key for multivariate predictive modeling. ReLU-activated neural networks create piecewise linear prediction models, and other nonlinear activation functions lead to models with only high-order feature interactions. Recent methods incorporate candidate polynomial terms of fixed orders into deep learning, which is subject to the issue of combinatorial explosion, or learn the orders that are difficult to adapt to different regions of the feature space. We propose a Polyhedron Attention Module (PAM) to create piecewise polynomial models where the input space is split into polyhedrons which define the different pieces and on each piece the hyperplanes that define the polyhedron boundary multiply to form the interactive terms, resulting in interactions of adaptive order to each piece. PAM is interpretable to identify important interactions in predicting a target. Theoretic analysis shows that PAM has stronger expression capability than ReLU-activated networks. Extensive experimental results demonstrate the superior classification performance of PAM on massive datasets of the click-through rate prediction and PAM can learn meaningful interaction effects in a medical problem",
    "keywords": [],
    "checked": false,
    "id": "7f1571de064fea30f01c399d49fd81c6180a5b8f",
    "semantic_title": "group attention-based multi-wave graph learning for dementia diagnosis",
    "citation_count": 0,
    "authors": [
      "Tan Zhu",
      "Fei Dou",
      "Xinyu Wang",
      "Jin Lu",
      "Jinbo Bi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1d8e261c241aa72f9b4a02af7f52587e-Abstract-Conference.html": {
    "title": "Faster Query Times for Fully Dynamic $k$-Center Clustering with Outliers",
    "volume": "main",
    "abstract": "Given a point set $P\\subseteq M$ from a metric space $(M,d)$ and numbers $k, z \\in N$, the *metric $k$-center problem with $z$ outliers* is to find a set $C^\\ast\\subseteq P$ of $k$ points such that the maximum distance of all but at most $z$ outlier points of $P$ to their nearest center in ${C}^\\ast$ is minimized. We consider this problem in the fully dynamic model, i.e., under insertions and deletions of points, for the case that the metric space has a bounded doubling dimension $dim$. We utilize a hierarchical data structure to maintain the points and their neighborhoods, which enables us to efficiently find the clusters. In particular, our data structure can be queried at any time to generate a $(3+\\varepsilon)$-approximate solution for input values of $k$ and $z$ in worst-case query time $\\varepsilon^{-O(dim)}k \\log{n} \\log\\log{\\Delta}$, where $\\Delta$ is the ratio between the maximum and minimum distance between two points in $P$. Moreover, it allows insertion/deletion of a point in worst-case update time $\\varepsilon^{-O(dim)}\\log{n}\\log{\\Delta}$. Our result achieves a significantly faster query time with respect to $k$ and $z$ than the current state-of-the-art by Pellizzoni, Pietracaprina, and Pucci, which uses $\\varepsilon^{-O(dim)}(k+z)^2\\log{\\Delta}$ query time to obtain a $(3+\\varepsilon)$-approximation",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leyla Biabani",
      "Annika Hennes",
      "Morteza Monemizadeh",
      "Melanie Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1dc2fe8d9ae956616f86bab3ce5edc59-Abstract-Conference.html": {
    "title": "Natural Language Instruction-following with Task-related Language Development and Translation",
    "volume": "main",
    "abstract": "Natural language-conditioned reinforcement learning (RL) enables agents to follow human instructions. Previous approaches generally implemented language-conditioned RL by providing the policy with human instructions in natural language (NL) and training the policy to follow instructions. In this is outside-in approach, the policy must comprehend the NL and manage the task simultaneously. However, the unbounded NL examples often bring much extra complexity for solving concrete RL tasks, which can distract policy learning from completing the task. To ease the learning burden of the policy, we investigate an inside-out scheme for natural language-conditioned RL by developing a task language (TL) that is task-related and easily understood by the policy, thus reducing the policy learning burden. Besides, we employ a translator to translate natural language into the TL, which is used in RL to achieve efficient policy training. We implement this scheme as TALAR (TAsk Language with predicAte Representation) that learns multiple predicates to model object relationships as the TL. Experiments indicate that TALAR not only better comprehends NL instructions but also leads to a better instruction-following policy that significantly improves the success rate over baselines and adapts to unseen expressions of NL instruction. Besides, the TL is also an effective sub-task abstraction compatible with hierarchical RL",
    "keywords": [],
    "checked": false,
    "id": "4c3dc49abcdb1122472d15f1f8fde4bbabc3859f",
    "semantic_title": "natural language-conditioned reinforcement learning with inside-out task language development and translation",
    "citation_count": 2,
    "authors": [
      "Jing-Cheng Pang",
      "Xin-Yu Yang",
      "Si-Hang Yang",
      "Xiong-Hui Chen",
      "Yang Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1dc9fbdb6b4d9955ad377cb983232c9f-Abstract-Conference.html": {
    "title": "Convergence of Actor-Critic with Multi-Layer Neural Networks",
    "volume": "main",
    "abstract": "The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\\tilde{O} \\left( 1/\\sqrt{m} \\right) + O \\left( \\epsilon \\right)$, with $m$ being the width of the neural network and $\\epsilon$ the approximation quality of the best critic neural network over the projected set",
    "keywords": [],
    "checked": false,
    "id": "9d27941df4c6bd3d1413161827165dd72e8e6106",
    "semantic_title": "optimal tracking of nonlinear discrete-time systems using zero-sum game formulation and hybrid learning",
    "citation_count": 0,
    "authors": [
      "Haoxing Tian",
      "Alex Olshevsky",
      "Yannis Paschalidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1dec73169509c223220744b2c9b2df37-Abstract-Conference.html": {
    "title": "Percentile Criterion Optimization in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "In reinforcement learning, robust policies for high-stakes decision-making problems with limited data are usually computed by optimizing the percentile criterion. The percentile criterion is optimized by constructing an uncertainty set that contains the true model with high probability and optimizing the policy for the worst model in the set. Since the percentile criterion is non-convex, constructing these sets itself is challenging. Existing works use Bayesian credible regions as uncertainty sets, but they are often unnecessarily large and result in learning overly conservative policies. To overcome these shortcomings, we propose a novel Value-at-Risk based dynamic programming algorithm to optimize the percentile criterion without explicitly constructing any uncertainty sets. Our theoretical and empirical results show that our algorithm implicitly constructs much smaller uncertainty sets and learns less-conservative robust policies",
    "keywords": [],
    "checked": true,
    "id": "f4ed57902938c245c43789a32cbc434460803345",
    "semantic_title": "percentile criterion optimization in offline reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Cyrus Cousins",
      "Elita Lobo",
      "Marek Petrik",
      "Yair Zick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1df4afb0b4ebf492a41218ce16b6d8df-Abstract-Conference.html": {
    "title": "TextDiffuser: Diffusion Models as Text Painters",
    "volume": "main",
    "abstract": "Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds. TextDiffuser consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout. Additionally, we contribute the first large-scale text images dataset with OCR annotations, MARIO-10M, containing 10 million image-text pairs with text recognition, detection, and character-level segmentation annotations. We further collect the MARIO-Eval benchmark to serve as a comprehensive tool for evaluating text rendering quality. Through experiments and user studies, we demonstrate that TextDiffuser is flexible and controllable to create high-quality text images using text prompts alone or together with text template images, and conduct text inpainting to reconstruct incomplete images with text. We will make the code, model and dataset publicly available",
    "keywords": [],
    "checked": true,
    "id": "e779781f1bea273573fc9d3f1a5e874bcff2cd2b",
    "semantic_title": "textdiffuser: diffusion models as text painters",
    "citation_count": 8,
    "authors": [
      "Jingye Chen",
      "Yupan Huang",
      "Tengchao Lv",
      "Lei Cui",
      "Qifeng Chen",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e0d38c676d5855bcfab7f6d29d20ad9-Abstract-Conference.html": {
    "title": "Object-centric Learning with Cyclic Walks between Parts and Whole",
    "volume": "main",
    "abstract": "Learning object-centric representations from complex natural environments enables both humans and machines with reasoning abilities from low-level perceptual features. To capture compositional entities of the scene, we proposed cyclic walks between perceptual features extracted from vision transformers and object entities. First, a slot-attention module interfaces with these perceptual features and produces a finite set of slot representations. These slots can bind to any object entities in the scene via inter-slot competitions for attention. Next, we establish entity-feature correspondence with cyclic walks along high transition probability based on the pairwise similarity between perceptual features (aka \"parts\") and slot-binded object representations (aka \"whole\"). The whole is greater than its parts and the parts constitute the whole. The part-whole interactions form cycle consistencies, as supervisory signals, to train the slot-attention module. Our rigorous experiments on \\textit{seven} image datasets in \\textit{three} \\textit{unsupervised} tasks demonstrate that the networks trained with our cyclic walks can disentangle foregrounds and backgrounds, discover objects, and segment semantic objects in complex scenes. In contrast to object-centric models attached with a decoder for the pixel-level or feature-level reconstructions, our cyclic walks provide strong learning signals, avoiding computation overheads and enhancing memory efficiency. Our source code and data are available at: \\href{https://github.com/ZhangLab-DeepNeuroCogLab/Parts-Whole-Object-Centric-Learning/}{link}",
    "keywords": [],
    "checked": true,
    "id": "4c493f2bc9cfe5ef763c6187fab8134c135fe949",
    "semantic_title": "object-centric learning with cyclic walks between parts and whole",
    "citation_count": 3,
    "authors": [
      "Ziyu Wang",
      "Mike Zheng Shou",
      "Mengmi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e0d9f30c100129259f66660403fb1e2-Abstract-Conference.html": {
    "title": "Experiment Planning with Function Approximation",
    "volume": "main",
    "abstract": "We study the problem of experiment planning with function approximation in contextual bandit problems. In settings where there is a significant overhead to deploying adaptive algorithms; for example, when the execution of the data collection policies is required to be distributed, or a human in the loop is needed to implement these policies, producing in advance a set of policies for data collection is paramount. We study the setting where a large dataset of contexts -but not rewards- is available and may be used by the learner to design an effective data collection strategy. Although when rewards are linear this problem has been well studied, results are still missing for more complex reward models. In this work we propose two experiment planning strategies compatible with function approximation, first an eluder planning and sampling procedure that can recover optimality guarantees depending on the eluder dimension of the reward function class, and second we show the uniform sampler achieves competitive optimality rates in the setting where the number of actions is small. We finalize our results introducing a statistical gap fleshing out the fundamental differences between planning and adaptive learning and provide results for planning with model selection",
    "keywords": [],
    "checked": false,
    "id": "88298748d970690e09fe6043d68125b93ccce89d",
    "semantic_title": "near-optimal deployment efficiency in reward-free reinforcement learning with linear function approximation",
    "citation_count": 8,
    "authors": [
      "Aldo Pacchiano",
      "Jonathan Lee",
      "Emma Brunskill"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e118ba9ee76c20df728b42a35fb4704-Abstract-Conference.html": {
    "title": "White-Box Transformers via Sparse Rate Reduction",
    "volume": "main",
    "abstract": "In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. Code is at https://github.com/Ma-Lab-Berkeley/CRATE",
    "keywords": [],
    "checked": true,
    "id": "e94804f8df0e5a3eff6f0a278606d60dcb767d56",
    "semantic_title": "white-box transformers via sparse rate reduction",
    "citation_count": 12,
    "authors": [
      "Yaodong Yu",
      "Sam Buchanan",
      "Druv Pai",
      "Tianzhe Chu",
      "Ziyang Wu",
      "Shengbang Tong",
      "Benjamin Haeffele",
      "Yi Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e4322fddd833f83c855660ac65e428d-Abstract-Conference.html": {
    "title": "Task-Robust Pre-Training for Worst-Case Downstream Adaptation",
    "volume": "main",
    "abstract": "Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as downstream-task robustness.Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minimax lossand prove its convergence in the convex setting. In the experiments, we show both on large-scale natural language processing and computer vision datasets our method increases the metrics on worse-case downstream tasks. Additionally, some theoretical explanations for why our loss is beneficial are provided. Specifically, we show fewer samples are inherently required for the most challenging downstream task in some cases",
    "keywords": [],
    "checked": true,
    "id": "6ce1d63d8d56d1f45c09316b0c0314b405f86a66",
    "semantic_title": "task-robust pre-training for worst-case downstream adaptation",
    "citation_count": 0,
    "authors": [
      "Jianghui Wang",
      "Yang Chen",
      "Xingyu Xie",
      "Cong Fang",
      "Zhouchen Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e58b1bf9f218fcd19e4539e982752a5-Abstract-Conference.html": {
    "title": "Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training",
    "volume": "main",
    "abstract": "As deep neural networks are highly expressive, it is important to find solutions with small generalization gap (the difference between the performance on the training data and unseen data). Focusing on the stochastic nature of training, we first present a theoretical analysis in which the bound of generalization gap depends on what we call inconsistency and instability of model outputs, which can be estimated on unlabeled data. Our empirical study based on this analysis shows that instability and inconsistency are strongly predictive of generalization gap in various settings. In particular, our finding indicates that inconsistency is a more reliable indicator of generalization gap than the sharpness of the loss landscape. Furthermore, we show that algorithmic reduction of inconsistency leads to superior performance. The results also provide a theoretical basis for existing methods such as co-distillation and ensemble",
    "keywords": [],
    "checked": true,
    "id": "bb2b427da46cede418fffe340a6d5f1d31b214f8",
    "semantic_title": "inconsistency, instability, and generalization gap of deep neural network training",
    "citation_count": 0,
    "authors": [
      "Rie Johnson",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e5f58d98523298cba093f658cfdf2d6-Abstract-Conference.html": {
    "title": "Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions",
    "volume": "main",
    "abstract": "Learning distance functions between complex objects, such as the Wasserstein distance to compare point sets, is a common goal in machine learning applications. However, functions on such complex objects (e.g., point sets and graphs) are often required to be invariant to a wide variety of group actions e.g. permutation or rigid transformation. Therefore, continuous and symmetric *product* functions (such as distance functions) on such complex objects must also be invariant to the *product* of such group actions. We call these functions symmetric and factor-wise group invariant functions (or SGFI functions} in short).In this paper, we first present a general neural network architecture for approximating SFGI functions. The main contribution of this paper combines this general NN with a sketching idea in order to develop a specific and efficient neural network which can approximate the $p$-th Wasserstein distance between point sets.Very importantly, the required model complexity is *independent* of the sizes of input point sets. On the theoretical front, to the best of our knowledge, this is the first result showing that there exists a neural network with the capacity to approximate Wasserstein distance with bounded model complexity. Our work provides an interesting integration of sketching ideas for geometric problems with universal approximation of symmetric functions. On the empirical front, we present a range of results showing that our newly proposed neural network architecture performs comparatively or better than other models (including a SOTA Siamese Autoencoder based approach). In particular, our NN generalizes significantly better and trains much faster than the SOTA Siamese AE.Finally, this line of investigation could be useful in exploring effective neural network design for solving a broad range of geometric optimization problems (e.g., $k$-means in a metric space)",
    "keywords": [],
    "checked": true,
    "id": "b7e6ac97498338e1917aea70e2fafdc94a007e38",
    "semantic_title": "neural approximation of wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions",
    "citation_count": 0,
    "authors": [
      "Samantha Chen",
      "Yusu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e680f115a22d60cbc228a0c6dae5936-Abstract-Conference.html": {
    "title": "What Do Deep Saliency Models Learn about Visual Attention?",
    "volume": "main",
    "abstract": "In recent years, deep saliency models have made significant progress in predicting human visual attention. However, the mechanisms behind their success remain largely unexplained due to the opaque nature of deep neural networks. In this paper, we present a novel analytic framework that sheds light on the implicit features learned by saliency models and provides principled interpretation and quantification of their contributions to saliency prediction. Our approach decomposes these implicit features into interpretable bases that are explicitly aligned with semantic attributes and reformulates saliency prediction as a weighted combination of probability maps connecting the bases and saliency. By applying our framework, we conduct extensive analyses from various perspectives, including the positive and negative weights of semantics, the impact of training data and architectural designs, the progressive influences of fine-tuning, and common error patterns of state-of-the-art deep saliency models. Additionally, we demonstrate the effectiveness of our framework by exploring visual attention characteristics in various application scenarios, such as the atypical attention of people with autism spectrum disorder, attention to emotion-eliciting stimuli, and attention evolution over time. Our code is publicly available at \\url{https://github.com/szzexpoi/saliency_analysis}",
    "keywords": [],
    "checked": true,
    "id": "4b822d1fac2a712eceee8592a5d488fa3c68ffec",
    "semantic_title": "what do deep saliency models learn about visual attention?",
    "citation_count": 0,
    "authors": [
      "Shi Chen",
      "Ming Jiang",
      "Qi Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e6cf8f77bd8e907f53babcd7664c710-Abstract-Conference.html": {
    "title": "Three Iterations of (d − 1)-WL Test Distinguish Non Isometric Clouds of d-dimensional Points",
    "volume": "main",
    "abstract": "The Weisfeiler-Lehman (WL) test is a fundamental iterative algorithm for checking the isomorphism of graphs. It has also been observed that it underlies the design of several graph neural network architectures, whose capabilities and performance can be understood in terms of the expressive power of this test. Motivated by recent developments in machine learning applications to datasets involving three-dimensional objects, we study when the WL test is {\\em complete} for clouds of Euclidean points represented by complete distance graphs, i.e., when it can distinguish, up to isometry, any arbitrary such cloud. Our main result states that the $(d-1)$-dimensional WL test is complete for point clouds in $d$-dimensional Euclidean space, for any $d\\ge 2$, and only three iterations of the test suffice. Our result is tight for $d = 2, 3$. We also observe that the $d$-dimensional WL test only requires one iteration to achieve completeness",
    "keywords": [],
    "checked": false,
    "id": "7de523207140a88ab7e5f0d2960f55d1b40ddb86",
    "semantic_title": "three iterations of $(d-1)$-wl test distinguish non isometric clouds of $d$-dimensional points",
    "citation_count": 4,
    "authors": [
      "Valentino Delle Rose",
      "Alexander Kozachinskiy",
      "Cristobal Rojas",
      "Mircea Petrache",
      "Pablo Barceló"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e70ac91ad26ba5b24cf11b12a1f90fe-Abstract-Conference.html": {
    "title": "Puzzlefusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving",
    "volume": "main",
    "abstract": "This paper presents an end-to-end neural architecture based on Diffusion Models for spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks.In the latter task, for instance, the proposed system ``PuzzleFusion'' takes a set of room layouts as polygonal curves in the top-down view and aligns the room layout pieces by estimating their 2D translations and rotations, akin to solving the jigsaw puzzle of room layouts. A surprising discovery of the paper is that the simple use of a Diffusion Model effectively solves these challenging spatial puzzle tasks as a conditional generation process. To enable learning of an end-to-end neural system, the paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi Jigsaw Dataset, a synthetic one where pieces are generated by voronoi diagram of 2D pointset; and 2) MagicPlan Dataset, a real one from a production pipeline by MagicPlan, where pieces are room layouts constructed by augmented reality App by real-estate consumers.The qualitative and quantitative evaluations demonstrate that the proposed approach outperforms the competing methods by significant margins in all three spatial puzzle tasks. We have provided code and data in https://sepidsh.github.io/puzzlefusion",
    "keywords": [],
    "checked": true,
    "id": "048da9875ee5b64409b5e57a129585b36a4da5a0",
    "semantic_title": "puzzlefusion: unleashing the power of diffusion models for spatial puzzle solving",
    "citation_count": 2,
    "authors": [
      "Sepidehsadat (Sepid) Hossieni",
      "Mohammad Amin Shabani",
      "Saghar Irandoust",
      "Yasutaka Furukawa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e75f7539cbde5de895fab238ff42519-Abstract-Conference.html": {
    "title": "Visual Instruction Inversion: Image Editing via Image Prompting",
    "volume": "main",
    "abstract": "Text-conditioned image editing has emerged as a powerful tool for editing images.However, in many situations, language can be ambiguous and ineffective in describing specific image edits.When faced with such challenges, visual prompts can be a more informative and intuitive way to convey ideas.We present a method for image editing via visual prompting.Given pairs of example that represent the \"before\" and \"after\" images of an edit, our goal is to learn a text-based editing direction that can be used to perform the same edit on new images.We leverage the rich, pretrained editing capabilities of text-to-image diffusion models by inverting visual prompts into editing instructions.Our results show that with just one example pair, we can achieve competitive results compared to state-of-the-art text-conditioned image editing frameworks",
    "keywords": [],
    "checked": false,
    "id": "f4c62aa336de45273e0fdfcfbd65b3c2e552ad56",
    "semantic_title": "visual instruction inversion: image editing via visual prompting",
    "citation_count": 10,
    "authors": [
      "Thao Nguyen",
      "Yuheng Li",
      "Utkarsh Ojha",
      "Yong Jae Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e77af93008ee6cd248a31723ce357d8-Abstract-Conference.html": {
    "title": "Algorithm Selection for Deep Active Learning with Imbalanced Datasets",
    "volume": "main",
    "abstract": "Label efficiency has become an increasingly important objective in deep learning applications. Active learning aims to reduce the number of labeled examples needed to train deep networks, but the empirical performance of active learning algorithms can vary dramatically across datasets and applications. It is difficult to know in advance which active learning strategy will perform well or best in a given application. To address this, we propose the first adaptive algorithm selection strategy for deep active learning. For any unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning algORithm selection) iteratively and adaptively chooses among a set of candidate active learning algorithms. TAILOR uses novel reward functions aimed at gathering class-balanced examples. Extensive experiments in multi-class and multi-label applications demonstrate TAILOR's effectiveness in achieving accuracy comparable or better than that of the best of the candidate algorithms. Our implementation of TAILOR is open-sourced at https://github.com/jifanz/TAILOR",
    "keywords": [],
    "checked": true,
    "id": "b732e65c2d5c0bbe868430873ad7f1a682cf2232",
    "semantic_title": "algorithm selection for deep active learning with imbalanced datasets",
    "citation_count": 4,
    "authors": [
      "Jifan Zhang",
      "Shuai Shao",
      "Saurabh Verma",
      "Robert Nowak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e7b192fc8b3acb93749c5accfa60e0c-Abstract-Conference.html": {
    "title": "Federated Compositional Deep AUC Maximization",
    "volume": "main",
    "abstract": "Federated learning has attracted increasing attention due to the promise of balancing privacy and large-scale learning; numerous approaches have been proposed. However, most existing approaches focus on problems with balanced data, and prediction performance is far from satisfactory for many real-world applications where the number of samples in different classes is highly imbalanced. To address this challenging problem, we developed a novel federated learning method for imbalanced data by directly optimizing the area under curve (AUC) score. In particular, we formulate the AUC maximization problem as a federated compositional minimax optimization problem, develop a local stochastic compositional gradient descent ascent with momentum algorithm, and provide bounds on the computational and communication complexities of our algorithm. To the best of our knowledge, this is the first work to achieve such favorable theoretical results. Finally, extensive experimental results confirm the efficacy of our method",
    "keywords": [],
    "checked": true,
    "id": "a6b3d3c938e51e15a2e22b46c0d77be5bd30b08d",
    "semantic_title": "federated compositional deep auc maximization",
    "citation_count": 6,
    "authors": [
      "Xinwen Zhang",
      "Yihan Zhang",
      "Tianbao Yang",
      "Richard Souvenir",
      "Hongchang Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e83498c3eafe109a44b12979c2c73db-Abstract-Conference.html": {
    "title": "On Learning Latent Models with Multi-Instance Weak Supervision",
    "volume": "main",
    "abstract": "We consider a weakly supervised learning scenario where the supervision signal is generated by a transition function $\\sigma$ of labels associated with multiple input instances. We formulate this problem as *multi-instance Partial Label Learning (multi-instance PLL)*, which is an extension to the standard PLL problem. Our problem is met in different fields, including latent structural learning and neuro-symbolic integration. Despite the existence of many learning techniques, limited theoretical analysis has been dedicated to this problem. In this paper, we provide the first theoretical study of multi-instance PLL with possibly an unknown transition $\\sigma$. Our main contributions are as follows: First, we proposed a necessary and sufficient condition for the learnability of the problem. This condition nontrivially generalizes and relaxes the existing *small ambiguity degree* in PLL literature since we allow the transition to be deterministic. Second, we derived Rademacher-style error bounds based on the top-$k$ surrogate loss that is widely used in the neuro-symbolic literature. Furthermore, we conclude with empirical experiments for learning with an unknown transition. The empirical results align with our theoretical findings; however, they also expose the issue of scalability in the weak supervision literature",
    "keywords": [],
    "checked": true,
    "id": "1fcaad4365e0aea012bcc3315fb9c594e5378283",
    "semantic_title": "on learning latent models with multi-instance weak supervision",
    "citation_count": 2,
    "authors": [
      "Kaifu Wang",
      "Efthymia Tsamoura",
      "Dan Roth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1ec69275e9f002ee068f5d68380f3290-Abstract-Conference.html": {
    "title": "Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks",
    "volume": "main",
    "abstract": "We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Starting from a dynamical mean field theory description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the $\\mathcal{O}(1/\\sqrt{\\text{width}})$ fluctuations of the DMFT order parameters over random initializations of the network weights. Our results, while perturbative in width, unlike prior analyses, are non-perturbative in the strength of feature learning. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with a variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final tangent kernel and final network predictions. We also show how initialization variance can slow down online learning in wide but finite networks. In deeper networks, kernel variance can dramatically accumulate through subsequent layers at large feature learning strengths, but feature learning continues to improve the signal-to-noise ratio of the feature kernels. In discrete time, we demonstrate that large learning rate phenomena such as edge of stability effects can be well captured by infinite width dynamics and that initialization variance can decrease dynamically. For CNNs trained on CIFAR-10, we empirically find significant corrections to both the bias and variance of network dynamics due to finite width",
    "keywords": [],
    "checked": true,
    "id": "fe1be27f0f3ad3399ae5aea1e5d3eb06251a64af",
    "semantic_title": "dynamics of finite width kernel and prediction fluctuations in mean field neural networks",
    "citation_count": 13,
    "authors": [
      "Blake Bordelon",
      "Cengiz Pehlevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1ece70d2259b8e9510e2d4ca8754cecf-Abstract-Conference.html": {
    "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
    "volume": "main",
    "abstract": "Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our experiments actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and, as a proof of concept, propose TART which generically improves an LLM's reasoning abilities using a synthetically trained reasoning module. TART trains this Transformer-based reasoning module in a task-agnostic manner using only synthetic logistic regression tasks and composes it with an arbitrary real-world pre-trained model without any additional training. With a single inference module, TART improves performance across different model families (GPT-Neo, Pythia, Bloom), model sizes (100M - 6B), tasks (14 NLP classification tasks), and even across different modalities (audio and vision). On the RAFT Benchmark, TART improves GPT-Neo (125M)'s performance such that it outperforms Bloom (176B), and is within $4$% of GPT-3",
    "keywords": [],
    "checked": true,
    "id": "014c00319cb23c6322ea5218049661a4ce222946",
    "semantic_title": "tart: a plug-and-play transformer module for task-agnostic reasoning",
    "citation_count": 2,
    "authors": [
      "Kush Bhatia",
      "Avanika Narayan",
      "Christopher M. De Sa",
      "Christopher Ré"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1ed4723f12853cbd02aecb8160f5e0c9-Abstract-Conference.html": {
    "title": "Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment",
    "volume": "main",
    "abstract": "Active Learning (AL) aims to reduce the labeling burden by interactively selecting the most informative samples from a pool of unlabeled data. While there has been extensive research on improving AL query methods in recent years, some studies have questioned the effectiveness of AL compared to emerging paradigms such as semi-supervised (Semi-SL) and self-supervised learning (Self-SL), or a simple optimization of classifier configurations. Thus, today's AL literature presents an inconsistent and contradictory landscape, leaving practitioners uncertain about whether and how to use AL in their tasks. In this work, we make the case that this inconsistency arises from a lack of systematic and realistic evaluation of AL methods. Specifically, we identify five key pitfalls in the current literature that reflect the delicate considerations required for AL evaluation. Further, we present an evaluation framework that overcomes these pitfalls and thus enables meaningful statements about the performance of AL methods. To demonstrate the relevance of our protocol, we present a large-scale empirical study and benchmark for image classification spanning various data sets, query methods, AL settings, and training paradigms. Our findings clarify the inconsistent picture in the literature and enable us to give hands-on recommendations for practitioners. The benchmark is hosted at https://github.com/IML-DKFZ/realistic-al",
    "keywords": [],
    "checked": true,
    "id": "bf88d88e712fc626cdc8e010b5d0db7e9c471645",
    "semantic_title": "navigating the pitfalls of active learning evaluation: a systematic framework for meaningful performance assessment",
    "citation_count": 4,
    "authors": [
      "Carsten Lüth",
      "Till Bungert",
      "Lukas Klein",
      "Paul Jaeger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1eeacdf8770e6dd5164cdeec8bcfa8cc-Abstract-Conference.html": {
    "title": "Semantic HELM: A Human-Readable Memory for Reinforcement Learning",
    "volume": "main",
    "abstract": "Reinforcement learning agents deployed in the real world often have to cope with partially observable environments. Therefore, most agents employ memory mechanisms to approximate the state of the environment. Recently, there have been impressive success stories in mastering partially observable environments, mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft. However, existing methods lack interpretability in the sense that it is not comprehensible for humans what the agent stores in its memory.In this regard, we propose a novel memory mechanism that represents past events in human language.Our method uses CLIP to associate visual inputs with language tokens. Then we feed these tokens to a pretrained language model that serves the agent as memory and provides it with a coherent and human-readable representation of the past.We train our memory mechanism on a set of partially observable environments and find that it excels on tasks that require a memory component, while mostly attaining performance on-par with strong baselines on tasks that do not. On a challenging continuous recognition task, where memorizing the past is crucial, our memory mechanism converges two orders of magnitude faster than prior methods.Since our memory mechanism is human-readable, we can peek at an agent's memory and check whether crucial pieces of information have been stored.This significantly enhances troubleshooting and paves the way toward more interpretable agents",
    "keywords": [],
    "checked": true,
    "id": "c50348d3491567b2cdad5ea981620c31f876dad9",
    "semantic_title": "semantic helm: a human-readable memory for reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Fabian Paischer",
      "Thomas Adler",
      "Markus Hofmarcher",
      "Sepp Hochreiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1f05584d537c92c8271699f207677475-Abstract-Conference.html": {
    "title": "Empowering Convolutional Neural Nets with MetaSin Activation",
    "volume": "main",
    "abstract": "ReLU networks have remained the default choice for models in the area of image prediction despite their well-established spectral bias towards learning low frequencies faster, and consequently their difficulty of reproducing high frequency visual details. As an alternative, sin networks showed promising results in learning implicit representations of visual data. However training these networks in practically relevant settings proved to be difficult, requiring careful initialization, dealing with issues due to inconsistent gradients, and a degeneracy in local minima. In this work, we instead propose replacing a baseline network's existing activations with a novel ensemble function with trainable parameters. The proposed MetaSin activation can be trained reliably without requiring intricate initialization schemes, and results in consistently lower test loss compared to alternatives. We demonstrate our method in the areas of Monte-Carlo denoising and image resampling where we set new state-of-the-art through a knowledge distillation based training procedure. We present ablations on hyper-parameter settings, comparisons with alternative activation function formulations, and discuss the use of our method in other domains, such as image classification",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farnood Salehi",
      "Tunç Aydin",
      "André Gaillard",
      "Guglielmo Camporese",
      "Yuxuan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1f09e1ee5035a4c3fe38a5681cae5815-Abstract-Conference.html": {
    "title": "When Does Confidence-Based Cascade Deferral Suffice?",
    "volume": "main",
    "abstract": "Cascades are a classical strategy to enable inference cost to vary adaptively across samples, wherein a sequence of classifiers are invoked in turn. A deferral rule determines whether to invoke the next classifier in the sequence, or to terminate prediction. One simple deferral rule employs the confidence of the current classifier, e.g., based on the maximum predicted softmax probability. Despite being oblivious to the structure of the cascade --- e.g., not modelling the errors of downstream models --- such confidence-based deferral often works remarkably well in practice. In this paper, we seek to better understand the conditions under which confidence-based deferral may fail, and when alternate deferral strategies can perform better. We first present a theoretical characterisation of the optimal deferral rule, which precisely characterises settings under which confidence-based deferral may suffer. We then study post-hoc deferral mechanisms, and demonstrate they can significantly improve upon confidence-based deferral in settings where (i) downstream models are specialists that only work well on a subset of inputs, (ii) samples are subject to label noise, and (iii) there is distribution shift between the train and test set",
    "keywords": [],
    "checked": true,
    "id": "f4463ab8ecbc65b4fdab28aa02a019b29966c74f",
    "semantic_title": "when does confidence-based cascade deferral suffice?",
    "citation_count": 1,
    "authors": [
      "Wittawat Jitkrittum",
      "Neha Gupta",
      "Aditya K. Menon",
      "Harikrishna Narasimhan",
      "Ankit Rawat",
      "Sanjiv Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1f2fd23309a5b2d2537d063b29ec1b52-Abstract-Conference.html": {
    "title": "DeWave: Discrete Encoding of EEG Waves for EEG to Text Translation",
    "volume": "main",
    "abstract": "The translation of brain dynamics into natural language is pivotal for brain-computer interfaces (BCIs), a field that has seen substantial growth in recent years. With the swift advancement of large language models, such as ChatGPT, the need to bridge the gap between the brain and languages becomes increasingly pressing. Current methods, however, require eye-tracking fixations or event markers to segment brain dynamics into word-level features, which can restrict the practical application of these systems. These event markers may not be readily available or could be challenging to acquire during real-time inference, and the sequence of eye fixations may not align with the order of spoken words. To tackle these issues, we introduce a novel framework, DeWave, that integrates discrete encoding sequences into open-vocabulary EEG-to-text translation tasks. DeWave uses a quantized variational encoder to derive discrete codex encoding and align it with pre-trained language models. This discrete codex representation brings forth two advantages: 1) it alleviates the order mismatch between eye fixations and spoken words by introducing text-EEG contrastive alignment training, and 2) it minimizes the interference caused by individual differences in EEG waves through an invariant discrete codex. Our model surpasses the previous baseline (40.1 and 31.7) by 3.06% and 6.34\\%, respectively, achieving 41.35 BLEU-1 and 33.71 Rouge-F on the ZuCo Dataset. Furthermore, this work is the first to facilitate the translation of entire EEG signal periods without the need for word-level order markers (e.g., eye fixations), scoring 20.5 BLEU-1 and 29.5 Rouge-1 on the ZuCo Dataset, respectively",
    "keywords": [],
    "checked": false,
    "id": "053f10cd2ba7fe9c508a9e63476a4f68ab1deb57",
    "semantic_title": "dewave: discrete eeg waves encoding for brain dynamics to text translation",
    "citation_count": 4,
    "authors": [
      "Yiqun Duan",
      "Charles Chau",
      "Zhen Wang",
      "Yu-Kai Wang",
      "Chin-teng Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1f3cbee17170c3ffff3e413d2df54f6b-Abstract-Conference.html": {
    "title": "SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data",
    "volume": "main",
    "abstract": "The problem of urban event ranking aims at predicting the top-$k$ most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. Due to the common assumption that items are independent. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named SpatialRank. SpatialRank features adaptive graph convolution layers that dynamically learn the spatiotemporal dependencies across locations from data. In addition, the model optimizes through surrogates a hybrid NDCG loss with a spatial component to better rank neighboring spatial locations. We design an importance-sampling with a spatial filtering algorithm to effectively evaluate the loss during training. Comprehensive experiments on three real-world datasets demonstrate that SpatialRank can effectively identify the top riskiest locations of crimes and traffic accidents and outperform state-of-art methods in terms of NDCG by up to 12.7%",
    "keywords": [],
    "checked": true,
    "id": "28dce7a0c37fb335c71c5d43d6746157afb9566a",
    "semantic_title": "spatialrank: urban event ranking with ndcg optimization on spatiotemporal data",
    "citation_count": 0,
    "authors": [
      "BANG AN",
      "Xun Zhou",
      "YONGJIAN ZHONG",
      "Tianbao Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1f5c5cd01b864d53cc5fa0a3472e152e-Abstract-Conference.html": {
    "title": "An Information-Theoretic Evaluation of Generative Models in Learning Multi-modal Distributions",
    "volume": "main",
    "abstract": "The evaluation of generative models has received significant attention in the machine learning community. When applied to a multi-modal distribution which is common among image datasets, an intuitive evaluation criterion is the number of modes captured by the generative model. While several scores have been proposed to evaluate the quality and diversity of a model's generated data, the correspondence between existing scores and the number of modes in the distribution is unclear. In this work, we propose an information-theoretic diversity evaluation method for multi-modal underlying distributions. We utilize the R\\'enyi Kernel Entropy (RKE) as an evaluation score based on quantum information theory to measure the number of modes in generated samples. To interpret the proposed evaluation method, we show that the RKE score can output the number of modes of a mixture of sub-Gaussian components. We also prove estimation error bounds for estimating the RKE score from limited data, suggesting a fast convergence of the empirical RKE score to the score for the underlying data distribution. Utilizing the RKE score, we conduct an extensive evaluation of state-of-the-art generative models over standard image datasets. The numerical results indicate that while the recent algorithms for training generative models manage to improve the mode-based diversity over the earlier architectures, they remain incapable of capturing the full diversity of real data. Our empirical results provide a ranking of widely-used generative models based on the RKE score of their generated samples",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Jalali",
      "Cheuk Ting Li",
      "Farzan Farnia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1f6100363156cced8633f4e89dd8ceb1-Abstract-Conference.html": {
    "title": "A Cross-Moment Approach for Causal Effect Estimation",
    "volume": "main",
    "abstract": "We consider the problem of estimating the causal effect of a treatment on an outcome in linear structural causal models (SCM) with latent confounders when we have access to a single proxy variable.Several methods (such as difference-in-difference (DiD) estimator or negative outcome control) have been proposed in this setting in the literature. However, these approaches require either restrictive assumptions on the data generating model or having access to at least two proxy variables.We propose a method to estimate the causal effect using cross moments between the treatment, the outcome, and the proxy variable. In particular, we show that the causal effect can be identified with simple arithmetic operations on the cross moments if the latent confounder in linear SCM is non-Gaussian.In this setting, DiD estimator provides an unbiased estimate only in the special case where the latent confounder has exactly the same direct causal effects on the outcomes in the pre-treatment and post-treatment phases. This translates to the common trend assumption in DiD, which we effectively relax.Additionally, we provide an impossibility result that shows the causal effect cannot be identified if the observational distribution over the treatment, the outcome, and the proxy is jointly Gaussian. Our experiments on both synthetic and real-world datasets showcase the effectivenessof the proposed approach in estimating the causal effect",
    "keywords": [],
    "checked": true,
    "id": "ac9727d5fe06472278404f9465c137ea63786eec",
    "semantic_title": "a cross-moment approach for causal effect estimation",
    "citation_count": 0,
    "authors": [
      "Yaroslav Kivva",
      "Saber Salehkaleybar",
      "Negar Kiyavash"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1f69928210578f4cf5b538a8c8806798-Abstract-Conference.html": {
    "title": "Combining Behaviors with the Successor Features Keyboard",
    "volume": "main",
    "abstract": "The Option Keyboard (OK) was recently proposed as a method for transferring behavioral knowledge across tasks. OK transfers knowledge by adaptively combining subsets of known behaviors using Successor Features (SFs) and Generalized Policy Improvement (GPI).However, it relies on hand-designed state-features and task encodings which are cumbersome to design for every new environment.In this work, we propose the \"Successor Features Keyboard\" (SFK), which enables transfer with discovered state-features and task encodings.To enable discovery, we propose the \"Categorical Successor Feature Approximator\" (CSFA), a novel learning algorithm for estimating SFs while jointly discovering state-features and task encodings.With SFK and CSFA, we achieve the first demonstration of transfer with SFs in a challenging 3D environment where all the necessary representations are discovered.We first compare CSFA against other methods for approximating SFs and show that only CSFA discovers representations compatible with SF&GPI at this scale.We then compare SFK against transfer learning baselines and show that it transfers most quickly to long-horizon tasks",
    "keywords": [],
    "checked": true,
    "id": "5cc1e7bcabe56578efa2527ba1d86a2f527e7f70",
    "semantic_title": "combining behaviors with the successor features keyboard",
    "citation_count": 0,
    "authors": [
      "Wilka Carvalho Carvalho",
      "Andre Saraiva",
      "Angelos Filos",
      "Andrew Lampinen",
      "Loic Matthey",
      "Richard L Lewis",
      "Honglak Lee",
      "Satinder Singh",
      "Danilo Jimenez Rezende",
      "Daniel Zoran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1f7e6d5c84b0ed286d0e69b7d2c79b47-Abstract-Conference.html": {
    "title": "Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective",
    "volume": "main",
    "abstract": "For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth labels, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose $\\texttt{ARCO}$, a semi-supervised contrastive learning (CL) framework with stratified group theory for medical image segmentation. In particular, we first propose building $\\texttt{ARCO}$ through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in pixel/voxel-level segmentation tasks with extremely limited labels. Furthermore, we theoretically prove these sampling techniques are universal in variance reduction. Finally, we experimentally validate our approaches on eight benchmarks, i.e., five 2D/3D medical and three semantic segmentation datasets, with different label settings, and our methods consistently outperform state-of-the-art semi-supervised methods. Additionally, we augment the CL frameworks with these sampling techniques and demonstrate significant gains over previous methods. We believe our work is an important step towards semi-supervised medical image segmentation by quantifying the limitation of current self-supervision objectives for accomplishing such challenging safety-critical tasks",
    "keywords": [],
    "checked": true,
    "id": "032b85ca732add6f59ca801dafd0c690a8424ee2",
    "semantic_title": "rethinking semi-supervised medical image segmentation: a variance-reduction perspective",
    "citation_count": 19,
    "authors": [
      "Chenyu You",
      "Weicheng Dai",
      "Yifei Min",
      "Fenglin Liu",
      "David Clifton",
      "S. Kevin Zhou",
      "Lawrence Staib",
      "James Duncan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1f96b24df4b06f5d68389845a9a13ed9-Abstract-Conference.html": {
    "title": "Boosting Verification of Deep Reinforcement Learning via Piece-Wise Linear Decision Neural Networks",
    "volume": "main",
    "abstract": "Formally verifying deep reinforcement learning (DRL) systems suffers from both inaccurate verification results and limited scalability. The major obstacle lies in the large overestimation introduced inherently during training and then transforming the inexplicable decision-making models, i.e., deep neural networks (DNNs), into easy-to-verify models. In this paper, we propose an inverse transform-then-train approach, which first encodes a DNN into an equivalent set of efficiently and tightly verifiable linear control policies and then optimizes them via reinforcement learning. We accompany our inverse approach with a novel neural network model called piece-wise linear decision neural networks (PLDNNs), which are compatible with most existing DRL training algorithms with comparable performance against conventional DNNs. Our extensive experiments show that, compared to DNN-based DRL systems, PLDNN-based systems can be more efficiently and tightly verified with up to $438$ times speedup and a significant reduction in overestimation. In particular, even a complex $12$-dimensional DRL system is efficiently verified with up to 7 times deeper computation steps",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxu Tian",
      "Dapeng Zhi",
      "Si Liu",
      "Peixin Wang",
      "Cheng Chen",
      "Min Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1fcefa894924bb1688041b7a26fb8aea-Abstract-Conference.html": {
    "title": "Star-Shaped Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) provide the foundation for the recent breakthroughs in generative modeling.Their Markovian structure makes it difficult to define DDPMs with distributions other than Gaussian or discrete.In this paper, we introduce Star-Shaped DDPM (SS-DDPM).Its star-shaped diffusion process allows us to bypass the need to define the transition probabilities or compute posteriors.We establish duality between star-shaped and specific Markovian diffusions for the exponential family of distributions and derive efficient algorithms for training and sampling from SS-DDPMs.In the case of Gaussian distributions, SS-DDPM is equivalent to DDPM.However, SS-DDPMs provide a simple recipe for designing diffusion models with distributions such as Beta, von Mises–Fisher, Dirichlet, Wishart and others, which can be especially useful when data lies on a constrained manifold.We evaluate the model in different settings and find it competitive even on image data, where Beta SS-DDPM achieves results comparable to a Gaussian DDPM.Our implementation is available at https://github.com/andrey-okhotin/star-shaped",
    "keywords": [],
    "checked": true,
    "id": "6bdc3a4a2c462acecc33722453831a79ac52ebf8",
    "semantic_title": "star-shaped denoising diffusion probabilistic models",
    "citation_count": 5,
    "authors": [
      "Andrey Okhotin",
      "Dmitry Molchanov",
      "Arkhipkin Vladimir",
      "Grigory Bartosh",
      "Viktor Ohanesian",
      "Aibek Alanov",
      "Dmitry P. Vetrov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1fe6f635fe265292aba3987b5123ae3d-Abstract-Conference.html": {
    "title": "An Adaptive Algorithm for Learning with Unknown Distribution Drift",
    "volume": "main",
    "abstract": "We develop and analyze a general technique for learning with an unknown distribution drift. Given a sequence of independent observations from the last $T$ steps of a drifting distribution, our algorithm agnostically learns a family of functions with respect to the current distribution at time $T$. Unlike previous work, our technique does not require prior knowledge about the magnitude of the drift. Instead, the algorithm adapts to the sample data. Without explicitly estimating the drift, the algorithm learns a family of functions with almost the same error as a learning algorithm that knows the magnitude of the drift in advance. Furthermore, since our algorithm adapts to the data, it can guarantee a better learning error than an algorithm that relies on loose bounds on the drift. We demonstrate the application of our technique in two fundamental learning scenarios: binary classification and linear regression",
    "keywords": [],
    "checked": true,
    "id": "2c7f80548ef03ac6b6e1e949acb02c882f8eec98",
    "semantic_title": "an adaptive algorithm for learning with unknown distribution drift",
    "citation_count": 2,
    "authors": [
      "Alessio Mazzetto",
      "Eli Upfal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html": {
    "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "volume": "main",
    "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information-theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small, high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations, showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training",
    "keywords": [],
    "checked": true,
    "id": "32ac52069e562d4f900afee70bdca63f53461481",
    "semantic_title": "qlora: efficient finetuning of quantized llms",
    "citation_count": 473,
    "authors": [
      "Tim Dettmers",
      "Artidoro Pagnoni",
      "Ari Holtzman",
      "Luke Zettlemoyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/201408406e0c5cf7626c4baeae6eaadd-Abstract-Conference.html": {
    "title": "EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning",
    "volume": "main",
    "abstract": "Expressing universal semantics common to all languages is helpful to understand the meanings of complex and culture-specific sentences. The research theme underlying this scenario focuses on learning universal representations across languages with the usage of massive parallel corpora. However, due to the sparsity and scarcity of parallel data, there is still a big challenge in learning authentic ``universals'' for any two languages. In this paper, we propose Emma-X: an EM-like Multilingual pre-training Algorithm, to learn Cross-lingual universals with the aid of excessive multilingual non-parallel data. Emma-X unifies the cross-lingual representation learning task and an extra semantic relation prediction task within an EM framework. Both the extra semantic classifier and the cross-lingual sentence encoder approximate the semantic relation of two sentences, and supervise each other until convergence. To evaluate Emma-X, we conduct experiments on xrete, a newly introduced benchmark containing 12 widely studied cross-lingual tasks that fully depend on sentence-level representations. Results reveal that Emma-X achieves state-of-the-art performance. Further geometric analysis of the built representation space with three requirements demonstrates the superiority of Emma-X over advanced models",
    "keywords": [],
    "checked": true,
    "id": "7cf11e610eb924f83fb3e7816324f0b75c8d12b5",
    "semantic_title": "emma-x: an em-like multilingual pre-training algorithm for cross-lingual representation learning",
    "citation_count": 0,
    "authors": [
      "Ping Guo",
      "Xiangpeng Wei",
      "Yue Hu",
      "Baosong Yang",
      "Dayiheng Liu",
      "Fei Huang",
      "jun xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/204d9a9a4816a45909010587ffc3204b-Abstract-Conference.html": {
    "title": "Tester-Learners for Halfspaces: Universal Algorithms",
    "volume": "main",
    "abstract": "We give the first tester-learner for halfspaces that succeeds universally over a wide class of structured distributions. Our universal tester-learner runs in fully polynomial time and has the following guarantee: the learner achieves error $O(\\mathrm{opt}) + \\epsilon$ on any labeled distribution that the tester accepts, and moreover, the tester accepts whenever the marginal is any distribution that satisfies a Poincare inequality. In contrast to prior work on testable learning, our tester is not tailored to any single target distribution but rather succeeds for an entire target class of distributions. The class of Poincare distributions includes all strongly log-concave distributions, and, assuming the Kannan--Lovasz--Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner achieves error $\\mathrm{opt} + \\epsilon$ while accepting all log-concave distributions unconditionally (without assuming KLS).Our tests rely on checking hypercontractivity of the unknown distribution using a sum-of-squares (SOS) program, and crucially make use of the fact that Poincare distributions are certifiably hypercontractive in the SOS framework",
    "keywords": [],
    "checked": true,
    "id": "6e86f984b0f17e1902f766ccc9f65047b50575a8",
    "semantic_title": "tester-learners for halfspaces: universal algorithms",
    "citation_count": 1,
    "authors": [
      "Aravind Gollakota",
      "Adam Klivans",
      "Konstantinos Stavropoulos",
      "Arsen Vasilyan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/204f828ba287fdecf41dd002e9a07d8c-Abstract-Conference.html": {
    "title": "Revisit the Power of Vanilla Knowledge Distillation: from Small Scale to Large Scale",
    "volume": "main",
    "abstract": "The tremendous success of large models trained on extensive datasets demonstrates that scale is a key ingredient in achieving superior results. Therefore, the reflection on the rationality of designing knowledge distillation (KD) approaches for limited-capacity architectures solely based on small-scale datasets is now deemed imperative. In this paper, we identify the small data pitfall that presents in previous KD methods, which results in the underestimation of the power of vanilla KD framework on large-scale datasets such as ImageNet-1K. Specifically, we show that employing stronger data augmentation techniques and using larger datasets can directly decrease the gap between vanilla KD and other meticulously designed KD variants. This highlights the necessity of designing and evaluating KD approaches in the context of practical scenarios, casting off the limitations of small-scale datasets. Our investigation of the vanilla KD and its variants in more complex schemes, including stronger training strategies and different model capacities, demonstrates that vanilla KD is elegantly simple but astonishingly effective in large-scale scenarios. Without bells and whistles, we obtain state-of-the-art ResNet-50, ViT-S, and ConvNeXtV2-T models for ImageNet, which achieve 83.1%, 84.3%, and 85.0% top-1 accuracy, respectively. PyTorch code and checkpoints can be found at https://github.com/Hao840/vanillaKD",
    "keywords": [],
    "checked": false,
    "id": "da1946bb4220e743e8f46946397a9b31e609df74",
    "semantic_title": "vanillakd: revisit the power of vanilla knowledge distillation from small scale to large scale",
    "citation_count": 5,
    "authors": [
      "Zhiwei Hao",
      "Jianyuan Guo",
      "Kai Han",
      "Han Hu",
      "Chang Xu",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/206191b9b7349e2743d98d855dec9e58-Abstract-Conference.html": {
    "title": "LD2: Scalable Heterophilous Graph Neural Network with Decoupled Embeddings",
    "volume": "main",
    "abstract": "Heterophilous Graph Neural Network (GNN) is a family of GNNs that specializes in learning graphs under heterophily, where connected nodes tend to have different labels. Most existing heterophilous models incorporate iterative non-local computations to capture node relationships. However, these approaches have limited application to large-scale graphs due to their high computational costs and challenges in adopting minibatch schemes. In this work, we study the scalability issues of heterophilous GNN and propose a scalable model, LD2, which simplifies the learning process by decoupling graph propagation and generating expressive embeddings prior to training. Theoretical analysis demonstrates that LD2 achieves optimal time complexity in training, as well as a memory footprint that remains independent of the graph scale. We conduct extensive experiments to showcase that our model is capable of lightweight minibatch training on large-scale heterophilous graphs, with up to $15\\times$ speed improvement and efficient memory utilization, while maintaining comparable or better performance than the baselines",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningyi Liao",
      "Siqiang Luo",
      "Xiang Li",
      "Jieming Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2063a00c435aafbcc58c16ce1e522139-Abstract-Conference.html": {
    "title": "On Single-Index Models beyond Gaussian Data",
    "volume": "main",
    "abstract": "Sparse high-dimensional functions have arisen as a rich framework to study the behavior of gradient-descent methods using shallow neural networks, and showcasing its ability to perform feature learning beyond linear models. Amongst those functions, the simplest are single-index models $f(x) = \\phi( x \\cdot \\theta^*)$, where the labels are generated by an arbitrary non-linear link function $\\phi$ of an unknown one-dimensional projection $\\theta^*$ of the input data. By focusing on Gaussian data, several recent works have built a remarkable picture, where the so-called information exponent (related to the regularity of the link function) controls the required sample complexity. In essence, these tools exploit the stability and spherical symmetry of Gaussian distributions.In this work, we explore extensions of this picture beyond the Gaussian setting, where both stability or symmetry might be violated. Focusing on the planted setting where $\\phi$ is known, our main results establish that Stochastic Gradient Descent recovers the unknown direction $\\theta^*$ with constant probability in the high-dimensional regime, under mild assumptions that significantly extend ~[Yehudai and Shamir,20]",
    "keywords": [],
    "checked": false,
    "id": "c23d85e038718bd1ff0ada71df70ab1d934800be",
    "semantic_title": "on single index models beyond gaussian data",
    "citation_count": 2,
    "authors": [
      "Aaron Zweig",
      "Loucas PILLAUD-VIVIEN",
      "Joan Bruna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2082273791021571c410f41d565d0b45-Abstract-Conference.html": {
    "title": "Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?",
    "volume": "main",
    "abstract": "Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used to evaluate model privacy risk under reconstruction attacks. Under these metrics, reconstructed images that are determined to resemble the original one generally indicate more privacy leakage. Images determined as overall dissimilar, on the other hand, indicate higher robustness against attack. However, there is no guarantee that these metrics well reflect human opinions, which offers trustworthy judgement for model privacy leakage. In this paper, we comprehensively study the faithfulness of these hand-crafted metrics to human perception of privacy information from the reconstructed images. On 5 datasets ranging from natural images, faces, to fine-grained classes, we use 4 existing attack methods to reconstruct images from many different classification models and, for each reconstructed image, we ask multiple human annotators to assess whether this image is recognizable. Our studies reveal that the hand-crafted metrics only have a weak correlation with the human evaluation of privacy leakage and that even these metrics themselves often contradict each other. These observations suggest risks of current metrics in the community. To address this potential risk, we propose a learning-based measure called SemSim to evaluate the Semantic Similarity between the original and reconstructed images. SemSim is trained with a standard triplet loss, using an original image as an anchor, one of its recognizable reconstructed images as a positive sample, and an unrecognizable one as a negative. By training on human annotations, SemSim exhibits a greater reflection of privacy leakage on the semantic level. We show that SemSim has a significantly higher correlation with human judgment compared with existing metrics. Moreover, this strong correlation generalizes to unseen datasets, models and attack methods. We envision this work as a milestone for image quality evaluation closer to the human level. The project webpage can be accessed at https://sites.google.com/view/semsim",
    "keywords": [],
    "checked": true,
    "id": "d0739b282fb4da1a238dafd4dd43b2003312b1a9",
    "semantic_title": "privacy assessment on reconstructed images: are existing evaluation metrics faithful to human perception?",
    "citation_count": 2,
    "authors": [
      "Xiaoxiao Sun",
      "Nidham Gazagnadou",
      "Vivek Sharma",
      "Lingjuan Lyu",
      "Hongdong Li",
      "Liang Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/20888d00c5df685de2c09790040e0327-Abstract-Conference.html": {
    "title": "Graph Denoising Diffusion for Inverse Protein Folding",
    "volume": "main",
    "abstract": "Inverse protein folding is challenging due to its inherent one-to-many mapping characteristic, where numerous possible amino acid sequences can fold into a single, identical protein backbone. This task involves not only identifying viable sequences but also representing the sheer diversity of potential solutions. However, existing discriminative models, such as transformer-based auto-regressive models, struggle to encapsulate the diverse range of plausible solutions. In contrast, diffusion probabilistic models, as an emerging genre of generative approaches, offer the potential to generate a diverse set of sequence candidates for determined protein backbones. We propose a novel graph denoising diffusion model for inverse protein folding, where a given protein backbone guides the diffusion process on the corresponding amino acid residue types. The model infers the joint distribution of amino acids conditioned on the nodes' physiochemical properties and local environment. Moreover, we utilize amino acid replacement matrices for the diffusion forward process, encoding the biologically-meaningful prior knowledge of amino acids from their spatial and sequential neighbors as well as themselves, which reduces the sampling space of the generative process. Our model achieves state-of-the-art performance over a set of popular baseline methods in sequence recovery and exhibits great potential in generating diverse protein sequences for a determined protein backbone structure",
    "keywords": [],
    "checked": true,
    "id": "f3570e639e2acda861ea6efe02a4fcb321d76c40",
    "semantic_title": "graph denoising diffusion for inverse protein folding",
    "citation_count": 8,
    "authors": [
      "Kai Yi",
      "Bingxin Zhou",
      "Yiqing Shen",
      "Pietro Lió",
      "Yuguang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2093ed77c549eda95bd6f7212b735b43-Abstract-Conference.html": {
    "title": "Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation",
    "volume": "main",
    "abstract": "Open-vocabulary semantic segmentation is a challenging task that requires segmenting novel object categories at inference time. Recent works explore vision-language pre-training to handle this task, but suffer from unrealistic assumptions in practical scenarios, \\ie, low-quality textual category names.For example, this paradigm assumes that new textual categories will be accurately and completely provided, and exist in lexicons during pre-training.However, exceptions often happen when meet with ambiguity for brief or incomplete names, new words that are not present in the pre-trained lexicons, and difficult-to-describe categories for users.To address these issues, this work proposes a novel attribute decomposition-aggregation framework, inspired by human cognition in understanding new concepts. Specifically, in the decomposition stage, we decouple class names into diverse attribute descriptions to complement semantic contexts from multiple perspectives.Two attribute construction strategies are designed: using large language models for common categories, and involving manually labelling for human-invented categories. In the aggregation stage, we group diverse attributes into an integrated global description, to form a discriminative classifier that distinguishes the target object from others. One hierarchical aggregation architecture is further proposed to achieve multi-level aggregation, leveraging the meticulously designed clustering module.The final result is obtained by computing the similarity between aggregated attributes and images embedding.To evaluate the effectiveness, we annotate three datasets with attribute descriptions, and conduct extensive experiments and ablation studies. The results show the superior performance of attribute decomposition-aggregation.We refer readers to the latest arXiv version at https://arxiv.org/abs/2309.00096",
    "keywords": [],
    "checked": true,
    "id": "201bf774bbbad9ea9cae9394751c81c294fca971",
    "semantic_title": "open-vocabulary semantic segmentation via attribute decomposition-aggregation",
    "citation_count": 4,
    "authors": [
      "Chaofan Ma",
      "Yang Yuhuan",
      "Chen Ju",
      "Fei Zhang",
      "Ya Zhang",
      "Yanfeng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/20bd42d82998bc61732c00452228e814-Abstract-Conference.html": {
    "title": "Stable and low-precision training for large-scale vision-language models",
    "volume": "main",
    "abstract": "We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge---the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) For stability, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test",
    "keywords": [],
    "checked": true,
    "id": "b3c64a046d6cfc0f55a2aebc5176bbab69a7e021",
    "semantic_title": "stable and low-precision training for large-scale vision-language models",
    "citation_count": 7,
    "authors": [
      "Mitchell Wortsman",
      "Tim Dettmers",
      "Luke Zettlemoyer",
      "Ari Morcos",
      "Ali Farhadi",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/20dcab0f14046a5c6b02b61da9f13229-Abstract-Conference.html": {
    "title": "Recommender Systems with Generative Retrieval",
    "volume": "main",
    "abstract": "Modern recommender systems perform large-scale retrieval by embedding queries and item candidates in the same unified space, followed by approximate nearest neighbor search to select top candidates given a query embedding. In this paper, we propose a novel generative retrieval approach, where the retrieval model autoregressively decodes the identifiers of the target candidates. To that end, we create semantically meaningful tuple of codewords to serve as a Semantic ID for each item. Given Semantic IDs for items in a user session, a Transformer-based sequence-to-sequence model is trained to predict the Semantic ID of the next item that the user will interact with. We show that recommender systems trained with the proposed paradigm significantly outperform the current SOTA models on various datasets. In addition, we show that incorporating Semantic IDs into the sequence-to-sequence model enhances its ability to generalize, as evidenced by the improved retrieval performance observed for items with no prior interaction history",
    "keywords": [],
    "checked": true,
    "id": "a289100678e7d94af836d91cd48d7821ebc5b83d",
    "semantic_title": "recommender systems with generative retrieval",
    "citation_count": 11,
    "authors": [
      "Shashank Rajput",
      "Nikhil Mehta",
      "Anima Singh",
      "Raghunandan Hulikal Keshavan",
      "Trung Vu",
      "Lukasz Heldt",
      "Lichan Hong",
      "Yi Tay",
      "Vinh Tran",
      "Jonah Samost",
      "Maciej Kula",
      "Ed Chi",
      "Mahesh Sathiamoorthy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/20e6b4dd2b1f82bc599c593882f67f75-Abstract-Conference.html": {
    "title": "Active Vision Reinforcement Learning under Limited Visual Observability",
    "volume": "main",
    "abstract": "In this work, we investigate Active Vision Reinforcement Learning (ActiveVision-RL), where an embodied agent simultaneously learns action policy for the task while also controlling its visual observations in partially observable environments. We denote the former as motor policy and the latter as sensory policy. For example, humans solve real world tasks by hand manipulation (motor policy) together with eye movements (sensory policy). ActiveVision-RL poses challenges on coordinating two policies given their mutual influence. We propose SUGARL, Sensorimotor Understanding Guided Active Reinforcement Learning, a framework that models motor and sensory policies separately, but jointly learns them using with an intrinsic sensorimotor reward. This learnable reward is assigned by sensorimotor reward module, incentivizes the sensory policy to select observations that are optimal to infer its own motor action, inspired by the sensorimotor stage of humans. Through a series of experiments, we show the effectiveness of our method across a range of observability conditions and its adaptability to existed RL algorithms. The sensory policies learned through our method are observed to exhibit effective active vision strategies",
    "keywords": [],
    "checked": true,
    "id": "728d1de421289aa700a597f31ff2782faaf0fbf2",
    "semantic_title": "active vision reinforcement learning under limited visual observability",
    "citation_count": 0,
    "authors": [
      "Jinghuan Shang",
      "Michael S Ryoo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/20f814ecdaa8c76131e21683447e347b-Abstract-Conference.html": {
    "title": "Optimization of Inter-group criteria for clustering with minimum size constraints",
    "volume": "main",
    "abstract": "Internal measures that are used to assess the quality of a clustering usually take into account intra-group and/or inter-group criteria.There are many papers in the literature that propose algorithms with provable approximation guarantees for optimizing the former. However, the optimization of inter-group criteria is much less understood.Here, we contribute to the state-of-the-art of this literature by devising algorithms with provable guarantees for the maximization of two natural inter-group criteria, namely the minimum spacing and the minimum spanning tree spacing. The former is the minimum distance between points in different groups while the latter captures separability through the cost of the minimum spanning tree that connects all groups. We obtain results for both the unrestricted case, in which no constraint on the clusters is imposed, and for the constrained case where each group is required to have a minimum number of points. Our constraint is motivated by the fact that the popular Single-Linkage, which optimizes both criteria in the unrestricted case, produces clustering with many tiny groups.To complement our work, we present an empirical study with 10 real datasets that provides evidence that our methods work very well in practical settings",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduardo Laber",
      "Lucas Murtinho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/21293a43d0321c5602dd893be2c2332b-Abstract-Conference.html": {
    "title": "CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation",
    "volume": "main",
    "abstract": "Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. Cyclenet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train. Project homepage: https://cyclenetweb.github.io/",
    "keywords": [],
    "checked": true,
    "id": "2d895c3153a80d281428a14d14ae121536fe790d",
    "semantic_title": "cyclenet: rethinking cycle consistency in text-guided diffusion for image manipulation",
    "citation_count": 3,
    "authors": [
      "Sihan Xu",
      "Ziqiao Ma",
      "Yidong Huang",
      "Honglak Lee",
      "Joyce Chai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/212b143b5a5d6b88feb0fb1441b9756e-Abstract-Conference.html": {
    "title": "Bandit Social Learning under Myopic Behavior",
    "volume": "main",
    "abstract": "We study social learning dynamics motivated by reviews on online platforms. Theagents collectively follow a simple multi-armed bandit protocol, but each agentacts myopically, without regards to exploration. We allow a wide range of myopicbehaviors that are consistent with (parameterized) confidence intervals for the arms'expected rewards. We derive stark exploration failures for any such behavior, andprovide matching positive results. As a special case, we obtain the first generalresults on failure of the greedy algorithm in bandits, thus providing a theoreticalfoundation for why bandit algorithms should explore",
    "keywords": [],
    "checked": false,
    "id": "58a8abdc17952e20892d45cc81d74d96f59de470",
    "semantic_title": "bandit social learning: exploration under myopic behavior",
    "citation_count": 3,
    "authors": [
      "Kiarash Banihashem",
      "MohammadTaghi Hajiaghayi",
      "Suho Shin",
      "Aleksandrs Slivkins"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/214ce905bf2072535e34b3cf873cbbc8-Abstract-Conference.html": {
    "title": "Gradient Flossing: Improving Gradient Descent through Dynamic Control of Jacobians",
    "volume": "main",
    "abstract": "Training recurrent neural networks (RNNs) remains a challenge due to the instability of gradients across long time horizons, which can lead to exploding and vanishing gradients. Recent research has linked these problems to the values of Lyapunov exponents for the forward-dynamics, which describe the growth or shrinkage of infinitesimal perturbations. Here, we propose gradient flossing, a novel approach to tackling gradient instability by pushing Lyapunov exponents of the forward dynamics toward zero during learning. We achieve this by regularizing Lyapunov exponents through backpropagation using differentiable linear algebra. This enables us to \"floss\" the gradients, stabilizing them and thus improving network training. We show that gradient flossing controls not only the gradient norm but also the condition number of the long-term Jacobian, facilitating multidimensional error feedback propagation. We find that applying gradient flossing before training enhances both the success rate and convergence speed for tasks involving long time horizons.For challenging tasks, we show that gradient flossing during training can further increase the time horizon that can be bridged by backpropagation through time. Moreover, we demonstrate the effectiveness of our approach on various RNN architectures and tasks of variable temporal complexity. Additionally, we provide a simple implementation of our gradient flossing algorithm that can be used in practice. Our results indicate that gradient flossing via regularizing Lyapunov exponents can significantly enhance the effectiveness of RNN training and mitigate the exploding and vanishing gradients problem",
    "keywords": [],
    "checked": true,
    "id": "310472ff06fa60c856247f9dc968cb3a89d3a2ca",
    "semantic_title": "gradient flossing: improving gradient descent through dynamic control of jacobians",
    "citation_count": 0,
    "authors": [
      "Rainer Engelken"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/21718991f6acf19a42376b5c7a8668c5-Abstract-Conference.html": {
    "title": "Convolution Monge Mapping Normalization for learning on sleep data",
    "volume": "main",
    "abstract": "In many machine learning applications on signals and biomedical data, especially electroencephalogram (EEG), one major challenge is the variability of the data across subjects, sessions, and hardware devices. In this work, we propose a new method called Convolutional Monge Mapping Normalization ($\\texttt{CMMN}$), which consists in filtering the signals in order to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. $\\texttt{CMMN}$ relies on novel closed-form solutions for optimal transport mappings and barycenters and provides individual test time adaptation to new data without needing to retrain a prediction model. Numerical experiments on sleep EEG data show that $\\texttt{CMMN}$ leads to significant and consistent performance gains independent from the neural network architecture when adapting between subjects, sessions, and even datasets collected with different hardware. Notably our performance gain is on par with much more numerically intensive Domain Adaptation (DA) methods and can be used in conjunction with those for even better performances",
    "keywords": [],
    "checked": false,
    "id": "de16813dba47cb4b9a1ca61ad0f45bb304a86963",
    "semantic_title": "convolutional monge mapping normalization for learning on sleep data",
    "citation_count": 0,
    "authors": [
      "Théo Gnassounou",
      "Rémi Flamary",
      "Alexandre Gramfort"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2184d8450c8a641f9a10c49279087c97-Abstract-Conference.html": {
    "title": "Online learning of long-range dependencies",
    "volume": "main",
    "abstract": "Online learning holds the promise of enabling efficient long-term credit assignment in recurrent neural networks. However, current algorithms fall short of offline backpropagation by either not being scalable or failing to learn long-range dependencies. Here we present a high-performance online learning algorithm that merely doubles the memory and computational requirements of a single inference pass. We achieve this by leveraging independent recurrent modules in multi-layer networks, an architectural motif that has recently been shown to be particularly powerful. Experiments on synthetic memory problems and on the challenging long-range arena benchmark suite reveal that our algorithm performs competitively, establishing a new standard for what can be achieved through online learning. This ability to learn long-range dependencies offers a new perspective on learning in the brain and opens a promising avenue in neuromorphic computing",
    "keywords": [],
    "checked": true,
    "id": "55d8837c72863e63259a506b56222d08812699b0",
    "semantic_title": "online learning of long-range dependencies",
    "citation_count": 4,
    "authors": [
      "Nicolas Zucchet",
      "Robert Meier",
      "Simon Schug",
      "Asier Mujika",
      "Joao Sacramento"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/218d0323ce235090b43a1166159ee328-Abstract-Conference.html": {
    "title": "Fast Rank-1 Lattice Targeted Sampling for Black-box Optimization",
    "volume": "main",
    "abstract": "Black-box optimization has gained great attention for its success in recent applications. However, scaling up to high-dimensional problems with good query efficiency remains challenging. This paper proposes a novel Rank-1 Lattice Targeted Sampling (RLTS) technique to address this issue. Our RLTS benefits from random rank-1 lattice Quasi-Monte Carlo, which enables us to perform fast local exact Gaussian processes (GP) training and inference with $O(n \\log n)$ complexity w.r.t. $n$ batch samples. Furthermore, we developed a fast coordinate searching method with $O(n \\log n)$ time complexity for fast targeted sampling. The fast computation enables us to plug our RLTS into the sampling phase of stochastic optimization methods. This improves the query efficiency while scaling up to higher dimensional problems than Bayesian optimization. Moreover, to construct rank-1 lattices efficiently, we proposed a closed-form construction. Extensive experiments on challenging benchmark test functions and black-box prompt fine-tuning for large language models demonstrate the query efficiency of our RLTS technique",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueming LYU"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/21912f7057935149fa58408ee8cb460e-Abstract-Conference.html": {
    "title": "DreamHuman: Animatable 3D Avatars from Text",
    "volume": "main",
    "abstract": "We present \\emph{DreamHuman}, a method to generate realistic animatable 3D human avatar models entirely from textual descriptions. Recent text-to-3D methods have made considerable strides in generation, but are still lacking in important aspects. Control and often spatial resolution remain limited, existing methods produce fixed rather than 3D human models that can be placed in different poses (i.e. re-posable or animatable), and anthropometric consistency for complex structures like people remains a challenge. \\emph{DreamHuman} connects large text-to-image synthesis models, neural radiance fields, and statistical human body models in a novel optimization framework. This makes it possible to generate dynamic 3D human avatars with high-quality textures and learnt per-instance rigid and non rigid geometric deformations. We demonstrate that our method is capable to generate a wide variety of animatable, realistic 3D human models from text. These have diverse appearance, clothing, skin tones and body shapes, and outperform both generic text-to-3D approaches and previous text-based 3D avatar generators in visual fidelity",
    "keywords": [],
    "checked": true,
    "id": "1799398201d38f527cd0edcd23024b053984c4ee",
    "semantic_title": "dreamhuman: animatable 3d avatars from text",
    "citation_count": 28,
    "authors": [
      "Nikos Kolotouros",
      "Thiemo Alldieck",
      "Andrei Zanfir",
      "Eduard Bazavan",
      "Mihai Fieraru",
      "Cristian Sminchisescu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/21c426323068204f4199c490d730e88e-Abstract-Conference.html": {
    "title": "Rank-1 Matrix Completion with Gradient Descent and Small Random Initialization",
    "volume": "main",
    "abstract": "The nonconvex formulation of the matrix completion problem has received significant attention in recent years due to its affordable complexity compared to the convex formulation. Gradient Descent (GD) is a simple yet efficient baseline algorithm for solving nonconvex optimization problems. The success of GD has been witnessed in many different problems in both theory and practice when it is combined with random initialization. However, previous works on matrix completion require either careful initialization or regularizers to prove the convergence of GD. In this paper, we study the rank-1 symmetric matrix completion and prove that GD converges to the ground truth when small random initialization is used. We show that in a logarithmic number of iterations, the trajectory enters the region where local convergence occurs. We provide an upper bound on the initialization size that is sufficient to guarantee the convergence, and show that a larger initialization can be used as more samples are available. We observe that the implicit regularization effect of GD plays a critical role in the analysis, and for the entire trajectory, it prevents each entry from becoming much larger than the others",
    "keywords": [],
    "checked": true,
    "id": "0922c64a10218b234d6b8b926be6aaaab09f6088",
    "semantic_title": "rank-1 matrix completion with gradient descent and small random initialization",
    "citation_count": 2,
    "authors": [
      "Daesung Kim",
      "Hye Won Chung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/21c9fd36a6d94a491bf330a0ba0e5f6e-Abstract-Conference.html": {
    "title": "No-Regret Learning in Dynamic Competition with Reference Effects Under Logit Demand",
    "volume": "main",
    "abstract": "This work is dedicated to the algorithm design in a competitive framework, with the primary goal of learning a stable equilibrium. We consider the dynamic price competition between two firms operating within an opaque marketplace, where each firm lacks information about its competitor. The demand follows the multinomial logit (MNL) choice model, which depends on the consumers' observed price and their reference price, and consecutive periods in the repeated games are connected by reference price updates. We use the notion of stationary Nash equilibrium (SNE), defined as the fixed point of the equilibrium pricing policy for the single-period game, to simultaneously capture the long-run market equilibrium and stability. We propose the online projected gradient ascent algorithm (OPGA), where the firms adjust prices using the first-order derivatives of their log-revenues that can be obtained from the market feedback mechanism. Despite the absence of typical properties required for the convergence of online games, such as strong monotonicity and variational stability, we demonstrate that under diminishing step-sizes, the price and reference price paths generated by OPGA converge to the unique SNE, thereby achieving the no-regret learning and a stable market. Moreover, with appropriate step-sizes, we prove that this convergence exhibits a rate of $\\mathcal{O}(1/t)$",
    "keywords": [],
    "checked": true,
    "id": "5618c697d80baaedd91890eebf49fdb4ea9439f0",
    "semantic_title": "no-regret learning in dynamic competition with reference effects under logit demand",
    "citation_count": 0,
    "authors": [
      "Mengzi Amy Guo",
      "Donghao Ying",
      "Javad Lavaei",
      "Zuo-Jun Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/21f1c5bbf2519321c1bee9bfa9edcd46-Abstract-Conference.html": {
    "title": "VoxDet: Voxel Learning for Novel Instance Detection",
    "volume": "main",
    "abstract": "Detecting unseen instances based on multi-view templates is a challenging problem due to its open-world nature. Traditional methodologies, which primarily rely on $2 \\mathrm{D}$ representations and matching techniques, are often inadequate in handling pose variations and occlusions. To solve this, we introduce VoxDet, a pioneer 3D geometry-aware framework that fully utilizes the strong 3D voxel representation and reliable voxel matching mechanism. VoxDet first ingeniously proposes template voxel aggregation (TVA) module, effectively transforming multi-view 2D images into 3D voxel features. By leveraging associated camera poses, these features are aggregated into a compact 3D template voxel. In novel instance detection, this voxel representation demonstrates heightened resilience to occlusion and pose variations. We also discover that a $3 \\mathrm{D}$ reconstruction objective helps to pre-train the 2D-3D mapping in TVA. Second, to quickly align with the template voxel, VoxDet incorporates a Query Voxel Matching (QVM) module. The 2D queries are first converted into their voxel representation with the learned 2D-3D mapping. We find that since the 3D voxel representations encode the geometry, we can first estimate the relative rotation and then compare the aligned voxels, leading to improved accuracy and efficiency. In addition to method, we also introduce the first instance detection benchmark, RoboTools, where 20 unique instances are video-recorded with camera extrinsic. RoboTools also provides 24 challenging cluttered scenarios with more than $9 \\mathrm{k}$ box annotations. Exhaustive experiments are conducted on the demanding LineMod-Occlusion, YCB-video, and RoboTools benchmarks, where VoxDet outperforms various $2 \\mathrm{D}$ baselines remarkably with faster speed. To the best of our knowledge, VoxDet is the first to incorporate implicit 3D knowledge for 2D novel instance detection tasks",
    "keywords": [],
    "checked": true,
    "id": "f6f60d1348b4263977de90bb6a4692c72f562a01",
    "semantic_title": "voxdet: voxel learning for novel instance detection",
    "citation_count": 0,
    "authors": [
      "Bowen Li",
      "Jiashun Wang",
      "Yaoyu Hu",
      "Chen Wang",
      "Sebastian Scherer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/21f7b745f73ce0d1f9bcea7f40b1388e-Abstract-Conference.html": {
    "title": "Evaluating and Inducing Personality in Pre-trained Language Models",
    "volume": "main",
    "abstract": "Standardized and quantified evaluation of machine behaviors is a crux of understanding LLMs. In this study, we draw inspiration from psychometric studies by leveraging human personality theory as a tool for studying machine behaviors. Originating as a philosophical quest for human behaviors, the study of personality delves into how individuals differ in thinking, feeling, and behaving. Toward building and understanding human-like social machines, we are motivated to ask: Can we assess machine behaviors by leveraging human psychometric tests in a **principled** and **quantitative** manner? If so, can we induce a specific personality in LLMs? To answer these questions, we introduce the Machine Personality Inventory (MPI) tool for studying machine behaviors; MPI follows standardizedpersonality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence demonstrating the efficacy of MPI in studying LLMs behaviors. We further devise a Personality Prompting (P$^2$) method to induce LLMs with specific personalities in a **controllable** way, capable of producing diverse and verifiable behaviors. We hope this work sheds light on future studies by adopting personality as the essential indicator for various downstream tasks, and could further motivate research into equally intriguing human-like machine behaviors",
    "keywords": [],
    "checked": true,
    "id": "e30a39570bf8a153bae86a6afde00983be9d7d73",
    "semantic_title": "evaluating and inducing personality in pre-trained language models",
    "citation_count": 14,
    "authors": [
      "Guangyuan Jiang",
      "Manjie Xu",
      "Song-Chun Zhu",
      "Wenjuan Han",
      "Chi Zhang",
      "Yixin Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/220165f9c7f51163b73c8c7fff578b4e-Abstract-Conference.html": {
    "title": "On Measuring Fairness in Generative Models",
    "volume": "main",
    "abstract": "Recently, there has been increased interest in fair generative models. In this work,we conduct, for the first time, an in-depth study on fairness measurement, acritical component in gauging progress on fair generative models. We make threecontributions. First, we conduct a study that reveals that the existing fairnessmeasurement framework has considerable measurement errors, even when highlyaccurate sensitive attribute (SA) classifiers are used. These findings cast doubtson previously reported fairness improvements. Second, to address this issue,we propose CLassifier Error-Aware Measurement (CLEAM), a new frameworkwhich uses a statistical model to account for inaccuracies in SA classifiers. Ourproposed CLEAM reduces measurement errors significantly, e.g., 4.98%→0.62%for StyleGAN2 w.r.t. Gender. Additionally, CLEAM achieves this with minimaladditional overhead. Third, we utilize CLEAM to measure fairness in importanttext-to-image generator and GANs, revealing considerable biases in these modelsthat raise concerns about their applications. Code and more resources: https://sutd-visual-computing-group.github.io/CLEAM/",
    "keywords": [],
    "checked": true,
    "id": "ba4e5eb1dfb6da18e36de8ab7f12bd5f02e59ed3",
    "semantic_title": "on measuring fairness in generative models",
    "citation_count": 0,
    "authors": [
      "Christopher Teo",
      "Milad Abdollahzadeh",
      "Ngai-Man (Man) Cheung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/222dda29587fbc2979ca99fd5ed00735-Abstract-Conference.html": {
    "title": "IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI",
    "volume": "main",
    "abstract": "Diffusion-based image generation models, such as Stable Diffusion or DALL·E 2, are able to learn from given images and generate high-quality samples following the guidance from prompts. For instance, they can be used to create artistic images that mimic the style of an artist based on his/her original artworks or to maliciously edit the original images for fake content. However, such ability also brings serious ethical issues without proper authorization from the owner of the original images. In response, several attempts have been made to protect the original images from such unauthorized data usage by adding imperceptible perturbations, which are designed to mislead the diffusion model and make it unable to properly generate new samples. In this work, we introduce a perturbation purification platform, named IMPRESS, to evaluate the effectiveness of imperceptible perturbations as a protective measure.IMPRESS is based on the key observation that imperceptible perturbations could lead to a perceptible inconsistency between the original image and the diffusion-reconstructed image, which can be used to devise a new optimization strategy for purifying the image, which may weaken the protection of the original image from unauthorized data usage (e.g., style mimicking, malicious editing).The proposed IMPRESS platform offers a comprehensive evaluation of several contemporary protection methods, and can be used as an evaluation platform for future protection methods",
    "keywords": [],
    "checked": true,
    "id": "c0359ad23c7374bde0a361e01854553f7d73b80e",
    "semantic_title": "impress: evaluating the resilience of imperceptible perturbations against unauthorized data usage in diffusion-based generative ai",
    "citation_count": 2,
    "authors": [
      "Bochuan Cao",
      "Changjiang Li",
      "Ting Wang",
      "Jinyuan Jia",
      "Bo Li",
      "Jinghui Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2232e8fee69b150005ac420bfa83d705-Abstract-Conference.html": {
    "title": "Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks",
    "volume": "main",
    "abstract": "Contrastive vision-language representation learning has achieved state-of-the-art performance for zero-shot classification, by learning from millions of image-caption pairs crawled from the internet. However, the massive data that powers large multimodal models such as CLIP, makes them extremely vulnerable to various types of targeted data poisoning and backdoor attacks. Despite this vulnerability, robust contrastive vision-language pre-training against such attacks has remained unaddressed. In this work, we propose RoCLIP, the first effective method for robust pre-training multimodal vision-language models against targeted data poisoning and backdoor attacks. RoCLIP effectively breaks the association between poisoned image-caption pairs by considering a relatively large and varying pool of random captions, and matching every image with the text that is most similar to it in the pool instead of its own caption, every few epochs.It also leverages image and text augmentations to further strengthen the defense and improve the performance of the model. Our extensive experiments show that RoCLIP renders state-of-the-art targeted data poisoning and backdoor attacks ineffective during pre-training CLIP models. In particular, RoCLIP decreases the success rate for targeted data poisoning attacks from 93.75% to 12.5% and that of backdoor attacks down to 0%, while improving the model's linear probe performance by 10% and maintains a similar zero shot performance compared to CLIP. By increasing the frequency of matching, RoCLIP is able to defend strong attacks, which add up to 1% poisoned examples to the data, and successfully maintain a low attack success rate of 12.5%, while trading off the performance on some tasks",
    "keywords": [],
    "checked": false,
    "id": "86923a85225d18da6d7625e5768c0fc3f2a7299e",
    "semantic_title": "robust contrastive language-image pre-training against data poisoning and backdoor attacks",
    "citation_count": 3,
    "authors": [
      "Wenhan Yang",
      "Jingdong Gao",
      "Baharan Mirzasoleiman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2280faacd674566a5eace1bd1098f507-Abstract-Conference.html": {
    "title": "Block-Coordinate Methods and Restarting for Solving Extensive-Form Games",
    "volume": "main",
    "abstract": "Coordinate descent methods are popular in machine learning and optimization for their simple sparse updates and excellent practical performance. In the context of large-scale sequential game solving, these same properties would be attractive, but until now no such methods were known, because the strategy spaces do not satisfy the typical separable block structure exploited by such methods.We present the first cyclic coordinate-descent-like method for the polytope of sequence-form strategies, which form the strategy spaces for the players in an extensive-form game (EFG). Our method exploits the recursive structure of the proximal update induced by what are known as dilated regularizers, in order to allow for a pseudo block-wise update.We show that our method enjoys a O(1/T) convergence rate to a two-player zero-sum Nash equilibrium, while avoiding the worst-case polynomial scaling with the number of blocks common to cyclic methods. We empirically show that our algorithm usually performs better than other state-of-the-art first-order methods (i.e., mirror prox), and occasionally can even beat CFR$^+$, a state-of-the-art algorithm for numerical equilibrium computation in zero-sum EFGs. We then introduce a restarting heuristic for EFG solving. We show empirically that restarting can lead to speedups, sometimes huge, both for our cyclic method, as well as for existing methods such as mirror prox and predictive CFR$^+$",
    "keywords": [],
    "checked": true,
    "id": "7d29764a95b852a164095421536a16f566b9ed65",
    "semantic_title": "block-coordinate methods and restarting for solving extensive-form games",
    "citation_count": 2,
    "authors": [
      "Darshan Chakrabarti",
      "Jelena Diakonikolas",
      "Christian Kroer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/228b9279ecf9bbafe582406850c57115-Abstract-Conference.html": {
    "title": "ReContrast: Domain-Specific Anomaly Detection via Contrastive Reconstruction",
    "volume": "main",
    "abstract": "Most advanced unsupervised anomaly detection (UAD) methods rely on modeling feature representations of frozen encoder networks pre-trained on large-scale datasets, e.g. ImageNet. However, the features extracted from the encoders that are borrowed from natural image domains coincide little with the features required in the target UAD domain, such as industrial inspection and medical imaging. In this paper, we propose a novel epistemic UAD method, namely ReContrast, which optimizes the entire network to reduce biases towards the pre-trained image domain and orients the network in the target domain. We start with a feature reconstruction approach that detects anomalies from errors. Essentially, the elements of contrastive learning are elegantly embedded in feature reconstruction to prevent the network from training instability, pattern collapse, and identical shortcut, while simultaneously optimizing both the encoder and decoder on the target domain. To demonstrate our transfer ability on various image domains, we conduct extensive experiments across two popular industrial defect detection benchmarks and three medical image UAD tasks, which shows our superiority over current state-of-the-art methods",
    "keywords": [],
    "checked": true,
    "id": "bffb25187baf0282ccfeae9dc76dafe7a58ac6cb",
    "semantic_title": "recontrast: domain-specific anomaly detection via contrastive reconstruction",
    "citation_count": 1,
    "authors": [
      "Jia Guo",
      "shuai lu",
      "Lize Jia",
      "Weihang Zhang",
      "Huiqi Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/22a25fc3da528794d52664dacc7bd470-Abstract-Conference.html": {
    "title": "Transferable Adversarial Robustness for Categorical Data via Universal Robust Embeddings",
    "volume": "main",
    "abstract": "Research on adversarial robustness is primarily focused on image and text data. Yet, many scenarios in which lack of robustness can result in serious risks, such as fraud detection, medical diagnosis, or recommender systems often do not rely on images or text but instead on tabular data. Adversarial robustness in tabular data poses two serious challenges. First, tabular datasets often contain categorical features, and therefore cannot be tackled directly with existing optimization procedures. Second, in the tabular domain, algorithms that are not based on deep networks are widely used and offer great performance, but algorithms to enhance robustness are tailored to neural networks (e.g. adversarial training).In this paper, we tackle both challenges. We present a method that allows us to train adversarially robust deep networks for tabular data and to transfer this robustness to other classifiers via universal robust embeddings tailored to categorical data. These embeddings, created using a bilevel alternating minimization framework, can be transferred to boosted trees or random forests making them robust without the need for adversarial training while preserving their high accuracy on tabular data. We show that our methods outperform existing techniques within a practical threat model suitable for tabular data",
    "keywords": [],
    "checked": true,
    "id": "836b080564de7df1e385ea8e5da89842587a5324",
    "semantic_title": "transferable adversarial robustness for categorical data via universal robust embeddings",
    "citation_count": 0,
    "authors": [
      "Klim Kireev",
      "Maksym Andriushchenko",
      "Carmela Troncoso",
      "Nicolas Flammarion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/22f5d8e689d2a011cd8ead552ed59052-Abstract-Conference.html": {
    "title": "Drift doesn't Matter: Dynamic Decomposition with Diffusion Reconstruction for Unstable Multivariate Time Series Anomaly Detection",
    "volume": "main",
    "abstract": "Many unsupervised methods have recently been proposed for multivariate time series anomaly detection. However, existing works mainly focus on stable data yet often omit the drift generated from non-stationary environments, which may lead to numerous false alarms. We propose **D**ynamic **D**ecomposition with **D**iffusion **R**econstruction (D$^3$R), a novel anomaly detection network for real-world unstable data to fill the gap. D$^3$R tackles the drift via decomposition and reconstruction. In the decomposition procedure, we utilize data-time mix-attention to dynamically decompose long-period multivariate time series, overcoming the limitation of the local sliding window. The information bottleneck is critical yet difficult to determine in the reconstruction procedure. To avoid retraining once the bottleneck changes, we control it externally by noise diffusion and directly reconstruct the polluted data. The whole model can be trained end-to-end. Extensive experiments on various real-world datasets demonstrate that D$^3$R significantly outperforms existing methods, with a 11% average relative improvement over the previous SOTA models",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengsen Wang",
      "Zirui Zhuang",
      "Qi Qi",
      "Jingyu Wang",
      "Xingyu Wang",
      "Haifeng Sun",
      "Jianxin Liao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/233278d812e74a4f9848410881db86b1-Abstract-Conference.html": {
    "title": "Improving Adversarial Robustness via Information Bottleneck Distillation",
    "volume": "main",
    "abstract": "Previous studies have shown that optimizing the information bottleneck can significantly improve the robustness of deep neural networks. Our study closely examines the information bottleneck principle and proposes an Information Bottleneck Distillation approach. This specially designed, robust distillation technique utilizes prior knowledge obtained from a robust pre-trained model to boost information bottlenecks. Specifically, we propose two distillation strategies that align with the two optimization processes of the information bottleneck. Firstly, we use a robust soft-label distillation method to increase the mutual information between latent features and output prediction. Secondly, we introduce an adaptive feature distillation method that automatically transfers relevant knowledge from the teacher model to the student model, thereby reducing the mutual information between the input and latent features. We conduct extensive experiments to evaluate our approach's robustness against state-of-the-art adversarial attackers such as PGD-attack and AutoAttack. Our experimental results demonstrate the effectiveness of our approach in significantly improving adversarial robustness. Our code is available at https://github.com/SkyKuang/IBD",
    "keywords": [],
    "checked": false,
    "id": "849080395f9d1bc2efa897da4f8db595c55e443c",
    "semantic_title": "improving adversarial robustness via mutual information estimation",
    "citation_count": 6,
    "authors": [
      "Huafeng Kuang",
      "Hong Liu",
      "Yongjian Wu",
      "Shin'ichi Satoh",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2349293cb1bf2ce36d5c566f660f957e-Abstract-Conference.html": {
    "title": "Reading Relevant Feature from Global Representation Memory for Visual Object Tracking",
    "volume": "main",
    "abstract": "Reference features from a template or historical frames are crucial for visual object tracking. Prior works utilize all features from a fixed template or memory for visual object tracking. However, due to the dynamic nature of videos, the required reference historical information for different search regions at different time steps is also inconsistent. Therefore, using all features in the template and memory can lead to redundancy and impair tracking performance. To alleviate this issue, we propose a novel tracking paradigm, consisting of a relevance attention mechanism and a global representation memory, which can adaptively assist the search region in selecting the most relevant historical information from reference features. Specifically, the proposed relevance attention mechanism in this work differs from previous approaches in that it can dynamically choose and build the optimal global representation memory for the current frame by accessing cross-frame information globally. Moreover, it can flexibly read the relevant historical information from the constructed memory to reduce redundancy and counteract the negative effects of harmful information. Extensive experiments validate the effectiveness of the proposed method, achieving competitive performance on five challenging datasets with 71 FPS",
    "keywords": [],
    "checked": false,
    "id": "76b6d91b7d9c0647767127e637723ec875030d44",
    "semantic_title": "tools for decolonizing photography",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhou",
      "Pinxue Guo",
      "Lingyi Hong",
      "Jinglun Li",
      "Wei Zhang",
      "Weifeng Ge",
      "Wenqiang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/236b6a814a1d2c0ff504ca7bf380f7ff-Abstract-Conference.html": {
    "title": "Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks",
    "volume": "main",
    "abstract": "One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic features -- and show that in the latter setting three-layer networks obtain a sample complexity improvement over all existing guarantees for two-layer networks. Crucially, this sample complexity improvement relies on the ability of three-layer networks to efficiently learn nonlinear features. We then establish a concrete optimization-based depth separation by constructing a function which is efficiently learnable via gradient descent on a three-layer network, yet cannot be learned efficiently by a two-layer network. Our work makes progress towards understanding the provable benefit of three-layer neural networks over two-layer networks in the feature learning regime",
    "keywords": [],
    "checked": true,
    "id": "3a41d05efb2835ef4b2995b708e4dd5f76a637de",
    "semantic_title": "provable guarantees for nonlinear feature learning in three-layer neural networks",
    "citation_count": 3,
    "authors": [
      "Eshaan Nichani",
      "Alex Damian",
      "Jason D. Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2376f25ef1725a9e3516ee3c86a59f46-Abstract-Conference.html": {
    "title": "Lockdown: Backdoor Defense for Federated Learning with Isolated Subspace Training",
    "volume": "main",
    "abstract": "Federated learning (FL) is vulnerable to backdoor attacks due to its distributed computing nature. Existing defense solution usually requires larger amount of computation in either the training or testing phase, which limits their practicality in the resource-constrain scenarios. A more practical defense, i.e., neural network (NN) pruning based defense has been proposed in centralized backdoor setting. However, our empirical study shows that traditional pruning-based solution suffers \\textit{poison-coupling} effect in FL, which significantly degrades the defense performance.This paper presents Lockdown, an isolated subspace training method to mitigate the poison-coupling effect. Lockdown follows three key procedures. First, it modifies the training protocol by isolating the training subspaces for different clients. Second, it utilizes randomness in initializing isolated subspacess, and performs subspace pruning and subspace recovery to segregate the subspaces between malicious and benign clients. Third, it introduces quorum consensus to cure the global model by purging malicious/dummy parameters. Empirical results show that Lockdown achieves \\textit{superior} and \\textit{consistent} defense performance compared to existing representative approaches against backdoor attacks. Another value-added property of Lockdown is the communication-efficiency and model complexity reduction, which are both critical for resource-constrain FL scenario. Our code is available at \\url{https://github.com/git-disl/Lockdown}",
    "keywords": [],
    "checked": true,
    "id": "a381279b78f7ccc40df15762d91cadc56a9d0608",
    "semantic_title": "lockdown: backdoor defense for federated learning with isolated subspace training",
    "citation_count": 0,
    "authors": [
      "Tiansheng Huang",
      "Sihao Hu",
      "Ka-Ho Chow",
      "Fatih Ilhan",
      "Selim Tekin",
      "Ling Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/238f3b98bbe998b4f2234443907fe663-Abstract-Conference.html": {
    "title": "Robust Lipschitz Bandits to Adversarial Corruptions",
    "volume": "main",
    "abstract": "Lipschitz bandit is a variant of stochastic bandits that deals with a continuous arm set defined on a metric space, where the reward function is subject to a Lipschitz constraint. In this paper, we introduce a new problem of Lipschitz bandits in the presence of adversarial corruptions where an adaptive adversary corrupts the stochastic rewards up to a total budget $C$. The budget is measured by the sum of corruption levels across the time horizon $T$. We consider both weak and strong adversaries, where the weak adversary is unaware of the current action before the attack, while the strong one can observe it. Our work presents the first line of robust Lipschitz bandit algorithms that can achieve sub-linear regret under both types of adversary, even when the total budget of corruption $C$ is unrevealed to the agent. We provide a lower bound under each type of adversary, and show that our algorithm is optimal under the strong case. Finally, we conduct experiments to illustrate the effectiveness of our algorithms against two classic kinds of attacks",
    "keywords": [],
    "checked": true,
    "id": "f5db5aee63f5e91dc3d36ad072fa580e2021efe6",
    "semantic_title": "robust lipschitz bandits to adversarial corruptions",
    "citation_count": 4,
    "authors": [
      "Yue Kang",
      "Cho-Jui Hsieh",
      "Thomas Chun Man Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/23aa2163dea287441ebebc1295d5b3fc-Abstract-Conference.html": {
    "title": "Predicting Global Label Relationship Matrix for Graph Neural Networks under Heterophily",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have been shown to achieve remarkable performance on node classification tasks by exploiting both graph structures and node features. The majority of existing GNNs rely on the implicit homophily assumption. Recent studies have demonstrated that GNNs may struggle to model heterophilous graphs where nodes with different labels are more likely connected. To address this issue, we propose a generic GNN applicable to both homophilous and heterophilous graphs, namely Low-Rank Graph Neural Network (LRGNN). Our analysis demonstrates that a signed graph's global label relationship matrix has a low rank. This insight inspires us to predict the label relationship matrix by solving a robust low-rank matrix approximation problem, as prior research has proven that low-rank approximation could achieve perfect recovery under certain conditions. The experimental results reveal that the solution bears a strong resemblance to the label relationship matrix, presenting two advantages for graph modeling: a block diagonal structure and varying distributions of within-class and between-class entries",
    "keywords": [],
    "checked": false,
    "id": "5af294d2ea7ccddb52ccd3025584a2a1b9b895ea",
    "semantic_title": "revisiting homophily ratio: a relation-aware graph neural network for homophily and heterophily",
    "citation_count": 1,
    "authors": [
      "Langzhang Liang",
      "Xiangjing Hu",
      "Zenglin Xu",
      "Zixing Song",
      "Irwin King"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/23e6f78bdec844a9f7b6c957de2aae91-Abstract-Conference.html": {
    "title": "RRHF: Rank Responses to Align Language Models with Human Feedback",
    "volume": "main",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts.In contrast, we propose a novel learning paradigm called RRHF, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss.RRHF can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them.RRHF only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. Additionally, RRHF can be considered an extension of SFT and reward model training while being simpler than PPO in terms of coding, model counts, and hyperparameters. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling.Extensive experiments show that the performance of RRHF is highly related to sampling quality which suggests RRHF is a best-of-$n$ learner",
    "keywords": [],
    "checked": false,
    "id": "748698bd4387afd08594e0dc8150c2afa210d9ae",
    "semantic_title": "rrhf: rank responses to align language models with human feedback without tears",
    "citation_count": 120,
    "authors": [
      "Hongyi Yuan",
      "Zheng Yuan",
      "Chuanqi Tan",
      "Wei Wang",
      "Songfang Huang",
      "Fei Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/23ff02034404b65776080cbf7148addd-Abstract-Conference.html": {
    "title": "Sparsity-Preserving Differentially Private Training of Large Embedding Models",
    "volume": "main",
    "abstract": "As the use of large embedding models in recommendation systems and language applications increases, concerns over user data privacy have also risen. DP-SGD, a training algorithm that combines differential privacy with stochastic gradient descent, has been the workhorse in protecting user privacy without compromising model accuracy by much. However, applying DP-SGD naively to embedding models can destroy gradient sparsity, leading to reduced training efficiency. To address this issue, we present two new algorithms, DP-FEST and DP-AdaFEST, that preserve gradient sparsity during the private training of large embedding models. Our algorithms achieve substantial reductions ($10^6 \\times$) in gradient size, while maintaining comparable levels of accuracy, on benchmark real-world datasets",
    "keywords": [],
    "checked": true,
    "id": "76d1781c8ae8c28841e4ba8ed226384eb6f77fd0",
    "semantic_title": "sparsity-preserving differentially private training of large embedding models",
    "citation_count": 0,
    "authors": [
      "Badih Ghazi",
      "Yangsibo Huang",
      "Pritish Kamath",
      "Ravi Kumar",
      "Pasin Manurangsi",
      "Amer Sinha",
      "Chiyuan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/240225294cdd2c9b692c2519d3278a08-Abstract-Conference.html": {
    "title": "Bayesian target optimisation for high-precision holographic optogenetics",
    "volume": "main",
    "abstract": "Two-photon optogenetics has transformed our ability to probe the structure and function of neural circuits. However, achieving precise optogenetic control of neural ensemble activity has remained fundamentally constrained by the problem of off-target stimulation (OTS): the inadvertent activation of nearby non-target neurons due to imperfect confinement of light onto target neurons. Here we propose a novel computational approach to this problem called Bayesian target optimisation. Our approach uses nonparametric Bayesian inference to model neural responses to optogenetic stimulation, and then optimises the laser powers and optical target locations needed to achieve a desired activity pattern with minimal OTS. We validate our approach in simulations and using data from in vitro experiments, showing that Bayesian target optimisation considerably reduces OTS across all conditions we test. Together, these results establish our ability to overcome OTS, enabling optogenetic stimulation with substantially improved precision",
    "keywords": [],
    "checked": true,
    "id": "9d1fdb72be2328293d106365d9f20374230ad1a3",
    "semantic_title": "bayesian target optimisation for high-precision holographic optogenetics",
    "citation_count": 1,
    "authors": [
      "Marcus Triplett",
      "Marta Gajowa",
      "Hillel Adesnik",
      "Liam Paninski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/240cc9ac4789351653d13cfcba4ee85c-Abstract-Conference.html": {
    "title": "From ViT Features to Training-free Video Object Segmentation via Streaming-data Mixture Models",
    "volume": "main",
    "abstract": "In the task of semi-supervised video object segmentation, the input is the binary mask of an object in the first frame, and the desired output consists of the corresponding masks of that object in the subsequent frames. Existing leading solutions have two main drawbacks: 1) an expensive and typically-supervised training on videos; 2) a large memory footprint during inference. Here we present a training-free solution, with a low-memory footprint, that yields state-of-the-art results. The proposed method combines pre-trained deep learning-based features (trained on still images) with more classical methods for streaming-data clustering. Designed to adapt to temporal concept drifts and generalize to diverse video content without relying on annotated images or videos, the method eliminates the need for additional training or fine-tuning, ensuring fast inference and immediate applicability to new videos. Concretely, we represent an object via a dynamic ensemble of temporally- and spatially-coherent mixtures over a representation built from pre-trained ViT features and positional embeddings. A convolutional conditional random field further improves spatial coherence and helps reject outliers. We demonstrate the efficacy of the method on key benchmarks: the DAVIS-2017 and YouTube-VOS 2018 validation datasets. Moreover, by the virtue of the low-memory footprint of the compact cluster-based representation, the method scales gracefully to high-resolution ViT features. Our code is available at https://github.com/BGU-CS-VIL/Training-Free-VOS",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roy Uziel",
      "Or Dinari",
      "Oren Freifeld"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2433fec2144ccf5fea1c9c5ebdbc3924-Abstract-Conference.html": {
    "title": "What Knowledge Gets Distilled in Knowledge Distillation?",
    "volume": "main",
    "abstract": "Knowledge distillation aims to transfer useful information from a teacher network to a student network, with the primary goal of improving the student's performance for the task at hand. Over the years, there has a been a deluge of novel techniques and use cases of knowledge distillation. Yet, despite the various improvements, there seems to be a glaring gap in the community's fundamental understanding of the process. Specifically, what is the knowledge that gets distilled in knowledge distillation? In other words, in what ways does the student become similar to the teacher? Does it start to localize objects in the same way? Does it get fooled by the same adversarial samples? Does its data invariance properties become similar? Our work presents a comprehensive study to try to answer these questions. We show that existing methods can indeed indirectly distill these properties beyond improving task performance. We further study why knowledge distillation might work this way, and show that our findings have practical implications as well",
    "keywords": [],
    "checked": true,
    "id": "d3934917102f76bd59aa8fd47523bca6376cae81",
    "semantic_title": "what knowledge gets distilled in knowledge distillation?",
    "citation_count": 6,
    "authors": [
      "Utkarsh Ojha",
      "Yuheng Li",
      "Anirudh Sundara Rajan",
      "Yingyu Liang",
      "Yong Jae Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/243697ace81f57daef8737ff2c5cffd3-Abstract-Conference.html": {
    "title": "Kernelized Cumulants: Beyond Kernel Mean Embeddings",
    "volume": "main",
    "abstract": "In $\\mathbb{R}^d$, it is well-known that cumulants provide an alternative to moments that can achieve the same goals with numerous benefits such as lower variance estimators. In this paper we extend cumulants to reproducing kernel Hilbert spaces (RKHS) using tools from tensor algebras and show that they are computationally tractable by a kernel trick. These kernelized cumulants provide a new set of all-purpose statistics; the classical maximum mean discrepancy and Hilbert-Schmidt independence criterion arise as the degree one objects in our general construction. We argue both theoretically and empirically (on synthetic, environmental, and traffic data analysis) that going beyond degree one has several advantages and can be achieved with the same computational complexity and minimal overhead in our experiments",
    "keywords": [],
    "checked": true,
    "id": "d6c4c81a0bbb761485cd80f75aee266d9fc91081",
    "semantic_title": "kernelized cumulants: beyond kernel mean embeddings",
    "citation_count": 0,
    "authors": [
      "Patric Bonnier",
      "Harald Oberhauser",
      "Zoltan Szabo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2439ec22091b9d6cfbebf3284b40116e-Abstract-Conference.html": {
    "title": "Contrastive Training of Complex-Valued Autoencoders for Object Discovery",
    "volume": "main",
    "abstract": "Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, working examples of such synchrony-based models have been developed only very recently, and are still limited to toy grayscale datasets and simultaneous storage of less than three objects in practice. Here we introduce architectural modifications and a novel contrastive learning method that greatly improve the state-of-the-art synchrony-based model. For the first time, we obtain a class of synchrony-based models capable of discovering objects in an unsupervised manner in multi-object color datasets and simultaneously representing more than three objects",
    "keywords": [],
    "checked": true,
    "id": "fd7d2e45ff8bad1518c0ac307efc3a5f1db1cb75",
    "semantic_title": "contrastive training of complex-valued autoencoders for object discovery",
    "citation_count": 4,
    "authors": [
      "Aleksandar Stanić",
      "Anand Gopalakrishnan",
      "Kazuki Irie",
      "Jürgen Schmidhuber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2460396f2d0d421885997dd1612ac56b-Abstract-Conference.html": {
    "title": "Non-adversarial training of Neural SDEs with signature kernel scores",
    "volume": "main",
    "abstract": "Neural SDEs are continuous-time generative models for sequential data. State-of-the-art performance for irregular time series generation has been previously obtained by training these models adversarially as GANs. However, as typical for GAN architectures, training is notoriously unstable, often suffers from mode collapse, and requires specialised techniques such as weight clipping and gradient penalty to mitigate these issues. In this paper, we introduce a novel class of scoring rules on pathspace based on signature kernels and use them as objective for training Neural SDEs non-adversarially. By showing strict properness of such kernel scores and consistency of the corresponding estimators, we provide existence and uniqueness guarantees for the minimiser. With this formulation, evaluating the generator-discriminator pair amounts to solving a system of linear path-dependent PDEs which allows for memory-efficient adjoint-based backpropagation. Moreover, because the proposed kernel scores are well-defined for paths with values in infinite dimensional spaces of functions, our framework can be easily extended to generate spatiotemporal data. Our procedure significantly outperforms alternative ways of training Neural SDEs on a variety of tasks including the simulation of rough volatility models, the conditional probabilistic forecasts of real-world forex pairs where the conditioning variable is an observed past trajectory, and the mesh-free generation of limit order book dynamics",
    "keywords": [],
    "checked": true,
    "id": "48443928246a4bcf21f50a0e7d324ecacf12a637",
    "semantic_title": "non-adversarial training of neural sdes with signature kernel scores",
    "citation_count": 5,
    "authors": [
      "Zacharia Issa",
      "Blanka Horvath",
      "Maud Lemercier",
      "Cristopher Salvi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2468f84a13ff8bb6767a67518fb596eb-Abstract-Conference.html": {
    "title": "Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Text-to-Image diffusion models have made tremendous progress over the past two years, enabling the generation of highly realistic images based on open-domain text descriptions. However, despite their success, text descriptions often struggle to adequately convey detailed controls, even when composed of long and complex texts. Moreover, recent studies have also shown that these models face challenges in understanding such complex texts and generating the corresponding images. Therefore, there is a growing need to enable more control modes beyond text description. In this paper, we introduce Uni-ControlNet, a unified framework that allows for the simultaneous utilization of different local controls (e.g., edge maps, depth map, segmentation masks) and global controls (e.g., CLIP image embeddings) in a flexible and composable manner within one single model. Unlike existing methods, Uni-ControlNet only requires the fine-tuning of two additional adapters upon frozen pre-trained text-to-image diffusion models, eliminating the huge cost of training from scratch. Moreover, thanks to some dedicated adapter designs, Uni-ControlNet only necessitates a constant number (i.e., 2) of adapters, regardless of the number of local or global controls used. This not only reduces the fine-tuning costs and model size, making it more suitable for real-world deployment, but also facilitate composability of different conditions. Through both quantitative and qualitative comparisons, Uni-ControlNet demonstrates its superiority over existing methods in terms of controllability, generation quality and composability. Code is available at https://github.com/ShihaoZhaoZSH/Uni-ControlNet",
    "keywords": [],
    "checked": true,
    "id": "a483ff9557f29d21fe780b3dd969a037a3ffc3ed",
    "semantic_title": "uni-controlnet: all-in-one control to text-to-image diffusion models",
    "citation_count": 41,
    "authors": [
      "Shihao Zhao",
      "Dongdong Chen",
      "Yen-Chun Chen",
      "Jianmin Bao",
      "Shaozhe Hao",
      "Lu Yuan",
      "Kwan-Yee K. Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/249f73e01f0a2bb6c8d971b565f159a7-Abstract-Conference.html": {
    "title": "Loss Decoupling for Task-Agnostic Continual Learning",
    "volume": "main",
    "abstract": "Continual learning requires the model to learn multiple tasks in a sequential order. To perform continual learning, the model must possess the abilities to maintain performance on old tasks (stability) and adapt itself to learn new tasks (plasticity). Task-agnostic problem in continual learning is a challenging problem, in which task identities are not available in the inference stage and hence the model must learn to distinguish all the classes in all the tasks. In task-agnostic problem, the model needs to learn two new objectives for learning a new task, including distinguishing new classes from old classes and distinguishing between different new classes. For task-agnostic problem, replay-based methods are commonly used. These methods update the model with both saved old samples and new samples for continual learning. Most existing replay-based methods mix the two objectives in task-agnostic problem together, inhibiting the models from achieving a good trade-off between stability and plasticity. In this paper, we propose a simple yet effective method, called loss decoupling (LODE), for task-agnostic continual learning. LODE separates the two objectives for the new task by decoupling the loss of the new task. As a result, LODE can assign different weights for different objectives, which provides a way to obtain a better trade-off between stability and plasticity than those methods with coupled loss. Experiments show that LODE can outperform existing state-of-the-art replay-based methods on multiple continual learning datasets",
    "keywords": [],
    "checked": true,
    "id": "743bf92524f76456261ed4e764093c3b8f4d21e9",
    "semantic_title": "loss decoupling for task-agnostic continual learning",
    "citation_count": 2,
    "authors": [
      "Yan-Shuo Liang",
      "Wu-Jun Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/24a8d40f6656e542f3fd43bac678e71b-Abstract-Conference.html": {
    "title": "Federated Learning via Meta-Variational Dropout",
    "volume": "main",
    "abstract": "Federated Learning (FL) aims to train a global inference model from remotely distributed clients, gaining popularity due to its benefit of improving data privacy. However, traditional FL often faces challenges in practical applications, including model overfitting and divergent local models due to limited and non-IID data among clients. To address these issues, we introduce a novel Bayesian meta-learning approach called meta-variational dropout (MetaVD). MetaVD learns to predict client-dependent dropout rates via a shared hypernetwork, enabling effective model personalization of FL algorithms in limited non-IID data settings. We also emphasize the posterior adaptation view of meta-learning and the posterior aggregation view of Bayesian FL via the conditional dropout posterior. We conducted extensive experiments on various sparse and non-IID FL datasets. MetaVD demonstrated excellent classification accuracy and uncertainty calibration performance, especially for out-of-distribution (OOD) clients. MetaVD compresses the local model parameters needed for each client, mitigating model overfitting and reducing communication costs. Code is available at https://github.com/insujeon/MetaVD",
    "keywords": [],
    "checked": false,
    "id": "14f9cd6decba01a102788b7911f7bc0bc369405a",
    "semantic_title": "personalized federated learning via variational bayesian inference",
    "citation_count": 46,
    "authors": [
      "Insu Jeon",
      "Minui Hong",
      "Junhyeog Yun",
      "Gunhee Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/24cb8b08f3cb2f59671e33faac4790e6-Abstract-Conference.html": {
    "title": "Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space",
    "volume": "main",
    "abstract": "Hyperbolic spaces have been quite popular in the recent past for representing hierarchically organized data. Further, several classification algorithms for data in these spaces have been proposed in the literature. These algorithms mainly use either hyperplanes or geodesics for decision boundaries in a large margin classifiers setting leading to a non-convex optimization problem. In this paper, we propose a novel large margin classifier based on horospherical decision boundaries that leads to a geodesically convex optimization problem that can be optimized using any Riemannian gradient descent technique guaranteeing a globally optimal solution. We present several experiments depicting the competitive performance of our classifier in comparison to SOTA",
    "keywords": [],
    "checked": true,
    "id": "a3d1c852ce557a7167624a69021f9f8093f8f3c1",
    "semantic_title": "horospherical decision boundaries for large margin classification in hyperbolic space",
    "citation_count": 1,
    "authors": [
      "Xiran Fan",
      "Chun-Hao Yang",
      "Baba Vemuri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/24d36eee157559e0d2549455fba28f6a-Abstract-Conference.html": {
    "title": "MIM4DD: Mutual Information Maximization for Dataset Distillation",
    "volume": "main",
    "abstract": "Dataset distillation (DD) aims to synthesize a small dataset whose test performance is comparable to a full dataset using the same model. State-of-the-art (SoTA) methods optimize synthetic datasets primarily by matching heuristic indicators extracted from two networks: one from real data and one from synthetic data (see Fig.1, Left), such as gradients and training trajectories. DD is essentially a compression problem that emphasizes on maximizing the preservation of information contained in the data. We argue that well-defined metrics which measure the amount of shared information between variables in information theory are necessary for success measurement, but are never considered by previous works. Thus, we introduce mutual information (MI) as the metric to quantify the shared information between the synthetic and the real datasets, and devise MIM4DD numerically maximizing the MI via a newly designed optimizable objective within a contrastive learning framework to update the synthetic dataset. Specifically, we designate the samples in different datasets who share the same labels as positive pairs, and vice versa negative pairs. Then we respectively pull and push those samples in positive and negative pairs into contrastive space via minimizing NCE loss. As a result, the targeted MI can be transformed into a lower bound represented by feature maps of samples, which is numerically feasible. Experiment results show that MIM4DD can be implemented as an add-on module to existing SoTA DD methods",
    "keywords": [],
    "checked": true,
    "id": "3e992179e3ca0cd2cfeac1ce2fc47f93729b667e",
    "semantic_title": "mim4dd: mutual information maximization for dataset distillation",
    "citation_count": 0,
    "authors": [
      "Yuzhang Shang",
      "Zhihang Yuan",
      "Yan Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/24f8dd1b8f154f1ee0d7a59e368eccf3-Abstract-Conference.html": {
    "title": "Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "In various engineering and applied science applications, repetitive numerical simulations of partial differential equations (PDEs) for varying input parameters are often required (e.g., aircraft shape optimization over many design parameters) and solvers are required to perform rapid execution. In this study, we suggest a path that potentially opens up a possibility for physics-informed neural networks (PINNs), emerging deep-learning-based solvers, to be considered as one such solver. Although PINNs have pioneered a proper integration of deep-learning and scientific computing, they require repetitive time-consuming training of neural networks, which is not suitable for many-query scenarios. To address this issue, we propose a lightweight low-rank PINNs containing only hundreds of model parameters and an associated hypernetwork-based meta-learning algorithm, which allows efficient approximation of solutions of PDEs for varying ranges of PDE input parameters. Moreover, we show that the proposed method is effective in overcoming a challenging issue, known as \"failure modes\" of PINNs",
    "keywords": [],
    "checked": true,
    "id": "8f864b48afdabfd2f88dcc1c35be60136ec4f172",
    "semantic_title": "hypernetwork-based meta-learning for low-rank physics-informed neural networks",
    "citation_count": 1,
    "authors": [
      "Woojin Cho",
      "Kookjin Lee",
      "Donsub Rim",
      "Noseong Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/254009e8d528f98764a060e877a1b01c-Abstract-Conference.html": {
    "title": "Solving a Class of Non-Convex Minimax Optimization in Federated Learning",
    "volume": "main",
    "abstract": "The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization. To address the large-scale distributed data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity. Many optimization algorithms for minimax problems have been developed in the centralized setting (\\emph{i.e.}, single-machine). Nonetheless, the algorithm for minimax problems under FL is still underexplored. In this paper, we study a class of federated nonconvex minimax optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems. For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample complexity of $O(\\kappa^{3} N^{-1}\\varepsilon^{-3})$ and the best-known communication complexity of $O(\\kappa^{2}\\varepsilon^{-2})$. FedSGDA-M is the first algorithm to match the best sample complexity $O(\\varepsilon^{-3})$ achieved by the single-machine method under the nonconvex-strongly-concave setting. Extensive experimental results on fair classification and AUROC maximization show the efficiency of our algorithms",
    "keywords": [],
    "checked": true,
    "id": "3aad0cdabefb30046ba1c95671abf5bd16ba1d16",
    "semantic_title": "solving a class of non-convex minimax optimization in federated learning",
    "citation_count": 5,
    "authors": [
      "Xidong Wu",
      "Jianhui Sun",
      "Zhengmian Hu",
      "Aidong Zhang",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2561721d0ca69bab22b749cfc4f48f6c-Abstract-Conference.html": {
    "title": "End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes",
    "volume": "main",
    "abstract": "Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL objective with an auxiliary task that guides part of the architecture to learn a valid probabilistic model as an inductive bias. We demonstrate that our method achieves state-of-the-art regret results against various baselines in experiments on standard hyperparameter optimisation tasks and also outperforms others in the real-world problems of mixed-integer programming tuning, antibody design, and logic synthesis for electronic design automation",
    "keywords": [],
    "checked": true,
    "id": "8ef3f06353f65ca653b396b70d089f793bfc168b",
    "semantic_title": "end-to-end meta-bayesian optimisation with transformer neural processes",
    "citation_count": 2,
    "authors": [
      "Alexandre Maraval",
      "Matthieu Zimmer",
      "Antoine Grosnit",
      "Haitham Bou Ammar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2567c95fd41459a98a73ba893775d22a-Abstract-Conference.html": {
    "title": "Contextual Bandits and Imitation Learning with Preference-Based Active Queries",
    "volume": "main",
    "abstract": "We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\\min\\\\{\\sqrt{T}, d/\\Delta\\\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\\min\\\\{T, d^2/\\Delta^2\\\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals",
    "keywords": [],
    "checked": false,
    "id": "18a387c83289267c79ba4adf6af17c88b73d5180",
    "semantic_title": "contextual bandits and imitation learning via preference-based active queries",
    "citation_count": 4,
    "authors": [
      "Ayush Sekhari",
      "Karthik Sridharan",
      "Wen Sun",
      "Runzhe Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/257b3a7438b1f3709e91a86adf2fdc0a-Abstract-Conference.html": {
    "title": "Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding",
    "volume": "main",
    "abstract": "Spectral embedding is a powerful graph embedding technique that has received a lot of attention recently due to its effectiveness on Graph Transformers. However, from a theoretical perspective, the universal expressive power of spectral embedding comes at the price of losing two important invariance properties of graphs, sign and basis invariance, which also limits its effectiveness on graph data. To remedy this issue, many previous methods developed costly approaches to learn new invariants and suffer from high computation complexity. In this work, we explore a minimal approach that resolves the ambiguity issues by directly finding canonical directions for the eigenvectors, named Laplacian Canonization (LC). As a pure pre-processing method, LC is light-weighted and can be applied to any existing GNNs. We provide a thorough investigation, from theory to algorithm, on this approach, and discover an efficient algorithm named Maximal Axis Projection (MAP) that works for both sign and basis invariance and successfully canonizes more than 90\\% of all eigenvectors. Experiments on real-world benchmark datasets like ZINC, MOLTOX21, and MOLPCBA show that MAP consistently outperforms existing methods while bringing minimal computation overhead. Code is available at \\url{https://github.com/PKU-ML/LaplacianCanonization}",
    "keywords": [],
    "checked": true,
    "id": "d23c3df67a2e9c78160cadfab7f6bc8ee98a045e",
    "semantic_title": "laplacian canonization: a minimalist approach to sign and basis invariant spectral embedding",
    "citation_count": 1,
    "authors": [
      "George Ma",
      "Yifei Wang",
      "Yisen  Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/257be12f31dfa7cc158dda99822c6fd1-Abstract-Conference.html": {
    "title": "Localized Symbolic Knowledge Distillation for Visual Commonsense Models",
    "volume": "main",
    "abstract": "Instruction following vision-language (VL) models offer a flexibleinterface that supports a broad range of multimodal tasks in a zero-shot fashion.However, interfaces that operate on full images do not directly enable the user to\"point to\" and access specific regions within images. This capability is importantnot only to support reference-grounded VL benchmarks, but also, for practicalapplications that require precise within-image reasoning. We build LocalizedVisual Commonsense model which allows users to specify (multiple) regions-as-input. We train our model by sampling localized commonsense knowledgefrom a large language model (LLM): specifically, we prompt a LLM to collectcommonsense knowledge given a global literal image description and a localliteral region description automatically generated by a set of VL models. Thispipeline is scalable and fully automatic, as no aligned or human-authored imageand text pairs are required. With a separately trained critic model that selectshigh quality examples, we find that training on the localized commonsense corpusexpanded solely from images can successfully distill existing VL models to supporta reference-as-input interface. Empirical results and human evaluations in zero-shotsettings demonstrate that our distillation method results in more precise VL modelsof reasoning compared to a baseline of passing a generated referring expression",
    "keywords": [],
    "checked": true,
    "id": "5c17fa02a4a4c0655b1873cf3e34fffbc1e7d601",
    "semantic_title": "localized symbolic knowledge distillation for visual commonsense models",
    "citation_count": 1,
    "authors": [
      "Jae Sung Park",
      "Jack Hessel",
      "Khyathi Chandu",
      "Paul Pu Liang",
      "Ximing Lu",
      "Peter West",
      "Youngjae Yu",
      "Qiuyuan Huang",
      "Jianfeng Gao",
      "Ali Farhadi",
      "Yejin Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/25823c8eadef751dbd09a0ab9f463b59-Abstract-Conference.html": {
    "title": "SmooSeg: Smoothness Prior for Unsupervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Unsupervised semantic segmentation is a challenging task that segments images into semantic groups without manual annotation. Prior works have primarily focused on leveraging prior knowledge of semantic consistency or priori concepts from self-supervised learning methods, which often overlook the coherence property of image segments. In this paper, we demonstrate that the smoothness prior, asserting that close features in a metric space share the same semantics, can significantly simplify segmentation by casting unsupervised semantic segmentation as an energy minimization problem. Under this paradigm, we propose a novel approach called SmooSeg that harnesses self-supervised learning methods to model the closeness relationships among observations as smoothness signals. To effectively discover coherent semantic segments, we introduce a novel smoothness loss that promotes piecewise smoothness within segments while preserving discontinuities across different segments. Additionally, to further enhance segmentation quality, we design an asymmetric teacher-student style predictor that generates smoothly updated pseudo labels, facilitating an optimal fit between observations and labeling outputs. Thanks to the rich supervision cues of the smoothness prior, our SmooSeg significantly outperforms STEGO in terms of pixel accuracy on three datasets: COCOStuff (+14.9\\%), Cityscapes (+13.0\\%), and Potsdam-3 (+5.7\\%)",
    "keywords": [],
    "checked": true,
    "id": "e9ae3660a8d4d8ef42d67594240aca199a18ad12",
    "semantic_title": "smooseg: smoothness prior for unsupervised semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Mengcheng Lan",
      "Xinjiang Wang",
      "Yiping Ke",
      "Jiaxing Xu",
      "Litong Feng",
      "Wayne Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/259e59fe23ebd09252647fed42949182-Abstract-Conference.html": {
    "title": "Fast Trainable Projection for Robust Fine-tuning",
    "volume": "main",
    "abstract": "Robust fine-tuning aims to achieve competitive in-distribution (ID) performance while maintaining the out-of-distribution (OOD) robustness of a pre-trained model when transferring it to a downstream task. Recently, projected gradient descent has been successfully used in robust fine-tuning by constraining the deviation from the initialization of the fine-tuned model explicitly through projection. However, algorithmically, two limitations prevent this method from being adopted more widely, scalability and efficiency. In this paper, we propose a new projection-based fine-tuning algorithm, Fast Trainable Projection (FTP) for computationally efficient learning of per-layer projection constraints, resulting in an average 35% speedup on our benchmarks compared to prior works. FTP can be combined with existing optimizers such as AdamW, and be used in a plug-and-play fashion. Finally, we show that FTP is a special instance of hyper-optimizers that tune the hyper-parameters of optimizers in a learnable manner through nested differentiation. Empirically, we show superior robustness on OOD datasets, including domain shifts and natural corruptions, across four different vision tasks with five different pre-trained models. Additionally, we demonstrate that FTP is broadly applicable and beneficial to other learning scenarios such as low-label and continual learning settings thanks to its easy adaptability. The code will be available at https://github.com/GT-RIPL/FTP.git",
    "keywords": [],
    "checked": true,
    "id": "24cd9b362d6b5ae24008eea0744f5bf32f468865",
    "semantic_title": "fast trainable projection for robust fine-tuning",
    "citation_count": 1,
    "authors": [
      "Junjiao Tian",
      "Yen-Cheng Liu",
      "James S Smith",
      "Zsolt Kira"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/25b15618c98ff0c4655df0c5a277e1c6-Abstract-Conference.html": {
    "title": "Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation",
    "volume": "main",
    "abstract": "In applying reinforcement learning (RL) to high-stakes domains, quantitative and qualitative evaluation using observational data can help practitioners understand the generalization performance of new policies. However, this type of off-policy evaluation (OPE) is inherently limited since offline data may not reflect the distribution shifts resulting from the application of new policies. On the other hand, online evaluation by collecting rollouts according to the new policy is often infeasible, as deploying new policies in these domains can be unsafe. In this work, we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories. While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results. Instead, we design a new family of OPE estimators based on importance sampling (IS) and a novel weighting scheme that incorporate counterfactual annotations without introducing additional bias. We analyze the theoretical properties of our approach, showing its potential to reduce both bias and variance compared to standard IS estimators. Our analyses reveal important practical considerations for handling biased, noisy, or missing annotations. In a series of proof-of-concept experiments involving bandits and a healthcare-inspired simulator, we demonstrate that our approach outperforms purely offline IS estimators and is robust to imperfect annotations. Our framework, combined with principled human-centered design of annotation solicitation, can enable the application of RL in high-stakes domains",
    "keywords": [],
    "checked": true,
    "id": "f81f9b378677d78f57bb952c301da29f6cfcf65c",
    "semantic_title": "counterfactual-augmented importance sampling for semi-offline policy evaluation",
    "citation_count": 0,
    "authors": [
      "Shengpu Tang",
      "Jenna Wiens"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/25d463c05b414125f598cdf8022b3b46-Abstract-Conference.html": {
    "title": "Are GATs Out of Balance?",
    "volume": "main",
    "abstract": "While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms",
    "keywords": [],
    "checked": true,
    "id": "79ec215eb5d792e5e74dd9801dfc7d5e2aad5724",
    "semantic_title": "are gats out of balance?",
    "citation_count": 0,
    "authors": [
      "Nimrah Mustafa",
      "Aleksandar Bojchevski",
      "Rebekka Burkholz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/26300457961c3e056ea61c9d3ebec2a4-Abstract-Conference.html": {
    "title": "Fast Asymptotically Optimal Algorithms for Non-Parametric Stochastic Bandits",
    "volume": "main",
    "abstract": "We consider the problem of regret minimization in non-parametric stochastic bandits. When the rewards are known to be bounded from above, there exists asymptotically optimal algorithms, with asymptotic regret depending on an infimum of Kullback-Leibler divergences (KL). These algorithms are computationally expensive and require storing all past rewards, thus simpler but non-optimal algorithms are often used instead. We introduce several methods to approximate the infimum KL which reduce drastically the computational and memory costs of existing optimal algorithms, while keeping their regret guaranties. We apply our findings to design new variants of the MED and IMED algorithms, and demonstrate their interest with extensive numerical simulations",
    "keywords": [],
    "checked": false,
    "id": "4e240c739148a0dcf286dce5604f08247294347c",
    "semantic_title": "finite-time analysis of whittle index based q-learning for restless multi-armed bandits with neural network function approximation",
    "citation_count": 3,
    "authors": [
      "Dorian Baudry",
      "Fabien Pesquerel",
      "Rémy Degenne",
      "Odalric-Ambrym Maillard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/264f2e10479c9370972847e96107db7f-Abstract-Conference.html": {
    "title": "SHAP-IQ: Unified Approximation of any-order Shapley Interactions",
    "volume": "main",
    "abstract": "Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature attributions for any black box model. Shapley interaction indices extend the SV to define any-order feature interactions. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our approach reveals a novel representation of the SV and corresponds to Unbiased KernelSHAP with a greatly simplified calculation. We illustrate the computational efficiency and effectiveness by explaining language, image classification and high-dimensional synthetic models",
    "keywords": [],
    "checked": true,
    "id": "a7f2d57b18a93577b86f687794ff8b32662514c3",
    "semantic_title": "shap-iq: unified approximation of any-order shapley interactions",
    "citation_count": 6,
    "authors": [
      "Fabian Fumagalli",
      "Maximilian Muschalik",
      "Patrick Kolpaczki",
      "Eyke Hüllermeier",
      "Barbara Hammer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/265bee74aee86df77e8e36d25e786ab5-Abstract-Conference.html": {
    "title": "Towards Last-layer Retraining for Group Robustness with Fewer Annotations",
    "volume": "main",
    "abstract": "Empirical risk minimization (ERM) of neural networks is prone to over-reliance on spurious correlations and poor generalization on minority groups. The recent deep feature reweighting (DFR) technique achieves state-of-the-art group robustness via simple last-layer retraining, but it requires held-out group and class annotations to construct a group-balanced reweighting dataset. In this work, we examine this impractical requirement and find that last-layer retraining can be surprisingly effective with no group annotations (other than for model selection) and only a handful of class annotations. We first show that last-layer retraining can greatly improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data. This implies a \"free lunch\" where holding out a subset of training data to retrain the last layer can substantially outperform ERM on the entire dataset with no additional data, annotations, or computation for training. To further improve group robustness, we introduce a lightweight method called selective last-layer finetuning (SELF), which constructs the reweighting dataset using misclassifications or disagreements. Our experiments present the first evidence that model disagreement upsamples worst-group data, enabling SELF to nearly match DFR on four well-established benchmarks across vision and language tasks with no group annotations and less than 3% of the held-out class annotations",
    "keywords": [],
    "checked": true,
    "id": "2d14697232f03661cb86246df46e52816694a97f",
    "semantic_title": "towards last-layer retraining for group robustness with fewer annotations",
    "citation_count": 6,
    "authors": [
      "Tyler LaBonte",
      "Vidya Muthukumar",
      "Abhishek Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/26c233f48fb05bbd52a520e4bb9e3760-Abstract-Conference.html": {
    "title": "Analysis of Variance of Multiple Causal Networks",
    "volume": "main",
    "abstract": "Constructing a directed cyclic graph (DCG) is challenged by both algorithmic difficulty and computational burden. Comparing multiple DCGs is even more difficult, compounded by the need of identifying variational causalities across graphs. We propose to unify multiple DCGs with a single structural model and develop a limited-information-based method to simultaneously construct multiple networks and infer their disparities, which can be visualized by appropriate correspondence analysis. The algorithm provides DCGs with robust non-asymptotic theoretical properties. It is designed with two sequential stages, each of which involves parallel computation tasks that are scalable to the network complexity. Taking advantage of high-performance clusters, our method makes it possible to evaluate the statistical significance of DCGs using the bootstrap method. We demonstrated the effectiveness of our method by applying it to synthetic and real datasets",
    "keywords": [],
    "checked": false,
    "id": "695c3bf981a5d1d2fe60db9967acb239a03bedcb",
    "semantic_title": "only relative speed matters",
    "citation_count": 0,
    "authors": [
      "Zhongli Jiang",
      "Dabao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/26cce1e512793f2072fd27c391e04652-Abstract-Conference.html": {
    "title": "Revisiting the Minimalist Approach to Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Recent years have witnessed significant advancements in offline reinforcement learning (RL), resulting in the development of numerous algorithms with varying degrees of complexity. While these algorithms have led to noteworthy improvements, many incorporate seemingly minor design choices that impact their effectiveness beyond core algorithmic advances. However, the effect of these design choices on established baselines remains understudied. In this work, we aim to bridge this gap by conducting a retrospective analysis of recent works in offline RL and propose ReBRAC, a minimalistic algorithm that integrates such design elements built on top of the TD3+BC method. We evaluate ReBRAC on 51 datasets with both proprioceptive and visual state spaces using D4RL and V-D4RL benchmarks, demonstrating its state-of-the-art performance among ensemble-free methods in both offline and offline-to-online settings. To further illustrate the efficacy of these design choices, we perform a large-scale ablation study and hyperparameter sensitivity analysis on the scale of thousands of experiments",
    "keywords": [],
    "checked": true,
    "id": "1b6341b05471bb961b4e76467444d7b70c66bbb5",
    "semantic_title": "revisiting the minimalist approach to offline reinforcement learning",
    "citation_count": 6,
    "authors": [
      "Denis Tarasov",
      "Vladislav Kurenkov",
      "Alexander Nikulin",
      "Sergey Kolesnikov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/26f96550613971371c5d07f37f0e06c0-Abstract-Conference.html": {
    "title": "Complementary Benefits of Contrastive Learning and Self-Training Under Distribution Shift",
    "volume": "main",
    "abstract": "Self-training and contrastive learning have emerged as leading techniques for incorporating unlabeled data, both under distribution shift (unsupervised domain adaptation) and when it is absent (semi-supervised learning). However, despite the popularity and compatibility of these techniques, their efficacy in combination remains surprisingly unexplored. In this paper, we first undertake a systematic empirical investigation of this combination, finding (i) that in domain adaptation settings, self-training and contrastive learning offer significant complementary gains; and (ii) that in semi-supervised learning settings, surprisingly, the benefits are not synergistic. Across eight distribution shift datasets (e.g., BREEDs, WILDS), we demonstrate that the combined method obtains 3--8\\% higher accuracy than either approach independently. Finally, we theoretically analyze these techniques in a simplified model of distribution shift demonstrating scenarios under which the features produced by contrastive learning can yield a good initialization for self-training to further amplify gains and achieve optimal performance, even when either method alone would fail",
    "keywords": [],
    "checked": true,
    "id": "9dfa8eec291b3771b0dc1dd5c821a0a70528d23b",
    "semantic_title": "complementary benefits of contrastive learning and self-training under distribution shift",
    "citation_count": 1,
    "authors": [
      "Saurabh Garg",
      "Amrith Setlur",
      "Zachary Lipton",
      "Sivaraman Balakrishnan",
      "Virginia Smith",
      "Aditi Raghunathan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/27030ad2ec1d8f2c3847a64e382c30ca-Abstract-Conference.html": {
    "title": "Low Tensor Rank Learning of Neural Dynamics",
    "volume": "main",
    "abstract": "Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task. Finally, we present a set of mathematical results bounding the matrix and tensor ranks of gradient descent learning dynamics which show that low-tensor-rank weights emerge naturally in RNNs trained to solve low-dimensional tasks. Taken together, our findings provide insight on the evolution of population connectivity over learning in both biological and artificial neural networks, and enable reverse engineering of learning-induced changes in recurrent dynamics from large-scale neural recordings",
    "keywords": [],
    "checked": true,
    "id": "49d720873b5f793a13564a5777ab88152febb9f1",
    "semantic_title": "low tensor rank learning of neural dynamics",
    "citation_count": 1,
    "authors": [
      "Arthur Pellegrino",
      "N Alex Cayco Gajic",
      "Angus Chadwick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2703a0e3c2b33506295a77762338cf24-Abstract-Conference.html": {
    "title": "MonoUNI: A Unified Vehicle and Infrastructure-side Monocular 3D Object Detection Network with Sufficient Depth Clues",
    "volume": "main",
    "abstract": "Monocular 3D detection of vehicle and infrastructure sides are two important topics in autonomous driving. Due to diverse sensor installations and focal lengths, researchers are faced with the challenge of constructing algorithms for the two topics based on different prior knowledge. In this paper, by taking into account the diversity of pitch angles and focal lengths, we propose a unified optimization target named normalized depth, which realizes the unification of 3D detection problems for the two sides. Furthermore, to enhance the accuracy of monocular 3D detection, 3D normalized cube depth of obstacle is developed to promote the learning of depth information. We posit that the richness of depth clues is a pivotal factor impacting the detection performance on both the vehicle and infrastructure sides. A richer set of depth clues facilitates the model to learn better spatial knowledge, and the 3D normalized cube depth offers sufficient depth clues. Extensive experiments demonstrate the effectiveness of our approach. Without introducing any extra information, our method, named MonoUNI, achieves state-of-the-art performance on five widely used monocular 3D detection benchmarks, including Rope3D and DAIR-V2X-I for the infrastructure side, KITTI and Waymo for the vehicle side, and nuScenes for the cross-dataset evaluation",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Jinrang",
      "Zhenjia Li",
      "Yifeng Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2712b17bb58ea5b2b65c45857b024744-Abstract-Conference.html": {
    "title": "Active Reasoning in an Open-World Environment",
    "volume": "main",
    "abstract": "Recent advances in vision-language learning have achieved notable success on complete-information question-answering datasets through the integration of extensive world knowledge. Yet, most models operate passively, responding to questions based on pre-stored knowledge. In stark contrast, humans possess the ability to actively explore, accumulate, and reason using both newfound and existing information to tackle incomplete-information questions. In response to this gap, we introduce Conan, an interactive open-world environment devised for the assessment of active reasoning. Conan facilitates active exploration and promotes multi-round abductive inference, reminiscent of rich, open-world settings like Minecraft. Diverging from previous works that lean primarily on single-round deduction via instruction following, Conan compels agents to actively interact with their surroundings, amalgamating new evidence with prior knowledge to elucidate events from incomplete observations. Our analysis on \\bench underscores the shortcomings of contemporary state-of-the-art models in active exploration and understanding complex scenarios. Additionally, we explore Abduction from Deduction, where agents harness Bayesian rules to recast the challenge of abduction as a deductive process. Through Conan, we aim to galvanize advancements in active reasoning and set the stage for the next generation of artificial intelligence agents adept at dynamically engaging in environments",
    "keywords": [],
    "checked": true,
    "id": "8b1f2d0ff672a9ce920586f9291a18ca9fb3a20f",
    "semantic_title": "active reasoning in an open-world environment",
    "citation_count": 1,
    "authors": [
      "Manjie Xu",
      "Guangyuan Jiang",
      "Wei Liang",
      "Chi Zhang",
      "Yixin Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2717ad172c5495837582d70a8519abfb-Abstract-Conference.html": {
    "title": "2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression",
    "volume": "main",
    "abstract": "We consider distributed convex optimization problems in the regime when the communication between the server and the workers is expensive in both uplink and downlink directions. We develop a new and provably accelerated method, which we call 2Direction, based on fast bidirectional compressed communication and a new bespoke error-feedback mechanism which may be of independent interest. Indeed, we find that the EF and EF21-P mechanisms (Seide et al., 2014; Gruntkowska et al., 2023) that have considerable success in the design of efficient non-accelerated methods are not appropriate for accelerated methods. In particular, we prove that 2Direction improves the previous state-of-the-art communication complexity $\\widetilde{\\Theta}\\left(K \\times \\left(\\frac{L}{\\alpha \\mu} + \\frac{L_{\\max} \\omega}{n \\mu} + \\omega\\right)\\right)$ (Gruntkowska et al., 2023) to $\\widetilde{\\Theta}(K \\times (\\sqrt{\\frac{L (\\omega + 1)}{\\alpha \\mu}} + \\sqrt{\\frac{L_{\\max} \\omega^2}{n \\mu}} + \\frac{1}{\\alpha} + \\omega))$ in the $\\mu$--strongly-convex setting, where $L$ and $L_{\\max}$ are smoothness constants, $n$ is \\# of workers, $\\omega$ and $\\alpha$ are compression errors of the Rand$K$ and Top$K$ sparsifiers (as examples), $K$ is \\# of coordinates/bits that the server and workers send to each other. Moreover, our method is the first that improves upon the communication complexity of the vanilla accelerated gradient descent method (AGD). We obtain similar improvements in the general convex regime as well. Finally, our theoretical findings are corroborated by experimental evidence",
    "keywords": [],
    "checked": true,
    "id": "96b3e56c1cd8a369118573eeb62fb3c8bed6a464",
    "semantic_title": "2direction: theoretically faster distributed training with bidirectional communication compression",
    "citation_count": 1,
    "authors": [
      "Alexander Tyurin",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html": {
    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
    "volume": "main",
    "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\\% of tasks, our method achieved a success rate of 74\\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm",
    "keywords": [],
    "checked": true,
    "id": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
    "semantic_title": "tree of thoughts: deliberate problem solving with large language models",
    "citation_count": 444,
    "authors": [
      "Shunyu Yao",
      "Dian Yu",
      "Jeffrey Zhao",
      "Izhak Shafran",
      "Tom Griffiths",
      "Yuan Cao",
      "Karthik Narasimhan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/271ec4d1a9ff5e6b81a6e21d38b1ba96-Abstract-Conference.html": {
    "title": "What functions can Graph Neural Networks compute on random graphs? The role of Positional Encoding",
    "volume": "main",
    "abstract": "We aim to deepen the theoretical understanding of Graph Neural Networks (GNNs) on large graphs, with a focus on their expressive power.Existing analyses relate this notion to the graph isomorphism problem, which is mostly relevant for graphs of small sizes, or studied graph classification or regression tasks, while prediction tasks on \\emph{nodes} are far more relevant on large graphs. Recently, several works showed that, on very general random graphs models, GNNs converge to certains functions as the number of nodes grows.In this paper, we provide a more complete and intuitive description of the function space generated by equivariant GNNs for node-tasks, through general notions of convergence that encompass several previous examples. We emphasize the role of input node features, and study the impact of \\emph{node Positional Encodings} (PEs), a recent line of work that has been shown to yield state-of-the-art results in practice. Through the study of several examples of PEs on large random graphs, we extend previously known universality results to significantly more general models. Our theoretical results hint at some normalization tricks, which is shown numerically to have a positive impact on GNN generalization on synthetic and real data. Our proofs contain new concentration inequalities of independent interest",
    "keywords": [],
    "checked": true,
    "id": "a793c5a5125b2fedbd251ff5b964def24345916c",
    "semantic_title": "what functions can graph neural networks compute on random graphs? the role of positional encoding",
    "citation_count": 3,
    "authors": [
      "Nicolas Keriven",
      "Samuel Vaiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2722a0ccf6acfe3d144fdbb0dedd80b5-Abstract-Conference.html": {
    "title": "High-dimensional Asymptotics of Denoising Autoencoders",
    "volume": "main",
    "abstract": "We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear autoencoder with tied weights and a skip connection. We consider the high-dimensional limit where the number of training samples and the input dimension jointly tend to infinity while the number of hidden units remains bounded. We provide closed-form expressions for the denoising mean-squared test error. Building on this result, we quantitatively characterize the advantage of the considered architecture over the autoencoder without the skip connection that relates closely to principal component analysis. We further show that our results capture accurately the learning curves on a range of real datasets",
    "keywords": [],
    "checked": true,
    "id": "6e09980963055bd98a9e0a27fb1e5a77213ca901",
    "semantic_title": "high-dimensional asymptotics of denoising autoencoders",
    "citation_count": 4,
    "authors": [
      "Hugo Cui",
      "Lenka Zdeborová"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/274d0146144643ee2459a602123c60ff-Abstract-Conference.html": {
    "title": "Learning to Reason and Memorize with Self-Notes",
    "volume": "main",
    "abstract": "Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text",
    "keywords": [],
    "checked": true,
    "id": "1db6836f61695e558608bb57166feca6876edabf",
    "semantic_title": "learning to reason and memorize with self-notes",
    "citation_count": 9,
    "authors": [
      "Jack Lanchantin",
      "Shubham Toshniwal",
      "Jason Weston",
      "arthur szlam",
      "Sainbayar Sukhbaatar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/274db6bf1b01d8b4f07feaeb8c46f474-Abstract-Conference.html": {
    "title": "What can a Single Attention Layer Learn? A Study Through the Random Features Lens",
    "volume": "main",
    "abstract": "Attention layers---which map a sequence of inputs to a sequence of outputs---are core building blocks of the Transformer architecture which has achieved significant breakthroughs in modern artificial intelligence. This paper presents a rigorous theoretical study on the learning and generalization of a single multi-head attention layer, with a sequence of key vectors and a separate query vector as input. We consider the random feature setting where the attention layer has a large number of heads, with randomly sampled frozen query and key matrices, and trainable value matrices. We show that such a random-feature attention layer can express a broad class of target functions that are permutation invariant to the key vectors. We further provide quantitative excess risk bounds for learning these target functions from finite samples, using random feature attention with finitely many heads.Our results feature several implications unique to the attention structure compared with existing random features theory for neural networks, such as (1) Advantages in the sample complexity over standard two-layer random-feature networks; (2) Concrete and natural classes of functions that can be learned efficiently by a random-feature attention layer; and (3) The effect of the sampling distribution of the query-key weight matrix (the product of the query and key matrix), where Gaussian random weights with a non-zero mean result in better sample complexities over the zero-mean counterpart for learning certain natural target functions. Experiments on simulated data corroborate our theoretical findings and further illustrate the interplay between the sample size and the complexity of the target function",
    "keywords": [],
    "checked": true,
    "id": "b978428a41d4051f0fd744845a4bce0523dbee85",
    "semantic_title": "what can a single attention layer learn? a study through the random features lens",
    "citation_count": 8,
    "authors": [
      "Hengyu Fu",
      "Tianyu Guo",
      "Yu Bai",
      "Song Mei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/27571b74d6cd650b8eb6cf1837953ae8-Abstract-Conference.html": {
    "title": "Let the Flows Tell: Solving Graph Combinatorial Problems with GFlowNets",
    "volume": "main",
    "abstract": "Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space.On the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates.In this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. Efficient training techniques are also developed to benefit long-range credit assignment.Through extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quality solutions.Our implementation is open-sourced at https://github.com/zdhNarsil/GFlowNet-CombOpt",
    "keywords": [],
    "checked": false,
    "id": "e3d0b3f6c085cdf254bf127adc039a0dc45977e6",
    "semantic_title": "let the flows tell: solving graph combinatorial optimization problems with gflownets",
    "citation_count": 13,
    "authors": [
      "Dinghuai Zhang",
      "Hanjun Dai",
      "Nikolay Malkin",
      "Aaron C. Courville",
      "Yoshua Bengio",
      "Ling Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/27725882a88f202e07319abbb3be7693-Abstract-Conference.html": {
    "title": "Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation",
    "volume": "main",
    "abstract": "Existing score-distilling text-to-3D generation techniques, despite their considerable promise, often encounter the view inconsistency problem. One of the most notable issues is the Janus problem, where the most canonical view of an object (\\textit{e.g}., face or head) appears in other views. In this work, we explore existing frameworks for score-distilling text-to-3D generation and identify the main causes of the view inconsistency problem---the embedded bias of 2D diffusion models. Based on these findings, we propose two approaches to debias the score-distillation frameworks for view-consistent text-to-3D generation. Our first approach, called score debiasing, involves cutting off the score estimated by 2D diffusion models and gradually increasing the truncation value throughout the optimization process. Our second approach, called prompt debiasing, identifies conflicting words between user prompts and view prompts using a language model, and adjusts the discrepancy between view prompts and the viewing direction of an object. Our experimental results show that our methods improve the realism of the generated 3D objects by significantly reducing artifacts and achieve a good trade-off between faithfulness to the 2D diffusion models and 3D consistency with little overhead. Our project page is available at~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}",
    "keywords": [],
    "checked": true,
    "id": "23e806b147afcc44690f0867c0597ebe792378e4",
    "semantic_title": "debiasing scores and prompts of 2d diffusion for view-consistent text-to-3d generation",
    "citation_count": 1,
    "authors": [
      "Susung Hong",
      "Donghoon Ahn",
      "Seungryong Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2786baf8091ee8ecb060580239967ba0-Abstract-Conference.html": {
    "title": "On the Learnability of Multilabel Ranking",
    "volume": "main",
    "abstract": "Multilabel ranking is a central task in machine learning. However, the most fundamental question of learnability in a multilabel ranking setting with relevance-score feedback remains unanswered. In this work, we characterize the learnability of multilabel ranking problems in both batch and online settings for a large family of ranking losses. Along the way, we give two equivalence classes of ranking losses based on learnability that capture most losses used in practice",
    "keywords": [],
    "checked": true,
    "id": "afeb4dc2bf54459e0001c253038456546d4c35db",
    "semantic_title": "on the learnability of multilabel ranking",
    "citation_count": 1,
    "authors": [
      "Vinod Raman",
      "UNIQUE SUBEDI",
      "Ambuj Tewari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2788b4cdf421e03650868cc4184bfed8-Abstract-Conference.html": {
    "title": "MAG-GNN: Reinforcement Learning Boosted Graph Neural Network",
    "volume": "main",
    "abstract": "While Graph Neural Networks (GNNs) recently became powerful tools in graph learning tasks, considerable efforts have been spent on improving GNNs' structural encoding ability. A particular line of work proposed subgraph GNNs that use subgraph information to improve GNNs' expressivity and achieved great success. However, such effectivity sacrifices the efficiency of GNNs by enumerating all possible subgraphs. In this paper, we analyze the necessity of complete subgraph enumeration and show that a model can achieve a comparable level of expressivity by considering a small subset of the subgraphs. We then formulate the identification of the optimal subset as a combinatorial optimization problem and propose Magnetic Graph Neural Network (MAG-GNN), a reinforcement learning (RL) boosted GNN, to solve the problem. Starting with a candidate subgraph set, MAG-GNN employs an RL agent to iteratively update the subgraphs to locate the most expressive set for prediction. This reduces the exponential complexity of subgraph enumeration to the constant complexity of a subgraph search algorithm while keeping good expressivity. We conduct extensive experiments on many datasets, showing that MAG-GNN achieves competitive performance to state-of-the-art methods and even outperforms many subgraph GNNs. We also demonstrate that MAG-GNN effectively reduces the running time of subgraph GNNs",
    "keywords": [],
    "checked": true,
    "id": "60953d9393a42fd6f84de80ffe8527dc10a7c857",
    "semantic_title": "mag-gnn: reinforcement learning boosted graph neural network",
    "citation_count": 1,
    "authors": [
      "Lecheng Kong",
      "Jiarui Feng",
      "Hao Liu",
      "Dacheng Tao",
      "Yixin Chen",
      "Muhan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2793dc35e14003dd367684d93d236847-Abstract-Conference.html": {
    "title": "RanPAC: Random Projections and Pre-trained Models for Continual Learning",
    "volume": "main",
    "abstract": "Continual learning (CL) aims to incrementally learn different tasks (such as classification) in a non-stationary data stream without forgetting old ones. Most CL works focus on tackling catastrophic forgetting under a learning-from-scratch paradigm. However, with the increasing prominence of foundation models, pre-trained models equipped with informative representations have become available for various downstream requirements. Several CL methods based on pre-trained models have been explored, either utilizing pre-extracted features directly (which makes bridging distribution gaps challenging) or incorporating adaptors (which may be subject to forgetting). In this paper, we propose a concise and effective approach for CL with pre-trained models. Given that forgetting occurs during parameter updating, we contemplate an alternative approach that exploits training-free random projectors and class-prototype accumulation, which thus bypasses the issue. Specifically, we inject a frozen Random Projection layer with nonlinear activation between the pre-trained model's feature representations and output head, which captures interactions between features with expanded dimensionality, providing enhanced linear separability for class-prototype-based CL. We also demonstrate the importance of decorrelating the class-prototypes to reduce the distribution disparity when using pre-trained representations. These techniques prove to be effective and circumvent the problem of forgetting for both class- and domain-incremental continual learning. Compared to previous methods applied to pre-trained ViT-B/16 models, we reduce final error rates by between 10% and 62% on seven class-incremental benchmark datasets, despite not using any rehearsal memory. We conclude that the full potential of pre-trained models for simple, effective, and fast continual learning has not hitherto been fully tapped. Code is available at https://github.com/RanPAC/RanPAC",
    "keywords": [],
    "checked": true,
    "id": "a522efa0a479bcd368407576ba13d82ee011f581",
    "semantic_title": "ranpac: random projections and pre-trained models for continual learning",
    "citation_count": 5,
    "authors": [
      "Mark D. McDonnell",
      "Dong Gong",
      "Amin Parvaneh",
      "Ehsan Abbasnejad",
      "Anton van den Hengel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/27c4e15d9af120d7fef04432c7db577f-Abstract-Conference.html": {
    "title": "FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation",
    "volume": "main",
    "abstract": "Learning to navigate to an image-specified goal is an important but challenging task for autonomous systems like household robots. The agent is required to well understand and reason the location of the navigation goal from a picture shot in the goal position. Existing methods try to solve this problem by learning a navigation policy, which captures semantic features of the goal image and observation image independently and lastly fuses them for predicting a sequence of navigation actions. However, these methods suffer from two major limitations. 1) They may miss detailed information in the goal image, and thus fail to reason the goal location. 2) More critically, it is hard to focus on the goal-relevant regions in the observation image, because they attempt to understand observation without goal conditioning. In this paper, we aim to overcome these limitations by designing a Fine-grained Goal Prompting (\\sexyname) method for image-goal navigation. In particular, we leverage fine-grained and high-resolution feature maps in the goal image as prompts to perform conditioned embedding, which preserves detailed information in the goal image and guides the observation encoder to pay attention to goal-relevant regions. Compared with existing methods on the image-goal navigation benchmark, our method brings significant performance improvement on 3 benchmark datasets (\\textit{i.e.,} Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the state-of-the-art success rate by 8\\% with only 1/50 model size",
    "keywords": [],
    "checked": true,
    "id": "97a1ff36d80dcc3f6cf6fa161b21731f934c285b",
    "semantic_title": "fgprompt: fine-grained goal prompting for image-goal navigation",
    "citation_count": 0,
    "authors": [
      "Xinyu Sun",
      "Peihao Chen",
      "Jugang Fan",
      "Jian Chen",
      "Thomas Li",
      "Mingkui Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/27c852e9d6c76890ca633f111c556a4f-Abstract-Conference.html": {
    "title": "Inner-Outer Aware Reconstruction Model for Monocular 3D Scene Reconstruction",
    "volume": "main",
    "abstract": "Monocular 3D scene reconstruction aims to reconstruct the 3D structure of scenes based on posed images. Recent volumetric-based methods directly predict the truncated signed distance function (TSDF) volume and have achieved promising results. The memory cost of volumetric-based methods will grow cubically as the volume size increases, so a coarse-to-fine strategy is necessary for saving memory. Specifically, the coarse-to-fine strategy distinguishes surface voxels from non-surface voxels, and only potential surface voxels are considered in the succeeding procedure. However, the non-surface voxels have various features, and in particular, the voxels on the inner side of the surface are quite different from those on the outer side since there exists an intrinsic gap between them. Therefore, grouping inner-surface and outer-surface voxels into the same class will force the classifier to spend its capacity to bridge the gap. By contrast, it is relatively easy for the classifier to distinguish inner-surface and outer-surface voxels due to the intrinsic gap. Inspired by this, we propose the inner-outer aware reconstruction (IOAR) model. IOAR explores a new coarse-to-fine strategy to classify outer-surface, inner-surface and surface voxels. In addition, IOAR separates occupancy branches from TSDF branches to avoid mutual interference between them. Since our model can better classify the surface, outer-surface and inner-surface voxels, it can predict more precise meshes than existing methods. Experiment results on ScanNet, ICL-NUIM and TUM-RGBD datasets demonstrate the effectiveness and generalization of our model. The code is available at https://github.com/YorkQiu/InnerOuterAwareReconstruction",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Kun Qiu",
      "Guo-Hao Xu",
      "Wei-Shi Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/27f243af2887d7f248f518d9b967a882-Abstract-Conference.html": {
    "title": "Sheaf Hypergraph Networks",
    "volume": "main",
    "abstract": "Higher-order relations are widespread in nature, with numerous phenomena involving complex interactions that extend beyond simple pairwise connections. As a result, advancements in higher-order processing can accelerate the growth of various fields requiring structured data. Current approaches typically represent these interactions using hypergraphs.We enhance this representation by introducing cellular sheaves for hypergraphs, a mathematical construction that adds extra structure to the conventional hypergraph while maintaining their local, higher-order connectivity. Drawing inspiration from existing Laplacians in the literature, we develop two unique formulations of sheaf hypergraph Laplacians: linear and non-linear. Our theoretical analysis demonstrates that incorporating sheaves into the hypergraph Laplacian provides a more expressive inductive bias than standard hypergraph diffusion, creating a powerful instrument for effectively modelling complex data structures.We employ these sheaf hypergraph Laplacians to design two categories of models: Sheaf Hypergraph Neural Networks and Sheaf Hypergraph Convolutional Networks. These models generalize classical Hypergraph Networks often found in the literature. Through extensive experimentation, we show that this generalization significantly improves performance, achieving top results on multiple benchmark datasets for hypergraph node classification",
    "keywords": [],
    "checked": true,
    "id": "d46909af1c46728a3c635196c6cc8aee1f8005d9",
    "semantic_title": "sheaf hypergraph networks",
    "citation_count": 0,
    "authors": [
      "Iulia Duta",
      "Giulia Cassarà",
      "Fabrizio Silvestri",
      "Pietro Lió"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/27f4d95417bb722201597bf4d67cbacc-Abstract-Conference.html": {
    "title": "f-Policy Gradients: A General Framework for Goal-Conditioned RL using f-Divergences",
    "volume": "main",
    "abstract": "Goal-Conditioned Reinforcement Learning (RL) problems often have access to sparse rewards where the agent receives a reward signal only when it has achieved the goal, making policy optimization a difficult problem. Several works augment this sparse reward with a learned dense reward function, but this can lead to sub-optimal policies if the reward is misaligned. Moreover, recent works have demonstrated that effective shaping rewards for a particular problem can depend on the underlying learning algorithm. This paper introduces a novel way to encourage exploration called $f$-Policy Gradients, or $f$-PG. $f$-PG minimizes the f-divergence between the agent's state visitation distribution and the goal, which we show can lead to an optimal policy. We derive gradients for various f-divergences to optimize this objective. Our learning paradigm provides dense learning signals for exploration in sparse reward settings. We further introduce an entropy-regularized policy optimization objective, that we call $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective. We show that several metric-based shaping rewards like L2 can be used with $s$-MaxEnt RL, providing a common ground to study such metric-based shaping rewards with efficient exploration. We find that $f$-PG has better performance compared to standard policy gradient methods on a challenging gridworld as well as the Point Maze and FetchReach environments. More information on our website https://agarwalsiddhant10.github.io/projects/fpg.html",
    "keywords": [],
    "checked": false,
    "id": "2095a99fd992258995157e0e1bd67a8957a8b529",
    "semantic_title": "f-policy gradients: a general framework for goal conditioned rl using f-divergences",
    "citation_count": 0,
    "authors": [
      "Siddhant Agarwal",
      "Ishan Durugkar",
      "Peter Stone",
      "Amy Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2828ee0c871f78a98ed2a198a166a439-Abstract-Conference.html": {
    "title": "Counterfactually Fair Representation",
    "volume": "main",
    "abstract": "The use of machine learning models in high-stake applications (e.g., healthcare, lending, college admission) has raised growing concerns due to potential biases against protected social groups. Various fairness notions and methods have been proposed to mitigate such biases. In this work, we focus on Counterfactual Fairness (CF), a fairness notion that is dependent on an underlying causal graph and first proposed by Kusner $\\textit{et al.}$; it requires that the outcome an individual perceives is the same in the real world as it would be in a \"counterfactual\" world, in which the individual belongs to another social group. Learning fair models satisfying CF can be challenging. It was shown in (Kusner $\\textit{et al.}$) that a sufficient condition for satisfying CF is to $\\textbf{not}$ use features that are descendants of sensitive attributes in the causal graph. This implies a simple method that learns CF models only using non-descendants of sensitive attributes while eliminating all descendants. Although several subsequent works proposed methods that use all features for training CF models, there is no theoretical guarantee that they can satisfy CF. In contrast, this work proposes a new algorithm that trains models using all the available features. We theoretically and empirically show that models trained with this method can satisfy CF",
    "keywords": [],
    "checked": true,
    "id": "e3b8dc7644db5ac89bf54ba1d957aa3a421a9477",
    "semantic_title": "counterfactually fair representation",
    "citation_count": 0,
    "authors": [
      "Zhiqun Zuo",
      "Mahdi Khalili",
      "Xueru Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/28312c9491d60ed0c77f7fff4ad86dd1-Abstract-Conference.html": {
    "title": "Truncating Trajectories in Monte Carlo Policy Evaluation: an Adaptive Approach",
    "volume": "main",
    "abstract": "Policy evaluation via Monte Carlo (MC) simulation is at the core of many MC Reinforcement Learning (RL) algorithms (e.g., policy gradient methods). In this context, the designer of the learning system specifies an interaction budget that the agent usually spends by collecting trajectories of fixed length within a simulator. However, is this data collection strategy the best option? To answer this question, in this paper, we consider as quality index the variance of an unbiased policy return estimator that uses trajectories of different lengths, i.e., truncated. We first derive a closed-form expression of this variance that clearly shows the sub-optimality of the fixed-length trajectory schedule. Furthermore, it suggests that adaptive data collection strategies that spend the available budget sequentially might be able to allocate a larger portion of transitions in timesteps in which more accurate sampling is required to reduce the variance of the final estimate. Building on these findings, we present an adaptive algorithm called Robust and Iterative Data collection strategy Optimization (RIDO). The main intuition behind RIDO is to split the available interaction budget into mini-batches. At each round, the agent determines the most convenient schedule of trajectories that minimizes an empirical and robust estimate of the estimator's variance. After discussing the theoretical properties of our method, we conclude by assessing its performance across multiple domains. Our results show that RIDO can adapt its trajectory schedule toward timesteps where more sampling is required to increase the quality of the final estimation",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Poiani",
      "Nicole Nobili",
      "Alberto Maria Metelli",
      "Marcello Restelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/28484cee66f27fa070796b631cc5242d-Abstract-Conference.html": {
    "title": "DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning",
    "volume": "main",
    "abstract": "Data augmentation techniques, such as simple image transformations and combinations, are highly effective at improving the generalization of computer vision models, especially when training data is limited. However, such techniques are fundamentally incompatible with differentially private learning approaches, due to the latter's built-in assumption that each training image's contribution to the learned model is bounded. In this paper, we investigate why naive applications of multi-sample data augmentation techniques, such as mixup, fail to achieve good performance and propose two novel data augmentation techniques specifically designed for the constraints of differentially private learning. Our first technique, DP-MixSelf, achieves SoTA classification performance across a range of datasets and settings by performing mixup on self-augmented data. Our second technique, DP-MixDiff, further improves performance by incorporating synthetic data from a pre-trained diffusion model into the mixup process. We open-source the code at https://github.com/wenxuan-Bao/DP-Mix",
    "keywords": [],
    "checked": true,
    "id": "985b98d742e5938909985789cbe531e3bb284ed8",
    "semantic_title": "dp-mix: mixup-based data augmentation for differentially private learning",
    "citation_count": 0,
    "authors": [
      "Wenxuan Bao",
      "Francesco Pittaluga",
      "Vijay Kumar B G",
      "Vincent Bindschaedler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/284afdc2309f9667d2d4fb9290235b0c-Abstract-Conference.html": {
    "title": "Point Cloud Completion with Pretrained Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Point cloud data collected in real-world applications are often incomplete. This is because they are observed from partial viewpoints, which capture only a specific perspective or angle, or due to occlusion and low resolution. Existing completion approaches rely on datasets of specific predefined objects to guide the completion of incomplete, and possibly noisy, point clouds. However, these approaches perform poorly with Out-Of-Distribution (OOD) objects, which are either absent from the dataset or poorly represented. In recent years, the field of text-guided image generation has made significant progress, leading to major breakthroughs in text guided shape generation. We describe an approach called SDS-Complete that uses a pre-trained text-to-image diffusion model and leverages the text semantic of a given incomplete point cloud of an object, to obtain a complete surface representation. SDS-Complete can complete a variety of objects at test time optimization without the need for an expensive collection of 3D information. We evaluate SDS-Complete on incomplete scanned objects, captured by real-world depth sensors and LiDAR scanners, and demonstrate that is effective in handling objects which are typically absent from common datasets",
    "keywords": [],
    "checked": false,
    "id": "2f9213b3289ccb1bd16c798c6ddcc8b1bbdebabe",
    "semantic_title": "point-cloud completion with pretrained text-to-image diffusion models",
    "citation_count": 2,
    "authors": [
      "Yoni Kasten",
      "Ohad Rahamim",
      "Gal Chechik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/286e7ab0ce6a68282394c92361c27b57-Abstract-Conference.html": {
    "title": "Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback",
    "volume": "main",
    "abstract": "In this work, we propose a multi-objective decision making framework that accommodates different user preferences over objectives, where preferences are learned via policy comparisons. Our model consists of a known Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user. We consider two user feedback models. We first address the case where a user is provided with two policies and returns their preferred policy as feedback. We then move to a different user feedback model, where a user is instead provided with two small weighted sets of representative trajectories and selects the preferred one. In both cases, we suggest an algorithm that finds a nearly optimal policy for the user using a number of comparison queries that scales quasilinearly in the number of objectives",
    "keywords": [],
    "checked": true,
    "id": "9001e28c3c1bf4a575d7765168e9c3baea2de0c8",
    "semantic_title": "eliciting user preferences for personalized multi-objective decision making through comparative feedback",
    "citation_count": 1,
    "authors": [
      "Han Shao",
      "Lee Cohen",
      "Avrim Blum",
      "Yishay Mansour",
      "Aadirupa Saha",
      "Matthew Walter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/28795419a644f41ede3fa058b13fc622-Abstract-Conference.html": {
    "title": "Deep Recurrent Optimal Stopping",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have recently emerged as a powerful paradigm for solving Markovian optimal stopping problems. However, a ready extension of DNN-based methods to non-Markovian settings requires significant state and parameter space expansion, manifesting the curse of dimensionality. Further, efficient state-space transformations permitting Markovian approximations, such as those afforded by recurrent neural networks (RNNs), are either structurally infeasible or are confounded by the curse of non-Markovianity. Considering these issues, we introduce, for the first time, an optimal stopping policy gradient algorithm (OSPG) that can leverage RNNs effectively in non-Markovian settings by implicitly optimizing value functions without recursion, mitigating the curse of non-Markovianity. The OSPG algorithm is derived from an inference procedure on a novel Bayesian network representation of discrete-time non-Markovian optimal stopping trajectories and, as a consequence, yields an offline policy gradient algorithm that eliminates expensive Monte Carlo policy rollouts",
    "keywords": [],
    "checked": false,
    "id": "1aec0c340834cf168c4a6693ea23fb8036462759",
    "semantic_title": "neural optimal stopping boundary",
    "citation_count": 11,
    "authors": [
      "Niranjan Damera Venkata",
      "Chiranjib Bhattacharyya"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/288b63aa98084366c4536ba0574a0f22-Abstract-Conference.html": {
    "title": "A Partially-Supervised Reinforcement Learning Framework for Visual Active Search",
    "volume": "main",
    "abstract": "Visual active search (VAS) has been proposed as a modeling framework in which visual cues are used to guide exploration, with the goal of identifying regions of interest in a large geospatial area. Its potential applications include identifying hot spots of rare wildlife poaching activity, search-and-rescue scenarios, identifying illegal trafficking of weapons, drugs, or people, and many others. State of the art approaches to VAS include applications of deep reinforcement learning (DRL), which yield end-to-end search policies, and traditional active search, which combines predictions with custom algorithmic approaches. While the DRL framework has been shown to greatly outperform traditional active search in such domains, its end-to-end nature does not make full use of supervised information attained either during training, or during actual search, a significant limitation if search tasks differ significantly from those in the training distribution. We propose an approach that combines the strength of both DRL and conventional active search approaches by decomposing the search policy into a prediction module, which produces a geospatial distribution of regions of interest based on task embedding and search history, and a search module, which takes the predictions and search history as input and outputs the search distribution. In addition, we develop a novel meta-learning approach for jointly learning the resulting combined policy that can make effective use of supervised information obtained both at training and decision time. Our extensive experiments demonstrate that the proposed representation and meta-learning frameworks significantly outperform state of the art in visual active search on several problem domains",
    "keywords": [],
    "checked": false,
    "id": "b09e54855068d0a1c7e3fea28b3eda56b8ccb27c",
    "semantic_title": "a partially supervised reinforcement learning framework for visual active search",
    "citation_count": 0,
    "authors": [
      "Anindya Sarkar",
      "Nathan Jacobs",
      "Yevgeniy Vorobeychik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/28b3dc0970fa4624a63278a4268de997-Abstract-Conference.html": {
    "title": "Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors",
    "volume": "main",
    "abstract": "Real-world time series are characterized by intrinsic non-stationarity that poses a principal challenge for deep forecasting models. While previous models suffer from complicated series variations induced by changing temporal distribution, we tackle non-stationary time series with modern Koopman theory that fundamentally considers the underlying time-variant dynamics. Inspired by Koopman theory of portraying complex dynamical systems, we disentangle time-variant and time-invariant components from intricate non-stationary series by Fourier Filter and design Koopman Predictor to advance respective dynamics forward. Technically, we propose Koopa as a novel Koopman forecaster composed of stackable blocks that learn hierarchical dynamics. Koopa seeks measurement functions for Koopman embedding and utilizes Koopman operators as linear portraits of implicit transition. To cope with time-variant dynamics that exhibits strong locality, Koopa calculates context-aware operators in the temporal neighborhood and is able to utilize incoming ground truth to scale up forecast horizon. Besides, by integrating Koopman Predictors into deep residual structure, we ravel out the binding reconstruction loss in previous Koopman forecasters and achieve end-to-end forecasting objective optimization. Compared with the state-of-the-art model, Koopa achieves competitive performance while saving 77.3% training time and 76.0% memory",
    "keywords": [],
    "checked": true,
    "id": "b923138069cdec59b976424cd0fbb41702a56239",
    "semantic_title": "koopa: learning non-stationary time series dynamics with koopman predictors",
    "citation_count": 10,
    "authors": [
      "Yong Liu",
      "Chenyu Li",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/28b5dfc51e5ae12d84fb7c6172a00df4-Abstract-Conference.html": {
    "title": "Bridging Discrete and Backpropagation: Straight-Through and Beyond",
    "volume": "main",
    "abstract": "Backpropagation, the cornerstone of deep learning, is limited to computing gradients for continuous variables. This limitation poses challenges for problems involving discrete latent variables. To address this issue, we propose a novel approach to approximate the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose ReinMax, which achieves second-order accuracy by integrating Heun's method, a second-order numerical method for solving ODEs. ReinMax does not require Hessian or other second-order derivatives, thus having negligible computation overheads. Extensive experimental results on various tasks demonstrate the superiority of ReinMax over the state of the art",
    "keywords": [],
    "checked": true,
    "id": "103ee4ea6dd56890c517dadc07c6bd8f4d29a359",
    "semantic_title": "bridging discrete and backpropagation: straight-through and beyond",
    "citation_count": 2,
    "authors": [
      "Liyuan Liu",
      "Chengyu Dong",
      "Xiaodong Liu",
      "Bin Yu",
      "Jianfeng Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/28bf1419b9a1f908c15f6195f58cb865-Abstract-Conference.html": {
    "title": "Distributed Inference and Fine-tuning of Large Language Models Over The Internet",
    "volume": "main",
    "abstract": "Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals — a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to $10\\times$ faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents",
    "keywords": [],
    "checked": true,
    "id": "2b05686607991a39aead43f371fd7ea2b08195f5",
    "semantic_title": "distributed inference and fine-tuning of large language models over the internet",
    "citation_count": 3,
    "authors": [
      "Alexander Borzunov",
      "Max Ryabinin",
      "Artem Chumachenko",
      "Dmitry Baranchuk",
      "Tim Dettmers",
      "Younes Belkada",
      "Pavel Samygin",
      "Colin A. Raffel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/28dad4a70f748a2980998d3ed0f1b8d2-Abstract-Conference.html": {
    "title": "Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities",
    "volume": "main",
    "abstract": "Decoding visual stimuli from neural responses recorded by functional Magnetic Resonance Imaging (fMRI) presents an intriguing intersection between cognitive neuroscience and machine learning, promising advancements in understanding human visual perception. However, the task is challenging due to the noisy nature of fMRI signals and the intricate pattern of brain visual representations. To mitigate these challenges, we introduce a two-phase fMRI representation learning framework. The first phase pre-trains an fMRI feature learner with a proposed Double-contrastive Mask Auto-encoder to learn denoised representations. The second phase tunes the feature learner to attend to neural activation patterns most informative for visual reconstruction with guidance from an image auto-encoder. The optimized fMRI feature learner then conditions a latent diffusion model to reconstruct image stimuli from brain activities. Experimental results demonstrate our model's superiority in generating high-resolution and semantically accurate images, substantially exceeding previous state-of-the-art methods by 39.34% in the 50-way-top-1 semantic classification accuracy. The code implementations will be available at https://github.com/soinx0629/visdecneurips/",
    "keywords": [],
    "checked": true,
    "id": "e9fbd2894968ae31118188758b6fd4d95adb6f39",
    "semantic_title": "contrast, attend and diffuse to decode high-resolution images from brain activities",
    "citation_count": 5,
    "authors": [
      "Jingyuan Sun",
      "Mingxiao Li",
      "Zijiao Chen",
      "Yunhao Zhang",
      "Shaonan Wang",
      "Marie-Francine Moens"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/28e4ee96c94e31b2d040b4521d2b299e-Abstract-Conference.html": {
    "title": "Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision",
    "volume": "main",
    "abstract": "Denoising diffusion models are a powerful type of generative models used to capture complex distributions of real-world signals. However, their applicability is limited to scenarios where training samples are readily available, which is not always the case in real-world applications. For example, in inverse graphics, the goal is to generate samples from a distribution of 3D scenes that align with a given image, but ground-truth 3D scenes are unavailable and only 2D images are accessible. To address this limitation, we propose a novel class of denoising diffusion probabilistic models that learn to sample from distributions of signals that are never directly observed. Instead, these signals are measured indirectly through a known differentiable forward model, which produces partial observations of the unknown signal. Our approach involves integrating the forward model directly into the denoising process. A key contribution of our work is the integration of a differentiable forward model into the denoising process. This integration effectively connects the generative modeling of observations with the generative modeling of the underlying signals, allowing for end-to-end training of a conditional generative model over signals. During inference, our approach enables sampling from the distribution of underlying signals that are consistent with a given partial observation. We demonstrate the effectiveness of our method on three challenging computer vision tasks. For instance, in the context of inverse graphics, our model enables direct sampling from the distribution of 3D scenes that align with a single 2D input image",
    "keywords": [],
    "checked": true,
    "id": "489b35b66fab138ea28ade179f68135a1cd06ff9",
    "semantic_title": "diffusion with forward models: solving stochastic inverse problems without direct supervision",
    "citation_count": 21,
    "authors": [
      "Ayush Tewari",
      "Tianwei Yin",
      "George Cazenavette",
      "Semon Rezchikov",
      "Josh Tenenbaum",
      "Fredo Durand",
      "Bill Freeman",
      "Vincent Sitzmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/28fa97d12d6e3877f1c10c605d2cffa0-Abstract-Conference.html": {
    "title": "Computational Guarantees for Doubly Entropic Wasserstein Barycenters",
    "volume": "main",
    "abstract": "We study the computation of doubly regularized Wasserstein barycenters, a recently introduced family of entropic barycenters governed by inner and outer regularization strengths. Previous research has demonstrated that various regularization parameter choices unify several notions of entropy-penalized barycenters while also revealing new ones, including a special case of debiased barycenters. In this paper, we propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters. Our procedure builds on damped Sinkhorn iterations followed by exact maximization/minimization steps and guarantees convergence for any choice of regularization parameters. An inexact variant of our algorithm, implementable using approximate Monte Carlo sampling, offers the first non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support/grid-free setting",
    "keywords": [],
    "checked": false,
    "id": "bbd800ec499ddf6f4aa357adf91f627d2f8ee9b3",
    "semantic_title": "computational guarantees for doubly entropic wasserstein barycenters via damped sinkhorn iterations",
    "citation_count": 1,
    "authors": [
      "Tomas Vaskevicius",
      "Lénaïc Chizat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2938ad0434a6506b125d8adaff084a4a-Abstract-Conference.html": {
    "title": "WITRAN: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting",
    "volume": "main",
    "abstract": "Capturing semantic information is crucial for accurate long-range time series forecasting, which involves modeling global and local correlations, as well as discovering long- and short-term repetitive patterns. Previous works have partially addressed these issues separately, but have not been able to address all of them simultaneously. Meanwhile, their time and memory complexities are still not sufficiently low for long-range forecasting. To address the challenge of capturing different types of semantic information, we propose a novel Water-wave Information Transmission (WIT) framework. This framework captures both long- and short-term repetitive patterns through bi-granular information transmission. It also models global and local correlations by recursively fusing and selecting information using Horizontal Vertical Gated Selective Unit (HVGSU). In addition, to improve the computing efficiency, we propose a generic Recurrent Acceleration Network (RAN) which reduces the time complexity to $\\mathcal{O}(\\sqrt{L})$ while maintaining the memory complexity at $\\mathcal{O}(L)$. Our proposed method, called Water-wave Information Transmission and Recurrent Acceleration Network (WITRAN), outperforms the state-of-the-art methods by 5.80% and 14.28% on long-range and ultra-long-range time series forecasting tasks respectively, as demonstrated by experiments on four benchmark datasets. The code is available at: https://github.com/Water2sea/WITRAN",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Jia",
      "Youfang Lin",
      "Xinyan Hao",
      "Yan Lin",
      "Shengnan Guo",
      "Huaiyu Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/294de0fa7149adcb88aa3119c239c63e-Abstract-Conference.html": {
    "title": "Spatio-Angular Convolutions for Super-resolution in Diffusion MRI",
    "volume": "main",
    "abstract": "Diffusion MRI (dMRI) is a widely used imaging modality, but requires long scanning times to acquire high resolution datasets. By leveraging the unique geometry present within this domain, we present a novel approach to dMRI angular super-resolution that extends upon the parametric continuous convolution (PCConv) framework. We introduce several additions to the operation including a Fourier feature mapping, 'global' co-ordinates, and domain specific context. Using this framework, we build a fully parametric continuous convolution network (PCCNN) and compare against existing models. We demonstrate the PCCNN performs competitively while using significantly fewer parameters. Moreover, we show that this formulation generalises well to clinically relevant downstream analyses such as fixel-based analysis, and neurite orientation dispersion and density imaging",
    "keywords": [],
    "checked": true,
    "id": "73bca8ca381cab3668339e902f04244e0cdfb6d6",
    "semantic_title": "spatio-angular convolutions for super-resolution in diffusion mri",
    "citation_count": 0,
    "authors": [
      "Matthew Lyon",
      "Paul Armitage",
      "Mauricio A Álvarez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29571f8fda54fe93631c41aad4215abc-Abstract-Conference.html": {
    "title": "Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning",
    "volume": "main",
    "abstract": "In this paper, we propose a Disentangled Counterfactual Learning (DCL) approach for physical audiovisual commonsense reasoning. The task aims to infer objects' physics commonsense based on both video and audio input, with the main challenge is how to imitate the reasoning ability of humans. Most of the current methods fail to take full advantage of different characteristics in multi-modal data, and lacking causal reasoning ability in models impedes the progress of implicit physical knowledge inferring. To address these issues, our proposed DCL method decouples videos into static (time-invariant) and dynamic (time-varying) factors in the latent space by the disentangled sequential encoder, which adopts a variational autoencoder (VAE) to maximize the mutual information with a contrastive loss function. Furthermore, we introduce a counterfactual learning module to augment the model's reasoning ability by modeling physical knowledge relationships among different objects under counterfactual intervention. Our proposed method is a plug-and-play module that can be incorporated into any baseline. In experiments, we show that our proposed method improves baseline methods and achieves state-of-the-art performance. Our source code is available at https://github.com/Andy20178/DCL",
    "keywords": [],
    "checked": true,
    "id": "95f977c6cb0079a505dbf2af4e6c5e280e9584ca",
    "semantic_title": "disentangled counterfactual learning for physical audiovisual commonsense reasoning",
    "citation_count": 2,
    "authors": [
      "Changsheng Lv",
      "Shuai Zhang",
      "Yapeng Tian",
      "Mengshi Qi",
      "Huadong Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29591f355702c3f4436991335784b503-Abstract-Conference.html": {
    "title": "Protein Design with Guided Discrete Diffusion",
    "volume": "main",
    "abstract": "A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to several therapeutic targets under locality and developability constraints, attaining a 99\\% expression rate and 40\\% binding rate in exploratory in vitro experiments",
    "keywords": [],
    "checked": true,
    "id": "7b14cef8a08519d7ea33800d52aba8410f48a3f7",
    "semantic_title": "protein design with guided discrete diffusion",
    "citation_count": 7,
    "authors": [
      "Nate Gruver",
      "Samuel Stanton",
      "Nathan Frey",
      "Tim G. J. Rudner",
      "Isidro Hotzel",
      "Julien Lafrance-Vanasse",
      "Arvind Rajpal",
      "Kyunghyun Cho",
      "Andrew G. Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/297fe652867e4897e9f1fe1cd715de19-Abstract-Conference.html": {
    "title": "Adaptive whitening with fast gain modulation and slow synaptic plasticity",
    "volume": "main",
    "abstract": "Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to match structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale whitening objective that factorizes the inverse whitening matrix into basis vectors, which correspond to synaptic weights, and a diagonal matrix, which corresponds to neuronal gains. We test our model on synthetic and natural datasets and find that the synapses learn optimal configurations over long timescales that enable adaptive whitening on short timescales using gain modulation",
    "keywords": [],
    "checked": true,
    "id": "f7db07b05b0fa9977b1032ab672a04d1a8aec913",
    "semantic_title": "adaptive whitening with fast gain modulation and slow synaptic plasticity",
    "citation_count": 0,
    "authors": [
      "Lyndon Duong",
      "Eero Simoncelli",
      "Dmitri Chklovskii",
      "David Lipshutz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/298281b9e89197195eb461e68ad20136-Abstract-Conference.html": {
    "title": "Tanh Works Better with Asymmetry",
    "volume": "main",
    "abstract": "Batch Normalization is commonly located in front of activation functions, as proposed by the original paper. Swapping the order, i.e., using Batch Normalization after activation functions, has also been attempted, but its performance is generally not much different from the conventional order when ReLU or a similar activation function is used. However, in the case of bounded activation functions like Tanh, we discovered that the swapped order achieves considerably better performance than the conventional order on various benchmarks and architectures. This paper reports this remarkable phenomenon and closely examines what contributes to this performance improvement. By looking at the output distributions of individual activation functions, not the whole layers, we found that many of them are asymmetrically saturated. The experiments designed to induce a different degree of asymmetric saturation support the hypothesis that asymmetric saturation helps improve performance. In addition, Batch Normalization after bounded activation functions relocates the asymmetrically saturated output of activation functions near zero, enabling the swapped model to have high sparsity, further improving performance. Extensive experiments with Tanh, LeCun Tanh, and Softsign show that the swapped models achieve improved performance with a high degree of asymmetric saturation. Finally, based on this investigation, we test a Tanh function shifted to be asymmetric. This shifted Tanh function that is manipulated to have consistent asymmetry shows even higher accuracy than the original Tanh used in the swapped order, confirming the asymmetry's importance. The code is available at https://github.com/hipros/tanhworksbetterwithasymmetry",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjin Kim",
      "Woojeong Kim",
      "Suhyun Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29906cbd165b78991da2c4dbabc2a04b-Abstract-Conference.html": {
    "title": "Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint thresholds data-efficiently. This makes our approach suitable for real-world dynamic applications",
    "keywords": [],
    "checked": true,
    "id": "b9dff780b3cbe81adca8f063a1fdcc699db0f79f",
    "semantic_title": "constraint-conditioned policy optimization for versatile safe reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Yihang Yao",
      "ZUXIN LIU",
      "Zhepeng Cen",
      "Jiacheng Zhu",
      "Wenhao Yu",
      "Tingnan Zhang",
      "DING ZHAO"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29962c2c9daf1fbd92530a7c958dfc2b-Abstract-Conference.html": {
    "title": "Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition",
    "volume": "main",
    "abstract": "This work proposes POMP, a prompt pre-training method for vision-language models. Being memory and computation efficient, POMP enables the learned prompt to condense semantic information for a rich set of visual concepts with over twenty-thousand classes. Once pre-trained, the prompt with a strong transferable ability can be directly plugged into a variety of visual recognition tasks including image classification, semantic segmentation, and object detection, to boost recognition performances in a zero-shot manner. Empirical evaluation shows that POMP achieves state-of-the-art performances on 21 datasets, e.g., 67.0% average accuracy on 10 classification datasets (+3.1% compared to CoOp) and 84.4 hIoU on open-vocabulary Pascal VOC segmentation (+6.9 compared to ZSSeg)",
    "keywords": [],
    "checked": true,
    "id": "c326672bfa418dd70a49d2dfe6dbbadb15354f7b",
    "semantic_title": "prompt pre-training with twenty-thousand classes for open-vocabulary visual recognition",
    "citation_count": 14,
    "authors": [
      "Shuhuai Ren",
      "Aston Zhang",
      "Yi Zhu",
      "Shuai Zhang",
      "Shuai Zheng",
      "Mu Li",
      "Alexander J. Smola",
      "Xu Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/299a08ee712d4752c890938da99a77c6-Abstract-Conference.html": {
    "title": "Composing Parameter-Efficient Modules with Arithmetic Operation",
    "volume": "main",
    "abstract": "As an efficient alternative to conventional full fine-tuning, parameter-efficient fine-tuning (PEFT) is becoming the prevailing method to adapt pretrained language models. In PEFT, a lightweight module is learned on each dataset while the underlying pretrained language model remains unchanged, resulting in multiple compact modules representing diverse skills when applied to various domains and tasks. In this paper, we propose to compose these parameter-efficient modules through linear arithmetic operations in the weight space, thereby integrating different module capabilities. Specifically, we first define an addition and negation operator for the module, and then further compose these two basic operators to perform flexible arithmetic. Our approach requires no additional training and enables highly flexible module composition. We apply different arithmetic operations to compose the parameter-efficient modules for (1) distribution generalization, (2) multi-tasking, (3) detoxifying, and (4) domain transfer. Additionally, we extend our approach to detoxify Alpaca-LoRA, the latest instruction-tuned large language model based on LLaMA. Empirical results demonstrate that our approach produces new and effective parameter-efficient modules that significantly outperform existing ones across all settings",
    "keywords": [],
    "checked": false,
    "id": "7f1a473834eea608980e4e04cce21be18d65b9b6",
    "semantic_title": "composing parameter-efficient modules with arithmetic operations",
    "citation_count": 21,
    "authors": [
      "Jinghan Zhang",
      "shiqi chen",
      "Junteng Liu",
      "Junxian He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29a0ea49a103a233b17c0705cdeccb66-Abstract-Conference.html": {
    "title": "UltraRE: Enhancing RecEraser for Recommendation Unlearning via Error Decomposition",
    "volume": "main",
    "abstract": "With growing concerns regarding privacy in machine learning models, regulations have committed to granting individuals the right to be forgotten while mandating companies to develop non-discriminatory machine learning systems, thereby fueling the study of the machine unlearning problem. Our attention is directed toward a practical unlearning scenario, i.e., recommendation unlearning. As the state-of-the-art framework, i.e., RecEraser, naturally achieves full unlearning completeness, our objective is to enhance it in terms of model utility and unlearning efficiency. In this paper, we rethink RecEraser from an ensemble-based perspective and focus on its three potential losses, i.e., redundancy, relevance, and combination. Under the theoretical guidance of the above three losses, we propose a new framework named UltraRE, which simplifies and powers RecEraser for recommendation tasks. Specifically, for redundancy loss, we incorporate transport weights in the clustering algorithm to optimize the equilibrium between collaboration and balance while enhancing efficiency; for relevance loss, we ensure that sub-models reach convergence on their respective group data; for combination loss, we simplify the combination estimator without compromising its efficacy. Extensive experiments on three real-world datasets demonstrate the effectiveness of UltraRE",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyuan Li",
      "Chaochao Chen",
      "Yizhao Zhang",
      "Weiming Liu",
      "Lingjuan Lyu",
      "Xiaolin Zheng",
      "Dan Meng",
      "Jun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29d319f7c1513c9ecd81d3a6e9632a6e-Abstract-Conference.html": {
    "title": "Weitzman's Rule for Pandora's Box with Correlations",
    "volume": "main",
    "abstract": "Pandora's Box is a central problem in decision making under uncertainty that can model various real life scenarios. In this problem we are given n boxes, each with a fixed opening cost, and an unknown value drawn from a known distribution, only revealed if we pay the opening cost. Our goal is to find a strategy for opening boxes to minimize the sum of the value selected and the opening cost paid.In this work we revisit Pandora's Box when the value distributions are correlated, first studied in [CGT+20]. We show that the optimal algorithm for the independent case, given by Weitzman's rule, directly works for the correlated case. In fact, our algorithm results in significantly improved approximation guarantees compared to the previous work, while also being substantially simpler. We also show how to implement the rule given only sample access to the correlated distribution of values. Specifically, we find that a number of samples that is polynomial in the number of boxes is sufficient for the algorithm to work",
    "keywords": [],
    "checked": true,
    "id": "0a50db847129e907d42d01803231356c606b9cd8",
    "semantic_title": "weitzman's rule for pandora's box with correlations",
    "citation_count": 2,
    "authors": [
      "Evangelia Gergatsouli",
      "Christos Tzamos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29dd9e016b7b2f15ceb0ea93dbf1fa53-Abstract-Conference.html": {
    "title": "Compositional Sculpting of Iterative Generative Processes",
    "volume": "main",
    "abstract": "High training costs of generative models and the need to fine-tune them for specific tasks have created a strong interest in model reuse and composition.A key challenge in composing iterative generative processes, such as GFlowNets and diffusion models, is that to realize the desired target distribution, all steps of the generative process need to be coordinated, and satisfy delicate balance conditions.In this work, we propose Compositional Sculpting: a general approach for defining compositions of iterative generative processes. We then introduce a method for sampling from these compositions built on classifier guidance.We showcase ways to accomplish compositional sculpting in both GFlowNets and diffusion models. We highlight two binary operations $\\\\unicode{x2014}$ the $\\\\textit{harmonic mean}\\\\unicode{x00A0}(p_1 \\\\otimes p_2$) and the $\\\\textit{contrast}\\\\unicode{x00A0}(p_1 \\\\,\\\\unicode{x25D1}\\\\,\\\\, p_2$) between pairs, and the generalization of these operations to multiple component distributions.We offer empirical results on image and molecular generation tasks. Project codebase: https://github.com/timgaripov/compositional-sculpting",
    "keywords": [],
    "checked": true,
    "id": "b7e0ea06f096d4963d96739ce154ba6bfe5af7ee",
    "semantic_title": "compositional sculpting of iterative generative processes",
    "citation_count": 4,
    "authors": [
      "Timur Garipov",
      "Sebastiaan De Peuter",
      "Ge Yang",
      "Vikas Garg",
      "Samuel Kaski",
      "Tommi Jaakkola"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29e4b51d45dc8f534260adc45b587363-Abstract-Conference.html": {
    "title": "Face Reconstruction from Facial Templates by Learning Latent Space of a Generator Network",
    "volume": "main",
    "abstract": "In this paper, we focus on the template inversion attack against face recognition systems and propose a new method to reconstruct face images from facial templates. Within a generative adversarial network (GAN)-based framework, we learn a mapping from facial templates to the intermediate latent space of a pre-trained face generation network, from which we can generate high-resolution realistic reconstructed face images. We show that our proposed method can be applied in whitebox and blackbox attacks against face recognition systems. Furthermore, we evaluate the transferability of our attack when the adversary uses the reconstructed face image to impersonate the underlying subject in an attack against another face recognition system. Considering the adversary's knowledge and the target face recognition system, we define five different attacks and evaluate the vulnerability of state-of-the-art face recognition systems. Our experiments show that our proposed method achieves high success attack rates in whitebox and blackbox scenarios. Furthermore, the reconstructed face images are transferable and can be used to enter target face recognition systems with a different feature extractor model. We also explore important areas in the reconstructed face images that can fool the target face recognition system",
    "keywords": [],
    "checked": true,
    "id": "75c3758f4bcd3504e8de21a5f45415aa5077d3e6",
    "semantic_title": "face reconstruction from facial templates by learning latent space of a generator network",
    "citation_count": 2,
    "authors": [
      "Hatef Otroshi Shahreza",
      "Sébastien Marcel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29e8437db7b549160ce03d336ff66f65-Abstract-Conference.html": {
    "title": "Triangulation Residual Loss for Data-efficient 3D Pose Estimation",
    "volume": "main",
    "abstract": "This paper presents Triangulation Residual loss (TR loss) for multiview 3D pose estimation in a data-efficient manner. Existing 3D supervised models usually require large-scale 3D annotated datasets, but the amount of existing data is still insufficient to train supervised models to achieve ideal performance, especially for animal pose estimation. To employ unlabeled multiview data for training, previous epipolar-based consistency provides a self-supervised loss that considers only the local consistency in pairwise views, resulting in limited performance and heavy calculations. In contrast, TR loss enables self-supervision with global multiview geometric consistency. Starting from initial 2D keypoint estimates, the TR loss can fine-tune the corresponding 2D detector without 3D supervision by simply minimizing the smallest singular value of the triangulation matrix in an end-to-end fashion. Our method achieves the state-of-the-art 25.8mm MPJPE and competitive 28.7mm MPJPE with only 5\\% 2D labeled training data on the Human3.6M dataset. Experiments on animals such as mice demonstrate our TR loss's data-efficient training ability",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Zhao",
      "Tao Yu",
      "Liang An",
      "Yipeng Huang",
      "Fang Deng",
      "Qionghai Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29ef811e72b2b97cf18dd5d866b0f472-Abstract-Conference.html": {
    "title": "A Long $N$-step Surrogate Stage Reward for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "We introduce a new stage reward estimator named the long $N$-step surrogate stage (LNSS) reward for deep reinforcement learning (RL). It aims at mitigating the high variance problem, which has shown impeding successful convergence of learning, hurting task performance, and hindering applications of deep RL in continuous control problems. In this paper we show that LNSS, which utilizes a long reward trajectory of rewards of future steps, provides consistent performance improvement measured by average reward, convergence speed, learning success rate,and variance reduction in $Q$ values and rewards. Our evaluations are based on a variety of environments in DeepMind Control Suite and OpenAI Gym by using LNSS in baseline deep RL algorithms such as DDPG, D4PG, and TD3. We show that LNSS reward has enabled good results that have been challenging to obtain by deep RL previously. Our analysis also shows that LNSS exponentially reduces the upper bound on the variances of $Q$ values from respective single-step methods",
    "keywords": [],
    "checked": false,
    "id": "ad55e3eca451705dfe8bb200b9f0f0e199fb1d75",
    "semantic_title": "long n-step surrogate stage reward to reduce variances of deep reinforcement learning in complex problems",
    "citation_count": 0,
    "authors": [
      "Junmin Zhong",
      "Ruofan Wu",
      "Jennie Si"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29f3514801f3f327d808799f5ac122ba-Abstract-Conference.html": {
    "title": "CODA: Generalizing to Open and Unseen Domains with Compaction and Disambiguation",
    "volume": "main",
    "abstract": "The generalization capability of machine learning systems degenerates notably when the test distribution drifts from the training distribution. Recently, Domain Generalization (DG) has been gaining momentum in enabling machine learning models to generalize to unseen domains. However, most DG methods assume that training and test data share an identical label space, ignoring the potential unseen categories in many real-world applications. In this paper, we delve into a more general but difficult problem termed Open Test-Time DG (OTDG), where both domain shift and open class may occur on the unseen test data. We propose Compaction and Disambiguation (CODA), a novel two-stage framework for learning compact representations and adapting to open classes in the wild. To meaningfully regularize the model's decision boundary, CODA introduces virtual unknown classes and optimizes a new training objective to insert unknowns into the latent space by compacting the embedding space of source known classes. To adapt target samples to the source model, we then disambiguate the decision boundaries between known and unknown classes with a test-time training objective, mitigating the adaptivity gap and catastrophic forgetting challenges. Experiments reveal that CODA can significantly outperform the previous best method on standard DG datasets and harmonize the classification accuracy between known and unknown classes",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoqi Chen",
      "Luyao Tang",
      "Yue Huang",
      "Xiaoguang Han",
      "Yizhou Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29f421fbdcc82aeb349d784d3aaccdb3-Abstract-Conference.html": {
    "title": "Scale-Space Hypernetworks for Efficient Biomedical Image Analysis",
    "volume": "main",
    "abstract": "Convolutional Neural Networks (CNNs) are the predominant model used for a variety of medical image analysis tasks. At inference time, these models are computationally intensive, especially with volumetric data.In principle, it is possible to trade accuracy for computational efficiency by manipulating the rescaling factor in the downsample and upsample layers of CNN architectures.However, properly exploring the accuracy-efficiency trade-off is prohibitively expensive with existing models.To address this, we introduce Scale-Space HyperNetworks (SSHN), a method that learns a spectrum of CNNs with varying internal rescaling factors.A single SSHN characterizes an entire Pareto accuracy-efficiency curve of models that match, and occasionally surpass, the outcomes of training many separate networks with fixed rescaling factors.We demonstrate the proposed approach in several medical image analysis applications, comparing SSHN against strategies with both fixed and dynamic rescaling factors.We find that SSHN consistently provides a better accuracy-efficiency trade-off at a fraction of the training cost. Trained SSHNs enable the user to quickly choose a rescaling factor that appropriately balances accuracy and computational efficiency for their particular needs at inference",
    "keywords": [],
    "checked": false,
    "id": "33721e37d3147dd639de5c14e16633b2e7302b5e",
    "semantic_title": "scale-space hypernetworks for efficient biomedical imaging",
    "citation_count": 0,
    "authors": [
      "Jose Javier Gonzalez Ortiz",
      "John Guttag",
      "Adrian Dalca"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29f47df77b7e536ebd0fe5e0cc964a32-Abstract-Conference.html": {
    "title": "Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts",
    "volume": "main",
    "abstract": "Standard contextual bandit problem assumes that all the relevant contexts are observed before the algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with problems in which additional valuable contexts can be observed after arm selection. For example, content recommendation platforms like Youtube, Instagram, Tiktok receive much additional features about a user's reward after the user clicks a content (e.g., how long the user stayed, what is the user's watch speed, etc.). To improve online learning efficiency in these applications, we study a novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB, that achieves tight regret under standard assumptions. Core to our technical proof is a robustified and generalized version of the well-known Elliptical Potential Lemma (EPL), which can accommodate noise in data. Such robustification is necessary for tackling our problem, though we believe it could also be of general interest.Extensive empirical tests on both synthetic and real-world datasets demonstrate the significant benefit of utilitzing post-serving contexts as well as the superior performance of our algorithm over the state-of-the-art approaches",
    "keywords": [],
    "checked": true,
    "id": "2eb88875085cddc5de428bcd0d71eb31aa2faef4",
    "semantic_title": "follow-ups also matter: improving contextual bandits via post-serving contexts",
    "citation_count": 0,
    "authors": [
      "Chaoqi Wang",
      "Ziyu Ye",
      "Zhe Feng",
      "Ashwinkumar Badanidiyuru Varadaraja",
      "Haifeng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a095b46705d7e6f81fc50270fe770c2-Abstract-Conference.html": {
    "title": "Offline Minimax Soft-Q-learning Under Realizability and Partial Coverage",
    "volume": "main",
    "abstract": "We consider offline reinforcement learning (RL) where we only have only access to offline data. In contrast to numerous offline RL algorithms that necessitate the uniform coverage of the offline data over state and action space, we propose value-based algorithms with PAC guarantees under partial coverage, specifically, coverage of offline data against a single policy, and realizability of soft Q-function (a.k.a., entropy-regularized Q-function) and another function, which is defined as a solution to a saddle point of certain minimax optimization problem). Furthermore, we show the analogous result for Q-functions instead of soft Q-functions. To attain these guarantees, we use novel algorithms with minimax loss functions to accurately estimate soft Q-functions and Q-functions with -convergence guarantees measured on the offline data. We introduce these loss functions by casting the estimation problems into nonlinear convex optimization problems and taking the Lagrange functions",
    "keywords": [],
    "checked": true,
    "id": "c42f14d71ff85ed3bef449d6df3e47ccbedcb176",
    "semantic_title": "offline minimax soft-q-learning under realizability and partial coverage",
    "citation_count": 2,
    "authors": [
      "Masatoshi Uehara",
      "Nathan Kallus",
      "Jason D. Lee",
      "Wen Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a0babff3ddd4ba12062219ec161ce86-Abstract-Conference.html": {
    "title": "Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption",
    "volume": "main",
    "abstract": "We study the infinite-horizon Restless Bandit problem with the average reward criterion, under both discrete-time and continuous-time settings.A fundamental goal is to design computationally efficient policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Existing results on asymptotic optimality all rely on the uniform global attractor property (UGAP), a complex and challenging-to-verify assumption. In this paper, we propose a general, simulation-based framework, Follow-the-Virtual-Advice, that converts any single-armed policy into a policy for the original $N$-armed problem. This is done by simulating the single-armed policy on each arm and carefully steering the real state towards the simulated state. Our framework can be instantiated to produce a policy with an $O(1/\\sqrt{N})$ optimality gap. In the discrete-time setting, our result holds under a simpler synchronization assumption, which covers some problem instances that violate UGAP. More notably, in the continuous-time setting, we do not require \\emph{any} additional assumptions beyond the standard unichain condition. In both settings, our work is the first asymptotic optimality result that does not require UGAP",
    "keywords": [],
    "checked": true,
    "id": "b12f976066a59e19faa4c0e4662ef4f8d4af0907",
    "semantic_title": "restless bandits with average reward: breaking the uniform global attractor assumption",
    "citation_count": 3,
    "authors": [
      "Yige Hong",
      "Qiaomin Xie",
      "Yudong Chen",
      "Weina Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a128987fe57b27fa0c7a0b748b0fa1e-Abstract-Conference.html": {
    "title": "Smooth Flipping Probability for Differential Private Sign Random Projection Methods",
    "volume": "main",
    "abstract": "We develop a series of differential privacy (DP) algorithms from a family of random projection (RP) and sign random projection (SignRP) methods. We first show how to improve the previous DP-RP approach using the ``optimal Gaussian mechanism''. Then, we propose a series of DP-SignRP algorithms that leverage the robustness of the ``sign flipping probability'' of random projections. That is, given $x = \\sum_{i=1}^p u_i w_{i}$ where $u$ is a $p$-dimensional data vector and $w$ is a symmetric random vector, $sign(x)$ only has a fairly small probability to be flipped if there is a small modification on data $u$, depending on the specific distribution of $w$. This robustness leads to our novel design of ``smooth flipping probability'' for SignRP-type algorithms with better utility than using the standard randomized response mechanism. Retrieval and classification experiments demonstrate that, among the presented DP-RP algorithms, \\textbf{DP-SignOPORP} (where OPORP is an improvement over the celebrated count-sketch algorithms), performs the best in general.In the industrial practice, DP methods were not very popular for machine learning or search, largely because the performance typically would drop substantially if DP is applied. Since our proposed new DP algorithms have significantly improved the performance, it is anticipated that our work will motivate a wide adoption of DP in practice. Finally, we stress that, since our methods are applied to the original data (i.e., feature vectors), the privacy of downstream tasks is naturally protected",
    "keywords": [],
    "checked": false,
    "id": "8568884eb35ef18b2896a4ae2a82264b0918d2f9",
    "semantic_title": "differential privacy with random projections and sign random projections",
    "citation_count": 2,
    "authors": [
      "Ping Li",
      "Xiaoyun Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a25d9d873e9ae6d242c62e36f89ee3a-Abstract-Conference.html": {
    "title": "Idempotent Learned Image Compression with Right-Inverse",
    "volume": "main",
    "abstract": "We consider the problem of idempotent learned image compression (LIC).The idempotence of codec refers to the stability of codec to re-compression.To achieve idempotence, previous codecs adopt invertible transforms such as DCT and normalizing flow.In this paper, we first identify that invertibility of transform is sufficient but not necessary for idempotence. Instead, it can be relaxed into right-invertibility. And such relaxation allows wider family of transforms.Based on this identification, we implement an idempotent codec using our proposed blocked convolution and null-space enhancement.Empirical results show that we achieve state-of-the-art rate-distortion performance among idempotent codecs. Furthermore, our codec can be extended into near-idempotent codec by relaxing the right-invertibility. And this near-idempotent codec has significantly less quality decay after $50$ rounds of re-compression compared with other near-idempotent codecs",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanghao Li",
      "Tongda Xu",
      "Yan Wang",
      "Jingjing Liu",
      "Ya-Qin Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a28bea6298d106eed091ac403d8c22b-Abstract-Conference.html": {
    "title": "A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm",
    "volume": "main",
    "abstract": "Meta learning is a promising paradigm to enable skill transfer across tasks.Most previous methods employ the empirical risk minimization principle in optimization.However, the resulting worst fast adaptation to a subset of tasks can be catastrophic in risk-sensitive scenarios.To robustify fast adaptation, this paper optimizes meta learning pipelines from a distributionally robust perspective and meta trains models with the measure of tail task risk.We take the two-stage strategy as heuristics to solve the robust meta learning problem, controlling the worst fast adaptation cases at a certain probabilistic level. Experimental results show that our simple method can improve the robustness of meta learning to task distributions and reduce the conditional expectation of the worst fast adaptation risk",
    "keywords": [],
    "checked": true,
    "id": "1a7f477963011836073eeb3390c230205639be8c",
    "semantic_title": "a simple yet effective strategy to robustify the meta learning paradigm",
    "citation_count": 0,
    "authors": [
      "Qi Wang",
      "Yiqin Lv",
      "yanghe feng",
      "Zheng Xie",
      "Jincai Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a4179ef39846557e99f6bfac580ea2e-Abstract-Conference.html": {
    "title": "Stabilized Neural Differential Equations for Learning Dynamics with Explicit Constraints",
    "volume": "main",
    "abstract": "Many successful methods to learn dynamical systems from data have recently been introduced. However, ensuring that the inferred dynamics preserve known constraints, such as conservation laws or restrictions on the allowed system states, remains challenging. We propose stabilized neural differential equations (SNDEs), a method to enforce arbitrary manifold constraints for neural differential equations. Our approach is based on a stabilization term that, when added to the original dynamics, renders the constraint manifold provably asymptotically stable. Due to its simplicity, our method is compatible with all common neural differential equation (NDE) models and broadly applicable. In extensive empirical evaluations, we demonstrate that SNDEs outperform existing methods while broadening the types of constraints that can be incorporated into NDE training",
    "keywords": [],
    "checked": true,
    "id": "e33477691c0ee4c69e5eacf421d7a4fba7a37e35",
    "semantic_title": "stabilized neural differential equations for learning dynamics with explicit constraints",
    "citation_count": 0,
    "authors": [
      "Alistair White",
      "Niki Kilbertus",
      "Maximilian Gelbrecht",
      "Niklas Boers"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a4310c4fd24bd336aa2f64f93cb5d39-Abstract-Conference.html": {
    "title": "On the Importance of Exploration for Generalization in Reinforcement Learning",
    "volume": "main",
    "abstract": "Existing approaches for improving generalization in deep reinforcement learning (RL) have mostly focused on representation learning, neglecting RL-specific aspects such as exploration. We hypothesize that the agent's exploration strategy plays a key role in its ability to generalize to new environments.Through a series of experiments in a tabular contextual MDP, we show that exploration is helpful not only for efficiently finding the optimal policy for the training environments but also for acquiring knowledge that helps decision making in unseen environments. Based on these observations, we propose EDE: Exploration via Distributional Ensemble, a method that encourages the exploration of states with high epistemic uncertainty through an ensemble of Q-value distributions. The proposed algorithm is the first value-based approach to achieve strong performance on both Procgen and Crafter, two benchmarks for generalization in RL with high-dimensional observations. The open-sourced implementation can be found at https://github.com/facebookresearch/ede",
    "keywords": [],
    "checked": true,
    "id": "1a73038804052a40c12aae696848ece2168f6da7",
    "semantic_title": "on the importance of exploration for generalization in reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Yiding Jiang",
      "J. Zico Kolter",
      "Roberta Raileanu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a50f08293e5f635655e8bec8f013d99-Abstract-Conference.html": {
    "title": "Uniform Convergence with Square-Root Lipschitz Loss",
    "volume": "main",
    "abstract": "We establish generic uniform convergence guarantees for Gaussian data in terms of the Radamacher complexity of the hypothesis class and the Lipschitz constant of the square root of the scalar loss function. We show how these guarantees substantially generalize previous results based on smoothness (Lipschitz constant of the derivative), and allow us to handle the broader class of square-root-Lipschtz losses, which includes also non-smooth loss functions appropriate for studying phase retrieval and ReLU regression, as well as rederive and better understand \"optimistic rate\" and interpolation learning guarantees",
    "keywords": [],
    "checked": true,
    "id": "2d6e92f0af54559eb7bfbb6fb56196a4acebd8ea",
    "semantic_title": "uniform convergence with square-root lipschitz loss",
    "citation_count": 0,
    "authors": [
      "Lijia Zhou",
      "Zhen Dai",
      "Frederic Koehler",
      "Nati Srebro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a514213ba899f2911723a38be8d4096-Abstract-Conference.html": {
    "title": "A Fractional Graph Laplacian Approach to Oversmoothing",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have shown state-of-the-art performances in various applications. However, GNNs often struggle to capture long-range dependencies in graphs due to oversmoothing. In this paper, we generalize the concept of oversmoothing from undirected to directed graphs. To this aim, we extend the notion of Dirichlet energy by considering a directed symmetrically normalized Laplacian. As vanilla graph convolutional networks are prone to oversmooth, we adopt a neural graph ODE framework. Specifically, we propose fractional graph Laplacian neural ODEs, which describe non-local dynamics. We prove that our approach allows propagating information between distant nodes while maintaining a low probability of long-distance jumps. Moreover, we show that our method is more flexible with respect to the convergence of the graph's Dirichlet energy, thereby mitigating oversmoothing. We conduct extensive experiments on synthetic and real-world graphs, both directed and undirected, demonstrating our method's versatility across diverse graph homophily levels. Ourcode is available at https://github.com/RPaolino/fLode",
    "keywords": [],
    "checked": true,
    "id": "2f2d4ce7ca8ed125e3ee5ecd97bd638eabadd2e1",
    "semantic_title": "a fractional graph laplacian approach to oversmoothing",
    "citation_count": 10,
    "authors": [
      "Sohir Maskey",
      "Raffaele Paolino",
      "Aras Bacho",
      "Gitta Kutyniok"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a91de02871011d0090e662ffd6f2328-Abstract-Conference.html": {
    "title": "On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\\epsilon$-Greedy Exploration",
    "volume": "main",
    "abstract": "This paper provides a theoretical understanding of deep Q-Network (DQN) with the $\\varepsilon$-greedy exploration in deep reinforcement learning.Despite the tremendous empirical achievement of the DQN, its theoretical characterization remains underexplored.First, the exploration strategy is either impractical or ignored in the existing analysis. Second, in contrast to conventional Q-learning algorithms, the DQN employs the target network and experience replay to acquire an unbiased estimation of the mean-square Bellman error (MSBE) utilized in training the Q-network. However,the existing theoretical analysis of DQNs lacks convergence analysis or bypasses the technical challenges by deploying a significantly overparameterized neural network, which is not computationally efficient. This paper provides the first theoretical convergence and sample complexity analysis of the practical setting of DQNs with $\\epsilon$-greedy policy. We prove an iterative procedure with decaying $\\epsilon$ converges to the optimal Q-value function geometrically. Moreover, a higher level of $\\epsilon$ values enlarges the region of convergence but slows down the convergence, while the opposite holds for a lower level of $\\epsilon$ values. Experiments justify our established theoretical insights on DQNs",
    "keywords": [],
    "checked": false,
    "id": "154a0dc6aa4fd7c0d3160cdce42735d65e58084d",
    "semantic_title": "on the convergence and sample complexity analysis of deep q-networks with ε-greedy exploration",
    "citation_count": 0,
    "authors": [
      "Shuai Zhang",
      "Hongkang Li",
      "Meng Wang",
      "Miao Liu",
      "Pin-Yu Chen",
      "Songtao Lu",
      "Sijia Liu",
      "Keerthiram Murugesan",
      "Subhajit Chaudhury"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a91fb5a4c03e0b6d889e1c52f775480-Abstract-Conference.html": {
    "title": "Joint Data-Task Generation for Auxiliary Learning",
    "volume": "main",
    "abstract": "Current auxiliary learning methods mainly adopt the methodology of reweighing losses for the manually collected auxiliary data and tasks. However, these methods heavily rely on domain knowledge during data collection, which may be hardly available in reality. Therefore, current methods will become less effective and even do harm to the primary task when unhelpful auxiliary data and tasks are employed. To tackle the problem, we propose a joint data-task generation framework for auxiliary learning (DTG-AuxL), which can bring benefits to the primary task by generating the new auxiliary data and task in a joint manner. The proposed DTG-AuxL framework contains a joint generator and a bi-level optimization strategy. Specifically, the joint generator contains a feature generator and a label generator, which are designed to be applicable and expressive for various auxiliary learning scenarios. The bi-level optimization strategy optimizes the joint generator and the task learning model, where the joint generator is effectively optimized in the upper level via the implicit gradient from the primary loss and the explicit gradient of our proposed instance regularization, while the task learning model is optimized in the lower level by the generated data and task. Extensive experiments show that our proposed DTG-AuxL framework consistently outperforms existing methods in various auxiliary learning scenarios, particularly when the manually collected auxiliary data and tasks are unhelpful",
    "keywords": [],
    "checked": false,
    "id": "7dbb2d81343fb30792b2a9850e6fd35e75e18b80",
    "semantic_title": "private image generation with dual-purpose auxiliary classifier",
    "citation_count": 0,
    "authors": [
      "Hong Chen",
      "Xin Wang",
      "Yuwei Zhou",
      "Yijian Qin",
      "Chaoyu Guan",
      "Wenwu Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a952768bb85041f95ed06a5b60cf4d5-Abstract-Conference.html": {
    "title": "On Proper Learnability between Average- and Worst-case Robustness",
    "volume": "main",
    "abstract": "Recently, Montasser at al. (2019) showed that finite VC dimension is not sufficient for proper adversarially robust PAC learning. In light of this hardness, there is a growing effort to study what type of relaxations to the adversarially robust PAC learning setup can enable proper learnability. In this work, we initiate the study of proper learning under relaxations of the worst-case robust loss. We give a family of robust loss relaxations under which VC classes are properly PAC learnable with sample complexity close to what one would require in the standard PAC learning setup. On the other hand, we show that for an existing and natural relaxation of the worst-case robust loss, finite VC dimension is not sufficient for proper learning. Lastly, we give new generalization guarantees for the adversarially robust empirical risk minimizer",
    "keywords": [],
    "checked": true,
    "id": "7e6dfdb1de0a71c467973608e88dd3d7e6c7345a",
    "semantic_title": "on proper learnability between average- and worst-case robustness",
    "citation_count": 1,
    "authors": [
      "Vinod Raman",
      "UNIQUE SUBEDI",
      "Ambuj Tewari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2a98af4fea6a24b73af7b588ca95f755-Abstract-Conference.html": {
    "title": "Distributional Policy Evaluation: a Maximum Entropy approach to Representation Learning",
    "volume": "main",
    "abstract": "The Maximum Entropy (Max-Ent) framework has been effectively employed in a variety of Reinforcement Learning (RL) tasks. In this paper, we first propose a novel Max-Ent framework for policy evaluation in a distributional RL setting, named Distributional Maximum Entropy Policy Evaluation (D-Max-Ent PE). We derive a generalization-error bound that depends on the complexity of the representation employed, showing that this framework can explicitly take into account the features used to represent the state space while evaluating a policy. Then, we exploit these favorable properties to drive the representation learning of the state space in a Structural Risk Minimization fashion. We employ state-aggregation functions as feature functions and we specialize the D-Max-Ent approach into an algorithm, named D-Max-Ent Progressive Factorization, which constructs a progressively finer-grained representation of the state space by balancing the trade-off between preserving information (bias) and reducing the effective number of states, i.e., the complexity of the representation space (variance). Finally, we report the results of some illustrative numerical simulations, showing that the proposed algorithm matches the expected theoretical behavior and highlighting the relationship between aggregations and sample regimes",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Zamboni",
      "Alberto Maria Metelli",
      "Marcello Restelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2aa212d6f40c1cb19b777e83db00ec6a-Abstract-Conference.html": {
    "title": "Thin and deep Gaussian processes",
    "volume": "main",
    "abstract": "Gaussian processes (GPs) can provide a principled approach to uncertainty quantification with easy-to-interpret kernel hyperparameters, such as the lengthscale, which controls the correlation distance of function values.However, selecting an appropriate kernel can be challenging.Deep GPs avoid manual kernel engineering by successively parameterizing kernels with GP layers, allowing them to learn low-dimensional embeddings of the inputs that explain the output data.Following the architecture of deep neural networks, the most common deep GPs warp the input space layer-by-layer but lose all the interpretability of shallow GPs. An alternative construction is to successively parameterize the lengthscale of a kernel, improving the interpretability but ultimately giving away the notion of learning lower-dimensional embeddings. Unfortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability.This work proposes a novel synthesis of both previous approaches: {Thin and Deep GP} (TDGP). Each TDGP layer defines locally linear transformations of the original input data maintaining the concept of latent embeddings while also retaining the interpretation of lengthscales of a kernel. Moreover, unlike the prior solutions, TDGP induces non-pathological manifolds that admit learning lower-dimensional representations.We show with theoretical and experimental results that i) TDGP is, unlike previous models, tailored to specifically discover lower-dimensional manifolds in the input data, ii) TDGP behaves well when increasing the number of layers, and iii) TDGP performs well in standard benchmark datasets",
    "keywords": [],
    "checked": true,
    "id": "d3e3ea946403a17c3579e44fdebc4ec9bb28230f",
    "semantic_title": "thin and deep gaussian processes",
    "citation_count": 1,
    "authors": [
      "Daniel Augusto de Souza",
      "Alexander Nikitin",
      "ST John",
      "Magnus Ross",
      "Mauricio A Álvarez",
      "Marc Deisenroth",
      "João Paulo Gomes",
      "Diego Mesquita",
      "César Lincoln Mattos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2aa9b18b9ab37b0ab1fdaae46fb781d4-Abstract-Conference.html": {
    "title": "Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language",
    "volume": "main",
    "abstract": "A core tension in models of concept learning is that the model must carefully balance the tractability of inference against the expressivity of the hypothesis class. Humans, however, can efficiently learn a broad range of concepts. We introduce a model of inductive learning that seeks to be human-like in that sense.It implements a Bayesian reasoning process where a language model first proposes candidate hypotheses expressed in natural language, which are then re-weighed by a prior and a likelihood.By estimating the prior from human data, we can predict human judgments on learning problems involving numbers and sets, spanning concepts that are generative, discriminative, propositional, and higher-order",
    "keywords": [],
    "checked": true,
    "id": "1a3ba6662ef1c5aebd3b343d3d9f77a8543e474d",
    "semantic_title": "human-like few-shot learning via bayesian reasoning over natural language",
    "citation_count": 5,
    "authors": [
      "Kevin Ellis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2aab54135bd206ef6d4949ce17528d98-Abstract-Conference.html": {
    "title": "CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion",
    "volume": "main",
    "abstract": "Electroencephalography (EEG) is a prominent non-invasive neuroimaging technique providing insights into brain function. Unfortunately, EEG data exhibit a high degree of noise and variability across subjects hampering generalizable signal extraction. Therefore, a key aim in EEG analysis is to extract the underlying neural activation (content) as well as to account for the individual subject variability (style). We hypothesize that the ability to convert EEG signals between tasks and subjects requires the extraction of latent representations accounting for content and style. Inspired by recent advancements in voice conversion technologies, we propose a novel contrastive split-latent permutation autoencoder (CSLP-AE) framework that directly optimizes for EEG conversion. Importantly, the latent representations are guided using contrastive learning to promote the latent splits to explicitly represent subject (style) and task (content). We contrast CSLP-AE to conventional supervised, unsupervised (AE), and self-supervised (contrastive learning) training and find that the proposed approach provides favorable generalizable characterizations of subject and task. Importantly, the procedure also enables zero-shot conversion between unseen subjects. While the present work only considers conversion of EEG, the proposed CSLP-AE provides a general framework for signal conversion and extraction of content (task activation) and style (subject variability) components of general interest for the modeling and analysis of biological signals",
    "keywords": [],
    "checked": true,
    "id": "74ff5f75ec2426b4a67806a7bebf708ebf2fe1c3",
    "semantic_title": "cslp-ae: a contrastive split-latent permutation autoencoder framework for zero-shot electroencephalography signal conversion",
    "citation_count": 0,
    "authors": [
      "Anders Nørskov",
      "Alexander Neergaard Zahid",
      "Morten Mørup"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2aab664e0d1656e8b56c74f868e1ea69-Abstract-Conference.html": {
    "title": "Delegated Classification",
    "volume": "main",
    "abstract": "When machine learning is outsourced to a rational agent, conflicts of interest might arise and severely impact predictive performance. In this work, we propose a theoretical framework for incentive-aware delegation of machine learning tasks. We model delegation as a principal-agent game, in which accurate learning can be incentivized by the principal using performance-based contracts. Adapting the economic theory of contract design to this setting, we define budget-optimal contracts and prove they take a simple threshold form under reasonable assumptions. In the binary-action case, the optimality of such contracts is shown to be equivalent to the classic Neyman-Pearson lemma, establishing a formal connection between contract design and statistical hypothesis testing. Empirically, we demonstrate that budget-optimal contracts can be constructed using small-scale data, leveraging recent advances in the study of learning curves and scaling laws. Performance and economic outcomes are evaluated using synthetic and real-world classification tasks",
    "keywords": [],
    "checked": true,
    "id": "1205d6406577389f583247b2eae4abae6215656d",
    "semantic_title": "delegated classification",
    "citation_count": 1,
    "authors": [
      "Eden Saig",
      "Inbal Talgam-Cohen",
      "Nir Rosenfeld"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2aab8a76c7e761b66eccaca0927787de-Abstract-Conference.html": {
    "title": "PTQD: Accurate Post-Training Quantization for Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have recently dominated image synthesis and other related generative tasks. However, the iterative denoising process is expensive in computations at inference time, making diffusion models less practical for low-latency and scalable real-world applications. Post-training quantization of diffusion models can significantly reduce the model size and accelerate the sampling process without requiring any re-training. Nonetheless, applying existing post-training quantization methods directly to low-bit diffusion models can significantly impair the quality of generated samples. Specifically, for each denoising step, quantization noise leads to deviations in the estimated mean and mismatches with the predetermined variance schedule. Moreover, as the sampling process proceeds, the quantization noise may accumulate, resulting in a low signal-to-noise ratio (SNR) during the later denoising steps. To address these challenges, we propose a unified formulation for the quantization noise and diffusion perturbed noise in the quantized denoising process. Specifically, we first disentangle the quantization noise into its correlated and residual uncorrelated parts regarding its full-precision counterpart. The correlated part can be easily corrected by estimating the correlation coefficient. For the uncorrelated part, we subtract the bias from the quantized results to correct the mean deviation and calibrate the denoising variance schedule to absorb the excess variance resulting from quantization. Moreover, we introduce a mixed-precision scheme for selecting the optimal bitwidth for each denoising step, which prioritizes lower bitwidths to expedite early denoising steps, while ensuring that higher bitwidths maintain a high signal-to-noise ratio (SNR) in the later steps. Extensive experiments demonstrate that our method outperforms previous post-training quantized diffusion models in generating high-quality samples, with only a $0.06$ increase in FID score compared to full-precision LDM-4 on ImageNet $256\\times256$, while saving $19.9\\times$ bit operations. Code is available at [https://github.com/ziplab/PTQD](https://github.com/ziplab/PTQD)",
    "keywords": [],
    "checked": true,
    "id": "936d32864240bde74f5824df3bd8343d8de08843",
    "semantic_title": "ptqd: accurate post-training quantization for diffusion models",
    "citation_count": 10,
    "authors": [
      "Yefei He",
      "Luping Liu",
      "Jing Liu",
      "Weijia Wu",
      "Hong Zhou",
      "Bohan Zhuang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2ab3163ee384cd46baa7f1abb2b1bf19-Abstract-Conference.html": {
    "title": "Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery",
    "volume": "main",
    "abstract": "Recent advances in machine learning have shown that Reinforcement Learning from Human Feedback (RLHF) can improve machine learning models and align them with human preferences. Although very successful for Large Language Models (LLMs), these advancements have not had a comparable impact in research for autonomous vehicles—where alignment with human expectations can be imperative. In this paper, we propose to adapt similar RL-based methods to unsupervised object discovery, i.e. learning to detect objects from LiDAR points without any training labels. Instead of labels, we use simple heuristics to mimic human feedback. More explicitly, we combine multiple heuristics into a simple reward function that positively correlates its score with bounding box accuracy, i.e., boxes containing objects are scored higher than those without. We start from the detector's own predictions to explore the space and reinforce boxes with high rewards through gradient updates. Empirically, we demonstrate that our approach is not only more accurate, but also orders of magnitudes faster to train compared to prior works on object discovery. Code is available at https://github.com/katieluo88/DRIFT",
    "keywords": [],
    "checked": true,
    "id": "229b546f24fe6487b62b53fb9d4cb327f736e31e",
    "semantic_title": "reward finetuning for faster and more accurate unsupervised object discovery",
    "citation_count": 0,
    "authors": [
      "Katie Luo",
      "Zhenzhen Liu",
      "Xiangyu Chen",
      "Yurong You",
      "Sagie Benaim",
      "Cheng Perng Phoo",
      "Mark Campbell",
      "Wen Sun",
      "Bharath Hariharan",
      "Kilian Q. Weinberger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2ab87e2179b8ea209b52463802d62560-Abstract-Conference.html": {
    "title": "Doubly Constrained Fair Clustering",
    "volume": "main",
    "abstract": "The remarkable attention which fair clustering has received in the last few years has resulted in a significant number of different notions of fairness. Despite the fact that these notions are well-justified, they are often motivated and studied in a disjoint manner where one fairness desideratum is considered exclusively in isolation from the others. This leaves the understanding of the relations between different fairness notions as an important open problem in fair clustering. In this paper, we take the first step in this direction. Specifically, we consider the two most prominent demographic representation fairness notions in clustering: (1) Group Fairness ($\\textbf{GF}$), where the different demographic groups are supposed to have close to population-level representation in each cluster and (2) Diversity in Center Selection ($\\textbf{DS}$), where the selected centers are supposed to have close to population-level representation of each group. We show that given a constant approximation algorithm for one constraint ($\\textbf{GF}$ or $\\textbf{DS}$ only) we can obtain a constant approximation solution that satisfies both constraints simultaneously. Interestingly, we prove that any given solution that satisfies the $\\textbf{GF}$ constraint can always be post-processed at a bounded degradation to the clustering cost to additionally satisfy the $\\textbf{DS}$ constraint while the same statement is not true given a solution that satisfies $\\textbf{DS}$ instead. Furthermore, we show that both $\\textbf{GF}$ and $\\textbf{DS}$ are incompatible (having an empty feasibility set in the worst case) with a collection of other distance-based fairness notions. Finally, we carry experiments to validate our theoretical findings",
    "keywords": [],
    "checked": true,
    "id": "b5d521028a9e46af92db3d4ca21a6a0cde52b693",
    "semantic_title": "doubly constrained fair clustering",
    "citation_count": 0,
    "authors": [
      "John Dickerson",
      "Seyed Esmaeili",
      "Jamie H. Morgenstern",
      "Claire Jie Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2ac2eac5098dba08208807b65c5851cc-Abstract-Conference.html": {
    "title": "ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting",
    "volume": "main",
    "abstract": "Diffusion-based image super-resolution (SR) methods are mainly limited by the low inference speed due to the requirements of hundreds or even thousands of sampling steps. Existing acceleration sampling techniques inevitably sacrifice performance to some extent, leading to over-blurry SR results. To address this issue, we propose a novel and efficient diffusion model for SR that significantly reduces the number of diffusion steps, thereby eliminating the need for post-acceleration during inference and its associated performance deterioration. Our method constructs a Markov chain that transfers between the high-resolution image and the low-resolution image by shifting the residual between them, substantially improving the transition efficiency. Additionally, an elaborate noise schedule is developed to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experiments demonstrate that the proposed method obtains superior or at least comparable performance to current state-of-the-art methods on both synthetic and real-world datasets, \\textit{\\textbf{even only with 20 sampling steps}}. Our code and model will be made publicly",
    "keywords": [],
    "checked": true,
    "id": "d4a60ef37125fcde198781c2eb578a9c9dc78c1c",
    "semantic_title": "resshift: efficient diffusion model for image super-resolution by residual shifting",
    "citation_count": 17,
    "authors": [
      "Zongsheng Yue",
      "Jianyi Wang",
      "Chen Change Loy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2ac879d1865475a7abc8dfc7a9c15c27-Abstract-Conference.html": {
    "title": "WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
    "volume": "main",
    "abstract": "Graphs are widely used to model interconnected entities and improve downstream predictions in various real-world applications. However, real-world graphs nowadays are often associated with complex attributes on multiple types of nodes and even links that are hard to model uniformly, while the widely used graph neural networks (GNNs) often require sufficient training toward specific downstream predictions to achieve strong performance. In this work, we take a fundamentally different approach than GNNs, to simultaneously achieve deep joint modeling of complex attributes and flexible structures of real-world graphs and obtain unsupervised generic graph representations that are not limited to specific downstream predictions. Our framework, built on a natural integration of language models (LMs) and random walks (RWs), is straightforward, powerful and data-efficient. Specifically, we first perform attributed RWs on the graph and design an automated program to compose roughly meaningful textual sequences directly from the attributed RWs; then we fine-tune an LM using the RW-based textual sequences and extract embedding vectors from the LM, which encapsulates both attribute semantics and graph structures. In our experiments, we evaluate the learned node embeddings towards different downstream prediction tasks on multiple real-world attributed graph datasets and observe significant improvements over a comprehensive set of state-of-the-art unsupervised node embedding methods. We believe this work opens a door for more sophisticated technical designs and empirical evaluations toward the leverage of LMs for the modeling of real-world graphs",
    "keywords": [],
    "checked": false,
    "id": "d2ecb191cb037c96d4c2ad0a47a49ba82b701285",
    "semantic_title": "walklm : a uniform language model fine-tuning framework for attributed graph embedding",
    "citation_count": 3,
    "authors": [
      "Yanchao Tan",
      "Zihao Zhou",
      "Hang Lv",
      "Weiming Liu",
      "Carl Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2aebc17b683792a17dd4a24fcb038ba6-Abstract-Conference.html": {
    "title": "Generalizing Nonlinear ICA Beyond Structural Sparsity",
    "volume": "main",
    "abstract": "Nonlinear independent component analysis (ICA) aims to uncover the true latent sources from their observable nonlinear mixtures. Despite its significance, the identifiability of nonlinear ICA is known to be impossible without additional assumptions. Recent advances have proposed conditions on the connective structure from sources to observed variables, known as Structural Sparsity, to achieve identifiability in an unsupervised manner. However, the sparsity constraint may not hold universally for all sources in practice. Furthermore, the assumptions of bijectivity of the mixing process and independence among all sources, which arise from the setting of ICA, may also be violated in many real-world scenarios. To address these limitations and generalize nonlinear ICA, we propose a set of new identifiability results in the general settings of undercompleteness, partial sparsity and source dependence, and flexible grouping structures. Specifically, we prove identifiability when there are more observed variables than sources (undercomplete), and when certain sparsity and/or source independence assumptions are not met for some changing sources. Moreover, we show that even in cases with flexible grouping structures (e.g., part of the sources can be divided into irreducible independent groups with various sizes), appropriate identifiability results can also be established. Theoretical claims are supported empirically on both synthetic and real-world datasets",
    "keywords": [],
    "checked": true,
    "id": "f57749e893c9d532e5b65845cea7adacedc0a4aa",
    "semantic_title": "generalizing nonlinear ica beyond structural sparsity",
    "citation_count": 1,
    "authors": [
      "Yujia Zheng",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2af57f909a99113db071672da236a5f2-Abstract-Conference.html": {
    "title": "Towards Characterizing the First-order Query Complexity of Learning (Approximate) Nash Equilibria in Zero-sum Matrix Games",
    "volume": "main",
    "abstract": "In the first-order query model for zero-sum $K\\times K$ matrix games, players observe the expected pay-offs for all their possible actions under the randomized action played by their opponent. This classical model has received renewed interest after the discovery by Rakhlin and Sridharan that $\\epsilon$-approximate Nash equilibria can be computed efficiently from $O(\\frac{\\ln K}{\\epsilon})$ instead of $O(\\frac{\\ln K}{\\epsilon^2})$ queries. Surprisingly, the optimal number of such queries, as a function of both $\\epsilon$ and $K$, is not known. We make progress on this question on two fronts. First, we fully characterise the query complexity of learning exact equilibria ($\\epsilon=0$), by showing that they require a number of queries that is linear in $K$, which means that it is essentially as hard as querying the whole matrix, which can also be done with $K$ queries. Second, for $\\epsilon > 0$, the current query complexity upper bound stands at $O(\\min(\\frac{\\ln(K)}{\\epsilon} , K))$. We argue that, unfortunately, obtaining a matching lower bound is not possible with existing techniques: we prove that no lower bound can be derived by constructing hard matrices whose entries take values in a known countable set, because such matrices can be fully identified by a single query. This rules out, for instance, reducing to an optimization problem over the hypercube by encoding it as a binary payoff matrix. We then introduce a new technique for lower bounds, which allows us to obtain lower bounds of order $\\tilde\\Omega(\\log(\\frac{1}{K\\epsilon})$ for any $\\epsilon \\leq 1 / (cK^4)$, where $c$ is a constant independent of $K$. We further discuss possible future directions to improve on our techniques in order to close the gap with the upper bounds",
    "keywords": [],
    "checked": true,
    "id": "245b5c90bf6cff9626fdbbdb0d00e3edd094fe2f",
    "semantic_title": "towards characterizing the first-order query complexity of learning (approximate) nash equilibria in zero-sum matrix games",
    "citation_count": 1,
    "authors": [
      "Hedi Hadiji",
      "Sarah Sachs",
      "Tim van Erven",
      "Wouter M. Koolen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2b00b3331bd0f5fbfdd966ac06338f6d-Abstract-Conference.html": {
    "title": "Fast Conditional Mixing of MCMC Algorithms for Non-log-concave Distributions",
    "volume": "main",
    "abstract": "MCMC algorithms offer empirically efficient tools for sampling from a target distribution $\\pi(x) \\propto \\exp(-V(x))$. However, on the theory side, MCMC algorithms suffer from slow mixing rate when $\\pi(x)$ is non-log-concave. Our work examines this gap and shows that when Poincar\\'e-style inequality holds on a subset $\\mathcal{X}$ of the state space, the conditional distribution of MCMC iterates over $\\mathcal{X}$ mixes fast to the true conditional distribution. This fast mixing guarantee can hold in cases when global mixing is provably slow. We formalize the statement and quantify the conditional mixing rate. We further show that conditional mixing can have interesting implications for sampling from mixtures of Gaussians, parameter estimation for Gaussian mixture models, and Gibbs-sampling with well-connected local minima",
    "keywords": [],
    "checked": true,
    "id": "39c9207e40dc4a55d816905f7c51495cdd9bbdf2",
    "semantic_title": "fast conditional mixing of mcmc algorithms for non-log-concave distributions",
    "citation_count": 0,
    "authors": [
      "Xiang Cheng",
      "Bohan Wang",
      "Jingzhao Zhang",
      "Yusong Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2b09bb02b90584e2be94ff3ae09289bc-Abstract-Conference.html": {
    "title": "How to Select Which Active Learning Strategy is Best Suited for Your Specific Problem and Budget",
    "volume": "main",
    "abstract": "In the domain of Active Learning (AL), a learner actively selects which unlabeled examples to seek labels from an oracle, while operating within predefined budget constraints. Importantly, it has been recently shown that distinct query strategies are better suited for different conditions and budgetary constraints. In practice, the determination of the most appropriate AL strategy for a given situation remains an open problem. To tackle this challenge, we propose a practical derivative-based method that dynamically identifies the best strategy for a given budget. Intuitive motivation for our approach is provided by the theoretical analysis of a simplified scenario. We then introduce a method to dynamically select an AL strategy, which takes into account the unique characteristics of the problem and the available budget. Empirical results showcase the effectiveness of our approach across diverse budgets and computer vision tasks",
    "keywords": [],
    "checked": true,
    "id": "b6261ab0a8ee31fcab0e2675280aa95521586894",
    "semantic_title": "how to select which active learning strategy is best suited for your specific problem and budget",
    "citation_count": 0,
    "authors": [
      "Guy Hacohen",
      "Daphna Weinshall"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2b1d1e5affe5fdb70372cd90dd8afd49-Abstract-Conference.html": {
    "title": "Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback",
    "volume": "main",
    "abstract": "Generative models capable of precisely capturing nuanced clinical features in medical images hold great promise for facilitating clinical data sharing, enhancing rare disease datasets, and efficiently synthesizing (annotated) medical images at scale. Despite their potential, assessing the quality of synthetic medical images remains a challenge. While modern generative models can synthesize visually-realistic medical images, the clinical plausibility of these images may be called into question. Domain-agnostic scores, such as FID score, precision, and recall, cannot incorporate clinical knowledge and are, therefore, not suitable for assessing clinical sensibility. Additionally, there are numerous unpredictable ways in which generative models may fail to synthesize clinically plausible images, making it challenging to anticipate potential failures and design automated scores for their detection. To address these challenges, this paper introduces a pathologist-in-the-loop framework for generating clinically-plausible synthetic medical images. Our framework comprises three steps: (1) pretraining a conditional diffusion model to generate medical images conditioned on a clinical concept, (2) expert pathologist evaluation of the generated images to assess whether they satisfy clinical desiderata, and (3) training a reward model that predicts human feedback on new samples, which we use to incorporate expert knowledge into the finetuning objective of the diffusion model. Our results show that human feedback significantly improves the quality of synthetic images in terms of fidelity, diversity, utility in downstream applications, and plausibility as evaluated by experts. We also demonstrate that human feedback can teach the model new clinical concepts not annotated in the original training data. Our results demonstrate the value of incorporating human feedback in clinical applications where generative models may struggle to capture extensive domain knowledge from raw data alone",
    "keywords": [],
    "checked": true,
    "id": "993f0eb1cf576a50cba4fbc667e10f15d63b07f4",
    "semantic_title": "aligning synthetic medical images with clinical knowledge using human feedback",
    "citation_count": 1,
    "authors": [
      "Shenghuan Sun",
      "Greg Goldgof",
      "Atul Butte",
      "Ahmed M. Alaa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2b2011a7d5396faf5899863d896a3c24-Abstract-Conference.html": {
    "title": "Interpretable Graph Networks Formulate Universal Algebra Conjectures",
    "volume": "main",
    "abstract": "The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA)---one of the fields laying the foundations of modern mathematics---is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhance interpretability without sacrificing task accuracy, (ii) strongly generalize when predicting universal algebra's properties, (iii) generate simple explanations that empirically validate existing conjectures, and (iv) identify subgraphs suggesting the formulation of novel conjectures",
    "keywords": [],
    "checked": true,
    "id": "2f5f35acb2f41ac7a04bf259ca51a2b892c790f8",
    "semantic_title": "interpretable graph networks formulate universal algebra conjectures",
    "citation_count": 0,
    "authors": [
      "Francesco Giannini",
      "Stefano Fioravanti",
      "Oguzhan Keskin",
      "Alisia Lupidi",
      "Lucie Charlotte Magister",
      "Pietro Lió",
      "Pietro Barbiero"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2b25c39788e5cf11d3541de433ebf4c0-Abstract-Conference.html": {
    "title": "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph",
    "volume": "main",
    "abstract": "Adapter-style efficient transfer learning (ETL) has shown excellent performance in the tuning of vision-language models (VLMs) under the low-data regime, where only a few additional parameters are introduced to excavate the task-specific knowledge based on the general and powerful representation of VLMs. However, most adapter-style works face two limitations: (i) modeling task-specific knowledge with a single modality only; and (ii) overlooking the exploitation of the inter-class relationships in downstream tasks, thereby leading to sub-optimal solutions. To mitigate that, we propose an effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dual-modality structure knowledge (i.e., the correlation of different semantics/classes in textual and visual modalities) with a dual knowledge graph. In particular, the dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph, where the nodes and edges represent the semantics/classes and their correlations in two modalities, respectively. This enables the textual feature of each prompt to leverage the task-specific structure knowledge from both textual and visual modalities, yielding a more effective classifier for downstream tasks. Extensive experimental results on 11 benchmark datasets reveal that our GraphAdapter significantly outperforms the previous adapter-based methods",
    "keywords": [],
    "checked": true,
    "id": "8fbe8c18f36f33314a3ee333cfe060ae9f790555",
    "semantic_title": "graphadapter: tuning vision-language models with dual knowledge graph",
    "citation_count": 6,
    "authors": [
      "Xin Li",
      "Dongze Lian",
      "Zhihe Lu",
      "Jiawang Bai",
      "Zhibo Chen",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2b4caf39e645680f826ae0a9e7ae9402-Abstract-Conference.html": {
    "title": "FaceComposer: A Unified Model for Versatile Facial Content Creation",
    "volume": "main",
    "abstract": "This work presents FaceComposer, a unified generative model that accomplishes a variety of facial content creation tasks, including text-conditioned face synthesis, text-guided face editing, face animation etc. Based on the latent diffusion framework, FaceComposer follows the paradigm of compositional generation and employs diverse face-specific conditions, e.g., Identity Feature and Projected Normalized Coordinate Code, to release the model creativity at all possible. To support text control and animation, we clean up some existing face image datasets and collect around 500 hours of talking-face videos, forming a high-quality large-scale multi-modal face database. A temporal self-attention module is incorporated into the U-Net structure, which allows learning the denoising process on the mixture of images and videos. Extensive experiments suggest that our approach not only achieves comparable or even better performance than state-of-the-arts on each single task, but also facilitates some combined tasks with one-time forward, demonstrating its potential in serving as a foundation generative model in face domain. We further develop an interface such that users can enjoy our one-step service to create, edit, and animate their own characters. Code, dataset, model, and interface will be made publicly available",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Wang",
      "Kang Zhao",
      "Yifeng Ma",
      "Shiwei Zhang",
      "Yingya Zhang",
      "Yujun Shen",
      "Deli Zhao",
      "Jingren Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2b5af479527167d4af78847a9b9b645f-Abstract-Conference.html": {
    "title": "A Unified Solution for Privacy and Communication Efficiency in Vertical Federated Learning",
    "volume": "main",
    "abstract": "Vertical Federated Learning (VFL) is a collaborative machine learning paradigm that enables multiple participants to jointly train a model on their private data without sharing it.To make VFL practical, privacy security and communication efficiency should both be satisfied. Recent research has shown that Zero-Order Optimization (ZOO) in VFL can effectively conceal the internal information of the model without adding costly privacy protective add-ons, making it a promising approach for privacy and efficiency.However, there are still two key problems that have yet to be resolved. First, the convergence rate of ZOO-based VFL is significantly slower compared to gradient-based VFL, resulting in low efficiency in model training and more communication round, which hinders its application on large neural networks. Second, although ZOO-based VFL has demonstrated resistance to state-of-the-art (SOTA) attacks, its privacy guarantee lacks a theoretical explanation.To address these challenges, we propose a novel cascaded hybrid optimization approach that employs a zeroth-order (ZO) gradient on the most critical output layer of the clients, with other parts utilizing the first-order (FO) gradient. This approach preserves the privacy protection of ZOO while significantly enhancing convergence.Moreover, we theoretically prove that applying ZOO to the VFL is equivalent to adding Gaussian Mechanism to the gradient information, which offers an implicit differential privacy guarantee. Experimental results demonstrate that our proposed framework achieves similar utility as the Gaussian mechanism under the same privacy budget, while also having significantly lower communication costs compared with SOTA communication-efficient VFL frameworks",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ganyu Wang",
      "Bin Gu",
      "Qingsong Zhang",
      "Xiang Li",
      "Boyu Wang",
      "Charles X. Ling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2b950a297fc888c95bfeb587ef000d70-Abstract-Conference.html": {
    "title": "Optimization and Bayes: A Trade-off for Overparameterized Neural Networks",
    "volume": "main",
    "abstract": "This paper proposes a novel algorithm, Transformative Bayesian Learning (TansBL), which bridges the gap between empirical risk minimization (ERM) and Bayesian learning for neural networks. We compare ERM, which uses gradient descent to optimize, and Bayesian learning with importance sampling for their generalization and computational complexity. We derive the first algorithm-dependent PAC-Bayesian generalization bound for infinitely wide networks based on an exact KL divergence between the trained posterior distribution obtained by infinitesimal step size gradient descent and a Gaussian prior. Moreover, we show how to transform gradient-based optimization into importance sampling by incorporating a weight. While Bayesian learning has better generalization, it suffers from low sampling efficiency. Optimization methods, on the other hand, have good sampling efficiency but poor generalization. Our proposed algorithm TansBL enables a trade-off between generalization and sampling efficiency",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengmian Hu",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2bab8865fa4511e445767e3750b2b5ac-Abstract-Conference.html": {
    "title": "Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests",
    "volume": "main",
    "abstract": "Multiple Instance Learning (MIL) is a sub-domain of classification problems with positive and negative labels and a \"bag\" of inputs, where the label is positive if and only if a positive element is contained within the bag, and otherwise is negative. Training in this context requires associating the bag-wide label to instance-level information, and implicitly contains a causal assumption and asymmetry to the task (i.e., you can't swap the labels without changing the semantics). MIL problems occur in healthcare (one malignant cell indicates cancer), cyber security (one malicious executable makes an infected computer), and many other tasks. In this work, we examine five of the most prominent deep-MIL models and find that none of them respects the standard MIL assumption. They are able to learn anti-correlated instances, i.e., defaulting to \"positive\" labels until seeing a negative counter-example, which should not be possible for a correct MIL model. We suspect that enhancements and other works derived from these models will share the same issue. In any context in which these models are being used, this creates the potential for learning incorrect models, which creates risk of operational failure. We identify and demonstrate this problem via a proposed ``algorithmic unit test'', where we create synthetic datasets that can be solved by a MIL respecting model, and which clearly reveal learning that violates MIL assumptions. The five evaluated methods each fail one or more of these tests. This provides a model-agnostic way to identify violations of modeling assumptions, which we hope will be useful for future development and evaluation of MIL models",
    "keywords": [],
    "checked": true,
    "id": "da61e19d9dad09345f30b242161952f938004467",
    "semantic_title": "reproducibility in multiple instance learning: a case for algorithmic unit tests",
    "citation_count": 0,
    "authors": [
      "Edward Raff",
      "James Holt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2bdc2267c3d7d01523e2e17ac0a754f3-Abstract-Conference.html": {
    "title": "Meta-learning families of plasticity rules in recurrent spiking networks using simulation-based inference",
    "volume": "main",
    "abstract": "There is substantial experimental evidence that learning and memory-related behaviours rely on local synaptic changes, but the search for distinct plasticity rules has been driven by human intuition, with limited success for multiple, co-active plasticity rules in biological networks. More recently, automated meta-learning approaches have been used in simplified settings, such as rate networks and small feed-forward spiking networks. Here, we develop a simulation-based inference (SBI) method for sequentially filtering plasticity rules through an increasingly fine mesh of constraints that can be modified on-the-fly. This method, filter SBI, allows us to infer entire families of complex and co-active plasticity rules in spiking networks. We first consider flexibly parameterized doublet (Hebbian) rules, and find that the set of inferred rules contains solutions that extend and refine -and also reject- predictions from mean-field theory. Next, we expand the search space of plasticity rules by modelling them as multi-layer perceptrons that combine several plasticity-relevant factors, such as weight, voltage, triplets and co-dependency. Out of the millions of possible rules, we identify thousands of unique rule combinations that satisfy biological constraints like plausible activity and weight dynamics. The resulting rules can be used as a starting point for further investigations into specific network computations, and already suggest refinements and predictions for classical experimental approaches on plasticity. This flexible approach for principled exploration of complex plasticity rules in large recurrent spiking networks presents the most advanced search tool to date for enabling robust predictions and deep insights into the plasticity mechanisms underlying brain function",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Basile Confavreux",
      "Poornima Ramesh",
      "Pedro J. Goncalves",
      "Jakob H Macke",
      "Tim Vogels"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2bde8fef08f7ebe42b584266cbcfc909-Abstract-Conference.html": {
    "title": "Joint Training of Deep Ensembles Fails Due to Learner Collusion",
    "volume": "main",
    "abstract": "Ensembles of machine learning models have been well established as a powerful method of improving performance over a single model. Traditionally, ensembling algorithms train their base learners independently or sequentially with the goal of optimizing their joint performance. In the case of deep ensembles of neural networks, we are provided with the opportunity to directly optimize the true objective: the joint performance of the ensemble as a whole. Surprisingly, however, directly minimizing the loss of the ensemble appears to rarely be applied in practice. Instead, most previous research trains individual models independently with ensembling performed post hoc. In this work, we show that this is for good reason - joint optimization of ensemble loss results in degenerate behavior. We approach this problem by decomposing the ensemble objective into the strength of the base learners and the diversity between them. We discover that joint optimization results in a phenomenon in which base learners collude to artificially inflate their apparent diversity. This pseudo-diversity fails to generalize beyond the training data, causing a larger generalization gap. We proceed to comprehensively demonstrate the practical implications of this effect on a range of standard machine learning tasks and architectures by smoothly interpolating between independent training and joint optimization",
    "keywords": [],
    "checked": true,
    "id": "672b6c1e0ec7e920ec7caa103df50b16944b8fa6",
    "semantic_title": "joint training of deep ensembles fails due to learner collusion",
    "citation_count": 2,
    "authors": [
      "Alan Jeffares",
      "Tennison Liu",
      "Jonathan Crabbé",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c23b3c72127e15fedc276722faee927-Abstract-Conference.html": {
    "title": "Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) agents have long sought to approach the efficiency of human learning. Humans are great observers who can learn by aggregating external knowledge from various sources, including observations from others' policies of attempting a task. Prior studies in RL have incorporated external knowledge policies to help agents improve sample efficiency. However, it remains non-trivial to perform arbitrary combinations and replacements of those policies, an essential feature for generalization and transferability. In this work, we present Knowledge-Grounded RL (KGRL), an RL paradigm fusing multiple knowledge policies and aiming for human-like efficiency and flexibility. We propose a new actor architecture for KGRL, Knowledge-Inclusive Attention Network (KIAN), which allows free knowledge rearrangement due to embedding-based attentive action prediction. KIAN also addresses entropy imbalance, a problem arising in maximum entropy KGRL that hinders an agent from efficiently exploring the environment, through a new design of policy distributions. The experimental results demonstrate that KIAN outperforms alternative methods incorporating external knowledge policies and achieves efficient and flexible learning. Our implementation is available at https://github.com/Pascalson/KGRL.git",
    "keywords": [],
    "checked": true,
    "id": "579cd3b80cb633fd23ff6d6b25dd754a197b3e9e",
    "semantic_title": "flexible attention-based multi-policy fusion for efficient deep reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Zih-Yun Chiu",
      "Yi-Lin Tuan",
      "William Yang Wang",
      "Michael Yip"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c28efa5a86dca4b603a36c08f49f240-Abstract-Conference.html": {
    "title": "Balanced Training for Sparse GANs",
    "volume": "main",
    "abstract": "Over the past few years, there has been growing interest in developing larger and deeper neural networks, including deep generative models like generative adversarial networks (GANs). However, GANs typically come with high computational complexity, leading researchers to explore methods for reducing the training and inference costs. One such approach gaining popularity in supervised learning is dynamic sparse training (DST), which maintains good performance while enjoying excellent training efficiency. Despite its potential benefits, applying DST to GANs presents challenges due to the adversarial nature of the training process. In this paper, we propose a novel metric called the balance ratio (BR) to study the balance between the sparse generator and discriminator. We also introduce a new method called balanced dynamic sparse training (ADAPT), which seeks to control the BR during GAN training to achieve a good trade-off between performance and computational cost. Our proposed method shows promising results on multiple datasets, demonstrating its effectiveness",
    "keywords": [],
    "checked": true,
    "id": "ac74bd92a9413d4b2be40647c7cb7725bc62029e",
    "semantic_title": "balanced training for sparse gans",
    "citation_count": 0,
    "authors": [
      "Yite Wang",
      "Jing Wu",
      "NAIRA HOVAKIMYAN",
      "Ruoyu Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c53bc01e30711a08f6ac86919193022-Abstract-Conference.html": {
    "title": "Policy Optimization for Continuous Reinforcement Learning",
    "volume": "main",
    "abstract": "We study reinforcement learning (RL) in the setting of continuous time and space, for an infinite horizon with a discounted objective and the underlying dynamics driven by a stochastic differential equation. Built upon recent advances in the continuous approach to RL, we develop a notion of occupation time (specifically for a discounted objective), and show how it can be effectively used to derive performance difference and local approximation formulas. We further extend these results to illustrate their applications in the PG (policy gradient) and TRPO/PPO (trust region policy optimization/ proximal policy optimization) methods, which have been familiar and powerful tools in the discrete RL setting but under-developed in continuous RL. Through numerical experiments, we demonstrate the effectiveness and advantages of our approach",
    "keywords": [],
    "checked": true,
    "id": "5b84f988ba68559a6580ffcfafc93fc09d53ae96",
    "semantic_title": "policy optimization for continuous reinforcement learning",
    "citation_count": 4,
    "authors": [
      "HANYANG ZHAO",
      "Wenpin Tang",
      "David Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c575c088de5cfef858b8837251f3027-Abstract-Conference.html": {
    "title": "PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation",
    "volume": "main",
    "abstract": "We present PrimDiffusion, the first diffusion-based framework for 3D human generation. Devising diffusion models for 3D human generation is difficult due to the intensive computational cost of 3D representations and the articulated topology of 3D humans. To tackle these challenges, our key insight is operating the denoising diffusion process directly on a set of volumetric primitives, which models the human body as a number of small volumes with radiance and kinematic information. This volumetric primitives representation marries the capacity of volumetric representations with the efficiency of primitive-based rendering. Our PrimDiffusion framework has three appealing properties: **1)** compact and expressive parameter space for the diffusion model, **2)** flexible representation that incorporates human prior, and **3)** decoder-free rendering for efficient novel-view and novel-pose synthesis. Extensive experiments validate that PrimDiffusion outperforms state-of-the-art methods in 3D human generation. Notably, compared to GAN-based methods, our PrimDiffusion supports real-time rendering of high-quality 3D humans at a resolution of $512\\times512$ once the denoising process is done. We also demonstrate the flexibility of our framework on training-free conditional generation such as texture transfer and 3D inpainting",
    "keywords": [],
    "checked": true,
    "id": "7d4dc7088f29fb7be571b9b8417f598b6c9c01bd",
    "semantic_title": "primdiffusion: volumetric primitives diffusion for 3d human generation",
    "citation_count": 2,
    "authors": [
      "Zhaoxi Chen",
      "Fangzhou Hong",
      "Haiyi Mei",
      "Guangcong Wang",
      "Lei Yang",
      "Ziwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c6be9f09e08ca166cdc0aa26306c61f-Abstract-Conference.html": {
    "title": "A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety measures beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related objectives is less-explored. Driven by the above, this work comprehensively investigates the safety measures of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study $83$ CLIP models and $127$ ImageNet classifiers. They are diverse in architecture (pre)training distribution and training strategies. We consider $10$ visual factors (\\emph{e.g.}, shape and pattern), $5$ types of out-of-distribution data, and $8$ natural and challenging test conditions with different shift types, such as texture, style, and perturbation shifts. Our study has unveiled several previously unknown insights into CLIP models. For instance, they are not consistently more calibrated than other ImageNet models, which contradicts existing findings. Additionally, our analysis underscores the significance of training source design by showcasing its profound influence on the three key properties. We believe our comprehensive study can shed light on and help guide the development of more robust and reliable CLIP models",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Tu",
      "Weijian Deng",
      "Tom Gedeon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c71b14637802ed08eaa3cf50342b2b9-Abstract-Conference.html": {
    "title": "Model Spider: Learning to Rank Pre-Trained Models Efficiently",
    "volume": "main",
    "abstract": "Figuring out which Pre-Trained Model (PTM) from a model zoo fits the target task is essential to take advantage of plentiful model resources. With the availability of numerous heterogeneous PTMs from diverse fields, efficiently selecting the most suitable one is challenging due to the time-consuming costs of carrying out forward or backward passes over all PTMs. In this paper, we propose Model Spider, which tokenizes both PTMs and tasks by summarizing their characteristics into vectors to enable efficient PTM selection. By leveraging the approximated performance of PTMs on a separate set of training tasks, Model Spider learns to construct representation and measure the fitness score between a model-task pair via their representation. The ability to rank relevant PTMs higher than others generalizes to new tasks. With the top-ranked PTM candidates, we further learn to enrich task repr. with their PTM-specific semantics to re-rank the PTMs for better selection. Model Spider balances efficiency and selection ability, making PTM selection like a spider preying on a web. Model Spider exhibits promising performance across diverse model zoos, including visual models and Large Language Models (LLMs). Code is available at https://github.com/zhangyikaii/Model-Spider",
    "keywords": [],
    "checked": true,
    "id": "bb2806625aa469af034727c4df71c33c9746ec85",
    "semantic_title": "model spider: learning to rank pre-trained models efficiently",
    "citation_count": 2,
    "authors": [
      "Yi-Kai Zhang",
      "Ting-Ji Huang",
      "Yao-Xiang Ding",
      "De-Chuan Zhan",
      "Han-Jia Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c74f005aabbf90a8f1747d99f387321-Abstract-Conference.html": {
    "title": "Investigating how ReLU-networks encode symmetries",
    "volume": "main",
    "abstract": "Many data symmetries can be described in terms of group equivariance and the most common way of encoding group equivariances in neural networks is by building linear layers that are group equivariant.In this work we investigate whether equivariance of a network implies that all layers are equivariant.On the theoretical side we find cases where equivariance implies layerwise equivariance, but alsodemonstrate that this is not the case generally.Nevertheless, we conjecture that CNNs that are trained to be equivariant will exhibit layerwise equivariance and explain how this conjecture is a weaker version of the recent permutation conjecture by Entezari et al.\\ [2022].We perform quantitative experiments with VGG-nets on CIFAR10 and qualitative experiments with ResNets on ImageNet to illustrate and support our theoretical findings. These experiments are not only of interest for understanding how group equivariance is encoded in ReLU-networks, but they also give a new perspective on Entezari et al.'s permutation conjecture as we find that itis typically easier to merge a network with a group-transformed version of itself than merging two different networks",
    "keywords": [],
    "checked": true,
    "id": "8659a5e60a3e9c81b4123bc9ad94f2b85162385b",
    "semantic_title": "investigating how relu-networks encode symmetries",
    "citation_count": 2,
    "authors": [
      "Georg Bökman",
      "Fredrik Kahl"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c7967a442300bff58e9d7b73aa26f24-Abstract-Conference.html": {
    "title": "Optimal and Fair Encouragement Policy Evaluation and Learning",
    "volume": "main",
    "abstract": "In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal policy rules are merely suggestions in the presence of human non-adherence to treatment recommendations. In these same domains, there may be heterogeneity both in who responds in taking-up treatment, and heterogeneity in treatment efficacy. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When in addition the decision-maker has distributional preferences over both access and average outcomes, the optimal decision rule changes. We study identification, doubly-robust estimation, and robust estimation under potential violations of positivity. We consider fairness constraints such as demographic parity in treatment take-up, and other constraints, via constrained optimization. Our framework can be extended to handle algorithmic recommendations under an often-reasonable covariate-conditional exclusion restriction, using our robustness checks for lack of positivity in the recommendation. We develop a two-stage, online learning-based algorithm for solving over parametrized policy classes under general constraints to obtain variance-sensitive regret bounds. We assess improved recommendation rules in a stylized case study of optimizing recommendation of supervised release in the PSA-DMF pretrial risk-assessment tool while reducing surveillance disparities",
    "keywords": [],
    "checked": true,
    "id": "958450d279a5e90dcc34fef552002735a0d188d9",
    "semantic_title": "optimal and fair encouragement policy evaluation and learning",
    "citation_count": 0,
    "authors": [
      "Angela Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c8047bf3ed8ef6905351608d641f02f-Abstract-Conference.html": {
    "title": "NICE: NoIse-modulated Consistency rEgularization for Data-Efficient GANs",
    "volume": "main",
    "abstract": "Generative Adversarial Networks (GANs) are powerful tools for image synthesis. However, they require access to vast amounts of training data, which is often costly and prohibitive. Limited data affects GANs, leading to discriminator overfitting and training instability. In this paper, we present a novel approach called NoIse-modulated Consistency rEgularization (NICE) to overcome these challenges. To this end, we introduce an adaptive multiplicative noise into the discriminator to modulate its latent features. We demonstrate the effectiveness of such a modulation in preventing discriminator overfitting by adaptively reducing the Rademacher complexity of the discriminator. However, this modulation leads to an unintended consequence of increased gradient norm, which can undermine the stability of GAN training. To mitigate this undesirable effect, we impose a constraint on the discriminator, ensuring its consistency for the same inputs under different noise modulations. The constraint effectively penalizes the first and second-order gradients of latent features, enhancing GAN stability. Experimental evidence aligns with our theoretical analysis, demonstrating the reduction of generalization error and gradient penalization of NICE. This substantiates the efficacy of NICE in reducing discriminator overfitting and improving stability of GAN training. NICE achieves state-of-the-art results on CIFAR-10, CIFAR-100, ImageNet and FFHQ datasets when trained with limited data, as well as in low-shot generation tasks",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Ni",
      "Piotr Koniusz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2c8d9636f74d0207ff4f65956010f450-Abstract-Conference.html": {
    "title": "Not All Out-of-Distribution Data Are Harmful to Open-Set Active Learning",
    "volume": "main",
    "abstract": "Active learning (AL) methods have been proven to be an effective way to reduce the labeling effort by intelligently selecting valuable instances for annotation. Despite their great success with in-distribution (ID) scenarios, AL methods suffer from performance degradation in many real-world applications because out-of-distribution (OOD) instances are always inevitably contained in unlabeled data, which may lead to inefficient sampling. Therefore, several attempts have been explored open-set AL by strategically selecting pure ID instances while filtering OOD instances. However, concentrating solely on selecting pseudo-ID instances may cause the training constraint of the ID classifier and OOD detector. To address this issue, we propose a simple yet effective sampling scheme, Progressive Active Learning (PAL), which employs a progressive sampling mechanism to leverage the active selection of valuable OOD instances. The proposed PAL measures unlabeled instances by synergistically evaluating instances' informativeness and representativeness, and thus it can balance the pseudo-ID and pseudo-OOD instances in each round to enhance both the capacity of the ID classifier and the OOD detector. %Meanwhile, PAL measures unlabeled instances by synergistically evaluating instances' informativeness and representativeness, which can more effectively estimate the values of instances. Extensive experiments on various open-set AL scenarios demonstrate the effectiveness of the proposed PAL, compared with the state-of-the-art methods. The code is available at \\url{https://github.com/njustkmg/PAL}",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yang",
      "Yuxuan Zhang",
      "XIN SONG",
      "Yi Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2ceda49041816da6d5a34eb3b612607f-Abstract-Conference.html": {
    "title": "Improving the Privacy and Practicality of Objective Perturbation for Differentially Private Linear Learners",
    "volume": "main",
    "abstract": "In the arena of privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) has outstripped the objective perturbation mechanism in popularity and interest. Though unrivaled in versatility, DP-SGD requires a non-trivial privacy overhead (for privately tuning the model's hyperparameters) and a computational complexity which might be extravagant for simple models such as linear and logistic regression. This paper revamps the objective perturbation mechanism with tighter privacy analyses and new computational tools that boost it to perform competitively with DP-SGD on unconstrained convex generalized linear problems",
    "keywords": [],
    "checked": true,
    "id": "fc5e3cc10ee2f901150210e533adc9255e1f0cca",
    "semantic_title": "improving the privacy and practicality of objective perturbation for differentially private linear learners",
    "citation_count": 1,
    "authors": [
      "Rachel Redberg",
      "Antti Koskela",
      "Yu-Xiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2cf153951b5e9b39564fc4a0ef6adc1a-Abstract-Conference.html": {
    "title": "Implicit Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis",
    "volume": "main",
    "abstract": "Deep network models are often purely inductive during both training and inference on unseen data. When these models are used for prediction, but they may fail to capture important semantic information and implicit dependencies within datasets. Recent advancements have shown that combining multiple modalities in large-scale vision and language settings can improve understanding and generalization performance. However, as the model size increases, fine-tuning and deployment become computationally expensive, even for a small number of downstream tasks. Moreover, it is still unclear how domain or prior modal knowledge can be specified in a backpropagation friendly manner, especially in large-scale and noisy settings. To address these challenges, we propose a simplified alternative of combining features from pretrained deep networks and freely available semantic explicit knowledge. In order to remove irrelevant explicit knowledge that does not correspond well to the images, we introduce an implicit Differentiable Out-of-Distribution (OOD) detection layer. This layer addresses outlier detection by solving for fixed points of a differentiable function and using the last iterate of fixed point solver to backpropagate. In practice, we apply our model on several vision and language downstream tasks including visual question answering, visual reasoning, and image-text retrieval on different datasets. Our experiments show that it is possible to design models that perform similarly to state-of-the-art results but with significantly fewer samples and less training time. Our models and code are available here: https://github.com/ellenzhuwang/implicit_vkood",
    "keywords": [],
    "checked": false,
    "id": "6c194ca1bcee1fab2dfb289dadcdd9a829b736df",
    "semantic_title": "differentiable outlier detection enable robust deep multimodal analysis",
    "citation_count": 0,
    "authors": [
      "Zhu Wang",
      "Sourav Medya",
      "Sathya Ravi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2d025936bae21d2c2d4cc74779aa77c7-Abstract-Conference.html": {
    "title": "DSR: Dynamical Surface Representation as Implicit Neural Networks for Protein",
    "volume": "main",
    "abstract": "We propose a novel neural network-based approach to modeling protein dynamics using an implicit representation of a protein's surface in 3D and time. Our method utilizes the zero-level set of signed distance functions (SDFs) to represent protein surfaces, enabling temporally and spatially continuous representations of protein dynamics. Our experimental results demonstrate that our model accurately captures protein dynamic trajectories and can interpolate and extrapolate in 3D and time. Importantly, this is the first study to introduce this method and successfully model large-scale protein dynamics. This approach offers a promising alternative to current methods, overcoming the limitations of first-principles-based and deep learning methods, and provides a more scalable and efficient approach to modeling protein dynamics. Additionally, our surface representation approach simplifies calculations and allows identifying movement trends and amplitudes of protein domains, making it a useful tool for protein dynamics research. Codes are available at https://github.com/Sundw-818/DSR, and we have a project webpage that shows some video results, https://sundw-818.github.io/DSR/",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiwen Sun",
      "He Huang",
      "Yao Li",
      "Xinqi Gong",
      "Qiwei Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2d0842550e6d92b0e27e7e810b1a4792-Abstract-Conference.html": {
    "title": "A Theory of Transfer-Based Black-Box Attacks: Explanation and Implications",
    "volume": "main",
    "abstract": "Transfer-based attacks are a practical method of black-box adversarial attacks, in which the attacker aims to craft adversarial examples from a source (surrogate) model that is transferable to the target model. A wide range of empirical works has tried to explain the transferability of adversarial examples from different angles. However, these works only provide ad hoc explanations without quantitative analyses. The theory behind transfer-based attacks remains a mystery.This paper studies transfer-based attacks under a unified theoretical framework. We propose an explanatory model, called the manifold attack model, that formalizes popular beliefs and explains the existing empirical results. Our model explains why adversarial examples are transferable even when the source model is inaccurate. Moreover, our model implies that the existence of transferable adversarial examples depends on the \"curvature\" of the data manifold, which quantitatively explains why the success rates of transfer-based attacks are hard to improve. We also discuss the expressive power and the possible extensions of our model in general applications",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbo Chen",
      "Weiwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2d1ef4aba0503226330661d74fdb236e-Abstract-Conference.html": {
    "title": "Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity.While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry.To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that more comprehensively explain V1 activity.Upon enhancing task-driven CNNs with architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification, we uncover models with latent representations that yield state-of-the-art explanation of V1 neural activity and tuning properties.Moreover, analyses of the learned parameters of these components and stimuli that maximally activate neurons of the evaluated networks provide support for their role in explaining neural properties of V1.Our results highlight an important advancement in the field of NeuroAI, as we systematically establish a set of architectural components that contribute to unprecedented explanation of V1.The neuroscience insights that could be gleaned from increasingly accurate in-silico models of the brain have the potential to greatly advance the fields of both neuroscience and artificial intelligence",
    "keywords": [],
    "checked": true,
    "id": "85e49127646e4961b0234487b2d6ba128a6a9102",
    "semantic_title": "explaining v1 properties with a biologically constrained deep learning architecture",
    "citation_count": 0,
    "authors": [
      "Galen Pogoncheff",
      "Jacob Granley",
      "Michael Beyeler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2d3b007613940def7a5ec9d6d635937b-Abstract-Conference.html": {
    "title": "Revisiting Adversarial Training for ImageNet: Architectures, Training and Generalization across Threat Models",
    "volume": "main",
    "abstract": "While adversarial training has been extensively studied for ResNet architectures and low resolution datasets like CIFAR-10, much less is known for ImageNet. Given the recent debate about whether transformers are more robust than convnets, we revisit adversarial training on ImageNet comparing ViTs and ConvNeXts. Extensive experiments show that minor changes in architecture, most notably replacing PatchStem with ConvStem, and training scheme have a significant impact on the achieved robustness. These changes not only increase robustness in the seen $\\ell_\\infty$-threat model, but even more so improve generalization to unseen $\\ell_1/\\ell_2$-attacks. Our modified ConvNeXt, ConvNeXt + ConvStem, yields the most robust $\\ell_\\infty$-models across different ranges of model parameters and FLOPs, while our ViT + ConvStem yields the best generalization to unseen threat models",
    "keywords": [],
    "checked": true,
    "id": "a2c041f9c32ce382735648f584cf4332ff0a6e33",
    "semantic_title": "revisiting adversarial training for imagenet: architectures, training and generalization across threat models",
    "citation_count": 18,
    "authors": [
      "Naman Deep Singh",
      "Francesco Croce",
      "Matthias Hein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2d52879ef2ba487445ca2e143b104c3b-Abstract-Conference.html": {
    "title": "FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing",
    "volume": "main",
    "abstract": "Text-driven motion generation has achieved substantial progress with the emergence of diffusion models. However, existing methods still struggle to generate complex motion sequences that correspond to fine-grained descriptions, depicting detailed and accurate spatio-temporal actions.This lack of fine controllability limits the usage of motion generation to a larger audience. To tackle these challenges, we present FineMoGen, a diffusion-based motion generation and editing framework that can synthesize fine-grained motions, with spatial-temporal composition to the user instructions. Specifically, FineMoGen builds upon diffusion model with a novel transformer architecture dubbed Spatio-Temporal Mixture Attention SAMI. SAMI optimizes the generation of the global attention template from two perspectives: 1) explicitly modeling the constraints of spatio-temporal composition; and 2) utilizing sparsely-activated mixture-of-experts to adaptively extract fine-grained features. To facilitate a large-scale study on this new fine-grained motion generation task, we contribute the HuMMan-MoGen dataset, which consists of 2,968 videos and 102,336 fine-grained spatio-temporal descriptions. Extensive experiments validate that FineMoGen exhibits superior motion generation quality over state-of-the-art methods. Notably, FineMoGen further enables zero-shot motion editing capabilities with the aid of modern large language models (LLM), which faithfully manipulates motion sequences with fine-grained instructions",
    "keywords": [],
    "checked": true,
    "id": "bba74dbce3d789aad00953a9dae8b6037b4446ea",
    "semantic_title": "finemogen: fine-grained spatio-temporal motion generation and editing",
    "citation_count": 1,
    "authors": [
      "Mingyuan Zhang",
      "Huirong Li",
      "Zhongang Cai",
      "Jiawei Ren",
      "Lei Yang",
      "Ziwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2d6336c1c2987e9d1d9894edd593478d-Abstract-Conference.html": {
    "title": "Stabilizing the Optimization of Neural Signed Distance Functions and Finer Shape Representation",
    "volume": "main",
    "abstract": "We present new insights and a novel paradigm for learning implicit neural representations (INR) of shapes. In particular, we shed light on the popular eikonal loss used for imposing a signed distance function constraint in INR. We show analytically that as the representation power of the network increases, the optimization approaches a partial differential equation (PDE) in the continuum limit that is unstable. We show that this instability can manifest in existing network optimization, leading to irregularities in the reconstructed surface and/or convergence to sub-optimal local minima, and thus fails to capture fine geometric and topological structure. We show analytically how other terms added to the loss, currently used in the literature for other purposes, can actually eliminate these instabilities. However, such terms can over-regularize the surface, preventing the representation of fine shape detail. Based on a similar PDE theory for the continuum limit, we introduce a new regularization term that still counteracts the eikonal instability but without over-regularizing. Furthermore, since stability is now guaranteed in the continuum limit, this stabilization also allows for considering new network structures that are able to represent finer shape detail. We introduce such a structure based on quadratic layers. Experiments on multiple benchmark data sets show that our new regularization and network are able to capture more precise shape details and more accurate topology than existing state-of-the-art",
    "keywords": [],
    "checked": false,
    "id": "fbc2a1007fa003265b88707c4926be34b951243f",
    "semantic_title": "steik: stabilizing the optimization of neural signed distance functions and finer shape representation",
    "citation_count": 1,
    "authors": [
      "Huizong Yang",
      "Yuxin Sun",
      "Ganesh Sundaramoorthi",
      "Anthony Yezzi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2d8911db9ecedf866015091b28946e15-Abstract-Conference.html": {
    "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
    "volume": "main",
    "abstract": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9\\% vs 1.9\\% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}",
    "keywords": [],
    "checked": true,
    "id": "a9e00c216ce69325a15fd139da0624978e54058a",
    "semantic_title": "voicebox: text-guided multilingual universal speech generation at scale",
    "citation_count": 49,
    "authors": [
      "Matthew Le",
      "Apoorv Vyas",
      "Bowen Shi",
      "Brian Karrer",
      "Leda Sari",
      "Rashel Moritz",
      "Mary Williamson",
      "Vimal Manohar",
      "Yossi Adi",
      "Jay Mahadeokar",
      "Wei-Ning Hsu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2d950a2cfd8a75124c178a89545b97fd-Abstract-Conference.html": {
    "title": "Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Method",
    "volume": "main",
    "abstract": "Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions.In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. As a byproduct of our analysis we introduce a novel regularization process over vanilla gradient descent and provide theoretical and experimental evidence that it helps address vanishing-gradient issues and escape bad stationary points",
    "keywords": [],
    "checked": false,
    "id": "c5fe40da42a4df3b58ab2a4204922dcf76368053",
    "semantic_title": "optimizing solution-samplers for combinatorial problems: the landscape of policy-gradient methods",
    "citation_count": 0,
    "authors": [
      "Constantine Caramanis",
      "Dimitris Fotakis",
      "Alkis Kalavasis",
      "Vasilis Kontonis",
      "Christos Tzamos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2da53cd1abdae59150e35f4693834f32-Abstract-Conference.html": {
    "title": "SoTTA: Robust Test-Time Adaptation on Noisy Data Streams",
    "volume": "main",
    "abstract": "Test-time adaptation (TTA) aims to address distributional shifts between training and testing data using only unlabeled test data streams for continual model adaptation. However, most TTA methods assume benign test streams, while test samples could be unexpectedly diverse in the wild. For instance, an unseen object or noise could appear in autonomous driving. This leads to a new threat to existing TTA algorithms; we found that prior TTA algorithms suffer from those noisy test samples as they blindly adapt to incoming samples. To address this problem, we present Screening-out Test-Time Adaptation (SoTTA), a novel TTA algorithm that is robust to noisy samples. The key enabler of SoTTA is two-fold: (i) input-wise robustness via high-confidence uniform-class sampling that effectively filters out the impact of noisy samples and (ii) parameter-wise robustness via entropy-sharpness minimization that improves the robustness of model parameters against large gradients from noisy samples. Our evaluation with standard TTA benchmarks with various noisy scenarios shows that our method outperforms state-of-the-art TTA methods under the presence of noisy samples and achieves comparable accuracy to those methods without noisy samples. The source code is available at https://github.com/taeckyung/SoTTA",
    "keywords": [],
    "checked": true,
    "id": "ec60f3875cb91fd777d2ff62d77090caa94af694",
    "semantic_title": "sotta: robust test-time adaptation on noisy data streams",
    "citation_count": 1,
    "authors": [
      "Taesik Gong",
      "Yewon Kim",
      "Taeckyung Lee",
      "Sorn Chottananurak",
      "Sung-Ju Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2dae7d1ccf1edf76f8ce7c282bdf4730-Abstract-Conference.html": {
    "title": "FouriDown: Factoring Down-Sampling into Shuffling and Superposing",
    "volume": "main",
    "abstract": "Spatial down-sampling techniques, such as strided convolution, Gaussian, and Nearest down-sampling, are essential in deep neural networks. In this study, we revisit the working mechanism of the spatial down-sampling family and analyze the biased effects caused by the static weighting strategy employed in previous approaches. To overcome this limitation, we propose a novel down-sampling paradigm in the Fourier domain, abbreviated as FouriDown, which unifies existing down-sampling techniques. Drawing inspiration from the signal sampling theorem, we parameterize the non-parameter static weighting down-sampling operator as a learnable and context-adaptive operator within a unified Fourier function. Specifically, we organize the corresponding frequency positions of the 2D plane in a physically-closed manner within a single channel dimension. We then perform point-wise channel shuffling based on an indicator that determines whether a channel's signal frequency bin is susceptible to aliasing, ensuring the consistency of the weighting parameter learning. FouriDown, as a generic operator, comprises four key components: 2D discrete Fourier transform, context shuffling rules, Fourier weighting-adaptively superposing rules, and 2D inverse Fourier transform. These components can be easily integrated into existing image restoration networks. To demonstrate the efficacy of FouriDown, we conduct extensive experiments on image de-blurring and low-light image enhancement. The results consistently show that FouriDown can provide significant performance improvements. We will make the code publicly available to facilitate further exploration and application of FouriDown",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhu",
      "man zhou",
      "Jie Huang",
      "Naishan Zheng",
      "Hongzhi Gao",
      "Chongyi Li",
      "Yuan Xu",
      "Feng Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2dbb8bfe4cd3875609b23799830ee865-Abstract-Conference.html": {
    "title": "Participatory Personalization in Classification",
    "volume": "main",
    "abstract": "Machine learning models are often personalized based on information that is protected, sensitive, self-reported, or costly to acquire. These models use information about people, but do not facilitate nor inform their consent. Individuals cannot opt out of reporting information that a model needs to personalize their predictions nor tell if they benefit from personalization in the first place. We introduce a new family of prediction models, called participatory systems, that let individuals opt into personalization at prediction time. We present a model-agnostic algorithm to learn participatory systems for supervised learning tasks where models are personalized with categorical group attributes. We conduct a comprehensive empirical study of participatory systems in clinical prediction tasks, comparing them to common approaches for personalization and imputation. Our results show that participatory systems can facilitate and inform consent in a way that improves performance and privacy across all groups who report personal data",
    "keywords": [],
    "checked": true,
    "id": "04a20a27a5b39749a0fbdb752f07109a55cf9ee2",
    "semantic_title": "participatory personalization in classification",
    "citation_count": 0,
    "authors": [
      "Hailey Joren",
      "Chirag Nagpal",
      "Katherine A. Heller",
      "Berk Ustun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2dd8a2a8685602586c1173f0b644d0e3-Abstract-Conference.html": {
    "title": "A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the \"Neural Collapse\" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an \"optimistic\" mathematical model requires that the graphs obey a strict structural condition in order to possess a minimizer with exact collapse. Furthermore, by studying the gradient dynamics of this model, we provide reasoning for the partial collapse observed empirically. Finally, we present a study on the evolution of within- and between-class feature variability across layers of a well-trained GNN and contrast the behavior with spectral methods",
    "keywords": [],
    "checked": true,
    "id": "278a5e27f70347c78d5ff6131a276883a9e5a42e",
    "semantic_title": "a neural collapse perspective on feature evolution in graph neural networks",
    "citation_count": 0,
    "authors": [
      "Vignesh Kothapalli",
      "Tom Tirer",
      "Joan Bruna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e0bd92a1d3600d4288df51ac5e6be5f-Abstract-Conference.html": {
    "title": "ResoNet: Noise-Trained Physics-Informed MRI Off-Resonance Correction",
    "volume": "main",
    "abstract": "Magnetic Resonance Imaging (MRI) is a powerful medical imaging modality that offers diagnostic information without harmful ionizing radiation. Unlike optical imaging, MRI sequentially samples the spatial Fourier domain (k-space) of the image. Measurements are collected in multiple shots, or readouts, and in each shot, data along a smooth trajectory is sampled.Conventional MRI data acquisition relies on sampling k-space row-by-row in short intervals, which is slow and inefficient. More efficient, non-Cartesian sampling trajectories (e.g., Spirals) use longer data readout intervals, but are more susceptible to magnetic field inhomogeneities, leading to off-resonance artifacts. Spiral trajectories cause off-resonance blurring in the image, and the mathematics of this blurring resembles that of optical blurring, where magnetic field variation corresponds to depth and readout duration to aperture size. Off-resonance blurring is a system issue with a physics-based, accurate forward model. We present a physics-informed deep learning framework for off-resonance correction in MRI, which is trained exclusively on synthetic, noise-like data with representative marginal statistics. Our approach allows for fat/water separation and is compatible with parallel imaging acceleration. Through end-to-end training using synthetic randomized data (i.e., noise-like images, coil sensitivities, field maps), we train the network to reverse off-resonance effects across diverse anatomies and contrasts without retraining. We demonstrate the effectiveness of our approach through results on phantom and in-vivo data. This work has the potential to facilitate the clinical adoption of non-Cartesian sampling trajectories, enabling efficient, rapid, and motion-robust MRI scans. Code is publicly available at: https://github.com/mikgroup/ResoNet",
    "keywords": [],
    "checked": false,
    "id": "1fddeff40140739228e242dcd2da53517dc72668",
    "semantic_title": "resonet: physics informed deep learning based off-resonance correction trained on synthetic data",
    "citation_count": 1,
    "authors": [
      "Alfredo De Goyeneche Macaya",
      "Shreya Ramachandran",
      "Ke Wang",
      "Ekin Karasan",
      "Joseph Y. Cheng",
      "Stella X. Yu",
      "Michael Lustig"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e0d3c6ad1a4d85bef3cfe63af58bc76-Abstract-Conference.html": {
    "title": "Eliminating Domain Bias for Federated Learning in Representation Space",
    "volume": "main",
    "abstract": "Recently, federated learning (FL) is popular for its privacy-preserving and collaborative learning abilities. However, under statistically heterogeneous scenarios, we observe that biased data domains on clients cause a representation bias phenomenon and further degenerate generic representations during local training, i.e., the representation degeneration phenomenon. To address these issues, we propose a general framework Domain Bias Eliminator (DBE) for FL. Our theoretical analysis reveals that DBE can promote bi-directional knowledge transfer between server and client, as it reduces the domain discrepancy between server and client in representation space. Besides, extensive experiments on four datasets show that DBE can greatly improve existing FL methods in both generalization and personalization abilities. The DBE-equipped FL method can outperform ten state-of-the-art personalized FL methods by a large margin. Our code is public at https://github.com/TsingZ0/DBE",
    "keywords": [],
    "checked": true,
    "id": "8952a28279e8452ba7ac0cc7e1e5d676ccf94616",
    "semantic_title": "eliminating domain bias for federated learning in representation space",
    "citation_count": 1,
    "authors": [
      "Jianqing Zhang",
      "Yang Hua",
      "Jian Cao",
      "Hao Wang",
      "Tao Song",
      "Zhengui XUE",
      "Ruhui Ma",
      "Haibing Guan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e10b2c2e1aa4f8083c37dfe269873f8-Abstract-Conference.html": {
    "title": "Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression",
    "volume": "main",
    "abstract": "Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally new tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a task diversity threshold for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks, instead behaving like a Bayesian estimator with the non-diverse pretraining task distribution as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over all tasks, including those not seen during pretraining. Thus, when pretrained on data with task diversity greater than the threshold, transformers can optimally solve fundamentally new tasks in-context. Importantly, this capability hinges on it deviating from the Bayes optimal estimator with the pretraining distribution as the prior. This study also explores the effect of regularization, model capacity and task structure and underscores, in a concrete example, the critical role of task diversity, alongside data and model scale, in the emergence of ICL",
    "keywords": [],
    "checked": true,
    "id": "4c60ce3e5116037390b3b92866f43df83f3e9c6f",
    "semantic_title": "pretraining task diversity and the emergence of non-bayesian in-context learning for regression",
    "citation_count": 16,
    "authors": [
      "Allan Raventós",
      "Mansheej Paul",
      "Feng Chen",
      "Surya Ganguli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e14be0332c04c76742710e417cedb2a-Abstract-Conference.html": {
    "title": "Two-Stage Predict+Optimize for MILPs with Unknown Parameters in Constraints",
    "volume": "main",
    "abstract": "Consider the setting of constrained optimization, with some parameters unknown at solving time and requiring prediction from relevant features. Predict+Optimize is a recent framework for end-to-end training supervised learning models for such predictions, incorporating information about the optimization problem in the training process in order to yield better predictions in terms of the quality of the predicted solution under the true parameters. Almost all prior works have focused on the special case where the unknowns appear only in the optimization objective and not the constraints. Hu et al. proposed the first adaptation of Predict+Optimize to handle unknowns appearing in constraints, but the framework has somewhat ad-hoc elements, and they provided a training algorithm only for covering and packing linear programs. In this work, we give a new simpler and more powerful framework called Two-Stage Predict+Optimize, which we believe should be the canonical framework for the Predict+Optimize setting. We also give a training algorithm usable for all mixed integer linear programs, vastly generalizing the applicability of the framework. Experimental results demonstrate the superior prediction performance of our training framework over all classical and state-of-the-art methods",
    "keywords": [],
    "checked": false,
    "id": "7b6937eb7ae6636b56b65c035a8cf1b58d165e2c",
    "semantic_title": "two-stage predict+optimize for mixed integer linear programs with unknown parameters in constraints",
    "citation_count": 0,
    "authors": [
      "Xinyi Hu",
      "Jasper Lee",
      "Jimmy Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e19dab94882bc95ed094c4399cfda02-Abstract-Conference.html": {
    "title": "Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective",
    "volume": "main",
    "abstract": "Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to \\textbf{SAN}, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiding Liu",
      "Mingyue Cheng",
      "Zhi Li",
      "Zhenya Huang",
      "Qi Liu",
      "Yanhu Xie",
      "Enhong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e2e7c2e3c2e70fa2e9756dce728fcca-Abstract-Conference.html": {
    "title": "Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power",
    "volume": "main",
    "abstract": "The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs---$d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs, based on the well-known FWL(2) algorithm. As a heuristic method for graph isomorphism testing, FWL(2) colors all node pairs in a graph and performs message passing among those node pairs. In order to balance the expressive power and complexity, $d$-DRFWL(2) GNNs simplify FWL(2) by restricting the range of message passing to node pairs whose mutual distances are at most $d$. This way, $d$-DRFWL(2) GNNs exploit graph sparsity while avoiding the expensive subgraph extraction operations in subgraph GNNs, making both the time and space complexity lower. We theoretically investigate both the discriminative power and the cycle counting power of $d$-DRFWL(2) GNNs. Our most important finding is that $d$-DRFWL(2) GNNs have provably strong cycle counting power even with $d=2$: they can count all 3, 4, 5, 6-cycles. Since 6-cycles (e.g., benzene rings) are ubiquitous in organic molecules, being able to detect and count them is crucial for achieving robust and generalizable performance on molecular tasks. Experiments on both synthetic datasets and molecular datasets verify our theory. To the best of our knowledge, 2-DRFWL(2) GNN is the most efficient GNN model to date (both theoretically and empirically) that can count up to 6-cycles",
    "keywords": [],
    "checked": true,
    "id": "8f7eb270c0e338d544af82e81769abaaf53d6c2d",
    "semantic_title": "distance-restricted folklore weisfeiler-leman gnns with provable cycle counting power",
    "citation_count": 0,
    "authors": [
      "Junru Zhou",
      "Jiarui Feng",
      "Xiyuan Wang",
      "Muhan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e351740d4ec4200df6160f34cd181c3-Abstract-Conference.html": {
    "title": "Computing a human-like reaction time metric from stable recurrent vision models",
    "volume": "main",
    "abstract": "The meteoric rise in the adoption of deep neural networks as computational models of vision has inspired efforts to ``align\" these models with humans. One dimension of interest for alignment includes behavioral choices, but moving beyond characterizing choice patterns to capturing temporal aspects of visual decision-making has been challenging. Here, we sketch a general-purpose methodology to construct computational accounts of reaction times from a stimulus-computable, task-optimized model. Specifically, we introduce a novel metric leveraging insights from subjective logic theory summarizing evidence accumulation in recurrent vision models. We demonstrate that our metric aligns with patterns of human reaction times for stimulus manipulations across four disparate visual decision-making tasks spanning perceptual grouping, mental simulation, and scene categorization. This work paves the way for exploring the temporal alignment of model and human visual strategies in the context of various other cognitive tasks toward generating testable hypotheses for neuroscience. Links to the code and data can be found on the project page: https://serre-lab.github.io/rnnrtssite/",
    "keywords": [],
    "checked": true,
    "id": "383eb317cfb30e17d72aa87402fcf57ea6725bb4",
    "semantic_title": "computing a human-like reaction time metric from stable recurrent vision models",
    "citation_count": 2,
    "authors": [
      "Lore Goetschalckx",
      "Lakshmi Narasimhan Govindarajan",
      "Alekh Karkada Ashok",
      "Aarit Ahuja",
      "David Sheinberg",
      "Thomas Serre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e37e56599e3f49cc899f40ae4f5d1fa-Abstract-Conference.html": {
    "title": "ReHLine: Regularized Composite ReLU-ReHU Loss Minimization with Linear Computation and Linear Convergence",
    "volume": "main",
    "abstract": "Empirical risk minimization (ERM) is a crucial framework that offers a general approach to handling a broad range of machine learning tasks. In this paper, we propose a novel algorithm, called ReHLine, for minimizing a set of regularized ERMs with convex piecewise linear-quadratic loss functions and optional linear constraints. The proposed algorithm can effectively handle diverse combinations of loss functions, regularization, and constraints, making it particularly well-suited for complex domain-specific problems. Examples of such problems include FairSVM, elastic net regularized quantile regression, Huber minimization, etc. In addition, ReHLine enjoys a provable linear convergence rate and exhibits a per-iteration computational complexity that scales linearly with the sample size. The algorithm is implemented with both Python and R interfaces, and its performance is benchmarked on various tasks and datasets. Our experimental results demonstrate that ReHLine significantly surpasses generic optimization solvers in terms of computational efficiency on large-scale datasets. Moreover, it also outperforms specialized solvers such as Liblinear in SVMs, hqreg in Huber minimization, and Lightning (SAGA, SAG, SDCA, SVRG) in smoothed SVMs, exhibiting exceptional flexibility and efficiency. The source code, project page, accompanying software, and the Python/R interface can be accessed through the link: https://github.com/softmin/ReHLine",
    "keywords": [],
    "checked": true,
    "id": "398fca2fcf18393c1d73aa542c5243e641473bf6",
    "semantic_title": "rehline: regularized composite relu-rehu loss minimization with linear computation and linear convergence",
    "citation_count": 0,
    "authors": [
      "Ben Dai",
      "Yixuan Qiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e49934cac6cb8604b0c67cfa0828718-Abstract-Conference.html": {
    "title": "Improved Frequency Estimation Algorithms with and without Predictions",
    "volume": "main",
    "abstract": "Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. without the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches",
    "keywords": [],
    "checked": true,
    "id": "9c3539908ead498b1ea31c6654f5ccc7c681a860",
    "semantic_title": "improved frequency estimation algorithms with and without predictions",
    "citation_count": 0,
    "authors": [
      "Anders Aamand",
      "Justin Chen",
      "Huy Nguyen",
      "Sandeep Silwal",
      "Ali Vakilian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e57e2c14232a7b99cf76213e190822d-Abstract-Conference.html": {
    "title": "BCDiff: Bidirectional Consistent Diffusion for Instantaneous Trajectory Prediction",
    "volume": "main",
    "abstract": "The objective of pedestrian trajectory prediction is to estimate the future paths of pedestrians by leveraging historical observations, which plays a vital role in ensuring the safety of self-driving vehicles and navigation robots. Previous works usually rely on a sufficient amount of observation time to accurately predict future trajectories. However, there are many real-world situations where the model lacks sufficient time to observe, such as when pedestrians abruptly emerge from blind spots, resulting in inaccurate predictions and even safety risks. Therefore, it is necessary to perform trajectory prediction based on instantaneous observations, which has rarely been studied before. In this paper, we propose a Bi-directional Consistent Diffusion framework tailored for instantaneous trajectory prediction, named BCDiff. At its heart, we develop two coupled diffusion models by designing a mutual guidance mechanism which can bidirectionally and consistently generate unobserved historical trajectories and future trajectories step-by-step, to utilize the complementary information between them. Specifically, at each step, the predicted unobserved historical trajectories and limited observed trajectories guide one diffusion model to generate future trajectories, while the predicted future trajectories and observed trajectories guide the other diffusion model to predict unobserved historical trajectories. Given the presence of relatively high noise in the generated trajectories during the initial steps, we introduce a gating mechanism to learn the weights between the predicted trajectories and the limited observed trajectories for automatically balancing their contributions. By means of this iterative and mutually guided generation process, both the future and unobserved historical trajectories undergo continuous refinement, ultimately leading to accurate predictions. Essentially, BCDiff is an encoder-free framework that can be compatible with existing trajectory prediction models in principle. Experiments show that our proposed BCDiff significantly improves the accuracy of instantaneous trajectory prediction on the ETH/UCY and Stanford Drone datasets, compared to related approaches",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongqing Li",
      "Changsheng Li",
      "Dongchun Ren",
      "Guangyi Chen",
      "Ye Yuan",
      "Guoren Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e6744370a8616c90d1e3b7a41993b7c-Abstract-Conference.html": {
    "title": "Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial Expression Recognition",
    "volume": "main",
    "abstract": "Facial expression data is characterized by a significant imbalance, with most collected data showing happy or neutral expressions and fewer instances of fear or disgust. This imbalance poses challenges to facial expression recognition (FER) models, hindering their ability to fully understand various human emotional states. Existing FER methods typically report overall accuracy on highly imbalanced test sets but exhibit low performance in terms of the mean accuracy across all expression classes. In this paper, our aim is to address the imbalanced FER problem. Existing methods primarily focus on learning knowledge of minor classes solely from minor-class samples. However, we propose a novel approach to extract extra knowledge related to the minor classes from both major and minor class samples. Our motivation stems from the belief that FER resembles a distribution learning task, wherein a sample may contain information about multiple classes. For instance, a sample from the major class surprise might also contain useful features of the minor class fear. Inspired by that, we propose a novel method that leverages re-balanced attention maps to regularize the model, enabling it to extract transformation invariant information about the minor classes from all training samples. Additionally, we introduce re-balanced smooth labels to regulate the cross-entropy loss, guiding the model to pay more attention to the minor classes by utilizing the extra information regarding the label distribution of the imbalanced training data. Extensive experiments on different datasets and backbones show that the two proposed modules work together to regularize the model and achieve state-of-the-art performance under the imbalanced FER task. Code is available at https://github.com/zyh-uaiaaaa",
    "keywords": [],
    "checked": true,
    "id": "a62732428c6d53bbaabee9596a473c943368f9fb",
    "semantic_title": "leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition",
    "citation_count": 0,
    "authors": [
      "Yuhang Zhang",
      "Yaqi Li",
      "lixiong Qin",
      "Xuannan Liu",
      "Weihong Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e9e513860b1342f3a12ebecf0528a21-Abstract-Conference.html": {
    "title": "ARTree: A Deep Autoregressive Model for Phylogenetic Inference",
    "volume": "main",
    "abstract": "Designing flexible probabilistic models over tree topologies is important for developing efficient phylogenetic inference methods. To do that, previous works often leverage the similarity of tree topologies via hand-engineered heuristic features which would require domain expertise and may suffer from limited approximation capability. In this paper, we propose a deep autoregressive model for phylogenetic inference based on graph neural networks (GNNs), called ARTree. By decomposing a tree topology into a sequence of leaf node addition operations and modeling the involved conditional distributions based on learnable topological features via GNNs, ARTree can provide a rich family of distributions over tree topologies that have simple sampling algorithms, without using heuristic features. We demonstrate the effectiveness and efficiency of our method on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems",
    "keywords": [],
    "checked": true,
    "id": "3b07d884a5e1c68b98e731c18c769e4b16deed1e",
    "semantic_title": "artree: a deep autoregressive model for phylogenetic inference",
    "citation_count": 0,
    "authors": [
      "Tianyu Xie",
      "Cheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2e9f9cde1b709281a06dd14f679e4c51-Abstract-Conference.html": {
    "title": "A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment",
    "volume": "main",
    "abstract": "The assignment of papers to reviewers is a crucial part of the peer review processes of large publication venues, where organizers (e.g., conference program chairs) rely on algorithms to perform automated paper assignment. As such, a major challenge for the organizers of these processes is to specify paper assignment algorithms that find appropriate assignments with respect to various desiderata. Although the main objective when choosing a good paper assignment is to maximize the expertise of each reviewer for their assigned papers, several other considerations make introducing randomization into the paper assignment desirable: robustness to malicious behavior, the ability to evaluate alternative paper assignments, reviewer diversity, and reviewer anonymity. However, it is unclear in what way one should randomize the paper assignment in order to best satisfy all of these considerations simultaneously. In this work, we present a practical, one-size-fits-all method for randomized paper assignment intended to perform well across different motivations for randomness. We show theoretically and experimentally that our method outperforms currently-deployed methods for randomized paper assignment on several intuitive randomness metrics, demonstrating that the randomized assignments produced by our method are general-purpose",
    "keywords": [],
    "checked": true,
    "id": "b002b9763941f839f24b2bee2344b00d5a9dfe22",
    "semantic_title": "a one-size-fits-all approach to improving randomness in paper assignment",
    "citation_count": 0,
    "authors": [
      "Yixuan Xu",
      "Steven Jecmen",
      "Zimeng Song",
      "Fei Fang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2ea04b568a2deb2d000c59f3a72829b5-Abstract-Conference.html": {
    "title": "Loss Dynamics of Temporal Difference Reinforcement Learning",
    "volume": "main",
    "abstract": "Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent dynamics. We study how learning dynamics and plateaus depend on feature structure, learning rate, discount factor, and reward function. We then analyze how strategies like learning rate annealing and reward shaping can favorably alter learning dynamics and plateaus. To conclude, our work introduces new tools to open a new direction towards developing a theory of learning dynamics in reinforcement learning",
    "keywords": [],
    "checked": true,
    "id": "15fff00f7ceb51dbe9cbdf26d86907ac1f6ec99f",
    "semantic_title": "loss dynamics of temporal difference reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Blake Bordelon",
      "Paul Masset",
      "Henry Kuo",
      "Cengiz Pehlevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2ecc80084c96cc25b11b0ab995c25f47-Abstract-Conference.html": {
    "title": "Fast Model DeBias with Machine Unlearning",
    "volume": "main",
    "abstract": "Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing method (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. Moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets demonstrate that our method achieves superior or competing classification accuracies compared with state-of-the-art retraining-based methods while attaining significantly fewer biases and requiring much less debiasing cost. Notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice",
    "keywords": [],
    "checked": true,
    "id": "2f1487214876e6048680b47fc833bb6813c0d0bf",
    "semantic_title": "fast model debias with machine unlearning",
    "citation_count": 2,
    "authors": [
      "Ruizhe Chen",
      "Jianfei Yang",
      "Huimin Xiong",
      "Jianhong Bai",
      "Tianxiang Hu",
      "Jin Hao",
      "YANG FENG",
      "Joey Tianyi Zhou",
      "Jian Wu",
      "Zuozhu Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f0435cffef91068ced08d7c7d8e643e-Abstract-Conference.html": {
    "title": "Coherent Soft Imitation Learning",
    "volume": "main",
    "abstract": "Imitation learning methods seek to learn from an expert either through behavioral cloning (BC) for the policy or inverse reinforcement learning (IRL) for the reward.Such methods enable agents to learn complex tasks from humans that are difficult to capture with hand-designed reward functions.Choosing between BC or IRL for imitation depends on the quality and state-action coverage of the demonstrations, as well as additional access to the Markov decision process. Hybrid strategies that combine BC and IRL are rare, as initial policy optimization against inaccurate rewards diminishes the benefit of pretraining the policy with BC.Our work derives an imitation method that captures the strengths of both BC and IRL.In the entropy-regularized (`soft') reinforcement learning setting, we show that the behavioral-cloned policy can be used as both a shaped reward and a critic hypothesis space by inverting the regularized policy update. This coherency facilitates fine-tuning cloned policies using the reward estimate and additional interactions with the environment.This approach conveniently achieves imitation learning through initial behavioral cloning and subsequent refinement via RL with online or offline data sources.The simplicity of the approach enables graceful scaling to high-dimensional and vision-based tasks, with stable learning and minimal hyperparameter tuning, in contrast to adversarial approaches.For the open-source implementation and simulation results, see https://joemwatson.github.io/csil/",
    "keywords": [],
    "checked": true,
    "id": "5b5c03667055aae101ba6d57399457732a6fae99",
    "semantic_title": "coherent soft imitation learning",
    "citation_count": 1,
    "authors": [
      "Joe Watson",
      "Sandy Huang",
      "Nicolas Heess"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f060912eacace9ce61ef339205ec54c-Abstract-Conference.html": {
    "title": "Exact Generalization Guarantees for (Regularized) Wasserstein Distributionally Robust Models",
    "volume": "main",
    "abstract": "Wasserstein distributionally robust estimators have emerged as powerful models for prediction and decision-making under uncertainty. These estimators provide attractive generalization guarantees: the robust objective obtained from the training distribution is an exact upper bound on the true risk with high probability. However, existing guarantees either suffer from the curse of dimensionality, are restricted to specific settings, or lead to spurious error terms. In this paper, we show that these generalization guarantees actually hold on general classes of models, do not suffer from the curse of dimensionality, and can even cover distribution shifts at testing. We also prove that these results carry over to the newly-introduced regularized versions of Wasserstein distributionally robust problems",
    "keywords": [],
    "checked": true,
    "id": "0affd22f71bc0a845a9a26f26fe79cbfc5a69fc9",
    "semantic_title": "exact generalization guarantees for (regularized) wasserstein distributionally robust models",
    "citation_count": 2,
    "authors": [
      "Waïss Azizian",
      "Franck Iutzeler",
      "Jérôme Malick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f1486343c2c942a617e4f5bb0cc64c8-Abstract-Conference.html": {
    "title": "Supply-Side Equilibria in Recommender Systems",
    "volume": "main",
    "abstract": "Algorithmic recommender systems such as Spotify and Netflix affect not only consumer behavior but also producer incentives. Producers seek to create content that will be shown by the recommendation algorithm, which can impact both the diversity and quality of their content. In this work, we investigate the resulting supply-side equilibria in personalized content recommender systems. We model the decisions of producers as choosing multi-dimensional content vectors and users as having heterogenous preferences, which contrasts with classical low-dimensional models. Multi-dimensionality and heterogeneity creates the potential for specialization, where different producers create different types of content at equilibrium. Using a duality argument, we derive necessary and sufficient conditions for whether specialization occurs. Then, we characterize the distribution of content at equilibrium in concrete settings with two populations of users. Lastly, we show that specialization can enable producers to achieve positive profit at equilibrium, which means that specialization can reduce the competitiveness of the marketplace. At a conceptual level, our analysis of supply-side competition takes a step towards elucidating how personalized recommendations shape the marketplace of digital goods",
    "keywords": [],
    "checked": true,
    "id": "2839009c867435ebcf117d341a28f1534382a02b",
    "semantic_title": "supply-side equilibria in recommender systems",
    "citation_count": 15,
    "authors": [
      "Meena Jagadeesan",
      "Nikhil Garg",
      "Jacob Steinhardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f1d1196426ba84f47d115cac3dcb9d8-Abstract-Conference.html": {
    "title": "Human-Aligned Calibration for AI-Assisted Decision Making",
    "volume": "main",
    "abstract": "Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exists data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values—an optimal decision maker would need to sometimes place more (less) trust on predictions with lower (higher) confidence values. However, we then show that, if the confidence values satisfy a natural alignment property with respect to the decision maker's confidence on her own predictions, there always exists an optimal decision policy under which the level of trust the decision maker would need to place on predictions is monotone on the confidence values, facilitating its discoverability. Further, we show that multicalibration with respect to the decision maker's confidence on her own prediction is a sufficient condition for alignment. Experiments on a real AI-assisted decision making scenario where a classifier provides decision support to human decision makers validate our theoretical results and suggest that alignment may lead to better decisions",
    "keywords": [],
    "checked": true,
    "id": "28f5160cdd7c186c9c457b06fe6b505d317df96a",
    "semantic_title": "human-aligned calibration for ai-assisted decision making",
    "citation_count": 3,
    "authors": [
      "Nina Corvelo Benz",
      "Manuel Rodriguez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f1eb4c897e63870eee9a0a0f7a10332-Abstract-Conference.html": {
    "title": "Transformer as a hippocampal memory consolidation model based on NMDAR-inspired nonlinearity",
    "volume": "main",
    "abstract": "The hippocampus plays a critical role in learning, memory, and spatial representation, processes that depend on the NMDA receptor (NMDAR). Inspired by recent findings that compare deep learning models to the hippocampus, we propose a new nonlinear activation function that mimics NMDAR dynamics. NMDAR-like nonlinearity has a beneficial role in shifting short-term working memory into long-term reference memory in transformers, thus enhancing a process that is similar to memory consolidation in the mammalian brain. We design a navigation task assessing these two memory functions and show that manipulating the activation function (i.e., mimicking the Mg$^{2+}$-gating of NMDAR) disrupts long-term memory processes. Our experiments suggest that place cell-like functions and reference memory reside in the feed-forward network layer of transformers and that nonlinearity drives these processes. We discuss the role of NMDAR-like nonlinearity in establishing this striking resemblance between transformer architecture and hippocampal spatial representation",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Kyum Kim",
      "Jea Kwon",
      "Meeyoung Cha",
      "C. Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f27964513a28d034530bfdd117ea31d-Abstract-Conference.html": {
    "title": "Gaussian Differential Privacy on Riemannian Manifolds",
    "volume": "main",
    "abstract": "We develop an advanced approach for extending Gaussian Differential Privacy (GDP) to general Riemannian manifolds. The concept of GDP stands out as a prominent privacy definition that strongly warrants extension to manifold settings, due to its central limit properties. By harnessing the power of the renowned Bishop-Gromov theorem in geometric analysis, we propose a Riemannian Gaussian distribution that integrates the Riemannian distance, allowing us to achieve GDP in Riemannian manifolds with bounded Ricci curvature. To the best of our knowledge, this work marks the first instance of extending the GDP framework to accommodate general Riemannian manifolds, encompassing curved spaces, and circumventing the reliance on tangent space summaries. We provide a simple algorithm to evaluate the privacy budget $\\mu$ on any one-dimensional manifold and introduce a versatile Markov Chain Monte Carlo (MCMC)-based algorithm to calculate $\\mu$ on any Riemannian manifold with constant curvature. Through simulations on one of the most prevalent manifolds in statistics, the unit sphere $S^d$, we demonstrate the superior utility of our Riemannian Gaussian mechanism in comparison to the previously proposed Riemannian Laplace mechanism for implementing GDP",
    "keywords": [],
    "checked": true,
    "id": "fae7be79e900dfbc17348912e3d8c863c7fe816f",
    "semantic_title": "gaussian differential privacy on riemannian manifolds",
    "citation_count": 0,
    "authors": [
      "Yangdi Jiang",
      "Xiaotian Chang",
      "Yi Liu",
      "Lei Ding",
      "Linglong Kong",
      "Bei Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f46ef5725a8eca24f7f24a17955ad1a-Abstract-Conference.html": {
    "title": "Agnostically Learning Single-Index Models using Omnipredictors",
    "volume": "main",
    "abstract": "We give the first result for agnostically learning Single-Index Models (SIMs) with arbitrary monotone and Lipschitz activations. All prior work either held only in the realizable setting or required the activation to be known. Moreover, we only require the marginal to have bounded second moments, whereas all prior work required stronger distributional assumptions (such as anticoncentration or boundedness). Our algorithm is based on recent work by Gopalan et al. [2023] on Omniprediction using predictors satisfying calibrated multiaccuracy. Our analysis is simple and relies on the relationship between Bregman divergences (or matching losses) and $\\ell_p$ distances. We also provide new guarantees for standard algorithms like GLMtron and logistic regression in the agnostic setting",
    "keywords": [],
    "checked": true,
    "id": "088edd4fb2f1d44c4be5b443d18c3084468bf72f",
    "semantic_title": "agnostically learning single-index models using omnipredictors",
    "citation_count": 1,
    "authors": [
      "Aravind Gollakota",
      "Parikshit Gopalan",
      "Adam Klivans",
      "Konstantinos Stavropoulos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f4d6f8e0f4f543db12260696b2a3551-Abstract-Conference.html": {
    "title": "On skip connections and normalisation layers in deep optimisation",
    "volume": "main",
    "abstract": "We introduce a general theoretical framework, designed for the study of gradient optimisation of deep neural networks, that encompasses ubiquitous architecture choices including batch normalisation, weight normalisation and skip connections. Our framework determines the curvature and regularity properties of multilayer loss landscapes in terms of their constituent layers, thereby elucidating the roles played by normalisation layers and skip connections in globalising these properties. We then demonstrate the utility of this framework in two respects. First, we give the only proof of which we are aware that a class of deep neural networks can be trained using gradient descent to global optima even when such optima only exist at infinity, as is the case for the cross-entropy cost. Second, we identify a novel causal mechanism by which skip connections accelerate training, which we verify predictively with ResNets on MNIST, CIFAR10, CIFAR100 and ImageNet",
    "keywords": [],
    "checked": true,
    "id": "2043449b1386cd709cb121036d840854b37bbd3d",
    "semantic_title": "on skip connections and normalisation layers in deep optimisation",
    "citation_count": 2,
    "authors": [
      "Lachlan MacDonald",
      "Jack Valmadre",
      "Hemanth Saratchandran",
      "Simon Lucey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f75a57e9c71e8369da0150ea769d5a2-Abstract-Conference.html": {
    "title": "Efficient Low-rank Backpropagation for Vision Transformer Adaptation",
    "volume": "main",
    "abstract": "The increasing scale of vision transformers (ViT) has made the efficient fine-tuning of these large models for specific needs a significant challenge in various applications. This issue originates from the computationally demanding matrix multiplications required during the backpropagation process through linear layers in ViT.In this paper, we tackle this problem by proposing a new Low-rank BackPropagation via Walsh-Hadamard Transformation (LBP-WHT) method. Intuitively, LBP-WHT projects the gradient into a low-rank space and carries out backpropagation. This approach substantially reduces the computation needed for adapting ViT, as matrix multiplication in the low-rank space is far less resource-intensive. We conduct extensive experiments with different models (ViT, hybrid convolution-ViT model) on multiple datasets to demonstrate the effectiveness of our method. For instance, when adapting an EfficientFormer-L1 model on CIFAR100, our LBP-WHT achieves 10.4\\% higher accuracy than the state-of-the-art baseline, while requiring 9 MFLOPs less computation.As the first work to accelerate ViT adaptation with low-rank backpropagation, our LBP-WHT method is complementary to many prior efforts and can be combined with them for better performance",
    "keywords": [],
    "checked": true,
    "id": "53698d5b88e56cef0f569ee4788b21d8963b50bf",
    "semantic_title": "efficient low-rank backpropagation for vision transformer adaptation",
    "citation_count": 0,
    "authors": [
      "Yuedong Yang",
      "Hung-Yueh Chiang",
      "Guihong Li",
      "Diana Marculescu",
      "Radu Marculescu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f76a3a0f44263b5e56fec69ee1220f9-Abstract-Conference.html": {
    "title": "AdaVAE: Bayesian Structural Adaptation for Variational Autoencoders",
    "volume": "main",
    "abstract": "The neural network structures of generative models and their corresponding inference models paired in variational autoencoders (VAEs) play a critical role in the models' generative performance. However, powerful VAE network structures are hand-crafted and fixed prior to training, resulting in a one-size-fits-all approach that requires heavy computation to tune for given data. Moreover, existing VAE regularization methods largely overlook the importance of network structures and fail to prevent overfitting in deep VAE models with cascades of hidden layers. To address these issues, we propose a Bayesian inference framework that automatically adapts VAE network structures to data and prevent overfitting as they grow deeper. We model the number of hidden layers with a beta process to infer the most plausible encoding/decoding network depths warranted by data and perform layer-wise dropout regularization with a conjugate Bernoulli process. We develop a scalable estimator that performs joint inference on both VAE network structures and latent variables. Our experiments show that the inference framework effectively prevents overfitting in both shallow and deep VAE models, yielding state-of-the-art performance. We demonstrate that our framework is compatible with different types of VAE backbone networks and can be applied to various VAE variants, further improving their performance",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paribesh Regmi",
      "Rui Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f89a23a19d1617e7fb16d4f7a049ce2-Abstract-Conference.html": {
    "title": "Safety Verification of Decision-Tree Policies in Continuous Time",
    "volume": "main",
    "abstract": "Decision trees have gained popularity as interpretable surrogate models for learning-based control policies. However, providing safety guarantees for systems controlled by decision trees is an open challenge. We show that the problem is undecidable even for systems with the simplest dynamics, and PSPACE-complete for finite-horizon properties. The latter can be verified for discrete-time systems via bounded model checking. However, for continuous-time systems, such an approach requires discretization, thereby weakening the guarantees for the original system. This paper presents the first algorithm to directly verify decision-tree controlled system in continuous time. The key aspect of our method is exploiting the decision-tree structure to propagate a set-based approximation through the decision nodes. We demonstrate the effectiveness of our approach by verifying safety of several decision trees distilled to imitate neural-network policies for nonlinear systems",
    "keywords": [],
    "checked": false,
    "id": "4fcc703b3c63dddd91154f05f9fc473fb9cd6dd4",
    "semantic_title": "interpretable reinforcement learning for robotics and continuous control",
    "citation_count": 0,
    "authors": [
      "Christian Schilling",
      "Anna Lukina",
      "Emir Demirović",
      "Kim Larsen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f9b3ee2bcea04b327c09d7e3145bd1e-Abstract-Conference.html": {
    "title": "Quasi-Monte Carlo Graph Random Features",
    "volume": "main",
    "abstract": "We present a novel mechanism to improve the accuracy of the recently-introduced class of graph random features (GRFs). Our method induces negative correlations between the lengths of the algorithm's random walks by imposing antithetic termination: a procedure to sample more diverse random walks which may be of independent interest. It has a trivial drop-in implementation. We derive strong theoretical guarantees on the properties of these quasi-Monte Carlo GRFs (q-GRFs), proving that they yield lower-variance estimators of the $2$-regularised Laplacian kernel under mild conditions. Remarkably, our results hold for any graph topology. We demonstrate empirical accuracy improvements on a variety of tasks including a new practical application: time-efficient approximation of the graph diffusion process. To our knowledge, q-GRFs constitute the first rigorously studied quasi-Monte Carlo scheme for kernels defined on combinatorial objects, inviting new research on correlations between graph random walks",
    "keywords": [],
    "checked": true,
    "id": "257048f305436fc870ae08bb5fa871ff16799c1c",
    "semantic_title": "quasi-monte carlo graph random features",
    "citation_count": 3,
    "authors": [
      "Isaac Reid",
      "Adrian Weller",
      "Krzysztof M Choromanski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2f9ee101e35b890d9eae79ee27bcd69a-Abstract-Conference.html": {
    "title": "Functional Renyi Differential Privacy for Generative Modeling",
    "volume": "main",
    "abstract": "Differential privacy (DP) has emerged as a rigorous notion to quantify data privacy. Subsequently, Renyi differential privacy (RDP) becomes an alternative to the ordinary DP notion in both theoretical and empirical studies, for its convenient compositional rules and flexibility. However, most mechanisms with DP (RDP) guarantees are essentially based on randomizing a fixed, finite-dimensional vector output. In this work, following Hall et al. (2013) we further extend RDP to functional outputs, where the output space can be infinite-dimensional, and develop all necessary tools, *e.g.*, (subsampled) Gaussian mechanism, composition, and post-processing rules, to facilitate its practical adoption. As an illustration, we apply functional RDP (f-RDP) to functions in the reproducing kernel Hilbert space (RKHS) to develop a differentially private generative model (DPGM), where training can be interpreted as iteratively releasing loss functions (in an RKHS) with DP (RDP) guarantees. Empirically, the new training paradigm achieves a significant improvement in privacy-utility trade-off compared to existing alternatives, especially when $\\epsilon=0.2$. Our code is available at https://github.com/dihjiang/DP-kernel",
    "keywords": [],
    "checked": false,
    "id": "4b32e492828a880b4d2859301a005126312b8e8b",
    "semantic_title": "adam-dpgan: a differential private mechanism for generative adversarial network",
    "citation_count": 1,
    "authors": [
      "Dihong Jiang",
      "Sun Sun",
      "Yaoliang Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2fb57276bfbaf1b832d7bfcba36bb41c-Abstract-Conference.html": {
    "title": "FedL2P: Federated Learning to Personalize",
    "volume": "main",
    "abstract": "Federated learning (FL) research has made progress in developing algorithms for distributed learning of global models, as well as algorithms for local personalization of those common models to the specifics of each client's local data distribution. However, different FL problems may require different personalization strategies, and it may not even be possible to define an effective one-size-fits-all personalization strategy for all clients: Depending on how similar each client's optimal predictor is to that of the global model, different personalization strategies may be preferred. In this paper, we consider the federated meta-learning problem of learning personalization strategies. Specifically, we consider meta-nets that induce the batch-norm and learning rate parameters for each client given local data statistics. By learning these meta-nets through FL, we allow the whole FL network to collaborate in learning a customized personalization strategy for each client. Empirical results show that this framework improves on a range of standard hand-crafted personalization baselines in both label and feature shift situations",
    "keywords": [],
    "checked": true,
    "id": "37b2af849c63dccd6a9bf24ccc3a7165b8a7558b",
    "semantic_title": "fedl2p: federated learning to personalize",
    "citation_count": 0,
    "authors": [
      "Royson Lee",
      "Minyoung Kim",
      "Da Li",
      "Xinchi Qiu",
      "Timothy Hospedales",
      "Ferenc Huszar",
      "Nicholas Lane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2ff46d83d1dcc063e075058b29d55efe-Abstract-Conference.html": {
    "title": "Learning Reliable Logical Rules with SATNet",
    "volume": "main",
    "abstract": "Bridging logical reasoning and deep learning is crucial for advanced AI systems. In this work, we present a new framework that addresses this goal by generating interpretable and verifiable logical rules through differentiable learning, without relying on pre-specified logical structures. Our approach builds upon SATNet, a differentiable MaxSAT solver that learns the underlying rules from input-output examples. Despite its efficacy, the learned weights in SATNet are not straightforwardly interpretable, failing to produce human-readable rules. To address this, we propose a novel specification method called ``maximum equality'', which enables the interchangeability between the learned weights of SATNet and a set of propositional logical rules in weighted MaxSAT form. With the decoded weighted MaxSAT formula, we further introduce several effective verification techniques to validate it against the ground truth rules. Experiments on stream transformations and Sudoku problems show that our decoded rules are highly reliable: using exact solvers on them could achieve 100% accuracy, whereas the original SATNet fails to give correct solutions in many cases. Furthermore, we formally verify that our decoded logical rules are functionally equivalent to the ground truth ones",
    "keywords": [],
    "checked": true,
    "id": "ec0e707d8519d286beffb9a7c4f6374729d40acc",
    "semantic_title": "learning reliable logical rules with satnet",
    "citation_count": 0,
    "authors": [
      "Zhaoyu Li",
      "Jinpei Guo",
      "Yuhe Jiang",
      "Xujie Si"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/30699996ff411d48903c9752b782a5c1-Abstract-Conference.html": {
    "title": "Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought",
    "volume": "main",
    "abstract": "Language instructions and demonstrations are two natural ways for users to teach robots personalized tasks. Recent progress in Large Language Models (LLMs) has shown impressive performance in translating language instructions into code for robotic tasks. However, translating demonstrations into task code continues to be a challenge due to the length and complexity of both demonstrations and code, making learning a direct mapping intractable. This paper presents Demo2Code, a novel framework that generates robot task code from demonstrations via an extended chain-of-thought and defines a common latent specification to connect the two. Our framework employs a robust two-stage process: (1) a recursive summarization technique that condenses demonstrations into concise specifications, and (2) a code synthesis approach that expands each function recursively from the generated specifications. We conduct extensive evaluation on various robot task benchmarks, including a novel game benchmark Robotouille, designed to simulate diverse cooking tasks in a kitchen environment",
    "keywords": [],
    "checked": true,
    "id": "9dee1aceb09f7d4c22fdbaf49d238e1502effd1b",
    "semantic_title": "demo2code: from summarizing demonstrations to synthesizing code via extended chain-of-thought",
    "citation_count": 11,
    "authors": [
      "Yuki Wang",
      "Gonzalo Gonzalez-Pumariega",
      "Yash Sharma",
      "Sanjiban Choudhury"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3085fd61063840fdb2e6eafac58589f8-Abstract-Conference.html": {
    "title": "Easy Learning from Label Proportions",
    "volume": "main",
    "abstract": "We consider the problem of Learning from Label Proportions (LLP), a weakly supervised classification setup where instances are grouped into i.i.d. \"bags\", and only the frequency of class labels at each bag is available. Albeit, the objective of the learner is to achieve low task loss at an individual instance level. Here we propose EASYLLP, a flexible and simple-to-implement debiasing approach based on aggregate labels, which operates on arbitrary loss functions. Our technique allows us to accurately estimate the expected loss of an arbitrary model at an individual level. We elucidate the differences between our method and standard methods based on label proportion matching, in terms of applicability and optimality conditions. We showcase the flexibility of our approach compared to alternatives by applying our method to popular learning frameworks, like Empirical Risk Minimization (ERM) and Stochastic Gradient Descent (SGD) with provable guarantees on instance level performance. Finally, we validate our theoretical results on multiple datasets, empirically illustrating the conditions under which our algorithm is expected to perform better or worse than previous LLP approaches",
    "keywords": [],
    "checked": true,
    "id": "34b30c340fd8cbece9705a9aec929df832a2d06c",
    "semantic_title": "easy learning from label proportions",
    "citation_count": 7,
    "authors": [
      "Róbert Busa-Fekete",
      "Heejin Choi",
      "Travis Dick",
      "Claudio Gentile",
      "Andres Munoz Medina"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/30ae2af8612ac74357363e8ae877d80c-Abstract-Conference.html": {
    "title": "Jigsaw: Learning to Assemble Multiple Fractured Objects",
    "volume": "main",
    "abstract": "Automated assembly of 3D fractures is essential in orthopedics, archaeology, and our daily life. This paper presents Jigsaw, a novel framework for assembling physically broken 3D objects from multiple pieces. Our approach leverages hierarchical features of global and local geometry to match and align the fracture surfaces. Our framework consists of four components: (1) front-end point feature extractor with attention layers, (2) surface segmentation to separate fracture and original parts, (3) multi-parts matching to find correspondences among fracture surface points, and (4) robust global alignment to recover the global poses of the pieces. We show how to jointly learn segmentation and matching and seamlessly integrate feature matching and rigidity constraints. We evaluate Jigsaw on the Breaking Bad dataset and achieve superior performance compared to state-of-the-art methods. Our method also generalizes well to diverse fracture modes, objects, and unseen instances. To the best of our knowledge, this is the first learning-based method designed specifically for 3D fracture assembly over multiple pieces. Our code is available at https://jiaxin-lu.github.io/Jigsaw/",
    "keywords": [],
    "checked": true,
    "id": "5ecd3ee175fb519d0268c247ca1adae6989fc009",
    "semantic_title": "jigsaw: learning to assemble multiple fractured objects",
    "citation_count": 1,
    "authors": [
      "Jiaxin Lu",
      "Yifan Sun",
      "Qixing Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/30b28eb87fe7a6c4af8520293317d4c6-Abstract-Conference.html": {
    "title": "Persuading Farsighted Receivers in MDPs: the Power of Honesty",
    "volume": "main",
    "abstract": "Bayesian persuasion studies the problem faced by an informed sender who strategically discloses information to influence the behavior of an uninformed receiver. Recently, a growing attention has been devoted to settings where the sender and the receiver interact sequentially, in which the receiver's decision-making problem is usually modeled as a Markov decision process (MDP). However, the literature focuses on computing optimal information-revelation policies (a.k.a. signaling schemes) under the restrictive assumption that the receiver acts myopically, selecting actions to maximize the one-step utility and disregarding future rewards. This is justified by the fact that, when the receiver is farsighted and thus considers future rewards, finding an optimal Markovian signaling scheme is NP-hard. In this paper, we show that Markovian signaling schemes do not constitute the \"right\" class of policies. Indeed, differently from most of the MDPs settings, we show that Markovian signaling schemes are not optimal, and general history-dependent signaling schemes should be considered. Moreover, we also show that history-dependent signaling schemes circumvent the negative complexity results affecting Markovian signaling schemes. Formally, we design an algorithm that computes an optimal and $\\epsilon$-persuasive history-dependent signaling scheme in time polynomial in ${1}/{\\epsilon}$ and in the instance size. The crucial challenge is that general history-dependent signaling schemes cannot be represented in polynomial space. Nevertheless, we introduce a convenient subclass of history-dependent signaling schemes, called promise-form, which are as powerful as general history-dependent ones and efficiently representable. Intuitively, promise-form signaling schemes compactly encode histories in the form of honest promises on future receiver's rewards",
    "keywords": [],
    "checked": true,
    "id": "cb88fb7c62a14c35484f58e169616f558844a7ae",
    "semantic_title": "persuading farsighted receivers in mdps: the power of honesty",
    "citation_count": 1,
    "authors": [
      "Martino Bernasconi",
      "Matteo Castiglioni",
      "Alberto Marchesi",
      "Mirco Mutti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/30b6fa308e62ed52180c31ae3ba6bb0a-Abstract-Conference.html": {
    "title": "When are ensembles really effective?",
    "volume": "main",
    "abstract": "Ensembling has a long history in statistical data analysis, with many impactful applications. However, in many modern machine learning settings, the benefits of ensembling are less ubiquitous and less obvious. We study, both theoretically and empirically, the fundamental question of when ensembling yields significant performance improvements in classification tasks. Theoretically, we prove new results relating the \\emph{ensemble improvement rate} (a measure of how much ensembling decreases the error rate versus a single model, on a relative scale) to the \\emph{disagreement-error ratio}. We show that ensembling improves performance significantly whenever the disagreement rate is large relative to the average error rate; and that, conversely, one classifier is often enough whenever the disagreement rate is low relative to the average error rate. On the way to proving these results, we derive, under a mild condition called \\emph{competence}, improved upper and lower bounds on the average test error rate of the majority vote classifier.To complement this theory, we study ensembling empirically in a variety of settings, verifying the predictions made by our theory, and identifying practical scenarios where ensembling does and does not result in large performance improvements. Perhaps most notably, we demonstrate a distinct difference in behavior between interpolating models (popular in current practice) and non-interpolating models (such as tree-based methods, where ensembling is popular), demonstrating that ensembling helps considerably more in the latter case than in the former",
    "keywords": [],
    "checked": true,
    "id": "26f4ac43e9586c110cb40b8dfc6820346375bf9b",
    "semantic_title": "when are ensembles really effective?",
    "citation_count": 1,
    "authors": [
      "Ryan Theisen",
      "Hyunsuk Kim",
      "Yaoqing Yang",
      "Liam Hodgkinson",
      "Michael W. Mahoney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/30d046e94d7b8037d6ef27c4357a8dd4-Abstract-Conference.html": {
    "title": "A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm",
    "volume": "main",
    "abstract": "Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL unifies various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different fixed coefficients; based on insights from this unification, our UDIL allows adaptive coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthetic and real-world datasets. Code will be available at https://github.com/Wang-ML-Lab/unified-continual-learning",
    "keywords": [],
    "checked": true,
    "id": "1dea0427ced025fb722bc42e08075aae1e1d6fea",
    "semantic_title": "a unified approach to domain incremental learning with memory: theory and algorithm",
    "citation_count": 2,
    "authors": [
      "Haizhou Shi",
      "Hao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/30dfe47a3ccbee68cffa0c19ccb1bc00-Abstract-Conference.html": {
    "title": "Few-Shot Class-Incremental Learning via Training-Free Prototype Calibration",
    "volume": "main",
    "abstract": "Real-world scenarios are usually accompanied by continuously appearing classes with scare labeled samples, which require the machine learning model to incrementally learn new classes and maintain the knowledge of base classes. In this Few-Shot Class-Incremental Learning (FSCIL) scenario, existing methods either introduce extra learnable components or rely on a frozen feature extractor to mitigate catastrophic forgetting and overfitting problems. However, we find a tendency for existing methods to misclassify the samples of new classes into base classes, which leads to the poor performance of new classes. In other words, the strong discriminability of base classes distracts the classification of new classes. To figure out this intriguing phenomenon, we observe that although the feature extractor is only trained on base classes, it can surprisingly represent the semantic similarity between the base and unseen new classes. Building upon these analyses, we propose a simple yet effective Training-frEE calibratioN (TEEN) strategy to enhance the discriminability of new classes by fusing the new prototypes (i.e., mean features of a class) with weighted base prototypes. In addition to standard benchmarks in FSCIL, TEEN demonstrates remarkable performance and consistent improvements over baseline methods in the few-shot learning scenario. Code is available at: https://github.com/wangkiw/TEEN",
    "keywords": [],
    "checked": true,
    "id": "c49a493203e2153c45d156a971fd0984a03e506d",
    "semantic_title": "few-shot class-incremental learning via training-free prototype calibration",
    "citation_count": 2,
    "authors": [
      "Qi-Wei Wang",
      "Da-Wei Zhou",
      "Yi-Kai Zhang",
      "De-Chuan Zhan",
      "Han-Jia Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/30e15e5941ae0cdab7ef58cc8d59a4ca-Abstract-Conference.html": {
    "title": "RADAR: Robust AI-Text Detection via Adversarial Learning",
    "volume": "main",
    "abstract": "Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a $\\underline{r}$obust $\\underline{A}$I-text $\\underline{d}$etector via $\\underline{a}$dversarial lea$\\underline{r}$ning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection.RADAR uses the feedback from the detector to update the paraphraser, and vice versa.Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo",
    "keywords": [],
    "checked": true,
    "id": "ef3bdb9e805887a1a14e1e4cdb27145135591305",
    "semantic_title": "radar: robust ai-text detection via adversarial learning",
    "citation_count": 16,
    "authors": [
      "Xiaomeng Hu",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3106c718fe84b91fc301fe2f5b738448-Abstract-Conference.html": {
    "title": "Alleviating the Semantic Gap for Generalized fMRI-to-Image Reconstruction",
    "volume": "main",
    "abstract": "Although existing fMRI-to-image reconstruction methods could predict high-quality images, they do not explicitly consider the semantic gap between training and testing data, resulting in reconstruction with unstable and uncertain semantics. This paper addresses the problem of generalized fMRI-to-image reconstruction by explicitly alleviates the semantic gap. Specifically, we leverage the pre-trained CLIP model to map the training data to a compact feature representation, which essentially extends the sparse semantics of training data to dense ones, thus alleviating the semantic gap of the instances nearby known concepts (i.e., inside the training super-classes). Inspired by the robust low-level representation in fMRI data, which could help alleviate the semantic gap for instances that far from the known concepts (i.e., outside the training super-classes), we leverage structural information as a general cue to guide image reconstruction. Further, we quantify the semantic uncertainty based on probability density estimation and achieve Generalized fMRI-to-image reconstruction by adaptively integrating Expanded Semantics and Structural information (GESS) within a diffusion process. Experimental results demonstrate that the proposed GESS model outperforms state-of-the-art methods, and we propose a generalized scenario split strategy to evaluate the advantage of GESS in closing the semantic gap",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Fang",
      "Qian Zheng",
      "Gang Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/311257424b6d80e930fc93b224f0a63e-Abstract-Conference.html": {
    "title": "Softmax Output Approximation for Activation Memory-Efficient Training of Attention-based Networks",
    "volume": "main",
    "abstract": "In this paper, we propose to approximate the softmax output, which is the key product of the attention mechanism, to reduce its activation memory usage when training attention-based networks (aka Transformers). During the forward pass of the network, the proposed softmax output approximation method stores only a small fraction of the entire softmax output required for back-propagation and evicts the rest of the softmax output from memory. Then, during the backward pass, the evicted softmax activation output is approximated to compose the gradient to perform back-propagation for model training. Considering most attention-based models heavily rely on the softmax-based attention module that usually takes one of the biggest portions of the network, approximating the softmax activation output can be a simple yet effective way to decrease the training memory requirement of many attention-based networks. The experiment with various attention-based models and relevant tasks, i.e., machine translation, text classification, and sentiment analysis, shows that it curtails the activation memory usage of the softmax-based attention module by up to 84% (6.2× less memory) in model training while achieving comparable or better performance, e.g., up to 5.4% higher classification accuracy",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhyeon Lee",
      "Seulki Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3122aaa22b2fe83f9cead1a696f65ceb-Abstract-Conference.html": {
    "title": "Memory Efficient Optimizers with 4-bit States",
    "volume": "main",
    "abstract": "Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizers are evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency",
    "keywords": [],
    "checked": true,
    "id": "5639f5467655581fd780440d88e43af40711d9a6",
    "semantic_title": "memory efficient optimizers with 4-bit states",
    "citation_count": 1,
    "authors": [
      "Bingrui Li",
      "Jianfei Chen",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/313829757739365201b5adb3a1cbd9bd-Abstract-Conference.html": {
    "title": "Replicable Reinforcement Learning",
    "volume": "main",
    "abstract": "The replicability crisis in the social, behavioral, and data sciences has led to the formulation of algorithm frameworks for replicability --- i.e., a requirement that an algorithm produce identical outputs (with high probability) when run on two different samples from the same underlying distribution. While still in its infancy, provably replicable algorithms have been developed for many fundamental tasks in machine learning and statistics, including statistical query learning, the heavy hitters problem, and distribution testing. In this work we initiate the study of replicable reinforcement learning, providing a provably replicable algorithm for parallel value iteration, and a provably replicable version of R-Max in the episodic setting. These are the first formal replicability results for control problems, which present different challenges for replication than batch learning settings",
    "keywords": [],
    "checked": true,
    "id": "475b6130dcaef639dfd4a2540cd8d85842c854c8",
    "semantic_title": "replicable reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Eric Eaton",
      "Marcel Hussing",
      "Michael Kearns",
      "Jessica Sorrell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3151e460c41ba67dc55412861184ef35-Abstract-Conference.html": {
    "title": "Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT method. With this finding, we propose memory-efficient fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's starting point and making it reversible without additional pre-training. We evaluate MEFT on the GLUE benchmark and five question-answering tasks with various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to 84% of full fine-tuning with a negligible amount of trainable parameters. Moreover, MEFT achieves the same score on GLUE and a comparable score on the question-answering tasks as full fine-tuning. A similar finding is also observed for the image classification task",
    "keywords": [],
    "checked": true,
    "id": "08e0b903f56f77c6d8a66b66f9cdbc0e40586324",
    "semantic_title": "make pre-trained model reversible: from parameter to memory efficient fine-tuning",
    "citation_count": 4,
    "authors": [
      "Baohao Liao",
      "Shaomu Tan",
      "Christof Monz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/31610e68fe41a62e460e044216a10766-Abstract-Conference.html": {
    "title": "Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization",
    "volume": "main",
    "abstract": "In this work, we decouple the iterative bi-level offline RL (value estimation and policy extraction) from the offline training phase, forming a non-iterative bi-level paradigm and avoiding the iterative error propagation over two levels. Specifically, this non-iterative paradigm allows us to conduct inner-level optimization (value estimation) in training, while performing outer-level optimization (policy extraction) in testing. Naturally, such a paradigm raises three core questions that are not fully answered by prior non-iterative offline RL counterparts like reward-conditioned policy: (q1) What information should we transfer from the inner-level to the outer-level? (q2) What should we pay attention to when exploiting the transferred information for safe/confident outer-level optimization? (q3) What are the benefits of concurrently conducting outer-level optimization during testing? Motivated by model-based optimization (MBO), we propose DROP (design from policies), which fully answers the above questions. Specifically, in the inner-level, DROP decomposes offline data into multiple subsets, and learns an MBO score model (a1). To keep safe exploitation to the score model in the outer-level, we explicitly learn a behavior embedding and introduce a conservative regularization (a2). During testing, we show that DROP permits deployment adaptation, enabling an adaptive inference across states (a3). Empirically, we evaluate DROP on various tasks, showing that DROP gains comparable or better performance compared to prior methods",
    "keywords": [],
    "checked": true,
    "id": "535c385767b188d8112f2748ce5433b744b18361",
    "semantic_title": "design from policies: conservative test-time adaptation for offline policy optimization",
    "citation_count": 3,
    "authors": [
      "Jinxin Liu",
      "Hongyin Zhang",
      "Zifeng Zhuang",
      "Yachen Kang",
      "Donglin Wang",
      "Bin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3170de57bc1899315b97712043d8bb22-Abstract-Conference.html": {
    "title": "Lightweight Vision Transformer with Bidirectional Interaction",
    "volume": "main",
    "abstract": "Recent advancements in vision backbones have significantly improved their performance by simultaneously modeling images' local and global contexts. However, the bidirectional interaction between these two contexts has not been well explored and exploited, which is important in the human visual system. This paper proposes a Fully Adaptive Self-Attention (FASA) mechanism for vision transformer to model the local and global information as well as the bidirectional interaction between them in context-aware ways. Specifically, FASA employs self-modulated convolutions to adaptively extract local representation while utilizing self-attention in down-sampled space to extract global representation. Subsequently, it conducts a bidirectional adaptation process between local and global representation to model their interaction. In addition, we introduce a fine-grained downsampling strategy to enhance the down-sampled self-attention mechanism for finer-grained global perception capability. Based on FASA, we develop a family of lightweight vision backbones, Fully Adaptive Transformer (FAT) family. Extensive experiments on multiple vision tasks demonstrate that FAT achieves impressive performance. Notably, FAT accomplishes a 77.6% accuracy on ImageNet-1K using only 4.5M parameters and 0.7G FLOPs, which surpasses the most advanced ConvNets and Transformers with similar model size and computational costs. Moreover, our model exhibits faster speed on modern GPU compared to other models",
    "keywords": [],
    "checked": true,
    "id": "9c703f8d5ad3ba10564f25ffae62cf8a71f1f6cb",
    "semantic_title": "lightweight vision transformer with bidirectional interaction",
    "citation_count": 1,
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Xiaoqiang Zhou",
      "Ran He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3173c427cb4ed2d5eaab029c17f221ae-Abstract-Conference.html": {
    "title": "Fused Gromov-Wasserstein Graph Mixup for Graph-level Classifications",
    "volume": "main",
    "abstract": "Graph data augmentation has shown superiority in enhancing generalizability and robustness of GNNs in graph-level classifications. However, existing methods primarily focus on the augmentation in the graph signal space and the graph structure space independently, neglecting the joint interaction between them. In this paper, we address this limitation by formulating the problem as an optimal transport problem that aims to find an optimal inter-graph node matching strategy considering the interactions between graph structures and signals. To solve this problem, we propose a novel graph mixup algorithm called FGWMixup, which seeks a \"midpoint\" of source graphs in the Fused Gromov-Wasserstein (FGW) metric space. To enhance the scalability of our method, we introduce a relaxed FGW solver that accelerates FGWMixup by improving the convergence rate from $\\mathcal{O}(t^{-1})$ to $\\mathcal{O}(t^{-2})$. Extensive experiments conducted on five datasets using both classic (MPNNs) and advanced (Graphormers) GNN backbones demonstrate that \\mname\\xspace effectively improves the generalizability and robustness of GNNs. Codes are available at https://github.com/ArthurLeoM/FGWMixup",
    "keywords": [],
    "checked": true,
    "id": "dc072922b4a72d649842d53cee83eea7a70b77ae",
    "semantic_title": "fused gromov-wasserstein graph mixup for graph-level classifications",
    "citation_count": 2,
    "authors": [
      "Xinyu Ma",
      "Xu Chu",
      "Yasha Wang",
      "Yang Lin",
      "Junfeng Zhao",
      "Liantao Ma",
      "Wenwu Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/317470b3fde29f3bb8d6dee563afffc4-Abstract-Conference.html": {
    "title": "Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message Passing",
    "volume": "main",
    "abstract": "Data over non-Euclidean manifolds, often discretized as surface meshes, naturally arise in computer graphics and biological and physical systems. In particular, solutions to partial differential equations (PDEs) over manifolds depend critically on the underlying geometry. While graph neural networks have been successfully applied to PDEs, they do not incorporate surface geometry and do not consider local gauge symmetries of the manifold. Alternatively, recent works on gauge equivariant convolutional and attentional architectures on meshes leverage the underlying geometry but underperform in modeling surface PDEs with complex nonlinear dynamics. To address these issues, we introduce a new gauge equivariant architecture using nonlinear message passing. Our novel architecture achieves higher performance than either convolutional or attentional networks on domains with highly complex and nonlinear dynamics. However, similar to the non-mesh case, design trade-offs favor convolutional, attentional, or message passing networks for different tasks; we investigate in which circumstances our message passing method provides the most benefit",
    "keywords": [],
    "checked": true,
    "id": "7b0e8c49ea09f3caa9c4c168424c8f100b014116",
    "semantic_title": "modeling dynamics over meshes with gauge equivariant nonlinear message passing",
    "citation_count": 0,
    "authors": [
      "Jung Yeon Park",
      "Lawson Wong",
      "Robin Walters"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3186591903d9db31770ad131adb5ceb4-Abstract-Conference.html": {
    "title": "ASIF: Coupled Data Turns Unimodal Models to Multimodal without Training",
    "volume": "main",
    "abstract": "CLIP proved that aligning visual and language spaces is key to solving many vision tasks without explicit training, but required to train image and text encoders from scratch on a huge dataset. LiT improved this by only training the text encoder and using a pre-trained vision network. In this paper, we show that a common space can be created without any training at all, using single-domain encoders (trained with or without supervision) and a much smaller amount of image-text pairs. Furthermore, our model has unique properties. Most notably, deploying a new version with updated training samples can be done in a matter of seconds. Additionally, the representations in the common space are easily interpretable as every dimension corresponds to the similarity of the input to a unique entry in the multimodal dataset. Experiments on standard zero-shot visual benchmarks demonstrate the typical transfer ability of image-text models. Overall, our method represents a simple yet surprisingly strong baseline for foundation multi-modal models, raising important questions on their data efficiency and on the role of retrieval in machine learning",
    "keywords": [],
    "checked": true,
    "id": "81e4973ba9c1f3aa72dd12f059617c9e64b2ae9a",
    "semantic_title": "asif: coupled data turns unimodal models to multimodal without training",
    "citation_count": 11,
    "authors": [
      "Antonio Norelli",
      "Marco Fumero",
      "Valentino Maiorca",
      "Luca Moschella",
      "Emanuele Rodolà",
      "Francesco Locatello"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/31994923f58ae5b2d661b300bd439107-Abstract-Conference.html": {
    "title": "A Metadata-Driven Approach to Understand Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in various applications, but their performance can be sensitive to specific data properties of the graph datasets they operate on. Current literature on understanding the limitations of GNNs has primarily employed a \\emph{model-driven} approach that leverage heuristics and domain knowledge from network science or graph theory to model the GNN behaviors, which is time-consuming and highly subjective. In this work, we propose a \\emph{metadata-driven} approach to analyze the sensitivity of GNNs to graph data properties, motivated by the increasing availability of graph learning benchmarks. We perform a multivariate sparse regression analysis on the metadata derived from benchmarking GNN performance across diverse datasets, yielding a set of salient data properties. To validate the effectiveness of our data-driven approach, we focus on one identified data property, the degree distribution, and investigate how this property influences GNN performance through theoretical analysis and controlled experiments. Our theoretical findings reveal that datasets with more balanced degree distribution exhibit better linear separability of node representations, thus leading to better GNN performance. We also conduct controlled experiments using synthetic datasets with varying degree distributions, and the results align well with our theoretical findings. Collectively, both the theoretical analysis and controlled experiments verify that the proposed metadata-driven approach is effective in identifying critical data properties for GNNs",
    "keywords": [],
    "checked": true,
    "id": "595e294e0cb68853f4ab8438c592426f177c074f",
    "semantic_title": "a metadata-driven approach to understand graph neural networks",
    "citation_count": 1,
    "authors": [
      "Ting Wei Li",
      "Qiaozhu Mei",
      "Jiaqi Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/31a19921acd38cdf7a8c86ec032cef2d-Abstract-Conference.html": {
    "title": "Multimodal Deep Learning Model Unveils Behavioral Dynamics of V1 Activity in Freely Moving Mice",
    "volume": "main",
    "abstract": "Despite their immense success as a model of macaque visual cortex, deep convolutional neural networks (CNNs) have struggled to predict activity in visual cortex of the mouse, which is thought to be strongly dependent on the animal's behavioral state. Furthermore, most computational models focus on predicting neural responses to static images presented under head fixation, which are dramatically different from the dynamic, continuous visual stimuli that arise during movement in the real world. Consequently, it is still unknown how natural visual input and different behavioral variables may integrate over time to generate responses in primary visual cortex (V1). To address this, we introduce a multimodal recurrent neural network that integrates gaze-contingent visual input with behavioral and temporal dynamics to explain V1 activity in freely moving mice. We show that the model achieves state-of-the-art predictions of V1 activity during free exploration and demonstrate the importance of each component in an extensive ablation study. Analyzing our model using maximally activating stimuli and saliency maps, we reveal new insights into cortical function, including the prevalence of mixed selectivity for behavioral variables in mouse V1. In summary, our model offers a comprehensive deep-learning framework for exploring the computational principles underlying V1 neurons in freely-moving animals engaged in natural behavior",
    "keywords": [],
    "checked": true,
    "id": "7e8b7c78380d052387aa9ad2ce559381bfb707ef",
    "semantic_title": "multimodal deep learning model unveils behavioral dynamics of v1 activity in freely moving mice",
    "citation_count": 0,
    "authors": [
      "Aiwen Xu",
      "Yuchen Hou",
      "Cristopher Niell",
      "Michael Beyeler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/31ceb5aed43e2ec1b132e389cc1dcb56-Abstract-Conference.html": {
    "title": "Goal-conditioned Offline Planning from Curious Exploration",
    "volume": "main",
    "abstract": "Curiosity has established itself as a powerful exploration strategy in deep reinforcement learning. Notably, leveraging expected future novelty as intrinsic motivation has been shown to efficiently generate exploratory trajectories, as well as a robust dynamics model. We consider the challenge of extracting goal-conditioned behavior from the products of such unsupervised exploration techniques, without any additional environment interaction. We find that conventional goal-conditioned reinforcement learning approaches for extracting a value function and policy fall short in this difficult offline setting. By analyzing the geometry of optimal goal-conditioned value functions, we relate this issue to a specific class of estimation artifacts in learned values. In order to mitigate their occurrence, we propose to combine model-based planning over learned value landscapes with a graph-based value aggregation scheme. We show how this combination can correct both local and global artifacts, obtaining significant improvements in zero-shot goal-reaching performance across diverse simulated environments",
    "keywords": [],
    "checked": true,
    "id": "06cb176ff32fbf498f0df93016a46dbbb333d533",
    "semantic_title": "goal-conditioned offline planning from curious exploration",
    "citation_count": 0,
    "authors": [
      "Marco Bagatella",
      "Georg Martius"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/31d997278ee9069d6721bc194174bb4c-Abstract-Conference.html": {
    "title": "Rethinking the Role of Token Retrieval in Multi-Vector Retrieval",
    "volume": "main",
    "abstract": "Multi-vector retrieval models such as ColBERT [Khattab et al., 2020] allow token-level interactions between queries and documents, and hence achieve state of the art on many information retrieval benchmarks. However, their non-linear scoring function cannot be scaled to millions of documents, necessitating a three-stage process for inference: retrieving initial candidates via token retrieval, accessing all token vectors, and scoring the initial candidate documents. The non-linear scoring function is applied over all token vectors of each candidate document, making the inference process complicated and slow. In this paper, we aim to simplify the multi-vector retrieval by rethinking the role of token retrieval. We present XTR, ConteXtualized Token Retriever, which introduces a simple, yet novel, objective function that encourages the model to retrieve the most important document tokens first. The improvement to token retrieval allows XTR to rank candidates only using the retrieved tokens rather than all tokens in the document, and enables a newly designed scoring stage that is two-to-three orders of magnitude cheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the state-of-the-art by 2.8 nDCG@10 without any distillation. Detailed analysis confirms our decision to revisit the token retrieval stage, as XTR demonstrates much better recall of the token retrieval stage compared to ColBERT",
    "keywords": [],
    "checked": true,
    "id": "bc85e1ef6b2f1cca1abecdbcd77c8cfc1281cdb1",
    "semantic_title": "rethinking the role of token retrieval in multi-vector retrieval",
    "citation_count": 1,
    "authors": [
      "Jinhyuk Lee",
      "Zhuyun Dai",
      "Sai Meher Karthik Duddu",
      "Tao Lei",
      "Iftekhar Naim",
      "Ming-Wei Chang",
      "Vincent Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/31e018f43ab9c7065c058cc2c5848128-Abstract-Conference.html": {
    "title": "Optimal Exploration for Model-Based RL in Nonlinear Systems",
    "volume": "main",
    "abstract": "Learning to control unknown nonlinear dynamical systems is a fundamental problem in reinforcement learning and control theory. A commonly applied approach is to first explore the environment (exploration), learn an accurate model of it (system identification), and then compute an optimal controller with the minimum cost on this estimated system (policy optimization). While existing work has shown that it is possible to learn a uniformly good model of the system (Mania et al., 2020), in practice, if we aim to learn a good controller with a low cost on the actual system, certain system parameters may be significantly more critical than others, and we therefore ought to focus our exploration on learning such parameters.In this work, we consider the setting of nonlinear dynamical systems and seek to formally quantify, in such settings, (a) which parameters are most relevant to learning a good controller, and (b) how we can best explore so as to minimize uncertainty in such parameters. Inspired by recent work in linear systems (Wagenmaker et al., 2021), we show that minimizing the controller loss in nonlinear systems translates to estimating the system parameters in a particular, task-dependent metric. Motivated by this, we develop an algorithm able to efficiently explore the system to reduce uncertainty in this metric, and prove a lower bound showing that our approach learns a controller at a near-instance-optimal rate. Our algorithm relies on a general reduction from policy optimization to optimal experiment design in arbitrary systems, and may be of independent interest. We conclude with experiments demonstrating the effectiveness of our method in realistic nonlinear robotic systems",
    "keywords": [],
    "checked": true,
    "id": "8037c0795aa73f7a5f4be1d452cccbbad70254a2",
    "semantic_title": "optimal exploration for model-based rl in nonlinear systems",
    "citation_count": 2,
    "authors": [
      "Andrew Wagenmaker",
      "Guanya Shi",
      "Kevin G. Jamieson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/31ed129feae64a7e44a15b148c15558d-Abstract-Conference.html": {
    "title": "ELDEN: Exploration via Local Dependencies",
    "volume": "main",
    "abstract": "Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent's actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between entities. ELDEN utilizes a novel scheme --- the partial derivative of the learned dynamics to model the local dependencies between entities accurately and computationally efficiently. The uncertainty of the predicted dependencies is then used as an intrinsic reward to encourage exploration toward new interactions. We evaluate the performance of ELDEN on four different domains with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In all domains, ELDEN correctly identifies local dependencies and learns successful policies, significantly outperforming previous state-of-the-art exploration methods",
    "keywords": [],
    "checked": true,
    "id": "c35cd153d65b6a0f72f6fb398879344eb2150198",
    "semantic_title": "elden: exploration via local dependencies",
    "citation_count": 1,
    "authors": [
      "Zizhao Wang",
      "Jiaheng Hu",
      "Peter Stone",
      "Roberto Martín-Martín"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/31f04c174a6af322e9417b7a9a91097a-Abstract-Conference.html": {
    "title": "Maximization of Average Precision for Deep Learning with Adversarial Ranking Robustness",
    "volume": "main",
    "abstract": "This paper seeks to address a gap in optimizing Average Precision (AP) while ensuring adversarial robustness, an area that has not been extensively explored to the best of our knowledge. AP maximization for deep learning has widespread applications, particularly when there is a significant imbalance between positive and negative examples. Although numerous studies have been conducted on adversarial training, they primarily focus on robustness concerning accuracy, ensuring that the average accuracy on adversarially perturbed examples is well maintained. However, this type of adversarial robustness is insufficient for many applications, as minor perturbations on a single example can significantly impact AP while not greatly influencing the accuracy of the prediction system. To tackle this issue, we introduce a novel formulation that combines an AP surrogate loss with a regularization term representing adversarial ranking robustness, which maintains the consistency between ranking of clean data and that of perturbed data. We then devise an efficient stochastic optimization algorithm to optimize the resulting objective. Our empirical studies, which compare our method to current leading adversarial training baselines and other robust AP maximization strategies, demonstrate the effectiveness of the proposed approach. Notably, our methods outperform a state-of-the-art method (TRADES) by more than 4\\% in terms of robust AP against PGD attacks while achieving 7\\% higher AP on clean data simultaneously on CIFAR10 and CIFAR100",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Li",
      "Wei Tong",
      "Tianbao Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/31fc85f7461ce71eadf27fb7281973bd-Abstract-Conference.html": {
    "title": "Act As You Wish: Fine-Grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs",
    "volume": "main",
    "abstract": "Most text-driven human motion generation methods employ sequential modeling approaches, e.g., transformer, to extract sentence-level text representations automatically and implicitly for human motion synthesis. However, these compact text representations may overemphasize the action names at the expense of other important properties and lack fine-grained details to guide the synthesis of subtly distinct motion. In this paper, we propose hierarchical semantic graphs for fine-grained control over motion generation. Specifically, we disentangle motion descriptions into hierarchical semantic graphs including three levels of motions, actions, and specifics. Such global-to-local structures facilitate a comprehensive understanding of motion description and fine-grained control of motion generation. Correspondingly, to leverage the coarse-to-fine topology of hierarchical semantic graphs, we decompose the text-to-motion diffusion process into three semantic levels, which correspond to capturing the overall motion, local actions, and action specifics. Extensive experiments on two benchmark human motion datasets, including HumanML3D and KIT, with superior performances, justify the efficacy of our method. More encouragingly, by modifying the edge weights of hierarchical semantic graphs, our method can continuously refine the generated motion, which may have a far-reaching impact on the community. Code and pre-training weights are available at https://github.com/jpthu17/GraphMotion",
    "keywords": [],
    "checked": true,
    "id": "3b94dba88b1b6f09f5928a8741eb7036ce106645",
    "semantic_title": "act as you wish: fine-grained control of motion diffusion model with hierarchical semantic graphs",
    "citation_count": 4,
    "authors": [
      "Peng Jin",
      "Yang Wu",
      "Yanbo Fan",
      "Zhongqian Sun",
      "Wei Yang",
      "Li Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/320e941f53db45bddc8757d1c8c4f6aa-Abstract-Conference.html": {
    "title": "Online Constrained Meta-Learning: Provable Guarantees for Generalization",
    "volume": "main",
    "abstract": "Meta-learning has attracted attention due to its strong ability to learn experiences from known tasks, which can speed up and enhance the learning process for new tasks. However, most existing meta-learning approaches only can learn from tasks without any constraint. This paper proposes an online constrained meta-learning framework, which continuously learns meta-knowledge from sequential learning tasks, and the learning tasks are subject to hard constraints. Beyond existing meta-learning analyses, we provide the upper bounds of optimality gaps and constraint violations produced by the proposed framework, which considers the dynamic regret of online learning, as well as the generalization ability of the task-specific models. Moreover, we provide a practical algorithm for the framework, and validate its superior effectiveness through experiments conducted on meta-imitation learning and few-shot image classification",
    "keywords": [],
    "checked": false,
    "id": "536b9296f9f02048241645ec72bba8ad02d3cff9",
    "semantic_title": "meta-learning adversarial bandits",
    "citation_count": 4,
    "authors": [
      "Siyuan Xu",
      "Minghui Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32133a6a24d6554263d3584e3ac10faa-Abstract-Conference.html": {
    "title": "Mean-field Langevin dynamics: Time-space discretization, stochastic gradient, and variance reduction",
    "volume": "main",
    "abstract": "The mean-field Langevin dynamics (MFLD) is a nonlinear generalization of the Langevin dynamics that incorporates a distribution-dependent drift, and it naturally arises from the optimization of two-layer neural networks via (noisy) gradient descent. Recent works have shown that MFLD globally minimizes an entropy-regularized convex functional in the space of measures. However, all prior analyses assumed the infinite-particle or continuous-time limit, and cannot handle stochastic gradient updates. We provide a general framework to prove a uniform-in-time propagation of chaos for MFLD that takes into account the errors due to finite-particle approximation, time-discretization, and stochastic gradient. To demonstrate the wide applicability of our framework, we establish quantitative convergence rate guarantees to the regularized global optimal solution for $(i)$ a wide range of learning problems such as mean-field neural network and MMD minimization, and $(ii)$ different gradient estimators including SGD and SVRG. Despite the generality of our results, we achieve an improved convergence rate in both the SGD and SVRG settings when specialized to the standard Langevin dynamics",
    "keywords": [],
    "checked": false,
    "id": "898d1fffc0f2ec0a74c73b2802413dc270d6ac75",
    "semantic_title": "convergence of mean-field langevin dynamics: time and space discretization, stochastic gradient, and variance reduction",
    "citation_count": 6,
    "authors": [
      "Taiji Suzuki",
      "Denny Wu",
      "Atsushi Nitanda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32246544c237164c365c0527b677a79a-Abstract-Conference.html": {
    "title": "Public Opinion Field Effect Fusion in Representation Learning for Trending Topics Diffusion",
    "volume": "main",
    "abstract": "Trending topic diffusion and prediction analysis is an important problem and has been well studied in social networks. Representation learning is an effective way to extract node embeddings, which can help for topic propagation analysis by completing downstream tasks such as link prediction and node classification. In real world, there are often several trending topics or opinion leaders in public opinion space at the same time and they can be regarded as different centers of public opinion. A public opinion field will be formed surrounding every center. These public opinion fields compete for public's attention and it will potentially affect the development of public opinion. However, the existing methods do not consider public opinion field effect for trending topics diffusion. In this paper, we introduce three well-known observations about public opinion field effect in media and communication studies, and propose a novel and effective heterogeneous representation learning framework to incorporate public opinion field effect and social circle influence effect. To the best of our knowledge, our work is the first to consider these effects in representation learning for trending topic diffusion. Extensive experiments on real-world datasets validate the superiority of our model",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junliang Li",
      "Yang Yajun",
      "Qinghua Hu",
      "Xin Wang",
      "Hong Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32285dd184dbfc33cb2d1f0db53c23c5-Abstract-Conference.html": {
    "title": "Distributional Pareto-Optimal Multi-Objective Reinforcement Learning",
    "volume": "main",
    "abstract": "Multi-objective reinforcement learning (MORL) has been proposed to learn control policies over multiple competing objectives with each possible preference over returns. However, current MORL algorithms fail to account for distributional preferences over the multi-variate returns, which are particularly important in real-world scenarios such as autonomous driving. To address this issue, we extend the concept of Pareto-optimality in MORL into distributional Pareto-optimality, which captures the optimality of return distributions, rather than the expectations. Our proposed method, called Distributional Pareto-Optimal Multi-Objective Reinforcement Learning~(DPMORL), is capable of learning distributional Pareto-optimal policies that balance multiple objectives while considering the return uncertainty. We evaluated our method on several benchmark problems and demonstrated its effectiveness in discovering distributional Pareto-optimal policies and satisfying diverse distributional preferences compared to existing MORL methods",
    "keywords": [],
    "checked": true,
    "id": "bd74dff05ea32996c8be5ca7ff516fe6999c2b88",
    "semantic_title": "distributional pareto-optimal multi-objective reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Xin-Qiang Cai",
      "Pushi Zhang",
      "Li Zhao",
      "Jiang Bian",
      "Masashi Sugiyama",
      "Ashley Llorens"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3255a7554605a88800f4e120b3a929e1-Abstract-Conference.html": {
    "title": "Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning",
    "volume": "main",
    "abstract": "In recent years, pre-trained large language models (LLMs) have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. Current understandings of the underlying mechanisms by which this capability arises from regular language model pretraining objectives remain disconnected from the real-world LLMs. This study aims to examine the in-context learning phenomenon through a Bayesian lens, viewing real-world LLMs as latent variable models. On this premise, we propose an algorithm to select optimal demonstrations from a set of annotated data with a small LM, and then directly generalize the selected demonstrations to larger LMs. We demonstrate significant improvement over baselines, averaged over eight GPT models on eight real-world text classification datasets. We also demonstrate the real-world usefulness of our algorithm on GSM8K, a math word problem dataset. Our empirical findings support our hypothesis that LLMs implicitly infer a latent variable containing task information",
    "keywords": [],
    "checked": true,
    "id": "29bd550d0ab53296790ceba31dfe0a06754bcdde",
    "semantic_title": "large language models are latent variable models: explaining and finding good demonstrations for in-context learning",
    "citation_count": 25,
    "authors": [
      "Xinyi Wang",
      "Wanrong Zhu",
      "Michael Saxon",
      "Mark Steyvers",
      "William Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3268353cd4ff87451347f242c7401773-Abstract-Conference.html": {
    "title": "Physics-Driven ML-Based Modelling for Correcting Inverse Estimation",
    "volume": "main",
    "abstract": "When deploying machine learning estimators in science and engineering (SAE) domains, it is critical to avoid failed estimations that can have disastrous consequences, e.g., in aero engine design. This work focuses on detecting and correcting failed state estimations before adopting them in SAE inverse problems, by utilizing simulations and performance metrics guided by physical laws. We suggest to flag a machine learning estimation when its physical model error exceeds a feasible threshold, and propose a novel approach, GEESE, to correct it through optimization, aiming at delivering both low error and high efficiency. The key designs of GEESE include (1) a hybrid surrogate error model to provide fast error estimations to reduce simulation cost and to enable gradient based backpropagation of error feedback, and (2) two generative models to approximate the probability distributions of the candidate states for simulating the exploitation and exploration behaviours. All three models are constructed as neural networks. GEESE is tested on three real-world SAE inverse problems and compared to a number of state-of-the-art optimization/search approaches. Results show that it fails the least number of times in terms of finding a feasible state correction, and requires physical evaluations less frequently in general",
    "keywords": [],
    "checked": true,
    "id": "c78164daf8da5e36f098d7efbea17f7d748e77af",
    "semantic_title": "physics-driven ml-based modelling for correcting inverse estimation",
    "citation_count": 1,
    "authors": [
      "ruiyuan kang",
      "Tingting Mu",
      "Panagiotis Liatsis",
      "Dimitrios Kyritsis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32c2f3e0a44d55820da7fbcee0a1d95c-Abstract-Conference.html": {
    "title": "One-Pass Distribution Sketch for Measuring Data Heterogeneity in Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) is a machine learning paradigm where multiple client devices train models collaboratively without data exchange. Data heterogeneity problem is naturally inherited in FL since data in different clients follow diverse distributions. To mitigate the negative influence of data heterogeneity, we need to start by measuring it across clients. However, the efficient measurement between distributions is a challenging problem, especially in high dimensionality. In this paper, we propose a one-pass distribution sketch to represent the client data distribution. Our sketching algorithm only requires a single pass of the client data, which is efficient in terms of time and memory. Moreover, we show in both theory and practice that the distance between two distribution sketches represents the divergence between their corresponding distributions. Furthermore, we demonstrate with extensive experiments that our distribution sketch improves the client selection in the FL training. We also showcase that our distribution sketch is an efficient solution to the cold start problem in FL for new clients with unlabeled data",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichang Liu",
      "Zhaozhuo Xu",
      "Benjamin Coleman",
      "Anshumali Shrivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32c6d65ec2591dfcfb3f0e345a51f585-Abstract-Conference.html": {
    "title": "Kernel-Based Tests for Likelihood-Free Hypothesis Testing",
    "volume": "main",
    "abstract": "Given $n$ observations from two balanced classes, consider the task of labeling an additional $m$ inputs that are known to all belong to \\emph{one} of the two classes. Special cases of this problem are well-known: with completeknowledge of class distributions ($n=\\infty$) theproblem is solved optimally by the likelihood-ratio test; when$m=1$ it corresponds to binary classification; and when $m\\approx n$ it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations and the unlabeled sample is collected experimentally. In recent work it was discovered that there is a fundamental trade-offbetween $m$ and $n$: increasing the data sample $m$ reduces the amount $n$ of training/simulationdata needed. In this work we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes -- a case often encountered in practice; (b) study the minimax sample complexity for non-parametric classes of densities under \\textit{maximum meandiscrepancy} (MMD) separation; and (c) investigate the empirical performance of kernels parameterized by neural networks on two tasks: detectionof the Higgs boson and detection of planted DDPM generated images amidstCIFAR-10 images. For both problems we confirm the existence of the theoretically predicted asymmetric $m$ vs $n$ trade-off",
    "keywords": [],
    "checked": true,
    "id": "6a0ef90c6d4be76f666b6431adea56c4104d287f",
    "semantic_title": "kernel-based tests for likelihood-free hypothesis testing",
    "citation_count": 0,
    "authors": [
      "Patrik Robert Gerber",
      "Tianze Jiang",
      "Yury Polyanskiy",
      "Rui Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32cc61322f1e2f56f989d29ccc7cfbb7-Abstract-Conference.html": {
    "title": "Deep Equilibrium Based Neural Operators for Steady-State PDEs",
    "volume": "main",
    "abstract": "Data-driven machine learning approaches are being increasingly used to solve partial differential equations (PDEs). They have shown particularly striking successes when training an operator, which takes as input a PDE in some family, and outputs its solution. However, the architectural design space, especially given structural knowledge of the PDE family of interest, is still poorly understood. We seek to remedy this gap by studying the benefits of weight-tied neural network architectures for steady-state PDEs. To achieve this, we first demonstrate that the solution of most steady-state PDEs can be expressed as a fixed point of a non-linear operator. Motivated by this observation, we propose FNO-DEQ, a deep equilibrium variant of the FNO architecture that directly solves for the solution of a steady-state PDE as the infinite-depth fixed point of an implicit operator layer using a black-box root solver and differentiates analytically through this fixed point resulting in $\\mathcal{O}(1)$ training memory. Our experiments indicate that FNO-DEQ-based architectures outperform FNO-based baselines with $4\\times$ the number of parameters in predicting the solution to steady-state PDEs such as Darcy Flow and steady-state incompressible Navier-Stokes. Finally, we show FNO-DEQ is more robust when trained with datasets with more noisy observations than the FNO-based baselines, demonstrating the benefits of using appropriate inductive biases in architectural design for different neural network based PDE solvers. Further, we show a universal approximation result that demonstrates that FNO-DEQ can approximate the solution to any steady-state PDE that can be written as a fixed point equation",
    "keywords": [],
    "checked": true,
    "id": "9a80eb74901b2ac1a4602f79671e864527369cb9",
    "semantic_title": "deep equilibrium based neural operators for steady-state pdes",
    "citation_count": 0,
    "authors": [
      "Tanya Marwah",
      "Ashwini Pokle",
      "J. Zico Kolter",
      "Zachary Lipton",
      "Jianfeng Lu",
      "Andrej Risteski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32e41d6b0a51a63a9a90697da19d235d-Abstract-Conference.html": {
    "title": "An Efficient and Robust Framework for Approximate Nearest Neighbor Search with Attribute Constraint",
    "volume": "main",
    "abstract": "This paper introduces an efficient and robust framework for hybrid query (HQ) processing, which combines approximate nearest neighbor search (ANNS) with attribute constraint. HQ aims to find objects that are similar to a feature vector and match some structured attributes. Existing methods handle ANNS and attribute filtering separately, leading to inefficiency and inaccuracy. Our framework, called native hybrid query (NHQ), builds a composite index based on proximity graph (PG) and applies joint pruning for HQ. We can easily adapt existing PGs to this framework for efficient HQ processing. We also propose two new navigable PGs (NPGs) with optimized edge selection and routing, which improve the overall ANNS performance. We implement five HQ methods based on the proposed NPGs and existing PGs in NHQ, and show that they outperform the state-of-the-art methods on 10 real-world datasets (up to 315$\\times$ faster with the same accuracy)",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengzhao Wang",
      "Lingwei Lv",
      "Xiaoliang Xu",
      "Yuxiang Wang",
      "Qiang Yue",
      "Jiongkang Ni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32ebb6b560ee58abbdae834e5f37cb5d-Abstract-Conference.html": {
    "title": "Parameter-efficient Tuning of Large-scale Multimodal Foundation Model",
    "volume": "main",
    "abstract": "Driven by the progress of large-scale pre-training, parameter-efficient transfer learning has gained immense popularity across different subfields of Artificial Intelligence. The core is to adapt the model to downstream tasks with only a small set of parameters. Recently, researchers have leveraged such proven techniques in multimodal tasks and achieve promising results. However, two critical issues remain unresolved: how to further reduce the complexity with lightweight design and how to boost alignment between modalities under extremely low parameters. In this paper, we propose A gracefUl pRompt framewOrk for cRoss-modal trAnsfer (AURORA) to overcome these challenges. Considering the redundancy in existing architectures, we first utilize the mode approximation to generate 0.1M trainable parameters to implement the multimodal parameter-efficient tuning, which explores the low intrinsic dimension with only 0.04% parameters of the pre-trained model. Then, for better modality alignment, we propose the Informative Context Enhancement and Gated Query Transformation module under extremely few parameters scenes. A thorough evaluation on six cross-modal benchmarks shows that it not only outperforms the state-of-the-art but even outperforms the full fine-tuning approach. Our code is available at: https://github.com/WillDreamer/Aurora",
    "keywords": [],
    "checked": true,
    "id": "0c7ce5898dab92da540457b754254d72b8592fc2",
    "semantic_title": "parameter-efficient tuning of large-scale multimodal foundation model",
    "citation_count": 2,
    "authors": [
      "Haixin Wang",
      "Xinlong Yang",
      "Jianlong Chang",
      "Dian Jin",
      "Jinan Sun",
      "Shikun Zhang",
      "Xiao Luo",
      "Qi Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32f9049217da6e718a426b07242dff73-Abstract-Conference.html": {
    "title": "Balance, Imbalance, and Rebalance: Understanding Robust Overfitting from a Minimax Game Perspective",
    "volume": "main",
    "abstract": "Adversarial Training (AT) has become arguably the state-of-the-art algorithm for extracting robust features. However, researchers recently notice that AT suffers from severe robust overfitting problems, particularly after learning rate (LR) decay. In this paper, we explain this phenomenon by viewing adversarial training as a dynamic minimax game between the model trainer and the attacker. Specifically, we analyze how LR decay breaks the balance between the minimax game by empowering the trainer with a stronger memorization ability, and show such imbalance induces robust overfitting as a result of memorizing non-robust features. We validate this understanding with extensive experiments, and provide a holistic view of robust overfitting from the dynamics of both the two game players. This understanding further inspires us to alleviate robust overfitting by rebalancing the two players by either regularizing the trainer's capacity or improving the attack strength. Experiments show that the proposed ReBalanced Adversarial Training (ReBAT) can attain good robustness and does not suffer from robust overfitting even after very long training. Code is available at https://github.com/PKU-ML/ReBAT",
    "keywords": [],
    "checked": true,
    "id": "c77538947e7aa9acf298e479624778bca65e4920",
    "semantic_title": "balance, imbalance, and rebalance: understanding robust overfitting from a minimax game perspective",
    "citation_count": 1,
    "authors": [
      "Yifei Wang",
      "Liangchen Li",
      "Jiansheng Yang",
      "Zhouchen Lin",
      "Yisen  Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3322a9a72a1707de14badd5e552ff466-Abstract-Conference.html": {
    "title": "Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations",
    "volume": "main",
    "abstract": "Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests",
    "keywords": [],
    "checked": true,
    "id": "f734d43db6474d839f559d42970a7afbbe3453fc",
    "semantic_title": "should i stop or should i go: early stopping with heterogeneous populations",
    "citation_count": 0,
    "authors": [
      "Hammaad Adam",
      "Fan Yin",
      "Huibin Hu",
      "Neil Tenenholtz",
      "Lorin Crawford",
      "Lester Mackey",
      "Allison Koenecke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/33301bb40020a56ef56b8b5081e5c4d5-Abstract-Conference.html": {
    "title": "Adaptive Privacy Composition for Accuracy-first Mechanisms",
    "volume": "main",
    "abstract": "Although there has been work to develop ex-post private mechanisms from Ligett et al. '17 and Whitehouse et al '22 that seeks to provide privacy guarantees subject to a target level of accuracy, there was not a way to use them in conjunction with differentially private mechanisms. Furthermore, there has yet to be work in developing a theory for how these ex-post privacy mechanisms compose, so that we can track the accumulated privacy over several mechanisms. We develop privacy filters that allow an analyst to adaptively switch between differentially private mechanisms and ex-post private mechanisms subject to an overall privacy loss guarantee. We show that using a particular ex-post private mechanism --- noise reduction mechanisms --- can substantially outperform baseline approaches that use existing privacy loss composition bounds. We use the common task of returning as many counts as possible subject to a relative error guarantee and an overall privacy budget as a motivating example",
    "keywords": [],
    "checked": true,
    "id": "b6b060272e75406ae522384cf8a5b75ef5e44123",
    "semantic_title": "adaptive privacy composition for accuracy-first mechanisms",
    "citation_count": 1,
    "authors": [
      "Ryan M. Rogers",
      "Gennady Samorodnitsk",
      "Steven Z. Wu",
      "Aaditya Ramdas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/333581887bf483296118a97773cab0c1-Abstract-Conference.html": {
    "title": "CaMP: Causal Multi-policy Planning for Interactive Navigation in Multi-room Scenes",
    "volume": "main",
    "abstract": "Visual navigation has been widely studied under the assumption that there may be several clear routes to reach the goal. However, in more practical scenarios such as a house with several messy rooms, there may not. Interactive Navigation (InterNav) considers agents navigating to their goals more effectively with object interactions, posing new challenges of learning interaction dynamics and extra action space. Previous works learn single vision-to-action policy with the guidance of designed representations. However, the causality between actions and outcomes is prone to be confounded when the attributes of obstacles are diverse and hard to measure. Learning policy for long-term action planning in complex scenes also leads to extensive inefficient exploration. In this paper, we introduce a causal diagram of InterNav clarifying the confounding bias caused by obstacles. To address the problem, we propose a multi-policy model that enables the exploration of counterfactual interactions as well as reduces unnecessary exploration. We develop a large-scale dataset containing 600k task episodes in 12k multi-room scenes based on the ProcTHOR simulator and showcase the effectiveness of our method with the evaluations on our dataset",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Wang",
      "Yuehu Liu",
      "Xinhang Song",
      "Beibei Wang",
      "Shuqiang Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/333e67fc4728f147d31608db3ca78e09-Abstract-Conference.html": {
    "title": "DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models",
    "volume": "main",
    "abstract": "Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates \\textit{vectorized} free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of Bézier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than prior work. The code and demo of DiffSketcher can be found at https://ximinng.github.io/DiffSketcher-project/",
    "keywords": [],
    "checked": true,
    "id": "34509b045b625bab87f5d3747fcae0736ea4f880",
    "semantic_title": "diffsketcher: text guided vector sketch synthesis through latent diffusion models",
    "citation_count": 5,
    "authors": [
      "XiMing Xing",
      "Chuang Wang",
      "Haitao Zhou",
      "Jing Zhang",
      "Qian Yu",
      "Dong Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3340ee1e4a8bad8d32c35721712b4d0a-Abstract-Conference.html": {
    "title": "Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models",
    "volume": "main",
    "abstract": "Public large-scale text-to-image diffusion models, such as Stable Diffusion, have gained significant attention from the community. These models can be easily customized for new concepts using low-rank adaptations (LoRAs). However, the utilization of multiple-concept LoRAs to jointly support multiple customized concepts presents a challenge. We refer to this scenario as decentralized multi-concept customization, which involves single-client concept tuning and center-node concept fusion. In this paper, we propose a new framework called Mix-of-Show that addresses the challenges of decentralized multi-concept customization, including concept conflicts resulting from existing single-client LoRA tuning and identity loss during model fusion. Mix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client tuning and gradient fusion for the center node to preserve the in-domain essence of single concepts and support theoretically limitless concept fusion. Additionally, we introduce regionally controllable sampling, which extends spatially controllable sampling (e.g., ControlNet and T2I-Adapter) to address attribute binding and missing object problems in multi-concept sampling. Extensive experiments demonstrate that Mix-of-Show is capable of composing multiple customized concepts with high fidelity, including characters, objects, and scenes",
    "keywords": [],
    "checked": true,
    "id": "5728ecb3a11c1586c4ae53e11ab395a0263eb5f4",
    "semantic_title": "mix-of-show: decentralized low-rank adaptation for multi-concept customization of diffusion models",
    "citation_count": 32,
    "authors": [
      "Yuchao Gu",
      "Xintao Wang",
      "Jay Zhangjie Wu",
      "Yujun Shi",
      "Yunpeng Chen",
      "Zihan Fan",
      "WUYOU XIAO",
      "Rui Zhao",
      "Shuning Chang",
      "Weijia Wu",
      "Yixiao Ge",
      "Ying Shan",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/33646ef0ed554145eab65f6250fab0c9-Abstract-Conference.html": {
    "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation",
    "volume": "main",
    "abstract": "We present a comprehensive solution to learn and improve text-to-image models from human preference feedback.To begin with, we build ImageReward---the first general-purpose text-to-image human preference reward model---to effectively encode human preferences.Its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date.In human evaluation, ImageReward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis.On top of it, we propose Reward Feedback Learning (ReFL), a direct tuning algorithm to optimize diffusion models against a scorer.Both automatic and human evaluation support ReFL's advantages over compared methods.All code and datasets are provided at \\url{https://github.com/THUDM/ImageReward}",
    "keywords": [],
    "checked": true,
    "id": "1b2355c3c674b26a977768a91a164384ad51bbb1",
    "semantic_title": "imagereward: learning and evaluating human preferences for text-to-image generation",
    "citation_count": 51,
    "authors": [
      "Jiazheng Xu",
      "Xiao Liu",
      "Yuchen Wu",
      "Yuxuan Tong",
      "Qinkai Li",
      "Ming Ding",
      "Jie Tang",
      "Yuxiao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/336572db3e99930814d6b328d4220cb6-Abstract-Conference.html": {
    "title": "To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in Transfer Learning",
    "volume": "main",
    "abstract": "Transfer learning and ensembling are two popular techniques for improving the performance and robustness of neural networks. Due to the high cost of pre-training, ensembles of models fine-tuned from a single pre-trained checkpoint are often used in practice. Such models end up in the same basin of the loss landscape, which we call the pre-train basin, and thus have limited diversity. In this work, we show that ensembles trained from a single pre-trained checkpoint may be improved by better exploring the pre-train basin, however, leaving the basin results in losing the benefits of transfer learning and in degradation of the ensemble quality. Based on the analysis of existing exploration methods, we propose a more effective modification of the Snapshot Ensembles (SSE) for transfer learning setup, StarSSE, which results in stronger ensembles and uniform model soups",
    "keywords": [],
    "checked": true,
    "id": "18ff657c37950d89a8ed5493f6969a9dcd367719",
    "semantic_title": "to stay or not to stay in the pre-train basin: insights on ensembling in transfer learning",
    "citation_count": 0,
    "authors": [
      "Ildus Sadrtdinov",
      "Dmitrii Pozdeev",
      "Dmitry P. Vetrov",
      "Ekaterina Lobacheva"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3368e8f592b0d46ed85def795fd5168f-Abstract-Conference.html": {
    "title": "Statistical Limits of Adaptive Linear Models: Low-Dimensional Estimation and Inference",
    "volume": "main",
    "abstract": "Estimation and inference in statistics pose significant challenges when data are collected adaptively. Even in linear models, the Ordinary Least Squares (OLS) estimator may fail to exhibit asymptotic normality for single coordinate estimation and have inflated error. This issue is highlighted by a recent minimax lower bound, which shows that the error of estimating a single coordinate can be enlarged by a multiple of $\\sqrt{d}$ when data are allowed to be arbitrarily adaptive, compared with the case when they are i.i.d. Our work explores this striking difference in estimation performance between utilizing i.i.d. and adaptive data. We investigate how the degree of adaptivity in data collection impacts the performance of estimating a low-dimensional parameter component in high-dimensional linear models. We identify conditions on the data collection mechanism under which the estimation error for a low-dimensional parameter component matches its counterpart in the i.i.d. setting, up to a factor that depends on the degree of adaptivity. We show that OLS or OLS on centered data can achieve this matching error. In addition, we propose a novel estimator for single coordinate inference via solving a Two-stage Adaptive Linear Estimating equation (TALE). Under a weaker form of adaptivity in data collection, we establish an asymptotic normality property of the proposed estimator",
    "keywords": [],
    "checked": true,
    "id": "ed5668d91dbb297717ff85c74f366d1539deb511",
    "semantic_title": "statistical limits of adaptive linear models: low-dimensional estimation and inference",
    "citation_count": 0,
    "authors": [
      "Licong Lin",
      "Mufang Ying",
      "Suvrojit Ghosh",
      "Koulik Khamaru",
      "Cun-Hui Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3380e8116452e0efbf36f35d95e88c94-Abstract-Conference.html": {
    "title": "Future-Dependent Value-Based Off-Policy Evaluation in POMDPs",
    "volume": "main",
    "abstract": "We study off-policy evaluation (OPE) for partially observable MDPs (POMDPs) with general function approximation. Existing methods such as sequential importance sampling estimators and fitted-Q evaluation suffer from the curse of horizon in POMDPs. To circumvent this problem, we develop a novel model-free OPE method by introducing future-dependent value functions that take future proxies as inputs. Future-dependent value functions play similar roles as classical value functions in fully-observable MDPs. We derive a new off-policy Bellman equation for future-dependent value functions as conditional moment equations that use history proxies as instrumental variables. We further propose a minimax learning method to learn future-dependent value functions using the new Bellman equation. We obtain the PAC result, which implies our OPE estimator is close to the true policy value as long as futures and histories contain sufficient information about latent states, and the Bellman completeness. Our code is available at https://github.com/aiueola/neurips2023-future-dependent-ope",
    "keywords": [],
    "checked": true,
    "id": "d2a22a46fa4b4e7c917d434576b6b7ae0a0d09b5",
    "semantic_title": "future-dependent value-based off-policy evaluation in pomdps",
    "citation_count": 9,
    "authors": [
      "Masatoshi Uehara",
      "Haruka Kiyohara",
      "Andrew Bennett",
      "Victor Chernozhukov",
      "Nan Jiang",
      "Nathan Kallus",
      "Chengchun Shi",
      "Wen Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/339caf45a6fa281cae8adc6465343464-Abstract-Conference.html": {
    "title": "Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution",
    "volume": "main",
    "abstract": "Vision-language pretrained models have seen remarkable success, but their application to safety-critical settings is limited by their lack of interpretability. To improve the interpretability of vision-language models such as CLIP, we propose a multi-modal information bottleneck (M2IB) approach that learns latent representations that compress irrelevant information while preserving relevant visual and textual features. We demonstrate how M2IB can be applied to attribution analysis of vision-language pretrained models, increasing attribution accuracy and improving the interpretability of such models when applied to safety-critical domains such as healthcare. Crucially, unlike commonly used unimodal attribution methods, M2IB does not require ground truth labels, making it possible to audit representations of vision-language pretrained models when multiple modalities but no ground-truth data is available. Using CLIP as an example, we demonstrate the effectiveness of M2IB attribution and show that it outperforms gradient-based, perturbation-based, and attention-based attribution methods both qualitatively and quantitatively",
    "keywords": [],
    "checked": true,
    "id": "cab8726de462497bbbdcac5e10736b9f5a5c2d8b",
    "semantic_title": "visual explanations of image-text representations via multi-modal information bottleneck attribution",
    "citation_count": 0,
    "authors": [
      "Ying Wang",
      "Tim G. J. Rudner",
      "Andrew G. Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/33b47b3d2441a17b95344cd635f3dd01-Abstract-Conference.html": {
    "title": "PlanE: Representation Learning over Planar Graphs",
    "volume": "main",
    "abstract": "Graph neural networks are prominent models for representation learning over graphs, where the idea is to iteratively compute representations of nodes of an input graph through a series of transformations in such a way that the learned graph function is isomorphism-invariant on graphs, which makes the learned representations graph invariants. On the other hand, it is well-known that graph invariants learned by these class of models are incomplete: there are pairs of non-isomorphic graphs which cannot be distinguished by standard graph neural networks. This is unsurprising given the computational difficulty of graph isomorphism testing on general graphs, but the situation begs to differ for special graph classes, for which efficient graph isomorphism testing algorithms are known, such as planar graphs. The goal of this work is to design architectures for efficiently learning complete invariants of planar graphs. Inspired by the classical planar graph isomorphism algorithm of Hopcroft and Tarjan, we propose PlanE as a framework for planar representation learning. PlanE includes architectures which can learn complete invariants over planar graphs while remaining practically scalable. We empirically validate the strong performance of the resulting model architectures on well-known planar graph benchmarks, achieving multiple state-of-the-art results",
    "keywords": [],
    "checked": true,
    "id": "a9927a4e1c0da6bf5446e3ba93e5c395b3bbd1bf",
    "semantic_title": "plane: representation learning over planar graphs",
    "citation_count": 2,
    "authors": [
      "Radoslav Dimitrov",
      "Zeyang Zhao",
      "Ralph Abboud",
      "Ismail Ceylan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/33bb58be3f0e903c75afa73d75b5c67e-Abstract-Conference.html": {
    "title": "Optimal Regret Is Achievable with Bounded Approximate Inference Error: An Enhanced Bayesian Upper Confidence Bound Framework",
    "volume": "main",
    "abstract": "Bayesian bandit algorithms with approximate Bayesian inference have been widely used in real-world applications. However, there is a large discrepancy between the superior practical performance of these approaches and their theoretical justification. Previous research only indicates a negative theoretical result: Thompson sampling could have a worst-case linear regret $\\Omega(T)$ with a constant threshold on the inference error measured by one $\\alpha$-divergence. To bridge this gap, we propose an Enhanced Bayesian Upper Confidence Bound (EBUCB) framework that can efficiently accommodate bandit problems in the presence of approximate inference. Our theoretical analysis demonstrates that for Bernoulli multi-armed bandits, EBUCB can achieve the optimal regret order $O(\\log T)$ if the inference error measured by two different $\\alpha$-divergences is less than a constant, regardless of how large this constant is. To our best knowledge, our study provides the first theoretical regret bound that is better than $o(T)$ in the setting of constant approximate inference error. Furthermore, in concordance with the negative results in previous studies, we show that only one bounded $\\alpha$-divergence is insufficient to guarantee a sub-linear regret",
    "keywords": [],
    "checked": true,
    "id": "a9f3210fb0762b8712fc5d58adbcb642e26fa5c9",
    "semantic_title": "optimal regret is achievable with bounded approximate inference error: an enhanced bayesian upper confidence bound framework",
    "citation_count": 0,
    "authors": [
      "Ziyi Huang",
      "Henry Lam",
      "Amirhossein Meisami",
      "Haofeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/33edf072fe44f19079d66713a1831550-Abstract-Conference.html": {
    "title": "Any-to-Any Generation via Composable Diffusion",
    "volume": "main",
    "abstract": "We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis",
    "keywords": [],
    "checked": true,
    "id": "9f411fda2ad5b141a3115f707bcf5ee865b3fb94",
    "semantic_title": "any-to-any generation via composable diffusion",
    "citation_count": 40,
    "authors": [
      "Zineng Tang",
      "Ziyi Yang",
      "Chenguang Zhu",
      "Michael Zeng",
      "Mohit Bansal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/34074479ee2186a9f236b8fd03635372-Abstract-Conference.html": {
    "title": "Rank-DETR for High Quality Object Detection",
    "volume": "main",
    "abstract": "Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design that prioritizes predictions of more accurate localization accuracy during ranking to boost the AP under high IoU thresholds. We apply our method to improve the recent SOTA methods (e.g., H-DETR and DINO-DETR) and report strong COCO object detection results when using different backbones such as ResNet-$50$, Swin-T, and Swin-L, demonstrating the effectiveness of our approach. Code is available at \\url{https://github.com/LeapLabTHU/Rank-DETR}",
    "keywords": [],
    "checked": true,
    "id": "084bc25747b92be54134d4e4257789b4b6c33c10",
    "semantic_title": "rank-detr for high quality object detection",
    "citation_count": 3,
    "authors": [
      "Yifan Pu",
      "Weicong Liang",
      "Yiduo Hao",
      "YUHUI YUAN",
      "Yukang Yang",
      "Chao Zhang",
      "Han Hu",
      "Gao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/34260a400e39a802961470b3d3de99cc-Abstract-Conference.html": {
    "title": "Towards Efficient Pre-Trained Language Model via Feature Correlation Distillation",
    "volume": "main",
    "abstract": "Knowledge Distillation (KD) has emerged as a promising approach for compressing large Pre-trained Language Models (PLMs). The performance of KD relies on how to effectively formulate and transfer the knowledge from the teacher model to the student model. Prior arts mainly focus on directly aligning output features from the transformer block, which may impose overly strict constraints on the student model's learning process and complicate the training process by introducing extra parameters and computational cost. Moreover, our analysis indicates that the different relations within self-attention, as adopted in other works, involves more computation complexities and can easily be constrained by the number of heads, potentially leading to suboptimal solutions. To address these issues, we propose a novel approach that builds relationships directly from output features. Specifically, we introduce token-level and sequence-level relations concurrently to fully exploit the knowledge from the teacher model. Furthermore, we propose a correlation-based distillation loss to alleviate the exact match properties inherent in traditional KL divergence or MSE loss functions. Our method, dubbed FCD, presents a simple yet effective method to compress various architectures (BERT, RoBERTa, and GPT) and model sizes (base-size and large-size). Extensive experimental results demonstrate that our distilled, smaller language models significantly surpass existing KD methods across various NLP tasks",
    "keywords": [],
    "checked": false,
    "id": "8c499a4c032e9298220b578768310ce141e1fea8",
    "semantic_title": "selecting language models features via software-hardware co-design",
    "citation_count": 0,
    "authors": [
      "Kun Huang",
      "Xin Guo",
      "Meng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3430bcc30cdaabd0bf6c5d0c31bda67c-Abstract-Conference.html": {
    "title": "Simple and Asymmetric Graph Contrastive Learning without Augmentations",
    "volume": "main",
    "abstract": "Graph Contrastive Learning (GCL) has shown superior performance in representation learning in graph-structured data. Despite their success, most existing GCL methods rely on prefabricated graph augmentation and homophily assumptions. Thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. In this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. We find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), is easy to implement and does not rely on graph augmentations and homophily assumptions. We provide theoretical and empirical evidence that GraphACL can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. Experimental results show that the simple GraphACL significantly outperforms state-of-the-art graph contrastive learning and self-supervised learning methods on homophilic and heterophilic graphs. The code of GraphACL is available at https://github.com/tengxiao1/GraphACL",
    "keywords": [],
    "checked": true,
    "id": "cb41f8163dd866f51f7e58d9e77f354b11181967",
    "semantic_title": "simple and asymmetric graph contrastive learning without augmentations",
    "citation_count": 1,
    "authors": [
      "Teng Xiao",
      "Huaisheng Zhu",
      "Zhengyu Chen",
      "Suhang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/34345e243156da67605d4b63d71c8d98-Abstract-Conference.html": {
    "title": "Large-Scale Distributed Learning via Private On-Device LSH",
    "volume": "main",
    "abstract": "Locality-sensitive hashing (LSH) based frameworks have been used efficiently to select weight vectors in a dense hidden layer with high cosine similarity to an input, enabling dynamic pruning. While this type of scheme has been shown to improve computational training efficiency, existing algorithms require repeated randomized projection of the full layer weight, which is impractical for computational- and memory-constrained devices. In a distributed setting, deferring LSH analysis to a centralized host is (i) slow if the device cluster is large and (ii) requires access to input data which is forbidden in a federated context. Using a new family of hash functions, we develop the first private, personalized, and memory-efficient on-device LSH framework.Our framework enables privacy and personalization by allowing each device to generate hash tables, without the help of a central host, using device-specific hashing hyper-parameters (e.g., number of hash tables or hash length).Hash tables are generated with a compressed set of the full weights, and can be serially generated and discarded if the process is memory-intensive.This allows devices to avoid maintaining (i) the fully-sized model and (ii) large amounts of hash tables in local memory for LSH analysis. We prove several statistical and sensitivity properties of our hash functions, and experimentally demonstrate that our framework is competitive in training large scale recommender networks compared to other LSH frameworks which assume unrestricted on-device capacity",
    "keywords": [],
    "checked": false,
    "id": "619e711f9f9cf00a9bfa6e483e33d8e6d54cf26b",
    "semantic_title": "large-scale distributed learning via private on-device locality-sensitive hashing",
    "citation_count": 0,
    "authors": [
      "Tahseen Rabbani",
      "Marco Bornstein",
      "Furong Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/345208bdbbb6104616311dfc1d093fe7-Abstract-Conference.html": {
    "title": "Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes",
    "volume": "main",
    "abstract": "Node-level random walk has been widely used to improve Graph Neural Networks. However, there is limited attention to random walk on edge and, more generally, on $k$-simplices. This paper systematically analyzes how random walk on different orders of simplicial complexes (SC) facilitates GNNs in their theoretical expressivity. First, on $0$-simplices or node level, we establish a connection between existing positional encoding (PE) and structure encoding (SE) methods through the bridge of random walk. Second, on $1$-simplices or edge level, we bridge edge-level random walk and Hodge $1$-Laplacians and design corresponding edge PE respectively. In spatial domain, we directly make use of edge level random walk to construct EdgeRWSE. Based on spectral analysis of Hodge $1$-Laplcians, we propose Hodge1Lap, a permutation equivariant and expressive edge-level positional encoding. Third, we generalize our theory to random walk on higher-order simplices and propose the general principle to design PE on simplices based on random walk and Hodge Laplacians. Inter-level random walk is also introduced to unify a wide range of simplicial networks. Extensive experiments verify the effectiveness of our random walk-based methods",
    "keywords": [],
    "checked": true,
    "id": "1d099eddeba2655ad82b894c7f79052ad3f3f922",
    "semantic_title": "facilitating graph neural networks with random walk on simplicial complexes",
    "citation_count": 0,
    "authors": [
      "Cai Zhou",
      "Xiyuan Wang",
      "Muhan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/34678d08b36076de986df95c5bbba92f-Abstract-Conference.html": {
    "title": "Koopman Kernel Regression",
    "volume": "main",
    "abstract": "Many machine learning approaches for decision making, such as reinforcement learning, rely on simulators or predictive models to forecast the time-evolution of quantities of interest, e.g., the state of an agent or the reward of a policy. Forecasts of such complex phenomena are commonly described by highly nonlinear dynamical systems, making their use in optimization-based decision-making challenging.Koopman operator theory offers a beneficial paradigm for addressing this problem by characterizing forecasts via linear time-invariant (LTI) ODEs -- turning multi-step forecasting into sparse matrix multiplications.Though there exists a variety of learning approaches, they usually lack crucial learning-theoretic guarantees, making the behavior of the obtained models with increasing data and dimensionality unclear.We address the aforementioned by deriving a novel reproducing kernel Hilbert space (RKHS) over trajectories that solely spans transformations into LTI dynamical systems. The resulting Koopman Kernel Regression (KKR) framework enables the use of statistical learning tools from function approximation for novel convergence results and generalization error bounds under weaker assumptions than existing work. Our experiments demonstrate superior forecasting performance compared to Koopman operator and sequential data predictors in RKHS",
    "keywords": [],
    "checked": true,
    "id": "c9cd6ab48be283347c68966078ee4a4569245df8",
    "semantic_title": "koopman kernel regression",
    "citation_count": 5,
    "authors": [
      "Petar Bevanda",
      "Max Beier",
      "Armin Lederer",
      "Stefan Sosnowski",
      "Eyke Hüllermeier",
      "Sandra Hirche"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3469b211b829b39d2b0cfd3b880a869c-Abstract-Conference.html": {
    "title": "Diffusion Self-Guidance for Controllable Image Generation",
    "volume": "main",
    "abstract": "Large-scale generative models are capable of producing high-quality images from detailed prompts. However, many aspects of an image are difficult or impossible to convey through text. We introduce self-guidance, a method that provides precise control over properties of the generated image by guiding the internal representations of diffusion models. We demonstrate that the size, location, and appearance of objects can be extracted from these representations, and show how to use them to steer the sampling process. Self-guidance operates similarly to standard classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. We demonstrate the flexibility and effectiveness of self-guided generation through a wide range of challenging image manipulations, such as modifying the position or size of a single object (keeping the rest of the image unchanged), merging the appearance of objects in one image with the layout of another, composing objects from multiple images into one, and more. We also propose a new method for reconstruction using self-guidance, which allows extending our approach to editing real images",
    "keywords": [],
    "checked": true,
    "id": "fbebb1a5d72aec2a6b13fc909f781f6ba9b04925",
    "semantic_title": "diffusion self-guidance for controllable image generation",
    "citation_count": 54,
    "authors": [
      "Dave Epstein",
      "Allan Jabri",
      "Ben Poole",
      "Alexei Efros",
      "Aleksander Holynski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3477ca0ce484aa2fa42c1361ab601c25-Abstract-Conference.html": {
    "title": "Neural (Tangent Kernel) Collapse",
    "volume": "main",
    "abstract": "This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory",
    "keywords": [],
    "checked": true,
    "id": "4c3a4956cb2f088359c50ed38b0102f6742d449b",
    "semantic_title": "neural (tangent kernel) collapse",
    "citation_count": 3,
    "authors": [
      "Mariia Seleznova",
      "Dana Weitzner",
      "Raja Giryes",
      "Gitta Kutyniok",
      "Hung-Hsu Chou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/34822dab66c13f0100017b8ea373038a-Abstract-Conference.html": {
    "title": "Fast Optimal Locally Private Mean Estimation via Random Projections",
    "volume": "main",
    "abstract": "We study the problem of locally private mean estimation of high-dimensional vectors in the Euclidean ball. Existing algorithms for this problem either incur sub-optimal error or have high communication and/or run-time complexity. We propose a new algorithmic framework, namely ProjUnit, for private mean estimation that yields algorithms that are computationally efficient, have low communication complexity, and incur optimal error up to a $1+o(1)$-factor. Our framework is deceptively simple: each randomizer projects its input to a random low-dimensional subspace and then runs an optimal algorithm such a PrivUnitG in the lower dimensional space. We analyze the error of the algorithm in terms of properties of the random projection ensemble, and study two instantiations. We conduct several experiments for private mean estimation and private federated learning which demonstrate that our algorithms obtain nearly the same utility as optimal algorithms while having significantly lower communication and computational cost",
    "keywords": [],
    "checked": true,
    "id": "48f4873d8c7aa2b393118cc8a4118ee1e3cd739a",
    "semantic_title": "fast optimal locally private mean estimation via random projections",
    "citation_count": 4,
    "authors": [
      "Hilal Asi",
      "Vitaly Feldman",
      "Jelani Nelson",
      "Huy Nguyen",
      "Kunal Talwar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/348346383eb58ed19def02e233c408d6-Abstract-Conference.html": {
    "title": "Expert load matters: operating networks at high accuracy and low manual effort",
    "volume": "main",
    "abstract": "In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts. Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes.Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized.The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare. The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as confidence operating characteristic (COC) curve. In this paper, we argue that deep neural networks should be trained by taking into account both accuracy and expert load and, to that end, propose a new complementary loss function for classification that maximizes the area under this COC curve.This promotes simultaneously the increase in network accuracy and the reduction in number of samples delegated to humans.We perform experiments on multiple computer vision and medical image datasets for classification.Our results demonstrate that the proposed loss improves classification accuracy and delegates less number of decisions to experts, achieves better out-of-distribution samples detection and on par calibration performance compared to existing loss functions",
    "keywords": [],
    "checked": true,
    "id": "707ee57ec048eb55fe2fa8b383747a589e2c7af9",
    "semantic_title": "expert load matters: operating networks at high accuracy and low manual effort",
    "citation_count": 0,
    "authors": [
      "Sara Sangalli",
      "Ertunc Erdil",
      "Ender Konukoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/34dce0dc3121951dd0399ba02c0f0d06-Abstract-Conference.html": {
    "title": "PRODIGY: Enabling In-context Learning Over Graphs",
    "volume": "main",
    "abstract": "In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \\textbf{Pr}etraining \\textbf{O}ver \\textbf{D}iverse \\textbf{I}n-Context \\textbf{G}raph S\\textbf{y}stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \\emph{prompt graph} representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\\% on average with in-context learning",
    "keywords": [],
    "checked": true,
    "id": "0088c9f4d50706c7ab71efa13bcb4b42cf2058e2",
    "semantic_title": "prodigy: enabling in-context learning over graphs",
    "citation_count": 12,
    "authors": [
      "Qian Huang",
      "Hongyu Ren",
      "Peng Chen",
      "Gregor Kržmanc",
      "Daniel Zeng",
      "Percy S. Liang",
      "Jure Leskovec"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html": {
    "title": "Towards Automated Circuit Discovery for Mechanistic Interpretability",
    "volume": "main",
    "abstract": "Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors oftransformer models. This paper systematizes the mechanistic interpretability process they followed. First, researcherschoose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find whichabstract neural network units are involved in the behavior. By varying the dataset, metric, and units underinvestigation, researchers can understand the functionality of each component.We automate one of the process' steps: finding the connections between the abstract neural network units that form a circuit. We propose several algorithms and reproduce previous interpretability results to validate them. Forexample, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes theGreater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found byprevious work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery",
    "keywords": [],
    "checked": true,
    "id": "eefbd8b384a58f464827b19e30a6920ba976def9",
    "semantic_title": "towards automated circuit discovery for mechanistic interpretability",
    "citation_count": 50,
    "authors": [
      "Arthur Conmy",
      "Augustine Mavor-Parker",
      "Aengus Lynch",
      "Stefan Heimersheim",
      "Adrià Garriga-Alonso"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/34e278fbbd7d6d7d788c98065988e1a9-Abstract-Conference.html": {
    "title": "Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation",
    "volume": "main",
    "abstract": "The task of Visual Object Navigation (VON) involves an agent's ability to locate a particular object within a given scene. To successfully accomplish the VON task, two essential conditions must be fulfiled: 1) the user knows the name of the desired object; and 2) the user-specified object actually is present within the scene. To meet these conditions, a simulator can incorporate predefined object names and positions into the metadata of the scene. However, in real-world scenarios, it is often challenging to ensure that these conditions are always met. Humans in an unfamiliar environment may not know which objects are present in the scene, or they may mistakenly specify an object that is not actually present. Nevertheless, despite these challenges, humans may still have a demand for an object, which could potentially be fulfilled by other objects present within the scene in an equivalent manner. Hence, this paper proposes Demand-driven Navigation (DDN), which leverages the user's demand as the task instruction and prompts the agent to find an object which matches the specified demand. DDN aims to relax the stringent conditions of VON by focusing on fulfilling the user's demand rather than relying solely on specified object names. This paper proposes a method of acquiring textual attribute features of objects by extracting common sense knowledge from a large language model (LLM). These textual attribute features are subsequently aligned with visual attribute features using Contrastive Language-Image Pre-training (CLIP). Incorporating the visual attribute features as prior knowledge, enhances the navigation process. Experiments on AI2Thor with the ProcThor dataset demonstrate that the visual attribute features improve the agent's navigation performance and outperform the baseline methods commonly used in the VON and VLN task and methods with LLMs. The codes and demonstrations can be viewed at https://sites.google.com/view/demand-driven-navigation",
    "keywords": [],
    "checked": true,
    "id": "18e73620979ac99d775251da435733661e549041",
    "semantic_title": "find what you want: learning demand-conditioned object attribute space for demand-driven navigation",
    "citation_count": 2,
    "authors": [
      "Hongcheng Wang",
      "Andy Guan Hong Chen",
      "Xiaoqi Li",
      "Mingdong Wu",
      "Hao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/34f1c2e7ab91b6fa481ad0286a08ad02-Abstract-Conference.html": {
    "title": "On the Convergence of No-Regret Learning Dynamics in Time-Varying Games",
    "volume": "main",
    "abstract": "Most of the literature on learning in games has focused on the restrictive setting where the underlying repeated game does not change over time. Much less is known about the convergence of no-regret learning algorithms in dynamic multiagent settings. In this paper, we characterize the convergence of optimistic gradient descent (OGD) in time-varying games. Our framework yields sharp convergence bounds for the equilibrium gap of OGD in zero-sum games parameterized on natural variation measures of the sequence of games, subsuming known results for static games. Furthermore, we establish improved second-order variation bounds under strong convexity-concavity, as long as each game is repeated multiple times. Our results also apply to time-varying general-sum multi-player games via a bilinear formulation of correlated equilibria, which has novel implications for meta-learning and for obtaining refined variation-dependent regret bounds, addressing questions left open in prior papers. Finally, we leverage our framework to also provide new insights on dynamic regret guarantees in static games",
    "keywords": [],
    "checked": true,
    "id": "8ee3099493cc536827cce6c45bcfc15cb6d3085b",
    "semantic_title": "on the convergence of no-regret learning dynamics in time-varying games",
    "citation_count": 6,
    "authors": [
      "Ioannis Anagnostides",
      "Ioannis Panageas",
      "Gabriele Farina",
      "Tuomas Sandholm"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3504a4fa45685d668ce92797fbbf1895-Abstract-Conference.html": {
    "title": "Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design",
    "volume": "main",
    "abstract": "Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models and pave a path for a more informed scaling",
    "keywords": [],
    "checked": true,
    "id": "16b4620b59bfef414d702214a717856c943db7fb",
    "semantic_title": "getting vit in shape: scaling laws for compute-optimal model design",
    "citation_count": 6,
    "authors": [
      "Ibrahim M. Alabdulmohsin",
      "Xiaohua Zhai",
      "Alexander Kolesnikov",
      "Lucas Beyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3516aa3393f0279e04c099f724664f99-Abstract-Conference.html": {
    "title": "Expressive Sign Equivariant Networks for Spectral Geometric Learning",
    "volume": "main",
    "abstract": "Recent work has shown the utility of developing machine learning models that respect the structure and symmetries of eigenvectors. These works promote sign invariance, since for any eigenvector v the negation -v is also an eigenvector. However, we show that sign invariance is theoretically limited for tasks such as building orthogonally equivariant models and learning node positional encodings for link prediction in graphs. In this work, we demonstrate the benefits of sign equivariance for these tasks. To obtain these benefits, we develop novel sign equivariant neural network architectures. Our models are based on a new analytic characterization of sign equivariant polynomials and thus inherit provable expressiveness properties. Controlled synthetic experiments show that our networks can achieve the theoretically predicted benefits of sign equivariant models",
    "keywords": [],
    "checked": true,
    "id": "3b06f48c97bbd84f8fba74554ce134435d8e0ebb",
    "semantic_title": "expressive sign equivariant networks for spectral geometric learning",
    "citation_count": 5,
    "authors": [
      "Derek Lim",
      "Joshua Robinson",
      "Stefanie Jegelka",
      "Haggai Maron"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/355091f86e3e2296fbeefa10676ddb17-Abstract-Conference.html": {
    "title": "Strategic Data Sharing between Competitors",
    "volume": "main",
    "abstract": "Collaborative learning techniques have significantly advanced in recent years, enabling private model training across multiple organizations. Despite this opportunity, firms face a dilemma when considering data sharing with competitors—while collaboration can improve a company's machine learning model, it may also benefit competitors and hence reduce profits. In this work, we introduce a general framework for analyzing this data-sharing trade-off. The framework consists of three components, representing the firms' production decisions, the effect of additional data on model quality, and the data-sharing negotiation process, respectively. We then study an instantiation of the framework, based on a conventional market model from economic theory, to identify key factors that affect collaboration incentives. Our findings indicate a profound impact of market conditions on the data-sharing incentives. In particular, we find that reduced competition, in terms of the similarities between the firms' products, and harder learning tasks foster collaboration",
    "keywords": [],
    "checked": true,
    "id": "5cce7e2288cbf252e0e02d2e6a650f5659b4502b",
    "semantic_title": "strategic data sharing between competitors",
    "citation_count": 0,
    "authors": [
      "Nikita Tsoy",
      "Nikola Konstantinov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3563abb1040f4e150f4242a7282cd1ec-Abstract-Conference.html": {
    "title": "Optimal Time Complexities of Parallel Stochastic Optimization Methods Under a Fixed Computation Model",
    "volume": "main",
    "abstract": "Parallelization is a popular strategy for improving the performance of methods. Optimization methods are no exception: design of efficient parallel optimization methods and tight analysis of their theoretical properties are important research endeavors. While the minimax complexities are well known for sequential optimization methods, the theory of parallel optimization methods is less explored. In this paper, we propose a new protocol that generalizes the classical oracle framework approach. Using this protocol, we establish minimax complexities for parallel optimization methods that have access to an unbiased stochastic gradient oracle with bounded variance. We consider a fixed computation model characterized by each worker requiring a fixed but worker-dependent time to calculate stochastic gradient. We prove lower bounds and develop optimal algorithms that attain them. Our results have surprising consequences for the literature of asynchronous optimization methods",
    "keywords": [],
    "checked": true,
    "id": "79a0123444aad9b92b911e689eada3fa211f05aa",
    "semantic_title": "optimal time complexities of parallel stochastic optimization methods under a fixed computation model",
    "citation_count": 1,
    "authors": [
      "Alexander Tyurin",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/358e4a39b8ace4744fbad77e84a7e757-Abstract-Conference.html": {
    "title": "An $\\varepsilon$-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond",
    "volume": "main",
    "abstract": "We propose EB-TC$\\varepsilon$, a novel sampling rule for $\\varepsilon$-best arm identification in stochastic bandits.It is the first instance of Top Two algorithm analyzed for approximate best arm identification. EB-TC$\\varepsilon$ is an *anytime* sampling rule that can therefore be employed without modification for fixed confidence or fixed budget identification (without prior knowledge of the budget).We provide three types of theoretical guarantees for EB-TC$\\varepsilon$.First, we prove bounds on its expected sample complexity in the fixed confidence setting, notably showing its asymptotic optimality in combination with an adaptive tuning of its exploration parameter.We complement these findings with upper bounds on its probability of error at any time and for any slack parameter, which further yield upper bounds on its simple regret at any time.Finally, we show through numerical simulations that EB-TC$\\varepsilon$ performs favorably compared to existing algorithms for different approximate best arm identification tasks",
    "keywords": [],
    "checked": true,
    "id": "0705c91bf2a26d12abcb7f320bb1b61085570eab",
    "semantic_title": "an $\\varepsilon$-best-arm identification algorithm for fixed-confidence and beyond",
    "citation_count": 1,
    "authors": [
      "Marc Jourdan",
      "Rémy Degenne",
      "Emilie Kaufmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/359ddb9caccb4c54cc915dceeacf4892-Abstract-Conference.html": {
    "title": "Debiased and Denoised Entity Recognition from Distant Supervision",
    "volume": "main",
    "abstract": "While distant supervision has been extensively explored and exploited in NLP tasks like named entity recognition, a major obstacle stems from the inevitable noisy distant labels tagged unsupervisedly. A few past works approach this problem by adopting a self-training framework with a sample-selection mechanism. In this work, we innovatively identify two types of biases that were omitted by prior work, and these biases lead to inferior performance of the distant-supervised NER setup. First, we characterize the noise concealed in the distant labels as highly structural rather than fully randomized. Second, the self-training framework would ubiquitously introduce an inherent bias that causes erroneous behavior in both sample selection and eventually prediction. To cope with these problems, we propose a novel self-training framework, dubbed DesERT. This framework augments the conventional NER predicative pathway to a dual form that effectively adapts the sample-selection process to conform to its innate distributional-bias structure. The other crucial component of DesERT composes a debiased module aiming to enhance the token representations, hence the quality of the pseudo-labels. Extensive experiments are conducted to validate the DesERT. The results show that our framework establishes a new state-of-art performance, it achieves a +2.22% average F1 score improvement on five standardized benchmarking datasets. Lastly, DesERT demonstrates its effectiveness under a new DSNER benchmark where additional distant supervision comes from the ChatGPT model",
    "keywords": [],
    "checked": false,
    "id": "20af1061f27bb44dab8e09aeb80c07efc5e23ac7",
    "semantic_title": "improving distantly-supervised named entity recognition with self-collaborative denoising learning",
    "citation_count": 13,
    "authors": [
      "Haobo Wang",
      "Yiwen Dong",
      "Ruixuan Xiao",
      "Fei Huang",
      "Gang Chen",
      "Junbo Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/359ffa88712bd688963a0ca641d8330b-Abstract-Conference.html": {
    "title": "Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation",
    "volume": "main",
    "abstract": "We develop an approach to efficiently grow neural networks, within which parameterization and optimization strategies are designed by considering their effects on the training dynamics. Unlike existing growing methods, which follow simple replication heuristics or utilize auxiliary gradient-based local optimization, we craft a parameterization scheme which dynamically stabilizes weight, activation, and gradient scaling as the architecture evolves, and maintains the inference functionality of the network. To address the optimization difficulty resulting from imbalanced training effort distributed to subnetworks fading in at different growth phases, we propose a learning rate adaption mechanism that rebalances the gradient contribution of these separate subcomponents. Experiments show that our method achieves comparable or better accuracy than training large fixed-size models, while saving a substantial portion of the original training computation budget. We demonstrate that these gains translate into real wall-clock training speedups",
    "keywords": [],
    "checked": true,
    "id": "bb968f409664bc9122f04647f79ae4b99275ff1d",
    "semantic_title": "accelerated training via incrementally growing neural networks using variance transfer and learning rate adaptation",
    "citation_count": 1,
    "authors": [
      "Xin Yuan",
      "Pedro Savarese",
      "Michael Maire"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/35b5c175e139bff5f22a5361270fce87-Abstract-Conference.html": {
    "title": "Likelihood-Based Diffusion Language Models",
    "volume": "main",
    "abstract": "Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings",
    "keywords": [],
    "checked": true,
    "id": "d9ffb44ee3c8ec0b6692df8a90451384c1edd89b",
    "semantic_title": "likelihood-based diffusion language models",
    "citation_count": 6,
    "authors": [
      "Ishaan Gulrajani",
      "Tatsunori B. Hashimoto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/35c1d69d23bb5dd6b9abcd68be005d5c-Abstract-Conference.html": {
    "title": "Structural Pruning for Diffusion Models",
    "volume": "main",
    "abstract": "Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across several datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50\\% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusion models inherently preserve generative behavior congruent with their pre-trained models",
    "keywords": [],
    "checked": true,
    "id": "6f75f34440e73095afea3dd407e37d95bb1968ef",
    "semantic_title": "structural pruning for diffusion models",
    "citation_count": 18,
    "authors": [
      "Gongfan Fang",
      "Xinyin Ma",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/35fdecdf8861bc15110d48fbec3193cf-Abstract-Conference.html": {
    "title": "Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms",
    "volume": "main",
    "abstract": "This paper considers a stochastic Multi-Armed Bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \\emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present an algorithm called EOCP and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\\mathcal{O}(\\log T)$ rounds with pre-determined stopping time and $\\mathcal{O}(\\log^2 T)$ rounds with adaptive stopping time. We further characterize lower bounds on the commitment time (equivalent to the sample complexity) of ROBAI, showing that EOCP and its variants are sample optimal with pre-determined stopping time, and almost sample optimal with adaptive stopping time. Numerical results confirm our theoretical analysis and reveal an interesting ``over-exploration'' phenomenon carried by classic UCB algorithms, such that EOCP has smaller regret even though it stops exploration much earlier than UCB, i.e., $\\mathcal{O}(\\log T)$ versus $\\mathcal{O}(T)$, which suggests over-exploration is unnecessary and potentially harmful to system performance",
    "keywords": [],
    "checked": true,
    "id": "b85f16e0a89ad64afc29af407b8026b6329f039d",
    "semantic_title": "fast and regret optimal best arm identification: fundamental limits and low-complexity algorithms",
    "citation_count": 0,
    "authors": [
      "Qining Zhang",
      "Lei Ying"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/361e5112d2eca09513bbd266e4b2d2be-Abstract-Conference.html": {
    "title": "Simultaneous embedding of multiple attractor manifolds in a recurrent neural network using constrained gradient optimization",
    "volume": "main",
    "abstract": "The storage of continuous variables in working memory is hypothesized to be sustained in the brain by the dynamics of recurrent neural networks (RNNs) whose steady states form continuous manifolds. In some cases, it is thought that the synaptic connectivity supports multiple attractor manifolds, each mapped to a different context or task. For example, in hippocampal area CA3, positions in distinct environments are represented by distinct sets of population activity patterns, each forming a continuum. It has been argued that the embedding of multiple continuous attractors in a single RNN inevitably causes detrimental interference: quenched noise in the synaptic connectivity disrupts the continuity of each attractor, replacing it by a discrete set of steady states that can be conceptualized as lying on local minima of an abstract energy landscape. Consequently, population activity patterns exhibit systematic drifts towards one of these discrete minima, thereby degrading the stored memory over time. Here we show that it is possible to dramatically attenuate these detrimental interference effects by adjusting the synaptic weights. Synaptic weight adjustment are derived from a loss function that quantifies the roughness of the energy landscape along each of the embedded attractor manifolds. By minimizing this loss function, the stability of states can be dramatically improved, without compromising the capacity",
    "keywords": [],
    "checked": true,
    "id": "4668d1815b9401ba0b5af77881a8c7c5336ecd74",
    "semantic_title": "simultaneous embedding of multiple attractor manifolds in a recurrent neural network using constrained gradient optimization",
    "citation_count": 0,
    "authors": [
      "Haggai Agmon",
      "Yoram Burak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/364071531ff2398e0fb8bae31f615b69-Abstract-Conference.html": {
    "title": "Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization",
    "volume": "main",
    "abstract": "Adversarial contrastive learning (ACL) is a technique that enhances standard contrastive learning (SCL) by incorporating adversarial data to learn a robust representation that can withstand adversarial attacks and common corruptions without requiring costly annotations. To improve transferability, the existing work introduced the standard invariant regularization (SIR) to impose style-independence property to SCL, which can exempt the impact of nuisance style factors in the standard representation. However, it is unclear how the style-independence property benefits ACL-learned robust representations. In this paper, we leverage the technique of causal reasoning to interpret the ACL and propose adversarial invariant regularization (AIR) to enforce independence from style factors. We regulate the ACL using both SIR and AIR to output the robust representation. Theoretically, we show that AIR implicitly encourages the representational distance between different views of natural data and their adversarial variants to be independent of style factors. Empirically, our experimental results show that invariant regularization significantly improves the performance of state-of-the-art ACL methods in terms of both standard generalization and robustness on downstream tasks. To the best of our knowledge, we are the first to apply causal reasoning to interpret ACL and develop AIR for enhancing ACL-learned robust representations. Our source code is at https://github.com/GodXuxilie/EnhancingACLvia_AIR",
    "keywords": [],
    "checked": true,
    "id": "7db8b1f1ab13558fb5e657010a44837997c981fa",
    "semantic_title": "enhancing adversarial contrastive learning via adversarial invariant regularization",
    "citation_count": 2,
    "authors": [
      "Xilie Xu",
      "Jingfeng ZHANG",
      "Feng Liu",
      "Masashi Sugiyama",
      "Mohan S. Kankanhalli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/364d565b4b726c607aa40e1632045873-Abstract-Conference.html": {
    "title": "Best Arm Identification with Fixed Budget: A Large Deviation Perspective",
    "volume": "main",
    "abstract": "We consider the problem of identifying the best arm in stochastic Multi-Armed Bandits (MABs) using a fixed sampling budget. Characterizing the minimal instance-specific error probability for this problem constitutes one of the important remaining open problems in MABs. When arms are selected using a static sampling strategy, the error probability decays exponentially with the number of samples at a rate that can be explicitly derived via Large Deviation techniques. Analyzing the performance of algorithms with adaptive sampling strategies is however much more challenging. In this paper, we establish a connection between the Large Deviation Principle (LDP) satisfied by the empirical proportions of arm draws and that satisfied by the empirical arm rewards. This connection holds for any adaptive algorithm, and is leveraged (i) to improve error probability upper bounds of some existing algorithms, such as the celebrated SR (Successive Rejects) algorithm \\cite{audibert2010best}, and (ii) to devise and analyze new algorithms. In particular, we present CR (Continuous Rejects), a truly adaptive algorithm that can reject arms in {\\it any} round based on the observed empirical gaps between the rewards of various arms. Applying our Large Deviation results, we prove that CR enjoys better performance guarantees than existing algorithms, including SR. Extensive numerical experiments confirm this observation",
    "keywords": [],
    "checked": true,
    "id": "218289d526a0565052269d6e3088edd8273cf068",
    "semantic_title": "best arm identification with fixed budget: a large deviation perspective",
    "citation_count": 2,
    "authors": [
      "Po-An Wang",
      "Ruo-Chun Tzeng",
      "Alexandre Proutiere"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/365a6f71486ecdfa7eb8d61cbe168782-Abstract-Conference.html": {
    "title": "Full-Atom Protein Pocket Design via Iterative Refinement",
    "volume": "main",
    "abstract": "The design of \\emph{de novo} functional proteins that bind with specific ligand molecules is crucial in various domains like therapeutics and bio-engineering. One vital yet challenging step is to design the protein pocket, the cavity region of protein where the ligand binds with. Existing methods suffer from inefficient generation, insufficient context modeling (ligand molecule), and incapability of generating sidechain atoms. To overcome the limitations, we propose a \\textbf{F}ull-\\textbf{A}tom \\textbf{I}terative \\textbf{R}efinement framework (\\textbf{FAIR}) for protein pocket sequence (i.e., residue types) and 3D structure co-design. Generally, FAIR consists of two steps that follow a coarse-to-fine pipeline (backbone atoms to full atoms including sidechain) for full-atom generation. For efficiency, all residue types and structures are updated together in each round (i.e., full-shot refinement). In the first step, the residue types and backbone coordinates are updated with a hierarchical context encoder and two structure refinement modules capturing inter-residue and pocket-ligand interactions. The second step further models the sidechain atoms of pockets and updates residue types to achieve sequence-structure consistency. The structure of the binding ligand is also updated along with the above refinement iterations accounting for its flexibility. Finally, extensive evaluations showthat FAIR outperforms baselines in efficiently designing high-quality pocket sequences and structures. Specifically, the average improvements on AAR and RMSD are over 10$\\%$",
    "keywords": [],
    "checked": true,
    "id": "7850333f1958889d73e262831d3c4587120abd91",
    "semantic_title": "full-atom protein pocket design via iterative refinement",
    "citation_count": 4,
    "authors": [
      "ZAIXI ZHANG",
      "Zepu Lu",
      "Hao Zhongkai",
      "Marinka Zitnik",
      "Qi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3663ae53ec078860bb0b9c6606e092a0-Abstract-Conference.html": {
    "title": "Flow Matching for Scalable Simulation-Based Inference",
    "volume": "main",
    "abstract": "Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures---making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30\\% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems",
    "keywords": [],
    "checked": true,
    "id": "ecf1530ff9cc50058fb915329b59161e461c371c",
    "semantic_title": "flow matching for scalable simulation-based inference",
    "citation_count": 7,
    "authors": [
      "Jonas Wildberger",
      "Maximilian Dax",
      "Simon Buchholz",
      "Stephen Green",
      "Jakob H Macke",
      "Bernhard Schölkopf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/367ab3106d990825d5b47ce91db75a73-Abstract-Conference.html": {
    "title": "Learning DAGs from Data with Few Root Causes",
    "volume": "main",
    "abstract": "We present a novel perspective and algorithm for learning directed acyclic graphs (DAGs) from data generated by a linear structural equation model (SEM). First, we show that a linear SEM can be viewed as a linear transform that, in prior work, computes the data from a dense input vector of random valued root causes (as we will call them) associated with the nodes. Instead, we consider the case of (approximately) few root causes and also introduce noise in the measurement of the data. Intuitively, this means that the DAG data is produced by few data generating events whose effect percolates through the DAG. We prove identifiability in this new setting and show that the true DAG is the global minimizer of the $L^0$-norm of the vector of root causes. For data satisfying the few root causes assumption, we show superior performance compared to prior DAG learning methods",
    "keywords": [],
    "checked": true,
    "id": "5d55b3ad5426547b002a8998818fee9cab0b1160",
    "semantic_title": "learning dags from data with few root causes",
    "citation_count": 3,
    "authors": [
      "Panagiotis Misiakos",
      "Chris Wendler",
      "Markus Püschel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/36848567d39a5128e671ad04a6075374-Abstract-Conference.html": {
    "title": "Robust Learning for Smoothed Online Convex Optimization with Feedback Delay",
    "volume": "main",
    "abstract": "We study a general form of Smoothed Online Convex Optimization, a.k.a. SOCO, including multi-step switching costs and feedback delay. We propose a novel machine learning (ML) augmented online algorithm, Robustness-Constrained Learning (RCL), which combines untrusted ML predictions with a trusted expert online algorithm via constrained projection to robustify the ML prediction. Specifically, we prove that RCL is able to guarantee $(1+\\lambda)$-competitiveness against any given expert for any $\\lambda>0$, while also explicitly training the ML model in a robustification-aware manner to improve the average-case performance. Importantly, RCL is the first ML-augmented algorithm with a provable robustness guarantee in the case of multi-step switching cost and feedback delay. We demonstrate the improvement of RCL in both robustness and average performance using battery management as a case study",
    "keywords": [],
    "checked": true,
    "id": "48a64a2d9ce317cb0a4daa796b3d34cbff0145ed",
    "semantic_title": "robust learning for smoothed online convex optimization with feedback delay",
    "citation_count": 0,
    "authors": [
      "Pengfei Li",
      "Jianyi Yang",
      "Adam Wierman",
      "Shaolei Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/368559ed8ede03b21f624feaeb3a5867-Abstract-Conference.html": {
    "title": "Scalarization for Multi-Task and Multi-Domain Learning at Scale",
    "volume": "main",
    "abstract": "Training a single model on multiple input domains and/or output tasks allows for compressing information from multiple sources into a unified backbone hence improves model efficiency. It also enables potential positive knowledge transfer across tasks/domains, leading to improved accuracy and data-efficient training. However, optimizing such networks is a challenge, in particular due to discrepancies between the different tasks or domains: Despite several hypotheses and solutions proposed over the years, recent work has shown that uniform scalarization training, i.e., simply minimizing the average of the task losses, yields on-par performance with more costly SotA optimization methods. This raises the issue of how well we understand the training dynamics of multi-task and multi-domain networks. In this work, we first devise a large-scale unified analysis of multi-domain and multi-task learning to better understand the dynamics of scalarization across varied task/domain combinations and model sizes. Following these insights, we then propose to leverage population-based training to efficiently search for the optimal scalarization weights when dealing with a large number of tasks or domains",
    "keywords": [],
    "checked": true,
    "id": "611bd91e0050185319513493ad9a135a639438bd",
    "semantic_title": "scalarization for multi-task and multi-domain learning at scale",
    "citation_count": 1,
    "authors": [
      "Amelie Royer",
      "Tijmen Blankevoort",
      "Babak Ehteshami Bejnordi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/368cba57d00902c752eaa9e4770bbbbe-Abstract-Conference.html": {
    "title": "Causal discovery from observational and interventional data across multiple environments",
    "volume": "main",
    "abstract": "A fundamental problem in many sciences is the learning of causal structure underlying a system, typically through observation and experimentation. Commonly, one even collects data across multiple domains, such as gene sequencing from different labs, or neural recordings from different species. Although there exist methods for learning the equivalence class of causal diagrams from observational and experimental data, they are meant to operate in a single domain. In this paper, we develop a fundamental approach to structure learning in non-Markovian systems (i.e. when there exist latent confounders) leveraging observational and interventional data collected from multiple domains. Specifically, we start by showing that learning from observational data in multiple domains is equivalent to learning from interventional data with unknown targets in a single domain. But there are also subtleties when considering observational and experimental data. Using causal invariances derived from do-calculus, we define a property called S-Markov that connects interventional distributions from multiple-domains to graphical criteria on a selection diagram. Leveraging the S-Markov property, we introduce a new constraint-based causal discovery algorithm, S-FCI, that can learn from observational and interventional data from different domains. We prove that the algorithm is sound and subsumes existing constraint-based causal discovery algorithms",
    "keywords": [],
    "checked": true,
    "id": "013b1e73a335ad3874b52075546c3e9f108480d6",
    "semantic_title": "causal discovery from observational and interventional data across multiple environments",
    "citation_count": 1,
    "authors": [
      "Adam Li",
      "Amin Jaber",
      "Elias Bareinboim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/36b80eae70ff629d667f210e13497edf-Abstract-Conference.html": {
    "title": "Beyond Normal: On the Evaluation of Mutual Information Estimators",
    "volume": "main",
    "abstract": "Mutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an estimator to a new data set",
    "keywords": [],
    "checked": true,
    "id": "89e5426f2abecb211034851cbc5f4f4e1a0c8304",
    "semantic_title": "beyond normal: on the evaluation of mutual information estimators",
    "citation_count": 6,
    "authors": [
      "Paweł Czyż",
      "Frederic Grabowski",
      "Julia Vogt",
      "Niko Beerenwinkel",
      "Alexander Marx"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/36ce475705c1dc6c50a5956cedff3d01-Abstract-Conference.html": {
    "title": "Structured Semidefinite Programming for Recovering Structured Preconditioners",
    "volume": "main",
    "abstract": "We develop a general framework for finding approximately-optimal preconditioners for solving linear systems. Leveraging this framework we obtain improved runtimes for fundamental preconditioning and linear system solving problems including:Diagonal preconditioning. We give an algorithm which, given positive definite $\\mathbf{K} \\in \\mathbb{R}^{d \\times d}$ with $\\mathrm{nnz}(\\mathbf{K})$ nonzero entries, computes an $\\epsilon$-optimal diagonal preconditioner in time $\\widetilde{O}(\\mathrm{nnz}(\\mathbf{K}) \\cdot \\mathrm{poly}(\\kappa^\\star,\\epsilon^{-1}))$, where $\\kappa^\\star$ is the optimal condition number of the rescaled matrix.Structured linear systems. We give an algorithm which, given $\\mathbf{M} \\in \\mathbb{R}^{d \\times d}$ that is either the pseudoinverse of a graph Laplacian matrix or a constant spectral approximation of one, solves linear systems in $\\mathbf{M}$ in $\\widetilde{O}(d^2)$ time. Our diagonal preconditioning results improve state-of-the-art runtimes of $\\Omega(d^{3.5})$ attained by general-purpose semidefinite programming, and our solvers improve state-of-the-art runtimes of $\\Omega(d^{\\omega})$ where $\\omega > 2.3$ is the current matrix multiplication constant. We attain our results via new algorithms for a class of semidefinite programs (SDPs) we call matrix-dictionary approximation SDPs, which we leverage to solve an associated problem we call matrix-dictionary recovery",
    "keywords": [],
    "checked": true,
    "id": "f858f1b22d3505149f69b6613d44344a1da45035",
    "semantic_title": "structured semidefinite programming for recovering structured preconditioners",
    "citation_count": 1,
    "authors": [
      "Arun Jambulapati",
      "Jerry Li",
      "Christopher Musco",
      "Kirankumar Shiragur",
      "Aaron Sidford",
      "Kevin Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/37050ebbbd7096719ab96cec19a4c69f-Abstract-Conference.html": {
    "title": "Certifiably Robust Graph Contrastive Learning",
    "volume": "main",
    "abstract": "Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph representation learning method. However, it has been shown that GCL is vulnerable to adversarial attacks on both the graph structure and node attributes. Although empirical approaches have been proposed to enhance the robustness of GCL, the certifiable robustness of GCL is still remain unexplored. In this paper, we develop the first certifiably robust framework in GCL. Specifically, we first propose a unified criteria to evaluate and certify the robustness of GCL. We then introduce a novel technique, RES (Randomized Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and this certified robustness can be provably preserved in downstream tasks. Furthermore, an effective training method is proposed for robust GCL. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method in providing effective certifiable robustness and enhancing the robustness of any GCL model. The source code of RES is available at https://github.com/ventr1c/RES-GCL",
    "keywords": [],
    "checked": true,
    "id": "f51c6d89283decef2deb76dbcc6f9122f39fa506",
    "semantic_title": "certifiably robust graph contrastive learning",
    "citation_count": 0,
    "authors": [
      "Minhua Lin",
      "Teng Xiao",
      "Enyan Dai",
      "Xiang Zhang",
      "Suhang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/37094fdc81632915a5738293cf9b7ad4-Abstract-Conference.html": {
    "title": "FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy",
    "volume": "main",
    "abstract": "Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores",
    "keywords": [],
    "checked": true,
    "id": "025a6b2d22f4f7761922bf8bedd04f1c54529387",
    "semantic_title": "face: evaluating natural language generation with fourier analysis of cross-entropy",
    "citation_count": 1,
    "authors": [
      "Zuhao Yang",
      "Yingfang Yuan",
      "Yang Xu",
      "SHUO ZHAN",
      "Huajun Bai",
      "Kefan Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/370fa2e691f57eb319bc263a07dad4a5-Abstract-Conference.html": {
    "title": "3D Copy-Paste: Physically Plausible Object Insertion for Monocular 3D Detection",
    "volume": "main",
    "abstract": "A major challenge in monocular 3D object detection is the limited diversity and quantity of objects in real datasets. While augmenting real scenes with virtual objects holds promise to improve both the diversity and quantity of the objects, it remains elusive due to the lack of an effective 3D object insertion method in complex real captured scenes. In this work, we study augmenting complex real indoor scenes with virtual objects for monocular 3D object detection. The main challenge is to automatically identify plausible physical properties for virtual assets (e.g., locations, appearances, sizes, etc.) in cluttered real scenes. To address this challenge, we propose a physically plausible indoor 3D object insertion approach to automatically copy virtual objects and paste them into real scenes. The resulting objects in scenes have 3D bounding boxes with plausible physical locations and appearances. In particular, our method first identifies physically feasible locations and poses for the inserted objects to prevent collisions with the existing room layout. Subsequently, it estimates spatially-varying illumination for the insertion location, enabling the immersive blending of the virtual objects into the original scene with plausible appearances and cast shadows. We show that our augmentation method significantly improves existing monocular 3D object models and achieves state-of-the-art performance. For the first time, we demonstrate that a physically plausible 3D object insertion, serving as a generative data augmentation technique, can lead to significant improvements for discriminative downstream tasks such as monocular 3D object detection. Code: https://github.com/gyhandy/3D-Copy-Paste",
    "keywords": [],
    "checked": true,
    "id": "1d39a9df99c7ea2b956749b16f9d482499ca0cef",
    "semantic_title": "3d copy-paste: physically plausible object insertion for monocular 3d detection",
    "citation_count": 1,
    "authors": [
      "Yunhao Ge",
      "Hong-Xing Yu",
      "Cheng Zhao",
      "Yuliang Guo",
      "Xinyu Huang",
      "Liu Ren",
      "Laurent Itti",
      "Jiajun Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/371355cd42caaf83412c3fbef4688979-Abstract-Conference.html": {
    "title": "Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions",
    "volume": "main",
    "abstract": "Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation",
    "keywords": [],
    "checked": true,
    "id": "cb0ac335adda4ceef9987cbcbca9129e71c37f0a",
    "semantic_title": "laughing hyena distillery: extracting compact recurrences from convolutions",
    "citation_count": 2,
    "authors": [
      "Stefano Massaroli",
      "Michael Poli",
      "Dan Fu",
      "Hermann Kumbong",
      "Rom Parnichkun",
      "David Romero",
      "Aman Timalsina",
      "Quinn McIntyre",
      "Beidi Chen",
      "Atri Rudra",
      "Ce Zhang",
      "Christopher Ré",
      "Stefano Ermon",
      "Yoshua Bengio"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/372cb7805eaccb2b7eed641271a30eec-Abstract-Conference.html": {
    "title": "Incomplete Multimodality-Diffused Emotion Recognition",
    "volume": "main",
    "abstract": "Human multimodal emotion recognition (MER) aims to perceive and understand human emotions via various heterogeneous modalities, such as language, vision, and acoustic. Compared with unimodality, the complementary information in the multimodalities facilitates robust emotion understanding. Nevertheless, in real-world scenarios, the missing modalities hinder multimodal understanding and result in degraded MER performance. In this paper, we propose an Incomplete Multimodality-Diffused emotion recognition (IMDer) method to mitigate the challenge of MER under incomplete multimodalities. To recover the missing modalities, IMDer exploits the score-based diffusion model that maps the input Gaussian noise into the desired distribution space of the missing modalities and recovers missing data abided by their original distributions. Specially, to reduce semantic ambiguity between the missing and the recovered modalities, the available modalities are embedded as the condition to guide and refine the diffusion-based recovering process. In contrast to previous work, the diffusion-based modality recovery mechanism in IMDer allows to simultaneously reach both distribution consistency and semantic disambiguation. Feature visualization of the recovered modalities illustrates the consistent modality-specific distribution and semantic alignment. Besides, quantitative experimental results verify that IMDer obtains state-of-the-art MER accuracy under various missing modality patterns",
    "keywords": [],
    "checked": false,
    "id": "859ccb98f1ca79b1ddb1ff6d3f7a7766762a26df",
    "semantic_title": "multimodal learning with incompleteness towards multimodal sentiment analysis and emotion recognition task",
    "citation_count": 0,
    "authors": [
      "Yuanzhi Wang",
      "Yong Li",
      "Zhen Cui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/374050dc3f211267bd6bf0ea24eae184-Abstract-Conference.html": {
    "title": "Diffusion-Based Probabilistic Uncertainty Estimation for Active Domain Adaptation",
    "volume": "main",
    "abstract": "Active Domain Adaptation (ADA) has emerged as an attractive technique for assisting domain adaptation by actively annotating a small subset of target samples. Most ADA methods focus on measuring the target representativeness beyond traditional active learning criteria to handle the domain shift problem, while leaving the uncertainty estimation to be performed by an uncalibrated deterministic model. In this work, we introduce a probabilistic framework that captures both data-level and prediction-level uncertainties beyond a point estimate. Specifically, we use variational inference to approximate the joint posterior distribution of latent representation and model prediction. The variational objective of labeled data can be formulated by a variational autoencoder and a latent diffusion classifier, and the objective of unlabeled data can be implemented in a knowledge distillation framework. We utilize adversarial learning to ensure an invariant latent space. The resulting diffusion classifier enables efficient sampling of all possible predictions for each individual to recover the predictive distribution. We then leverage a t-test-based criterion upon the sampling and select informative unlabeled target samples based on the p-value, which encodes both prediction variability and cross-category ambiguity. Experiments on both ADA and Source-Free ADA settings show that our method provides more calibrated predictions than previous ADA methods and achieves favorable performance on three domain adaptation datasets",
    "keywords": [],
    "checked": false,
    "id": "d4838211d7f65628f56b9f6faab30a95ff7b51f8",
    "semantic_title": "for prediction city region re-weighting",
    "citation_count": 0,
    "authors": [
      "Zhekai Du",
      "Jingjing Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3755a02b1035fbadd5f93a022170e46f-Abstract-Conference.html": {
    "title": "Augmented Memory Replay-based Continual Learning Approaches for Network Intrusion Detection",
    "volume": "main",
    "abstract": "Intrusion detection is a form of anomalous activity detection in communication network traffic. Continual learning (CL) approaches to the intrusion detection task accumulate old knowledge while adapting to the latest threat knowledge. Previous works have shown the effectiveness of memory replay-based CL approaches for this task. In this work, we present two novel contributions to improve the performance of CL-based network intrusion detection in the context of class imbalance and scalability. First, we extend class balancing reservoir sampling (CBRS), a memory-based CL method, to address the problems of severe class imbalance for large datasets. Second, we propose a novel approach titled perturbation assistance for parameter approximation (PAPA) based on the Gaussian mixture model to reduce the number of \\textit{virtual stochastic gradient descent (SGD) parameter} computations needed to discover maximally interfering samples for CL. We demonstrate that the proposed approaches perform remarkably better than the baselines on standard intrusion detection benchmarks created over shorter periods (KDDCUP'99, NSL-KDD, CICIDS-2017/2018, UNSW-NB15, and CTU-13) and a longer period with distribution shift (AnoShift). We also validated proposed approaches on standard continual learning benchmarks (SVHN, CIFAR-10/100, and CLEAR-10/100) and anomaly detection benchmarks (SMAP, SMD, and MSL). Further, the proposed PAPA approach significantly lowers the number of virtual SGD update operations, thus resulting in training time savings in the range of 12 to 40\\% compared to the maximally interfered samples retrieval algorithm",
    "keywords": [],
    "checked": false,
    "id": "15d0c308c77a078836db7b68b16d93972e7aa66f",
    "semantic_title": "analysis of continual learning models for intrusion detection system",
    "citation_count": 3,
    "authors": [
      "suresh kumar amalapuram",
      "Sumohana Channappayya",
      "Bheemarjuna Reddy Tamma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/376276a95781fa17c177b1ccdd0a03ac-Abstract-Conference.html": {
    "title": "Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models",
    "volume": "main",
    "abstract": "The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models",
    "keywords": [],
    "checked": true,
    "id": "4e75ae56dc134abb076c3c6513d4d80751393df1",
    "semantic_title": "selective amnesia: a continual learning approach to forgetting in deep generative models",
    "citation_count": 21,
    "authors": [
      "Alvin Heng",
      "Harold Soh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3764a9c8abc84c7482f778fefc24f10b-Abstract-Conference.html": {
    "title": "Structure from Duplicates: Neural Inverse Graphics from a Pile of Objects",
    "volume": "main",
    "abstract": "Abstract Our world is full of identical objects (\\emph{e.g.}, cans of coke, cars of same model). These duplicates, when seen together, provide additional and strong cues for us to effectively reason about 3D. Inspired by this observation, we introduce Structure from Duplicates (SfD), a novel inverse graphics framework that reconstructs geometry, material, and illumination from a single image containing multiple identical objects. SfD begins by identifying multiple instances of an object within an image, and then jointly estimates the 6DoF pose for all instances. An inverse graphics pipeline is subsequently employed to jointly reason about the shape, material of the object, and the environment light, while adhering to the shared geometry and material constraint across instances.Our primary contributions involve utilizing object duplicates as a robust prior for single-image inverse graphics and proposing an in-plane rotation-robust Structure from Motion (SfM) formulation for joint 6-DoF object pose estimation. By leveraging multi-view cues from a single image, SfD generates more realistic and detailed 3D reconstructions, significantly outperforming existing single image reconstruction models and multi-view reconstruction approaches with a similar or greater number of observations",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhang Cheng",
      "Wei-Chiu Ma",
      "Kaiyu Guan",
      "Antonio Torralba",
      "Shenlong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/377b2e39e97e917b9e625b35241e33df-Abstract-Conference.html": {
    "title": "Weakly-Supervised Audio-Visual Segmentation",
    "volume": "main",
    "abstract": "Audio-visual segmentation is a challenging task that aims to predict pixel-level masks for sound sources in a video. Previous work applied a comprehensive manually designed architecture with countless pixel-wise accurate masks as supervision. However, these pixel-level masks are expensive and not available in all cases. In this work, we aim to simplify the supervision as the instance-level annotation, $\\textit{i.e.}$, weakly-supervised audio-visual segmentation. We present a novel Weakly-Supervised Audio-Visual Segmentation framework, namely WS-AVS, that can learn multi-scale audio-visual alignment with multi-scale multiple-instance contrastive learning for audio-visual segmentation. Extensive experiments on AVSBench demonstrate the effectiveness of our WS-AVS in the weakly-supervised audio-visual segmentation of single-source and multi-source scenarios",
    "keywords": [],
    "checked": true,
    "id": "2b05f911cecf2c49b445f9eb0bd1ae9df9598c73",
    "semantic_title": "weakly-supervised audio-visual segmentation",
    "citation_count": 1,
    "authors": [
      "Shentong Mo",
      "Bhiksha Raj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/378b284f7f03274d1bf5322bb15c5c16-Abstract-Conference.html": {
    "title": "Adversarial Examples Are Not Real Features",
    "volume": "main",
    "abstract": "The existence of adversarial examples has been a mystery for years and attracted much interest. A well-known theory by \\citet{ilyas2019adversarial} explains adversarial vulnerability from a data perspective by showing that one can extract non-robust features from adversarial examples and these features alone are useful for classification. However, the explanation remains quite counter-intuitive since non-robust features are mostly noise features to humans. In this paper, we re-examine the theory from a larger context by incorporating multiple learning paradigms. Notably, we find that contrary to their good usefulness under supervised learning, non-robust features attain poor usefulness when transferred to other self-supervised learning paradigms, such as contrastive learning, masked image modeling, and diffusion models. It reveals that non-robust features are not really as useful as robust or natural features that enjoy good transferability between these paradigms. Meanwhile, for robustness, we also show that naturally trained encoders from robust features are largely non-robust under AutoAttack. Our cross-paradigm examination suggests that the non-robust features are not really useful but more like paradigm-wise shortcuts, and robust features alone might be insufficient to attain reliable model robustness. Code is available at \\url{https://github.com/PKU-ML/AdvNotRealFeatures}",
    "keywords": [],
    "checked": true,
    "id": "8889c5a2326b137fd544ec933c1136e9e38eca27",
    "semantic_title": "adversarial examples are not real features",
    "citation_count": 1,
    "authors": [
      "Ang Li",
      "Yifei Wang",
      "Yiwen Guo",
      "Yisen  Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3815d62554efad0878fad6c1c30ffda0-Abstract-Conference.html": {
    "title": "Online Ad Allocation with Predictions",
    "volume": "main",
    "abstract": "Display Ads and the generalized assignment problem are two well-studied online packing problems with important applications in ad allocation and other areas. In both problems, ad impressions arrive online and have to be allocated immediately to budget-constrained advertisers. Worst-case algorithms that achieve the ideal competitive ratio are known for both problems, but might act overly conservative given the predictable and usually tame nature of real-world input. Given this discrepancy, we develop an algorithm for both problems that incorporate machine-learned predictions and can thus improve the performance beyond the worst-case. Our algorithm is based on the work of Feldman et al. (2009) and similar in nature to Mahdian et al. (2007) who were the first to develop a learning-augmented algorithm for the related, but more structured Ad Words problem. We use a novel analysis to show that our algorithm is able to capitalize on a good prediction, while being robust against poor predictions. We experimentally evaluate our algorithm on synthetic and real-world data on a wide range of predictions. Our algorithm is consistently outperforming the worst-case algorithm without predictions",
    "keywords": [],
    "checked": true,
    "id": "0a8a7deaeb94c32967e89e97afd45151c73816c8",
    "semantic_title": "online ad allocation with predictions",
    "citation_count": 0,
    "authors": [
      "Fabian Spaeh",
      "Alina Ene"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3819a070922cc0d19f3d66ce108f28e0-Abstract-Conference.html": {
    "title": "Transfer Learning with Affine Model Transformation",
    "volume": "main",
    "abstract": "Supervised transfer learning has received considerable attention due to its potential to boost the predictive power of machine learning in scenarios where data are scarce. Generally, a given set of source models and a dataset from a target domain are used to adapt the pre-trained models to a target domain by statistically learning domain shift and domain-specific factors. While such procedurally and intuitively plausible methods have achieved great success in a wide range of real-world applications, the lack of a theoretical basis hinders further methodological development. This paper presents a general class of transfer learning regression called affine model transfer, following the principle of expected-square loss minimization. It is shown that the affine model transfer broadly encompasses various existing methods, including the most common procedure based on neural feature extractors. Furthermore, the current paper clarifies theoretical properties of the affine model transfer such as generalization error and excess risk. Through several case studies, we demonstrate the practical benefits of modeling and estimating inter-domain commonality and domain-specific factors separately with the affine-type transfer models",
    "keywords": [],
    "checked": true,
    "id": "d2ce629d4ead023dd8a24ce76967410b6d1352dd",
    "semantic_title": "transfer learning with affine model transformation",
    "citation_count": 0,
    "authors": [
      "Shunya Minami",
      "Kenji Fukumizu",
      "Yoshihiro Hayashi",
      "Ryo Yoshida"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/381d36bf8e115cdeda48763c9cb77616-Abstract-Conference.html": {
    "title": "Towards Robust and Expressive Whole-body Human Pose and Shape Estimation",
    "volume": "main",
    "abstract": "Whole-body pose and shape estimation aims to jointly predict different behaviors (e.g., pose, hand gesture, facial expression) of the entire human body from a monocular image. Existing methods often exhibit suboptimal performance due to the complexity of in-the-wild scenarios. We argue that the prediction accuracy of these models is significantly affected by the quality of the bounding box, e.g., scale, alignment. The natural discrepancy between the ideal bounding box annotations and model detection results is particularly detrimental to the performance of whole-body pose and shape estimation.In this paper, we propose a novel framework to enhance the robustness of whole-body pose and shape estimation. Our framework incorporates three new modules to address the above challenges from three perspectives: (1) a Localization Module enhances the model's awareness of the subject's location and semantics within the image space; (2) a Contrastive Feature Extraction Module encourages the model to be invariant to robust augmentations by incorporating a contrastive loss and positive samples; (3) a Pixel Alignment Module ensures the reprojected mesh from the predicted camera and body model parameters are more accurate and pixel-aligned. We perform comprehensive experiments to demonstrate the effectiveness of our proposed framework on body, hands, face and whole-body benchmarks",
    "keywords": [],
    "checked": true,
    "id": "bf78a3ef490de1c3463cdc7efe9ab5f17c40afce",
    "semantic_title": "towards robust and expressive whole-body human pose and shape estimation",
    "citation_count": 0,
    "authors": [
      "Hui En Pang",
      "Zhongang Cai",
      "Lei Yang",
      "Qingyi Tao",
      "Zhonghua Wu",
      "Tianwei Zhang",
      "Ziwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/382a8606a85ca6ec7c06185a1a95ce8b-Abstract-Conference.html": {
    "title": "Neural Lad: A Neural Latent Dynamics Framework for Times Series Modeling",
    "volume": "main",
    "abstract": "Neural ordinary differential equation (Neural ODE) is an elegant yet powerful framework to learn the temporal dynamics for time series modeling.However, we observe that existing Neural ODE forecasting models suffer from two disadvantages:i) controlling the latent states only through the linear transformation over the local change of the observed signals may be inadequate;ii) lacking the ability to capture the inherent periodical property in time series forecasting tasks;To overcome the two issues, we introduce a new neural ODE framework called \\textbf{Neural Lad}, a \\textbf{Neural} \\textbf{La}tent \\textbf{d}ynamics model in which the latent representations evolve with an ODE enhanced by the change of observed signal and seasonality-trend characterization. We incorporate the local change of input signal into the latent dynamics in an attention-based manner and design a residual architecture over basis expansion to depict the periodicity in the underlying dynamics. To accommodate the multivariate time series forecasting, we extend the Neural Lad through learning an adaptive relationship between multiple time series. Experiments demonstrate that our model can achieve better or comparable performance against existing neural ODE families and transformer variants in various datasets. Remarkably, the empirical superiority of Neural Lad is consistent across short and long-horizon forecasting for both univariate, multivariate and even irregular sampled time series",
    "keywords": [],
    "checked": false,
    "id": "c777d00b967db948b903490190cb621c86dfc818",
    "semantic_title": "2022 bohrer memorial student workshop in statistics",
    "citation_count": 0,
    "authors": [
      "ting li",
      "Jianguo Li",
      "Zhanxing Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/38687a6a01f127d6db92561508a225b7-Abstract-Conference.html": {
    "title": "Weighted ROC Curve in Cost Space: Extending AUC to Cost-Sensitive Learning",
    "volume": "main",
    "abstract": "In this paper, we aim to tackle flexible cost requirements for long-tail datasets, where we need to construct a (a) cost-sensitive and (b) class-distribution robust learning framework. The misclassification cost and the area under the ROC curve (AUC) are popular metrics for (a) and (b), respectively. However, limited by their formulations, models trained with AUC cannot be applied to cost-sensitive decision problems, and models trained with fixed costs are sensitive to the class distribution shift. To address this issue, we present a new setting where costs are treated like a dataset to deal with arbitrarily unknown cost distributions. Moreover, we propose a novel weighted version of AUC where the cost distribution can be integrated into its calculation through decision thresholds. To formulate this setting, we propose a novel bilevel paradigm to bridge weighted AUC (WAUC) and cost. The inner-level problem approximates the optimal threshold from sampling costs, and the outer-level problem minimizes the WAUC loss over the optimal threshold distribution. To optimize this bilevel paradigm, we employ a stochastic optimization algorithm (SACCL) to optimize it. Finally, experiment results show that our algorithm performs better than existing cost-sensitive learning methods and two-stage AUC decisions approach",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HuiYang Shao",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Peisong Wen",
      "Gao Peifeng",
      "Qingming Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/387982dbf23d9975c7fc45813dd3dabc-Abstract-Conference.html": {
    "title": "Dynamic Non-monotone Submodular Maximization",
    "volume": "main",
    "abstract": "Maximizing submodular functions has been increasingly used in many applications of machine learning, such as data summarization, recommendation systems, and feature selection. Moreover, there has been a growing interest in both submodular maximization and dynamic algorithms. In 2020, Monemizadeh and Lattanzi, Mitrovic, Norouzi-Fard, Tarnawski, and Zadimoghaddam initiated developing dynamic algorithms for the monotone submodular maximization problem under the cardinality constraint $k$. In 2022, Chen and Peng studied the complexity of this problem and raised an important open question: \"\\emph{Can we extend [fully dynamic] results (algorithm or hardness) to non-monotone submodular maximization?}\". We affirmatively answer their question by demonstrating a reduction from maximizing a non-monotone submodular function under the cardinality constraint $k$ to maximizing a monotone submodular function under the same constraint. Through this reduction, we obtain the first dynamic algorithms to solve the non-monotone submodular maximization problem under the cardinality constraint $k$. Our algorithms maintain an $(8+\\epsilon)$-approximate of the solution and use expected amortized $O(\\epsilon^{-3}k^3\\log^3(n)\\log(k))$ or $O(\\epsilon^{-1}k^2\\log^3(k))$ oracle queries per update, respectively. Furthermore, we showcase the benefits of our dynamic algorithm for video summarization and max-cut problems on several real-world data sets",
    "keywords": [],
    "checked": true,
    "id": "e20bd78f2e00e223343f309e8cbc5b5b2b7644ad",
    "semantic_title": "dynamic non-monotone submodular maximization",
    "citation_count": 0,
    "authors": [
      "Kiarash Banihashem",
      "Leyla Biabani",
      "Samira Goudarzi",
      "MohammadTaghi Hajiaghayi",
      "Peyman Jabbarzade",
      "Morteza Monemizadeh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3882ca2c952276247fe9a993193b00e4-Abstract-Conference.html": {
    "title": "Semi-Implicit Denoising Diffusion Models (SIDDMs)",
    "volume": "main",
    "abstract": "Despite the proliferation of generative models, achieving fast sampling during inference without compromising sample diversity and quality remains challenging. Existing models such as Denoising Diffusion Probabilistic Models (DDPM) deliver high-quality, diverse samples but are slowed by an inherently high number of iterative steps. The Denoising Diffusion Generative Adversarial Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN model for larger jumps in the diffusion process. However, DDGAN encountered scalability limitations when applied to large datasets. To address these limitations, we introduce a novel approach that tackles the problem by matching implicit and explicit factors. More specifically, our approach involves utilizing an implicit model to match the marginal distributions of noisy data and the explicit conditional distribution of the forward diffusion. This combination allows us to effectively match the joint denoising distributions. Unlike DDPM but similar to DDGAN, we do not enforce a parametric distribution for the reverse step, enabling us to take large steps during inference. Similar to the DDPM but unlike DDGAN, we take advantage of the exact form of the diffusion process. We demonstrate that our proposed method obtains comparable generative performance to diffusion-based models and vastly superior results to models with a small number of sampling steps",
    "keywords": [],
    "checked": true,
    "id": "5aa5f76a0b3a7c41abbe9ed98ae667c18ff445cd",
    "semantic_title": "semi-implicit denoising diffusion models (siddms)",
    "citation_count": 2,
    "authors": [
      "yanwu xu",
      "Mingming Gong",
      "Shaoan Xie",
      "Wei Wei",
      "Matthias Grundmann",
      "Kayhan Batmanghelich",
      "Tingbo Hou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/389a55c90f839d58188060a42bb9138a-Abstract-Conference.html": {
    "title": "Implicit Convolutional Kernels for Steerable CNNs",
    "volume": "main",
    "abstract": "Steerable convolutional neural networks (CNNs) provide a general framework for building neural networks equivariant to translations and transformations of an origin-preserving group $G$, such as reflections and rotations. They rely on standard convolutions with $G$-steerable kernels obtained by analytically solving the group-specific equivariance constraint imposed onto the kernel space. As the solution is tailored to a particular group $G$, implementing a kernel basis does not generalize to other symmetry transformations, complicating the development of general group equivariant models. We propose using implicit neural representation via multi-layer perceptrons (MLPs) to parameterize $G$-steerable kernels. The resulting framework offers a simple and flexible way to implement Steerable CNNs and generalizes to any group $G$ for which a $G$-equivariant MLP can be built. We prove the effectiveness of our method on multiple tasks, including N-body simulations, point cloud classification and molecular property prediction",
    "keywords": [],
    "checked": true,
    "id": "ae9d5ae9f7ab573499f9864d9dec267e46a1d1a3",
    "semantic_title": "implicit convolutional kernels for steerable cnns",
    "citation_count": 0,
    "authors": [
      "Maksim Zhdanov",
      "Nico Hoffmann",
      "Gabriele Cesa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/389cfad711d2b1e2128e931feee80230-Abstract-Conference.html": {
    "title": "Block Low-Rank Preconditioner with Shared Basis for Stochastic Optimization",
    "volume": "main",
    "abstract": "Adaptive methods with non-diagonal preconditioning have shown state-of-the-art results on various tasks. However, their computational complexity and memory requirement makes it challenging to scale these methods to modern neural network architectures. To address this challenge, some previous works have adopted block-diagonal preconditioners. However, the memory cost of storing the block-diagonal matrix remains substantial, leading to the use of smaller block sizes and ultimately resulting in suboptimal performance. To reduce the time and memory complexity without sacrificing performance, we propose approximating each diagonal block of the second moment matrix by low-rank matrices and enforcing the same basis for the blocks within each layer. We provide theoretical justification for such sharing and design an algorithm to efficiently maintain this shared-basis block low-rank approximation during training. Our results on a deep autoencoder and a transformer benchmark demonstrate that the proposed method outperforms first-order methods with slightly more time and memory usage, while also achieving competitive or superior performance compared to other second-order methods with less time and memory usage",
    "keywords": [],
    "checked": true,
    "id": "17356dff2519a66840eac1fe54c05d12d0e07b3b",
    "semantic_title": "block low-rank preconditioner with shared basis for stochastic optimization",
    "citation_count": 0,
    "authors": [
      "Jui-Nan Yen",
      "Sai Surya Duvvuri",
      "Inderjit Dhillon",
      "Cho-Jui Hsieh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/38a1671ab0747b6ffe4d1c6ef117a3a9-Abstract-Conference.html": {
    "title": "Learning in the Presence of Low-dimensional Structure: A Spiked Random Matrix Perspective",
    "volume": "main",
    "abstract": "We consider the learning of a single-index target function $f_*: \\mathbb{R}^d\\to\\mathbb{R}$ under spiked covariance data: $$f_*(\\boldsymbol{x}) = \\textstyle\\sigma_*(\\frac{1}{\\sqrt{1+\\theta}}\\langle\\boldsymbol{x},\\boldsymbol{\\mu}\\rangle), ~~ \\boldsymbol{x}\\overset{\\small\\mathrm{i.i.d.}}{\\sim}\\mathcal{N}(0,\\boldsymbol{I_d} + \\theta\\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top), ~~ \\theta\\asymp d^{\\beta} \\text{ for } \\beta\\in[0,1), $$ where the link function $\\sigma_*:\\mathbb{R}\\to\\mathbb{R}$ is a degree-$p$ polynomial with information exponent $k$ (defined as the lowest degree in the Hermite expansion of $\\sigma_*$), and it depends on the projection of input $\\boldsymbol{x}$ onto the spike (signal) direction $\\boldsymbol{\\mu}\\in\\mathbb{R}^d$. In the proportional asymptotic limit where the number of training examples $n$ and the dimensionality $d$ jointly diverge: $n,d\\to\\infty, n/d\\to\\psi\\in(0,\\infty)$, we ask the following question: how large should the spike magnitude $\\theta$ (i.e., the strength of the low-dimensional component) be, in order for $(i)$ kernel methods, $(ii)$ neural networks optimized by gradient descent, to learn $f_*$? We show that for kernel ridge regression, $\\beta\\ge 1-\\frac{1}{p}$ is both sufficient and necessary. Whereas for two-layer neural networks trained with gradient descent, $\\beta>1-\\frac{1}{k}$ suffices. Our results demonstrate that both kernel methods and neural networks benefit from low-dimensional structures in the data. Further, since $k\\le p$ by definition, neural networks can adapt to such structures more effectively",
    "keywords": [],
    "checked": true,
    "id": "a0e7e3029f2b852c549b4a8bdd73b58231e2d798",
    "semantic_title": "learning in the presence of low-dimensional structure: a spiked random matrix perspective",
    "citation_count": 5,
    "authors": [
      "Jimmy Ba",
      "Murat A. Erdogdu",
      "Taiji Suzuki",
      "Zhichao Wang",
      "Denny Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/38b23e2328096520e9c889ae03e372c9-Abstract-Conference.html": {
    "title": "Efficient Neural Music Generation",
    "volume": "main",
    "abstract": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge.In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7\\% to 99.6\\% forward passes in MusicLM, respectively, for sampling 10s to 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.Our samples are available at https://Efficient-MeLoDy.github.io/",
    "keywords": [],
    "checked": true,
    "id": "38b93d01e08ab19ada89a40051a1ed4309fbe834",
    "semantic_title": "efficient neural music generation",
    "citation_count": 12,
    "authors": [
      "Max W. Y. Lam",
      "Qiao Tian",
      "Tang Li",
      "Zongyu Yin",
      "Siyuan Feng",
      "Ming Tu",
      "Yuliang Ji",
      "Rui Xia",
      "Mingbo Ma",
      "Xuchen Song",
      "Jitong Chen",
      "Wang Yuping",
      "Yuxuan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/38b787fc530d0b31825827e2cc306656-Abstract-Conference.html": {
    "title": "Crystal Structure Prediction by Joint Equivariant Diffusion",
    "volume": "main",
    "abstract": "Crystal Structure Prediction (CSP) is crucial in various scientific disciplines. While CSP can be addressed by employing currently-prevailing generative models (e.g. diffusion models), this task encounters unique challenges owing to the symmetric geometry of crystal structures---the invariance of translation, rotation, and periodicity. To incorporate the above symmetries, this paper proposes DiffCSP, a novel diffusion model to learn the structure distribution from stable crystals. To be specific, DiffCSP jointly generates the lattice and atom coordinates for each crystal by employing a periodic-E(3)-equivariant denoising model, to better model the crystal geometry. Notably, different from related equivariant generative approaches, DiffCSP leverages fractional coordinates other than Cartesian coordinates to represent crystals, remarkably promoting the diffusion and the generation process of atom positions. Extensive experiments verify that our DiffCSP remarkably outperforms existing CSP methods, with a much lower computation cost in contrast to DFT-based methods. Moreover, the superiority of DiffCSP is still observed when it is extended for ab initio crystal generation",
    "keywords": [],
    "checked": true,
    "id": "d6f5cef6f1eb24b9e851b2cd1f30b0f9b5664179",
    "semantic_title": "crystal structure prediction by joint equivariant diffusion",
    "citation_count": 2,
    "authors": [
      "Rui Jiao",
      "Wenbing Huang",
      "Peijia Lin",
      "Jiaqi Han",
      "Pin Chen",
      "Yutong Lu",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/38c05a5410a6ab7eeeb26c9dbebbc41b-Abstract-Conference.html": {
    "title": "Understanding the detrimental class-level effects of data augmentation",
    "volume": "main",
    "abstract": "Data augmentation (DA) encodes invariance and provides implicit regularization critical to a model's performance in image classification tasks. However, while DA improves average accuracy, recent studies have shown that its impact can be highly class dependent: achieving optimal average accuracy comes at the cost of significantly hurting individual class accuracy by as much as 20% on ImageNet. There has been little progress in resolving class-level accuracy drops due to a limited understanding of these effects. In this work, we present a framework for understanding how DA interacts with class-level learning dynamics. Using higher-quality multi-label annotations on ImageNet, we systematically categorize the affected classes and find that the majority are inherently ambiguous, co-occur, or involve fine-grained distinctions, while DA controls the model's bias towards one of the closely related classes. While many of the previously reported performance drops are explained by multi-label annotations, we identify other sources of accuracy degradations by analyzing class confusions. We show that simple class-conditional augmentation strategies informed by our framework improve performance on the negatively affected classes",
    "keywords": [],
    "checked": true,
    "id": "3b99d63b05aee56ec632364d83e2c8630cb3f96a",
    "semantic_title": "understanding the detrimental class-level effects of data augmentation",
    "citation_count": 0,
    "authors": [
      "Polina Kirichenko",
      "Mark Ibrahim",
      "Randall Balestriero",
      "Diane Bouchacourt",
      "Shanmukha Ramakrishna Vedantam",
      "Hamed Firooz",
      "Andrew G. Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/38c5feed4b72c96f6cf925ccc9832ecf-Abstract-Conference.html": {
    "title": "Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization",
    "volume": "main",
    "abstract": "Algorithmic reproducibility measures the deviation in outputs of machine learning algorithms upon minor changes in the training process. Previous work suggests that first-order methods would need to trade-off convergence rate (gradient complexity) for better reproducibility. In this work, we challenge this perception and demonstrate that both optimal reproducibility and near-optimal convergence guarantees can be achieved for smooth convex minimization and smooth convex-concave minimax problems under various error-prone oracle settings. Particularly, given the inexact initialization oracle, our regularization-based algorithms achieve the best of both worlds -- optimal reproducibility and near-optimal gradient complexity -- for minimization and minimax optimization. With the inexact gradient oracle, the near-optimal guarantees also hold for minimax optimization. Additionally, with the stochastic gradient oracle, we show that stochastic gradient descent ascent is optimal in terms of both reproducibility and gradient complexity. We believe our results contribute to an enhanced understanding of the reproducibility-convergence trade-off in the context of convex optimization",
    "keywords": [],
    "checked": true,
    "id": "5d141a9f0d3b1bb01c08ec3ef0aa0963dd473a8a",
    "semantic_title": "optimal guarantees for algorithmic reproducibility and gradient complexity in convex optimization",
    "citation_count": 0,
    "authors": [
      "Liang Zhang",
      "Junchi YANG",
      "Amin Karbasi",
      "Niao He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/38e511a690709603d4cc3a1c52b4a9fd-Abstract-Conference.html": {
    "title": "Test-time Adaptation of Discriminative Models via Diffusion Generative Feedback",
    "volume": "main",
    "abstract": "The advancements in generative modeling, particularly the advent of diffusion models, have sparked a fundamental question: how can these models be effectively used for discriminative tasks? In this work, we find that generative models can be great test-time adapters for discriminative models. Our method, Diffusion-TTA, adapts pre-trained discriminative models such as image classifiers, segmenters and depth predictors, to each unlabelled example in the test set using generative feedback from a diffusion model. We achieve this by modulating the conditioning of the diffusion model using the output of the discriminative model. We then maximize the image likelihood objective by backpropagating the gradients to discriminative model's parameters. We show Diffusion-TTA significantly enhances the accuracy of various large-scale pre-trained discriminative models, such as, ImageNet classifiers, CLIP models, image pixel labellers and image depth predictors. Diffusion-TTA outperforms existing test-time adaptation methods, including TTT-MAE and TENT, and particularly shines in online adaptation setups, where the discriminative model is continually adapted to each example in the test set. We provide access to code, results, and visualizations on our website: https://diffusion-tta.github.io/",
    "keywords": [],
    "checked": false,
    "id": "1e0abe0e8c9471de5df11c24d5ded5c94911b226",
    "semantic_title": "diffusion-tta: test-time adaptation of discriminative models via generative feedback",
    "citation_count": 0,
    "authors": [
      "Mihir Prabhudesai",
      "Tsung-Wei Ke",
      "Alex Li",
      "Deepak Pathak",
      "Katerina Fragkiadaki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/38ec60a949c3538e5cbb337b1b386dcf-Abstract-Conference.html": {
    "title": "Fragment-based Pretraining and Finetuning on Molecular Graphs",
    "volume": "main",
    "abstract": "Property prediction on molecular graphs is an important application of Graph Neural Networks (GNNs). Recently, unlabeled molecular data has become abundant, which facilitates the rapid development of self-supervised learning for GNNs in the chemical domain. In this work, we propose pretraining GNNs at the fragment level, a promising middle ground to overcome the limitations of node-level and graph-level pretraining. Borrowing techniques from recent work on principal subgraph mining, we obtain a compact vocabulary of prevalent fragments from a large pretraining dataset. From the extracted vocabulary, we introduce several fragment-based contrastive and predictive pretraining tasks. The contrastive learning task jointly pretrains two different GNNs: one on molecular graphs and the other on fragment graphs, which represents higher-order connectivity within molecules. By enforcing consistency between the fragment embedding and the aggregated embedding of the corresponding atoms from the molecular graphs, we ensure that the embeddings capture structural information at multiple resolutions. The structural information of fragment graphs is further exploited to extract auxiliary labels for graph-level predictive pretraining. We employ both the pretrained molecular-based and fragment-based GNNs for downstream prediction, thus utilizing the fragment information during finetuning. Our graph fragment-based pretraining (GraphFP) advances the performances on 5 out of 8 common molecular benchmarks and improves the performances on long-range biological benchmarks by at least 11.5%. Code is available at: https://github.com/lvkd84/GraphFP",
    "keywords": [],
    "checked": true,
    "id": "dc45bc281e3ec50346924ed410bb46370c3ac6ee",
    "semantic_title": "fragment-based pretraining and finetuning on molecular graphs",
    "citation_count": 0,
    "authors": [
      "Kha-Dinh Luong",
      "Ambuj K Singh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/38fd51cf36f28566230a93a5fbeaabbf-Abstract-Conference.html": {
    "title": "Characterizing Out-of-Distribution Error via Optimal Transport",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) data poses serious challenges in deployed machine learning models,so methods of predicting a model's performance on OOD data without labels are important for machine learning safety.While a number of methods have been proposed by prior work,they often underestimate the actual error, sometimes by a large margin, which greatly impacts their applicability to real tasks.In this work, we identify pseudo-label shift, or the difference between the predicted and true OOD label distributions, as a key indicator to this underestimation. Based on this observation, we introduce a novel method for estimating model performance by leveraging optimal transport theory, Confidence Optimal Transport (COT), and show that it provably provides more robust error estimates in the presence of pseudo-label shift. Additionally, we introduce an empirically-motivated variant of COT, Confidence Optimal Transport with Thresholding (COTT), which applies thresholding to the individual transport costs and further improves the accuracy of COT's error estimates. We evaluate COT and COTT on a variety of standard benchmarks that induce various types of distribution shift -- synthetic, novel subpopulation, and natural -- and show that our approaches significantly outperform existing state-of-the-art methods with up to 3x lower prediction errorS",
    "keywords": [],
    "checked": true,
    "id": "80e5f8994731103b4ba87bc37676dfd88fb3c2ce",
    "semantic_title": "characterizing out-of-distribution error via optimal transport",
    "citation_count": 1,
    "authors": [
      "Yuzhe Lu",
      "Yilong Qin",
      "Runtian Zhai",
      "Andrew Shen",
      "Ketong Chen",
      "Zhenlin Wang",
      "Soheil Kolouri",
      "Simon Stepputtis",
      "Joseph Campbell",
      "Katia  Sycara"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/39235c56aef13fb05a6adc95eb9d8d66-Abstract-Conference.html": {
    "title": "Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective",
    "volume": "main",
    "abstract": "Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to handle the spatial and temporal domain divergence separately through disentanglement. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static information and another encoding the dynamic information. A Transfer Sequential VAE (TranSVAE) framework is then developed to model such generation. To better serve for adaptation, we propose several objectives to constrain the latent factors. With these constraints, the spatial divergence can be readily removed by disentangling the static domain-specific information out, and the temporal divergence is further reduced from both frame- and video-levels through adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE compared with several state-of-the-art approaches",
    "keywords": [],
    "checked": true,
    "id": "3712442ffa6db4454fc3357902c3bfce2c7a2a3e",
    "semantic_title": "unsupervised video domain adaptation for action recognition: a disentanglement perspective",
    "citation_count": 3,
    "authors": [
      "Pengfei Wei",
      "Lingdong Kong",
      "Xinghua Qu",
      "Yi Ren",
      "Zhiqiang Xu",
      "Jing Jiang",
      "Xiang Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3927bbdcf0e8d1fa8aa23c26f358a281-Abstract-Conference.html": {
    "title": "Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models",
    "volume": "main",
    "abstract": "Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing problem, including erasing and amplifying facts. For one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior",
    "keywords": [],
    "checked": true,
    "id": "9c0a434b240299cec0029a1be93ab263d7ec9963",
    "semantic_title": "does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models",
    "citation_count": 53,
    "authors": [
      "Peter Hase",
      "Mohit Bansal",
      "Been Kim",
      "Asma Ghandeharioun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/395371f778ebd4854b88521100af30ad-Abstract-Conference.html": {
    "title": "The Geometry of Neural Nets' Parameter Spaces Under Reparametrization",
    "volume": "main",
    "abstract": "Model reparametrization, which follows the change-of-variable rule of calculus, is a popular way to improve the training of neural nets. But it can also be problematic since it can induce inconsistencies in, e.g., Hessian-based flatness measures, optimization trajectories, and modes of probability densities. This complicates downstream analyses: e.g. one cannot definitively relate flatness with generalization since arbitrary reparametrization changes their relationship. In this work, we study the invariance of neural nets under reparametrization from the perspective of Riemannian geometry. From this point of view, invariance is an inherent property of any neural net if one explicitly represents the metric and uses the correct associated transformation rules. This is important since although the metric is always present, it is often implicitly assumed as identity, and thus dropped from the notation, then lost under reparametrization. We discuss implications for measuring the flatness of minima, optimization, and for probability-density maximization. Finally, we explore some interesting directions where invariance is useful",
    "keywords": [],
    "checked": true,
    "id": "77e999252bd2bbb6f59df32d317dec02f92c6302",
    "semantic_title": "the geometry of neural nets' parameter spaces under reparametrization",
    "citation_count": 3,
    "authors": [
      "Agustinus Kristiadi",
      "Felix Dangel",
      "Philipp Hennig"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/396ea38391e8b96a3add6126006f1a53-Abstract-Conference.html": {
    "title": "Multi-Objective Intrinsic Reward Learning for Conversational Recommender Systems",
    "volume": "main",
    "abstract": "Conversational Recommender Systems (CRS) actively elicit user preferences to generate adaptive recommendations. Mainstream reinforcement learning-based CRS solutions heavily rely on handcrafted reward functions, which may not be aligned with user intent in CRS tasks. Therefore, the design of task-specific rewards is critical to facilitate CRS policy learning, which remains largely under-explored in the literature. In this work, we propose a novel approach to address this challenge by learning intrinsic rewards from interactions with users. Specifically, we formulate intrinsic reward learning as a multi-objective bi-level optimization problem. The inner level optimizes the CRS policy augmented by the learned intrinsic rewards, while the outer level drives the intrinsic rewards to optimize two CRS-specific objectives: maximizing the success rate and minimizing the number of turns to reach a successful recommendation}in conversations. To evaluate the effectiveness of our approach, we conduct extensive experiments on three public CRS benchmarks. The results show that our algorithm significantly improves CRS performance by exploiting informative learned intrinsic rewards",
    "keywords": [],
    "checked": true,
    "id": "f7ea50dea238913894dde804d696f22b13c7cbdf",
    "semantic_title": "multi-objective intrinsic reward learning for conversational recommender systems",
    "citation_count": 0,
    "authors": [
      "Zhendong Chu",
      "Nan Wang",
      "Hongning Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/397271e11322fae8ba7f827c50ca8d9b-Abstract-Conference.html": {
    "title": "Predict-then-Calibrate: A New Perspective of Robust Contextual LP",
    "volume": "main",
    "abstract": "Contextual optimization, also known as predict-then-optimize or prescriptive analytics, considers an optimization problem with the presence of covariates (context or side information). The goal is to learn a prediction model (from the training data) that predicts the objective function from the covariates, and then in the test phase, solve the optimization problem with the covariates but without the observation of the objective function. In this paper, we consider a risk-sensitive version of the problem and propose a generic algorithm design paradigm called predict-then-calibrate. The idea is to first develop a prediction model without concern for the downstream risk profile or robustness guarantee, and then utilize calibration (or recalibration) methods to quantify the uncertainty of the prediction. While the existing methods suffer from either a restricted choice of the prediction model or strong assumptions on the underlying data, we show the disentangling of the prediction model and the calibration/uncertainty quantification has several advantages. First, it imposes no restriction on the prediction model and thus fully unleashes the potential of off-the-shelf machine learning methods. Second, the derivation of the risk and robustness guarantee can be made independent of the choice of the prediction model through a data-splitting idea. Third, our paradigm of predict-then-calibrate applies to both (risk-sensitive) robust and (risk-neutral) distributionally robust optimization (DRO) formulations. Theoretically, it gives new generalization bounds for the contextual LP problem and sheds light on the existing results of DRO for contextual LP. Numerical experiments further reinforce the advantage of the predict-then-calibrate paradigm in that an improvement on either the prediction model or the calibration model will lead to a better final performance",
    "keywords": [],
    "checked": true,
    "id": "037393db4805f47f3bf3c8d9c352d837bf4e7c7e",
    "semantic_title": "predict-then-calibrate: a new perspective of robust contextual lp",
    "citation_count": 3,
    "authors": [
      "Chunlin Sun",
      "Linyu Liu",
      "Xiaocheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/398ae57ed4fda79d0781c65c926d667b-Abstract-Conference.html": {
    "title": "What Makes Good Examples for Visual In-Context Learning?",
    "volume": "main",
    "abstract": "Large vision models with billions of parameters and trained on broad data have great potential in numerous downstream applications. However, these models are typically difficult to adapt due to their large parameter size and sometimes lack of accesss to their weights---entities able to develop large vision models often provide APIs only. In this paper, we study how to better utilize large vision models through the lens of in-context learning, a concept that has been well-known in natural language processing but has only been studied very recently in computer vision. In-context learning refers to the ability to perform inference on tasks never seen during training by simply conditioning on in-context examples (i.e., input-output pairs) without updating any internal model parameters. To demystify in-context learning in computer vision, we conduct an extensive research and identify a critical problem: downstream performance is highly sensitivie to the choice of visual in-context examples. To address this problem, we propose a prompt retrieval framework specifically for large vision models, allowing the selection of in-context examples to be fully automated. Concretely, we provide two implementations: (i) an unsupervised prompt retrieval method based on nearest example search using an off-the-shelf model, and (ii) a supervised prompt retrieval method, which trains a neural network to choose examples that directly maximize in-context learning performance. Both methods do not require access to the internal weights of large vision models. Our results demonstrate that our methods can bring non-trivial improvements to visual in-context learning in comparison to the commonly-used random selection. Code and models will be released",
    "keywords": [],
    "checked": true,
    "id": "5f61ddc37476acf3741b0bfe5fcb59639cadbb86",
    "semantic_title": "what makes good examples for visual in-context learning?",
    "citation_count": 27,
    "authors": [
      "Yuanhan Zhang",
      "Kaiyang Zhou",
      "Ziwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/398b00a05b847ac65eb98c8e5e865fe8-Abstract-Conference.html": {
    "title": "Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing",
    "volume": "main",
    "abstract": "Continual table semantic parsing aims to train a parser on a sequence of tasks, where each task requires the parser to translate natural language into SQL based on task-specific tables but only offers limited training examples. Conventional methods tend to suffer from overfitting with limited supervision, as well as catastrophic forgetting due to parameter updates.Despite recent advancements that partially alleviate these issues through semi-supervised data augmentation and retention of a few past examples, the performance is still limited by the volume of unsupervised data and stored examples.To overcome these challenges, this paper introduces a novel method integrating parameter-efficient fine-tuning (PEFT) and in-context tuning (ICT) for training a continual table semantic parser. Initially, we present a task-adaptive PEFT framework capable of fully circumventing catastrophic forgetting, which is achieved by freezing the pre-trained model backbone and fine-tuning small-scale prompts. Building on this, we propose a teacher-student framework-based solution. The teacher addresses the few-shot problem using ICT, which procures contextual information by demonstrating a few training examples. In turn, the student leverages the proposed PEFT framework to learn from the teacher's output distribution, and subsequently compresses and saves the contextual information to the prompts, eliminating the need to store any training examples.Experimental evaluations on two benchmarks affirm the superiority of our method over prevalent few-shot and continual learning baselines across various metrics",
    "keywords": [],
    "checked": true,
    "id": "fd183daf4870ec6abd5a5d997c0f9b88d1483f84",
    "semantic_title": "parameterizing context: unleashing the power of parameter-efficient fine-tuning and in-context tuning for continual table semantic parsing",
    "citation_count": 0,
    "authors": [
      "Yongrui Chen",
      "Shenyu Zhang",
      "Guilin Qi",
      "Xinnan Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/39b77b5e422b4e070e2811b73ea9bcf7-Abstract-Conference.html": {
    "title": "Incentives in Federated Learning: Equilibria, Dynamics, and Mechanisms for Welfare Maximization",
    "volume": "main",
    "abstract": "Federated learning (FL) has emerged as a powerful scheme to facilitate the collaborative learning of models amongst a set of agents holding their own private data. Although the agents benefit from the global model trained on shared data, by participating in federated learning, they may also incur costs (related to privacy and communication) due to data sharing. In this paper, we model a collaborative FL framework, where every agent attempts to achieve an optimal trade-off between her learning payoff and data sharing cost. We show the existence of Nash equilibrium (NE) under mild assumptions on agents' payoff and costs. Furthermore, we show that agents can discover the NE via best response dynamics. However, some of the NE may be bad in terms of overall welfare for the agents, implying little incentive for some fraction of the agents to participate in the learning. To remedy this, we design a budget-balanced mechanism involving payments to the agents, that ensures that any $p$-mean welfare function of the agents' utilities is maximized at NE. In addition, we introduce a FL protocol FedBR-BG that incorporates our budget-balanced mechanism, utilizing best response dynamics. Our empirical validation on MNIST and CIFAR-10 substantiates our theoretical analysis. We show that FedBR-BG outperforms the basic best-response-based protocol without additional incentivization, the standard federated learning protocol FedAvg, as well as a recent baseline MWFed in terms of achieving superior $p$-mean welfare",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniket Murhekar",
      "Zhuowen Yuan",
      "Bhaskar Ray Chaudhury",
      "Bo Li",
      "Ruta Mehta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/39bc6e3cbf5a1991d33dc10ebff9a9cf-Abstract-Conference.html": {
    "title": "MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates",
    "volume": "main",
    "abstract": "This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 updates, called MKOR, that improves the training time and convergence properties of deep neural networks (DNNs). Second-order techniques, while enjoying higher convergence rates vs first-order counterparts, have cubic complexity with respect to either the model size and/or the training batch size. Hence they exhibit poor scalability and performance in transformer models, e.g. large language models (LLMs), because the batch sizes in these models scale by the attention mechanism sequence length, leading to large model size and batch sizes. MKOR's complexity is quadratic with respect to the model size, alleviating the computation bottlenecks in second-order methods. Because of their high computation complexity, state-of-the-art implementations of second-order methods can only afford to update the second order information infrequently, and thus do not fully exploit the promise of better convergence from these updates. By reducing the communication complexity of the second-order updates as well as achieving a linear communication complexity, MKOR increases the frequency of second order updates. We also propose a hybrid version of MKOR (called MKOR-H) that mid-training falls backs to a first order optimizer if the second order updates no longer accelerate convergence. Our experiments show that MKOR outperforms state -of-the-art first order methods, e.g. the LAMB optimizer, and best implementations of second-order methods, i.e. KAISA/KFAC, up to 2.57x and 1.85x respectively on BERT-Large-Uncased on 64 GPUs",
    "keywords": [],
    "checked": true,
    "id": "2b1c86705e329e80cac96b6bf5df353d079ff10b",
    "semantic_title": "mkor: momentum-enabled kronecker-factor-based optimizer using rank-1 updates",
    "citation_count": 1,
    "authors": [
      "Mohammad Mozaffari",
      "Sikan Li",
      "Zhao Zhang",
      "Maryam Mehri Dehnavi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/39ca8893ea38905a9d2ffe786e85af0f-Abstract-Conference.html": {
    "title": "DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a privacy-constrained decentralized machine learning paradigm in which clients enable collaborative training without compromising private data. However, how to learn a robust global model in the data-heterogeneous and model-heterogeneous FL scenarios is challenging. To address it, we resort to data-free knowledge distillation to propose a new FL method (namely DFRD).DFRD equips a conditional generator on the server to approximate the training space of the local models uploaded by clients, and systematically investigates its training in terms of fidelity, transferability and diversity. To overcome the catastrophic forgetting of the global model caused by the distribution shifts of the generator across communication rounds, we maintain an exponential moving average copy of the generator on the server. Additionally, we propose dynamic weighting and label sampling to accurately extract knowledge from local models. Finally, our extensive experiments on various image classification tasks illustrate that DFRD achieves significant performance gains compared to SOTA baselines",
    "keywords": [],
    "checked": true,
    "id": "6104bdd786836ae05a575eac50694160e6c388a2",
    "semantic_title": "dfrd: data-free robustness distillation for heterogeneous federated learning",
    "citation_count": 1,
    "authors": [
      "kangyang Luo",
      "Shuai Wang",
      "Yexuan Fu",
      "Xiang Li",
      "Yunshi Lan",
      "Ming Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/39e9c5913c970e3e49c2df629daff636-Abstract-Conference.html": {
    "title": "Rank-N-Contrast: Learning Continuous Representations for Regression",
    "volume": "main",
    "abstract": "Deep regression models typically learn in an end-to-end fashion without explicitly emphasizing a regression-aware representation. Consequently, the learned representations exhibit fragmentation and fail to capture the continuous nature of sample orders, inducing suboptimal results across a wide range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a framework that learns continuous representations for regression by contrasting samples against each other based on their rankings in the target space. We demonstrate, theoretically and empirically, that RNC guarantees the desired order of learned representations in accordance with the target orders, enjoying not only better performance but also significantly improved robustness, efficiency, and generalization. Extensive experiments using five real-world regression datasets that span computer vision, human-computer interaction, and healthcare verify that RNC achieves state-of-the-art performance, highlighting its intriguing properties including better data efficiency, robustness to spurious targets and data corruptions, and generalization to distribution shifts",
    "keywords": [],
    "checked": true,
    "id": "94785337539d5a7bc4a8b74c2b651e51482fce92",
    "semantic_title": "rank-n-contrast: learning continuous representations for regression",
    "citation_count": 4,
    "authors": [
      "Kaiwen Zha",
      "Peng Cao",
      "Jeany Son",
      "Yuzhe Yang",
      "Dina Katabi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a02b6df276223b68c69ca572cb3c4a8-Abstract-Conference.html": {
    "title": "Global Optimality in Bivariate Gradient-based DAG Learning",
    "volume": "main",
    "abstract": "Recently, a new class of non-convex optimization problems motivated by the statistical problem of learning an acyclic directed graphical model from data has attracted significant interest. While existing work uses standard first-order optimization schemes to solve this problem, proving the global optimality of such approaches has proven elusive. The difficulty lies in the fact that unlike other non-convex problems in the literature, this problem is not \"benign\", and possesses multiple spurious solutions that standard approaches can easily get trapped in. In this paper, we prove that a simple path-following optimization scheme globally converges to the global minimum of the population loss in the bivariate setting",
    "keywords": [],
    "checked": true,
    "id": "d2da7b7540ec1c97aa5b58453d6b3468b8ef1fc2",
    "semantic_title": "global optimality in bivariate gradient-based dag learning",
    "citation_count": 0,
    "authors": [
      "Chang Deng",
      "Kevin Bello",
      "Pradeep Ravikumar",
      "Bryon Aragam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a07c3a67cfe50d3236b71fb674c7f30-Abstract-Conference.html": {
    "title": "Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning",
    "volume": "main",
    "abstract": "We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query (PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties",
    "keywords": [],
    "checked": true,
    "id": "a7cc93456eba6437be5419161f37ea847ec8aa04",
    "semantic_title": "perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning",
    "citation_count": 0,
    "authors": [
      "Austin Xu",
      "Andrew McRae",
      "Jingyan Wang",
      "Mark Davenport",
      "Ashwin Pananjady"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a0e2de215bd17c39ad08ba1d16c1b12-Abstract-Conference.html": {
    "title": "Joint processing of linguistic properties in brains and language models",
    "volume": "main",
    "abstract": "Language models have been shown to be very effective in predicting brain recordings of subjects experiencing complex language stimuli. For a deeper understanding of this alignment, it is important to understand the correspondence between the detailed processing of linguistic information by the human brain versus language models. We investigate this correspondence via a direct approach, in which we eliminate information related to specific linguistic properties in the language model representations and observe how this intervention affects the alignment with fMRI brain recordings obtained while participants listened to a story. We investigate a range of linguistic properties (surface, syntactic, and semantic) and find that the elimination of each one results in a significant decrease in brain alignment. Specifically, we find that syntactic properties (i.e. Top Constituents and Tree Depth) have the largest effect on the trend of brain alignment across model layers. These findings provide clear evidence for the role of specific linguistic information in the alignment between brain and language models, and open new avenues for mapping the joint information processing in both systems. We make the code publicly available [https://github.com/subbareddy248/linguistic-properties-brain-alignment]",
    "keywords": [],
    "checked": true,
    "id": "a103da21d92a715cdb0e7f42eb90d505e2a81986",
    "semantic_title": "joint processing of linguistic properties in brains and language models",
    "citation_count": 12,
    "authors": [
      "SUBBAREDDY OOTA",
      "Manish Gupta",
      "Mariya Toneva"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a13be0c5dae69e0f08065f113fb10b8-Abstract-Conference.html": {
    "title": "$S^3$: Increasing GPU Utilization during Generative Inference for Higher Throughput",
    "volume": "main",
    "abstract": "Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself. This problem is exacerbated in one of the current LLM serving frameworks which reserves the maximum sequence length of memory for the KV cache to guarantee generating a complete sequence as they do not know the output sequence length. This restricts us to use a smaller batch size leading to lower GPU utilization and above all, lower throughput. We argue that designing a system with a priori knowledge of the output sequence can mitigate this problem. To this end, we propose $S^3$, which predicts the output sequence length, schedules generation queries based on the prediction to increase device resource utilization and throughput, and handle mispredictions. Our proposed method achieves 6.49× throughput over those systems that assume the worst case for the output sequence length",
    "keywords": [],
    "checked": false,
    "id": "0423fc7bc1880b850d07aec8ebd9217a70626572",
    "semantic_title": "s3: increasing gpu utilization during generative inference for higher throughput",
    "citation_count": 5,
    "authors": [
      "Yunho Jin",
      "Chun-Feng Wu",
      "David Brooks",
      "Gu-Yeon Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a14ae9951e8153a8fc814b5f506b5b7-Abstract-Conference.html": {
    "title": "Disentangling Cognitive Diagnosis with Limited Exercise Labels",
    "volume": "main",
    "abstract": "Cognitive diagnosis is an important task in intelligence education, which aims at measuring students' proficiency in specific knowledge concepts. Given a fully labeled exercise-concept matrix, most existing models focused on mining students' response records for cognitive diagnosis. Despite their success, due to the huge cost of labeling exercises, a more practical scenario is that limited exercises are labeled with concepts. Performing cognitive diagnosis with limited exercise labels is under-explored and remains pretty much open. In this paper, we propose Disentanglement based Cognitive Diagnosis (DCD) to address the challenges of limited exercise labels. Specifically, we utilize students' response records to model student proficiency, exercise difficulty and exercise label distribution. Then, we introduce two novel modules - group-based disentanglement and limited-labeled alignment modules - to disentangle the factors relevant to concepts and align them with real limited labels. Particularly, we introduce the tree-like structure of concepts with negligible cost for group-based disentangling, as concepts of different levels exhibit different independence relationships.Extensive experiments on widely used benchmarks demonstrate the superiority of our proposed model",
    "keywords": [],
    "checked": false,
    "id": "dda30e6f24d9fa2f933a04379049736dcc92a3cb",
    "semantic_title": "society for neuroscience 2019 satellite symposium",
    "citation_count": 0,
    "authors": [
      "Xiangzhi Chen",
      "Le Wu",
      "Fei Liu",
      "Lei Chen",
      "Kun Zhang",
      "Richang Hong",
      "Meng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a23caeb904c822575fa56fb114ca499-Abstract-Conference.html": {
    "title": "Energy-Based Sliced Wasserstein Distance",
    "volume": "main",
    "abstract": "The sliced Wasserstein (SW) distance has been widely recognized as a statistically effective and computationally efficient metric between two probability measures. A key component of the SW distance is the slicing distribution. There are two existing approaches for choosing this distribution. The first approach is using a fixed prior distribution. The second approach is optimizing for the best distribution which belongs to a parametric family of distributions and can maximize the expected distance. However, both approaches have their limitations. A fixed prior distribution is non-informative in terms of highlighting projecting directions that can discriminate two general probability measures. Doing optimization for the best distribution is often expensive and unstable. Moreover, designing the parametric family of the candidate distribution could be easily misspecified. To address the issues, we propose to design the slicing distribution as an energy-based distribution that is parameter-free and has the density proportional to an energy function of the projected one-dimensional Wasserstein distance. We then derive a novel sliced Wasserstein variant, energy-based sliced Waserstein (EBSW) distance, and investigate its topological, statistical, and computational properties via importance sampling, sampling importance resampling, and Markov Chain methods. Finally, we conduct experiments on point-cloud gradient flow, color transfer, and point-cloud reconstruction to show the favorable performance of the EBSW",
    "keywords": [],
    "checked": true,
    "id": "316bfdad9f6c44050b9dfc8805747ffa111cd1d9",
    "semantic_title": "energy-based sliced wasserstein distance",
    "citation_count": 6,
    "authors": [
      "Khai Nguyen",
      "Nhat Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a2d1bf9bc0a9794cf82c1341a7a75e6-Abstract-Conference.html": {
    "title": "E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning",
    "volume": "main",
    "abstract": "Event cameras have emerged as a promising vision sensor in recent years due to their unparalleled temporal resolution and dynamic range. While registration of 2D RGB images to 3D point clouds is a long-standing problem in computer vision, no prior work studies 2D-3D registration for event cameras. To this end, we propose E2PNet, the first learning-based method for event-to-point cloud registration.The core of E2PNet is a novel feature representation network called Event-Points-to-Tensor (EP2T), which encodes event data into a 2D grid-shaped feature tensor. This grid-shaped feature enables matured RGB-based frameworks to be easily used for event-to-point cloud registration, without changing hyper-parameters and the training procedure. EP2T treats the event input as spatio-temporal point clouds. Unlike standard 3D learning architectures that treat all dimensions of point clouds equally, the novel sampling and information aggregation modules in EP2T are designed to handle the inhomogeneity of the spatial and temporal dimensions. Experiments on the MVSEC and VECtor datasets demonstrate the superiority of E2PNet over hand-crafted and other learning-based methods. Compared to RGB-based registration, E2PNet is more robust to extreme illumination or fast motion due to the use of event data. Beyond 2D-3D registration, we also show the potential of EP2T for other vision tasks such as flow estimation, event-to-image reconstruction and object recognition. The source code can be found at: https://github.com/Xmu-qcj/E2PNet",
    "keywords": [],
    "checked": true,
    "id": "6183bd6943648ab95be3fb9a6f165200be802df8",
    "semantic_title": "e2pnet: event to point cloud registration with spatio-temporal representation learning",
    "citation_count": 0,
    "authors": [
      "Xiuhong Lin",
      "Changjie Qiu",
      "zhipeng cai",
      "Siqi Shen",
      "Yu Zang",
      "Weiquan Liu",
      "Xuesheng Bian",
      "Matthias Müller",
      "Cheng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a2e5889b4bbef997ddb13b55d5acf77-Abstract-Conference.html": {
    "title": "Pengi: An Audio Language Model for Audio Tasks",
    "volume": "main",
    "abstract": "In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 21 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding",
    "keywords": [],
    "checked": true,
    "id": "ad22af138fa1d1490cda0301abf8159a7c30c5a2",
    "semantic_title": "pengi: an audio language model for audio tasks",
    "citation_count": 33,
    "authors": [
      "Soham Deshmukh",
      "Benjamin Elizalde",
      "Rita Singh",
      "Huaming Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a33ddacb2798fc7d83b8334d552e05a-Abstract-Conference.html": {
    "title": "Unleashing the Power of Graph Data Augmentation on Covariate Distribution Shift",
    "volume": "main",
    "abstract": "The issue of distribution shifts is emerging as a critical concern in graph representation learning. From the perspective of invariant learning and stable learning, a recently well-established paradigm for out-of-distribution generalization, stable features of the graph are assumed to causally determine labels, while environmental features tend to be unstable and can lead to the two primary types of distribution shifts. The correlation shift is often caused by the spurious correlation between environmental features and labels that differs between the training and test data; the covariate shift often stems from the presence of new environmental features in test data. However, most strategies, such as invariant learning or graph augmentation, typically struggle with limited training environments or perturbed stable features, thus exposing limitations in handling the problem of covariate shift. To address this challenge, we propose a simple-yet-effective data augmentation strategy, Adversarial Invariant Augmentation (AIA), to handle the covariate shift on graphs. Specifically, given the training data, AIA aims to extrapolate and generate new environments, while concurrently preserving the original stable features during the augmentation process. Such a design equips the graph classification model with an enhanced capability to identify stable features in new environments, thereby effectively tackling the covariate shift in data. Extensive experiments with in-depth empirical analysis demonstrate the superiority of our approach. The implementation codes are publicly available at https://github.com/yongduosui/AIA",
    "keywords": [],
    "checked": true,
    "id": "a47dd0b41db6a4c2fd29866edcca0f5265b1708e",
    "semantic_title": "unleashing the power of graph data augmentation on covariate distribution shift",
    "citation_count": 7,
    "authors": [
      "Yongduo Sui",
      "Qitian Wu",
      "Jiancan Wu",
      "Qing Cui",
      "Longfei Li",
      "Jun Zhou",
      "Xiang Wang",
      "Xiangnan He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a40e042c66e84659249f3254460c123-Abstract-Conference.html": {
    "title": "Adaptive recurrent vision performs zero-shot computation scaling to unseen difficulty levels",
    "volume": "main",
    "abstract": "Humans solving algorithmic (or) reasoning problems typically exhibit solution times that grow as a function of problem difficulty. Adaptive recurrent neural networks have been shown to exhibit this property for various language-processing tasks. However, little work has been performed to assess whether such adaptive computation can also enable vision models to extrapolate solutions beyond their training distribution's difficulty level, with prior work focusing on very simple tasks. In this study, we investigate a critical functional role of such adaptive processing using recurrent neural networks: to dynamically scale computational resources conditional on input requirements that allow for zero-shot generalization to novel difficulty levels not seen during training using two challenging visual reasoning tasks: PathFinder and Mazes. We combine convolutional recurrent neural networks (ConvRNNs) with a learnable halting mechanism based on Graves (2016). We explore various implementations of such adaptive ConvRNNs (AdRNNs) ranging from tying weights across layers to more sophisticated biologically inspired recurrent networks that possess lateral connections and gating. We show that 1) AdRNNs learn to dynamically halt processing early (or late) to solve easier (or harder) problems, 2) these RNNs zero-shot generalize to more difficult problem settings not shown during training by dynamically increasing the number of recurrent iterations at test time. Our study provides modeling evidence supporting the hypothesis that recurrent processing enables the functional advantage of adaptively allocating compute resources conditional on input requirements and hence allowing generalization to harder difficulty levels of a visual reasoning problem without training",
    "keywords": [],
    "checked": true,
    "id": "ea535502e112590c1d571d68cd4e75093c596440",
    "semantic_title": "adaptive recurrent vision performs zero-shot computation scaling to unseen difficulty levels",
    "citation_count": 0,
    "authors": [
      "Vijay Veerabadran",
      "Srinivas Ravishankar",
      "Yuan Tang",
      "Ritik Raina",
      "Virginia de Sa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a4f287883609241031e6818bd01133e-Abstract-Conference.html": {
    "title": "The Pick-to-Learn Algorithm: Empowering Compression for Tight Generalization Bounds and Improved Post-training Performance",
    "volume": "main",
    "abstract": "Generalization bounds are valuable both for theory and applications. On the one hand, they shed light on the mechanisms that underpin the learning processes; on the other, they certify how well a learned model performs against unseen inputs. In this work we build upon a recent breakthrough in compression theory to develop a new framework yielding tight generalization bounds of wide practical applicability. The core idea is to embed any given learning algorithm into a suitably-constructed meta-algorithm (here called Pick-to-Learn, P2L) in order to instill desirable compression properties. When applied to the MNIST classification dataset and to a synthetic regression problem, P2L not only attains generalization bounds that compare favorably with the state of the art (test-set and PAC-Bayes bounds), but it also learns models with better post-training performance",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dario Paccagnan",
      "Marco Campi",
      "Simone Garatti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a5b75ce6cbd3aaaa32d6e935ffc4cff-Abstract-Conference.html": {
    "title": "Orthogonal Non-negative Tensor Factorization based Multi-view Clustering",
    "volume": "main",
    "abstract": "Multi-view clustering (MVC) based on non-negative matrix factorization (NMF) and its variants have attracted much attention due to their advantages in clustering interpretability. However, existing NMF-based multi-view clustering methods perform NMF on each view respectively and ignore the impact of between-view. Thus, they can't well exploit the within-view spatial structure and between-view complementary information. To resolve this issue, we present orthogonal non-negative tensor factorization (Orth-NTF) and develop a novel multi-view clustering based on Orth-NTF with one-side orthogonal constraint. Our model directly performs Orth-NTF on the 3rd-order tensor which is composed of anchor graphs of views. Thus, our model directly considers the between-view relationship. Moreover, we use the tensor Schatten $p$-norm regularization as a rank approximation of the 3rd-order tensor which characterizes the cluster structure of multi-view data and exploits the between-view complementary information. In addition, we provide an optimization algorithm for the proposed method and prove mathematically that the algorithm always converges to the stationary KKT point. Extensive experiments on various benchmark datasets indicate that our proposed method is able to achieve satisfactory clustering performance",
    "keywords": [],
    "checked": false,
    "id": "d41dada02e52c7937148cdc9646675e9106b1b47",
    "semantic_title": "deep multiple non-negative matrix factorization for multi-view clustering",
    "citation_count": 5,
    "authors": [
      "Jing Li",
      "Quanxue Gao",
      "QIANQIAN WANG",
      "Ming Yang",
      "Wei Xia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a7f9e485845dac27423375c934cb4db-Abstract-Conference.html": {
    "title": "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models",
    "volume": "main",
    "abstract": "Attaining a high degree of user controllability in visual generation often requires intricate, fine-grained inputs like layouts. However, such inputs impose a substantial burden on users when compared to simple text inputs. To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models. We propose LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance visual planning skills of LLMs. We show that LayoutGPT can generate plausible layouts in multiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation. When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40\\% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness. Lastly, LayoutGPT achieves comparable performance to supervised methods in 3D indoor scene synthesis, demonstrating its effectiveness and potential in multiple visual domains",
    "keywords": [],
    "checked": true,
    "id": "66d755730f5d08a6f4fcc5e81f24982ba389dca9",
    "semantic_title": "layoutgpt: compositional visual planning and generation with large language models",
    "citation_count": 27,
    "authors": [
      "Weixi Feng",
      "Wanrong Zhu",
      "Tsu-Jui Fu",
      "Varun Jampani",
      "Arjun Akula",
      "Xuehai He",
      "S Basu",
      "Xin Eric Wang",
      "William Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3abe23bf7e295b44369c24465d68987a-Abstract-Conference.html": {
    "title": "Data Pruning via Moving-one-Sample-out",
    "volume": "main",
    "abstract": "In this paper, we propose a novel data-pruning approach called moving-one-sample-out (MoSo), which aims to identify and remove the least informative samples from the training set. The core insight behind MoSo is to determine the importance of each sample by assessing its impact on the optimal empirical risk. This is achieved by measuring the extent to which the empirical risk changes when a particular sample is excluded from the training set. Instead of using the computationally expensive leaving-one-out-retraining procedure, we propose an efficient first-order approximator that only requires gradient information from different training stages. The key idea behind our approximation is that samples with gradients that are consistently aligned with the average gradient of the training set are more informative and should receive higher scores, which could be intuitively understood as follows: if the gradient from a specific sample is consistent with the average gradient vector, it implies that optimizing the network using the sample will yield a similar effect on all remaining samples. Experimental results demonstrate that MoSo effectively mitigates severe performance degradation at high pruning ratios and achieves satisfactory performance across various settings. Experimental results demonstrate that MoSo effectively mitigates severe performance degradation at high pruning ratios and outperforms state-of-the-art methods by a large margin across various settings",
    "keywords": [],
    "checked": true,
    "id": "ae7f82b255c7a4ce129632ece3670bea6f047f6f",
    "semantic_title": "data pruning via moving-one-sample-out",
    "citation_count": 3,
    "authors": [
      "Haoru Tan",
      "Sitong Wu",
      "Fei Du",
      "Yukang Chen",
      "Zhibin Wang",
      "Fan Wang",
      "Xiaojuan Qi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3acb49252187efa352a1ae0e4b066ced-Abstract-Conference.html": {
    "title": "Alternation makes the adversary weaker in two-player games",
    "volume": "main",
    "abstract": "Motivated by alternating game-play in two-player games, we study an altenating variant of the \\textit{Online Linear Optimization} (OLO). In alternating OLO, a \\textit{learner} at each round $t \\in [n]$ selects a vector $x^t$ and then an \\textit{adversary} selects a cost-vector $c^t \\in [-1,1]^n$. The learner then experiences cost $(c^t + c^{t-1})^\\top x^t$ instead of $(c^t)^\\top x^t$ as in standard OLO. We establish that under this small twist, the $\\Omega(\\sqrt{T})$ lower bound on the regret is no longer valid. More precisely, we present two online learning algorithms for alternating OLO that respectively admit $\\mathcal{O}((\\log n)^{4/3} T^{1/3})$ regret for the $n$-dimensional simplex and $\\mathcal{O}(\\rho \\log T)$ regret for the ball of radius $\\rho>0$. Our results imply that in alternating game-play, an agent can always guarantee $\\mathcal{\\tilde{O}}((\\log n)^{4/3} T^{1/3})$ regardless the strategies of the other agent while the regret bound improves to $\\mathcal{O}(\\log T)$ in case the agent admits only two actions",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Volkan Cevher",
      "Ashok Cutkosky",
      "Ali Kavis",
      "Georgios Piliouras",
      "Stratis Skoulakis",
      "Luca Viano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3acbe9dc3a1e8d48a57b16e9aef91879-Abstract-Conference.html": {
    "title": "Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features",
    "volume": "main",
    "abstract": "To avoid failures on out-of-distribution data, recent works have sought to extract features that have an invariant or stable relationship with the label across domains, discarding \"spurious\" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information that could boost performance if used correctly in the test domain. In this work, we show how this can be done without test-domain labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data",
    "keywords": [],
    "checked": true,
    "id": "ff3c339d6d2a13b4c3e50bd679b89cf5a25757d2",
    "semantic_title": "spuriosity didn't kill the classifier: using invariant predictions to harness spurious features",
    "citation_count": 3,
    "authors": [
      "Cian Eastwood",
      "Shashank Singh",
      "Andrei L Nicolicioiu",
      "Marin Vlastelica Pogančić",
      "Julius von Kügelgen",
      "Bernhard Schölkopf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3accfe8332366a6f740d8740cd4cd653-Abstract-Conference.html": {
    "title": "A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints",
    "volume": "main",
    "abstract": "Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive auto-regressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire likelihood distribution, we propose to do so on a random, local approximation thereof. More precisely, we approximate the likelihood of the constraint with the pseudolikelihood of the constraint centered around a model sample. Our approach is factorizable, allowing us to reuse solutions to sub-problems---a main tenet for the efficient computation of neuro-symbolic losses. It also provides a local, high fidelity approximation of the likelihood: it exhibits low entropy and KL-divergence around the model sample. We tested our approach on Sudoku and shortest-path prediction cast as auto-regressive generation, and observe that we greatly improve upon the base model's ability to predict logically-consistent outputs. We also tested our approach on the task of detoxifying large language models. We observe that using a simple constraint disallowing a list of toxic words, we are able to steer the model's outputs away from toxic generations, achieving SoTA compared to previous approaches",
    "keywords": [],
    "checked": true,
    "id": "bcc998aa4f11d5690a45a215a9734655be9a8c19",
    "semantic_title": "a pseudo-semantic loss for autoregressive models with logical constraints",
    "citation_count": 0,
    "authors": [
      "Kareem Ahmed",
      "Kai-Wei Chang",
      "Guy Van den Broeck"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3adb85a348a18cdd74ce99fbbab20301-Abstract-Conference.html": {
    "title": "Physics-Informed Bayesian Optimization of Variational Quantum Circuits",
    "volume": "main",
    "abstract": "In this paper, we propose a novel and powerful method to harness Bayesian optimization for variational quantum eigensolvers (VQEs) - a hybrid quantum-classical protocol used to approximate the ground state of a quantum Hamiltonian. Specifically, we derive a VQE-kernel which incorporates important prior information about quantum circuits: the kernel feature map of the VQE-kernel exactly matches the known functional form of the VQE's objective function and thereby significantly reduces the posterior uncertainty.Moreover, we propose a novel acquisition function for Bayesian optimization called \\emph{Expected Maximum Improvement over Confident Regions} (EMICoRe) which can actively exploit the inductive bias of the VQE-kernel by treating regions with low predictive uncertainty as indirectly \"observed\". As a result, observations at as few as three points in the search domain are sufficient to determine the complete objective function along an entire one-dimensional subspace of the optimization landscape. Our numerical experiments demonstrate that our approach improves over state-of-the-art baselines",
    "keywords": [],
    "checked": false,
    "id": "f4613763a41221c88547423a664d6aae1441809e",
    "semantic_title": "translational quantum machine intelligence for modeling tumor dynamics in oncology",
    "citation_count": 2,
    "authors": [
      "Kim Nicoli",
      "Christopher J. Anders",
      "Lena Funcke",
      "Tobias Hartung",
      "Karl Jansen",
      "Stefan Kühn",
      "Klaus-Robert Müller",
      "Paolo Stornati",
      "Pan Kessel",
      "Shinichi Nakajima"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ae86071c169649bff21188c536163dc-Abstract-Conference.html": {
    "title": "Rubik's Cube: High-Order Channel Interactions with a Hierarchical Receptive Field",
    "volume": "main",
    "abstract": "Image restoration techniques, spanning from the convolution to the transformer paradigm, have demonstrated robust spatial representation capabilities to deliver high-quality performance.Yet, many of these methods, such as convolution and the Feed Forward Network (FFN) structure of transformers, primarily leverage the basic first-order channel interactions and have not maximized the potential benefits of higher-order modeling. To address this limitation, our research dives into understanding relationships within the channel dimension and introduces a simple yet efficient, high-order channel-wise operator tailored for image restoration. Instead of merely mimicking high-order spatial interaction, our approach offers several added benefits: Efficiency: It adheres to the zero-FLOP and zero-parameter principle, using a spatial-shifting mechanism across channel-wise groups. Simplicity: It turns the favorable channel interaction and aggregation capabilities into element-wise multiplications and convolution units with $1 \\times 1$ kernel. Our new formulation expands the first-order channel-wise interactions seen in previous works to arbitrary high orders, generating a hierarchical receptive field akin to a Rubik's cube through the combined action of shifting and interactions. Furthermore, our proposed Rubik's cube convolution is a flexible operator that can be incorporated into existing image restoration networks, serving as a drop-in replacement for the standard convolution unit with fewer parameters overhead. We conducted experiments across various low-level vision tasks, including image denoising, low-light image enhancement, guided image super-resolution, and image de-blurring. The results consistently demonstrate that our Rubik's cube operator enhances performance across all tasks. Code is publicly available at https://github.com/zheng980629/RubikCube",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naishan Zheng",
      "man zhou",
      "Chong Zhou",
      "Chen Change Loy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ae8a7d6fc6d0d45e7c1ad9d4b063a01-Abstract-Conference.html": {
    "title": "Closing the Computational-Statistical Gap in Best Arm Identification for Combinatorial Semi-bandits",
    "volume": "main",
    "abstract": "We study the best arm identification problem in combinatorial semi-bandits in the fixed confidence setting. We present Perturbed Frank-Wolfe Sampling (P-FWS), an algorithm that (i) runs in polynomial time, (ii) achieves the instance-specific minimal sample complexity in the high confidence regime, and (iii) enjoys polynomial sample complexity guarantees in the moderate confidence regime. To our best knowledge, existing algorithms cannot achieve (ii) and (iii) simultaneously in vanilla bandits. With P-FWS, we close the computational-statistical gap in best arm identification in combinatorial semi-bandits. The design of P-FWS starts from the optimization problem that defines the information-theoretical and instance-specific sample complexity lower bound. P-FWS solves this problem in an online manner using, in each round, a single iteration of the Frank-Wolfe algorithm. Structural properties of the problem are leveraged to make the P-FWS successive updates computationally efficient. In turn, P-FWS only relies on a simple linear maximization oracle",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruo-Chun Tzeng",
      "Po-An Wang",
      "Alexandre Proutiere",
      "Chi-Jen Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3af25aa3de8b7b02ddbd1b6be5031be8-Abstract-Conference.html": {
    "title": "Imitation Learning from Imperfection: Theoretical Justifications and Algorithms",
    "volume": "main",
    "abstract": "Imitation learning (IL) algorithms excel in acquiring high-quality policies from expert data for sequential decision-making tasks. But, their effectiveness is hampered when faced with limited expert data. To tackle this challenge, a novel framework called (offline) IL with supplementary data has emerged, which enhances learning by incorporating an additional yet imperfect dataset obtained inexpensively from sub-optimal policies. Nonetheless, learning becomes challenging due to the potential inclusion of out-of-expert-distribution samples. In this work, we pioneer the mathematical formalization of this framework, uncovering its limitations. Our theoretical analysis reveals that a naive approach—applying the behavioral cloning (BC) algorithm concept to the combined set of expert and supplementary data—may fall short of vanilla BC, which solely relies on expert data. This deficiency arises due to the distribution shift between the two data sources. To address this issue, we propose a new importance-sampling-based technique for selecting data within the expert distribution. We prove that the proposed method theoretically eliminates the gap of the naive approach, highlighting its efficacy when handling imperfect data. Empirical studies demonstrate that our method outperforms previous state-of-the-art methods in tasks including robotics locomotion control, Atari video games, and image classification. Overall, our work underscores the potential of improving IL by leveraging diverse data sources through effective data selection",
    "keywords": [],
    "checked": false,
    "id": "fab87cc094c0aedf2a283371de8339f466fdf3f8",
    "semantic_title": "learning to weight imperfect demonstrations",
    "citation_count": 31,
    "authors": [
      "Ziniu Li",
      "Tian Xu",
      "Zeyu Qin",
      "Yang Yu",
      "Zhi-Quan Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3af8c40dcf1bc94fa570a5e42edf219d-Abstract-Conference.html": {
    "title": "Detection Based Part-level Articulated Object Reconstruction from Single RGBD Image",
    "volume": "main",
    "abstract": "We propose an end-to-end trainable, cross-category method for reconstructing multiple man-made articulated objects from a single RGBD image, focusing on part-level shape reconstruction and pose and kinematics estimation. We depart from previous works that rely on learning instance-level latent space, focusing on man-made articulated objects with predefined part counts. Instead, we propose a novel alternative approach that employs part-level representation, representing instances as combinations of detected parts. While our detect-then-group approach effectively handles instances with diverse part structures and various part counts, it faces issues of false positives, varying part sizes and scales, and an increasing model size due to end-to-end training. To address these challenges, we propose 1) test-time kinematics-aware part fusion to improve detection performance while suppressing false positives, 2) anisotropic scale normalization for part shape learning to accommodate various part sizes and scales, and 3) a balancing strategy for cross-refinement between feature space and output space to improve part detection while maintaining model size. Evaluation on both synthetic and real data demonstrates that our method successfully reconstructs variously structured multiple instances that previous works cannot handle, and outperforms prior works in shape reconstruction and kinematics estimation",
    "keywords": [],
    "checked": false,
    "id": "13b7073f012a9ee711f0bb8a3914055180db21e9",
    "semantic_title": "saor: single-view articulated object reconstruction",
    "citation_count": 1,
    "authors": [
      "Yuki Kawana",
      "Tatsuya Harada"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3b11c5cc84b6da2838db348b37dbd1a2-Abstract-Conference.html": {
    "title": "FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Semi-Supervised Learning (SSL) has been an effective way to leverage abundant unlabeled data with extremely scarce labeled data. However, most SSL methods are commonly based on instance-wise consistency between different data transformations. Therefore, the label guidance on labeled data is hard to be propagated to unlabeled data. Consequently, the learning process on labeled data is much faster than on unlabeled data which is likely to fall into a local minima that does not favor unlabeled data, leading to sub-optimal generalization performance. In this paper, we propose FlatMatch which minimizes a cross-sharpness measure to ensure consistent learning performance between the two datasets. Specifically, we increase the empirical risk on labeled data to obtain a worst-case model which is a failure case needing to be enhanced. Then, by leveraging the richness of unlabeled data, we penalize the prediction difference (i.e., cross-sharpness) between the worst-case model and the original model so that the learning direction is beneficial to generalization on unlabeled data. Therefore, we can calibrate the learning process without being limited to insufficient label information. As a result, the mismatched learning performance can be mitigated, further enabling the effective exploitation of unlabeled data and improving SSL performance. Through comprehensive validation, we show FlatMatch achieves state-of-the-art results in many SSL settings",
    "keywords": [],
    "checked": true,
    "id": "e9aacc249093b439a27704e5a1a199aef2c5ab65",
    "semantic_title": "flatmatch: bridging labeled data and unlabeled data with cross-sharpness for semi-supervised learning",
    "citation_count": 1,
    "authors": [
      "Zhuo Huang",
      "Li Shen",
      "Jun Yu",
      "Bo Han",
      "Tongliang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3b1675de6b49cc00084374213f8c38ae-Abstract-Conference.html": {
    "title": "Neural Sculpting: Uncovering hierarchically modular task structure in neural networks through pruning and network analysis",
    "volume": "main",
    "abstract": "Natural target functions and tasks typically exhibit hierarchical modularity -- they can be broken down into simpler sub-functions that are organized in a hierarchy. Such sub-functions have two important features: they have a distinct set of inputs (input-separability) and they are reused as inputs higher in the hierarchy (reusability). Previous studies have established that hierarchically modular neural networks, which are inherently sparse, offer benefits such as learning efficiency, generalization, multi-task learning, and transfer. However, identifying the underlying sub-functions and their hierarchical structure for a given task can be challenging. The high-level question in this work is: if we learn a task using a sufficiently deep neural network, how can we uncover the underlying hierarchy of sub-functions in that task? As a starting point, we examine the domain of Boolean functions, where it is easier to determine whether a task is hierarchically modular. We propose an approach based on iterative unit and edge pruning (during training), combined with network analysis for module detection and hierarchy inference. Finally, we demonstrate that this method can uncover the hierarchical modularity of a wide range of Boolean functions and two vision tasks based on the MNIST digits dataset",
    "keywords": [],
    "checked": false,
    "id": "1ffdd834926cefd94279b5638e2d67d9bfd11f6c",
    "semantic_title": "neural sculpting: uncovering hierarchically modular task structure through pruning and network analysis",
    "citation_count": 0,
    "authors": [
      "Shreyas Malakarjun Patil",
      "Loizos Michael",
      "Constantine Dovrolis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3b3889d313ba9476c12c2d77ea66b24f-Abstract-Conference.html": {
    "title": "Elastic Decision Transformer",
    "volume": "main",
    "abstract": "This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to \"stitch\" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regime on the D4RL locomotion benchmark and Atari games",
    "keywords": [],
    "checked": true,
    "id": "ca0dab2fc125a998c8af92e731a04d298e5fd0ec",
    "semantic_title": "elastic decision transformer",
    "citation_count": 7,
    "authors": [
      "Yueh-Hua Wu",
      "Xiaolong Wang",
      "Masashi Hamaya"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3b3a83a5d86e1d424daefed43d998079-Abstract-Conference.html": {
    "title": "Asymptotically Optimal Quantile Pure Exploration for Infinite-Armed Bandits",
    "volume": "main",
    "abstract": "We study pure exploration with infinitely many bandit arms generated \\iid from an unknown distribution. Our goal is to efficiently select a single high quality arm whose average reward is, with probability $1-\\delta$, within $\\varepsilon$ of being with the top $\\eta$-fraction of arms; this is a natural adaptation of the classical PAC guarantee for infinite action sets. We consider both the fixed confidence and fixed budget settings, aiming respectively for optimal \\emph{expected} and \\emph{fixed} sample complexity.For fixed confidence, we give an algorithm with expected sample complexity $O\\left(\\frac{\\log (1/\\eta)\\log (1/\\delta)}{\\eta\\varepsilon^2}\\right)$. This is optimal except for the $\\log (1/\\eta)$ factor, and the $\\delta$-dependence closes a quadratic gap in the literature. For fixed budget, we show the asymptotically optimal sample complexity as $\\delta\\to 0$ is $c^{-1}\\log(1/\\delta)\\big(\\log\\log(1/\\delta)\\big)^2$ to leading order; equivalently, the optimal failure probability with exactly $N$ samples decays as $\\exp\\big(-(1\\pm o(1))\\frac{cN}{\\log^2 N}\\big)$.The value of $c$ depends explicitly on the problem parameters (including the unknown arm distribution) through a certain Fisher information distance. Even the strictly super-linear dependence on $\\log(1/\\delta)$ was not known and resolves a question of Grossman-Moshkovitz (FOCS 2015)",
    "keywords": [],
    "checked": false,
    "id": "abbc4d77cd6bfe2893609792ca2f4e65f3a15b8b",
    "semantic_title": "asymptotically optimal pure exploration for infinite-armed bandits",
    "citation_count": 0,
    "authors": [
      "Evelyn Xiao-Yue Gong",
      "Mark Sellke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3b5c7c9c5c7bd77eb73d0baec7a07165-Abstract-Conference.html": {
    "title": "Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance",
    "volume": "main",
    "abstract": "We present a novel framework to overcome the limitations of equivariant architectures in learning functions with group symmetries. In contrary to equivariant architectures, the framework uses an arbitrary backbone (such as an MLP or a transformer) and symmetrizes it to be equivariant to given group by employing a small equivariant network that parameterizes the probabilistic distribution underlying the symmetrization. The distribution is end-to-end trained with the backbone which can maximize performance while reducing sample complexity of symmetrization. We show that this approach ensures not only equivariance to the given group but also universal approximation ability in expectation. We implement our method on a simple patch-based transformer backbone initialized from pretrained vision transformer, and test it for a wide range of symmetry groups including permutation and Euclidean groups and their combinations. Empirical tests show competitive results against tailored equivariant architectures, suggesting the potential for learning equivariant functions for diverse groups using a non-equivariant universal backbone. We further show evidence of enhanced learning in symmetric modalities, like graphs, when pretrained from non-symmetric modalities, like vision",
    "keywords": [],
    "checked": true,
    "id": "ae7ae7b7e123b8d5ec86cc1c53548943e88f386f",
    "semantic_title": "learning probabilistic symmetrization for architecture agnostic equivariance",
    "citation_count": 3,
    "authors": [
      "Jinwoo Kim",
      "Dat Nguyen",
      "Ayhan Suleymanzade",
      "Hyeokjun An",
      "Seunghoon Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3b7a66b2d1258e892c89f485b8f896e0-Abstract-Conference.html": {
    "title": "Distributionally Robust Linear Quadratic Control",
    "volume": "main",
    "abstract": "Linear-Quadratic-Gaussian (LQG) control is a fundamental control paradigm that is studied in various fields such as engineering, computer science, economics, and neuroscience. It involves controlling a system with linear dynamics and imperfect observations, subject to additive noise, with the goal of minimizing a quadratic cost function for the state and control variables. In this work, we consider a generalization of the discrete-time, finite-horizon LQG problem, where the noise distributions are unknown and belong to Wasserstein ambiguity sets centered at nominal (Gaussian) distributions. The objective is to minimize a worst-case cost across all distributions in the ambiguity set, including non-Gaussian distributions. Despite the added complexity, we prove that a control policy that is linear in the observations is optimal for this problem, as in the classic LQG problem. We propose a numerical solution method that efficiently characterizes this optimal control policy. Our method uses the Frank-Wolfe algorithm to identify the least-favorable distributions within the Wasserstein ambiguity sets and computes the controller's optimal policy using Kalman filter estimation under these distributions",
    "keywords": [],
    "checked": true,
    "id": "9cd2c80bd9279aeda6d9a252c3abefbfd73ab672",
    "semantic_title": "distributionally robust linear quadratic control",
    "citation_count": 6,
    "authors": [
      "Bahar Taskesen",
      "Dan Iancu",
      "Çağıl Koçyiğit",
      "Daniel Kuhn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3b7ba46201bf15e5c3935272afae50db-Abstract-Conference.html": {
    "title": "Fully Dynamic $k$-Clustering in $\\tilde O(k)$ Update Time",
    "volume": "main",
    "abstract": "We present a $O(1)$-approximate fully dynamic algorithm for the $k$-median and $k$-means problems on metric spaces with amortized update time $\\tilde O(k)$ and worst-case query time $\\tilde O(k^2)$. We complement our theoretical analysis with the first in-depth experimental study for the dynamic $k$-median problem on general metrics, focusing on comparing our dynamic algorithm to the current state-of-the-art by Henzinger and Kale [ESA'20]. Finally, we also provide a lower bound for dynamic $k$-median which shows that any $O(1)$-approximate algorithm with $\\tilde O(\\text{poly}(k))$ query time must have $\\tilde \\Omega(k)$ amortized update time, even in the incremental setting",
    "keywords": [],
    "checked": true,
    "id": "629821f9b4e9e053de725a8a78d2b47ba3710f33",
    "semantic_title": "fully dynamic $k$-clustering in $\\tilde o(k)$ update time",
    "citation_count": 0,
    "authors": [
      "Sayan Bhattacharya",
      "Martín Costa",
      "Silvio Lattanzi",
      "Nikos Parotsidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ba7560b4c3e66d760fbdd472cf4a5a9-Abstract-Conference.html": {
    "title": "FreeMask: Synthetic Images with Dense Annotations Make Stronger Segmentation Models",
    "volume": "main",
    "abstract": "Semantic segmentation has witnessed tremendous progress due to the proposal of various advanced network architectures. However, they are extremely hungry for delicate annotations to train, and the acquisition is laborious and unaffordable. Therefore, we present FreeMask in this work, which resorts to synthetic images from generative models to ease the burden of both data collection and annotation procedures. Concretely, we first synthesize abundant training images conditioned on the semantic masks provided by realistic datasets. This yields extra well-aligned image-mask training pairs for semantic segmentation models. We surprisingly observe that, solely trained with synthetic images, we already achieve comparable performance with real ones (e.g., 48.3 vs. 48.5 mIoU on ADE20K, and 49.3 vs. 50.5 on COCO-Stuff). Then, we investigate the role of synthetic images by joint training with real images, or pre-training for real images. Meantime, we design a robust filtering principle to suppress incorrectly synthesized regions. In addition, we propose to inequally treat different semantic masks to prioritize those harder ones and sample more corresponding synthetic images for them. As a result, either jointly trained or pre-trained with our filtered and re-sampled synthesized images, segmentation models can be greatly enhanced, e.g., from 48.7 to 52.0 on ADE20K",
    "keywords": [],
    "checked": true,
    "id": "9d0e80302cdd1cfac77637591e140053d2524d7c",
    "semantic_title": "freemask: synthetic images with dense annotations make stronger segmentation models",
    "citation_count": 4,
    "authors": [
      "Lihe Yang",
      "Xiaogang Xu",
      "Bingyi Kang",
      "Yinghuan Shi",
      "Hengshuang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ba82362eb0aa75487069f19fde794fe-Abstract-Conference.html": {
    "title": "RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers via Randomized Deletion",
    "volume": "main",
    "abstract": "Randomized smoothing is a leading approach for constructing classifiers that are certifiably robust against adversarial examples. Existing work on randomized smoothing has focused on classifiers with continuous inputs, such as images, where $\\ell_p$-norm bounded adversaries are commonly studied. However, there has been limited work for classifiers with discrete or variable-size inputs, such as for source code, which require different threat models and smoothing mechanisms. In this work, we adapt randomized smoothing for discrete sequence classifiers to provide certified robustness against edit distance-bounded adversaries. Our proposed smoothing mechanism randomized deletion (RS-Del) applies random deletion edits, which are (perhaps surprisingly) sufficient to confer robustness against adversarial deletion, insertion and substitution edits. Our proof of certification deviates from the established Neyman-Pearson approach, which is intractable in our setting, and is instead organized around longest common subsequences. We present a case study on malware detection—a binary classification problem on byte sequences where classifier evasion is a well-established threat model. When applied to the popular MalConv malware detection model, our smoothing mechanism RS-Del achieves a certified accuracy of 91% at an edit distance radius of 128 bytes",
    "keywords": [],
    "checked": true,
    "id": "ae29593c989689962b9887957db5a528188eee60",
    "semantic_title": "rs-del: edit distance robustness certificates for sequence classifiers via randomized deletion",
    "citation_count": 2,
    "authors": [
      "Zhuoqun Huang",
      "Neil G Marchant",
      "Keane Lucas",
      "Lujo Bauer",
      "Olga Ohrimenko",
      "Benjamin Rubinstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3baf4eeffad860ca9c54aeab632716b4-Abstract-Conference.html": {
    "title": "Flow: Per-instance Personalized Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) suffers from data heterogeneity, where the diverse data distributions across clients make it challenging to train a single global model effectively. Existing personalization approaches aim to address the data heterogeneity issue by creating a personalized model for each client from the global model that fits their local data distribution. However, these personalized models may achieve lower accuracy than the global model in some clients, resulting in limited performance improvement compared to that without personalization. To overcome this limitation, we propose a per-instance personalization FL algorithm Flow. Flow creates dynamic personalized models that are adaptive not only to each client's data distributions but also to each client's data instances. The personalized model allows each instance to dynamically determine whether it prefers the local parameters or its global counterpart to make correct predictions, thereby improving clients'accuracy. We provide theoretical analysis on the convergence of Flow and empirically demonstrate the superiority of Flow in improving clients' accuracy compared to state-of-the-art personalization approaches on both vision and language-based tasks",
    "keywords": [],
    "checked": false,
    "id": "a11d9fd38780a506a6d8ac4ed9da4abda186599d",
    "semantic_title": "flow: per-instance personalized federated learning through dynamic routing",
    "citation_count": 0,
    "authors": [
      "Kunjal Panchal",
      "Sunav Choudhary",
      "Nisarg Parikh",
      "Lijun Zhang",
      "Hui Guan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3be7859b36d9440372cae0a293f2e4cc-Abstract-Conference.html": {
    "title": "Inverse Preference Learning: Preference-based RL without a Reward Function",
    "volume": "main",
    "abstract": "Reward functions are difficult to design and often hard to align with human intent. Preference-based Reinforcement Learning (RL) algorithms address these problems by learning reward functions from human feedback. However, the majority of preference-based RL methods na\\\"ively combine supervised reward models with off-the-shelf RL algorithms. Contemporary approaches have sought to improve performance and query complexity by using larger and more complex reward architectures such as transformers. Instead of using highly complex architectures, we develop a new and parameter-efficient algorithm, Inverse Preference Learning (IPL), specifically designed for learning from offline preference data. Our key insight is that for a fixed policy, the $Q$-function encodes all information about the reward function, effectively making them interchangeable. Using this insight, we completely eliminate the need for a learned reward function. Our resulting algorithm is simpler and more parameter-efficient. Across a suite of continuous control and robotics benchmarks, IPL attains competitive performance compared to more complex approaches that leverage transformer-based and non-Markovian reward functions while having fewer algorithmic hyperparameters and learned network parameters. Our code is publicly released",
    "keywords": [],
    "checked": true,
    "id": "4367911d9d28d83fafbcf6c908698dd981ddbe9e",
    "semantic_title": "inverse preference learning: preference-based rl without a reward function",
    "citation_count": 11,
    "authors": [
      "Joey Hejna",
      "Dorsa Sadigh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3bf4b55960aaa23553cd2a6bdc6e1b57-Abstract-Conference.html": {
    "title": "Matrix Compression via Randomized Low Rank and Low Precision Factorization",
    "volume": "main",
    "abstract": "Matrices are exceptionally useful in various fields of study as they provide a convenient framework to organize and manipulate data in a structured manner. However, modern matrices can involve billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Although prohibitively large, such matrices are often approximately low rank. We propose an algorithm that exploits this structure to obtain a low rank decomposition of any matrix $\\mathbf{A}$ as $\\mathbf{A} \\approx \\mathbf{L}\\mathbf{R}$, where $\\mathbf{L}$ and $\\mathbf{R}$ are the low rank factors. The total number of elements in $\\mathbf{L}$ and $\\mathbf{R}$ can be significantly less than that in $\\mathbf{A}$. Furthermore, the entries of $\\mathbf{L}$ and $\\mathbf{R}$ are quantized to low precision formats -- compressing $\\mathbf{A}$ by giving us a low rank and low precision factorization. Our algorithm first computes an approximate basis of the range space of $\\mathbf{A}$ by randomly sketching its columns, followed by a quantization of the vectors constituting this basis. It then computes approximate projections of the columns of $\\mathbf{A}$ onto this quantized basis. We derive upper bounds on the approximation error of our algorithm, and analyze the impact of target rank and quantization bit-budget. The tradeoff between compression ratio and approximation accuracy allows for flexibility in choosing these parameters based on specific application requirements. We empirically demonstrate the efficacy of our algorithm in image compression, nearest neighbor classification of image and text embeddings, and compressing the layers of LlaMa-$7$b. Our results illustrate that we can achieve compression ratios as aggressive as one bit per matrix coordinate, all while surpassing or maintaining the performance of traditional compression techniques",
    "keywords": [],
    "checked": true,
    "id": "43017a16dfe593c09533c5fb3c3612c83761a98a",
    "semantic_title": "matrix compression via randomized low rank and low precision factorization",
    "citation_count": 0,
    "authors": [
      "Rajarshi Saha",
      "Varun Srivastava",
      "Mert Pilanci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3c129892b4f9c8326aba665425a470c5-Abstract-Conference.html": {
    "title": "Prompt-augmented Temporal Point Process for Streaming Event Sequence",
    "volume": "main",
    "abstract": "Neural Temporal Point Processes (TPPs) are the prevalent paradigm for modeling continuous-time event sequences, such as user activities on the web and financial transactions. In real world applications, the event data typically comes in a streaming manner, where the distribution of the patterns may shift over time. Under the privacy and memory constraints commonly seen in real scenarios, how to continuously monitor a TPP to learn the streaming event sequence is an important yet under-investigated problem. In this work, we approach this problem by adopting Continual Learning (CL), which aims to enable a model to continuously learn a sequence of tasks without catastrophic forgetting. While CL for event sequence is less well studied, we present a simple yet effective framework, PromptTPP, by integrating the base TPP with a continuous-time retrieval prompt pool. In our proposed framework, prompts are small learnable parameters, maintained in a memory space and jointly optimized with the base TPP so that the model is properly instructed to learn event streams arriving sequentially without buffering past examples or task-specific attributes. We formalize a novel and realistic experimental setup for modeling event streams, where PromptTPP consistently sets state-of-the-art performance across two real user behavior datasets",
    "keywords": [],
    "checked": true,
    "id": "f3f9cd510ab2e0518e85e1ea88e5c8d9a9d98202",
    "semantic_title": "prompt-augmented temporal point process for streaming event sequence",
    "citation_count": 7,
    "authors": [
      "Siqiao Xue",
      "Yan Wang",
      "Zhixuan Chu",
      "Xiaoming Shi",
      "Caigao JIANG",
      "Hongyan Hao",
      "Gangwei Jiang",
      "Xiaoyun Feng",
      "James Zhang",
      "Jun Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3c2b60a3f269c404e9329ee119f2d34a-Abstract-Conference.html": {
    "title": "Leveraging Locality and Robustness to Achieve Massively Scalable Gaussian Process Regression",
    "volume": "main",
    "abstract": "The accurate predictions and principled uncertainty measures provided by GP regression incur $O(n^3)$ cost which is prohibitive for modern-day large-scale applications. This has motivated extensive work on computationally efficient approximations. We introduce a new perspective by exploring robustness properties and limiting behaviour of GP nearest-neighbour (GPnn) prediction. We demonstrate through theory and simulation that as the data-size $n$ increases, accuracy of estimated parameters and GP model assumptions become increasingly irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend small amounts of work on parameter estimation in order to achieve high MSE accuracy, even in the presence of gross misspecification. In contrast, as $n \\rightarrow \\infty$, uncertainty calibration and NLL are shown to remain sensitive to just one parameter, the additive noise-variance; but we show that this source of inaccuracy can be corrected for, thereby achieving both well-calibrated uncertainty measures and accurate predictions at remarkably low computational cost. We exhibit a very simple GPnn regression algorithm with stand-out performance compared to other state-of-the-art GP approximations as measured on large UCI datasets. It operates at a small fraction of those other methods' training costs, for example on a basic laptop taking about 30 seconds to train on a dataset of size $n = 1.6 \\times 10^6$",
    "keywords": [],
    "checked": true,
    "id": "8b3450a8bbfb19bd7f489b86da7b2f9890db44ec",
    "semantic_title": "leveraging locality and robustness to achieve massively scalable gaussian process regression",
    "citation_count": 0,
    "authors": [
      "Robert Allison",
      "Anthony Stephenson",
      "Samuel F",
      "Edward O Pyzer-Knapp"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3c5e64f26a97db6a2b0bbb788236431e-Abstract-Conference.html": {
    "title": "Direct Training of SNN using Local Zeroth Order Method",
    "volume": "main",
    "abstract": "Spiking neural networks are becoming increasingly popular for their low energy requirement in real-world tasks with accuracy comparable to traditional ANNs. SNN training algorithms face the loss of gradient information and non-differentiability due to the Heaviside function in minimizing the model loss over model parameters. To circumvent this problem, the surrogate method employs a differentiable approximation of the Heaviside function in the backward pass, while the forward pass continues to use the Heaviside as the spiking function. We propose to use the zeroth-order technique at the local or neuron level in training SNNs, motivated by its regularizing and potential energy-efficient effects and establish a theoretical connection between it and the existing surrogate methods. We perform experimental validation of the technique on standard static datasets (CIFAR-10, CIFAR-100, ImageNet-100) and neuromorphic datasets (DVS-CIFAR-10, DVS-Gesture, N-Caltech-101, NCARS) and obtain results that offer improvement over the state-of-the-art results. The proposed method also lends itself to efficient implementations of the back-propagation method, which could provide 3-4 times overall speedup in training time. The code is available at \\url{https://github.com/BhaskarMukhoty/LocalZO}",
    "keywords": [],
    "checked": false,
    "id": "41ccf8438295b3aa439cf54944ab96d0f7f368e9",
    "semantic_title": "energy efficient training of snn using local zeroth order method",
    "citation_count": 0,
    "authors": [
      "Bhaskar Mukhoty",
      "Velibor Bojkovic",
      "William de Vazelhes",
      "Xiaohan Zhao",
      "Giulia De Masi",
      "Huan Xiong",
      "Bin Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3c646b713f5de2cf1ab1939d49a4036d-Abstract-Conference.html": {
    "title": "Discover and Align Taxonomic Context Priors for Open-world Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Open-world Semi-Supervised Learning (OSSL) is a realistic and challenging task, aiming to classify unlabeled samples from both seen and novel classes using partially labeled samples from the seen classes. Previous works typically explore the relationship of samples as priors on the pre-defined single-granularity labels to help novel class recognition. In fact, classes follow a taxonomy and samples can be classified at multiple levels of granularity, which contains more underlying relationships for supervision. We thus argue that learning with single-granularity labels results in sub-optimal representation learning and inaccurate pseudo labels, especially with unknown classes. In this paper, we take the initiative to explore and propose a uniformed framework, called Taxonomic context prIors Discovering and Aligning (TIDA), which exploits the relationship of samples under various granularity. It allows us to discover multi-granularity semantic concepts as taxonomic context priors (i.e., sub-class, target-class, and super-class), and then collaboratively leverage them to enhance representation learning and improve the quality of pseudo labels.Specifically, TIDA comprises two components: i) A taxonomic context discovery module that constructs a set of hierarchical prototypes in the latent space to discover the underlying taxonomic context priors; ii) A taxonomic context-based prediction alignment module that enforces consistency across hierarchical predictions to build the reliable relationship between classes among various granularity and provide additions supervision. We demonstrate that these two components are mutually beneficial for an effective OSSL framework, which is theoretically explained from the perspective of the EM algorithm. Extensive experiments on seven commonly used datasets show that TIDA can significantly improve the performance and achieve a new state of the art. The source codes are publicly available at https://github.com/rain305f/TIDA",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Zhun Zhong",
      "Pengchong Qiao",
      "Xuxin Cheng",
      "Xiawu Zheng",
      "Chang Liu",
      "Nicu Sebe",
      "Rongrong Ji",
      "Jie Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3c6696d70d364337cf98dcb7c652a770-Abstract-Conference.html": {
    "title": "Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models",
    "volume": "main",
    "abstract": "Generalized Additive Models (GAMs) have recently experienced a resurgence in popularity due to their interpretability, which arises from expressing the target value as a sum of non-linear transformations of the features. Despite the current enthusiasm for GAMs, their susceptibility to concurvity — i.e., (possibly non-linear) dependencies between the features — has hitherto been largely overlooked. Here, we demonstrate how concurvity can severly impair the interpretability of GAMs and propose a remedy: a conceptually simple, yet effective regularizer which penalizes pairwise correlations of the non-linearly transformed feature variables. This procedure is applicable to any differentiable additive model, such as Neural Additive Models or NeuralProphet, and enhances interpretability by eliminating ambiguities due to self-canceling feature contributions. We validate the effectiveness of our regularizer in experiments on synthetic as well as real-world datasets for time-series and tabular data. Our experiments show that concurvity in GAMs can be reduced without significantly compromising prediction quality, improving interpretability and reducing variance in the feature importances",
    "keywords": [],
    "checked": true,
    "id": "15118e8f0a09f4ed275e3cdae5968f1c93c37186",
    "semantic_title": "curve your enthusiasm: concurvity regularization in differentiable generalized additive models",
    "citation_count": 0,
    "authors": [
      "Julien Siems",
      "Konstantin Ditschuneit",
      "Winfried Ripken",
      "Alma Lindborg",
      "Maximilian Schambach",
      "Johannes Otterbach",
      "Martin Genzel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3c6bd2021c10462c5164638d22f3d5d8-Abstract-Conference.html": {
    "title": "Mutual Information Regularized Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "The major challenge of offline RL is the distribution shift that appears when out-of-distribution actions are queried, which makes the policy improvement direction biased by extrapolation errors. Most existing methods address this problem by penalizing the policy or value for deviating from the behavior policy during policy improvement or evaluation. In this work, we propose a novel MISA framework to approach offline RL from the perspective of Mutual Information between States and Actions in the dataset by directly constraining the policy improvement direction. MISA constructs lower bounds of mutual information parameterized by the policy and Q-values. We show that optimizing this lower bound is equivalent to maximizing the likelihood of a one-step improved policy on the offline dataset. Hence, we constrain the policy improvement direction to lie in the data manifold. The resulting algorithm simultaneously augments the policy evaluation and improvement by adding mutual information regularizations. MISA is a general framework that unifies conservative Q-learning (CQL) and behavior regularization methods (e.g., TD3+BC) as special cases. We introduce 3 different variants of MISA, and empirically demonstrate that tighter mutual information lower bound gives better offline RL performance. In addition, our extensive experiments show MISA significantly outperforms a wide range of baselines on various tasks of the D4RL benchmark, e.g., achieving 742.9 total points on gym-locomotion tasks. Our code is attached and will be released upon publication",
    "keywords": [],
    "checked": true,
    "id": "2b23a0e5c1709d4a82e1bd09a83e2cef83d8dfe8",
    "semantic_title": "mutual information regularized offline reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Xiao Ma",
      "Bingyi Kang",
      "Zhongwen Xu",
      "Min Lin",
      "Shuicheng Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3cbf627fa24fb6cb576e04e689b9428b-Abstract-Conference.html": {
    "title": "Have it your way: Individualized Privacy Assignment for DP-SGD",
    "volume": "main",
    "abstract": "When training a machine learning model with differential privacy, one sets a privacy budget. This uniform budget represents an overall maximal privacy violation that any user is willing to face by contributing their data to the training set. We argue that this approach is limited because different users may have different privacy expectations. Thus, setting a uniform privacy budget across all points may be overly conservative for some users or, conversely, not sufficiently protective for others. In this paper, we capture these preferences through individualized privacy budgets. To demonstrate their practicality, we introduce a variant of Differentially Private Stochastic Gradient Descent (DP-SGD) which supports such individualized budgets. DP-SGD is the canonical approach to training models with differential privacy. We modify its data sampling and gradient noising mechanisms to arrive at our approach, which we call Individualized DP-SGD (IDP-SGD). Because IDP-SGD provides privacy guarantees tailored to the preferences of individual users and their data points, we empirically find it to improve privacy-utility trade-offs",
    "keywords": [],
    "checked": true,
    "id": "96e36b9e93079cd57ffaf3fe7ae75734cfd35318",
    "semantic_title": "have it your way: individualized privacy assignment for dp-sgd",
    "citation_count": 6,
    "authors": [
      "Franziska Boenisch",
      "Christopher Mühl",
      "Adam Dziedzic",
      "Roy Rinberg",
      "Nicolas Papernot"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3cc685788a311fa35d8d41df93e288ca-Abstract-Conference.html": {
    "title": "Penguin: Parallel-Packed Homomorphic Encryption for Fast Graph Convolutional Network Inference",
    "volume": "main",
    "abstract": "The marriage of Graph Convolutional Network (GCN) and Homomorphic Encryption (HE) enables the inference of graph data on the cloud with significantly enhanced client data privacy. However, the tremendous computation and memory overhead associated with HE operations challenges the practicality of HE-based GCN inference. GCN inference involves a sequence of expensive matrix-matrix multiplications, and we observe that directly applying the state-of-the-art HE-based secure matrix-matrix multiplication solutions to accelerate HE-GCN inference is far less efficient as it does not exploit the unique aggregation mechanism of two-dimension graph node-features in GCN layer computation. As a result, in this paper, we propose a novel HE-based ciphertext packing technique, i.e., Penguin, that can take advantage of the unique computation pattern during the HE-GCN inference to significantly reduce the computation and memory overhead associated with HE operations.Specifically, Penguin employs (i) an effective two-dimension parallel packing technique for feature ciphertext with optimal graph node partitioning and graph feature interleaving, and (ii) an interleaved assembly technique that can effectively make use of the blank slots to merge ciphertexts after feature reduction and significantly reduce the costly rotation operation.We provide theoretical analysis and experimental validation to demonstrate the speedup achieved by Penguin in accelerating GCN inference using popular GCN models and datasets. Our results show that Penguin can achieve up to $\\sim10\\times$ speedup and around $\\sim79$% reduction in computational memory overhead, significantly outperforming state-of-the-art solutions. To the best of our knowledge, this is the first work that can ensure the protection of both graph structure and features when accelerating HE-GCN inference on encrypted data. Our code is publicly available at https://github.com/ranran0523/Penguin",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Ran",
      "Nuo Xu",
      "Tao Liu",
      "Wei Wang",
      "Gang Quan",
      "Wujie Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3cc87f2bd3e3b4df8f9217326761c322-Abstract-Conference.html": {
    "title": "Learning Dynamic Attribute-factored World Models for Efficient Multi-object Reinforcement Learning",
    "volume": "main",
    "abstract": "In many reinforcement learning tasks, the agent has to learn to interact with many objects of different types and generalize to unseen combinations and numbers of objects. Often a task is a composition of previously learned tasks (e.g. block stacking).These are examples of compositional generalization, in which we compose object-centric representations to solve complex tasks. Recent works have shown the benefits of object-factored representations and hierarchical abstractions for improving sample efficiency in these settings. On the other hand, these methods do not fully exploit the benefits of factorization in terms of object attributes. In this paper, we address this opportunity and introduce the Dynamic Attribute FacTored RL (DAFT-RL) framework. In DAFT-RL, we leverage object-centric representation learning to extract objects from visual inputs. We learn to classify them into classes and infer their latent parameters. For each class of object, we learn a class template graph that describes how the dynamics and reward of an object of this class factorize according to its attributes. We also learn an interaction pattern graph that describes how objects of different classes interact with each other at the attribute level. Through these graphs and a dynamic interaction graph that models the interactions between objects, we can learn a policy that can then be directly applied in a new environment by estimating the interactions and latent parameters.We evaluate DAFT-RL in three benchmark datasets and show our framework outperforms the state-of-the-art in generalizing across unseen objects with varying attributes and latent parameters, as well as in the composition of previously learned tasks",
    "keywords": [],
    "checked": true,
    "id": "351df52b955b7663de307bf7d9360c39dcc223aa",
    "semantic_title": "learning dynamic attribute-factored world models for efficient multi-object reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Fan Feng",
      "Sara Magliacane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3cfc102893d47c46295cb437949dccb5-Abstract-Conference.html": {
    "title": "Statistical Insights into HSIC in High Dimensions",
    "volume": "main",
    "abstract": "Measuring the nonlinear dependence between random vectors and testing for their statistical independence is a fundamental problem in statistics. One of the most popular dependence measures is the Hilbert-Schmidt independence criterion (HSIC), which has attracted increasing attention in recent years. However, most existing works have focused on either fixed or very high-dimensional covariates. In this work, we bridge the gap between these two scenarios and provide statistical insights into the performance of HSIC when the dimensions grow at different rates. We first show that, under the null hypothesis, the rescaled HSIC converges in distribution to a standard normal distribution. Then we provide a general condition for the HSIC based tests to have nontrivial power in high dimensions. By decomposing this condition, we illustrate how the ability of HSIC to measure nonlinear dependence changes with increasing dimensions. Moreover, we demonstrate that, depending on the sample size, the covariate dimensions and the dependence structures within covariates, the HSIC can capture different types of associations between random vectors. We also conduct extensive numerical studies to validate our theoretical results",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Zhang",
      "Yaowu Zhang",
      "Tingyou Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d007df4ae13adf9001f8969555b11bd-Abstract-Conference.html": {
    "title": "Fair Adaptive Experiments",
    "volume": "main",
    "abstract": "Randomized experiments have been the gold standard for assessing the effectiveness of a treatment, policy, or intervention, spanning various fields, including social sciences, biomedical studies, and e-commerce. The classical complete randomization approach assigns treatments based on a pre-specified probability and may lead to inefficient use of data. Adaptive experiments improve upon complete randomization by sequentially learning and updating treatment assignment probabilities using accrued evidence during the experiment. Hence, they can help achieve efficient data use and higher estimation efficiency. However, their application can also raise fairness and equity concerns, as assignment probabilities may vary drastically across groups of participants. Furthermore, when treatment is expected to be extremely beneficial to certain groups of participants, it is more appropriate to expose many of these participants to favorable treatment. In response to these challenges, we propose a fair adaptive experiment strategy that simultaneously enhances data use efficiency, achieves an ``envy-free'' treatment assignment guarantee, and improves the overall welfare of participants. An important feature of our proposed strategy is that we do not impose parametric modeling assumptions on the outcome variables, making it more versatile and applicable to a wider array of applications. Through our theoretical investigation, we characterize the convergence rate of the estimated treatment effects and the associated standard deviations at the group level and further prove that our adaptive treatment assignment algorithm, despite not having a closed-form expression, approaches the optimal allocation rule asymptotically. Our proof strategy takes into account the fact that the allocation decisions in our design depend on sequentially accumulated data, which poses a significant challenge in characterizing the properties and conducting statistical inference of our method. We further provide simulation evidence and two synthetic data studies to showcase the performance of our fair adaptive experiment strategy",
    "keywords": [],
    "checked": true,
    "id": "8556a2564cc22eaf9c6e0256602ba0eba6c15ec9",
    "semantic_title": "fair adaptive experiments",
    "citation_count": 0,
    "authors": [
      "Waverly Wei",
      "Xinwei Ma",
      "Jingshen Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d17b7f7d52c83ab6e97e2dc0bda2e71-Abstract-Conference.html": {
    "title": "Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions",
    "volume": "main",
    "abstract": "We consider a setting where there are $N$ heterogeneous units and $p$ interventions. Our goal is to learn unit-specific potential outcomes for any combination of these $p$ interventions, i.e., $N \\times 2^p$ causal parameters. Choosing a combination of interventions is a problem that naturally arises in a variety of applications such as factorial design experiments and recommendation engines (e.g., showing a set of movies that maximizes engagement for a given user). Running $N \\times 2^p$ experiments to estimate the various parameters is likely expensive and/or infeasible as $N$ and $p$ grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome under that combination. We study this problem under a novel model that imposes latent structure across both units and combinations of interventions. Specifically, we assume latent similarity in potential outcomes across units (i.e., the matrix of potential outcomes is approximately rank $r$) and regularity in how combinations of interventions interact (i.e., the coefficients in the Fourier expansion of the potential outcomes is approximately $s$ sparse). We establish identification for all $N \\times 2^p$ parameters despite unobserved confounding. We propose an estimation procedure, Synthetic Combinations, and establish finite-sample consistency under precise conditions on the observation pattern. We show that Synthetic Combinations is able to consistently estimate unit-specific potential outcomes given a total of $\\text{poly}(r) \\times \\left( N + s^2p\\right)$ observations. In comparison, previous methods that do not exploit structure across both units and combinations have poorer sample complexity scaling as $\\min(N \\times s^2p, \\ \\ r \\times (N + 2^p))$",
    "keywords": [],
    "checked": true,
    "id": "f66b41d61c46d71a67eb8d34ae2572f888fcfb29",
    "semantic_title": "synthetic combinations: a causal inference framework for combinatorial interventions",
    "citation_count": 3,
    "authors": [
      "Abhineet Agarwal",
      "Anish Agarwal",
      "Suhas Vijaykumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d226fb8fbd6ee6ec70d0427f1319707-Abstract-Conference.html": {
    "title": "PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised Image Denoising",
    "volume": "main",
    "abstract": "Although supervised image denoising networks have shown remarkable performance on synthesized noisy images, they often fail in practice due to the difference between real and synthesized noise. Since clean-noisy image pairs from the real world are extremely costly to gather, self-supervised learning, which utilizes noisy input itself as a target, has been studied. To prevent a self-supervised denoising model from learning identical mapping, each output pixel should not be influenced by its corresponding input pixel; This requirement is known as J-invariance. Blind-spot networks (BSNs) have been a prevalent choice to ensure J-invariance in self-supervised image denoising. However, constructing variations of BSNs by injecting additional operations such as downsampling can expose blinded information, thereby violating J-invariance. Consequently, convolutions designed specifically for BSNs have been allowed only, limiting architectural flexibility. To overcome this limitation, we propose PUCA, a novel J-invariant U-Net architecture, for self-supervised denoising. PUCA leverages patch-unshuffle/shuffle to dramatically expand receptive fields while maintaining J-invariance and dilated attention blocks (DABs) for global context incorporation. Experimental results demonstrate that PUCA achieves state-of-the-art performance, outperforming existing methods in self-supervised image denoising",
    "keywords": [],
    "checked": true,
    "id": "36f854492802182821d1ee8b05a070785f06d6ab",
    "semantic_title": "puca: patch-unshuffle and channel attention for enhanced self-supervised image denoising",
    "citation_count": 0,
    "authors": [
      "Hyemi Jang",
      "Junsung Park",
      "Dahuin Jung",
      "Jaihyun Lew",
      "Ho Bae",
      "Sungroh Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d27d607586984908900eaa8ce19c96c-Abstract-Conference.html": {
    "title": "Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models",
    "volume": "main",
    "abstract": "Novelty detection is a fundamental task of machine learning which aims to detect abnormal (i.e. out-of-distribution (OOD)) samples. Since diffusion models have recently emerged as the de facto standard generative framework with surprising generation results, novelty detection via diffusion models has also gained much attention. Recent methods have mainly utilized the reconstruction property of in-distribution samples. However, they often suffer from detecting OOD samples that share similar background information to the in-distribution data. Based on our observation that diffusion models can project any sample to an in-distribution sample with similar background information, we propose Projection Regret (PR), an efficient novelty detection method that mitigates the bias of non-semantic information. To be specific, PR computes the perceptual distance between the test image and its diffusion-based projection to detect abnormality. Since the perceptual distance often fails to capture semantic changes when the background information is dominant, we cancel out the background bias by comparing it against recursive projections. Extensive experiments demonstrate that PR outperforms the prior art of generative-model-based novelty detection methods by a significant margin",
    "keywords": [],
    "checked": true,
    "id": "fe9437b92b098a6835f772e7f80fd58c7b7ab938",
    "semantic_title": "projection regret: reducing background bias for novelty detection via diffusion models",
    "citation_count": 0,
    "authors": [
      "Sungik Choi",
      "Hankook Lee",
      "Honglak Lee",
      "Moontae Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d4c0a618d0acd7921493e4f30395c22-Abstract-Conference.html": {
    "title": "Versatile Energy-Based Probabilistic Models for High Energy Physics",
    "volume": "main",
    "abstract": "As a classical generative modeling approach, energy-based models have the natural advantage of flexibility in the form of the energy function. Recently, energy-based models have achieved great success in modeling high-dimensional data in computer vision and natural language processing. In line with these advancements, we build a multi-purpose energy-based probabilistic model for High Energy Physics events at the Large Hadron Collider. This framework builds on a powerful generative model and describes higher-order inter-particle interactions. It suits different encoding architectures and builds on implicit generation. As for applicational aspects, it can serve as a powerful parameterized event generator for physics simulation, a generic anomalous signal detector free from spurious correlations, and an augmented event classifier for particle identification",
    "keywords": [],
    "checked": true,
    "id": "ba4ba8b18da2a2e0f1932545ffd892165ecf2a68",
    "semantic_title": "versatile energy-based probabilistic models for high energy physics",
    "citation_count": 0,
    "authors": [
      "Taoli Cheng",
      "Aaron C. Courville"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d57795f0e263aa69577f1bbceade46b-Abstract-Conference.html": {
    "title": "User-Level Differential Privacy With Few Examples Per User",
    "volume": "main",
    "abstract": "Previous work on user-level differential privacy (DP) [Ghazi et al. NeurIPS 2021, Bun et al. STOC 2023] obtained generic algorithms that work for various learning tasks. However, their focus was on the *example-rich* regime, where the users have so many examples that each user could themselves solve the problem. In this work we consider the *example-scarce* regime, where each user has only a few examples, and obtain the following results:* For approximate-DP, we give a generic transformation of any item-level DP algorithm to a user-level DP algorithm. Roughly speaking, the latter gives a (multiplicative) savings of $O_{\\varepsilon,\\delta}(\\sqrt{m})$ in terms of the number of users required for achieving the same utility, where $m$ is the number of examples per user. This algorithm, while recovering most known bounds for specific problems, also gives new bounds, e.g., for PAC learning. * For pure-DP, we present a simple technique for adapting the exponential mechanism [McSherry & Talwar, FOCS 2007] to the user-level setting. This gives new bounds for a variety of tasks, such as private PAC learning, hypothesis selection, and distribution learning. For some of these problems, we show that our bounds are near-optimal",
    "keywords": [],
    "checked": true,
    "id": "e126f66778f5a790378d693bf4138ee0714d92f6",
    "semantic_title": "user-level differential privacy with few examples per user",
    "citation_count": 1,
    "authors": [
      "Badih Ghazi",
      "Pritish Kamath",
      "Ravi Kumar",
      "Pasin Manurangsi",
      "Raghu Meka",
      "Chiyuan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d7259031023c5aa463187c4a31c95c8-Abstract-Conference.html": {
    "title": "Neural Lighting Simulation for Urban Scenes",
    "volume": "main",
    "abstract": "Different outdoor illumination conditions drastically alter the appearance of urban scenes, and they can harm the performance of image-based robot perception systems if not seen during training. Camera simulation provides a cost-effective solution to create a large dataset of images captured under different lighting conditions. Towards this goal, we propose LightSim, a neural lighting camera simulation system that enables diverse, realistic, and controllable data generation. LightSim automatically builds lighting-aware digital twins at scale from collected raw sensor data and decomposes the scene into dynamic actors and static background with accurate geometry, appearance, and estimated scene lighting. These digital twins enable actor insertion, modification, removal, and rendering from a new viewpoint, all in a lighting-aware manner. LightSim then combines physically-based and learnable deferred rendering to perform realistic relighting of modified scenes, such as altering the sun location and modifying the shadows or changing the sun brightness, producing spatially- and temporally-consistent camera videos. Our experiments show that LightSim generates more realistic relighting results than prior work. Importantly, training perception models on data generated by LightSim can significantly improve their performance. Our project page is available at https://waabi.ai/lightsim/",
    "keywords": [],
    "checked": false,
    "id": "85c60cb1589528f72d3dc5b5c82722fbe5d480f6",
    "semantic_title": "lightsim: neural lighting simulation for urban scenes",
    "citation_count": 0,
    "authors": [
      "Ava Pun",
      "Gary Sun",
      "Jingkang Wang",
      "Yun Chen",
      "Ze Yang",
      "Sivabalan Manivasagam",
      "Wei-Chiu Ma",
      "Raquel Urtasun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d77c6dcc7f143aa2154e7f4d5e22d68-Abstract-Conference.html": {
    "title": "Learning to Compress Prompts with Gist Tokens",
    "volume": "main",
    "abstract": "Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of \"gist\" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality",
    "keywords": [],
    "checked": true,
    "id": "b9870e130f61ff900fe00dbcc5782c9b31773d32",
    "semantic_title": "learning to compress prompts with gist tokens",
    "citation_count": 47,
    "authors": [
      "Jesse Mu",
      "Xiang Li",
      "Noah Goodman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d8f7945cd7f4446cb05a390d4c00558-Abstract-Conference.html": {
    "title": "A Heavy-Tailed Algebra for Probabilistic Programming",
    "volume": "main",
    "abstract": "Despite the successes of probabilistic models based on passing noise through neural networks, recent work has identified that such methods often fail to capture tail behavior accurately---unless the tails of the base distribution are appropriately calibrated. To overcome this deficiency, we propose a systematic approach for analyzing the tails of random variables, and we illustrate how this approach can be used during the static analysis (before drawing samples) pass of a probabilistic programming language (PPL) compiler. To characterize how the tails change under various operations, we develop an algebra which acts on a three-parameter family of tail asymptotics and which is based on the generalized Gamma distribution. Our algebraic operations are closed under addition and multiplication; they are capable of distinguishing sub-Gaussians with differing scales; and they handle ratios sufficiently well to reproduce the tails of most important statistical distributions directly from their definitions. Our empirical results confirm that inference algorithms that leverage our heavy-tailed algebra attain superior performance across a number of density modeling and variational inference (VI) tasks",
    "keywords": [],
    "checked": true,
    "id": "8dd7e3600ba1a5480c50c38a4ee50cbdea886fd9",
    "semantic_title": "a heavy-tailed algebra for probabilistic programming",
    "citation_count": 0,
    "authors": [
      "Feynman T. Liang",
      "Liam Hodgkinson",
      "Michael W. Mahoney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3da292ced54290c19fc55d9dba3da793-Abstract-Conference.html": {
    "title": "AIMS: All-Inclusive Multi-Level Segmentation for Anything",
    "volume": "main",
    "abstract": "Despite the progress of image segmentation for accurate visual entity segmentation, completing the diverse requirements of image editing applications for different-level region-of-interest selections remains unsolved. In this paper, we propose a new task, All-Inclusive Multi-Level Segmentation (AIMS), which segments visual regions into three levels: part, entity, and relation (two entities with some semantic relationships). We also build a unified AIMS model through multi-dataset multi-task training to address the two major challenges of annotation inconsistency and task correlation. Specifically, we propose task complementarity, association, and prompt mask encoder for three-level predictions. Extensive experiments demonstrate the effectiveness and generalization capacity of our method compared to other state-of-the-art methods on a single dataset or the concurrent work on segment anything. We will make our code and training model publicly available",
    "keywords": [],
    "checked": false,
    "id": "e7de0864c2bef274471399782f5bd35498853895",
    "semantic_title": "aims: all-inclusive multi-level segmentation",
    "citation_count": 6,
    "authors": [
      "Lu Qi",
      "Jason Kuen",
      "Weidong Guo",
      "Jiuxiang Gu",
      "Zhe Lin",
      "Bo Du",
      "Yu Xu",
      "Ming-Hsuan Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3da8e709fa1a7d9e23bee89d3c25b5b4-Abstract-Conference.html": {
    "title": "Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms",
    "volume": "main",
    "abstract": "Many policy-based reinforcement learning (RL) algorithms can be viewed as instantiations of approximate policy iteration (PI), i.e., where policy improvement and policy evaluation are both performed approximately. In applications where the average reward objective is the meaningful performance metric, often discounted reward formulations are used with the discount factor being close to $1,$ which is equivalent to making the expected horizon very large. However, the corresponding theoretical bounds for error performance scale with the square of the horizon. Thus, even after dividing the total reward by the length of the horizon, the corresponding performance bounds for average reward problems go to infinity. Therefore, an open problem has been to obtain meaningful performance bounds for approximate PI and RL algorithms for the average-reward setting. In this paper, we solve this open problem by obtaining the first non-trivial finite time error bounds for average-reward MDPs which go to zero in the limit as policy evaluation and policy improvement errors go to zero",
    "keywords": [],
    "checked": true,
    "id": "36f23ead10acc50f47824fd9b082118c98eb0263",
    "semantic_title": "performance bounds for policy-based average reward reinforcement learning algorithms",
    "citation_count": 1,
    "authors": [
      "Yashaswini Murthy",
      "Mehrdad Moharrami",
      "R. Srikant"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3df38ca67befaed9c03b95ffee07d9f8-Abstract-Conference.html": {
    "title": "Understanding Few-Shot Learning: Measuring Task Relatedness and Adaptation Difficulty via Attributes",
    "volume": "main",
    "abstract": "Few-shot learning (FSL) aims to learn novel tasks with very few labeled samples by leveraging experience from \\emph{related} training tasks. In this paper, we try to understand FSL by exploring two key questions: (1) How to quantify the relationship between \\emph{ training} and \\emph{novel} tasks? (2) How does the relationship affect the \\emph{adaptation difficulty} on novel tasks for different models? To answer the first question, we propose Task Attribute Distance (TAD) as a metric to quantify the task relatedness via attributes. Unlike other metrics, TAD is independent of models, making it applicable to different FSL models. To address the second question, we utilize TAD metric to establish a theoretical connection between task relatedness and task adaptation difficulty. By deriving the generalization error bound on a novel task, we discover how TAD measures the adaptation difficulty on novel tasks for different models. To validate our theoretical results, we conduct experiments on three benchmarks. Our experimental results confirm that TAD metric effectively quantifies the task relatedness and reflects the adaptation difficulty on novel tasks for various FSL methods, even if some of them do not learn attributes explicitly or human-annotated attributes are not provided. Our code is available at \\href{https://github.com/hu-my/TaskAttributeDistance}{https://github.com/hu-my/TaskAttributeDistance}",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minyang Hu",
      "Hong Chang",
      "Zong Guo",
      "Bingpeng MA",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3df874367ce2c43891aab1ab23ae6959-Abstract-Conference.html": {
    "title": "Locally Invariant Explanations: Towards Stable and Unidirectional Explanations through Local Invariant Learning",
    "volume": "main",
    "abstract": "Locally interpretable model agnostic explanations (LIME) method is one of the most popular methods used to explain black-box models at a per example level. Although many variants have been proposed, few provide a simple way to produce high fidelity explanations that are also stable and intuitive. In this work, we provide a novel perspective by proposing a model agnostic local explanation method inspired by the invariant risk minimization (IRM) principle -- originally proposed for (global) out-of-distribution generalization -- to provide such high fidelity explanations that are also stable and unidirectional across nearby examples. Our method is based on a game theoretic formulation where we theoretically show that our approach has a strong tendency to eliminate features where the gradient of the black-box function abruptly changes sign in the locality of the example we want to explain, while in other cases it is more careful and will choose a more conservative (feature) attribution, a behavior which can be highly desirable for recourse. Empirically, we show on tabular, image and text data that the quality of our explanations with neighborhoods formed using random perturbations are much better than LIME and in some cases even comparable to other methods that use realistic neighbors sampled from the data manifold. This is desirable given that learning a manifold to either create realistic neighbors or to project explanations is typically expensive or may even be impossible. Moreover, our algorithm is simple and efficient to train, and can ascertain stable input features for local decisions of a black-box without access to side information such as a (partial) causal graph as has been seen in some recent works",
    "keywords": [],
    "checked": true,
    "id": "d11fd8a361cd1a438ce40fef2abfc814301b2141",
    "semantic_title": "locally invariant explanations: towards stable and unidirectional explanations through local invariant learning",
    "citation_count": 1,
    "authors": [
      "Amit Dhurandhar",
      "Karthikeyan Natesan Ramamurthy",
      "Kartik Ahuja",
      "Vijay Arya"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3e0b96206965f5f05b0b4550c0e73ff0-Abstract-Conference.html": {
    "title": "Quantification of Uncertainty with Adversarial Models",
    "volume": "main",
    "abstract": "Quantifying uncertainty is important for actionable predictions in real-world applications. A crucial part of predictive uncertainty quantification is the estimation of epistemic uncertainty, which is defined as an integral of the product between a divergence function and the posterior. Current methods such as Deep Ensembles or MC dropout underperform at estimating the epistemic uncertainty, since they primarily consider the posterior when sampling models. We suggest Quantification of Uncertainty with Adversarial Models (QUAM) to better estimate the epistemic uncertainty. QUAM identifies regions where the whole product under the integral is large, not just the posterior. Consequently, QUAM has lower approximation error of the epistemic uncertainty compared to previous methods. Models for which the product is large correspond to adversarial models (not adversarial examples!). Adversarial models have both a high posterior as well as a high divergence between their predictions and that of a reference model. Our experiments show that QUAM excels in capturing epistemic uncertainty for deep learning models and outperforms previous methods on challenging tasks in the vision domain",
    "keywords": [],
    "checked": true,
    "id": "877fa88cdd5dddaa5007dd196d67d5ab170ced61",
    "semantic_title": "quantification of uncertainty with adversarial models",
    "citation_count": 1,
    "authors": [
      "Kajetan Schweighofer",
      "Lukas Aichberger",
      "Mykyta Ielanskyi",
      "Günter Klambauer",
      "Sepp Hochreiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3e22abb329d44080460b0eb11bf21da1-Abstract-Conference.html": {
    "title": "NeuroGF: A Neural Representation for Fast Geodesic Distance and Path Queries",
    "volume": "main",
    "abstract": "Geodesics play a critical role in many geometry processing applications. Traditional algorithms for computing geodesics on 3D mesh models are often inefficient and slow, which make them impractical for scenarios requiring extensive querying of arbitrary point-to-point geodesics. Recently, deep implicit functions have gained popularity for 3D geometry representation, yet there is still no research on neural implicit representation of geodesics. To bridge this gap, we make the first attempt to represent geodesics using implicit learning frameworks. Specifically, we propose neural geodesic field (NeuroGF), which can be learned to encode all-pairs geodesics of a given 3D mesh model, enabling to efficiently and accurately answer queries of arbitrary point-to-point geodesic distances and paths. Evaluations on common 3D object models and real-captured scene-level meshes demonstrate our exceptional performances in terms of representation accuracy and querying efficiency. Besides, NeuroGF also provides a convenient way of jointly encoding both 3D geometry and geodesics in a unified representation. Moreover, the working mode of per-model overfitting is further extended to generalizable learning frameworks that can work on various input formats such as unstructured point clouds, which also show satisfactory performances for unseen shapes and categories. Our code and data are available at https://github.com/keeganhk/NeuroGF",
    "keywords": [],
    "checked": true,
    "id": "081ca297eaf250268ff4ff0852445145f3d81b23",
    "semantic_title": "neurogf: a neural representation for fast geodesic distance and path queries",
    "citation_count": 2,
    "authors": [
      "Qijian Zhang",
      "Junhui Hou",
      "Yohanes Adikusuma",
      "Wenping Wang",
      "Ying He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3e32af2df2cd13dfbcbe6e8d38111068-Abstract-Conference.html": {
    "title": "A Trichotomy for Transductive Online Learning",
    "volume": "main",
    "abstract": "We present new upper and lower bounds on the number of learner mistakes in the `transductive' online learning setting of Ben-David, Kushilevitz and Mansour (1997). This setting is similar to standard online learning, except that the adversary fixes a sequence of instances $x_1,\\dots,x_n$ to be labeled at the start of the game, and this sequence is known to the learner. Qualitatively, we prove a \\emph{trichotomy}, stating that the minimal number of mistakes made by the learner as $n$ grows can take only one of precisely three possible values: $n$, $\\Theta\\left(\\log (n)\\right)$, or $\\Theta(1)$. Furthermore, this behavior is determined by a combination of the VC dimension and the Littlestone dimension. Quantitatively, we show a variety of bounds relating the number of mistakes to well-known combinatorial dimensions. In particular, we improve the known lower bound on the constant in the $\\Theta(1)$ case from $\\Omega\\left(\\sqrt{\\log(d)}\\right)$ to $\\Omega(\\log(d))$ where $d$ is the Littlestone dimension. Finally, we extend our results to cover multiclass classification and the agnostic setting",
    "keywords": [],
    "checked": true,
    "id": "4f02c28bfdf43a8dd474cb8240ec9819f50ab00c",
    "semantic_title": "a trichotomy for transductive online learning",
    "citation_count": 0,
    "authors": [
      "Steve Hanneke",
      "Shay Moran",
      "Jonathan Shafer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3e53d82a1113e3d240059a9195668edc-Abstract-Conference.html": {
    "title": "Evolutionary Neural Architecture Search for Transformer in Knowledge Tracing",
    "volume": "main",
    "abstract": "Knowledge tracing (KT) aims to trace students' knowledge states by predicting whether students answer correctly on exercises. Despite the excellent performance of existing Transformer-based KT approaches, they are criticized for the manually selected input features for fusion and the defect of single global context modelling to directly capture students' forgetting behavior in KT, when the related records are distant from the current record in terms of time. To address the issues, this paper first considers adding convolution operations to the Transformer to enhance its local context modelling ability used for students' forgetting behavior, then proposes an evolutionary neural architecture search approach to automate the input feature selection and automatically determine where to apply which operation for achieving the balancing of the local/global context modelling. In the search space, the original global path containing the attention module in Transformer is replaced with the sum of a global path and a local path that could contain different convolutions, and the selection of input features is also considered. To search the best architecture, we employ an effective evolutionary algorithm to explore the search space and also suggest a search space reduction strategy to accelerate the convergence of the algorithm. Experimental results on the two largest and most challenging education datasets demonstrate the effectiveness of the architecture found by the proposed approach",
    "keywords": [],
    "checked": true,
    "id": "66efd58b2cbd5dd3b727183a6fe3805ca54f3482",
    "semantic_title": "evolutionary neural architecture search for transformer in knowledge tracing",
    "citation_count": 0,
    "authors": [
      "Shangshang Yang",
      "Xiaoshan Yu",
      "Ye Tian",
      "Xueming Yan",
      "Haiping Ma",
      "Xingyi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3e592c571de69a43d7a870ea89c7e33a-Abstract-Conference.html": {
    "title": "Learning threshold neurons via edge of stability",
    "volume": "main",
    "abstract": "Existing analyses of neural network training often operate under the unrealistic assumption of an extremely small learning rate. This lies in stark contrast to practical wisdom and empirical studies, such as the work of J. Cohen et al. (ICLR 2021), which exhibit startling new phenomena (the \"edge of stability\"' or \"unstable convergence\") and potential benefits for generalization in the large learning rate regime. Despite a flurry of recent works on this topic, however, the latter effect is still poorly understood. In this paper, we take a step towards understanding genuinely non-convex training dynamics with large learning rates by performing a detailed analysis of gradient descent for simplified models of two-layer neural networks. For these models, we provably establish the edge of stability phenomenon and discover a sharp phase transition for the step size below which the neural network fails to learn ``threshold-like'' neurons (i.e., neurons with a non-zero first-layer bias). This elucidates one possible mechanism by which the edge of stability can in fact lead to better generalization, as threshold neurons are basic building blocks with useful inductive bias for many tasks",
    "keywords": [],
    "checked": false,
    "id": "273345a0de931b25e62503d0b51c13e4868aab81",
    "semantic_title": "learning threshold neurons via the \"edge of stability",
    "citation_count": 18,
    "authors": [
      "Kwangjun Ahn",
      "Sebastien Bubeck",
      "Sinho Chewi",
      "Yin Tat Lee",
      "Felipe Suarez",
      "Yi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3e8d9bf1dd1eb9d3d9d500fb3543c87b-Abstract-Conference.html": {
    "title": "$k$-Means Clustering with Distance-Based Privacy",
    "volume": "main",
    "abstract": "In this paper, we initiate the study of Euclidean clustering with Distance-based privacy. Distance-based privacy is motivated by the fact that it is often only needed to protect the privacy of exact, rather than approximate, locations. We provide constant-approximate algorithms for $k$-means and $k$-median clustering, with additive error depending only on the attacker's precision bound $\\rho$, rather than the radius $\\Lambda$ of the space. In addition, we empirically demonstrate that our algorithm performs significantly better than previous differentially private clustering algorithms, as well as naive distance-based private clustering baselines",
    "keywords": [],
    "checked": false,
    "id": "f8dfc8a4ce50d4a2be8d7b2bda75897a381cc350",
    "semantic_title": "fuzzy k-means clustering with discriminative embedding",
    "citation_count": 27,
    "authors": [
      "Alessandro Epasto",
      "Vahab Mirrokni",
      "Shyam Narayanan",
      "Peilin Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3eaad2a0b62b5ed7a2e66c2188bb1449-Abstract-Conference.html": {
    "title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models",
    "volume": "main",
    "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/",
    "keywords": [],
    "checked": true,
    "id": "a5d26eb03dd52a3d588d5a8057091928b56538f9",
    "semantic_title": "styletts 2: towards human-level text-to-speech through style diffusion and adversarial training with large speech language models",
    "citation_count": 6,
    "authors": [
      "Yinghao Aaron Li",
      "Cong Han",
      "Vinay Raghavan",
      "Gavin Mischler",
      "Nima Mesgarani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3eb7ca52e8207697361b2c0fb3926511-Abstract-Conference.html": {
    "title": "Large Language Models Are Zero-Shot Time Series Forecasters",
    "volume": "main",
    "abstract": "By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF",
    "keywords": [],
    "checked": true,
    "id": "123acfbccca0460171b6b06a4012dbb991cde55b",
    "semantic_title": "large language models are zero-shot time series forecasters",
    "citation_count": 29,
    "authors": [
      "Nate Gruver",
      "Marc Finzi",
      "Shikai Qiu",
      "Andrew G. Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ec077b4af90f2556b517b556e186f64-Abstract-Conference.html": {
    "title": "Learning Mixtures of Gaussians Using the DDPM Objective",
    "volume": "main",
    "abstract": "Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation.Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed. In this work, we give the first provably efficient results for one of the most fundamental distribution families, Gaussian mixture models.We prove that GD on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings:1. We show GD with random initialization learns mixtures of two spherical Gaussians in $d$ dimensions with $1/\\text{poly}(d)$-separated centers.2. We show GD with a warm start learns mixtures of $K$ spherical Gaussians with $\\Omega(\\sqrt{\\log(\\min(K,d))})$-separated centers.A key ingredient in our proofs is a new connection between score-based methods and two other approaches to distribution learning, EM and spectral methods",
    "keywords": [],
    "checked": true,
    "id": "60eef3b81e50bbd1582bb24fc173f7d93552bb53",
    "semantic_title": "learning mixtures of gaussians using the ddpm objective",
    "citation_count": 5,
    "authors": [
      "Kulin Shah",
      "Sitan Chen",
      "Adam Klivans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ec6c6fc9065aa57785eb05dffe7c3db-Abstract-Conference.html": {
    "title": "Graph Convolutional Kernel Machine versus Graph Convolutional Networks",
    "volume": "main",
    "abstract": "Graph convolutional networks (GCN) with one or two hidden layers have been widely used in handling graph data that are prevalent in various disciplines. Many studies showed that the gain of making GCNs deeper is tiny or even negative. This implies that the complexity of graph data is often limited and shallow models are often sufficient to extract expressive features for various tasks such as node classification. Therefore, in this work, we present a framework called graph convolutional kernel machine (GCKM) for graph-based machine learning. GCKMs are built upon kernel functions integrated with graph convolution. An example is the graph convolutional kernel support vector machine (GCKSVM) for node classification, for which we analyze the generalization error bound and discuss the impact of the graph structure. Compared to GCNs, GCKMs require much less effort in architecture design, hyperparameter tuning, and optimization. More importantly, GCKMs are guaranteed to obtain globally optimal solutions and have strong generalization ability and high interpretability. GCKMs are composable, can be extended to large-scale data, and are applicable to various tasks (e.g., node or graph classification, clustering, feature extraction, dimensionality reduction). The numerical results on benchmark datasets show that, besides the aforementioned advantages, GCKMs have at least competitive accuracy compared to GCNs",
    "keywords": [],
    "checked": false,
    "id": "9e19f942e1babecf5526b3969f3f18747993a727",
    "semantic_title": "evolution-driven randomized graph convolutional networks",
    "citation_count": 7,
    "authors": [
      "Zhihao Wu",
      "Zhao Zhang",
      "Jicong Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ec90b0eec9c1151c152ba865713f184-Abstract-Conference.html": {
    "title": "First Order Stochastic Optimization with Oblivious Noise",
    "volume": "main",
    "abstract": "We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.In our setting, in addition to random observation noise, the stochastic gradient may be subject to independent \\emph{oblivious noise}, which may not have bounded moments and is not necessarily centered. Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ at $x$, which returns a vector $\\nabla f(\\gamma, x) + \\xi$, where $\\gamma$ is the bounded variance observation noise and $\\xi$ is the oblivious noise that is independent of $\\gamma$ and $x$. The only assumption we make on the oblivious noise $\\xi$ is that $\\Pr[\\xi = 0] \\ge \\alpha$, for some $\\alpha \\in (0, 1)$.In this setting, it is not information-theoretically possible to recover a single solution close to the target when the fraction of inliers $\\alpha$ is less than $1/2$. Our main result is an efficient {\\em list-decodable} learner that recovers a small list of candidates at least one of which is close to the true solution. On the other hand, if $\\alpha = 1-\\epsilon$, where $0< \\epsilon < 1/2$ is sufficiently smallconstant, the algorithm recovers a single solution.Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, which may be of independent interest",
    "keywords": [],
    "checked": true,
    "id": "cc9a61e5e0993205b8fa02a5ed9b84dd4dc8f814",
    "semantic_title": "first order stochastic optimization with oblivious noise",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Sushrut Karmalkar",
      "Jong Ho Park",
      "Christos Tzamos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3eceb70f47690051d6769739fbf6294b-Abstract-Conference.html": {
    "title": "A Theory of Link Prediction via Relational Weisfeiler-Leman on Knowledge Graphs",
    "volume": "main",
    "abstract": "Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically",
    "keywords": [],
    "checked": false,
    "id": "3f4dff0c50a2edc4d38973669f8459449f114e28",
    "semantic_title": "a theory of link prediction via relational weisfeiler-leman",
    "citation_count": 8,
    "authors": [
      "Xingyue Huang",
      "Miguel Romero",
      "Ismail Ceylan",
      "Pablo Barceló"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3eec5006051d9544e717067de3220198-Abstract-Conference.html": {
    "title": "Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization",
    "volume": "main",
    "abstract": "We present a novel method for tuning the regularization hyper-parameter, $\\lambda$, of a ridge regression that is faster to compute than leave-one-out cross-validation (LOOCV) while yielding estimates of the regression parameters of equal, or particularly in the setting of sparse covariates, superior quality to those obtained by minimising the LOOCV risk. The LOOCV risk can suffer from multiple and bad local minima for finite $n$ and thus requires the specification of a set of candidate $\\lambda$, which can fail to provide good solutions. In contrast, we show that the proposed method is guaranteed to find a unique optimal solution for large enough $n$, under relatively mild conditions, without requiring the specification of any difficult to determine hyper-parameters. This is based on a Bayesian formulation of ridge regression that we prove to have a unimodal posterior for large enough $n$, allowing for both the optimal $\\lambda$ and the regression coefficients to be jointly learned within an iterative expectation maximization (EM) procedure. Importantly, we show that by utilizing an appropriate preprocessing step, a single iteration of the main EM loop can be implemented in $O(\\min(n, p))$ operations, for input data with $n$ rows and $p$ columns. In contrast, evaluating a single value of $\\lambda$ using fast LOOCV costs $O(n \\min(n, p))$ operations when using the same preprocessing. This advantage amounts to an asymptotic improvement of a factor of $l$ for $l$ candidate values for $\\lambda$ (in the regime $q, p \\in O(\\sqrt{n})$ where $q$ is the number of regression targets)",
    "keywords": [],
    "checked": true,
    "id": "0c74668384653c14cb58eb161af43155677451a3",
    "semantic_title": "bayes beats cross validation: efficient and accurate ridge regression via expectation maximization",
    "citation_count": 1,
    "authors": [
      "Shu Yu Tew",
      "Mario Boley",
      "Daniel Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ef61f7e4afacf9a2c5b71c726172b86-Abstract-Conference.html": {
    "title": "Segment Everything Everywhere All at Once",
    "volume": "main",
    "abstract": "In this work, we present SEEM, a promotable and interactive model for segmenting everything everywhere all at once in an image. In SEEM, we propose a novel and versatile decoding mechanism that enables diverse prompting for all types of segmentation tasks, aiming at a universal interface that behaves like large language models (LLMs). More specifically, SEEM is designed with four desiderata:i) Versatility. We introduce a new visual prompt to unify different spatial queries including points, boxes, scribbles, and masks, which can further generalize to a different referring image; ii) Compositionality. We learn a joint visual-semantic space between text and visual prompts, which facilitates the dynamic composition of two prompt types required for various segmentation tasks, as shown in Fig. 1;iii) Interactivity. We further incorporate learnable memory prompts into the decoder to retain segmentation history through mask-guided cross-attention from the decoder to image features; iv) Semantic awareness. We use a text encoder to encode text queries and mask labels into the same semantic space for open-vocabulary segmentation. We conduct a comprehensive empirical study to validate the effectiveness of SEEM across diverse segmentation tasks. The results demonstrate that SEEM exhibits robust generalizing to unseen user intents as it learns to compose prompts of different types in a unified representation space. Our approach achieves competitive performance on interactive segmentation, generic segmentation, referring segmentation, and video object segmentation on 9 datasets with minimum 1/100 supervision in a single set of weights",
    "keywords": [],
    "checked": true,
    "id": "0819c1e60c13b9797f937282d06b54d252d9d6ec",
    "semantic_title": "segment everything everywhere all at once",
    "citation_count": 117,
    "authors": [
      "Xueyan Zou",
      "Jianwei Yang",
      "Hao Zhang",
      "Feng Li",
      "Linjie Li",
      "Jianfeng Wang",
      "Lijuan Wang",
      "Jianfeng Gao",
      "Yong Jae Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3efb4bdc6bfe13e1ff95b4407c37961d-Abstract-Conference.html": {
    "title": "PUe: Biased Positive-Unlabeled Learning Enhancement by Causal Inference",
    "volume": "main",
    "abstract": "Positive-Unlabeled (PU) learning aims to achieve high-accuracy binary classification with limited labeled positive examples and numerous unlabeled ones. Existing cost-sensitive-based methods often rely on strong assumptions that examples with an observed positive label were selected entirely at random. In fact, the uneven distribution of labels is prevalent in real-world PU problems, indicating that most actual positive and unlabeled data are subject to selection bias. In this paper, we propose a PU learning enhancement (PUe) algorithm based on causal inference theory, which employs normalized propensity scores and normalized inverse probability weighting (NIPW) techniques to reconstruct the loss function, thus obtaining a consistent, unbiased estimate of the classifier and enhancing the model's performance. Moreover, we investigate and propose a method for estimating propensity scores in deep learning using regularization techniques when the labeling mechanism is unknown. Our experiments on three benchmark datasets demonstrate the proposed PUe algorithm significantly improves the accuracy of classifiers on non-uniform label distribution datasets compared to advanced cost-sensitive PU methods. Codes are available at https://github.com/huawei-noah/Noah-research/tree/master/PUe and https://gitee.com/mindspore/models/tree/master/research/cv/PUe",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xutao Wang",
      "Hanting Chen",
      "Tianyu Guo",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3f0739410e1c9c5da04fa10c1f3f86b6-Abstract-Conference.html": {
    "title": "Sparse Modular Activation for Efficient Sequence Modeling",
    "volume": "main",
    "abstract": "Recent hybrid models combining Linear State Space Models (SSMs) with self-attention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat",
    "keywords": [],
    "checked": true,
    "id": "d2d0371158803df93a249c9f7237ffd79b875816",
    "semantic_title": "sparse modular activation for efficient sequence modeling",
    "citation_count": 1,
    "authors": [
      "Liliang Ren",
      "Yang Liu",
      "Shuohang Wang",
      "Yichong Xu",
      "Chenguang Zhu",
      "Cheng Xiang  Zhai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3f1a5e8bfcc3005724d246abe454c1e5-Abstract-Conference.html": {
    "title": "Efficient Bayesian Learning Curve Extrapolation using Prior-Data Fitted Networks",
    "volume": "main",
    "abstract": "Learning curve extrapolation aims to predict model performance in later epochs of training, based on the performance in earlier epochs.In this work, we argue that, while the inherent uncertainty in the extrapolation of learning curves warrants a Bayesian approach, existing methods are (i) overly restrictive, and/or (ii) computationally expensive. We describe the first application of prior-data fitted neural networks (PFNs) in this context. A PFN is a transformer, pre-trained on data generated from a prior, to perform approximate Bayesian inference in a single forward pass. We propose LC-PFN, a PFN trained to extrapolate 10 million artificial right-censored learning curves generated from a parametric prior proposed in prior art using MCMC. We demonstrate that LC-PFN can approximate the posterior predictive distribution more accurately than MCMC, while being over 10 000 times faster. We also show that the same LC-PFN achieves competitive performance extrapolating a total of 20 000 real learning curves from four learning curve benchmarks (LCBench, NAS-Bench-201, Taskset, and PD1) that stem from training a wide range of model architectures (MLPs, CNNs, RNNs, and Transformers) on 53 different datasets with varying input modalities (tabular, image, text, and protein data). Finally, we investigate its potential in the context of model selection and find that a simple LC-PFN based predictive early stopping criterion obtains 2 - 6x speed-ups on 45 of these datasets, at virtually no overhead",
    "keywords": [],
    "checked": true,
    "id": "19edd41688c844d4ffd5230b417b99b5fd041cf1",
    "semantic_title": "efficient bayesian learning curve extrapolation using prior-data fitted networks",
    "citation_count": 3,
    "authors": [
      "Steven Adriaensen",
      "Herilalaina Rakotoarison",
      "Samuel Müller",
      "Frank Hutter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3f1b6e97a5eb3b10e6b0c99b022988eb-Abstract-Conference.html": {
    "title": "Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective",
    "volume": "main",
    "abstract": "Off-policy Learning to Rank (LTR) aims to optimize a ranker from data collected by a deployed logging policy. However, existing off-policy learning to rank methods often make strong assumptions about how users generate the click data, i.e., the click model, and hence need to tailor their methods specifically under different click models. In this paper, we unified the ranking process under general stochastic click models as a Markov Decision Process (MDP), and the optimal ranking could be learned with offline reinforcement learning (RL) directly. Building upon this, we leverage offline RL techniques for off-policy LTR and propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which could be easily applied to a wide range of click models. Through a dedicated formulation of the MDP, we show that offline RL algorithms can adapt to various click models without complex debiasing techniques and prior knowledge of the model. Results on various large-scale datasets demonstrate that CUOLR consistently outperforms the state-of-the-art off-policy learning to rank algorithms while maintaining consistency and robustness under different click models",
    "keywords": [],
    "checked": true,
    "id": "da25e0c1780d819a9053f38122bd9626dca34655",
    "semantic_title": "unified off-policy learning to rank: a reinforcement learning perspective",
    "citation_count": 1,
    "authors": [
      "Zeyu Zhang",
      "Yi Su",
      "Hui Yuan",
      "Yiran Wu",
      "Rishab Balasubramanian",
      "Qingyun Wu",
      "Huazheng Wang",
      "Mengdi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3f20f2b0315c72201e23512fdbd1ee91-Abstract-Conference.html": {
    "title": "Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints",
    "volume": "main",
    "abstract": "In safety-critical robotic tasks, potential failures must be reduced, and multiple constraints must be met, such as avoiding collisions, limiting energy consumption, and maintaining balance.Thus, applying safe reinforcement learning (RL) in such robotic tasks requires to handle multiple constraints and use risk-averse constraints rather than risk-neutral constraints.To this end, we propose a trust region-based safe RL algorithm for multiple constraints called a safe distributional actor-critic (SDAC).Our main contributions are as follows: 1) introducing a gradient integration method to manage infeasibility issues in multi-constrained problems, ensuring theoretical convergence, and 2) developing a TD($\\lambda$) target distribution to estimate risk-averse constraints with low biases. We evaluate SDAC through extensive experiments involving multi- and single-constrained robotic tasks.While maintaining high scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in multi-constrained tasks and 1.78 times fewer constraint violations in single-constrained tasks compared to safe RL baselines.Code is available at: https://github.com/rllab-snu/Safe-Distributional-Actor-Critic",
    "keywords": [],
    "checked": true,
    "id": "2260cf7df47436c682545cc75de67ae689ad645b",
    "semantic_title": "trust region-based safe distributional reinforcement learning for multiple constraints",
    "citation_count": 2,
    "authors": [
      "Dohyeong Kim",
      "Kyungjae Lee",
      "Songhwai Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3f226824426a4d6ae3d3efad8883fc53-Abstract-Conference.html": {
    "title": "The Contextual Lasso: Sparse Linear Models via Deep Neural Networks",
    "volume": "main",
    "abstract": "Sparse linear models are one of several core tools for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the network with a novel lasso regularizer in the form of a projection layer that maps the network's output onto the space of $\\ell_1$-constrained linear models. An extensive suite of experiments on real and synthetic data suggests that the learned models, which remain highly transparent, can be sparser than the regular lasso without sacrificing the predictive power of a standard deep neural network",
    "keywords": [],
    "checked": true,
    "id": "338cab5c432e3927092b003abf3db57077c5d8c5",
    "semantic_title": "the contextual lasso: sparse linear models via deep neural networks",
    "citation_count": 2,
    "authors": [
      "Ryan Thompson",
      "Amir Dezfouli",
      "robert kohn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3f52ab4322e967efd312c38a68d07f01-Abstract-Conference.html": {
    "title": "No Representation Rules Them All in Category Discovery",
    "volume": "main",
    "abstract": "In this paper we tackle the problem of Generalized Category Discovery (GCD). Specifically, given a dataset with labelled and unlabelled images, the task is to cluster all images in the unlabelled subset, whether or not they belong to the labelled categories. Our first contribution is to recognise that most existing GCD benchmarks only contain labels for a single clustering of the data, making it difficult to ascertain whether models are leveraging the available labels to solve the GCD task, or simply solving an unsupervised clustering problem. As such, we present a synthetic dataset, named 'Clevr-4', for category discovery. Clevr-4 contains four equally valid partitions of the data, i.e based on object 'shape', 'texture' or 'color' or 'count'. To solve the task, models are required to extrapolate the taxonomy specified by labelled set, rather than simply latch onto a single natural grouping of the data. We use this dataset to demonstrate the limitations of unsupervised clustering in the GCD setting, showing that even very strong unsupervised models fail on Clevr-4. We further use Clevr-4 to examine the weaknesses of existing GCD algorithms, and propose a new method which addresses these shortcomings, leveraging consistent findings from the representation learning literature to do so. Our simple solution, which is based on `Mean Teachers' and termed $\\mu$GCD, substantially outperforms implemented baselines on Clevr-4. Finally, when we transfer these findings to real data on the challenging Semantic Shift Benchmark suite, we find that $\\mu$GCD outperforms all prior work, setting a new state-of-the-art",
    "keywords": [],
    "checked": true,
    "id": "a12e9bec9e8d6cf782c92eb623537a761e94a819",
    "semantic_title": "no representation rules them all in category discovery",
    "citation_count": 1,
    "authors": [
      "Sagar Vaze",
      "Andrea Vedaldi",
      "Andrew Zisserman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3f8c7eb848ffec848f3ed2b7ca44915d-Abstract-Conference.html": {
    "title": "CS4ML: A general framework for active learning with arbitrary data based on Christoffel functions",
    "volume": "main",
    "abstract": "We introduce a general framework for active learning in regression problems. Our framework extends the standard setup by allowing for general types of data, rather than merely pointwise samples of the target function. This generalization covers many cases of practical interest, such as data acquired in transform domains (e.g., Fourier data), vector-valued data (e.g., gradient-augmented data), data acquired along continuous curves, and, multimodal data (i.e., combinations of different types of measurements). Our framework considers random sampling according to a finite number of sampling measures and arbitrary nonlinear approximation spaces (model classes). We introduce the concept of \\textit{generalized Christoffel functions} and show how these can be used to optimize the sampling measures. We prove that this leads to near-optimal sample complexity in various important cases. This paper focuses on applications in scientific computing, where active learning is often desirable, since it is usually expensive to generate data. We demonstrate the efficacy of our framework for gradient-augmented learning with polynomials, Magnetic Resonance Imaging (MRI) using generative models and adaptive sampling for solving PDEs using Physics-Informed Neural Networks (PINNs)",
    "keywords": [],
    "checked": true,
    "id": "a8ad7babcf93fdcd9271cf80a085955fd2e83679",
    "semantic_title": "cs4ml: a general framework for active learning with arbitrary data based on christoffel functions",
    "citation_count": 2,
    "authors": [
      "Juan M. Cardenas",
      "Ben Adcock",
      "Nick Dexter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3fa2d2b637122007845a2fbb7c21453b-Abstract-Conference.html": {
    "title": "Two Heads are Better Than One: A Simple Exploration Framework for Efficient Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Exploration strategy plays an important role in reinforcement learning, especially in sparse-reward tasks. In cooperative multi-agent reinforcement learning~(MARL), designing a suitable exploration strategy is much more challenging due to the large state space and the complex interaction among agents. Currently, mainstream exploration methods in MARL either contribute to exploring the unfamiliar states which are large and sparse, or measuring the interaction among agents with high computational costs. We found an interesting phenomenon that different kinds of exploration plays a different role in different MARL scenarios, and choosing a suitable one is often more effective than designing an exquisite algorithm. In this paper, we propose a exploration method that incorporate the \\underline{C}uri\\underline{O}sity-based and \\underline{IN}fluence-based exploration~(COIN) which is simple but effective in various situations. First, COIN measures the influence of each agent on the other agents based on mutual information theory and designs it as intrinsic rewards which are applied to each individual value function. Moreover, COIN computes the curiosity-based intrinsic rewards via prediction errors which are added to the extrinsic reward. For integrating the two kinds of intrinsic rewards, COIN utilizes a novel framework in which they complement each other and lead to a sufficient and effective exploration on cooperative MARL tasks. We perform extensive experiments on different challenging benchmarks, and results across different scenarios show the superiority of our method",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Li",
      "Kun Kuang",
      "Baoxiang Wang",
      "Xingchen Li",
      "Fei Wu",
      "Jun Xiao",
      "Long Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3fadcbd0437f4717723ff3f6f7216800-Abstract-Conference.html": {
    "title": "Cross-Scale MAE: A Tale of Multiscale Exploitation in Remote Sensing",
    "volume": "main",
    "abstract": "Remote sensing images present unique challenges to image analysis due to the extensive geographic coverage, hardware limitations, and misaligned multi-scale images. This paper revisits the classical multi-scale representation learning problem but under the general framework of self-supervised learning for remote sensing image understanding. We present Cross-Scale MAE, a self-supervised model built upon the Masked Auto-Encoder (MAE). During pre-training, Cross-Scale MAE employs scale augmentation techniques and enforces cross-scale consistency constraints through both contrastive and generative losses, to ensure consistent and meaningful representations well-suited for a wide range of downstream tasks. Further, our implementation leverages the xFormers library to accelerate network pre training on a single GPU while maintaining the quality of learned representations. Experimental evaluations demonstrate that Cross-Scale MAE exhibits superior performance compared to standard MAE and other state-of-the-art remote sensing MAE methods",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maofeng Tang",
      "Andrei Cozma",
      "Konstantinos Georgiou",
      "Hairong Qi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3fbf0c1ea0716c03dea93bb6be78dd6f-Abstract-Conference.html": {
    "title": "MotionGPT: Human Motion as a Foreign Language",
    "volume": "main",
    "abstract": "Though the advancement of pre-trained large language models unfolds, the exploration of building a unified model for language and other multimodal data, such as motion, remains challenging and untouched so far. Fortunately, human motion displays a semantic coupling akin to human language, often perceived as a form of body language. By fusing language data with large-scale motion models, motion-language pre-training that can enhance the performance of motion-related tasks becomes feasible. Driven by this insight, we propose MotionGPT, a unified, versatile, and user-friendly motion-language model to handle multiple motion-relevant tasks. Specifically, we employ the discrete vector quantization for human motion and transfer 3D motion into motion tokens, similar to the generation process of word tokens. Building upon this \"motion vocabulary\", we perform language modeling on both motion and text in a unified manner, treating human motion as a specific language. Moreover, inspired by prompt learning, we pre-train MotionGPT with a mixture of motion-language data and fine-tune it on prompt-based question-and-answer tasks. Extensive experiments demonstrate that MotionGPT achieves state-of-the-art performances on multiple motion tasks including text-driven motion generation, motion captioning, motion prediction, and motion in-between",
    "keywords": [],
    "checked": true,
    "id": "d212fa27f5868f0fd106e1a7bba908fd47da0816",
    "semantic_title": "motiongpt: human motion as a foreign language",
    "citation_count": 43,
    "authors": [
      "Biao Jiang",
      "Xin Chen",
      "Wen Liu",
      "Jingyi Yu",
      "Gang Yu",
      "Tao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3fcd0f8747f9217c6dbc45ed138b1fde-Abstract-Conference.html": {
    "title": "Model-Free Reinforcement Learning with the Decision-Estimation Coefficient",
    "volume": "main",
    "abstract": "We consider the problem of interactive decision making, encompassing structured bandits and reinforcementlearning with general function approximation. Recently, Foster et al. (2021) introduced theDecision-Estimation Coefficient, a measure of statistical complexity that lower bounds the optimal regret for interactive decisionmaking, as well as a meta-algorithm, Estimation-to-Decisions, which achieves upperbounds in terms of the same quantity. Estimation-to-Decisions is a reduction, which liftsalgorithms for (supervised) online estimation into algorithms fordecision making. In this paper, we show that by combining Estimation-to-Decisions witha specialized form of \"optimistic\" estimation introduced byZhang (2022), it is possible to obtain guaranteesthat improve upon those of Foster et al. (2021) byaccommodating more lenient notions of estimation error. We use this approach to derive regret bounds formodel-free reinforcement learning with value function approximation, and give structural results showing when it can and cannot help more generally",
    "keywords": [],
    "checked": true,
    "id": "b4698d868a31d48421d53b016f49677da53d5cba",
    "semantic_title": "model-free reinforcement learning with the decision-estimation coefficient",
    "citation_count": 6,
    "authors": [
      "Dylan J Foster",
      "Noah Golowich",
      "Jian Qian",
      "Alexander Rakhlin",
      "Ayush Sekhari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3fd9fe8ec6d7238bf71784797399bb61-Abstract-Conference.html": {
    "title": "FlowPG: Action-constrained Policy Gradient with Normalizing Flows",
    "volume": "main",
    "abstract": "Action-constrained reinforcement learning (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learned normalizing flow with the DDPG algorithm. By design, a well-trained normalizing flow will transform policy output into a valid action without requiring an optimization solver. Empirically, our approach results in significantly fewer constraint violations (upto an order-of-magnitude for several instances) and is multiple times faster on a variety of continuous control tasks",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Janaka Brahmanage",
      "Jiajing LING",
      "Akshat Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3feb8ed3c33c3310b45f80be7dfef707-Abstract-Conference.html": {
    "title": "Distributionally Robust Bayesian Optimization with $\\varphi$-divergences",
    "volume": "main",
    "abstract": "The study of robustness has received much attention due to its inevitability in data-driven settings where many systems face uncertainty. One such example of concern is Bayesian Optimization (BO), where uncertainty is multi-faceted, yet there only exists a limited number of works dedicated to this direction. In particular, there is the work of Kirschner et al., which bridges the existing literature of Distributionally Robust Optimization (DRO) by casting the BO problem from the lens of DRO. While this work is pioneering, it admittedly suffers from various practical shortcomings such as finite contexts assumptions, leaving behind the main question \\textit{Can one devise a computationally tractable algorithm for solving this DRO-BO problem}? In this work, we tackle this question to a large degree of generality by considering robustness against data-shift in $\\varphi$-divergences, which subsumes many popular choices, such as the $\\chi^2$-divergence, Total Variation, and the extant Kullback-Leibler (KL) divergence. We show that the DRO-BO problem in this setting is equivalent to a finite-dimensional optimization problem which, even in the continuous context setting, can be easily implemented with provable sublinear regret bounds. We then show experimentally that our method surpasses existing methods, attesting to the theoretical results",
    "keywords": [],
    "checked": false,
    "id": "bdb613a79fa8d7c4eb6f4d12f5ab866f1bccece5",
    "semantic_title": "distributionally robust bayesian optimization with φ-divergences",
    "citation_count": 6,
    "authors": [
      "Hisham Husain",
      "Vu Nguyen",
      "Anton van den Hengel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ff48dde82306fe8f26f3e51dd1054d7-Abstract-Conference.html": {
    "title": "Connected Superlevel Set in (Deep) Reinforcement Learning and its Application to Minimax Theorems",
    "volume": "main",
    "abstract": "The aim of this paper is to improve the understanding of the optimization landscape for policy optimization problems in reinforcement learning. Specifically, we show that the superlevel set of the objective function with respect to the policy parameter is always a connected set both in the tabular setting and under policies represented by a class of neural networks. In addition, we show that the optimization objective as a function of the policy parameter and reward satisfies a stronger \"equiconnectedness\" property. To our best knowledge, these are novel and previously unknown discoveries.We present an application of the connectedness of these superlevel sets to the derivation of minimax theorems for robust reinforcement learning. We show that any minimax optimization program which is convex on one side and is equiconnected on the other side observes the minimax equality (i.e. has a Nash equilibrium). We find that this exact structure is exhibited by an interesting class of robust reinforcement learning problems under an adversarial reward attack, and the validity of its minimax equality immediately follows. This is the first time such a result is established in the literature",
    "keywords": [],
    "checked": true,
    "id": "ac340a8e50c2276e5e4d0651bf671eae2de72c83",
    "semantic_title": "connected superlevel set in (deep) reinforcement learning and its application to minimax theorems",
    "citation_count": 1,
    "authors": [
      "Sihan Zeng",
      "Thinh Doan",
      "Justin Romberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/400a2e6a82520b690810b97fd67fcc4e-Abstract-Conference.html": {
    "title": "Towards Efficient and Accurate Winograd Convolution via Full Quantization",
    "volume": "main",
    "abstract": "The Winograd algorithm is an efficient convolution implementation, which performs calculations in the transformed domain. To further improve the computation efficiency, recent works propose to combine it with model quantization. Although Post-Training Quantization has the advantage of low computational cost and has been successfully applied in many other scenarios, a severe accuracy drop exists when utilizing it in Winograd convolution. Besides, despite the Winograd algorithm consisting of four stages, most existing methods only quantize the element-wise multiplication stage, leaving a considerable portion of calculations in full precision.In this paper, observing the inconsistency among different transformation procedures, we present PTQ-Aware Winograd (PAW) to optimize them collaboratively under a unified objective function. Moreover, we explore the full quantization of faster Winograd (tile size $\\geq4$) for the first time. We further propose a hardware-friendly method called Factorized Scale Quantization (FSQ), which can effectively balance the significant range differences in the Winograd domain. Experiments demonstrate the effectiveness of our method, e.g., with 8-bit quantization and a tile size of 6, our method outperforms the previous Winograd PTQ method by 8.27\\% and 5.38\\% in terms of the top-1 accuracy on ResNet-18 and ResNet-34, respectively",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianqi Chen",
      "Weixiang Xu",
      "Weihan Chen",
      "Peisong Wang",
      "Jian Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/401aa72e0e3be680348a5b0ffdb1a5aa-Abstract-Conference.html": {
    "title": "Quantum Bayesian Optimization",
    "volume": "main",
    "abstract": "Kernelized bandits, also known as Bayesian optimization (BO), has been a prevalent method for optimizing complicated black-box reward functions. Various BO algorithms have been theoretically shown to enjoy upper bounds on their cumulative regret which are sub-linear in the number $T$ of iterations, and a regret lower bound of $\\Omega(\\sqrt{T})$ has been derived which represents the unavoidable regrets for any classical BO algorithm. Recent works on quantum bandits have shown that with the aid of quantum computing, it is possible to achieve tighter regret upper bounds better than their corresponding classical lower bounds. However, these works are restricted to either multi-armed or linear bandits, and are hence not able to solve sophisticated real-world problems with non-linear reward functions. To this end, we introduce the quantum-Gaussian process-upper confidence bound (Q-GP-UCB) algorithm. To the best of our knowledge, our Q-GP-UCB is the first BO algorithm able to achieve a regret upper bound of $\\mathcal{O}(\\text{poly}\\log T)$, which is significantly smaller than its regret lower bound of $\\Omega(\\sqrt{T})$ in the classical setting. Moreover, thanks to our novel analysis of the confidence ellipsoid, our Q-GP-UCB with the linear kernel achieves a smaller regret than the quantum linear UCB algorithm from the previous work. We use simulations, as well as an experiment using a real quantum computer, to verify that the theoretical quantum speedup achieved by our Q-GP-UCB is also potentially relevant in practice",
    "keywords": [],
    "checked": true,
    "id": "db81a05dd29b9e4df94df3d445ffec161d4748d4",
    "semantic_title": "quantum bayesian optimization",
    "citation_count": 2,
    "authors": [
      "Zhongxiang Dai",
      "Gregory Kang Ruey Lau",
      "Arun Verma",
      "YAO SHU",
      "Bryan Kian Hsiang Low",
      "Patrick Jaillet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/402e12102d6ec3ea3df40ce1b23d423a-Abstract-Conference.html": {
    "title": "Interpretable Reward Redistribution in Reinforcement Learning: A Causal Approach",
    "volume": "main",
    "abstract": "A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Reward redistribution serves as a solution to re-assign credits for each time step from observed sequences. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable reward redistribution and preserving policy invariance. In this paper, we start by studying the role of causal generative models in reward redistribution by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal generative model to form a compact representation to train policy over the most favorable subspace of the state space of the agent. Theoretically, we show that the unobservable Markovian reward function is identifiable, as well as the underlying causal structure and causal models. Experimental results show that our method outperforms state-of-the-art methods and the provided visualization further demonstrates the interpretability of our method.The project page is located at https://reedzyd.github.io/GenerativeReturnDecomposition/",
    "keywords": [],
    "checked": true,
    "id": "1f56856748285c3fdbd0e8ee97d678123b9f17d5",
    "semantic_title": "interpretable reward redistribution in reinforcement learning: a causal approach",
    "citation_count": 4,
    "authors": [
      "Yudi Zhang",
      "Yali Du",
      "Biwei Huang",
      "Ziyan Wang",
      "Jun Wang",
      "Meng Fang",
      "Mykola Pechenizkiy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/40386e4770bebd63fdf47cbc67341c0b-Abstract-Conference.html": {
    "title": "Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability",
    "volume": "main",
    "abstract": "Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multiplayer games. We show that in games that approximately decompose into a set of two-player constant-sum games (called constant-sum polymatrix games) where global $\\epsilon$-Nash equilibria are boundedly far from Nash equilibria in each subgame (called subgame stability), any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify a structural property of multiplayer games that enable performance guarantees for the strategies produced by a broad class of self-play algorithms. We demonstrate our findings through experiments on Leduc poker",
    "keywords": [],
    "checked": true,
    "id": "003186bc4161c9e7d35f805c5c7d7e1a70320a0f",
    "semantic_title": "guarantees for self-play in multiplayer games via polymatrix decomposability",
    "citation_count": 0,
    "authors": [
      "Revan MacQueen",
      "James  Wright"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4054556fcaa934b0bf76da52cf4f92cb-Abstract-Conference.html": {
    "title": "VCC: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens",
    "volume": "main",
    "abstract": "Transformers are central in modern natural language processing and computer vision applications. Despite recent works devoted to reducing the quadratic cost of such models with respect to sequence length, dealing with ultra long sequences (e.g., $>$16K tokens) remains challenging. Applications such as answering questions based on a book or summarizing a scientific article are inefficient or infeasible. Here, we propose to significantly improve the efficiency of Transformers for ultra long sequences, by compressing the sequence into a much smaller representation at each layer. Specifically, by exploiting the fact that in many tasks, only a small subset of special tokens, which we call VIP-tokens, are most relevant to the final prediction, we propose a VIP-token centric compression (VCC) scheme which selectively compresses the sequence based on their impact on approximating the representation of the VIP-tokens. Compared with competitive baselines, our algorithm is not only efficient (achieving more than $3\\times$ compute efficiency gain compared to baselines on 4K and 16K lengths), but also offers competitive/better performance on a large number of tasks. Further, we show that our algorithm scales to 128K tokens (or more) while consistently offering accuracy improvement. Code is available at https://github.com/mlpen/VCC",
    "keywords": [],
    "checked": true,
    "id": "ffdf78392747ad50c6bb39adab124726db05c3da",
    "semantic_title": "vcc: scaling transformers to 128k tokens or more by prioritizing important tokens",
    "citation_count": 1,
    "authors": [
      "Zhanpeng Zeng",
      "Cole Hawkins",
      "Mingyi Hong",
      "Aston Zhang",
      "Nikolaos Pappas",
      "Vikas Singh",
      "Shuai Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4065a881baab1744bfba208a4361bbb1-Abstract-Conference.html": {
    "title": "Greatness in Simplicity: Unified Self-Cycle Consistency for Parser-Free Virtual Try-On",
    "volume": "main",
    "abstract": "Image-based virtual try-on tasks remain challenging, primarily due to inherent complexities associated with non-rigid garment deformation modeling and strong feature entanglement of clothing within human body. Recent groundbreaking formulations, such as in-painting, cycle consistency, and knowledge distillation, have facilitated self-supervised generation of try-on images. However, these paradigms necessitate the disentanglement of garment features within human body features through auxiliary tasks, such as leveraging 'teacher knowledge' and dual generators. The potential presence of irresponsible prior knowledge in the auxiliary task can serve as a significant bottleneck for the main generator (e.g., 'student model') in the downstream task. Moreover, existing garment deformation methods lack the ability to perceive the correlation between the garment and the human body in the real world, leading to unrealistic alignment effects. To tackle these limitations, we present a new parser-free virtual try-on network based on unified self-cycle consistency (USC-PFN), which enables robust translation between different garments using just a single generator, faithfully replicating non-rigid geometric deformation of garments in real-life scenarios. Specifically, we first propose a self-cycle consistency architecture with a circular mode. It utilizes real unpaired garment-person images exclusively as input for training, effectively eliminating the impact of irresponsible prior knowledge at the model input end. Additionally, we formulate a Markov Random Field to simulate a more natural and realistic garment deformation. Furthermore, USC-PFN can leverage a general generator for self-supervised cycle training. Experiments demonstrate that our method achieves state-of-the-art performance on a popular virtual try-on benchmark",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenghu Du",
      "junyin Wang",
      "Shuqing Liu",
      "Shengwu Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/407106f4b56040b2e8dcad75a6e461e5-Abstract-Conference.html": {
    "title": "VPGTrans: Transfer Visual Prompt Generator across LLMs",
    "volume": "main",
    "abstract": "Since developing a new multimodal LLM (MLLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG component of the MLLM still incurs significant computational costs, such as thousands of GPU hours and millions of training data points. An alternative solution is transferring an existing VPG from one MLLM to the target MLLM. In this work, we investigate VPG transferability across LLMs for the first time, aiming to reduce the cost of VPG training. Specifically, we explore VPG transfer across different LLM sizes (e.g., small-to-large) and types. We identify key factors to maximize transfer efficiency, based on which we develop a simple yet highly effective two-stage transfer framework, called VPGTrans. Notably, it enables VPG transfer from BLIP-2 OPT 2.7B to BLIP-2 OPT 6.7B with less than 10% of the GPU hours using only 10.7% of the training data compared to training a VPG for OPT 6.7B from scratch. Furthermore, we provide a series of intriguing findings and discuss potential explanations behind them. Finally, we showcase the practical value of our VPGTrans approach, by customizing two novel MLLMs, including VL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs",
    "keywords": [],
    "checked": false,
    "id": "0046306876ff2d5600699327e52bc29fa5e9ec91",
    "semantic_title": "transfer visual prompt generator across llms",
    "citation_count": 43,
    "authors": [
      "Ao Zhang",
      "Hao Fei",
      "Yuan Yao",
      "Wei Ji",
      "Li Li",
      "Zhiyuan Liu",
      "Tat-Seng Chua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4078c8b648dc107aedbdf561dd4edc2a-Abstract-Conference.html": {
    "title": "Nearest Neighbour with Bandit Feedback",
    "volume": "main",
    "abstract": "In this paper we adapt the nearest neighbour rule to the contextual bandit problem. Our algorithm handles the fully adversarial setting in which no assumptions at all are made about the data-generation process. When combined with a sufficiently fast data-structure for (perhaps approximate) adaptive nearest neighbour search, such as a navigating net, our algorithm is extremely efficient - having a per trial running time polylogarithmic in both the number of trials and actions, and taking only quasi-linear space. We give generic regret bounds for our algorithm and further analyse them when applied to the stochastic bandit problem in euclidean space. A side result of this paper is that, when applied to the online classification problem with stochastic labels, our algorithm can, under certain conditions, have sublinear regret whilst only finding a single nearest neighbour per trial - in stark contrast to the k-nearest neighbours algorithm",
    "keywords": [],
    "checked": true,
    "id": "d81699824858af4621657d78a3f933a24bf4bc98",
    "semantic_title": "nearest neighbour with bandit feedback",
    "citation_count": 1,
    "authors": [
      "Stephen Pasteris",
      "Chris Hicks",
      "Vasilios Mavroudis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/407fb8c5f3fda374c57d1bb18313ea5d-Abstract-Conference.html": {
    "title": "Generative Neural Fields by Mixtures of Neural Implicit Functions",
    "volume": "main",
    "abstract": "We propose a novel approach to learning the generative neural fields represented by linear combinations of implicit basis networks. Our algorithm learns basis networks in the form of implicit neural representations and their coefficients in a latent space by either conducting meta-learning or adopting auto-decoding paradigms. The proposed method easily enlarges the capacity of generative neural fields by increasing the number of basis networks while maintaining the size of a network for inference to be small through their weighted model averaging. Consequently, sampling instances using the model is efficient in terms of latency and memory footprint. Moreover, we customize denoising diffusion probabilistic model for a target task to sample latent mixture coefficients, which allows our final model to generate unseen data effectively. Experiments show that our approach achieves competitive generation performance on diverse benchmarks for images, voxel data, and NeRF scenes without sophisticated designs for specific modalities and domains",
    "keywords": [],
    "checked": true,
    "id": "6ff14e75df375f7888aa9a0727f6bf8c456911d0",
    "semantic_title": "generative neural fields by mixtures of neural implicit functions",
    "citation_count": 2,
    "authors": [
      "Tackgeun You",
      "Mijeong Kim",
      "Jungtaek Kim",
      "Bohyung Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/40b60852a4abdaa696b5a1a78da34635-Abstract-Conference.html": {
    "title": "MAViL: Masked Audio-Video Learners",
    "volume": "main",
    "abstract": "We present Masked Audio-Video Learners (MAViL) to learn audio-visual representations with three complementary forms of self-supervision: (1) reconstructing masked raw audio and video inputs, (2) intra-modal and inter-modal contrastive learning with masking, and (3) self-training to predict aligned and contextualized audio-video representations learned from the first two objectives. Empirically, MAViL achieves state-of-the-art audio-video classification performance on AudioSet (53.3 mAP) and VGGSound (67.1\\% accuracy), surpassing recent self-supervised models and supervised models that utilize external labeled data. Notably, pre-training with MAViL not only enhances performance in multimodal classification and retrieval tasks, but it also improves the representations of each modality in isolation, without relying on information from the other modality during uni-modal fine-tuning or inference. The code and models are available at https://github.com/facebookresearch/MAViL",
    "keywords": [],
    "checked": true,
    "id": "de46871d33c5ceab19bcf05db31e6a16b3d5ba03",
    "semantic_title": "mavil: masked audio-video learners",
    "citation_count": 19,
    "authors": [
      "Po-Yao Huang",
      "Vasu Sharma",
      "Hu Xu",
      "Chaitanya Ryali",
      "haoqi fan",
      "Yanghao Li",
      "Shang-Wen Li",
      "Gargi Ghosh",
      "Jitendra Malik",
      "Christoph Feichtenhofer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/40bb79c081828bebdc39d65a82367246-Abstract-Conference.html": {
    "title": "Combating Representation Learning Disparity with Geometric Harmonization",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) as an effective paradigm of representation learning has achieved tremendous success on various curated datasets in diverse scenarios. Nevertheless, when facing the long-tailed distribution in real-world applications, it is still hard for existing methods to capture transferable and robust representation. The attribution is that the vanilla SSL methods that pursue the sample-level uniformity easily leads to representation learning disparity, where head classes with the huge sample number dominate the feature regime but tail classes with the small sample number passively collapse. To address this problem, we propose a novel Geometric Harmonization (GH) method to encourage the category-level uniformity in representation learning, which is more benign to the minority and almost does not hurt the majority under long-tailed distribution. Specially, GH measures the population statistics of the embedding space on top of self-supervised learning, and then infer an fine-grained instance-wise calibration to constrain the space expansion of head classes and avoid the passive collapse of tail classes. Our proposal does not alter the setting of SSL and can be easily integrated into existing methods in a low-cost manner. Extensive results on a range of benchmark datasets show the effectiveness of \\methodspace with high tolerance to the distribution skewness",
    "keywords": [],
    "checked": true,
    "id": "d2c3dde5a5b6cade440c15c187bf82f62f3a1377",
    "semantic_title": "combating representation learning disparity with geometric harmonization",
    "citation_count": 0,
    "authors": [
      "Zhihan Zhou",
      "Jiangchao Yao",
      "Feng Hong",
      "Ya Zhang",
      "Bo Han",
      "Yanfeng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/41128e5b3a7622da5b17588757599077-Abstract-Conference.html": {
    "title": "Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex Optimization",
    "volume": "main",
    "abstract": "We study multi-product inventory control problems where a manager makes sequential replenishment decisions based on partial historical information in order to minimize its cumulative losses. Our motivation is to consider general demands, losses and dynamics to go beyond standard models which usually rely on newsvendor-type losses, fixed dynamics, and unrealistic i.i.d. demand assumptions. We propose MaxCOSD, an online algorithm that has provable guarantees even for problems with non-i.i.d. demands and stateful dynamics, including for instance perishability. We consider what we call non-degeneracy assumptions on the demand process, and argue that they are necessary to allow learning",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Massil HIHAT",
      "Stéphane Gaïffas",
      "Guillaume Garrigos",
      "Simon Bussy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/411fa9d368b5485be4c6bb62615b365e-Abstract-Conference.html": {
    "title": "On kernel-based statistical learning theory in the mean field limit",
    "volume": "main",
    "abstract": "In many applications of machine learning, a large number of variables are considered. Motivated by machine learning of interacting particle systems, we consider the situation when the number of input variables goes to infinity. First, we continue the recent investigation of the mean field limit of kernels and their reproducing kernel Hilbert spaces, completing the existing theory. Next, we provide results relevant for approximation with such kernels in the mean field limit, including a representer theorem. Finally, we use these kernels in the context of statistical learning in the mean field limit, focusing on Support Vector Machines. In particular, we show mean field convergence of empirical and infinite-sample solutions as well as the convergence of the corresponding risks. On the one hand, our results establish rigorous mean field limits in the context of kernel methods, providing new theoretical tools and insights for large-scale problems. On the other hand, our setting corresponds to a new form of limit of learning problems, which seems to have not been investigated yet in the statistical learning theory literature",
    "keywords": [],
    "checked": false,
    "id": "4feb49424f0a7d4aa1433ac7440b7354d3b623b8",
    "semantic_title": "on kernel-based statistical learning in the mean field limit",
    "citation_count": 0,
    "authors": [
      "Christian Fiedler",
      "Michael Herty",
      "Sebastian Trimpe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/413885e70482b95dcbeeddc1daf39177-Abstract-Conference.html": {
    "title": "3D-LLM: Injecting the 3D World into Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) and Vision-Language Models (VLMs) have been proved to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3Dgrounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs could better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (\\textit{e.g.}, the BLEU-1 score surpasses state-of-the-art score by 9\\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Our model and data will be publicly available",
    "keywords": [],
    "checked": true,
    "id": "7637ed79d30d0139901175ae4abedd822c217ab4",
    "semantic_title": "3d-llm: injecting the 3d world into large language models",
    "citation_count": 45,
    "authors": [
      "Yining Hong",
      "Haoyu Zhen",
      "Peihao Chen",
      "Shuhong Zheng",
      "Yilun Du",
      "Zhenfang Chen",
      "Chuang Gan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/414f4c9fe9653e5de98fad6964d50315-Abstract-Conference.html": {
    "title": "An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions",
    "volume": "main",
    "abstract": "Noisy marginals are a common form of confidentiality-protecting data release and are useful for many downstream tasks such as contingency table analysis, construction of Bayesian networks, and even synthetic data generation. Privacy mechanisms that provide unbiased noisy answers to linear queries (such as marginals) are known as matrix mechanisms.We propose ResidualPlanner, a matrix mechanism for marginals with Gaussian noise that is both optimal and scalable. ResidualPlanner can optimize for many loss functions that can be written as a convex function of marginal variances (prior work was restricted to just one predefined objective function). ResidualPlanner can optimize the accuracy of marginals in large scale settings in seconds, even when the previous state of the art (HDMM) runs out of memory. It even runs on datasets with 100 attributes in a couple of minutes. Furthermore ResidualPlanner can efficiently compute variance/covariance values for each marginal (prior methods quickly run out of memory, even for relatively small datasets)",
    "keywords": [],
    "checked": true,
    "id": "68aec831af77ba45886440a419ed77edda192e10",
    "semantic_title": "an optimal and scalable matrix mechanism for noisy marginals under convex loss functions",
    "citation_count": 1,
    "authors": [
      "Yingtai Xiao",
      "Guanlin He",
      "Danfeng Zhang",
      "Daniel Kifer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/41623b137cd34807f56028aa9f6f84a7-Abstract-Conference.html": {
    "title": "Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle",
    "volume": "main",
    "abstract": "The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\\tilde{\\Omega}({\\sqrt{n}})$, even in the presence of $\\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\\tilde{\\Omega}(n^{2/5})$ small clusters",
    "keywords": [],
    "checked": true,
    "id": "20d53034ba2b6d1d3293b73708cadf426373f9aa",
    "semantic_title": "recovering unbalanced communities in the stochastic block model with application to clustering with a faulty oracle",
    "citation_count": 4,
    "authors": [
      "Chandra Sekhar Mukherjee",
      "Pan Peng",
      "Jiapeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4163873c9ad623a87989d0a6eefe9442-Abstract-Conference.html": {
    "title": "Transition-constant Normalization for Image Enhancement",
    "volume": "main",
    "abstract": "Normalization techniques that capture image style by statistical representation have become a popular component in deep neural networks.Although image enhancement can be considered as a form of style transformation, there has been little exploration of how normalization affect the enhancement performance. To fully leverage the potential of normalization, we present a novel Transition-Constant Normalization (TCN) for various image enhancement tasks.Specifically, it consists of two streams of normalization operations arranged under an invertible constraint, along with a feature sub-sampling operation that satisfies the normalization constraint.TCN enjoys several merits, including being parameter-free, plug-and-play, and incurring no additional computational costs.We provide various formats to utilize TCN for image enhancement, including seamless integration with enhancement networks, incorporation into encoder-decoder architectures for downsampling, and implementation of efficient architectures.Through extensive experiments on multiple image enhancement tasks, like low-light enhancement, exposure correction, SDR2HDR translation, and image dehazing, our TCN consistently demonstrates performance improvements.Besides, it showcases extensive ability in other tasks including pan-sharpening and medical segmentation.The code is available at \\textit{\\textcolor{blue}{https://github.com/huangkevinj/TCNorm}}",
    "keywords": [],
    "checked": true,
    "id": "e4edf21997d8bfad5b60f30eebdaf44c13a5f8fd",
    "semantic_title": "transition-constant normalization for image enhancement",
    "citation_count": 0,
    "authors": [
      "Jie Huang",
      "man zhou",
      "Jinghao Zhang",
      "Gang Yang",
      "Mingde Yao",
      "Chongyi Li",
      "Zhiwei Xiong",
      "Feng Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/419f72cbd568ad62183f8132a3605a2a-Abstract-Conference.html": {
    "title": "Unexpected Improvements to Expected Improvement for Bayesian Optimization",
    "volume": "main",
    "abstract": "Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in \"classic\" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature",
    "keywords": [],
    "checked": true,
    "id": "0a1bf4740dc9f02c292d5489c5097cc8da7f4368",
    "semantic_title": "unexpected improvements to expected improvement for bayesian optimization",
    "citation_count": 5,
    "authors": [
      "Sebastian Ament",
      "Samuel Daulton",
      "David Eriksson",
      "Maximilian Balandat",
      "Eytan Bakshy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/41aa1c9f57ea83d7c41f0d3e98ed3dd4-Abstract-Conference.html": {
    "title": "Pseudo-Likelihood Inference",
    "volume": "main",
    "abstract": "Simulation-Based Inference (SBI) is a common name for an emerging family of approaches that infer the model parameters when the likelihood is intractable. Existing SBI methods either approximate the likelihood, such as Approximate Bayesian Computation (ABC) or directly model the posterior, such as Sequential Neural Posterior Estimation (SNPE). While ABC is efficient on low-dimensional problems, on higher-dimensional tasks, it is generally outperformed by SNPE, which leverages function approximation. In this paper, we propose Pseudo-Likelihood Inference (PLI), a new method that brings neural approximation into ABC, making it competitive on challenging Bayesian system identification tasks. By utilizing integral probability metrics, we introduce a smooth likelihood kernel with an adaptive bandwidth that is updated based on information-theoretic trust regions. Thanks to this formulation, our method (i) allows for optimizing neural posteriors via gradient descent, (ii) does not rely on summary statistics, and (iii) enables multiple observations as input. In comparison to SNPE, it leads to improved performance when more data is available. The effectiveness of PLI is evaluated on four classical SBI benchmark tasks and on a highly dynamic physical system, showing particular advantages on stochastic simulations and multi-modal posterior landscapes",
    "keywords": [],
    "checked": true,
    "id": "82921417ef5f68e3250f649efcf88b95efdca5e2",
    "semantic_title": "pseudo-likelihood inference",
    "citation_count": 0,
    "authors": [
      "Theo Gruner",
      "Boris Belousov",
      "Fabio Muratore",
      "Daniel Palenicek",
      "Jan R. Peters"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/41badd36e935f8a80175e95d8bc6192e-Abstract-Conference.html": {
    "title": "Calibrating \"Cheap Signals\" in Peer Review without a Prior",
    "volume": "main",
    "abstract": "Peer review lies at the core of the academic process, but even well-intentioned reviewers can still provide noisy ratings. While ranking papers by average ratings may reduce noise, varying noise levels and systematic biases stemming from ``cheap'' signals (e.g. author identity, proof length) can lead to unfairness. Detecting and correcting bias is challenging, as ratings are subjective and unverifiable. Unlike previous works relying on prior knowledge or historical data, we propose a one-shot noise calibration process without any prior information. We ask reviewers to predict others' scores and use these predictions for calibration. Assuming reviewers adjust their predictions according to the noise, we demonstrate that the calibrated score results in a more robust ranking compared to average ratings, even with varying noise levels and biases.In detail, we show that the error probability of the calibrated score approaches zero as the number of reviewers increases and is significantly lower compared to average ratings when the number of reviewers is small",
    "keywords": [],
    "checked": true,
    "id": "c455fbdc1e20fd48d494880cd01b8078f28247a7",
    "semantic_title": "calibrating \"cheap signals\" in peer review without a prior",
    "citation_count": 0,
    "authors": [
      "Yuxuan Lu",
      "Yuqing Kong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/41bcc9d3bddd9c90e1f44b29e26d97ff-Abstract-Conference.html": {
    "title": "SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds",
    "volume": "main",
    "abstract": "Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers. However, these models are large, with complex network architectures and tens of denoising iterations, making them computationally expensive and slow to run. As a result, high-end GPUs and cloud-based inference are required to run diffusion models at scale. This is costly and has privacy implications, especially when user data is sent to a third party. To overcome these challenges, we present a generic approach that, for the first time, unlocks running text-to-image diffusion models on mobile devices in **less than 2 seconds**. We achieve so by introducing efficient network architecture and improving step distillation. Specifically, we propose an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, we enhance the step distillation by exploring training strategies and introducing regularization from classifier-free guidance. Our extensive experiments on MS-COCO show that our model with $8$ denoising steps achieves better FID and CLIP scores than Stable Diffusion v$1.5$ with $50$ steps. Our work democratizes content creation by bringing powerful text-to-image diffusion models to the hands of users",
    "keywords": [],
    "checked": true,
    "id": "9b504916f0e8fbeb1891f0db299dcbbc118f4898",
    "semantic_title": "snapfusion: text-to-image diffusion model on mobile devices within two seconds",
    "citation_count": 29,
    "authors": [
      "Yanyu Li",
      "Huan Wang",
      "Qing Jin",
      "Ju Hu",
      "Pavlo Chemerys",
      "Yun Fu",
      "Yanzhi Wang",
      "Sergey Tulyakov",
      "Jian Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/41bd71e7bf7f9fe68f1c936940fd06bd-Abstract-Conference.html": {
    "title": "LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference",
    "volume": "main",
    "abstract": "The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a model with minimized multiplication depth. (2) A compact node-wise polynomial replacement policy with a second-order trainable activation function, steered towards superior convergence by a two-level distillation approach from an all-ReLU based teacher model. (3) an enhanced HE solution that enables finer-grained operator fusion for node-wise activation functions, further reducing multiplication level consumption in HE-based inference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal that LinGCN excels in latency, accuracy, and scalability for homomorphically encrypted inference, outperforming solutions such as CryptoGCN. Remarkably, LinGCN achieves a 14.2× latency speedup relative to CryptoGCN, while preserving an inference accuracy of ~75\\% and notably reducing multiplication depth. Additionally, LinGCN proves scalable for larger models, delivering a substantial 85.78\\% accuracy with 6371s latency, a 10.47\\% accuracy improvement over CryptoGCN",
    "keywords": [],
    "checked": true,
    "id": "b787f4c24bd7b8296bb3837321c25e523f7e0043",
    "semantic_title": "lingcn: structural linearized graph convolutional network for homomorphically encrypted inference",
    "citation_count": 4,
    "authors": [
      "Hongwu Peng",
      "Ran Ran",
      "Yukui Luo",
      "Jiahui Zhao",
      "Shaoyi Huang",
      "Kiran Thorat",
      "Tong Geng",
      "Chenghong Wang",
      "Xiaolin Xu",
      "Wujie Wen",
      "Caiwen Ding"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/41ed4bd197d0a5fa036d361c1fc606ad-Abstract-Conference.html": {
    "title": "Spectral Evolution and Invariance in Linear-width Neural Networks",
    "volume": "main",
    "abstract": "We investigate the spectral properties of linear-width feed-forward neural networks, where the sample size is asymptotically proportional to network width. Empirically, we show that the spectra of weight in this high dimensional regime are invariant when trained by gradient descent for small constant learning rates; we provide a theoretical justification for this observation and prove the invariance of the bulk spectra for both conjugate and neural tangent kernels. We demonstrate similar characteristics when training with stochastic gradient descent with small learning rates. When the learning rate is large, we exhibit the emergence of an outlier whose corresponding eigenvector is aligned with the training data structure. We also show that after adaptive gradient training, where a lower test error and feature learning emerge, both weight and kernel matrices exhibit heavy tail behavior. Simple examples are provided to explain when heavy tails can have better generalizations. We exhibit different spectral properties such as invariant bulk, spike, and heavy-tailed distribution from a two-layer neural network using different training strategies, and then correlate them to the feature learning. Analogous phenomena also appear when we train conventional neural networks with real-world data. We conclude that monitoring the evolution of the spectra during training is an essential step toward understanding the training dynamics and feature learning",
    "keywords": [],
    "checked": true,
    "id": "489909b61decbad2f8b6aa33200783d5b322fe21",
    "semantic_title": "spectral evolution and invariance in linear-width neural networks",
    "citation_count": 3,
    "authors": [
      "Zhichao Wang",
      "Andrew Engel",
      "Anand D Sarwate",
      "Ioana Dumitriu",
      "Tony Chiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/420492060687ca7448398c4c3fa10366-Abstract-Conference.html": {
    "title": "Paxion: Patching Action Knowledge in Video-Language Foundation Models",
    "volume": "main",
    "abstract": "Action knowledge involves the understanding of textual, visual, and temporal aspects of actions. We introduce the Action Dynamics Benchmark (ActionBench) containing two carefully designed probing tasks: Action Antonym and Video Reversal, which targets multimodal alignment capabilities and temporal understanding skills of the model, respectively. Despite recent video-language models' (VidLM) impressive performance on various benchmark tasks, our diagnostic tasks reveal their surprising deficiency (near-random performance) in action knowledge, suggesting that current models rely on object recognition abilities as a shortcut for action understanding. To remedy this, we propose a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective. The Paxion framework utilizes a Knowledge Patcher network to encode new action knowledge and a Knowledge Fuser component to integrate the Patcher into frozen VidLMs without compromising their existing capabilities. Due to limitations of the widely-used Video-Text Contrastive (VTC) loss for learning action knowledge, we introduce the DVDM objective to train the Knowledge Patcher. DVDM forces the model to encode the correlation between the action text and the correct ordering of video frames. Our extensive analyses show that Paxion and DVDM together effectively fill the gap in action knowledge understanding (~50% → 80%), while maintaining or improving performance on a wide spectrum of both object- and action-centric downstream tasks",
    "keywords": [],
    "checked": true,
    "id": "3130643a5d02f0e849d83bb1f85577a924081f36",
    "semantic_title": "paxion: patching action knowledge in video-language foundation models",
    "citation_count": 8,
    "authors": [
      "Zhenhailong Wang",
      "Ansel Blume",
      "Sha Li",
      "Genglin Liu",
      "Jaemin Cho",
      "Zineng Tang",
      "Mohit Bansal",
      "Heng Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/420678bb4c8251ab30e765bc27c3b047-Abstract-Conference.html": {
    "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
    "volume": "main",
    "abstract": "The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web",
    "keywords": [],
    "checked": true,
    "id": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
    "semantic_title": "propile: probing privacy leakage in large language models",
    "citation_count": 17,
    "authors": [
      "Siwon Kim",
      "Sangdoo Yun",
      "Hwaran Lee",
      "Martin Gubri",
      "Sungroh Yoon",
      "Seong Joon Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/421f83663c02cdaec8c3c38337709989-Abstract-Conference.html": {
    "title": "Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension",
    "volume": "main",
    "abstract": "The success of over-parameterized neural networks trained to near-zero training error has caused great interest in the phenomenon of benign overfitting, where estimators are statistically consistent even though they interpolate noisy training data. While benign overfitting in fixed dimension has been established for some learning methods, current literature suggests that for regression with typical kernel methods and wide neural networks, benign overfitting requires a high-dimensional setting, where the dimension grows with the sample size. In this paper, we show that the smoothness of the estimators, and not the dimension, is the key: benign overfitting is possible if and only if the estimator's derivatives are large enough. We generalize existing inconsistency results to non-interpolating models and more kernels to show that benign overfitting with moderate derivatives is impossible in fixed dimension. Conversely, we show that benign overfitting is possible for regression with a sequence of spiky-smooth kernels with large derivatives. Using neural tangent kernels, we translate our results to wide neural networks. We prove that while infinite-width networks do not overfit benignly with the ReLU activation, this can be fixed by adding small high-frequency fluctuations to the activation function. Our experiments verify that such neural networks, while overfitting, can indeed generalize well even on low-dimensional data sets",
    "keywords": [],
    "checked": true,
    "id": "5dc47ebf42525aed852644c8281bf750705051e6",
    "semantic_title": "mind the spikes: benign overfitting of kernels and neural networks in fixed dimension",
    "citation_count": 2,
    "authors": [
      "Moritz Haas",
      "David Holzmüller",
      "Ulrike Luxburg",
      "Ingo Steinwart"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4241fec6e94221526b0a9b24828bb774-Abstract-Conference.html": {
    "title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
    "volume": "main",
    "abstract": "Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context---incorporating its pragmatics. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response \"I wore gloves\" to the question \"Did you leave fingerprints?\" as meaning \"No\". To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate four categories of widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), models in three of these categories perform close to random. However, LLMs instruction-tuned at the example-level perform significantly better. These results suggest that certain fine-tuning strategies are far better at inducing pragmatic understanding in models. We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse",
    "keywords": [],
    "checked": true,
    "id": "2ed471dbf2eac410b344de1b00e0ad9671bc1967",
    "semantic_title": "the goldilocks of pragmatic understanding: fine-tuning strategy matters for implicature resolution by llms",
    "citation_count": 17,
    "authors": [
      "Laura Ruis",
      "Akbir Khan",
      "Stella Biderman",
      "Sara Hooker",
      "Tim Rocktäschel",
      "Edward Grefenstette"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/425ee25d6c22ef98b67328273b8f95d5-Abstract-Conference.html": {
    "title": "Slow and Weak Attractor Computation Embedded in Fast and Strong E-I Balanced Neural Dynamics",
    "volume": "main",
    "abstract": "Attractor networks require neuronal connections to be highly structured in order to maintain attractor states that represent information, while excitation and inhibition balanced networks (E-INNs) require neuronal connections to be random and sparse to generate irregular neuronal firings. Despite being regarded as canonical models of neural circuits, both types of networks are usually studied in isolation, and it remains unclear how they coexist in the brain, given their very different structural demands. In this study, we investigate the compatibility of continuous attractor neural networks (CANNs) and E-INNs. In line with recent experimental data, we find that a neural circuit can exhibit both the traits of CANNs and E-INNs if the neuronal synapses consist of two sets: one set is strong and fast for irregular firing, and the other set is weak and slow for attractor dynamics. Our results from simulations and theoretical analysis reveal that the network also exhibits enhanced performance compared to the case of using only one set of synapses, with accelerated convergence of attractor states and retained E-I balanced condition for localized input. We also apply the network model to solve a real-world tracking problem and demonstrate that it can track fast-moving objects well. We hope that this study provides insight into how structured neural computations are realized by irregular firings of neurons",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Lin",
      "Liyuan Li",
      "Boxin Shi",
      "Tiejun Huang",
      "Yuanyuan Mi",
      "Si Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4267d84ca2f6fbb4aa5172b76b433aca-Abstract-Conference.html": {
    "title": "Test-time Training for Matching-based Video Object Segmentation",
    "volume": "main",
    "abstract": "The video object segmentation (VOS) task involves the segmentation of an object over time based on a single initial mask. Current state-of-the-art approaches use a memory of previously processed frames and rely on matching to estimate segmentation masks of subsequent frames. Lacking any adaptation mechanism, such methods are prone to test-time distribution shifts. This work focuses on matching-based VOS under distribution shifts such as video corruptions, stylization, and sim-to-real transfer. We explore test-time training strategies that are agnostic to the specific task as well as strategies that are designed specifically for VOS. This includes a variant based on mask cycle consistency tailored to matching-based VOS methods. The experimental results on common benchmarks demonstrate that the proposed test-time training yields significant improvements in performance. In particular for the sim-to-real scenario and despite using only a single test video, our approach manages to recover a substantial portion of the performance gain achieved through training on real videos. Additionally, we introduce DAVIS-C, an augmented version of the popular DAVIS test set, featuring extreme distribution shifts like image-/video-level corruptions and stylizations. Our results illustrate that test-time training enhances performance even in these challenging cases",
    "keywords": [],
    "checked": false,
    "id": "76008adddaee68dfe913af09f740432b9cd74ba0",
    "semantic_title": "label propagation for one-shot video object segmentation",
    "citation_count": 0,
    "authors": [
      "Juliette Bertrand",
      "Giorgos Kordopatis Zilos",
      "Yannis Kalantidis",
      "Giorgos Tolias"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42770daf4a3384b712ea9c36e9279998-Abstract-Conference.html": {
    "title": "Causal Effect Regularization: Automated Detection and Removal of Spurious Correlations",
    "volume": "main",
    "abstract": "In many classification datasets, the task labels are spuriously correlated with some input attributes. Classifiers trained on such datasets often rely on these attributes for prediction, especially when the spurious correlation is high, and thus fail to generalize whenever there is a shift in the attributes' correlation at deployment. If we assume that the spurious attributes are known a priori, several methods have been proposed to learn a classifier that is invariant to the specified attributes. However, in real-world data, information about spurious attributes is typically unavailable. Therefore, we propose a method to automatically identify spurious attributes by estimating their causal effect on the label and then use a regularization objective to mitigate the classifier's reliance on them. Compared to a recent method for identifying spurious attributes, we find that our method is more accurate in removing the attribute from the learned model, especially when spurious correlation is high. Specifically, across synthetic, semi-synthetic, and real-world datasets, our method shows significant improvement in a metric used to quantify the dependence of a classifier on spurious attributes ($\\Delta$Prob), while obtaining better or similar accuracy. In addition, our method mitigates the reliance on spurious attributes even under noisy estimation of causal effects. To explain the empirical robustness of our method, we create a simple linear classification task with two sets of attributes: causal and spurious. We prove that our method only requires that the ranking of estimated causal effects is correct across attributes to select the correct classifier",
    "keywords": [],
    "checked": false,
    "id": "b59849cc90bcc367aec6ea1670b53496d413e2fa",
    "semantic_title": "causal effect regularization: automated detection and removal of spurious attributes",
    "citation_count": 1,
    "authors": [
      "Abhinav Kumar",
      "Amit Deshpande",
      "Amit Sharma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/427f20d90386fd27804f1831d6a3d48f-Abstract-Conference.html": {
    "title": "Multi-resolution Spectral Coherence for Graph Generation with Score-based Diffusion",
    "volume": "main",
    "abstract": "Successful graph generation depends on the accurate estimation of the joint distribution of graph components such as nodes and edges from training data. While recent deep neural networks have demonstrated sampling of realistic graphs together with diffusion models, however, they still suffer from oversmoothing problems which are inherited from conventional graph convolution and thus high-frequency characteristics of nodes and edges become intractable. To overcome such issues and generate graphs with high fidelity, this paper introduces a novel approach that captures the dependency between nodes and edges at multiple resolutions in the spectral space. By modeling the joint distribution of node and edge signals in a shared graph wavelet space, together with a score-based diffusion model, we propose a Wavelet Graph Diffusion Model (Wave-GD) which lets us sample synthetic graphs with real-like frequency characteristics of nodes and edges. Experimental results on four representative benchmark datasets validate the superiority of the Wave-GD over existing approaches, highlighting its potential for a wide range of applications that involve graph data",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyuna Cho",
      "Minjae Jeong",
      "Sooyeon Jeon",
      "Sungsoo Ahn",
      "Won Hwa Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42806406dd99e30c3796bc98b2670fa2-Abstract-Conference.html": {
    "title": "Real-World Image Super-Resolution as Multi-Task Learning",
    "volume": "main",
    "abstract": "In this paper, we take a new look at real-world image super-resolution (real-SR) from a multi-task learning perspective. We demonstrate that the conventional formulation of real-SR can be viewed as solving multiple distinct degradation tasks using a single shared model. This poses a challenge known as task competition or task conflict in multi-task learning, where certain tasks dominate the learning process, resulting in poor performance on other tasks. This problem is exacerbated in the case of real-SR, due to the involvement of numerous degradation tasks. To address the issue of task competition in real-SR, we propose a task grouping approach. Our approach efficiently identifies the degradation tasks where a real-SR model falls short and groups these unsatisfactory tasks into multiple task groups for further training. By grouping similar tasks together, our approach mitigates task competition and facilitates effective knowledge transfer. Extensive experiments demonstrate our method achieves significantly enhanced performance across a wide range of degradation scenarios",
    "keywords": [],
    "checked": false,
    "id": "875d438ca5d17ae398997ec963b57bed832ef16d",
    "semantic_title": "blind image super-resolution via contrastive representation learning",
    "citation_count": 39,
    "authors": [
      "Wenlong Zhang",
      "Xiaohui Li",
      "Guangyuan SHI",
      "Xiangyu Chen",
      "Yu Qiao",
      "Xiaoyun Zhang",
      "Xiao-Ming Wu",
      "Chao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/428ceef2cd8a53add7213e04d1746479-Abstract-Conference.html": {
    "title": "Exact Representation of Sparse Networks with Symmetric Nonnegative Embeddings",
    "volume": "main",
    "abstract": "Graph models based on factorization of the adjacency matrix often fail to capture network structures related to links between dissimilar nodes (heterophily). We introduce a novel graph factorization model that leverages two nonnegative vectors per node to interpretably account for links between both similar and dissimilar nodes. We prove that our model can exactly represent any graph with low arboricity, a property that many real-world networks satisfy; our proof also applies to related models but has much greater scope than the closest prior bound, which is based on low max degree. Our factorization also has compelling properties besides expressiveness: due to its symmetric structure and nonnegativity, fitting the model inherently finds node communities, and the model's link predictions can be interpreted in terms of these communities. In experiments on real-world networks, we demonstrate our factorization's effectiveness on a variety of tasks, including community detection and link prediction",
    "keywords": [],
    "checked": true,
    "id": "ddc08e96d4d52c34bc9f9e61a77b7c59be179a21",
    "semantic_title": "exact representation of sparse networks with symmetric nonnegative embeddings",
    "citation_count": 0,
    "authors": [
      "Sudhanshu Chanpuriya",
      "Ryan Rossi",
      "Anup B. Rao",
      "Tung Mai",
      "Nedim Lipka",
      "Zhao Song",
      "Cameron Musco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4290cccf23be59e42a575d026ccbeeb8-Abstract-Conference.html": {
    "title": "Data-Centric Learning from Unlabeled Graphs with Diffusion Model",
    "volume": "main",
    "abstract": "Graph property prediction tasks are important and numerous. While each task offers a small size of labeled examples, unlabeled graphs have been collected from various sources and at a large scale. A conventional approach is training a model with the unlabeled graphs on self-supervised tasks and then fine-tuning the model on the prediction tasks. However, the self-supervised task knowledge could not be aligned or sometimes conflicted with what the predictions needed. In this paper, we propose to extract the knowledge underlying the large set of unlabeled graphs as a specific set of useful data points to augment each property prediction model. We use a diffusion model to fully utilize the unlabeled graphs and design two new objectives to guide the model's denoising process with each task's labeled data to generate task-specific graph examples and their labels. Experiments demonstrate that our data-centric approach performs significantly better than fifteen existing various methods on fifteen tasks. The performance improvement brought by unlabeled data is visible as the generated labeled examples unlike the self-supervised learning",
    "keywords": [],
    "checked": true,
    "id": "41c40a0ef508169c9fcfac13bf557be284acef3a",
    "semantic_title": "data-centric learning from unlabeled graphs with diffusion model",
    "citation_count": 8,
    "authors": [
      "Gang Liu",
      "Eric Inae",
      "Tong Zhao",
      "Jiaxin Xu",
      "Tengfei Luo",
      "Meng Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/429b5216a4d08850c586fbf809e17877-Abstract-Conference.html": {
    "title": "Wasserstein Gradient Flows for Optimizing Gaussian Mixture Policies",
    "volume": "main",
    "abstract": "Robots often rely on a repertoire of previously-learned motion policies for performing tasks of diverse complexities. When facing unseen task conditions or when new task requirements arise, robots must adapt their motion policies accordingly. In this context, policy optimization is the \\emph{de facto} paradigm to adapt robot policies as a function of task-specific objectives. Most commonly-used motion policies carry particular structures that are often overlooked in policy optimization algorithms. We instead propose to leverage the structure of probabilistic policies by casting the policy optimization as an optimal transport problem. Specifically, we focus on robot motion policies that build on Gaussian mixture models (GMMs) and formulate the policy optimization as a Wassertein gradient flow over the GMMs space. This naturally allows us to constrain the policy updates via the $L^2$-Wasserstein distance between GMMs to enhance the stability of the policy optimization process. Furthermore, we leverage the geometry of the Bures-Wasserstein manifold to optimize the Gaussian distributions of the GMM policy via Riemannian optimization. We evaluate our approach on common robotic settings: Reaching motions, collision-avoidance behaviors, and multi-goal tasks. Our results show that our method outperforms common policy optimization baselines in terms of task success rate and low-variance solutions",
    "keywords": [],
    "checked": true,
    "id": "edad8bbdbdb6dc4838988598c328fb92f654f06b",
    "semantic_title": "wasserstein gradient flows for optimizing gaussian mixture policies",
    "citation_count": 1,
    "authors": [
      "Hanna Ziesche",
      "Leonel Rozo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42a0de6b8a1809ceba8fdad1661be06c-Abstract-Conference.html": {
    "title": "Change point detection and inference in multivariate non-parametric models under mixing conditions",
    "volume": "main",
    "abstract": "This paper addresses the problem of localizing and inferring multiple change points, in non-parametric multivariate time series settings. Specifically, we consider a multivariate time series with potentially short-range dependence, whose underlying distributions have Hölder smooth densities and can change over time in a piecewise-constant manner. The change points, which correspond to the times when the distribution changes, are unknown. We present the limiting distributions of the change point estimators under the scenarios where the minimal jump size vanishes or remains constant. Such results have not been revealed in the literature in non-parametric change point settings. As byproducts, we develop a sharp estimator that can accurately localize the change points in multivariate non-parametric time series, and a consistent block-type long-run variance estimator. Numerical studies are provided to complement our theoretical findings",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Misael Madrid Padilla",
      "Haotian Xu",
      "Daren Wang",
      "OSCAR HERNAN MADRID PADILLA",
      "Yi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42afce512806ab874b9f99ed9a08055e-Abstract-Conference.html": {
    "title": "Near-optimal learning with average Hölder smoothness",
    "volume": "main",
    "abstract": "We generalize the notion of average Lipschitz smoothness proposed by Ashlagi et al. (COLT 2021) by extending it to Hölder smoothness. This measure of the \"effective smoothness\" of a function is sensitive to the underlying distribution and can be dramatically smaller than its classic \"worst-case\" Hölder constant.We consider both the realizable and the agnostic (noisy) regression settings, proving upper and lower risk bounds in terms of the average Hölder smoothness; these rates improve upon both previously known rates even in the special case of average Lipschitz smoothness.Moreover, our lower bound is tight in the realizable setting up to log factors, thus we establish the minimax rate.From an algorithmic perspective, since our notion of average smoothness is defined with respect to the unknown underlying distribution, the learner does not have an explicit representation of the function class, hence is unable to execute ERM. Nevertheless, we provide distinct learning algorithms that achieve both (nearly) optimal learning rates.Our results hold in any totally bounded metric space, and are stated in terms of its intrinsic geometry.Overall, our results show that the classic worst-case notion of Hölder smoothness can be essentially replaced by its average, yielding considerably sharper guarantees",
    "keywords": [],
    "checked": true,
    "id": "b4200edccd78a9209e9a8d46b7fb493e60e60db8",
    "semantic_title": "near-optimal learning with average hölder smoothness",
    "citation_count": 2,
    "authors": [
      "Guy Kornowski",
      "Steve Hanneke",
      "Aryeh Kontorovich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42b7c2f6d320d1fe1afa899a6319d6d7-Abstract-Conference.html": {
    "title": "Neural-Logic Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "The interaction decoder utilized in prevalent Transformer-based HOI detectors typically accepts pre-composed human-object pairs as inputs. Though achieving remarkable performance, such a paradigm lacks feasibility and cannot explore novel combinations over entities during decoding. We present LogicHOI, a new HOI detector that leverages neural-logic reasoning and Transformer to infer feasible interactions between. entities. Specifically, we modify. self-attention mechanism in the vanilla Transformer, enabling it to reason over the ⟨ human, action, object ⟩ triplet and constitute novel interactions. Meanwhile, such a reasoning process is guided by two crucial properties for understanding HOI: affordances (the potential actions an object can facilitate) and proxemics (the spatial relations between humans and objects). We formulate these two properties in first-order logic and ground them into continuous space to constrain the learning process of our approach, leading to improved performance and zero-shot generalization capabilities. We evaluate L OGIC HOI on V-COCO and HICO-DET under both normal and zero-shot setups, achieving significant improvements over existing methods",
    "keywords": [],
    "checked": true,
    "id": "e1c377356feade5be3bbaaccc9d254582f6056e3",
    "semantic_title": "neural-logic human-object interaction detection",
    "citation_count": 1,
    "authors": [
      "Liulei Li",
      "Jianan Wei",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42bbe2bfdbbfcadda643e8f89025716c-Abstract-Conference.html": {
    "title": "Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness",
    "volume": "main",
    "abstract": "One of the remarkable properties of robust computer vision models is that their input-gradients are often aligned with human perception, referred to in the literature as perceptually-aligned gradients (PAGs). Despite only being trained for classification, PAGs cause robust models to have rudimentary generative capabilities, including image generation, denoising, and in-painting. However, the underlying mechanisms behind these phenomena remain unknown. In this work, we provide a first explanation of PAGs via \\emph{off-manifold robustness}, which states that models must be more robust off- the data manifold than they are on-manifold. We first demonstrate theoretically that off-manifold robustness leads input gradients to lie approximately on the data manifold, explaining their perceptual alignment. We then show that Bayes optimal models satisfy off-manifold robustness, and confirm the same empirically for robust models trained via gradient norm regularization, randomized smoothing, and adversarial training with projected gradient descent. Quantifying the perceptual alignment of model gradients via their similarity with the gradients of generative models, we show that off-manifold robustness correlates well with perceptual alignment. Finally, based on the levels of on- and off-manifold robustness, we identify three different regimes of robustness that affect both perceptual alignment and model accuracy: weak robustness, bayes-aligned robustness, and excessive robustness. Code is available at https://github.com/tml-tuebingen/pags",
    "keywords": [],
    "checked": true,
    "id": "5c46a0bfb23a1df660ba41cb1de387ced85bcf0a",
    "semantic_title": "which models have perceptually-aligned gradients? an explanation via off-manifold robustness",
    "citation_count": 3,
    "authors": [
      "Suraj Srinivas",
      "Sebastian Bordt",
      "Himabindu Lakkaraju"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42c3438f432bc62014ce65af880e0d94-Abstract-Conference.html": {
    "title": "Inferring the Future by Imagining the Past",
    "volume": "main",
    "abstract": "A single panel of a comic book can say a lot: it can depict not only where the characters currently are, but also their motions, their motivations, their emotions, and what they might do next. More generally, humans routinely infer complex sequences of past and future events from a static snapshot of a dynamic scene, even in situations they have never seen before.In this paper, we model how humans make such rapid and flexible inferences. Building on a long line of work in cognitive science, we offer a Monte Carlo algorithm whose inferences correlate well with human intuitions in a wide variety of domains, while only using a small, cognitively-plausible number of samples. Our key technical insight is a surprising connection between our inference problem and Monte Carlo path tracing, which allows us to apply decades of ideas from the computer graphics community to this seemingly-unrelated theory of mind task",
    "keywords": [],
    "checked": true,
    "id": "d75ba73f2471f1c163bffdd94a731c5335733d84",
    "semantic_title": "inferring the future by imagining the past",
    "citation_count": 0,
    "authors": [
      "Kartik Chandra",
      "Tony Chen",
      "Tzu-Mao Li",
      "Jonathan Ragan-Kelley",
      "Josh Tenenbaum"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42c40aff7814e9796266e12053b1c610-Abstract-Conference.html": {
    "title": "The Grand Illusion: The Myth of Software Portability and Implications for ML Progress",
    "volume": "main",
    "abstract": "Pushing the boundaries of machine learning often requires exploring different hardware and software combinations. However, this ability to experiment with different systems can be at odds with the drive for efficiency, which has produced increasingly specialized AI hardware and incentivized consolidation around a narrow set of ML frameworks. Exploratory research can be further restricted if software and hardware are co-evolving, making it even harder to stray away from a given tooling stack. While this friction increasingly impacts the rate of innovation in machine learning, to our knowledge the lack of portability in tooling has not been quantified. In this work we ask: How portable are popular ML software frameworks? We conduct a large scale study of the portability of mainstream ML frameworks across different hardware types. Our findings paint an uncomfortable picture -- frameworks can lose more than 40% of their key functions when ported to other hardware. Worse, even when functions are portable, the slowdown in their performance can be extreme. Collectively, our results reveal how costly straying from a narrow set of hardware-software combinations can be - and thus how specialization incurs an exploration cost that can impede innovation in machine learning research",
    "keywords": [],
    "checked": true,
    "id": "87ed9604338cca1c075b8988b43864781b32a0d0",
    "semantic_title": "the grand illusion: the myth of software portability and implications for ml progress",
    "citation_count": 2,
    "authors": [
      "Fraser Mince",
      "Dzung Dinh",
      "Jonas Kgomo",
      "Neil Thompson",
      "Sara Hooker"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42cac45fb00f7038c892f1a1bfc216d3-Abstract-Conference.html": {
    "title": "Computing Optimal Nash Equilibria in Multiplayer Games",
    "volume": "main",
    "abstract": "Designing efficient algorithms to compute a Nash Equilibrium (NE) in multiplayer games is still an open challenge. In this paper, we focus on computing an NE that optimizes a given objective function. For example, when there is a team of players independently playing against an adversary in a game (e.g., several groups in a forest trying to interdict illegal loggers in green security games), these team members may need to find an NE minimizing the adversary's utility. Finding an optimal NE in multiplayer games can be formulated as a mixed-integer bilinear program by introducing auxiliary variables to represent bilinear terms, leading to a huge number of bilinear terms, making it hard to solve. To overcome this challenge, we first propose a general framework for this formulation based on a set of correlation plans. We then develop a novel algorithm called CRM based on this framework, which uses correlation plans with their relations to strictly reduce the feasible solution space after the convex relaxation of bilinear terms while minimizing the number of correlation plans to significantly reduce the number of bilinear terms. We show that our techniques can significantly reduce the time complexity and CRM can be several orders of magnitude faster than the state-of-the-art baseline",
    "keywords": [],
    "checked": true,
    "id": "537eb0b61085c6e93e270e5151d70f1adf6a2a6b",
    "semantic_title": "computing optimal nash equilibria in multiplayer games",
    "citation_count": 0,
    "authors": [
      "Youzhi Zhang",
      "Bo An",
      "Venkatramanan Subrahmanian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42eb37cdbefd7abae0835f4b67548c39-Abstract-Conference.html": {
    "title": "AND: Adversarial Neural Degradation for Learning Blind Image Super-Resolution",
    "volume": "main",
    "abstract": "Learnt deep neural networks for image super-resolution fail easily if the assumed degradation model in training mismatches that of the real degradation source at the inference stage. Instead of attempting to exhaust all degradation variants in simulation, which is unwieldy and impractical, we propose a novel adversarial neural degradation (AND) model that can, when trained in conjunction with a deep restoration neural network under a minmax criterion, generate a wide range of highly nonlinear complex degradation effects without any explicit supervision. The AND model has a unique advantage over the current state of the art in that it can generalize much better to unseen degradation variants and hence deliver significantly improved restoration performance on real-world images",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangzhou Luo",
      "Xiaolin Wu",
      "Yanhui Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/43069caa6776eac8bca4bfd74d4a476d-Abstract-Conference.html": {
    "title": "SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "In this paper, we introduce an SE(3) diffusion model-based point cloud registration framework for 6D object pose estimation in real-world scenarios. Our approach formulates the 3D registration task as a denoising diffusion process, which progressively refines the pose of the source point cloud to obtain a precise alignment with the model point cloud. Training our framework involves two operations: An SE(3) diffusion process and an SE(3) reverse process. The SE(3) diffusion process gradually perturbs the optimal rigid transformation of a pair of point clouds by continuously injecting noise (perturbation transformation). By contrast, the SE(3) reverse process focuses on learning a denoising network that refines the noisy transformation step-by-step, bringing it closer to the optimal transformation for accurate pose estimation. Unlike standard diffusion models used in linear Euclidean spaces, our diffusion model operates on the SE(3) manifold. This requires exploiting the linear Lie algebra $\\mathfrak{se}(3)$ associated with SE(3) to constrain the transformation transitions during the diffusion and reverse processes. Additionally, to effectively train our denoising network, we derive a registration-specific variational lower bound as the optimization objective for model learning. Furthermore, we show that our denoising network can be constructed with a surrogate registration model, making our approach applicable to different deep registration networks. Extensive experiments demonstrate that our diffusion registration framework presents outstanding pose estimation performance on the real-world TUD-L, LINEMOD, and Occluded-LINEMOD datasets",
    "keywords": [],
    "checked": true,
    "id": "f8a45226715b20deb6618b5f21e307c02cdf8777",
    "semantic_title": "se(3) diffusion model-based point cloud registration for robust 6d object pose estimation",
    "citation_count": 0,
    "authors": [
      "Haobo Jiang",
      "Mathieu Salzmann",
      "Zheng Dang",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/431d53d513461ff155d5bc8faa9a440c-Abstract-Conference.html": {
    "title": "Fed-CO$_{2}$: Cooperation of Online and Offline Models for Severe Data Heterogeneity in Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) has emerged as a promising distributed learning paradigm that enables multiple clients to learn a global model collaboratively without sharing their private data. However, the effectiveness of FL is highly dependent on the quality of the data that is being used for training. In particular, data heterogeneity issues, such as label distribution skew and feature skew, can significantly impact the performance of FL. Previous studies in FL have primarily focused on addressing label distribution skew data heterogeneity, while only a few recent works have made initial progress in tackling feature skew issues. Notably, these two forms of data heterogeneity have been studied separately and have not been well explored within a unified FL framework. To address this gap, we propose Fed-CO$_2$, a universal FL framework that handles both label distribution skew and feature skew within a Cooperation mechanism between the Online and Offline models. Specifically, the online model learns general knowledge that is shared among all clients, while the offline model is trained locally to learn the specialized knowledge of each individual client. To further enhance model cooperation in the presence of feature shifts, we design an intra-client knowledge transfer mechanism that reinforces mutual learning between the online and offline models, and an inter-client knowledge transfer mechanism to increase the models' domain generalization ability. Extensive experiments show that our Fed-CO$_2$ outperforms a wide range of existing personalized federated learning algorithms in terms of handling label distribution skew and feature skew, both individually and collectively. The empirical results are supported by our convergence analyses in a simplified setting",
    "keywords": [],
    "checked": false,
    "id": "236944622100a48e4841940e93571bb329491133",
    "semantic_title": "fed-co2: cooperation of online and offline models for severe data heterogeneity in federated learning",
    "citation_count": 0,
    "authors": [
      "Zhongyi Cai",
      "Ye Shi",
      "Wei Huang",
      "Jingya Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/435986a8cc3e0667648df5d1c2d55c83-Abstract-Conference.html": {
    "title": "Combating Bilateral Edge Noise for Robust Link Prediction",
    "volume": "main",
    "abstract": "Although link prediction on graphs has achieved great success with the development of graph neural networks (GNNs), the potential robustness under the edge noise is still less investigated. To close this gap, we first conduct an empirical study to disclose that the edge noise bilaterally perturbs both input topology and target label, yielding severe performance degradation and representation collapse. To address this dilemma, we propose an information-theory-guided principle, Robust Graph Information Bottleneck (RGIB), to extract reliable supervision signals and avoid representation collapse. Different from the basic information bottleneck, RGIB further decouples and balances the mutual dependence among graph topology, target labels, and representation, building new learning objectives for robust representation against the bilateral noise. Two instantiations, RGIB-SSL and RGIB-REP, are explored to leverage the merits of different methodologies, i.e., self-supervised learning and data reparameterization, for implicit and explicit data denoising, respectively. Extensive experiments on six datasets and three GNNs with diverse noisy scenarios verify the effectiveness of our RGIB instantiations. The code is publicly available at: https://github.com/tmlr-group/RGIB",
    "keywords": [],
    "checked": true,
    "id": "76210786f024577acf636777cd1f668eb5a4a710",
    "semantic_title": "combating bilateral edge noise for robust link prediction",
    "citation_count": 2,
    "authors": [
      "Zhanke Zhou",
      "Jiangchao Yao",
      "Jiaxu Liu",
      "Xiawei Guo",
      "Quanming Yao",
      "LI He",
      "Liang Wang",
      "Bo Zheng",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/435e8fbbfc2c6072d4f3a5cb6e56a39a-Abstract-Conference.html": {
    "title": "SyncTREE: Fast Timing Analysis for Integrated Circuit Design through a Physics-informed Tree-based Graph Neural Network",
    "volume": "main",
    "abstract": "Nowadays integrated circuits (ICs) are underpinning all major information technology innovations including the current trends of artificial intelligence (AI). Modern IC designs often involve analyses of complex phenomena (such as timing, noise, and power etc.) for tens of billions of electronic components, like resistance (R), capacitance (C), transistors and gates, interconnected in various complex structures. Those analyses often need to strike a balance between accuracy and speed as those analyses need to be carried out many times throughout the entire IC design cycles. With the advancement of AI, researchers also start to explore news ways in leveraging AI to improve those analyses. This paper focuses on one of the most important analyses, timing analysis for interconnects. Since IC interconnects can be represented as an RC-tree, a specialized graph as tree, we design a novel tree-based graph neural network, SyncTREE, to speed up the timing analysis by incorporating both the structural and physical properties of electronic circuits. Our major innovations include (1) a two-pass message-passing (bottom-up and top-down) for graph embedding, (2) a tree contrastive loss to guide learning, and (3) a closed formular-based approach to conduct fast timing. Our experiments show that, compared to conventional GNN models, SyncTREE achieves the best timing prediction in terms of both delays and slews, all in reference to the industry golden numerical analyses results on real IC design data",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Hu",
      "Jiajie Li",
      "Florian Klemme",
      "Gi-Joon Nam",
      "Tengfei Ma",
      "Hussam Amrouch",
      "Jinjun Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/43663f64775ae439ec52b64305d219d3-Abstract-Conference.html": {
    "title": "Hierarchical Open-vocabulary Universal Image Segmentation",
    "volume": "main",
    "abstract": "Open-vocabulary image segmentation aims to partition an image into semantic regions according to arbitrary text descriptions. However, complex visual scenes can be naturally decomposed into simpler parts and abstracted at multiple lev4 els of granularity, introducing inherent segmentation ambiguity. Unlike existing methods that typically sidestep this ambiguity and treat it as an external factor, our approach actively incorporates a hierarchical representation encompassing different semantic-levels into the learning process. We propose a decoupled text-image fusion mechanism and representation learning modules for both \"things\" and \"stuff\". Additionally, we systematically examine the differences that exist in the textual and visual features between these types of categories. Our resulting model, named HIPIE, tackles HIerarchical, oPen-vocabulary, and unIvErsal segmentation tasks within a unified framework. Benchmarked on diverse datasets, e.g., ADE20K,COCO, Pascal-VOC Part, and RefCOCO/RefCOCOg, HIPIE achieves the state-of14 the-art results at various levels of image comprehension, including semantic-level (e.g., semantic segmentation), instance-level (e.g., panoptic/referring segmentationand object detection), as well as part-level (e.g., part/subpart segmentation) tasks",
    "keywords": [],
    "checked": true,
    "id": "0d358b0d06b5daa388c1c3dcf08de43d4a50279e",
    "semantic_title": "hierarchical open-vocabulary universal image segmentation",
    "citation_count": 7,
    "authors": [
      "Xudong Wang",
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Trevor Darrell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/436d042b2dd81214d23ae43eb196b146-Abstract-Conference.html": {
    "title": "Fairly Recommending with Social Attributes: A Flexible and Controllable Optimization Approach",
    "volume": "main",
    "abstract": "Item-side group fairness (IGF) requires a recommendation model to treat different item groups similarly, and has a crucial impact on information diffusion, consumption activity, and market equilibrium. Previous IGF notions only focus on the direct utility of the item exposures, i.e., the exposure numbers across different item groups. Nevertheless, the item exposures also facilitate utility gained from the neighboring users via social influence, called social utility, such as information sharing on the social media. To fill this gap, this paper introduces two social attribute-aware IGF metrics, which require similar user social attributes on the exposed items across the different item groups. In light of the trade-off between the direct utility and social utility, we formulate a new multi-objective optimization problem for training recommender models with flexible trade-off while ensuring controllable accuracy. To solve this problem, we develop a gradient-based optimization algorithm and theoretically show that the proposed algorithm can find Pareto optimal solutions with varying trade-off and guaranteed accuracy. Extensive experiments on two real-world datasets validate the effectiveness of our approach",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqiu Jin",
      "Haoxuan Li",
      "Fuli Feng",
      "Sihao Ding",
      "Peng Wu",
      "Xiangnan He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/437cd2749391ad40f67e4dd1d87c4596-Abstract-Conference.html": {
    "title": "Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos",
    "volume": "main",
    "abstract": "The analysis and use of egocentric videos for robotics tasks is made challenging by occlusion and the visual mismatch between the human hand and a robot end-effector. Past work views the human hand as a nuisance and removes it from the scene. However, the hand also provides a valuable signal for learning. In this work, we propose to extract a factored representation of the scene that separates the agent (human hand) and the environment. This alleviates both occlusion and mismatch while preserving the signal, thereby easing the design of models for downstream robotics tasks. At the heart of this factorization is our proposed Video Inpainting via Diffusion Model (VIDM) that leverages both a prior on real-world images (through a large-scale pre-trained diffusion model) and the appearance of the object in earlier frames of the video (through attention). Our experiments demonstrate the effectiveness of VIDM at improving the in-painting quality in egocentric videos and the power of our factored representation for numerous tasks: object detection, 3D reconstruction of manipulated objects, and learning of reward functions, policies, and affordances from videos",
    "keywords": [],
    "checked": true,
    "id": "26dc7a63c0660d6861de980f26a53c7fc2ee0d5f",
    "semantic_title": "look ma, no hands! agent-environment factorization of egocentric videos",
    "citation_count": 3,
    "authors": [
      "Matthew Chang",
      "Aditya Prakash",
      "Saurabh Gupta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/43a69d143273bd8215578bde887bb552-Abstract-Conference.html": {
    "title": "Generating Images with Multimodal Language Models",
    "volume": "main",
    "abstract": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text — outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence",
    "keywords": [],
    "checked": true,
    "id": "6fb5c0eff3696ef252aca9638e10176ecce7cecb",
    "semantic_title": "generating images with multimodal language models",
    "citation_count": 62,
    "authors": [
      "Jing Yu Koh",
      "Daniel Fried",
      "Russ R. Salakhutdinov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/43b77cef2a83a25aa27d3271d209e4fd-Abstract-Conference.html": {
    "title": "MoVie: Visual Model-Based Policy Adaptation for View Generalization",
    "volume": "main",
    "abstract": "Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\\textbf{Mo}$del-based policies for $\\textbf{Vie}$w generalization ($\\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\\mathbf{33}$%, $\\mathbf{86}$%, and $\\mathbf{152}$% respectively. The superior results highlight the immense potential of our approach for real-world robotics applications. Code and videos are available at https://yangsizhe.github.io/MoVie/",
    "keywords": [],
    "checked": true,
    "id": "41dc40dc47dcd3ba65044395b20973b155233ff7",
    "semantic_title": "movie: visual model-based policy adaptation for view generalization",
    "citation_count": 2,
    "authors": [
      "Sizhe Yang",
      "Yanjie Ze",
      "Huazhe Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/43ba0466af2b1ac76aa85d8fbec714e3-Abstract-Conference.html": {
    "title": "Does Visual Pretraining Help End-to-End Reasoning?",
    "volume": "main",
    "abstract": "We aim to investigate whether end-to-end learning of visual reasoning can be achieved with general-purpose neural networks, with the help of visual pretraining. A positive result would refute the common belief that explicit visual abstraction (e.g. object detection) is essential for compositional generalization on visual reasoning, and confirm the feasibility of a neural network ''generalist'' to solve visual recognition and reasoning tasks. We propose a simple and general self-supervised framework which ''compresses'' each video frame into a small set of tokens with a transformer network, and reconstructs the remaining frames based on the compressed temporal context. To minimize the reconstruction loss, the network must learn a compact representation for each image, as well as capture temporal dynamics and object permanence from temporal context. We perform evaluation on two visual reasoning benchmarks, CATER and ACRE. We observe that pretraining is essential to achieve compositional generalization for end-to-end visual reasoning. Our proposed framework outperforms traditional supervised pretraining, including image classification and explicit object detection, by large margins",
    "keywords": [],
    "checked": true,
    "id": "929de208dc5f275117bf992d12af99206109f240",
    "semantic_title": "does visual pretraining help end-to-end reasoning?",
    "citation_count": 1,
    "authors": [
      "Chen Sun",
      "Calvin Luo",
      "Xingyi Zhou",
      "Anurag Arnab",
      "Cordelia Schmid"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/43e8fd8b9581faa71a6a61602bc28435-Abstract-Conference.html": {
    "title": "Newton–Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems",
    "volume": "main",
    "abstract": "Reasoning system dynamics is one of the most important analytical approaches for many scientific studies. With the initial state of a system as input, the recent graph neural networks (GNNs)-based methods are capable of predicting the future state distant in time with high accuracy. Although these methods have diverse designs in modeling the coordinates and interacting forces of the system, we show that they actually share a common paradigm that learns the integration of the velocity over the interval between the initial and terminal coordinates. However, their integrand is constant w.r.t. time. Inspired by this observation, we propose a new approach to predict the integration based on several velocity estimations with Newton–Cotes formulas and prove its effectiveness theoretically. Extensive experiments on several benchmarks empirically demonstrate consistent and significant improvement compared with the state-of-the-art methods",
    "keywords": [],
    "checked": false,
    "id": "4ada6508596ccede68a5ac96b2c0c45cec39d979",
    "semantic_title": "newton-cotes graph neural networks: on the time evolution of dynamic systems",
    "citation_count": 0,
    "authors": [
      "Lingbing Guo",
      "Weiqing Wang",
      "Zhuo Chen",
      "Ningyu Zhang",
      "Zequn Sun",
      "Yixuan Lai",
      "Qiang Zhang",
      "Huajun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html": {
    "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
    "volume": "main",
    "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus – a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research",
    "keywords": [],
    "checked": true,
    "id": "b45ec1cb2ba6b2d1ac24723fa836aee06a3db97a",
    "semantic_title": "is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
    "citation_count": 123,
    "authors": [
      "Jiawei Liu",
      "Chunqiu Steven Xia",
      "Yuyao Wang",
      "LINGMING ZHANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/444b09beab8438d4a58e9bc694dca32a-Abstract-Conference.html": {
    "title": "Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections",
    "volume": "main",
    "abstract": "Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models. Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering' in vision models. First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further, these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space. We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Talia Konkle",
      "George Alvarez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4459c3c143db74ee52afebdf56836375-Abstract-Conference.html": {
    "title": "Neuro-symbolic Learning Yielding Logical Constraints",
    "volume": "main",
    "abstract": "Neuro-symbolic systems combine the abilities of neural perception and logical reasoning. However, end-to-end learning of neuro-symbolic systems is still an unsolved challenge. This paper proposes a natural framework that fuses neural network training, symbol grounding, and logical constraint synthesis into a coherent and efficient end-to-end learning process. The capability of this framework comes from the improved interactions between the neural and the symbolic parts of the system in both the training and inference stages. Technically, to bridge the gap between the continuous neural network and the discrete logical constraint, we introduce a difference-of-convex programming technique to relax the logical constraints while maintaining their precision. We also employ cardinality constraints as the language for logical constraint learning and incorporate a trust region method to avoid the degeneracy of logical constraint in learning. Both theoretical analyses and empirical evaluations substantiate the effectiveness of the proposed framework",
    "keywords": [],
    "checked": false,
    "id": "a0fd6cff760a8d563fdf2f59ce4588900ddca005",
    "semantic_title": "neuro-symbolic rule learning in real-world classification tasks",
    "citation_count": 1,
    "authors": [
      "Zenan Li",
      "Yunpeng Huang",
      "Zhaoyu Li",
      "Yuan Yao",
      "Jingwei Xu",
      "Taolue Chen",
      "Xiaoxing Ma",
      "Jian Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4462db5eee6823b2abad0d1f955e187a-Abstract-Conference.html": {
    "title": "Exploiting Connections between Lipschitz Structures for Certifiably Robust Deep Equilibrium Models",
    "volume": "main",
    "abstract": "Recently, deep equilibrium models (DEQs) have drawn increasing attention from the machine learning community. However, DEQs are much less understood in terms of certified robustness than their explicit network counterparts. In this paper, we advance the understanding of certified robustness of DEQs via exploiting the connections between various Lipschitz network parameterizations for both explicit and implicit models. Importantly, we show that various popular Lipschitz network structures, including convex potential layers (CPL), SDP-based Lipschitz layers (SLL), almost orthogonal layers (AOL), Sandwich layers, and monotone DEQs (MonDEQ) can all be reparameterized as special cases of the Lipschitz-bounded equilibrium networks (LBEN) without changing the prescribed Lipschitz constant in the original network parameterization. A key feature of our reparameterization technique is that it preserves the Lipschitz prescription used in different structures. This opens the possibility of achieving improved certified robustness of DEQs via a combination of network reparameterization, structure-preserving regularization, and LBEN-based fine-tuning. We also support our theoretical understanding with new empirical results, which show that our proposed method improves the certified robust accuracy of DEQs on classification tasks. All codes and experiments are made available at \\url{https://github.com/AaronHavens/ExploitingLipschitzDEQ}",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron Havens",
      "Alexandre Araujo",
      "Siddharth Garg",
      "Farshad Khorrami",
      "Bin Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/448444518637da106d978ae7409d9789-Abstract-Conference.html": {
    "title": "A Combinatorial Algorithm for Approximating the Optimal Transport in the Parallel and MPC Settings",
    "volume": "main",
    "abstract": "Optimal Transport is a popular distance metric for measuring similarity between distributions. Exact and approximate combinatorial algorithms for computing the optimal transport distance are hard to parallelize. This has motivated the development of numerical solvers (e.g. Sinkhorn method) that can exploit GPU parallelism and produce approximate solutions. We introduce the first parallel combinatorial algorithm to find an additive $\\varepsilon$-approximation of the OT distance. The parallel complexity of our algorithm is $O(\\log(n)/ \\varepsilon^2)$ where $n$ is the total support size for the input distributions. In Massive Parallel Computation (MPC) frameworks such as Hadoop and MapReduce, our algorithm computes an $\\varepsilon$-approximate transport plan in $O(\\log (\\log (n/\\varepsilon))/\\varepsilon^2)$ rounds with $O(n/\\varepsilon)$ space per machine; all prior algorithms in the MPC framework take $\\Omega(\\log n)$ rounds. We also provide a GPU-friendly matrix-based interpretation of our algorithm where each step of the algorithm is row or column manipulation of the matrix. Experiments suggest that our combinatorial algorithm is faster than the state-of-the-art approximate solvers in the GPU, especially for higher values of $n$",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathaniel Lahn",
      "Sharath Raghvendra",
      "Kaiyi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4488bf8354049b1cd592b6418dc30466-Abstract-Conference.html": {
    "title": "RegBN: Batch Normalization of Multimodal Data with Regularization",
    "volume": "main",
    "abstract": "Recent years have witnessed a surge of interest in integrating high-dimensional data captured by multisource sensors, driven by the impressive success of neural networks in integrating multimodal data. However, the integration of heterogeneous multimodal data poses a significant challenge, as confounding effects and dependencies among such heterogeneous data sources introduce unwanted variability and bias, leading to suboptimal performance of multimodal models. Therefore, it becomes crucial to normalize the low- or high-level features extracted from data modalities before their fusion takes place. This paper introduces RegBN, a novel approach for multimodal Batch Normalization with REGularization. RegBN uses the Frobenius norm as a regularizer term to address the side effects of confounders and underlying dependencies among different data sources. The proposed method generalizes well across multiple modalities and eliminates the need for learnable parameters, simplifying training and inference. We validate the effectiveness of RegBN on eight databases from five research areas, encompassing diverse modalities such as language, audio, image, video, depth, tabular, and 3D MRI. The proposed method demonstrates broad applicability across different architectures such as multilayer perceptrons, convolutional neural networks, and vision transformers, enabling effective normalization of both low- and high-level features in multimodal neural networks. RegBN is available at https://mogvision.github.io/RegBN",
    "keywords": [],
    "checked": true,
    "id": "ab8f8c7c62a9c1ab2bf18b6afe074d87a2206e2d",
    "semantic_title": "regbn: batch normalization of multimodal data with regularization",
    "citation_count": 0,
    "authors": [
      "Morteza Ghahremani Boozandani",
      "Christian Wachinger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/44956951349095f74492a5471128a7e0-Abstract-Conference.html": {
    "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code will be made public",
    "keywords": [],
    "checked": true,
    "id": "017010b941d902a467f6d329ae5e74fd67e67912",
    "semantic_title": "llm-pruner: on the structural pruning of large language models",
    "citation_count": 58,
    "authors": [
      "Xinyin Ma",
      "Gongfan Fang",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/449a016a6ce6fba3fe50d05482abf836-Abstract-Conference.html": {
    "title": "Nearly Optimal VC-Dimension and Pseudo-Dimension Bounds for Deep Neural Network Derivatives",
    "volume": "main",
    "abstract": "This paper addresses the problem of nearly optimal Vapnik--Chervonenkis dimension (VC-dimension) and pseudo-dimension estimations of the derivative functions of deep neural networks (DNNs). Two important applications of these estimations include: 1) Establishing a nearly tight approximation result of DNNs in the Sobolev space; 2) Characterizing the generalization error of machine learning methods with loss functions involving function derivatives. This theoretical investigation fills the gap of learning error estimations for a wide range of physics-informed machine learning models and applications including generative models, solving partial differential equations, operator learning, network compression, distillation, regularization, etc",
    "keywords": [],
    "checked": true,
    "id": "185b4543c0781d4ddf608c282ef5e208dbdb164d",
    "semantic_title": "nearly optimal vc-dimension and pseudo-dimension bounds for deep neural network derivatives",
    "citation_count": 5,
    "authors": [
      "Yahong Yang",
      "Haizhao Yang",
      "Yang Xiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/44c150733f9c5b6f98cb0caad0c664c7-Abstract-Conference.html": {
    "title": "Near-Optimal Bounds for Learning Gaussian Halfspaces with Random Classification Noise",
    "volume": "main",
    "abstract": "We study the problem of learning general (i.e., not necessarily homogeneous) halfspaces with Random Classification Noise under the Gaussian distribution. We establish nearly-matching algorithmic and Statistical Query (SQ) lower bound results revealing a surprising information-computation gap for this basic problem. Specifically, the sample complexity of this learning problem is $\\widetilde{\\Theta}(d/\\epsilon)$, where $d$ is the dimension and $\\epsilon$ is the excess error. Our positive result is a computationally efficient learning algorithm with sample complexity$\\tilde{O}(d/\\epsilon + d/\\max(p, \\epsilon))^2)$, where $p$ quantifies the bias of the target halfspace. On the lower bound side, we show that any efficient SQ algorithm (or low-degree test)for the problem requires sample complexity at least $\\Omega(d^{1/2}/(\\max(p, \\epsilon))^2)$. Our lower bound suggests that this quadratic dependence on $1/\\epsilon$ is inherent for efficient algorithms",
    "keywords": [],
    "checked": true,
    "id": "6523426e8b5eebd191f7ade203871f6d90b64fff",
    "semantic_title": "near-optimal bounds for learning gaussian halfspaces with random classification noise",
    "citation_count": 1,
    "authors": [
      "Ilias Diakonikolas",
      "Jelena Diakonikolas",
      "Daniel Kane",
      "Puqian Wang",
      "Nikos Zarifis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/44cdeb5ab7da31d9b5cd88fd44e3da84-Abstract-Conference.html": {
    "title": "Explain Any Concept: Segment Anything Meets Concept-Based Explanation",
    "volume": "main",
    "abstract": "EXplainable AI (XAI) is an essential topic to improve human understanding of deep neural networks (DNNs) given their black-box internals. For computer vision tasks, mainstream pixel-based XAI methods explain DNN decisions by identifying important pixels, and emerging concept-based XAI explore forming explanations with concepts (e.g., a head in an image). However, pixels are generally hard to interpret and sensitive to the imprecision of XAI methods, whereas \"concepts\" in prior works require human annotation or are limited to pre-defined concept sets. On the other hand, driven by large-scale pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promotable framework for performing precise and comprehensive instance segmentation, enabling automatic preparation of concept sets from a given image. This paper for the first time explores using SAM to augment concept-based XAI. We offer an effective and flexible concept-based explanation method, namely Explain Any Concept (EAC), which explains DNN decisions with any concept. While SAM is highly effective and offers an \"out-of-the-box\" instance segmentation, it is costly when being integrated into defacto XAI pipelines. We thus propose a lightweight per-input equivalent (PIE) scheme, enabling efficient explanation with a surrogate model. Our evaluation over two popular datasets (ImageNet and COCO) illustrate the highly encouraging performance of EAC over commonly-used XAI methods",
    "keywords": [],
    "checked": true,
    "id": "ede1078e40189190483554f2a84a547e9186872b",
    "semantic_title": "explain any concept: segment anything meets concept-based explanation",
    "citation_count": 7,
    "authors": [
      "Ao Sun",
      "Pingchuan Ma",
      "Yuanyuan Yuan",
      "Shuai Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/44ecfb60950e868a13172b935b7964a9-Abstract-Conference.html": {
    "title": "No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling",
    "volume": "main",
    "abstract": "For each of $T$ time steps, $m$ experts report probability distributions over $n$ outcomes; we wish to learn to aggregate these forecasts in a way that attains a no-regret guarantee. We focus on the fundamental and practical aggregation method known as *logarithmic pooling* -- a weighted average of log odds -- which is in a certain sense the optimal choice of pooling method if one is interested in minimizing log loss (as we take to be our loss function). We consider the problem of learning the best set of parameters (i.e. expert weights) in an online adversarial setting. We assume (by necessity) that the adversarial choices of outcomes and forecasts are consistent, in the sense that experts report calibrated forecasts. Imposing this constraint creates a (to our knowledge) novel semi-adversarial setting in which the adversary retains a large amount of flexibility. In this setting, we present an algorithm based on online mirror descent that learns expert weights in a way that attains $O(\\sqrt{T} \\log T)$ expected regret as compared with the best weights in hindsight",
    "keywords": [],
    "checked": true,
    "id": "2787ffed374d35ad0074951b1783a2bf50ffa694",
    "semantic_title": "no-regret learning with unbounded losses: the case of logarithmic pooling",
    "citation_count": 5,
    "authors": [
      "Eric Neyman",
      "Tim Roughgarden"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4522de4178bddb36b49aa26efad537cf-Abstract-Conference.html": {
    "title": "PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation",
    "volume": "main",
    "abstract": "Vision-and-Language Navigation requires the agent to follow language instructions to navigate through 3D environments. One main challenge in Vision-and-Language Navigation is the limited availability of photorealistic training environments, which makes it hard to generalize to new and unseen environments. To address this problem, we propose PanoGen, a generation method that can potentially create an infinite number of diverse panoramic environments conditioned on text. Specifically, we collect room descriptions by captioning the room images in existing Matterport3D environments, and leverage a state-of-the-art text-to-image diffusion model to generate the new panoramic environments. We use recursive outpainting over the generated images to create consistent 360-degree panorama views. Our new panoramic environments share similar semantic information with the original environments by conditioning on text descriptions, which ensures the co-occurrence of objects in the panorama follows human intuition, and creates enough diversity in room appearance and layout with image outpainting. Lastly, we explore two ways of utilizing PanoGen in VLN pre-training and fine-tuning. We generate instructions for paths in our PanoGen environments with a speaker built on a pre-trained vision-and-language model for VLN pre-training, and augment the visual observation with our panoramic environments during agents' fine-tuning to avoid overfitting to seen environments. Empirically, learning with our PanoGen environments achieves the new state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets. Besides, we find that pre-training with our PanoGen speaker data is especially effective for CVDN, which has under-specified instructions and needs commonsense knowledge to reach the target. Lastly, we show that the agent can benefit from training with more generated panoramic environments, suggesting promising results for scaling up the PanoGen environments to enhance agents' generalization to unseen environments",
    "keywords": [],
    "checked": true,
    "id": "d7f1a876b7df0e10627bccb0c5b63faf2a1005e4",
    "semantic_title": "panogen: text-conditioned panoramic environment generation for vision-and-language navigation",
    "citation_count": 7,
    "authors": [
      "Jialu Li",
      "Mohit Bansal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4533e4a352440a32558c1c227602c323-Abstract-Conference.html": {
    "title": "Scaling laws for language encoding models in fMRI",
    "volume": "main",
    "abstract": "Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales logarithmically with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models showed that performance is nearing the theoretical maximum for brain areas such as the precuneus and higher auditory cortex. These results suggest that increasing scale in both models and data will yield incredibly effective models of language processing in the brain, enabling better scientific understanding as well as applications such as decoding",
    "keywords": [],
    "checked": true,
    "id": "8376e50e81329b3db5049a90851cc0418d071e3d",
    "semantic_title": "scaling laws for language encoding models in fmri",
    "citation_count": 10,
    "authors": [
      "Richard Antonello",
      "Aditya Vaidya",
      "Alexander Huth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/45591d6727f0e127295f8d16adba6b23-Abstract-Conference.html": {
    "title": "Optimal Rates for Bandit Nonstochastic Control",
    "volume": "main",
    "abstract": "Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) control are foundational and extensively researched problems in optimal control. We investigate LQR and LQG problems with semi-adversarial perturbations and time-varying adversarial bandit loss functions. The best-known sublinear regret algorithm~\\cite{gradu2020non} has a $T^{\\frac{3}{4}}$ time horizon dependence, and its authors posed an open question about whether a tight rate of $\\sqrt{T}$ could be achieved. We answer in the affirmative, giving an algorithm for bandit LQR and LQG which attains optimal regret, up to logarithmic factors. A central component of our method is a new scheme for bandit convex optimization with memory, which is of independent interest",
    "keywords": [],
    "checked": true,
    "id": "c2eb79cd8e313ebeaf13992e92882b4a61c2a599",
    "semantic_title": "optimal rates for bandit nonstochastic control",
    "citation_count": 1,
    "authors": [
      "Y. Jennifer Sun",
      "Stephen Newman",
      "Elad Hazan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/456f9445d0fa1a932d19584ab788c787-Abstract-Conference.html": {
    "title": "Flow-Attention-based Spatio-Temporal Aggregation Network for 3D Mask Detection",
    "volume": "main",
    "abstract": "Anti-spoofing detection has become a necessity for face recognition systems due to the security threat posed by spoofing attacks. Despite great success in traditional attacks, most deep-learning-based methods perform poorly in 3D masks, which can highly simulate real faces in appearance and structure, suffering generalizability insufficiency while focusing only on the spatial domain with single frame input. This has been mitigated by the recent introduction of a biomedical technology called rPPG (remote photoplethysmography). However, rPPG-based methods are sensitive to noisy interference and require at least one second (> 25 frames) of observation time, which induces high computational overhead. To address these challenges, we propose a novel 3D mask detection framework, called FASTEN (Flow-Attention-based Spatio-Temporal aggrEgation Network). We tailor the network for focusing more on fine-grained details in large movements, which can eliminate redundant spatio-temporal feature interference and quickly capture splicing traces of 3D masks in fewer frames. Our proposed network contains three key modules: 1) a facial optical flow network to obtain non-RGB inter-frame flow information; 2) flow attention to assign different significance to each frame; 3) spatio-temporal aggregation to aggregate high-level spatial features and temporal transition features. Through extensive experiments, FASTEN only requires five frames of input and outperforms eight competitors for both intra-dataset and cross-dataset evaluations in terms of multiple detection metrics. Moreover, FASTEN has been deployed in real-world mobile devices for practical 3D mask detection",
    "keywords": [],
    "checked": true,
    "id": "b9997f748ddf3702942d56c8cd0d36e25c47ee8c",
    "semantic_title": "flow-attention-based spatio-temporal aggregation network for 3d mask detection",
    "citation_count": 0,
    "authors": [
      "Yuxin Cao",
      "Yian Li",
      "Yumeng Zhu",
      "Derui Wang",
      "Minhui Xue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/457ab261562014550e53351422f69834-Abstract-Conference.html": {
    "title": "On the Last-iterate Convergence in Time-varying Zero-sum Games: Extra Gradient Succeeds where Optimism Fails",
    "volume": "main",
    "abstract": "Last-iterate convergence has received extensive study in two player zero-sum games starting from bilinear, convex-concave up to settings that satisfy the MVI condition. Typical methods that exhibit last-iterate convergence for the aforementioned games include extra-gradient (EG) and optimistic gradient descent ascent (OGDA). However, all the established last-iterate convergence results hold for the restrictive setting where the underlying repeated game does not change over time.Recently, a line of research has focused on regret analysis of OGDA in time-varying games, i.e., games where payoffs evolve with time; the last-iterate behavior of OGDA and EG in time-varying environments remains unclear though. In this paper, we study the last-iterate behavior of various algorithms in two types of unconstrained, time-varying, bilinear zero-sum games: periodic and convergent perturbed games. These models expand upon the usual repeated game formulation and incorporate external environmental factors, such as the seasonal effects on species competition and vanishing external noise. In periodic games, we prove that EG will converge while OGDA and momentum method will diverge. This is quite surprising, as to the best of our knowledge, it is the first result that indicates EG and OGDA have qualitatively different last-iterate behaviors and do not exhibit similar behavior. In convergent perturbed games, we prove all these algorithms converge as long as the game itself stabilizes with a faster rate than $1/t$",
    "keywords": [],
    "checked": true,
    "id": "da5bd745f1081690b7863373473f36986b60057c",
    "semantic_title": "on the last-iterate convergence in time-varying zero-sum games: extra gradient succeeds where optimism fails",
    "citation_count": 2,
    "authors": [
      "Yi Feng",
      "Hu Fu",
      "Qun Hu",
      "Ping Li",
      "Ioannis Panageas",
      "bo peng",
      "Xiao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/458d9f2dd5c7565af60143630dc62f10-Abstract-Conference.html": {
    "title": "Taking the neural sampling code very seriously: A data-driven approach for evaluating generative models of the visual system",
    "volume": "main",
    "abstract": "Prevailing theories of perception hypothesize that the brain implements perception via Bayesian inference in a generative model of the world.One prominent theory, the Neural Sampling Code (NSC), posits that neuronal responses to a stimulus represent samples from the posterior distribution over latent world state variables that cause the stimulus.Although theoretically elegant, NSC does not specify the exact form of the generative model or prescribe how to link the theory to recorded neuronal activity.Previous works assume simple generative models and test their qualitative agreement with neurophysiological data.Currently, there is no precise alignment of the normative theory with neuronal recordings, especially in response to natural stimuli, and a quantitative, experimental evaluation of models under NSC has been lacking.Here, we propose a novel formalization of NSC, that (a) allows us to directly fit NSC generative models to recorded neuronal activity in response to natural images, (b) formulate richer and more flexible generative models, and (c) employ standard metrics to quantitatively evaluate different generative models under NSC.Furthermore, we derive a stimulus-conditioned predictive model of neuronal responses from the trained generative model using our formalization that we compare to neural system identification models.We demonstrate our approach by fitting and comparing classical- and flexible deep learning-based generative models on population recordings from the macaque primary visual cortex (V1) to natural images, and show that the flexible models outperform classical models in both their generative- and predictive-model performance.Overall, our work is an important step towards a quantitative evaluation of NSC. It provides a framework that lets us \\textit{learn} the generative model directly from neuronal population recordings, paving the way for an experimentally-informed understanding of probabilistic computational principles underlying perception and behavior",
    "keywords": [],
    "checked": true,
    "id": "c85f536a0dbbab41e2d679d3a85be54825c72421",
    "semantic_title": "taking the neural sampling code very seriously: a data-driven approach for evaluating generative models of the visual system",
    "citation_count": 0,
    "authors": [
      "Suhas Shrinivasan",
      "Konstantin-Klemens Lurz",
      "Kelli Restivo",
      "George Denfield",
      "Andreas Tolias",
      "Edgar Walker",
      "Fabian Sinz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/458fa8ee331566383d8e74bdb647f829-Abstract-Conference.html": {
    "title": "Can semi-supervised learning use all the data effectively? A lower bound perspective",
    "volume": "main",
    "abstract": "Prior theoretical and empirical works have established that semi-supervised learning algorithms can leverage the unlabeled data to improve over the labeled sample complexity of supervised learning (SL) algorithms. However, existing theoretical work focuses on regimes where the unlabeled data is sufficient to learn a good decision boundary using unsupervised learning (UL) alone. This begs the question: Can SSL algorithms simultaneously improve upon both UL and SL? To this end, we derive a tight lower bound for 2-Gaussian mixture models that explicitly depends on the labeled and the unlabeled dataset size as well as the signal-to-noise ratio of the mixture distribution. Surprisingly, our result implies that no SSL algorithm improves upon the minimax-optimal statistical error rates of SL or UL algorithms for these distributions. Nevertheless, in our real-world experiments, SSL algorithms can often outperform UL and SL algorithms. In summary, our work suggests that while it is possible to prove the performance gains of SSL algorithms, this would require careful tracking of constants in the theoretical analysis",
    "keywords": [],
    "checked": true,
    "id": "7676cd8561ebb67fc70e5453a8f2d9404004bb90",
    "semantic_title": "can semi-supervised learning use all the data effectively? a lower bound perspective",
    "citation_count": 0,
    "authors": [
      "Alexandru Tifrea",
      "Gizem Yüce",
      "Amartya Sanyal",
      "Fanny Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/459a911eb49cd2e0192055ee156d04e5-Abstract-Conference.html": {
    "title": "Evolving Standardization for Continual Domain Generalization over Temporal Drift",
    "volume": "main",
    "abstract": "The capability of generalizing to out-of-distribution data is crucial for the deployment of machine learning models in the real world. Existing domain generalization (DG) mainly embarks on offline and discrete scenarios, where multiple source domains are simultaneously accessible and the distribution shift among domains is abrupt and violent. Nevertheless, such setting may not be universally applicable to all real-world applications, as there are cases where the data distribution gradually changes over time due to various factors, e.g., the process of aging. Additionally, as the domain constantly evolves, new domains will continually emerge. Re-training and updating models with both new and previous domains using existing DG methods can be resource-intensive and inefficient. Therefore, in this paper, we present a problem formulation for Continual Domain Generalization over Temporal Drift (CDGTD). CDGTD addresses the challenge of gradually shifting data distributions over time, where domains arrive sequentially and models can only access the data of the current domain. The goal is to generalize to unseen domains that are not too far into the future. To this end, we propose an Evolving Standardization (EvoS) method, which characterizes the evolving pattern of feature distribution and mitigates the distribution shift by standardizing features with generated statistics of corresponding domain. Specifically, inspired by the powerful ability of transformers to model sequence relations, we design a multi-scale attention module (MSAM) to learn the evolving pattern under sliding time windows of different lengths. MSAM can generate statistics of current domain based on the statistics of previous domains and the learned evolving pattern. Experiments on multiple real-world datasets including images and texts validate the efficacy of our EvoS",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mixue Xie",
      "Shuang Li",
      "Longhui Yuan",
      "Chi Liu",
      "Zehui Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/45a7ca247462d9e465ee88c8a302ca70-Abstract-Conference.html": {
    "title": "Learning the Efficient Frontier",
    "volume": "main",
    "abstract": "The efficient frontier (EF) is a fundamental resource allocation problem where one has to find an optimal portfolio maximizing a reward at a given level of risk. This optimal solution is traditionally found by solving a convex optimization problem. In this paper, we introduce NeuralEF: a fast neural approximation framework that robustly forecasts the result of the EF convex optimizations problems with respect to heterogeneous linear constraints and variable number of optimization inputs. By reformulating an optimization problem as a sequence to sequence problem, we show that NeuralEF is a viable solution to accelerate large-scale simulation while handling discontinuous behavior",
    "keywords": [],
    "checked": true,
    "id": "b2608b0ab4207e1335df98853a23a68234233c1a",
    "semantic_title": "learning the efficient frontier",
    "citation_count": 0,
    "authors": [
      "Philippe Chatigny",
      "Ivan Sergienko",
      "Ryan Ferguson",
      "Jordan Weir",
      "Maxime Bergeron"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/45e15bae91a6f213d45e203b8a29be48-Abstract-Conference.html": {
    "title": "Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning",
    "volume": "main",
    "abstract": "Chain-of-thought (CoT) is a method that enables language models to handle complex reasoning tasks by decomposing them into simpler steps. Despite its success, the underlying mechanics of CoT are not yet fully understood. In an attempt to shed light on this, our study investigates the impact of CoT on the ability of transformers to in-context learn a simple to study, yet general family of compositional functions: multi-layer perceptrons (MLPs). In this setting, we find that the success of CoT can be attributed to breaking down in-context learning of a compositional function into two distinct phases: focusing on and filtering data related to each step of the composition and in-context learning the single-step composition function. Through both experimental and theoretical evidence, we demonstrate how CoT significantly reduces the sample complexity of in-context learning (ICL) and facilitates the learning of complex functions that non-CoT methods struggle with. Furthermore, we illustrate how transformers can transition from vanilla in-context learning to mastering a compositional function with CoT by simply incorporating additional layers that perform the necessary data-filtering for CoT via the attention mechanism. In addition to these test-time benefits, we show CoT helps accelerate pretraining by learning shortcuts to represent complex functions and filtering plays an important role in this process. These findings collectively provide insights into the mechanics of CoT, inviting further investigation of its role in complex reasoning tasks",
    "keywords": [],
    "checked": true,
    "id": "69bfa665e507fcee4a8d003933998eb89f336c9f",
    "semantic_title": "dissecting chain-of-thought: compositionality through in-context filtering and learning",
    "citation_count": 1,
    "authors": [
      "Yingcong Li",
      "Kartik Sreenivasan",
      "Angeliki Giannou",
      "Dimitris Papailiopoulos",
      "Samet Oymak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4621451c25a7aa175dc00e5dd4a243a3-Abstract-Conference.html": {
    "title": "Relative Entropic Optimal Transport: a (Prior-aware) Matching Perspective to (Unbalanced) Classification",
    "volume": "main",
    "abstract": "Classification is a fundamental problem in machine learning, and considerable efforts have been recently devoted to the demanding long-tailed setting due to its prevalence in nature. Departure from the Bayesian framework, this paper rethinks classification from a matching perspective by studying the matching probability between samples and labels with optimal transport (OT) formulation. Specifically, we first propose a new variant of optimal transport, called Relative Entropic Optimal Transport (RE-OT), which guides the coupling solution to a known prior information matrix. We gives some theoretical results and their proof for RE-OT and surprisingly find RE-OT can help to deblur for barycenter images. Then we adopt inverse RE-OT for training long-tailed data and find that the loss derived from RE-OT has a similar form to Softmax-based cross-entropy loss, indicating a close connection between optimal transport and classification and the potential for transferring concepts between these two academic fields, such as barycentric projection in OT, which can map the labels back to the feature space. We further derive an epoch-varying RE-OT loss, and do the experiments on unbalanced image classification, molecule classification, instance segmentation and representation learning. Experimental results show its effectiveness",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangliang Shi",
      "Haoyu Zhen",
      "Gu Zhang",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46362971bfc3a97e6a271f2eb90fba17-Abstract-Conference.html": {
    "title": "Connecting Multi-modal Contrastive Representations",
    "volume": "main",
    "abstract": "Multi-modal Contrastive Representation (MCR) learning aims to encode different modalities into a semantically aligned shared space. This paradigm shows remarkable generalization ability on numerous downstream tasks across various modalities. However, the reliance on massive high-quality data pairs limits its further development on more modalities. This paper proposes a novel training-efficient method for learning MCR without paired data called Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given two existing MCRs pre-trained on $(\\mathcal{A}$, $\\mathcal{B})$ and $(\\mathcal{B}$, $\\mathcal{C})$ modality pairs, we project them to a new space and use the data from the overlapping modality $\\mathcal{B}$ to aligning the two MCRs in the new space. Meanwhile, since the modality pairs $(\\mathcal{A}$, $\\mathcal{B})$ and $(\\mathcal{B}$, $\\mathcal{C})$ are already aligned within each MCR, the connection learned by overlapping modality can also be transferred to non-overlapping modality pair $(\\mathcal{A}$, $\\mathcal{C})$. To unleash the potential of C-MCR, we further introduce a semantic-enhanced inter- and intra-MCR connection method. We first enhance the semantic consistency and completion of embeddings across different modalities for more robust alignment. Then we utilize the inter-MCR alignment to establish the connection, and employ the intra-MCR alignment to better maintain the connection for inputs from non-overlapping modalities. To demonstrate the effectiveness of C-MCR, we take the field of audio-visual and 3D-language learning as examples. Specifically, we connect CLIP and CLAP via texts to derive audio-visual representations, and integrate CLIP and ULIP via images for 3D-language representations. Remarkably, without using any paired data, C-MCR for audio-visual achieves state-of-the-art performance on audio-image retrieval, audio-visual source localization, and counterfactual audio-image recognition tasks. Furthermore, C-MCR for 3D-language also attains advanced zero-shot 3D point cloud classification accuracy on ModelNet40. Our project page is available at \\url{https://c-mcr.github.io/C-MCR/}",
    "keywords": [],
    "checked": true,
    "id": "95863ab22c75eb38ec24cd5c077da92d9eea9cc8",
    "semantic_title": "connecting multi-modal contrastive representations",
    "citation_count": 3,
    "authors": [
      "Zehan Wang",
      "Yang Zhao",
      "Xize 成",
      "Haifeng Huang",
      "Jiageng Liu",
      "Aoxiong Yin",
      "Li Tang",
      "Linjun Li",
      "Yongqi Wang",
      "Ziang Zhang",
      "Zhou Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/463a91da3c832bd28912cd0d1b8d9974-Abstract-Conference.html": {
    "title": "Boosting Learning for LDPC Codes to Improve the Error-Floor Performance",
    "volume": "main",
    "abstract": "Low-density parity-check (LDPC) codes have been successfully commercialized in communication systems due to their strong error correction capabilities and simple decoding process. However, the error-floor phenomenon of LDPC codes, in which the error rate stops decreasing rapidly at a certain level, presents challenges for achieving extremely low error rates and deploying LDPC codes in scenarios demanding ultra-high reliability. In this work, we propose training methods for neural min-sum (NMS) decoders to eliminate the error-floor effect. First, by leveraging the boosting learning technique of ensemble networks, we divide the decoding network into two neural decoders and train the post decoder to be specialized for uncorrected words that the first decoder fails to correct. Secondly, to address the vanishing gradient issue in training, we introduce a block-wise training schedule that locally trains a block of weights while retraining the preceding block. Lastly, we show that assigning different weights to unsatisfied check nodes effectively lowers the error-floor with a minimal number of weights. By applying these training methods to standard LDPC codes, we achieve the best error-floor performance compared to other decoding methods. The proposed NMS decoder, optimized solely through novel training methods without additional modules, can be integrated into existing LDPC decoders without incurring extra hardware costs. The source code is available at https://github.com/ghy1228/LDPCErrorFloor",
    "keywords": [],
    "checked": true,
    "id": "b6349b90ff724023ad9281f539cecfd2e0f305bf",
    "semantic_title": "boosting learning for ldpc codes to improve the error-floor performance",
    "citation_count": 0,
    "authors": [
      "Hee-Youl Kwak",
      "Dae-Young Yun",
      "Yongjune Kim",
      "Sang-Hyo Kim",
      "Jong-Seon No"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/464012c83279e19be4cd42c25f341c92-Abstract-Conference.html": {
    "title": "Learning Score-based Grasping Primitive for Human-assisting Dexterous Grasping",
    "volume": "main",
    "abstract": "The use of anthropomorphic robotic hands for assisting individuals in situations where human hands may be unavailable or unsuitable has gained significant importance. In this paper, we propose a novel task called human-assisting dexterous grasping that aims to train a policy for controlling a robotic hand's fingers to assist users in grasping objects. Unlike conventional dexterous grasping, this task presents a more complex challenge as the policy needs to adapt to diverse user intentions, in addition to the object's geometry. We address this challenge by proposing an approach consisting of two sub-modules: a hand-object-conditional grasping primitive called Grasping Gradient Field (GraspGF), and a history-conditional residual policy. GraspGF learns 'how' to grasp by estimating the gradient of a synthesised success grasping example set, while the residual policy determines 'when' and at what speed the grasping action should be executed based on the trajectory history. Experimental results demonstrate the superiority of our proposed method compared to baselines, highlighting the user-awareness and practicality in real-world applications. The codes and demonstrations can be viewed at https://sites.google.com/view/graspgf",
    "keywords": [],
    "checked": true,
    "id": "a34085e633fc67ad6b2417302600a71c7b4e0eda",
    "semantic_title": "learning score-based grasping primitive for human-assisting dexterous grasping",
    "citation_count": 0,
    "authors": [
      "Tianhao Wu",
      "Mingdong Wu",
      "Jiyao Zhang",
      "Yunchong Gan",
      "Hao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4640d5da5888238b9de7e0dbacd2c605-Abstract-Conference.html": {
    "title": "Maximize to Explore: One Objective Function Fusing Estimation, Planning, and Exploration",
    "volume": "main",
    "abstract": "In reinforcement learning (RL), balancing exploration and exploitation is crucial for achieving an optimal policy in a sample-efficient way. To this end, existing sample- efficient algorithms typically consist of three components: estimation, planning, and exploration. However, to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as data-dependent level-set constraints or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called Maximize to Explore (MEX), which only needs to optimize unconstrainedly a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that the MEX achieves a sublinear regret with general function approximators and is extendable to the zero-sum Markov game setting. Meanwhile, we adapt deep RL baselines to design practical versions of MEX in both the model-based and model-free settings, which outperform baselines in various MuJoCo environments with sparse reward by a stable margin. Compared with existing sample-efficient algorithms with general function approximators, MEX achieves similar sample efficiency while also enjoying a lower computational cost and is more compatible with modern deep RL methods",
    "keywords": [],
    "checked": true,
    "id": "1311944e1ac408fd4a7829b254f25a6560b66e07",
    "semantic_title": "maximize to explore: one objective function fusing estimation, planning, and exploration",
    "citation_count": 3,
    "authors": [
      "Zhihan Liu",
      "Miao Lu",
      "WEI XIONG",
      "Han Zhong",
      "Hao Hu",
      "Shenao Zhang",
      "Sirui Zheng",
      "Zhuoran Yang",
      "Zhaoran Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4661b55200c03a8c4bb9c2974b4fb12d-Abstract-Conference.html": {
    "title": "Learning and Collusion in Multi-unit Auctions",
    "volume": "main",
    "abstract": "In a carbon auction, licenses for CO2 emissions are allocated among multiple interested players. Inspired by this setting, we consider repeated multi-unit auctions with uniform pricing, which are widely used in practice. Our contribution is to analyze these auctions in both the offline and online settings, by designing efficient bidding algorithms with low regret and giving regret lower bounds. We also analyze the quality of the equilibria in two main variants of the auction, finding that one variant is susceptible to collusion among the bidders while the other is not",
    "keywords": [],
    "checked": true,
    "id": "a7b4ffba50d613cfffc6f166bf20c0f0250d9367",
    "semantic_title": "learning and collusion in multi-unit auctions",
    "citation_count": 1,
    "authors": [
      "Simina Branzei",
      "Mahsa Derakhshan",
      "Negin Golrezaei",
      "Yanjun Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4683beb6bab325650db13afd05d1a14a-Abstract-Conference.html": {
    "title": "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization",
    "volume": "main",
    "abstract": "Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models",
    "keywords": [],
    "checked": true,
    "id": "c22ef963b2388cdbbfcc7a00b24f68710a7febd2",
    "semantic_title": "one-2-3-45: any single image to 3d mesh in 45 seconds without per-shape optimization",
    "citation_count": 97,
    "authors": [
      "Minghua Liu",
      "Chao Xu",
      "Haian Jin",
      "Linghao Chen",
      "Mukund Varma T",
      "Zexiang Xu",
      "Hao Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46907c2ff9fafd618095161d76461842-Abstract-Conference.html": {
    "title": "VeriX: Towards Verified Explainability of Deep Neural Networks",
    "volume": "main",
    "abstract": "We present VeriX (Verified eXplainability), a system for producing optimal robust explanations and generating counterfactuals along decision boundaries of machine learning models. We build such explanations and counterfactuals iteratively using constraint solving techniques and a heuristic based on feature-level sensitivity ranking. We evaluate our method on image recognition benchmarks and a real-world scenario of autonomous aircraft taxiing",
    "keywords": [],
    "checked": true,
    "id": "66ec306659b3c721952d255e2b721613a1764ef7",
    "semantic_title": "verix: towards verified explainability of deep neural networks",
    "citation_count": 5,
    "authors": [
      "Min Wu",
      "Haoze Wu",
      "Clark Barrett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46994b3d6dd0fd5fca5f780af6259db5-Abstract-Conference.html": {
    "title": "Generalized test utilities for long-tail performance in extreme multi-label classification",
    "volume": "main",
    "abstract": "Extreme multi-label classification (XMLC) is a task of selecting a small subset of relevant labels from a very large set of possible labels. As such, it is characterized by long-tail labels, i.e., most labels have very few positive instances. With standard performance measures such as precision@k, a classifier can ignore tail labels and still report good performance. However, it is often argued that correct predictions in the tail are more \"interesting\" or \"rewarding,\" but the community has not yet settled on a metric capturing this intuitive concept. The existing propensity-scored metrics fall short on this goal by confounding the problems of long-tail and missing labels. In this paper, we analyze generalized metrics budgeted \"at k\" as an alternative solution. To tackle the challenging problem of optimizing these metrics, we formulate it in the \\emph{expected test utility} (ETU) framework, which aims at optimizing the expected performance on a given test set. We derive optimal prediction rules and construct their computationally efficient approximations with provable regret guarantees and being robust against model misspecification. Our algorithm, based on block coordinate descent, scales effortlessly to XMLC problems and obtains promising results in terms of long-tail performance",
    "keywords": [],
    "checked": true,
    "id": "be065913eff790a4600f1dd9ba98ec04ee32dac5",
    "semantic_title": "generalized test utilities for long-tail performance in extreme multi-label classification",
    "citation_count": 0,
    "authors": [
      "Erik Schultheis",
      "Marek Wydmuch",
      "Wojciech Kotlowski",
      "Rohit Babbar",
      "Krzysztof Dembczynski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46a126492ea6fb87410e55a58df2e189-Abstract-Conference.html": {
    "title": "Compositional Foundation Models for Hierarchical Planning",
    "volume": "main",
    "abstract": "To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustrate the efficacy and adaptability of our approach in three different long-horizon table-top manipulation tasks",
    "keywords": [],
    "checked": true,
    "id": "024575aec54fb329eb6224684592d85b6672930d",
    "semantic_title": "compositional foundation models for hierarchical planning",
    "citation_count": 8,
    "authors": [
      "Anurag Ajay",
      "Seungwook Han",
      "Yilun Du",
      "Shuang Li",
      "Abhi Gupta",
      "Tommi Jaakkola",
      "Josh Tenenbaum",
      "Leslie Kaelbling",
      "Akash Srivastava",
      "Pulkit Agrawal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46ab9d9645b6975b947231ddb48da1ab-Abstract-Conference.html": {
    "title": "Diffusion Model for Graph Inverse Problems: Towards Effective Source Localization on Complex Networks",
    "volume": "main",
    "abstract": "Information diffusion problems, such as the spread of epidemics or rumors, are widespread in society. The inverse problems of graph diffusion, which involve locating the sources and identifying the paths of diffusion based on currently observed diffusion graphs, are crucial to controlling the spread of information. The problem of localizing the source of diffusion is highly ill-posed, presenting a major obstacle in accurately assessing the uncertainty involved. Besides, while comprehending how information diffuses through a graph is crucial, there is a scarcity of research on reconstructing the paths of information propagation. To tackle these challenges, we propose a probabilistic model called DDMSL (Discrete Diffusion Model for Source Localization). Our approach is based on the natural diffusion process of information propagation over complex networks, which can be formulated using a message-passing function. First, we model the forward diffusion of information using Markov chains. Then, we design a reversible residual network to construct a denoising-diffusion model in discrete space for both source localization and reconstruction of information diffusion paths. We provide rigorous theoretical guarantees for DDMSL and demonstrate its effectiveness through extensive experiments on five real-world datasets",
    "keywords": [],
    "checked": false,
    "id": "d4838211d7f65628f56b9f6faab30a95ff7b51f8",
    "semantic_title": "for prediction city region re-weighting",
    "citation_count": 0,
    "authors": [
      "Xin Yan",
      "Hui Fang",
      "Qiang He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46b065f7d301a15a23909f6cad409a97-Abstract-Conference.html": {
    "title": "UniT: A Unified Look at Certified Robust Training against Text Adversarial Perturbation",
    "volume": "main",
    "abstract": "Recent years have witnessed a surge of certified robust training pipelines against text adversarial perturbation constructed by synonym substitutions. Given a base model, existing pipelines provide prediction certificates either in the discrete word space or the continuous latent space. However, they are isolated from each other with a structural gap. We observe that existing training frameworks need unification to provide stronger certified robustness. Additionally, they mainly focus on building the certification process but neglect to improve the robustness of the base model. To mitigate the aforementioned limitations, we propose a unified framework named UniT that enables us to train flexibly in either fashion by working in the word embedding space. It can provide a stronger robustness guarantee obtained directly from the word embedding space without extra modules. In addition, we introduce the decoupled regularization (DR) loss to improve the robustness of the base model, which includes two separate robustness regularization terms for the feature extraction and classifier modules. Experimental results on widely used text classification datasets further demonstrate the effectiveness of the designed unified framework and the proposed DR loss for improving the certified robust accuracy",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muchao Ye",
      "Ziyi Yin",
      "Tianrong Zhang",
      "Tianyu Du",
      "Jinghui Chen",
      "Ting Wang",
      "Fenglong Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46c10f6c8ea5aa6f267bcdabcb123f97-Abstract-Conference.html": {
    "title": "Convergence of Alternating Gradient Descent for Matrix Factorization",
    "volume": "main",
    "abstract": "We consider alternating gradient descent (AGD) with fixed step size applied to the asymmetric matrix factorization objective. We show that, for a rank-$r$ matrix $A \\in \\mathbb{R}^{m \\times n}$, $T = C ( \\frac{\\sigma_1(A)}{\\sigma_r(A)} )^2 \\log(1/\\epsilon)$ iterations of alternating gradient descent suffice to reach an $\\epsilon$-optimal factorization $\\| A - X_{T} Y_{T}' \\|^2 \\leq \\epsilon \\| A \\|^2$ with high probability starting from an atypical random initialization. The factors have rank $d \\geq r$ so that $X_{T}\\in \\mathbb{R}^{m \\times d}$ and $Y_{T} \\in\\mathbb{R}^{n \\times d}$, and mild overparameterization suffices for the constant $C$ in the iteration complexity $T$ to be an absolute constant. Experiments suggest that our proposed initialization is not merely of theoretical benefit, but rather significantly improves the convergence rate of gradient descent in practice. Our proof is conceptually simple: a uniform Polyak-Lojasiewicz (PL) inequality and uniform Lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization. Our proof method should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems",
    "keywords": [],
    "checked": true,
    "id": "ab4184ba6f0e7bfb2eccc5f2e489b12df29c2982",
    "semantic_title": "convergence of alternating gradient descent for matrix factorization",
    "citation_count": 0,
    "authors": [
      "Rachel Ward",
      "Tamara Kolda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46c2a9a6f2b2be68682013eb1173c801-Abstract-Conference.html": {
    "title": "SPRING: Studying Papers and Reasoning to play Games",
    "volume": "main",
    "abstract": "Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read Crafter's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM).Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions.In our experiments, we study the quality of in-context \"reasoning\" induced by different forms of prompts under the setting of the Crafter environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of Crafter as a test bed for LLMs. Code at github.com/holmeswww/SPRING",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Wu",
      "So Yeon Min",
      "Shrimai Prabhumoye",
      "Yonatan Bisk",
      "Russ R. Salakhutdinov",
      "Amos Azaria",
      "Tom M. Mitchell",
      "Yuanzhi Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46d26daeb05fbbcfe5f3d8f7ca756e16-Abstract-Conference.html": {
    "title": "Hybrid Search for Efficient Planning with Completeness Guarantees",
    "volume": "main",
    "abstract": "Solving complex planning problems has been a long-standing challenge in computer science. Learning-based subgoal search methods have shown promise in tackling these problems, but they often suffer from a lack of completeness guarantees, meaning that they may fail to find a solution even if one exists. In this paper, we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces. Specifically, we augment the high-level search with low-level actions to execute a multi-level (hybrid) search, which we call complete subgoal search. This solution achieves the best of both worlds: the practical efficiency of high-level search and the completeness of low-level search. We apply the proposed search method to a recently proposed subgoal search algorithm and evaluate the algorithm trained on offline data on complex planning problems. We demonstrate that our complete subgoal search not only guarantees completeness but can even improve performance in terms of search expansions for instances that the high-level could solve without low-level augmentations. Our approach makes it possible to apply subgoal-level planning for systems where completeness is a critical requirement",
    "keywords": [],
    "checked": true,
    "id": "707d85c8ec94ced2cccc8cb242868d944f7a5163",
    "semantic_title": "hybrid search for efficient planning with completeness guarantees",
    "citation_count": 1,
    "authors": [
      "Kalle Kujanpää",
      "Joni Pajarinen",
      "Alexander Ilin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46d943bc6a15a57c923829efc0db7c7a-Abstract-Conference.html": {
    "title": "Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE",
    "keywords": [],
    "checked": true,
    "id": "7d1c7810b8ef57a43cd3eb0be197bd270f1c8dc3",
    "semantic_title": "diversified outlier exposure for out-of-distribution detection via informative extrapolation",
    "citation_count": 2,
    "authors": [
      "Jianing Zhu",
      "Yu Geng",
      "Jiangchao Yao",
      "Tongliang Liu",
      "Gang Niu",
      "Masashi Sugiyama",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/46e37aeccafc3b4b697b17b8a36f3b30-Abstract-Conference.html": {
    "title": "Attacks on Online Learners: a Teacher-Student Analysis",
    "volume": "main",
    "abstract": "Machine learning models are famously vulnerable to adversarial attacks: small ad-hoc perturbations of the data that can catastrophically alter the model predictions. While a large literature has studied the case of test-time attacks on pre-trained models, the important case of attacks in an online learning setting has received little attention so far. In this work, we use a control-theoretical perspective to study the scenario where an attacker may perturb data labels to manipulate the learning dynamics of an online learner. We perform a theoretical analysis of the problem in a teacher-student setup, considering different attack strategies, and obtaining analytical results for the steady state of simple linear learners. These results enable us to prove that a discontinuous transition in the learner's accuracy occurs when the attack strength exceeds a critical threshold. We then study empirically attacks on learners with complex architectures using real data, confirming the insights of our theoretical analysis. Our findings show that greedy attacks can be extremely efficient, especially when data stream in small batches",
    "keywords": [],
    "checked": true,
    "id": "6ec8c42880ef5f39f26db552dc66611dfbae0a29",
    "semantic_title": "attacks on online learners: a teacher-student analysis",
    "citation_count": 1,
    "authors": [
      "Riccardo Giuseppe Margiotta",
      "Sebastian Goldt",
      "Guido Sanguinetti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/470e23d14e330ab0daa5387916b95f9c-Abstract-Conference.html": {
    "title": "Delayed Algorithms for Distributed Stochastic Weakly Convex Optimization",
    "volume": "main",
    "abstract": "This paper studies delayed stochastic algorithms for weakly convex optimization in a distributed network with workers connected to a master node. Recently, Xu~et~al.~2022 showed that an inertial stochastic subgradient method converges at a rate of $\\mathcal{O}(\\tau_{\\text{max}}/\\sqrt{K})$ which depends on the maximum information delay $\\tau_{\\text{max}}$. In this work, we show that the delayed stochastic subgradient method ($\\texttt{DSGD}$) obtains a tighter convergence rate which depends on the expected delay $\\bar{\\tau}$. Furthermore, for an important class of composition weakly convex problems, we develop a new delayed stochastic prox-linear ($\\texttt{DSPL}$) method in which the delays only affect the high-order term in the rate and hence, are negligible after a certain number of $\\texttt{DSPL}$ iterations. In addition, we demonstrate the robustness of our proposed algorithms against arbitrary delays. By incorporating a simple safeguarding step in both methods, we achieve convergence rates that depend solely on the number of workers, eliminating the effect of delays. Our numerical experiments further confirm the empirical superiority of our proposed methods",
    "keywords": [],
    "checked": false,
    "id": "87352373e25cc16c4541ec711946952945029beb",
    "semantic_title": "delayed stochastic algorithms for distributed weakly convex optimization",
    "citation_count": 0,
    "authors": [
      "Wenzhi Gao",
      "Qi Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/47167991e38c65a72914763c11cd8d23-Abstract-Conference.html": {
    "title": "Grounding Neural Inference with Satisfiability Modulo Theories",
    "volume": "main",
    "abstract": "Recent techniques that integrate solver layers into Deep Neural Networks (DNNs) have shown promise in bridging a long-standing gap between inductive learning and symbolic reasoning techniques. In this paper we present a set of techniques for integrating Satisfiability Modulo Theories (SMT) solvers into the forward and backward passes of a deep network layer, called SMTLayer.Using this approach, one can encode rich domain knowledge into the network in the form of mathematical formulas.In the forward pass, the solver uses symbols produced by prior layers, along with these formulas, to construct inferences; in the backward pass, the solver informs updates to the network, driving it towards representations that are compatible with the solver's theory.Notably, the solver need not be differentiable. We implement SMTLayer as a Pytorch module, and our empirical results show that it leads to models that 1) require fewer training samples than conventional models, 2) that are robust to certain types of covariate shift, and 3) that ultimately learn representations that are consistent with symbolic knowledge, and thus naturally interpretable",
    "keywords": [],
    "checked": false,
    "id": "51510b3de4823ac647b78939be51e183e5b9b0d5",
    "semantic_title": "learning modulo theories",
    "citation_count": 0,
    "authors": [
      "Zifan Wang",
      "Saranya Vijayakumar",
      "Kaiji Lu",
      "Vijay Ganesh",
      "Somesh Jha",
      "Matt Fredrikson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4732d425125832887f6c5a9675d49ead-Abstract-Conference.html": {
    "title": "D$^2$CSG: Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts",
    "volume": "main",
    "abstract": "We present D$^2$CSG, a neural model composed of two dual and complementary network branches, with dropouts, for unsupervised learning of compact constructive solid geometry (CSG) representations of 3D CAD shapes. Our network is trained to reconstruct a 3D shape by a fixed-order assembly of quadric primitives, with both branches producing a union of primitive intersections or inverses. A key difference between D$^2$CSG and all prior neural CSG models is its dedicated residual branch to assemble the potentially complex shape complement, which is subtracted from an overall shape modeled by the cover branch. With the shape complements, our network is provably general, while the weight dropout further improves compactness of the CSG tree by removing redundant primitives. We demonstrate both quantitatively and qualitatively that D$^2$CSG produces compact CSG reconstructions with superior quality and more natural primitives than all existing alternatives, especially over complex and high-genus CAD shapes",
    "keywords": [],
    "checked": true,
    "id": "1361a533b74d96c6750c92185aacca9d6b72f844",
    "semantic_title": "d$^2$csg: unsupervised learning of compact csg trees with dual complements and dropouts",
    "citation_count": 1,
    "authors": [
      "Fenggen Yu",
      "Qimin Chen",
      "Maham Tanveer",
      "Ali Mahdavi Amiri",
      "Hao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/47393e8594c82ce8fd83adc672cf9872-Abstract-Conference.html": {
    "title": "Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering",
    "volume": "main",
    "abstract": "Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to utilize knowledge from external knowledge bases to answer visually-grounded questions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong framework to tackle KB-VQA, first retrieves related documents with Dense Passage Retrieval (DPR) and then uses them to answer questions. This paper proposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which significantly improves knowledge retrieval in RA-VQA. FLMR addresses two major limitations in RA-VQA's retriever: (1) the image representations obtained via image-to-text transforms can be incomplete and inaccurate and (2) similarity scores between queries and documents are computed with one-dimensional embeddings, which can be insensitive to finer-grained similarities.FLMR overcomes these limitations by obtaining image representations that complement those from the image-to-text transform using a vision model aligned with an existing text-based retriever through a simple alignment network. FLMR also encodes images and questions using multi-dimensional embeddings to capture finer-grained similarities between queries and documents. FLMR significantly improves the original RA-VQA retriever's PRRecall@5 by approximately 8\\%. Finally, we equipped RA-VQA with two state-of-the-art large multi-modal/language models to achieve $\\sim62$% VQA score in the OK-VQA dataset",
    "keywords": [],
    "checked": true,
    "id": "93183f050e0ce0aff8ba0aa850c8353e24fb169d",
    "semantic_title": "fine-grained late-interaction multi-modal retrieval for retrieval augmented visual question answering",
    "citation_count": 2,
    "authors": [
      "Weizhe Lin",
      "Jinghong Chen",
      "Jingbiao Mei",
      "Alexandru Coca",
      "Bill Byrne"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/473aadf077f8464dbae7e9600d9be6c4-Abstract-Conference.html": {
    "title": "Iteratively Learn Diverse Strategies with State Distance Information",
    "volume": "main",
    "abstract": "In complex reinforcement learning (RL) problems, policies with similar rewards may have substantially different behaviors. It remains a fundamental challenge to optimize rewards while also discovering as many diverse strategies as possible, which can be crucial in many practical applications. Our study examines two design choices for tackling this challenge, i.e., diversity measure and computation framework. First, we find that with existing diversity measures, visually indistinguishable policies can still yield high diversity scores. To accurately capture the behavioral difference, we propose to incorporate the state-space distance information into the diversity measure. In addition, we examine two common computation frameworks for this problem, i.e., population-based training (PBT) and iterative learning (ITR). We show that although PBT is the precise problem formulation, ITR can achieve comparable diversity scores with higher computation efficiency, leading to improved solution quality in practice. Based on our analysis, we further combine ITR with two tractable realizations of the state-distance-based diversity measures and develop a novel diversity-driven RL algorithm, State-based Intrinsic-reward Policy Optimization (SIPO), with provable convergence properties. We empirically examine SIPO across three domains from robot locomotion to multi-agent games. In all of our testing environments, SIPO consistently produces strategically diverse and human-interpretable policies that cannot be discovered by existing baselines",
    "keywords": [],
    "checked": true,
    "id": "e61b7cf01b7efff5c324ffbb2c5055080f2be937",
    "semantic_title": "iteratively learn diverse strategies with state distance information",
    "citation_count": 2,
    "authors": [
      "Wei Fu",
      "Weihua Du",
      "Jingwei Li",
      "Sunli Chen",
      "Jingzhao Zhang",
      "YI WU"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/47547ee84e3fbbcbbbbad7c1fd9a973b-Abstract-Conference.html": {
    "title": "Neural Fields with Hard Constraints of Arbitrary Differential Order",
    "volume": "main",
    "abstract": "While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as Constrained Neural Fields (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization",
    "keywords": [],
    "checked": true,
    "id": "8de8256dcddbd762125dbfe57914a934f8f3cfcf",
    "semantic_title": "neural fields with hard constraints of arbitrary differential order",
    "citation_count": 2,
    "authors": [
      "Fangcheng Zhong",
      "Kyle Fogarty",
      "Param Hanji",
      "Tianhao Wu",
      "Alejandro Sztrajman",
      "Andrew Spielberg",
      "Andrea Tagliasacchi",
      "Petra Bosilj",
      "Cengiz Oztireli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4761fab863f0900d90cf601fce6d5155-Abstract-Conference.html": {
    "title": "Thinker: Learning to Plan and Act",
    "volume": "main",
    "abstract": "We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for handcrafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. Thinker is the first work showing that an RL agent can learn to plan with a learned world model in complex environments",
    "keywords": [],
    "checked": true,
    "id": "84afaa30f4fbe368338a5a3bce5d80e0937a56a6",
    "semantic_title": "thinker: learning to plan and act",
    "citation_count": 1,
    "authors": [
      "Stephen Chung",
      "Ivan Anokhin",
      "David Krueger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/476ab8f369e489c04187ba84f68cfa68-Abstract-Conference.html": {
    "title": "Near-Optimal $k$-Clustering in the Sliding Window Model",
    "volume": "main",
    "abstract": "Clustering is an important technique for identifying structural information in large-scale data analysis, where the underlying dataset may be too large to store. In many applications, recent data can provide more accurate information and thus older data past a certain time is expired. The sliding window model captures these desired properties and thus there has been substantial interest in clustering in the sliding window model. In this paper, we give the first algorithm that achieves near-optimal $(1+\\varepsilon)$-approximation to $(k,z)$-clustering in the sliding window model. Our algorithm uses $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ words of space when the points are from $[\\Delta]^d$, thus significantly improving on works by Braverman et. al. (SODA 2016), Borassi et. al. (NeurIPS 2021), and Epasto et. al. (SODA 2022).Along the way, we develop a data structure for clustering called an online coreset, which outputs a coreset not only for the end of a stream, but also for all prefixes of the stream. Our online coreset samples $\\frac{k}{\\min(\\varepsilon^4,\\varepsilon^{2+z})}\\,\\text{polylog}\\frac{n\\Delta}{\\varepsilon}$ points from the stream. We then show that any online coreset requires $\\Omega\\left(\\frac{k}{\\varepsilon^2}\\log n\\right)$ samples, which shows a separation between the problem of constructing an offline coreset, i.e., constructing online coresets is strictly harder. Our results also extend to general metrics on $[\\Delta]^d$ and are near-optimal in light of a $\\Omega\\left(\\frac{k}{\\varepsilon^{2+z}}\\right)$ lower bound for the size of an offline coreset",
    "keywords": [],
    "checked": false,
    "id": "c9c6e8c94e62392b4dc557bbe6b8f5a71599a46b",
    "semantic_title": "near-optimal k-clustering in the sliding window model",
    "citation_count": 0,
    "authors": [
      "David Woodruff",
      "Peilin Zhong",
      "Samson Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4791edcba96fbd82a8962b0f790b52c9-Abstract-Conference.html": {
    "title": "Window-Based Distribution Shift Detection for Deep Neural Networks",
    "volume": "main",
    "abstract": "To deploy and operate deep neural models in production, the quality of their predictions, which might be contaminated benignly or manipulated maliciously by input distributional deviations, must be monitored and assessed. Specifically, we study the case of monitoring the healthy operation of a deep neural network (DNN) receiving a stream of data, with the aim of detecting input distributional deviations over which the quality of the network's predictions is potentially damaged. Using selective prediction principles, we propose a distribution deviation detection method for DNNs. The proposed method is derived from a tight coverage generalization bound computed over a sample of instances drawn from the true underlying distribution. Based on this bound, our detector continuously monitors the operation of the network over a test window and fires off an alarm whenever a deviation is detected. Our novel detection method performs on-par or better than the state-of-the-art, while consuming substantially lower computation time (five orders of magnitude reduction) and space complexity. Unlike previous methods, which require at least linear dependence on the size of the source distribution for each detection, rendering them inapplicable to ``Google-Scale'' datasets, our approach eliminates this dependence, making it suitable for real-world applications. Code is available at https://github.com/BarSGuy/Window-Based-Distribution-Shift-Detection",
    "keywords": [],
    "checked": true,
    "id": "11462c53277ae86727ed34d2987b9433ab6d51e7",
    "semantic_title": "window-based distribution shift detection for deep neural networks",
    "citation_count": 0,
    "authors": [
      "Guy Bar Shalom",
      "Yonatan Geifman",
      "Ran El-Yaniv"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4798eef078de031518beaf54f4b5fb5f-Abstract-Conference.html": {
    "title": "Towards Label Position Bias in Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful tool for semi-supervised node classification tasks. However, recent studies have revealed various biases in GNNs stemming from both node features and graph topology. In this work, we uncover a new bias - label position bias, which indicates that the node closer to the labeled nodes tends to perform better. We introduce a new metric, the Label Proximity Score, to quantify this bias, and find that it is closely related to performance disparities. To address the label position bias, we propose a novel optimization framework for learning a label position unbiased graph structure, which can be applied to existing GNNs. Extensive experiments demonstrate that our proposed method not only outperforms backbone methods but also significantly mitigates the issue of label position bias in GNNs",
    "keywords": [],
    "checked": true,
    "id": "fcf1badf515d74d8ee40fa8e6c64f5464da90c74",
    "semantic_title": "towards label position bias in graph neural networks",
    "citation_count": 0,
    "authors": [
      "Haoyu Han",
      "Xiaorui Liu",
      "Feng Shi",
      "MohamadAli Torkamani",
      "Charu Aggarwal",
      "Jiliang Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/47e74fca60b4af4846b7abab188b85f2-Abstract-Conference.html": {
    "title": "Label Robust and Differentially Private Linear Regression: Computational and Statistical Efficiency",
    "volume": "main",
    "abstract": "We study the canonical problem of linear regression under $(\\varepsilon,\\delta)$-differential privacy when the datapoints are sampled i.i.d.~from a distribution and a fraction of response variables are adversarially corrupted. We provide the first provably efficient -- both computationally and statistically -- method for this problem, assuming standard assumptions on the data distribution. Our algorithm is a variant of the popular differentially private stochastic gradient descent (DP-SGD) algorithm with two key innovations: a full-batch gradient descent to improve sample complexity and a novel adaptive clipping to guarantee robustness. Our method requires only linear time in input size, and still matches the information theoretical optimal sample complexity up to a data distribution dependent condition number factor. Interestingly, the same algorithm, when applied to a setting where there is no adversarial corruption, still improves upon the existing state-of-the-art and achieves a near optimal sample complexity",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyang Liu",
      "Prateek Jain",
      "Weihao Kong",
      "Sewoong Oh",
      "Arun Suggala"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/47eb2874a790d5b1f554b9bb93b3de9d-Abstract-Conference.html": {
    "title": "Explainable and Efficient Randomized Voting Rules",
    "volume": "main",
    "abstract": "With a rapid growth in the deployment of AI tools for making critical decisions (or aiding humans in doing so), there is a growing demand to be able to explain to the stakeholders how these tools arrive at a decision. Consequently, voting is frequently used to make such decisions due to its inherent explainability. Recent work suggests that using randomized (as opposed to deterministic) voting rules can lead to significant efficiency gains measured via the distortion framework. However, rules that use intricate randomization can often become too complex to explain to the stakeholders; losing explainability can eliminate the key advantage of voting over black-box AI tools, which may outweigh the efficiency gains.We study the efficiency gains which can be unlocked by using voting rules that add a simple randomization step to a deterministic rule, thereby retaining explainability. We focus on two such families of rules, randomized positional scoring rules and random committee member rules, and show, theoretically and empirically, that they indeed achieve explainability and efficiency simultaneously to some extent",
    "keywords": [],
    "checked": true,
    "id": "3f562dea2d5c9e1b08dd3aebe0cbbc79c04244d1",
    "semantic_title": "explainable and efficient randomized voting rules",
    "citation_count": 1,
    "authors": [
      "Soroush Ebadian",
      "Aris Filos-Ratsikas",
      "Mohamad Latifian",
      "Nisarg Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/47f2fad8c1111d07f83c91be7870f8db-Abstract-Conference.html": {
    "title": "Conformal PID Control for Time Series Prediction",
    "volume": "main",
    "abstract": "We study the problem of uncertainty quantification for time series prediction, with the goal of providing easy-to-use algorithms with formal guarantees. The algorithms we present build upon ideas from conformal prediction and control theory, are able to prospectively model conformal scores in an online setting, and adapt to the presence of systematic errors due to seasonality, trends, and general distribution shifts. Our theory both simplifies and strengthens existing analyses in online conformal prediction. Experiments on 4-week-ahead forecasting of statewide COVID-19 death counts in the U.S. show an improvement in coverage over the ensemble forecaster used inofficial CDC communications. We also run experiments on predicting electricity demand, market returns, and temperature using autoregressive, Theta, Prophet, and Transformer models. We provide an extendable codebase for testing our methods and for the integration of new algorithms, data sets, and forecasting rules at this link",
    "keywords": [],
    "checked": true,
    "id": "0062fc95d78412b2c5b1b397fe1154abd3ef945c",
    "semantic_title": "conformal pid control for time series prediction",
    "citation_count": 6,
    "authors": [
      "Anastasios Angelopoulos",
      "Emmanuel Candes",
      "Ryan J. Tibshirani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/47f30d67bce3e9824928267e9355420f-Abstract-Conference.html": {
    "title": "LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation",
    "volume": "main",
    "abstract": "Existing automatic evaluation on text-to-image synthesis can only provide an image-text matching score, without considering the object-level compositionality, which results in poor correlation with human judgments. In this work, we propose LLMScore, a new framework that offers evaluation scores with multi-granularity compositionality. LLMScore leverages the large language models (LLMs) to evaluate text-to-image models. Initially, it transforms the image into image-level and object-level visual descriptions. Then an evaluation instruction is fed into the LLMs to measure the alignment between the synthesized image and the text, ultimately generating a score accompanied by a rationale. Our substantial analysis reveals the highest correlation of LLMScore with human judgments on a wide range of datasets (Attribute Binding Contrast, Concept Conjunction, MSCOCO, DrawBench, PaintSkills). Notably, our LLMScore achieves Kendall's tau correlation with human evaluations that is 58.8% and 31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP, respectively",
    "keywords": [],
    "checked": true,
    "id": "972501b057e2b84d6ce6506f70bcac697bab7872",
    "semantic_title": "llmscore: unveiling the power of large language models in text-to-image synthesis evaluation",
    "citation_count": 10,
    "authors": [
      "Yujie Lu",
      "Xianjun Yang",
      "Xiujun Li",
      "Xin Eric Wang",
      "William Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/47fc64d05a394955b1ae2487bfef1ab0-Abstract-Conference.html": {
    "title": "Dynamically Masked Discriminator for GANs",
    "volume": "main",
    "abstract": "Training Generative Adversarial Networks (GANs) remains a challenging problem. The discriminator trains the generator by learning the distribution of real/generated data. However, the distribution of generated data changes throughout the training process, which is difficult for the discriminator to learn. In this paper, we propose a novel method for GANs from the viewpoint of online continual learning. We observe that the discriminator model, trained on historically generated data, often slows down its adaptation to the changes in the new arrival generated data, which accordingly decreases the quality of generated results. By treating the generated data in training as a stream, we propose to detect whether the discriminator slows down the learning of new knowledge in generated data. Therefore, we can explicitly enforce the discriminator to learn new knowledge fast. Particularly, we propose a new discriminator, which automatically detects its retardation and then dynamically masks its features, such that the discriminator can adaptively learn the temporally-vary distribution of generated data. Experimental results show our method outperforms the state-of-the-art approaches",
    "keywords": [],
    "checked": false,
    "id": "27eaed5c55547855301904c3130f46ed16160be6",
    "semantic_title": "dynamically masked discriminator for generative adversarial networks",
    "citation_count": 1,
    "authors": [
      "Wentian Zhang",
      "Haozhe Liu",
      "Bing Li",
      "Jinheng Xie",
      "Yawen Huang",
      "Yuexiang Li",
      "Yefeng Zheng",
      "Bernard Ghanem"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4818263715b25dc137d393af8af6d2fc-Abstract-Conference.html": {
    "title": "Diverse Conventions for Human-AI Collaboration",
    "volume": "main",
    "abstract": "Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce mixed-play, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits of our technique on various multi-agent collaborative games, including Overcooked, and find that our technique can adapt to the conventions of humans, surpassing human-level performance when paired with real users",
    "keywords": [],
    "checked": true,
    "id": "3e126b825d26f1c8c9ce86ab6ede9be11c25c7cd",
    "semantic_title": "diverse conventions for human-ai collaboration",
    "citation_count": 0,
    "authors": [
      "Bidipta Sarkar",
      "Andy Shih",
      "Dorsa Sadigh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4846257e355f6923fc2a1fbe35099e91-Abstract-Conference.html": {
    "title": "Self-Supervised Learning of Representations for Space Generates Multi-Modular Grid Cells",
    "volume": "main",
    "abstract": "To solve the spatial problems of mapping, localization and navigation, the mammalian lineage has developed striking spatial representations. One important spatial representation is the Nobel-prize winning grid cells: neurons that represent self-location, a local and aperiodic quantity, with seemingly bizarre non-local and spatially periodic activity patterns of a few discrete periods. Why has the mammalian lineage learnt this peculiar grid representation? Mathematical analysis suggests that this multi-periodic representation has excellent properties as an algebraic code with high capacity and intrinsic error-correction, but to date, synthesis of multi-modular grid cells in deep recurrent neural networks remains absent. In this work, we begin by identifying key insights from four families of approaches to answering the grid cell question: dynamical systems, coding theory, function optimization and supervised deep learning. We then leverage our insights to propose a new approach that elegantly combines the strengths of all four approaches. Our approach is a self-supervised learning (SSL) framework - including data, data augmentations, loss functions and a network architecture - motivated from a normative perspective, with no access to supervised position information. Without making assumptions about internal or readout representations, we show that multiple grid cell modules can emerge in networks trained on our SSL framework and that the networks generalize significantly beyond their training distribution. This work contains insights for neuroscientists interested in the origins of grid cells as well as machine learning researchers interested in novel SSL frameworks",
    "keywords": [],
    "checked": true,
    "id": "27bc20a95969cffa29065fcc1beb63ee2386b852",
    "semantic_title": "self-supervised learning of representations for space generates multi-modular grid cells",
    "citation_count": 4,
    "authors": [
      "Rylan Schaeffer",
      "Mikail Khona",
      "Tzuhsuan Ma",
      "Cristobal Eyzaguirre",
      "Sanmi Koyejo",
      "Ila Fiete"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/484d254ff80e99d543159440a06db0de-Abstract-Conference.html": {
    "title": "A Guide Through the Zoo of Biased SGD",
    "volume": "main",
    "abstract": "Stochastic Gradient Descent (SGD) is arguably the most important single algorithm in modern machine learning. Although SGD with unbiased gradient estimators has been studied extensively over at least half a century, SGD variants relying on biased estimators are rare. Nevertheless, there has been an increased interest in this topic in recent years. However, existing literature on SGD with biased estimators lacks coherence since each new paper relies on a different set of assumptions, without any clear understanding of how they are connected, which may lead to confusion. We address this gap by establishing connections among the existing assumptions, and presenting a comprehensive map of the underlying relationships. Additionally, we introduce a new set of assumptions that is provably weaker than all previous assumptions, and use it to present a thorough analysis of BiasedSGD in both convex and non-convex settings, offering advantages over previous results. We also provide examples where biased estimators outperform their unbiased counterparts or where unbiased versions are simply not available. Finally, we demonstrate the effectiveness of our framework through experimental results that validate our theoretical findings",
    "keywords": [],
    "checked": true,
    "id": "2b49533bf144c37515c7ec1f28e7466ca44ff251",
    "semantic_title": "a guide through the zoo of biased sgd",
    "citation_count": 2,
    "authors": [
      "Yury Demidovich",
      "Grigory Malinovsky",
      "Igor Sokolov",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4869f3f967dfe954439408dd92c50ee1-Abstract-Conference.html": {
    "title": "Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars",
    "volume": "main",
    "abstract": "The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Code is available at https://github.com/automl/hierarchicalnasconstruction",
    "keywords": [],
    "checked": true,
    "id": "7d867556d37be40758692ebd73f511c8f9f52260",
    "semantic_title": "construction of hierarchical neural architecture search spaces based on context-free grammars",
    "citation_count": 1,
    "authors": [
      "Simon Schrodi",
      "Danny Stoll",
      "Binxin Ru",
      "Rhea Sukthanker",
      "Thomas Brox",
      "Frank Hutter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/486ff0b164cf92b0255fe39863bcf99e-Abstract-Conference.html": {
    "title": "Data-Informed Geometric Space Selection",
    "volume": "main",
    "abstract": "Geometric representation learning (e.g., hyperbolic and spherical geometry) has proven to be efficacious in solving many intricate machine learning tasks. The fundamental challenge of geometric representation learning lies in aligning the inherent geometric bias with the underlying structure of the data, which is a rarely explored topic in the literature. Existing methods heavily rely on heuristic assumptions on the data structure to decide the type of geometry to be adopted, which often leads to suboptimal performance. This work aims to automate the alignment process via a data-informed strategy such that we optimize model performance with minimal overhead. Specifically, a sparse gating mechanism is employed to enable each input data point $\\mathit{p}$ to select $K$ geometric spaces from a given candidate geometric space pool with $N$ ($K",
    "keywords": [],
    "checked": false,
    "id": "28145e590c60a976628e33b5f10558e0b84d40cf",
    "semantic_title": "ctt: causally informed tensor train decomposition",
    "citation_count": 0,
    "authors": [
      "Shuai Zhang",
      "Wenqi Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/48726631f87322012c6be38e00c72a47-Abstract-Conference.html": {
    "title": "Prioritizing Samples in Reinforcement Learning with Reducible Loss",
    "volume": "main",
    "abstract": "Most reinforcement learning algorithms take advantage of an experience replay buffer to repeatedly train on samples the agent has observed in the past. Not all samples carry the same amount of significance and simply assigning equal importance to each of the samples is a naïve strategy. In this paper, we propose a method to prioritize samples based on how much we can learn from a sample. We define the learn-ability of a sample as the steady decrease of the training loss associated with this sample over time. We develop an algorithm to prioritize samples with high learn-ability, while assigning lower priority to those that are hard-to-learn, typically caused by noise or stochasticity. We empirically show that across multiple domains our method is more robust than random sampling and also better than just prioritizing with respect to the training loss, i.e. the temporal difference loss, which is used in prioritized experience replay",
    "keywords": [],
    "checked": true,
    "id": "eb92cedb7e9182b8887a36d25f319196e11d9ca4",
    "semantic_title": "prioritizing samples in reinforcement learning with reducible loss",
    "citation_count": 5,
    "authors": [
      "Shivakanth Sujit",
      "Somjit Nath",
      "Pedro Braga",
      "Samira Ebrahimi Kahou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/487667c56596138d36bbaa3bd8aac6df-Abstract-Conference.html": {
    "title": "Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks",
    "volume": "main",
    "abstract": "We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network",
    "keywords": [],
    "checked": true,
    "id": "ba0533d6fbc51753588cc04cf643a6f6c11294f8",
    "semantic_title": "intensity profile projection: a framework for continuous-time representation learning for dynamic networks",
    "citation_count": 0,
    "authors": [
      "Alexander Modell",
      "Ian Gallagher",
      "Emma Ceccherini",
      "Nick Whiteley",
      "Patrick Rubin-Delanchy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/48aaa5ea741ae8430bd58e25917d267d-Abstract-Conference.html": {
    "title": "Understanding Contrastive Learning via Distributionally Robust Optimization",
    "volume": "main",
    "abstract": "This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (\\eg labels). However, existing theories fall short in providing explanations for this phenomenon. We bridge this research gap by analyzing CL through the lens of distributionally robust optimization (DRO), yielding several key insights: (1) CL essentially conducts DRO over the negative sampling distribution, thus enabling robust performance across a variety of potential distributions and demonstrating robustness to sampling bias; (2) The design of the temperature $\\tau$ is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set; (3) A theoretical connection is established between DRO and mutual information, thus presenting fresh evidence for ``InfoNCE as an estimate of MI'' and a new estimation approach for $\\phi$-divergence-based generalized mutual information. We also identify CL's potential shortcomings, including over-conservatism and sensitivity to outliers, and introduce a novel Adjusted InfoNCE loss (ADNCE) to mitigate these issues. It refines potential distribution, improving performance and accelerating convergence. Extensive experiments on various domains (image, sentence, and graph) validate the effectiveness of the proposal",
    "keywords": [],
    "checked": true,
    "id": "8aa8249f129d2592549cb34fe4c403fc12e612a3",
    "semantic_title": "understanding contrastive learning via distributionally robust optimization",
    "citation_count": 3,
    "authors": [
      "Junkang Wu",
      "Jiawei Chen",
      "Jiancan Wu",
      "Wentao Shi",
      "Xiang Wang",
      "Xiangnan He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/48db67447e92539501bd71645ff33b72-Abstract-Conference.html": {
    "title": "K-Nearest-Neighbor Local Sampling Based Conditional Independence Testing",
    "volume": "main",
    "abstract": "Conditional independence (CI) testing is a fundamental task in statistics and machine learning, but its effectiveness is hindered by the challenges posed by high-dimensional conditioning variables and limited data samples. This article introduces a novel testing approach to address these challenges and enhance control of the type I error while achieving high power under alternative hypotheses. The proposed approach incorporates a computationally efficient classifier-based conditional mutual information (CMI) estimator, capable of capturing intricate dependence structures among variables. To approximate a distribution encoding the null hypothesis, a $k$-nearest-neighbor local sampling strategy is employed. An important advantage of this approach is its ability to operate without assumptions about distribution forms or feature dependencies. Furthermore, it eliminates the need to derive asymptotic null distributions for the estimated CMI and avoids dataset splitting, making it particularly suitable for small datasets. The method presented in this article demonstrates asymptotic control of the type I error and consistency against all alternative hypotheses. Extensive analyses using both synthetic and real data highlight the computational efficiency of the proposed test. Moreover, it outperforms existing state-of-the-art methods in terms of type I and II errors, even in scenarios with high-dimensional conditioning sets. Additionally, the proposed approach exhibits robustness in the presence of heavy-tailed data",
    "keywords": [],
    "checked": false,
    "id": "c7e27ec1d6edcfcd87c3c9793320ec27b65fb72a",
    "semantic_title": "nearest-neighbor sampling based conditional independence testing",
    "citation_count": 0,
    "authors": [
      "Shuai Li",
      "Yingjie Zhang",
      "Hongtu Zhu",
      "Christina Wang",
      "Hai Shu",
      "Ziqi Chen",
      "Zhuoran Sun",
      "Yanfeng Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/48f8143cebe113f4596e1781771578cd-Abstract-Conference.html": {
    "title": "Learning Large Graph Property Prediction via Graph Segment Training",
    "volume": "main",
    "abstract": "Learning to predict properties of large graphs is challenging because each prediction requires the knowledge of an entire graph, while the amount of memory available during training is bounded. Here we propose Graph Segment Training (GST), a general framework that utilizes a divide-and-conquer approach to allow learning large graph property prediction with a constant memory footprint. GST first divides a large graph into segments and then backpropagates through only a few segments sampled per training iteration. We refine the GST paradigm by introducing a historical embedding table to efficiently obtain embeddings for segments not sampled for backpropagation. To mitigate the staleness of historical embeddings, we design two novel techniques. First, we finetune the prediction head to fix the input distribution shift. Second, we introduce Stale Embedding Dropout to drop some stale embeddings during training to reduce bias. We evaluate our complete method GST-EFD (with all the techniques together) on two large graph property prediction benchmarks: MalNet and TpuGraphs. Our experiments show that GST-EFD is both memory-efficient and fast, while offering a slight boost on test accuracy over a typical full graph training regime",
    "keywords": [],
    "checked": true,
    "id": "3b7750ebd7be28146ed09fc17df8cd6ea6c360d2",
    "semantic_title": "learning large graph property prediction via graph segment training",
    "citation_count": 3,
    "authors": [
      "Kaidi Cao",
      "Mangpo Phothilimthana",
      "Sami Abu-El-Haija",
      "Dustin Zelle",
      "Yanqi Zhou",
      "Charith Mendis",
      "Jure Leskovec",
      "Bryan Perozzi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/491bbea0db340e2d0bc1feea9059909a-Abstract-Conference.html": {
    "title": "Online Nonstochastic Model-Free Reinforcement Learning",
    "volume": "main",
    "abstract": "We investigate robust model-free reinforcement learning algorithms designed for environments that may be dynamic or even adversarial. Traditional state-based policies often struggle to accommodate the challenges imposed by the presence of unmodeled disturbances in such settings. Moreover, optimizing linear state-based policies pose an obstacle for efficient optimization, leading to nonconvex objectives, even in benign environments like linear dynamical systems.Drawing inspiration from recent advancements in model-based control, we intro- duce a novel class of policies centered on disturbance signals. We define several categories of these signals, which we term pseudo-disturbances, and develop corresponding policy classes based on them. We provide efficient and practical algorithms for optimizing these policies.Next, we examine the task of online adaptation of reinforcement learning agents in the face of adversarial disturbances. Our methods seamlessly integrate with any black-box model-free approach, yielding provable regret guarantees when dealing with linear dynamics. These regret guarantees unconditionally improve the best-known results for bandit linear control in having no dependence on the state-space dimension. We evaluate our method over various standard RL benchmarks and demonstrate improved robustness",
    "keywords": [],
    "checked": true,
    "id": "4fd3f3b2f0f0a4b4e4a77d356f07dbc2d709b25d",
    "semantic_title": "online nonstochastic model-free reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Udaya Ghai",
      "Arushi Gupta",
      "Wenhan Xia",
      "Karan Singh",
      "Elad Hazan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4947292b9f5e7d4ab792fa35537f8b96-Abstract-Conference.html": {
    "title": "Time-Reversed Dissipation Induces Duality Between Minimizing Gradient Norm and Function Value",
    "volume": "main",
    "abstract": "In convex optimization, first-order optimization methods efficiently minimizing function values have been a central subject study since Nesterov's seminal work of 1983. Recently, however, Kim and Fessler's OGM-G and Lee et al.'s FISTA-G have been presented as alternatives that efficiently minimize the gradient magnitude instead. In this paper, we present H-duality, which represents a surprising one-to-one correspondence between methods efficiently minimizing function values and methods efficiently minimizing gradient magnitude. In continuous-time formulations, H-duality corresponds to reversing the time dependence of the dissipation/friction term. To the best of our knowledge, H-duality is different from Lagrange/Fenchel duality and is distinct from any previously known duality or symmetry relations. Using H-duality, we obtain a clearer understanding of the symmetry between Nesterov's method and OGM-G, derive a new class of methods efficiently reducing gradient magnitudes of smooth convex functions, and find a new composite minimization method that is simpler and faster than FISTA-G",
    "keywords": [],
    "checked": true,
    "id": "fb35e837464f40f218f00fea7a750479af3befe7",
    "semantic_title": "time-reversed dissipation induces duality between minimizing gradient norm and function value",
    "citation_count": 2,
    "authors": [
      "Jaeyeon Kim",
      "Asuman Ozdaglar",
      "Chanwoo Park",
      "Ernest Ryu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/49528141137087b8e94126d5f50b22da-Abstract-Conference.html": {
    "title": "Cascading Contextual Assortment Bandits",
    "volume": "main",
    "abstract": "We present a new combinatorial bandit model, the \\textit{cascading contextual assortment bandit}. This model serves as a generalization of both existing cascading bandits and assortment bandits, broadening their applicability in practice. For this model, we propose our first UCB bandit algorithm, UCB-CCA. We prove that this algorithm achieves a $T$-step regret upper-bound of $\\tilde{\\mathcal{O}}(\\frac{1}{\\kappa}d\\sqrt{T})$, sharper than existing bounds for cascading contextual bandits by eliminating dependence on cascade length $K$. To improve the dependence on problem-dependent constant $\\kappa$, we introduce our second algorithm, UCB-CCA+, which leverages a new Bernstein-type concentration result. This algorithm achieves $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ without dependence on $\\kappa$ in the leading term. We substantiate our theoretical claims with numerical experiments, demonstrating the practical efficacy of our proposed methods",
    "keywords": [],
    "checked": false,
    "id": "f3811cda345099e71ad47c07774aa2f99d7b834b",
    "semantic_title": "exposure-aware recommendation using contextual bandits",
    "citation_count": 1,
    "authors": [
      "Hyun-jun Choi",
      "Rajan Udwani",
      "Min-hwan Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4958a8ad01f524de2ec5274678ffa5a4-Abstract-Conference.html": {
    "title": "Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes",
    "volume": "main",
    "abstract": "Tensor decomposition is an important tool for multiway data analysis. In practice, the data is often sparse yet associated with rich temporal information. Existing methods, however, often under-use the time information and ignore the structural knowledge within the sparsely observed tensor entries. To overcome these limitations and to better capture the underlying temporal structure, we propose Dynamic EMbedIngs fOr dynamic Tensor dEcomposition (DEMOTE). We develop a neural diffusion-reaction process to estimate dynamic embeddings for the entities in each tensor mode. Specifically, based on the observed tensor entries, we build a multi-partite graph to encode the correlation between the entities. We construct a graph diffusion process to co-evolve the embedding trajectories of the correlated entities and use a neural network to construct a reaction process for each individual entity. In this way, our model can capture both the commonalities and personalities during the evolution of the embeddings for different entities. We then use a neural network to model the entry value as a nonlinear function of the embedding trajectories. For model estimation, we combine ODE solvers to develop a stochastic mini-batch learning algorithm. We propose a stratified sampling method to balance the cost of processing each mini-batch so as to improve the overall efficiency. We show the advantage of our approach in both simulation studies and real-world applications. The code is available at https://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes",
    "keywords": [],
    "checked": true,
    "id": "271348cff183ce7475674f237917e1a79911582c",
    "semantic_title": "dynamic tensor decomposition via neural diffusion-reaction processes",
    "citation_count": 0,
    "authors": [
      "Zheng Wang",
      "Shikai Fang",
      "Shibo Li",
      "Shandian Zhe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/49cf35ff2298c10452db99d08036805b-Abstract-Conference.html": {
    "title": "Sample based Explanations via Generalized Representers",
    "volume": "main",
    "abstract": "We propose a general class of sample based explanations of machine learning models, which we term generalized representers. To measure the effect of a training sample on a model's test prediction, generalized representers use two components: a global sample importance that quantifies the importance of the training point to the model and is invariant to test samples, and a local sample importance that measures similarity between the training sample and the test point with a kernel. A key contribution of the paper is to show that generalized representers are the only class of sample based explanations satisfying a natural set of axiomatic properties. We discuss approaches to extract global importances given a kernel, and also natural choices of kernels given modern non-linear models. As we show, many popular existing sample based explanations could be cast as generalized representers with particular choices of kernels and approaches to extract global importances. Additionally, we conduct empirical comparisons of different generalized representers on two image classification datasets",
    "keywords": [],
    "checked": true,
    "id": "968a5d418c8a4b68eb8b95ec9f72dce2b776d4c9",
    "semantic_title": "sample based explanations via generalized representers",
    "citation_count": 1,
    "authors": [
      "Che-Ping Tsai",
      "Chih-Kuan Yeh",
      "Pradeep Ravikumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/49d1cf22327c51331cbd52bcb76a09a6-Abstract-Conference.html": {
    "title": "Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting",
    "volume": "main",
    "abstract": "Images contain rich relational knowledge that can help machines understand the world. Existing methods on visual knowledge extraction often rely on the pre-defined format (e.g., sub-verb-obj tuples) or vocabulary (e.g., relation types), restricting the expressiveness of the extracted knowledge. In this work, we take a first exploration to a new paradigm of open visual knowledge extraction. To achieve this, we present OpenVik which consists of an open relational region detector to detect regions potentially containing relational knowledge and a visual knowledge generator that generates format-free knowledge by prompting the large multimodality model with the detected region of interest. We also explore two data enhancement techniques for diversifying the generated format-free visual knowledge. Extensive knowledge quality evaluations highlight the correctness and uniqueness of the extracted open visual knowledge by OpenVik. Moreover, integrating our extracted knowledge across various visual reasoning applications shows consistent improvements, indicating the real-world applicability of OpenVik",
    "keywords": [],
    "checked": true,
    "id": "2b6a3cad4e4cbc2f2ae2a9f2d5b9e349071f24c2",
    "semantic_title": "open visual knowledge extraction via relation-oriented multimodality model prompting",
    "citation_count": 1,
    "authors": [
      "Hejie Cui",
      "Xinyu Fang",
      "Zihan Zhang",
      "Ran Xu",
      "Xuan Kan",
      "Xin Liu",
      "Yue Yu",
      "Manling Li",
      "Yangqiu Song",
      "Carl Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/49f42aafbcce59b2665640cb9f3d794f-Abstract-Conference.html": {
    "title": "Continuous Parametric Optical Flow",
    "volume": "main",
    "abstract": "In this paper, we present continuous parametric optical flow, a parametric representation of dense and continuous motion over arbitrary time interval. In contrast to existing discrete-time representations (i.e., flow in between consecutive frames), this new representation transforms the frame-to-frame pixel correspondences to dense continuous flow. In particular, we present a temporal-parametric model that employs B-splines to fit point trajectories using a limited number of frames. To further improve the stability and robustness of the trajectories, we also add an encoder with a neural ordinary differential equation (ODE) to represent features associated with specific times. We also contribute a synthetic dataset and introduce two evaluation perspectives to measure the accuracy and robustness of continuous flow estimation. Benefiting from the combination of explicit parametric modeling and implicit feature optimization, our model focuses on motion continuity and outperforms than the flow-based and point-tracking approaches for fitting long-term and variable sequences",
    "keywords": [],
    "checked": false,
    "id": "2d766a59c792f04acc795b44d57a78cd48f9e655",
    "semantic_title": "dense continuous-time optical flow from events and frames",
    "citation_count": 8,
    "authors": [
      "Jianqin Luo",
      "Zhexiong Wan",
      "yuxin mao",
      "Bo Li",
      "Yuchao Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/49ff6951ef47bc9bab276a31a965528e-Abstract-Conference.html": {
    "title": "Reusable Slotwise Mechanisms",
    "volume": "main",
    "abstract": "Agents with the ability to comprehend and reason about the dynamics of objects would be expected to exhibit improved robustness and generalization in novel scenarios. However, achieving this capability necessitates not only an effective scene representation but also an understanding of the mechanisms governing interactions among object subsets. Recent studies have made significant progress in representing scenes using object slots. In this work, we introduce Reusable Slotwise Mechanisms, or RSM, a framework that models object dynamics by leveraging communication among slots along with a modular architecture capable of dynamically selecting reusable mechanisms for predicting the future states of each object slot. Crucially, RSM leverages the Central Contextual Information (CCI), enabling selected mechanisms to access the remaining slots through a bottleneck, effectively allowing for modeling of higher order and complex interactions that might require a sparse subset of objects. Experimental results demonstrate the superior performance of RSM compared to state-of-the-art methods across various future prediction and related downstream tasks, including Visual Question Answering and action planning. Furthermore, we showcase RSM's Out-of-Distribution generalization ability to handle scenes in intricate scenarios",
    "keywords": [],
    "checked": true,
    "id": "cf9e6ff7eb6a79e977717efb51444d40d0493fbf",
    "semantic_title": "reusable slotwise mechanisms",
    "citation_count": 3,
    "authors": [
      "Trang Nguyen",
      "Amin Mansouri",
      "Kanika Madan",
      "Khuong Duy Nguyen",
      "Kartik Ahuja",
      "Dianbo Liu",
      "Yoshua Bengio"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4a17cd29ced0443bcff689fbb0d32d5e-Abstract-Conference.html": {
    "title": "Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning",
    "volume": "main",
    "abstract": "In this paper, we prove state-of-the-art Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We present a refined analysis of the information ratio, and show an upper bound of order $\\widetilde{O}(H\\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how our results improve the state-of-the-art",
    "keywords": [],
    "checked": true,
    "id": "9796b9e6dfbbbbd274a999e31701206e6ee3814e",
    "semantic_title": "improved bayesian regret bounds for thompson sampling in reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Ahmadreza Moradipari",
      "Mohammad Pedramfar",
      "Modjtaba Shokrian Zini",
      "Vaneet Aggarwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4a4a3c197deac042461c677219efd36c-Abstract-Conference.html": {
    "title": "Moment Matching Denoising Gibbs Sampling",
    "volume": "main",
    "abstract": "Energy-Based Models (EBMs) offer a versatile framework for modelling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a noisy data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a noisy model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets",
    "keywords": [],
    "checked": true,
    "id": "9c94d99eb07d398ce9bf4232983fc540e48d9ee1",
    "semantic_title": "moment matching denoising gibbs sampling",
    "citation_count": 1,
    "authors": [
      "Mingtian Zhang",
      "Alex Hawkins-Hooker",
      "Brooks Paige",
      "David Barber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4a6695df88f2de0d49f875189ea181ef-Abstract-Conference.html": {
    "title": "Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff",
    "volume": "main",
    "abstract": "Previous work has shown that DNNs withlarge depth $L$ and $L_{2}$-regularization are biased towards learninglow-dimensional representations of the inputs, which can be interpretedas minimizing a notion of rank $R^{(0)}(f)$ of the learned function$f$, conjectured to be the Bottleneck rank. We compute finite depthcorrections to this result, revealing a measure $R^{(1)}$ of regularitywhich bounds the pseudo-determinant of the Jacobian $\\left\\|Jf(x)\\right\\|\\_\\+$and is subadditive under composition and addition. This formalizesa balance between learning low-dimensional representations and minimizingcomplexity/irregularity in the feature maps, allowing the networkto learn the `right' inner dimension. Finally, we prove the conjecturedbottleneck structure in the learned features as $L\\to\\infty$: forlarge depths, almost all hidden representations are approximately$R^{(0)}(f)$-dimensional, and almost all weight matrices $W_{\\ell}$have $R^{(0)}(f)$ singular values close to 1 while the others are$O(L^{-\\frac{1}{2}})$. Interestingly, the use of large learning ratesis required to guarantee an order $O(L)$ NTK which in turns guaranteesinfinite depth convergence of the representations of almost all layers",
    "keywords": [],
    "checked": true,
    "id": "28835fd8d4cb7b62c09fce9e85fe52afe82ba4f5",
    "semantic_title": "bottleneck structure in learned features: low-dimension vs regularity tradeoff",
    "citation_count": 5,
    "authors": [
      "Arthur Jacot"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4a6824f8f137e78f18e73d9cfc1d22ed-Abstract-Conference.html": {
    "title": "Noise-Adaptive Thompson Sampling for Linear Contextual Bandits",
    "volume": "main",
    "abstract": "Linear contextual bandits represent a fundamental class of models with numerous real-world applications, and it is critical to develop algorithms that can effectively manage noise with unknown variance, ensuring provable guarantees for both worst-case constant-variance noise and deterministic reward scenarios. In this paper, we study linear contextual bandits with heteroscedastic noise and propose the first noise-adaptive Thompson sampling-style algorithm that achieves a variance-dependent regret upper bound of $\\widetilde O\\Big(d^{3/2} + d^{3/2} \\sqrt{\\sum_{t=1}^T \\sigma_t^2}\\Big)$, where $d$ is the dimension of the context vectors and $\\sigma_t^2$ is the variance of the reward in round $t$. This recovers the existing $\\widetilde O(d^{3/2}\\sqrt{T})$ regret guarantee in the constant-variance regime and further improves to $\\widetilde O(d^{3/2})$ in the deterministic regime, thus achieving a smooth interpolation in between. Our approach utilizes a stratified sampling procedure to overcome the too-conservative optimism in the linear Thompson sampling algorithm for linear contextual bandits",
    "keywords": [],
    "checked": false,
    "id": "516270f7d8fa7acefd950b93a2ff6a684d575e06",
    "semantic_title": "doubly-adaptive thompson sampling for multi-armed and contextual bandits",
    "citation_count": 1,
    "authors": [
      "Ruitu Xu",
      "Yifei Min",
      "Tianhao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4aa13186c795a52ba88f5b822f4b77eb-Abstract-Conference.html": {
    "title": "Regularization properties of adversarially-trained linear regression",
    "volume": "main",
    "abstract": "State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the minimum-norm interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\\ell_\\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples",
    "keywords": [],
    "checked": true,
    "id": "f874fd7a1961be99c39a0cb3d1c615f51d8276d2",
    "semantic_title": "regularization properties of adversarially-trained linear regression",
    "citation_count": 2,
    "authors": [
      "Antonio Ribeiro",
      "Dave Zachariah",
      "Francis Bach",
      "Thomas Schön"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4ac4365b98bc242acd5ab974a05c68a8-Abstract-Conference.html": {
    "title": "Complex-valued Neurons Can Learn More but Slower than Real-valued Neurons via Gradient Descent",
    "volume": "main",
    "abstract": "Complex-valued neural networks potentially possess better representations and performance than real-valued counterparts when dealing with some complicated tasks such as acoustic analysis, radar image classification, etc. Despite empirical successes, it remains unknown theoretically when and to what extent complex-valued neural networks outperform real-valued ones. We take one step in this direction by comparing the learnability of real-valued neurons and complex-valued neurons via gradient descent. We show that a complex-valued neuron can efficiently learn functions expressed by any one real-valued neuron and any one complex-valued neuron with convergence rate $O(t^{-3})$ and $O(t^{-1})$ where $t$ is the iteration index of gradient descent, respectively, whereas a two-layer real-valued neural network with finite width cannot learn a single non-degenerate complex-valued neuron. We prove that a complex-valued neuron learns a real-valued neuron with rate $\\Omega (t^{-3})$, exponentially slower than the $O(\\mathrm{e}^{- c t})$ rate of learning one real-valued neuron using a real-valued neuron with a constant $c$. We further verify and extend these results via simulation experiments in more general settings",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin-Hui Wu",
      "Shao-Qun Zhang",
      "Yuan Jiang",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4af24e6ce753c181e703f3f0be3b5e20-Abstract-Conference.html": {
    "title": "Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs",
    "volume": "main",
    "abstract": "We prove that, for the fundamental regression task of learning a single neuron, training a one-hidden layer ReLU network of any width by gradient flow from a small initialisation converges to zero loss and is implicitly biased to minimise the rank of network parameters. By assuming that the training points are correlated with the teacher neuron, we complement previous work that considered orthogonal datasets. Our results are based on a detailed non-asymptotic analysis of the dynamics of each hidden neuron throughout the training. We also show and characterise a surprising distinction in this setting between interpolator networks of minimal rank and those of minimal Euclidean norm. Finally we perform a range of numerical experiments, which corroborate our theoretical findings",
    "keywords": [],
    "checked": true,
    "id": "36d9648ff6afd5d9f7cda828ccb9cc538e433ef0",
    "semantic_title": "learning a neuron by a shallow relu network: dynamics and implicit bias for correlated inputs",
    "citation_count": 2,
    "authors": [
      "Dmitry Chistikov",
      "Matthias Englert",
      "Ranko Lazic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4af827e7d0b7bdae6097d44977e87534-Abstract-Conference.html": {
    "title": "Separable Physics-Informed Neural Networks",
    "volume": "main",
    "abstract": "Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate very complex solution functions.The number of training points (collocation points) required on these challenging PDEs grows substantially, and it is severely limited due to the expensive computational costs and heavy memory overhead.To overcome this limit, we propose a network architecture and training algorithm for PINNs.The proposed method, separable PINN (SPINN), operates on a per-axis basis to decrease the number of network propagations in multi-dimensional PDEs instead of point-wise processing in conventional PINNs.We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points ($>10^7$) on a single commodity GPU. The experimental results show significantly reduced computational costs ($62\\times$ in wall-clock time, $1,394\\times$ in FLOPs given the same number of collocation points) in multi-dimensional PDEs while achieving better accuracy.Furthermore, we present that SPINN can solve a chaotic (2+1)-d Navier-Stokes equation much faster than the best-performing prior method (9 minutes vs. 10 hours in a single GPU), maintaining accuracy.Finally, we showcase that SPINN can accurately obtain the solution of a highly nonlinear and multi-dimensional PDE, a (3+1)-d Navier-Stokes equation.For visualized results and code, please see https://jwcho5576.github.io/spinn.github.io/",
    "keywords": [],
    "checked": true,
    "id": "be3ecf03c30561b6be525398d85a1cd6aea12de8",
    "semantic_title": "separable physics-informed neural networks",
    "citation_count": 3,
    "authors": [
      "Junwoo Cho",
      "Seungtae Nam",
      "Hyunmo Yang",
      "Seok-Bae Yun",
      "Youngjoon Hong",
      "Eunbyung Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b0c1645f3d6a1730931e65ecbf91ac3-Abstract-Conference.html": {
    "title": "Beyond Invariance: Test-Time Label-Shift Adaptation for Addressing \"Spurious\" Correlations",
    "volume": "main",
    "abstract": "Changes in the data distribution at test time can have deleterious effects on the performance of predictive models $p(y|x)$.We consider situations where there are additional meta-data labels (such as group labels), denoted by $z$, that can account for such changes in the distribution.In particular, we assume that the prior distribution $p(y,z)$, which models the dependence between the class label $y$ and the \"nuisance\" factors $z$, may change across domains, either due to a change in the correlation between these terms, or a change in one of their marginals.However, we assume that the generative model for features $p(x|y,z)$ is invariant across domains.We note that this corresponds to an expanded version of the widely used \"label shift\" assumption, where the labels now also include the nuisance factors $z$. Based on this observation, we propose a test-time label shift correction that adapts to changes in the joint distribution $p(y, z)$ using EM applied to unlabeled samples from the target domain distribution, $p_t(x)$.Importantly, we are able to avoid fitting a generative model $p(x|y,z)$, and merely need to reweight the outputs of a discriminative model $p_s(y,z|x)$ trained on the source distribution.We evaluate our method, which we call \"Test-Time Label-Shift Adaptation\" (TTLSA), on several standard image and text datasets, as well as the CheXpert chest X-ray dataset, and show that it improves performance over methods that target invariance to changes in the distribution, as well as baseline empirical risk minimization methods.Code for reproducing experiments is available at https://github.com/nalzok/test-time-label-shift",
    "keywords": [],
    "checked": false,
    "id": "83b59784f9880f960a2504b4ec966898ed75ebc2",
    "semantic_title": "beyond invariance: test-time label-shift adaptation for distributions with \"spurious\" correlations",
    "citation_count": 0,
    "authors": [
      "Qingyao Sun",
      "Kevin P. Murphy",
      "Sayna Ebrahimi",
      "Alexander D'Amour"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b0eea69deea512c9e2c469187643dc2-Abstract-Conference.html": {
    "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
    "volume": "main",
    "abstract": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks",
    "keywords": [],
    "checked": true,
    "id": "d671d62a1eb4d57343e4a0928297266dffc0c118",
    "semantic_title": "swiftsage: a generative agent with fast and slow thinking for complex interactive tasks",
    "citation_count": 46,
    "authors": [
      "Bill Yuchen Lin",
      "Yicheng Fu",
      "Karina Yang",
      "Faeze Brahman",
      "Shiyu Huang",
      "Chandra Bhagavatula",
      "Prithviraj Ammanabrolu",
      "Yejin Choi",
      "Xiang Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b4d25dc0c52d3cf43d5b203cdfdf241-Abstract-Conference.html": {
    "title": "Gradient-Free Kernel Stein Discrepancy",
    "volume": "main",
    "abstract": "Stein discrepancies have emerged as a powerful statistical tool, being applied to fundamental statistical problems including parameter inference, goodness-of-fit testing, and sampling. The canonical Stein discrepancies require the derivatives of a statistical model to be computed, and in return provide theoretical guarantees of convergence detection and control. However, for complex statistical models, the stable numerical computation of derivatives can require bespoke algorithmic development and render Stein discrepancies impractical. This paper focuses on posterior approximation using Stein discrepancies, and introduces a collection of non-canonical Stein discrepancies that are gradient-free, meaning that derivatives of the statistical model are not required. Sufficient conditions for convergence detection and control are established, and applications to sampling and variational inference are presented",
    "keywords": [],
    "checked": true,
    "id": "3de034085b9c771abcb8d6f36d5c3f1cb443c5b8",
    "semantic_title": "gradient-free kernel stein discrepancy",
    "citation_count": 2,
    "authors": [
      "Matthew Fisher",
      "Chris J. Oates"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b4f1272c73d5afd222b6dd3391c3f77-Abstract-Conference.html": {
    "title": "ConDaFormer: Disassembled Transformer with Local Structure Enhancement for 3D Point Cloud Understanding",
    "volume": "main",
    "abstract": "Transformers have been recently explored for 3D point cloud understanding with impressive progress achieved. A large number of points, over 0.1 million, make the global self-attention infeasible for point cloud data. Thus, most methods propose to apply the transformer in a local region, e.g., spherical or cubic window. However, it still contains a large number of Query-Key pairs, which requires high computational costs. In addition, previous methods usually learn the query, key, and value using a linear projection without modeling the local 3D geometric structure. In this paper, we attempt to reduce the costs and model the local geometry prior by developing a new transformer block, named ConDaFormer. Technically, ConDaFormer disassembles the cubic window into three orthogonal 2D planes, leading to fewer points when modeling the attention in a similar range. The disassembling operation is beneficial to enlarging the range of attention without increasing the computational complexity, but ignores some contexts. To provide a remedy, we develop a local structure enhancement strategy that introduces a depth-wise convolution before and after the attention. This scheme can also capture the local geometric information. Taking advantage of these designs, ConDaFormer captures both long-range contextual information and local priors. The effectiveness is demonstrated by experimental results on several 3D point cloud understanding benchmarks. Our code will be available",
    "keywords": [],
    "checked": true,
    "id": "9a795ee8cef1847625e279e0212686c1b2ec3b69",
    "semantic_title": "condaformer: disassembled transformer with local structure enhancement for 3d point cloud understanding",
    "citation_count": 0,
    "authors": [
      "Lunhao Duan",
      "Shanshan Zhao",
      "Nan Xue",
      "Mingming Gong",
      "Gui-Song Xia",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b5721f7fcc1672930d860e0dfcfee84-Abstract-Conference.html": {
    "title": "Variational Monte Carlo on a Budget — Fine-tuning pre-trained Neural Wavefunctions",
    "volume": "main",
    "abstract": "Obtaining accurate solutions to the Schrödinger equation is the key challenge in computational quantum chemistry. Deep-learning-based Variational Monte Carlo (DL-VMC) has recently outperformed conventional approaches in terms of accuracy, but only at large computational cost.Whereas in many domains models are trained once and subsequently applied for inference, accurate DL-VMC so far requires a full optimization for every new problem instance, consuming thousands of GPUhs even for small molecules.We instead propose a DL-VMC model which has been pre-trained using self-supervised wavefunction optimization on a large and chemically diverse set of molecules. Applying this model to new molecules without any optimization, yields wavefunctions and absolute energies that outperform established methods such as CCSD(T)-2Z.To obtain accurate relative energies, only few fine-tuning steps of this base model are required.We accomplish this with a fully end-to-end machine-learned model, consisting of an improved geometry embedding architecture and an existing SE(3)-equivariant model to represent molecular orbitals. Combining this architecture with continuous sampling of geometries, we improve zero-shot accuracy by two orders of magnitude compared to the state of the art.We extensively evaluate the accuracy, scalability and limitations of our base model on a wide variety of test systems",
    "keywords": [],
    "checked": false,
    "id": "5e91307ff39dc482d3ee56c563b68021d777eb02",
    "semantic_title": "variational monte carlo on a budget - fine-tuning pre-trained neural wavefunctions",
    "citation_count": 0,
    "authors": [
      "Michael Scherbela",
      "Leon Gerard",
      "Philipp Grohs"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b5d47949866d06ab5c03022b4a5a551-Abstract-Conference.html": {
    "title": "ReDS: Offline RL With Heteroskedastic Datasets via Support Constraints",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) learns policies entirely from static datasets. Practical applications of offline RL will inevitably require learning from datasets where the variability of demonstrated behaviors changes non-uniformly across the state space. For example, at a red light, nearly all human drivers behave similarly by stopping, but when merging onto a highway, some drivers merge quickly, efficiently, and safely, while many hesitate or merge dangerously. Both theoretically and empirically, we show that typical offline RL methods, which are based on distribution constraints fail to learn from data with such non-uniform variability, due to the requirement to stay close to the behavior policy to the same extent across the state space. Ideally, the learned policy should be free to choose per state how closely to follow the behavior policy to maximize long-term return, as long as the learned policy stays within the support of the behavior policy. To instantiate this principle, we reweight the data distribution in conservative Q-learning (CQL) to obtain an approximate support constraint formulation. The reweighted distribution is a mixture of the current policy and an additional policy trained to mine poor actions that are likely under the behavior policy. Our method, CQL (ReDS), is theoretically motivated, and improves performance across a wide range of offline RL problems in games, navigation, and pixel-based manipulation",
    "keywords": [],
    "checked": false,
    "id": "bacc496fc1adac13c2913638a75448e363a4b258",
    "semantic_title": "offline rl with realistic datasets: heteroskedasticity and support constraints",
    "citation_count": 8,
    "authors": [
      "Anikait Singh",
      "Aviral Kumar",
      "Quan Vuong",
      "Yevgen Chebotar",
      "Sergey Levine"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b6898c70d5b328deaf2216aefd8f77a-Abstract-Conference.html": {
    "title": "A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Open-world semi-supervised learning aims at inferring both known and novel classes in unlabeled data, by harnessing prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for this problem. This paper bridges the gap by formalizing a graph-theoretic framework tailored for the open-world setting, where the clustering can be theoretically characterized by graph factorization. Our graph-theoretic framework illuminates practical algorithms and provides guarantees. In particular, based on our graph formulation, we apply the algorithm called Spectral Open-world Representation Learning (SORL), and show that minimizing our loss is equivalent to performing spectral decomposition on the graph. Such equivalence allows us to derive a provable error bound on the clustering performance for both known and novel classes, and analyze rigorously when labeled data helps. Empirically, SORL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees",
    "keywords": [],
    "checked": true,
    "id": "23784d956043af0c247d7e985f15be407a3edd68",
    "semantic_title": "a graph-theoretic framework for understanding open-world semi-supervised learning",
    "citation_count": 0,
    "authors": [
      "Yiyou Sun",
      "Zhenmei Shi",
      "Yixuan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b719e74623f4fa238ded71b56f0a184-Abstract-Conference.html": {
    "title": "Near Optimal Reconstruction of Spherical Harmonic Expansions",
    "volume": "main",
    "abstract": "We propose an algorithm for robust recovery of the spherical harmonic expansion of functions defined on the $d$-dimensional unit sphere $\\mathbb{S}^{d-1}$ using a near-optimal number of function evaluations. We show that for any $f\\in L^2(\\mathbb{S}^{d-1})$, the number of evaluations of $f$ needed to recover its degree-$q$ spherical harmonic expansion equals the dimension of the space of spherical harmonics of degree at most $q$, up to a logarithmic factor. Moreover, we develop a simple yet efficient kernel regression-based algorithm to recover degree-$q$ expansion of $f$ by only evaluating the function on uniformly sampled points on $\\mathbb{S}^{d-1}$. Our algorithm is built upon the connections between spherical harmonics and Gegenbauer polynomials. Unlike the prior results on fast spherical harmonic transform, our proposed algorithm works efficiently using a nearly optimal number of samples in any dimension $d$. Furthermore, we illustrate the empirical performance of our algorithm on numerical examples",
    "keywords": [],
    "checked": true,
    "id": "8c7151e5cb21e8f95cc8b92da2d9999ef421cfca",
    "semantic_title": "near optimal reconstruction of spherical harmonic expansions",
    "citation_count": 0,
    "authors": [
      "Amir Zandieh",
      "Insu Han",
      "Haim Avron"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b734e95f0788a030a69caa987516186-Abstract-Conference.html": {
    "title": "Lexinvariant Language Models",
    "volume": "main",
    "abstract": "Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of any language model (LM). However, lexical symbol meanings can also be determined and even redefined by their structural role in a long context. In this paper, we ask: is it possible for a language model to be performant without \\emph{any} fixed token embeddings? Such a language model would have to rely entirely on the co-occurence and repetition of tokens in the context rather than the \\textit{a priori} identity of any token. To answer this, we study \\textit{lexinvariant}language models that are invariant to lexical symbols and therefore do not need fixed token embeddings in practice. First, we prove that we can construct a lexinvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode tokens using random Gaussian vectors, such that each token maps to the same representation within each sequence but different representations across sequences. Empirically, we demonstrate that it can indeed attain perplexity comparable to that of a standard language model, given a sufficiently long context. We further explore two properties of the lexinvariant language models: First, given text generated from a substitution cipher of English, it implicitly implements Bayesian in-context deciphering and infers the mapping to the underlying real tokens with high accuracy. Second, it has on average 4X better accuracy over synthetic in-context reasoning tasks. Finally, we discuss regularizing standard language models towards lexinvariance and potential practical applications",
    "keywords": [],
    "checked": true,
    "id": "ac36b65a6fa4a9e84de051f0d3e9d50348fa4160",
    "semantic_title": "lexinvariant language models",
    "citation_count": 0,
    "authors": [
      "Qian Huang",
      "Eric Zelikman",
      "Sarah Chen",
      "Yuhuai Wu",
      "Gregory Valiant",
      "Percy S. Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b7439a4ab0b8e4bcb4e2412c6a10a58-Abstract-Conference.html": {
    "title": "REFINE: A Fine-Grained Medication Recommendation System Using Deep Learning and Personalized Drug Interaction Modeling",
    "volume": "main",
    "abstract": "Patients with co-morbidities often require multiple medications to manage their conditions. However, existing medication recommendation systems only offer class-level medications and regard all interactions among drugs to have the same level of severity. This limits their ability to provide personalized and safe recommendations tailored to individual needs. In this work, we introduce a deep learning-based fine-grained medication recommendation system called REFINE, which is designed to improve treatment outcomes and minimize adverse drug interactions. In order to better characterize patients' health conditions, we model the trend in medication dosage titrations and lab test responses, and adapt the vision transformer to obtain effective patient representations. We also model drug interaction severity levels as weighted graphs to learn safe drug combinations and design a balanced loss function to avoid overly conservative recommendations and miss medications that might be needed for certain conditions. Extensive experiments on two real-world datasets show that REFINE outperforms state-of-the-art techniques",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suman Bhoi",
      "Mong Li Lee",
      "Wynne Hsu",
      "Ngiap Chuan Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b8afc47273c746662a96dfdf562f87f-Abstract-Conference.html": {
    "title": "Bayesian Extensive-Rank Matrix Factorization with Rotational Invariant Priors",
    "volume": "main",
    "abstract": "We consider a statistical model for matrix factorization in a regime where the rank of the two hidden matrix factors grows linearly with their dimension and their product is corrupted by additive noise. Despite various approaches, statistical and algorithmic limits of such problems have remained elusive. We study a Bayesian setting with the assumptions that (a) one of the matrix factors is symmetric, (b) both factors as well as the additive noise have rotational invariant priors, (c) the priors are known to the statistician. We derive analytical formulas for Rotation Invariant Estimators to reconstruct the two matrix factors, and conjecture that these are optimal in the large-dimension limit, in the sense that they minimize the average mean-square-error. We provide numerical checks which confirm the optimality conjecture when confronted to Oracle Estimators which are optimal by definition, but involve the ground-truth. Our derivation relies on a combination of tools, namely random matrix theory transforms, spherical integral formulas, and the replica method from statistical mechanics",
    "keywords": [],
    "checked": true,
    "id": "143824ed281c2ec88950af149d2e8092c6b6d7f8",
    "semantic_title": "bayesian extensive-rank matrix factorization with rotational invariant priors",
    "citation_count": 0,
    "authors": [
      "Farzad Pourkamali",
      "Nicolas Macris"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b91825aec2ed35150f1d3e8fb195556-Abstract-Conference.html": {
    "title": "Optimal Transport Model Distributional Robustness",
    "volume": "main",
    "abstract": "Distributional robustness is a promising framework for training deep learning models that are less vulnerable to adversarial examples and data distribution shifts. Previous works have mainly focused on exploiting distributional robustness in the data space. In this work, we explore an optimal transport-based distributional robustness framework in model spaces. Specifically, we examine a model distribution within a Wasserstein ball centered on a given model distribution that maximizes the loss. We have developed theories that enable us to learn the optimal robust center model distribution. Interestingly, our developed theories allow us to flexibly incorporate the concept of sharpness awareness into training, whether it's a single model, ensemble models, or Bayesian Neural Networks, by considering specific forms of the center model distribution. These forms include a Dirac delta distribution over a single model, a uniform distribution over several models, and a general Bayesian Neural Network. Furthermore, we demonstrate that Sharpness-Aware Minimization (SAM) is a specific case of our framework when using a Dirac delta distribution over a single model, while our framework can be seen as a probabilistic extension of SAM. To validate the effectiveness of our framework in the aforementioned settings, we conducted extensive experiments, and the results reveal remarkable improvements compared to the baselines",
    "keywords": [],
    "checked": true,
    "id": "7e0a4562b155be05917043846c468d8c1f7f01d0",
    "semantic_title": "optimal transport model distributional robustness",
    "citation_count": 2,
    "authors": [
      "Van-Anh Nguyen",
      "Trung Le",
      "Anh Bui",
      "Thanh-Toan Do",
      "Dinh Phung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4bbd69ce4cea6aa3a08bde08a40fd65a-Abstract-Conference.html": {
    "title": "Language Semantic Graph Guided Data-Efficient Learning",
    "volume": "main",
    "abstract": "Developing generalizable models that can effectively learn from limited data and with minimal reliance on human supervision is a significant objective within the machine learning community, particularly in the era of deep neural networks. Therefore, to achieve data-efficient learning, researchers typically explore approaches that can leverage more related or unlabeled data without necessitating additional manual labeling efforts, such as Semi-Supervised Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA).SSL leverages unlabeled data in the training process, while TL enables the transfer of expertise from related data distributions. DA broadens the dataset by synthesizing new data from existing examples. However, the significance of additional knowledge contained within labels has been largely overlooked in research. In this paper, we propose a novel perspective on data efficiency that involves exploiting the semantic information contained in the labels of the available data. Specifically, we introduce a Language Semantic Graph (LSG) which is constructed from labels manifest as natural language descriptions. Upon this graph, an auxiliary graph neural network is trained to extract high-level semantic relations and then used to guide the training of the primary model, enabling more adequate utilization of label knowledge. Across image, video, and audio modalities, we utilize the LSG method in both TL and SSL scenarios and illustrate its versatility in significantly enhancing performance compared to other data-efficient learning approaches. Additionally, our in-depth analysis shows that the LSG method also expedites the training process",
    "keywords": [],
    "checked": true,
    "id": "a0517287a30206ac15104300c5140d0ebcd8d24b",
    "semantic_title": "language semantic graph guided data-efficient learning",
    "citation_count": 0,
    "authors": [
      "Wenxuan Ma",
      "Shuang Li",
      "lincan Cai",
      "Jingxuan Kang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4bc6e94f2308c888fb69626138a2633e-Abstract-Conference.html": {
    "title": "Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations",
    "volume": "main",
    "abstract": "The efficient coding hypothesis proposes that the response properties of sensory systems are adapted to the statistics of their inputs such that they capture maximal information about the environment, subject to biological constraints. While elegant, information theoretic properties are notoriously difficult to measure in practical settings or to employ as objective functions in optimization. This difficulty has necessitated that computational models designed to test the hypothesis employ several different information metrics ranging from approximations and lower bounds to proxy measures like reconstruction error. Recent theoretical advances have characterized a novel and ecologically relevant efficiency metric, the ``manifold capacity,\" which is the number of object categories that may be represented in a linearly separable fashion. However, calculating manifold capacity is a computationally intensive iterative procedure that until now has precluded its use as an objective. Here we outline the simplifying assumptions that allow manifold capacity to be optimized directly, yielding Maximum Manifold Capacity Representations (MMCR). The resulting method is closely related to and inspired by advances in the field of self supervised learning (SSL), and we demonstrate that MMCRs are competitive with state of the art results on standard SSL benchmarks. Empirical analyses reveal differences between MMCRs and representations learned by other SSL frameworks, and suggest a mechanism by which manifold compression gives rise to class separability. Finally we evaluate a set of SSL methods on a suite of neural predicitivity benchmarks, and find MMCRs are higly competitive as models of the ventral stream",
    "keywords": [],
    "checked": true,
    "id": "edcdb546d54bf4305aa853bbdc238227dc6c4740",
    "semantic_title": "learning efficient coding of natural images with maximum manifold capacity representations",
    "citation_count": 3,
    "authors": [
      "Thomas Yerxa",
      "Yilun Kuang",
      "Eero Simoncelli",
      "SueYeon Chung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4bfcebedf7a2967c410b64670f27f904-Abstract-Conference.html": {
    "title": "Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry",
    "volume": "main",
    "abstract": "Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space $\\mathbf{x}_t \\in \\mathcal{X}$, we analyze them from a geometrical perspective. Our approach involves deriving the local latent basis within $\\mathcal{X}$ by leveraging the pullback metric associated with their encoding feature maps. Remarkably, our discovered local latent basis enables image editing capabilities by moving $\\mathbf{x}_t$, the latent space of DMs, along the basis vector at specific timesteps. We further analyze how the geometric structure of DMs evolves over diffusion timesteps and differs across different text conditions. This confirms the known phenomenon of coarse-to-fine generation, as well as reveals novel insights such as the discrepancy between $\\mathbf{x}_t$ across timesteps, the effect of dataset complexity, and the time-varying influence of text prompts. To the best of our knowledge, this paper is the first to present image editing through $\\mathbf{x}$-space traversal, editing only once at specific timestep $t$ without any additional training, and providing thorough analyses of the latent structure of DMs.The code to reproduce our experiments can be found at the [link](https://github.com/enkeejunior1/Diffusion-Pullback)",
    "keywords": [],
    "checked": true,
    "id": "d7074976c2609568902a6b6ca45f6c71d9cb66bf",
    "semantic_title": "understanding the latent space of diffusion models through the lens of riemannian geometry",
    "citation_count": 4,
    "authors": [
      "Yong-Hyun Park",
      "Mingi Kwon",
      "Jaewoong Choi",
      "Junghyo Jo",
      "Youngjung Uh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4bfe7af38d4e5cd85ae0da639a933652-Abstract-Conference.html": {
    "title": "Single-Stage Visual Query Localization in Egocentric Videos",
    "volume": "main",
    "abstract": "Visual Query Localization on long-form egocentric videos requires spatio-temporal search and localization of visually specified objects and is vital to build episodic memory systems. Prior work develops complex multi-stage pipelines that leverage well-established object detection and tracking methods to perform VQL. However, each stage is independently trained and the complexity of the pipeline results in slow inference speeds. We propose VQLoC, a novel single-stage VQL framework that is end-to-end trainable. Our key idea is to first build a holistic understanding of the query-video relationship and then perform spatio-temporal localization in a single shot manner. Specifically, we establish the query-video relationship by jointly considering query-to-frame correspondences between the query and each video frame and frame-to-frame correspondences between nearby video frames. Our experiments demonstrate that our approach outperforms prior VQL methods by $20$% accuracy while obtaining a $10\\times$ improvement in inference speed. VQLoC is also the top entry on the Ego4D VQ2D challenge leaderboard",
    "keywords": [],
    "checked": true,
    "id": "4b91e9814270cb83041bb031cc007e6791bdaa94",
    "semantic_title": "single-stage visual query localization in egocentric videos",
    "citation_count": 1,
    "authors": [
      "Hanwen Jiang",
      "Santhosh Kumar Ramakrishnan",
      "Kristen Grauman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4c2092ec0b1370cce3fb5965ab255fae-Abstract-Conference.html": {
    "title": "Generalizing Importance Weighting to A Universal Solver for Distribution Shift Problems",
    "volume": "main",
    "abstract": "Distribution shift (DS) may have two levels: the distribution itself changes, and the support (i.e., the set where the probability density is non-zero) also changes. When considering the support change between the training and test distributions, there can be four cases: (i) they exactly match; (ii) the training support is wider (and thus covers the test support); (iii) the test support is wider; (iv) they partially overlap. Existing methods are good at cases (i) and (ii), while cases (iii) and (iv) are more common nowadays but still under-explored. In this paper, we generalize importance weighting (IW), a golden solver for cases (i) and (ii), to a universal solver for all cases. Specifically, we first investigate why IW might fail in cases (iii) and (iv); based on the findings, we propose generalized IW (GIW) that could handle cases (iii) and (iv) and would reduce to IW in cases (i) and (ii). In GIW, the test support is split into an in-training (IT) part and an out-of-training (OOT) part, and the expected risk is decomposed into a weighted classification term over the IT part and a standard classification term over the OOT part, which guarantees the risk consistency of GIW. Then, the implementation of GIW consists of three components: (a) the split of validation data is carried out by the one-class support vector machine, (b) the first term of the empirical risk can be handled by any IW algorithm given training data and IT validation data, and (c) the second term just involves OOT validation data. Experiments demonstrate that GIW is a universal solver for DS problems, outperforming IW methods in cases (iii) and (iv)",
    "keywords": [],
    "checked": true,
    "id": "992122be8818e63f396c1b032a30600ac582b66a",
    "semantic_title": "generalizing importance weighting to a universal solver for distribution shift problems",
    "citation_count": 0,
    "authors": [
      "Tongtong Fang",
      "Nan Lu",
      "Gang Niu",
      "Masashi Sugiyama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4c454d34f3a4c8d6b4ca85a918e5d7ba-Abstract-Conference.html": {
    "title": "Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tailed Noise",
    "volume": "main",
    "abstract": "In this work, we study the convergence in high probability of clipped gradient methods when the noise distribution has heavy tails, i.e., with bounded $p$th moments, for some $1<p\\le2$. Prior works in this setting follow the same recipe of using concentration inequalities and an inductive argument with union bound to bound the iterates across all iterations. This method results in an increase in the failure probability by a factor of $T$, where $T$ is the number of iterations. We instead propose a new analysis approach based on bounding the moment generating function of a well chosen supermartingale sequence. We improve the dependency on $T$ in the convergence guarantee for a wide range of algorithms with clipped gradients, including stochastic (accelerated) mirror descent for convex objectives and stochastic gradient descent for nonconvex objectives. Our high probability bounds achieve the optimal convergence rates and match the best currently known in-expectation bounds. Our approach naturally allows the algorithms to use time-varying step sizes and clipping parameters when the time horizon is unknown, which appears difficult or even impossible using the techniques from prior works. Furthermore, we show that in the case of clipped stochastic mirror descent, several problem constants, including the initial distance to the optimum, are not required when setting step sizes and clipping parameters",
    "keywords": [],
    "checked": false,
    "id": "c89f5c58419f18a52d7d650254556b8437139aec",
    "semantic_title": "improved convergence in high probability of clipped gradient methods with heavy tails",
    "citation_count": 6,
    "authors": [
      "Ta Duy Nguyen",
      "Thien H Nguyen",
      "Alina Ene",
      "Huy Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4c5722bad9759216474df8fc46c97af2-Abstract-Conference.html": {
    "title": "Refining Diffusion Planner for Reliable Behavior Synthesis by Automatic Detection of Infeasible Plans",
    "volume": "main",
    "abstract": "Diffusion-based planning has shown promising results in long-horizon, sparse-reward tasks by training trajectory diffusion models and conditioning the sampled trajectories using auxiliary guidance functions. However, due to their nature as generative models, diffusion models are not guaranteed to generate feasible plans, resulting in failed execution and precluding planners from being useful in safety-critical applications. In this work, we propose a novel approach to refine unreliable plans generated by diffusion models by providing refining guidance to error-prone plans. To this end, we suggest a new metric named restoration gap for evaluating the quality of individual plans generated by the diffusion model. A restoration gap is estimated by a gap predictor which produces restoration gap guidance to refine a diffusion planner. We additionally present an attribution map regularizer to prevent adversarial refining guidance that could be generated from the sub-optimal gap predictor, which enables further refinement of infeasible plans. We demonstrate the effectiveness of our approach on three different benchmarks in offline control settings that require long-horizon planning. We also illustrate that our approach presents explainability by presenting the attribution maps of the gap predictor and highlighting error-prone transitions, allowing for a deeper understanding of the generated plans",
    "keywords": [],
    "checked": true,
    "id": "9d6ad3fc3d0575152fb755722d8e76ea512b9113",
    "semantic_title": "refining diffusion planner for reliable behavior synthesis by automatic detection of infeasible plans",
    "citation_count": 1,
    "authors": [
      "Kyowoon Lee",
      "Seongun Kim",
      "Jaesik Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4c5e2bcbf21bdf40d75fddad0bd43dc9-Abstract-Conference.html": {
    "title": "Generate What You Prefer: Reshaping Sequential Recommendation via Guided Diffusion",
    "volume": "main",
    "abstract": "Sequential recommendation aims to recommend the next item that matches a user'sinterest, based on the sequence of items he/she interacted with before. Scrutinizingprevious studies, we can summarize a common learning-to-classify paradigm—given a positive item, a recommender model performs negative sampling to addnegative items and learns to classify whether the user prefers them or not, based onhis/her historical interaction sequence. Although effective, we reveal two inherentlimitations: (1) it may differ from human behavior in that a user could imaginean oracle item in mind and select potential items matching the oracle; and (2)the classification is limited in the candidate pool with noisy or easy supervisionfrom negative samples, which dilutes the preference signals towards the oracleitem. Yet, generating the oracle item from the historical interaction sequence ismostly unexplored. To bridge the gap, we reshape sequential recommendationas a learning-to-generate paradigm, which is achieved via a guided diffusionmodel, termed DreamRec. Specifically, for a sequence of historical items, itapplies a Transformer encoder to create guidance representations. Noising targetitems explores the underlying distribution of item space; then, with the guidance ofhistorical interactions, the denoising process generates an oracle item to recoverthe positive item, so as to cast off negative sampling and depict the true preferenceof the user directly. We evaluate the effectiveness of DreamRec through extensiveexperiments and comparisons with existing methods. Codes and data are open-sourcedat https://github.com/YangZhengyi98/DreamRec",
    "keywords": [],
    "checked": true,
    "id": "c718c0a7185b16edc29432fe943489ca630c3c48",
    "semantic_title": "generate what you prefer: reshaping sequential recommendation via guided diffusion",
    "citation_count": 2,
    "authors": [
      "Zhengyi Yang",
      "Jiancan Wu",
      "Zhicai Wang",
      "Xiang Wang",
      "Yancheng Yuan",
      "Xiangnan He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4c79c359b3c5f077c0b955f93cb0f53e-Abstract-Conference.html": {
    "title": "Conditional score-based diffusion models for Bayesian inference in infinite dimensions",
    "volume": "main",
    "abstract": "Since their initial introduction, score-based diffusion models (SDMs) have been successfully applied to solve a variety of linear inverse problems in finite-dimensional vector spaces due to their ability to efficiently approximate the posterior distribution. However, using SDMs for inverse problems in infinite-dimensional function spaces has only been addressed recently, primarily through methods that learn the unconditional score. While this approach is advantageous for some inverse problems, it is mostly heuristic and involves numerous computationally costly forward operator evaluations during posterior sampling. To address these limitations, we propose a theoretically grounded method for sampling from the posterior of infinite-dimensional Bayesian linear inverse problems based on amortized conditional SDMs. In particular, we prove that one of the most successful approaches for estimating the conditional score in finite dimensions—the conditional denoising estimator—can also be applied in infinite dimensions. A significant part of our analysis is dedicated to demonstrating that extending infinite-dimensional SDMs to the conditional setting requires careful consideration, as the conditional score typically blows up for small times, contrarily to the unconditional score. We conclude by presenting stylized and large-scale numerical examples that validate our approach, offer additional insights, and demonstrate that our method enables large-scale, discretization-invariant Bayesian inference",
    "keywords": [],
    "checked": true,
    "id": "16a5c629a59d87a2616afc2b94cd37b9e2f5063f",
    "semantic_title": "conditional score-based diffusion models for bayesian inference in infinite dimensions",
    "citation_count": 3,
    "authors": [
      "Lorenzo Baldassari",
      "Ali Siahkoohi",
      "Josselin Garnier",
      "Knut Solna",
      "Maarten V. de Hoop"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4c8ce3c63f6b66d6811c6d67c68e487b-Abstract-Conference.html": {
    "title": "Provable Advantage of Curriculum Learning on Parity Targets with Mixed Inputs",
    "volume": "main",
    "abstract": "Experimental results have shown that curriculum learning, i.e., presenting simpler examples before more complex ones, can improve the efficiency of learning. Some recent theoretical results also showed that changing the sampling distribution can help neural networks learn parities, with formal results only for large learning rates and one-step arguments. Here we show a separation result in the number of training steps with standard (bounded) learning rates on a common sample distribution: if the data distribution is a mixture of sparse and dense inputs, there exists a regime in which a 2-layer ReLU neural network trained by a curriculum noisy-GD (or SGD) algorithm that uses sparse examples first, can learn parities of sufficiently large degree, while any fully connected neural network of possibly larger width or depth trained by noisy-GD on the unordered samples cannot learn without additional steps. We also provide experimental results supporting the qualitative separation beyond the specific regime of the theoretical results",
    "keywords": [],
    "checked": true,
    "id": "1de63f374706b35f351ae04bf0a5fba3fae20ce5",
    "semantic_title": "provable advantage of curriculum learning on parity targets with mixed inputs",
    "citation_count": 2,
    "authors": [
      "Emmanuel Abbe",
      "Elisabetta Cornacchia",
      "Aryo Lotfi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4c9477b9e2c7ec0ad3f4f15077aaf85a-Abstract-Conference.html": {
    "title": "Modeling Human Visual Motion Processing with Trainable Motion Energy Sensing and a Self-attention Network",
    "volume": "main",
    "abstract": "Visual motion processing is essential for humans to perceive and interact with dynamic environments. Despite extensive research in cognitive neuroscience, image-computable models that can extract informative motion flow from natural scenes in a manner consistent with human visual processing have yet to be established. Meanwhile, recent advancements in computer vision (CV), propelled by deep learning, have led to significant progress in optical flow estimation, a task closely related to motion perception. Here we propose an image-computable model of human motion perception by bridging the gap between biological and CV models. Specifically, we introduce a novel two-stages approach that combines trainable motion energy sensing with a recurrent self-attention network for adaptive motion integration and segregation. This model architecture aims to capture the computations in V1-MT, the core structure for motion perception in the biological visual system, while providing the ability to derive informative motion flow for a wide range of stimuli, including complex natural scenes. In silico neurophysiology reveals that our model's unit responses are similar to mammalian neural recordings regarding motion pooling and speed tuning. The proposed model can also replicate human responses to a range of stimuli examined in past psychophysical studies. The experimental results on the Sintel benchmark demonstrate that our model predicts human responses better than the ground truth, whereas the state-of-the-art CV models show the opposite. Our study provides a computational architecture consistent with human visual motion processing, although the physiological correspondence may not be exact",
    "keywords": [],
    "checked": false,
    "id": "8101ca751cd932ef6a5e2d6ea287d14165ced498",
    "semantic_title": "modelling human visual motion processing with trainable motion energy sensing and a self-attention network for adaptive motion integration",
    "citation_count": 0,
    "authors": [
      "Zitang Sun",
      "Yen-Ju Chen",
      "Yung-Hao Yang",
      "Shin'ya Nishida"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4cbec10b0cf25025e3f9fcfd943bb58c-Abstract-Conference.html": {
    "title": "Self-Supervised Visual Acoustic Matching",
    "volume": "main",
    "abstract": "Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment. Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples. We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio---without acoustically mismatched source audio for reference. Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio. Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments",
    "keywords": [],
    "checked": true,
    "id": "cc55253297f340c316485b9575d3632302ec8b39",
    "semantic_title": "self-supervised visual acoustic matching",
    "citation_count": 1,
    "authors": [
      "Arjun Somayazulu",
      "Changan Chen",
      "Kristen Grauman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4cca5640267b416cef4f00630aef93a2-Abstract-Conference.html": {
    "title": "Optimal Excess Risk Bounds for Empirical Risk Minimization on $p$-Norm Linear Regression",
    "volume": "main",
    "abstract": "We study the performance of empirical risk minimization on the $p$-norm linear regression problem for $p \\in (1, \\infty)$. We show that, in the realizable case, under no moment assumptions, and up to a distribution-dependent constant, $O(d)$ samples are enough to exactly recover the target. Otherwise, for $p \\in [2, \\infty)$, and under weak moment assumptions on the target and the covariates, we prove a high probability excess risk bound on the empirical risk minimizer whose leading term matches, up to a constant that depends only on $p$, the asymptotically exact rate. We extend this result to the case $p \\in (1, 2)$ under mild assumptions that guarantee the existence of the Hessian of the risk at its minimizer",
    "keywords": [],
    "checked": true,
    "id": "4db9e23a1460f7a3555a696ac1ce6330f8ebabf5",
    "semantic_title": "optimal excess risk bounds for empirical risk minimization on $p$-norm linear regression",
    "citation_count": 0,
    "authors": [
      "Ayoub El Hanchi",
      "Murat A. Erdogdu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4ccf72339d1f650cb898c55dccbc5cda-Abstract-Conference.html": {
    "title": "Failure-Aware Gaussian Process Optimization with Regret Bounds",
    "volume": "main",
    "abstract": "Real-world optimization problems often require black-box optimization with observation failure, where we can obtain the objective function value if we succeed, otherwise, we can only obtain a fact of failure. Moreover, this failure region can be complex by several latent constraints, whose number is also unknown. For this problem, we propose a failure-aware Gaussian process upper confidence bound (F-GP-UCB), which only requires a mild assumption for the observation failure that an optimal solution lies on an interior of a feasible region. Furthermore, we show that the number of successful observations grows linearly, by which we provide the first regret upper bounds and the convergence of F-GP-UCB. We demonstrate the effectiveness of F-GP-UCB in several benchmark functions, including the simulation function motivated by material synthesis experiments",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shogo Iwazaki",
      "Shion Takeno",
      "Tomohiko Tanabe",
      "Mitsuru Irie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4cddc8fc57039f8fe44e23aba1e4df40-Abstract-Conference.html": {
    "title": "Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of the manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no prior information about the underlying environment and the agents' algorithms",
    "keywords": [],
    "checked": true,
    "id": "1bed30e9712817f8b750f89fa7046321c5584354",
    "semantic_title": "efficient adversarial attacks on online multi-agent reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Guanlin Liu",
      "Lifeng LAI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4d0b6303d4a4811445f69f357bf6def5-Abstract-Conference.html": {
    "title": "Similarity-based cooperative equilibrium",
    "volume": "main",
    "abstract": "As machine learning agents act more autonomously in the world, they will increasingly interact with each other. Unfortunately, in many social dilemmas like the one-shot Prisoner's Dilemma, standard game theory predicts that ML agents will fail to cooperate with each other. Prior work has shown that one way to enable cooperative outcomes in the one-shot Prisoner's Dilemma is to make the agents mutually transparent to each other, i.e., to allow them to access one another's source code (Rubinstein, 1998; Tennenholtz, 2004) – or weights in the case of ML agents. However, full transparency is often unrealistic, whereas partial transparency is commonplace. Moreover, it is challenging for agents to learn their way to cooperation in the full transparency setting. In this paper, we introduce a more realistic setting in which agents only observe a single number indicating how similar they are to each other. We prove that this allows for the same set of cooperative outcomes as the full transparency setting. We also demonstrate experimentally that cooperation can be learned using simple ML methods",
    "keywords": [],
    "checked": true,
    "id": "435dbef1d60cf2bf282b2b139703fd1609ad0a81",
    "semantic_title": "similarity-based cooperative equilibrium",
    "citation_count": 3,
    "authors": [
      "Caspar Oesterheld",
      "Johannes Treutlein",
      "Roger B. Grosse",
      "Vincent Conitzer",
      "Jakob Foerster"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4d4a3b6a34332d80349137bcc98164a5-Abstract-Conference.html": {
    "title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning",
    "volume": "main",
    "abstract": "Aligning language models (LMs) with preferences is an important problem in natural language generation. A key challenge is that preferences are typically provided at the sequence level while LM training and generation both occur at the token level. There is, therefore, a granularity mismatch between the preference and the LM training losses, which may complicate the learning problem. In this paper, we address this issue by developing an alternate training process, where we iterate between grounding the sequence-level preference into token-level training guidance, and improving the LM with the learned guidance. For guidance learning, we design a framework that extends the pairwise-preference learning in imitation learning to both variable-length LM generation and the utilization of the preference among multiple generations. For LM training, based on the amount of supervised data, we present two minimalist learning objectives that utilize the learned guidance. In experiments, our method performs competitively on two distinct representative LM tasks --- discrete-prompt generation and text summarization",
    "keywords": [],
    "checked": true,
    "id": "b3f272d644fe580d18f635be4d6ac4c520ef0d0f",
    "semantic_title": "preference-grounded token-level guidance for language model fine-tuning",
    "citation_count": 3,
    "authors": [
      "Shentao Yang",
      "Shujian Zhang",
      "Congying Xia",
      "Yihao Feng",
      "Caiming Xiong",
      "Mingyuan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4d689f0f30199661a10aa2200488aebb-Abstract-Conference.html": {
    "title": "Joint Feature and Differentiable $ k $-NN Graph Learning using Dirichlet Energy",
    "volume": "main",
    "abstract": "Feature selection (FS) plays an important role in machine learning, which extracts important features and accelerates the learning process. In this paper, we propose a deep FS method that simultaneously conducts feature selection and differentiable $ k $-NN graph learning based on the Dirichlet Energy. The Dirichlet Energy identifies important features by measuring their smoothness on the graph structure, and facilitates the learning of a new graph that reflects the inherent structure in new feature subspace. We employ Optimal Transport theory to address the non-differentiability issue of learning $ k $-NN graphs in neural networks, which theoretically makes our method applicable to other graph neural networks for dynamic graph learning. Furthermore, the proposed framework is interpretable, since all modules are designed algorithmically. We validate the effectiveness of our model with extensive experiments on both synthetic and real-world datasets",
    "keywords": [],
    "checked": false,
    "id": "379ab92cf1ef46f9d304c2181e147548521641a5",
    "semantic_title": "joint feature and differentiable k-nn graph learning using dirichlet energy",
    "citation_count": 0,
    "authors": [
      "Lei Xu",
      "Lei Chen",
      "Rong Wang",
      "Feiping Nie",
      "Xuelong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4d69c1c057a8bd570ba4a7b71aae8331-Abstract-Conference.html": {
    "title": "Transformers learn through gradual rank increase",
    "volume": "main",
    "abstract": "We identify incremental learning dynamics in transformers, where the difference between trained and initial weights progressively increases in rank. We rigorously prove this occurs under the simplifying assumptions of diagonal weight matrices and small initialization. Our experiments support the theory and also show that phenomenon can occur in practice without the simplifying assumptions",
    "keywords": [],
    "checked": true,
    "id": "194a048814acc5cd5a9ee08102df3dcb61b2dfc9",
    "semantic_title": "transformers learn through gradual rank increase",
    "citation_count": 9,
    "authors": [
      "Emmanuel Abbe",
      "Samy Bengio",
      "Enric Boix-Adsera",
      "Etai Littwin",
      "Joshua Susskind"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4d893f766ab60e5337659b9e71883af4-Abstract-Conference.html": {
    "title": "Prototype-based Aleatoric Uncertainty Quantification for Cross-modal Retrieval",
    "volume": "main",
    "abstract": "Cross-modal Retrieval methods build similarity relations between vision and language modalities by jointly learning a common representation space. However, the predictions are often unreliable due to the Aleatoric uncertainty, which is induced by low-quality data, e.g., corrupt images, fast-paced videos, and non-detailed texts. In this paper, we propose a novel Prototype-based Aleatoric Uncertainty Quantification (PAU) framework to provide trustworthy predictions by quantifying the uncertainty arisen from the inherent data ambiguity. Concretely, we first construct a set of various learnable prototypes for each modality to represent the entire semantics subspace. Then Dempster-Shafer Theory and Subjective Logic Theory are utilized to build an evidential theoretical framework by associating evidence with Dirichlet Distribution parameters. The PAU model induces accurate uncertainty and reliable predictions for cross-modal retrieval. Extensive experiments are performed on four major benchmark datasets of MSR-VTT, MSVD, DiDeMo, and MS-COCO, demonstrating the effectiveness of our method. The code is accessible at https://github.com/leolee99/PAU",
    "keywords": [],
    "checked": true,
    "id": "689dd213ece0b92ce8b2becd0cf5bf2fc438ca11",
    "semantic_title": "prototype-based aleatoric uncertainty quantification for cross-modal retrieval",
    "citation_count": 1,
    "authors": [
      "Hao Li",
      "Jingkuan Song",
      "Lianli Gao",
      "Xiaosu Zhu",
      "Hengtao Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4d9944ab3330fe6af8efb9260aa9f307-Abstract-Conference.html": {
    "title": "A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference",
    "volume": "main",
    "abstract": "We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in performance",
    "keywords": [],
    "checked": true,
    "id": "8951dbf60a11838c78bffde64efba73da9a297cc",
    "semantic_title": "a-nesi: a scalable approximate method for probabilistic neurosymbolic inference",
    "citation_count": 11,
    "authors": [
      "Emile van Krieken",
      "Thiviyan Thanapalasingam",
      "Jakub Tomczak",
      "Frank van Harmelen",
      "Annette Ten Teije"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4dade38eae8c007f3a564b8ea820664a-Abstract-Conference.html": {
    "title": "Global Convergence Analysis of Local SGD for Two-layer Neural Network without Overparameterization",
    "volume": "main",
    "abstract": "Local SGD, a cornerstone algorithm in federated learning, is widely used in training deep neural networks and shown to have strong empirical performance. A theoretical understanding of such performance on nonconvex loss landscapes is currently lacking. Analysis of the global convergence of SGD is challenging, as the noise depends on the model parameters. Indeed, many works narrow their focus to GD and rely on injecting noise to enable convergence to the local or global optimum. When expanding the focus to local SGD, existing analyses in the nonconvex case can only guarantee finding stationary points or assume the neural network is overparameterized so as to guarantee convergence to the global minimum through neural tangent kernel analysis. In this work, we provide the first global convergence analysis of the vanilla local SGD for two-layer neural networks \\emph{without overparameterization} and \\textit{without injecting noise}, when the input data is Gaussian. The main technical ingredients of our proof are \\textit{a self-correction mechanism} and \\textit{a new exact recursive characterization of the direction of global model parameters}. The self-correction mechanism guarantees the algorithm reaches a good region even if the initialization is in a bad region. A good (bad) region means updating the model by gradient descent will move closer to (away from) the optimal solution. The main difficulty in establishing a self-correction mechanism is to cope with the gradient dependency between two layers. To address this challenge, we divide the landscape of the objective into several regions to carefully control the interference of two layers during the correction process. As a result, we show that local SGD can correct the two layers and enter the good region in polynomial time. After that, we establish a new exact recursive characterization of the direction of global parameters, which is the key to showing convergence to the global minimum with linear speedup in the number of machines and reduced communication rounds. Experiments on synthetic data confirm theoretical results",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yajie Bao",
      "Amarda Shehu",
      "Mingrui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4db8a681ae1e58376dc6227978829063-Abstract-Conference.html": {
    "title": "MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data",
    "volume": "main",
    "abstract": "Discovering genes with similar functions across diverse biomedical contexts poses a significant challenge in gene representation learning due to data heterogeneity. In this study, we resolve this problem by introducing a novel model called Multimodal Similarity Learning Graph Neural Network, which combines Multimodal Machine Learning and Deep Graph Neural Networks to learn gene representations from single-cell sequencing and spatial transcriptomic data. Leveraging 82 training datasets from 10 tissues, three sequencing techniques, and three species, we create informative graph structures for model training and gene representations generation, while incorporating regularization with weighted similarity learning and contrastive learning to learn cross-data gene-gene relationships. This novel design ensures that we can offer gene representations containing functional similarity across different contexts in a joint space. Comprehensive benchmarking analysis shows our model's capacity to effectively capture gene function similarity across multiple modalities, outperforming state-of-the-art methods in gene representation learning by up to $\\textbf{100.4}$%. Moreover, we employ bioinformatics tools in conjunction with gene representations to uncover pathway enrichment, regulation causal networks, and functions of disease-associated genes. Therefore, our model efficiently produces unified gene representations for the analysis of gene functions, tissue functions, diseases, and species evolution",
    "keywords": [],
    "checked": true,
    "id": "eeaef40830f99c9716ae55b843f9f5276edfbf52",
    "semantic_title": "muse-gnn: learning unified gene representation from multimodal biological graph data",
    "citation_count": 0,
    "authors": [
      "Tianyu Liu",
      "Yuge Wang",
      "Rex Ying",
      "Hongyu Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4ddab70bf41ffe5d423840644d3357f4-Abstract-Conference.html": {
    "title": "Reconstructing the Mind's Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors",
    "volume": "main",
    "abstract": "We present MindEye, a novel fMRI-to-image approach to retrieve and reconstruct viewed images from brain activity. Our model comprises two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior). MindEye can map fMRI brain activity to any high dimensional multimodal latent space, like CLIP image space, enabling image reconstruction using generative models that accept embeddings from this latent space. We comprehensively compare our approach with other existing methods, using both qualitative side-by-side comparisons and quantitative evaluations, and show that MindEye achieves state-of-the-art performance in both reconstruction and retrieval tasks. In particular, MindEye can retrieve the exact original image even among highly similar candidates indicating that its brain embeddings retain fine-grained image-specific information. This allows us to accurately retrieve images even from large-scale databases like LAION-5B. We demonstrate through ablations that MindEye's performance improvements over previous methods result from specialized submodules for retrieval and reconstruction, improved training techniques, and training models with orders of magnitude more parameters. Furthermore, we show that MindEye can better preserve low-level image features in the reconstructions by using img2img, with outputs from a separate autoencoder. All code is available on GitHub",
    "keywords": [],
    "checked": true,
    "id": "5efcfc6476c5ddbd69fa1ec3ca598604ae376c1c",
    "semantic_title": "reconstructing the mind's eye: fmri-to-image with contrastive learning and diffusion priors",
    "citation_count": 12,
    "authors": [
      "Paul Scotti",
      "Atmadeep Banerjee",
      "Jimmie Goode",
      "Stepan Shabalin",
      "Alex Nguyen",
      "ethan cohen",
      "Aidan Dempster",
      "Nathalie Verlinde",
      "Elad Yundler",
      "David Weisberg",
      "Kenneth Norman",
      "Tanishq Abraham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e3c5399729e06d2f0c22d04416904ab-Abstract-Conference.html": {
    "title": "Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data",
    "volume": "main",
    "abstract": "Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging.In this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization.Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality.In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods.We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the combination of exploration and exploitation is crucial.Through extensive experimentation we find that our methods outperform all pre-existing FLAD methods by 4% and lead to the first 3 billion parameter language models that outperform the 175 billion parameter GPT-3.Overall, our work suggests that the discovery of better, more efficient mixing strategies for FLAD may provide a viable path towards substantially improving generalization in few-shot learning",
    "keywords": [],
    "checked": true,
    "id": "6e0e04c29b8f151ae8aeca853710db9ee6401ffa",
    "semantic_title": "improving few-shot generalization by exploring and exploiting auxiliary data",
    "citation_count": 4,
    "authors": [
      "Alon Albalak",
      "Colin A. Raffel",
      "William Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e429936318af03ae99c01c90e2604ec-Abstract-Conference.html": {
    "title": "Outlier-Robust Gromov-Wasserstein for Graph Data",
    "volume": "main",
    "abstract": "Gromov-Wasserstein (GW) distance is a powerful tool for comparing and aligning probability distributions supported on different metric spaces. Recently, GW has become the main modeling technique for aligning heterogeneous data for a wide range of graph learning tasks. However, the GW distance is known to be highly sensitive to outliers, which can result in large inaccuracies if the outliers are given the same weight as other samples in the objective function. To mitigate this issue, we introduce a new and robust version of the GW distance called RGW. RGW features optimistically perturbed marginal constraints within a Kullback-Leibler divergence-based ambiguity set. To make the benefits of RGW more accessible in practice, we develop a computationally efficient and theoretically provable procedure using Bregman proximal alternating linearized minimization algorithm. Through extensive experimentation, we validate our theoretical results and demonstrate the effectiveness of RGW on real-world graph learning tasks, such as subgraph matching and partial shape correspondence",
    "keywords": [],
    "checked": false,
    "id": "85eb1ba3f4b557bdf8785fd70ec2b4f2f10b3689",
    "semantic_title": "outlier-robust gromov wasserstein for graph data",
    "citation_count": 0,
    "authors": [
      "Lemin Kong",
      "Jiajin Li",
      "Jianheng Tang",
      "Anthony Man-Cho So"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e52bbb99690d1e05c7ef7b4c8b3569a-Abstract-Conference.html": {
    "title": "Labeling Neural Representations with Inverse Recognition",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) demonstrated remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. Existing global explainability methods, such as Network Dissection, face limitations such as reliance on segmentation masks, lack of statistical significance testing, and high computational demands. We propose Inverse Recognition (INVERT), a scalable approach for linking the learned representations to human-interpretable concepts based on the ability to differentiate between concepts. In contrast to prior work, INVERT is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. Moreover, INVERT provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical significance, emphasizing its utility and credibility.We demonstrate the applicability of INVERT in various scenarios, including the identification of representations affected by spurious correlations, and the interpretation of the hierarchical structure of decision-making within the models",
    "keywords": [],
    "checked": true,
    "id": "037613dd9c9cb48cdfd9deb6023795a5aa1fb837",
    "semantic_title": "labeling neural representations with inverse recognition",
    "citation_count": 2,
    "authors": [
      "Kirill Bykov",
      "Laura Kopf",
      "Shinichi Nakajima",
      "Marius Kloft",
      "Marina Höhne"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e580cdd54fe38ca9a5b8ea6fe99bb44-Abstract-Conference.html": {
    "title": "Cross-modal Active Complementary Learning with Self-refining Correspondence",
    "volume": "main",
    "abstract": "Recently, image-text matching has attracted more and more attention from academia and industry, which is fundamental to understanding the latent correspondence across visual and textual modalities. However, most existing methods implicitly assume the training pairs are well-aligned while ignoring the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby inevitably leading to a performance drop. Although some methods attempt to address such noise, they still face two challenging problems: excessive memorizing/overfitting and unreliable correction for NC, especially under high noise. To address the two problems, we propose a generalized Cross-modal Robust Complementary Learning framework (CRCL), which benefits from a novel Active Complementary Loss (ACL) and an efficient Self-refining Correspondence Correction (SCC) to improve the robustness of existing methods. Specifically, ACL exploits active and complementary learning losses to reduce the risk of providing erroneous supervision, leading to theoretically and experimentally demonstrated robustness against NC. SCC utilizes multiple self-refining processes with momentum correction to enlarge the receptive field for correcting correspondences, thereby alleviating error accumulation and achieving accurate and stable corrections. We carry out extensive experiments on three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify the superior robustness of our CRCL against synthetic and real-world noisy correspondences",
    "keywords": [],
    "checked": true,
    "id": "85b4a40642184b060e2116eec78add8dd27727ec",
    "semantic_title": "cross-modal active complementary learning with self-refining correspondence",
    "citation_count": 1,
    "authors": [
      "Yang Qin",
      "Yuan Sun",
      "Dezhong Peng",
      "Joey Tianyi Zhou",
      "Xi Peng",
      "Peng Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e5e0daf4b05d8bfc6377f33fd53a8f4-Abstract-Conference.html": {
    "title": "Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity",
    "volume": "main",
    "abstract": "Reconstructing human vision from brain activities has been an appealing task that helps to understand our cognitive process. Even though recent research has seen great success in reconstructing static images from non-invasive brain recordings, work on recovering continuous visual experiences in the form of videos is limited. In this work, we propose Mind-Video that learns spatiotemporal information from continuous fMRI data of the cerebral cortex progressively through masked brain modeling, multimodal contrastive learning with spatiotemporal attention, and co-training with an augmented Stable Diffusion model that incorporates network temporal inflation. We show that high-quality videos of arbitrary frame rates can be reconstructed with Mind-Video using adversarial guidance. The recovered videos were evaluated with various semantic and pixel-level metrics. We achieved an average accuracy of 85% in semantic classification tasks and 0.19 in structural similarity index (SSIM), outperforming the previous state-of-the-art by 45%. We also show that our model is biologically plausible and interpretable, reflecting established physiological processes",
    "keywords": [],
    "checked": true,
    "id": "12cbf907d40a5406ca855f51af54cc16d0b28cd6",
    "semantic_title": "cinematic mindscapes: high-quality video reconstruction from brain activity",
    "citation_count": 13,
    "authors": [
      "Zijiao Chen",
      "Jiaxin Qing",
      "Juan Helen Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e5f5e4504759e3957e3eef2a44a535e-Abstract-Conference.html": {
    "title": "Retrieval-Augmented Multiple Instance Learning",
    "volume": "main",
    "abstract": "Multiple Instance Learning (MIL) is a crucial weakly supervised learning method applied across various domains, e.g., medical diagnosis based on whole slide images (WSIs). Recent advancements in MIL algorithms have yielded exceptional performance when the training and test data originate from the same domain, such as WSIs obtained from the same hospital. However, this paper reveals a performance deterioration of MIL models when tested on an out-of-domain test set, exemplified by WSIs sourced from a novel hospital. To address this challenge, this paper introduces the Retrieval-AugMented MIL (RAM-MIL) framework, which integrates Optimal Transport (OT) as the distance metric for nearest neighbor retrieval. The development of RAM-MIL is driven by two key insights. First, a theoretical discovery indicates that reducing the input's intrinsic dimension can minimize the approximation error in attention-based MIL. Second, previous studies highlight a link between input intrinsic dimension and the feature merging process with the retrieved data. Empirical evaluations conducted on WSI classification demonstrate that the proposed RAM-MIL framework achieves state-of-the-art performance in both in-domain scenarios, where the training and retrieval data are in the same domain, and more crucially, in out-of-domain scenarios, where the (unlabeled) retrieval data originates from a different domain. Furthermore, the use of the transportation matrix derived from OT renders the retrieval results interpretable at the instance level, in contrast to the vanilla $l_2$ distance, and allows for visualization for human experts. *Code can be found at \\url{https://github.com/ralphc1212/ram-mil*",
    "keywords": [],
    "checked": true,
    "id": "64eb5e1e8e8ae9a4fd9495d380f89f80eccf60c6",
    "semantic_title": "retrieval-augmented multiple instance learning",
    "citation_count": 0,
    "authors": [
      "Yufei CUI",
      "Ziquan Liu",
      "Yixin Chen",
      "Yuchen Lu",
      "Xinyue Yu",
      "Xue (Steve) Liu",
      "Tei-Wei Kuo",
      "Miguel Rodrigues",
      "Chun Jason XUE",
      "Antoni Chan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e839c9c398c58c878a394633b806ccd-Abstract-Conference.html": {
    "title": "Multi-task Graph Neural Architecture Search with Task-aware Collaboration and Curriculum",
    "volume": "main",
    "abstract": "Graph neural architecture search (GraphNAS) has shown great potential for automatically designing graph neural architectures for graph related tasks. However, multi-task GraphNAS capable of handling multiple tasks simultaneously has been largely unexplored in literature, posing great challenges to capture the complex relations and influences among different tasks. To tackle this problem, we propose a novel multi-task graph neural architecture search with task-aware collaboration and curriculum (MTGC3), which is able to simultaneously discover optimal architectures for different tasks and learn the collaborative relationships among different tasks in a joint manner. Specifically, we design the layer-wise disentangled supernet capable of managing multiple architectures in a unified framework, which combines with our proposed soft task-collaborative module to learn the transferability relationships between tasks. We further develop the task-wise curriculum training strategy to improve the architecture search procedure via reweighing the influence of different tasks based on task difficulties. Extensive experiments show that our proposed MTGC3 model achieves state-of-the-art performance against several baselines in multi-task scenarios, demonstrating its ability to discover effective architectures and capture the collaborative relationships for multiple tasks",
    "keywords": [],
    "checked": true,
    "id": "ebab20312d9b42cf45841ad2f38d687d752e5702",
    "semantic_title": "multi-task graph neural architecture search with task-aware collaboration and curriculum",
    "citation_count": 1,
    "authors": [
      "Yijian Qin",
      "Xin Wang",
      "Ziwei Zhang",
      "Hong Chen",
      "Wenwu Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e85362c02172c0c6567ce593122d31c-Abstract-Conference.html": {
    "title": "The Impact of Positional Encoding on Length Generalization in Transformers",
    "volume": "main",
    "abstract": "Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model's performance. Overall, our work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences",
    "keywords": [],
    "checked": true,
    "id": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
    "semantic_title": "the impact of positional encoding on length generalization in transformers",
    "citation_count": 38,
    "authors": [
      "Amirhossein Kazemnejad",
      "Inkit Padhi",
      "Karthikeyan Natesan Ramamurthy",
      "Payel Das",
      "Siva Reddy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e8a74988bc611495c2d3a5edac8493f-Abstract-Conference.html": {
    "title": "Attention as Implicit Structural Inference",
    "volume": "main",
    "abstract": "Attention mechanisms play a crucial role in cognitive systems by allowing them to flexibly allocate cognitive resources. Transformers, in particular, have become a dominant architecture in machine learning, with attention as their central innovation. However, the underlying intuition and formalism of attention in Transformers is based on ideas of keys and queries in database management systems. In this work, we pursue a structural inference perspective, building upon, and bringing together, previous theoretical descriptions of attention such as; Gaussian Mixture Models, alignment mechanisms and Hopfield Networks. Specifically, we demonstrate that attention can be viewed as inference over an implicitly defined set of possible adjacency structures in a graphical model, revealing the generality of such a mechanism. This perspective unifies different attentional architectures in machine learning and suggests potential modifications and generalizations of attention. Here we investigate two and demonstrate their behaviour on explanatory toy problems: (a) extending the value function to incorporate more nodes of a graphical model yielding a mechanism with a bias toward attending multiple tokens; (b) introducing a geometric prior (with conjugate hyper-prior) over the adjacency structures producing a mechanism which dynamically scales the context window depending on input. Moreover, by describing a link between structural inference and precision-regulation in Predictive Coding Networks, we discuss how this framework can bridge the gap between attentional mechanisms in machine learning and Bayesian conceptions of attention in Neuroscience. We hope by providing a new lens on attention architectures our work can guide the development of new and improved attentional mechanisms",
    "keywords": [],
    "checked": false,
    "id": "fed7e166c596f60129834aaaa9508fe3963a2978",
    "semantic_title": "o bject r epresentations as e quilibria : t raining i terative i nference a lgorithms with i mplicit d ifferentiation",
    "citation_count": 0,
    "authors": [
      "Ryan Singh",
      "Christopher L Buckley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e8f257e054abd24c550d55e57cec274-Abstract-Conference.html": {
    "title": "Nearly Tight Bounds For Differentially Private Multiway Cut",
    "volume": "main",
    "abstract": "Finding min $s$-$t$ cuts in graphs is a basic algorithmic tool, with applications in image segmentation, community detection, reinforcement learning, and data clustering. In this problem, we are given two nodes as terminals and the goal is to remove the smallest number of edges from the graph so that these two terminals are disconnected. We study the complexity of differential privacy for the min $s$-$t$ cut problem and show nearly tight lower and upper bounds where we achieve privacy at no cost for running time efficiency. We also develop a differentially private algorithm for the multiway $k$-cut problem, in which we are given $k$ nodes as terminals that we would like to disconnect. As a function of $k$, we obtain privacy guarantees that are exponentially more efficient than applying the advanced composition theorem to known algorithms for multiway $k$-cut. Finally, we empirically evaluate the approximation of our differentially private min $s$-$t$ cut algorithm and show that it almost matches the quality of the output of non-private ones",
    "keywords": [],
    "checked": false,
    "id": "ac48d78beb60bf3bf033fa05a780c309a30a10d7",
    "semantic_title": "nearly tight bounds for differentially private min s-t and multiway cut",
    "citation_count": 0,
    "authors": [
      "Mina Dalirrooyfard",
      "Slobodan Mitrovic",
      "Yuriy Nevmyvaka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e9d8aeeab6120c3c83ccf95d4c211d3-Abstract-Conference.html": {
    "title": "Permutation Equivariant Neural Functionals",
    "volume": "main",
    "abstract": "This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate parameter sharing scheme. In our experiments, we find that permutation equivariant neural functionals are effective on a diverse set of tasks that require processing the weights of MLPs and CNNs, such as predicting classifier generalization, producing \"winning ticket\" sparsity masks for initializations, and classifying or editing implicit neural representations (INRs). In addition, we provide code for our models and experiments at https://github.com/AllanYangZhou/nfn",
    "keywords": [],
    "checked": true,
    "id": "59854c05cb5c5ed2f2a1633dd08269aa843d3314",
    "semantic_title": "permutation equivariant neural functionals",
    "citation_count": 12,
    "authors": [
      "Allan Zhou",
      "Kaien Yang",
      "Kaylee Burns",
      "Adriano Cardace",
      "Yiding Jiang",
      "Samuel Sokota",
      "J. Zico Kolter",
      "Chelsea Finn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e9fa6e716940a7cfc60c46e6f702f52-Abstract-Conference.html": {
    "title": "Fine-Grained Visual Prompting",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs), such as CLIP, have demonstrated impressive zero-shot transfer capabilities in image-level visual perception. However, these models have shown limited performance in instance-level tasks that demand precise localization and recognition. Previous works have suggested that incorporating visual prompts, such as colorful boxes or circles, can improve the ability of models to recognize objects of interest. Nonetheless, compared to language prompting, visual prompting designs are rarely explored. Existing approaches, which employ coarse visual cues such as colorful boxes or circles, often result in sub-optimal performance due to the inclusion of irrelevant and noisy pixels. In this paper, we carefully study the visual prompting designs by exploring more fine-grained markings, such as segmentation masks and their variations. In addition, we introduce a new zero-shot framework that leverages pixel-level annotations acquired from a generalist segmentation model for fine-grained visual prompting. Consequently, our investigation reveals that a straightforward application of blur outside the target mask, referred to as the Blur Reverse Mask, exhibits exceptional effectiveness. This proposed prompting strategy leverages the precise mask annotations to reduce focus on weakly related regions while retaining spatial coherence between the target and the surrounding background. Our Fine-Grained Visual Prompting (FGVP) demonstrates superior performance in zero-shot comprehension of referring expressions on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks. It outperforms prior methods by an average margin of 3.0\\% to 4.6\\%, with a maximum improvement of 12.5\\% on the RefCOCO+ testA subset. The part detection experiments conducted on the PACO dataset further validate the preponderance of FGVP over existing visual prompting techniques. Code is available at https://github.com/ylingfeng/FGVP",
    "keywords": [],
    "checked": true,
    "id": "a01a9c4a114fbf201540268f928ccf77bc3f9357",
    "semantic_title": "fine-grained visual prompting",
    "citation_count": 8,
    "authors": [
      "Lingfeng Yang",
      "Yueze Wang",
      "Xiang Li",
      "Xinlong Wang",
      "Jian Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4eb2c0adafbe71269f3a772c130f9e53-Abstract-Conference.html": {
    "title": "Integration-free Training for Spatio-temporal Multimodal Covariate Deep Kernel Point Processes",
    "volume": "main",
    "abstract": "In this study, we propose a novel deep spatio-temporal point process model, Deep Kernel Mixture Point Processes (DKMPP), that incorporates multimodal covariate information. DKMPP is an enhanced version of Deep Mixture Point Processes (DMPP), which uses a more flexible deep kernel to model complex relationships between events and covariate data, improving the model's expressiveness. To address the intractable training procedure of DKMPP due to the non-integrable deep kernel, we utilize an integration-free method based on score matching, and further improve efficiency by adopting a scalable denoising score matching method. Our experiments demonstrate that DKMPP and its corresponding score-based estimators outperform baseline models, showcasing the advantages of incorporating covariate information, utilizing a deep kernel, and employing score-based estimators",
    "keywords": [],
    "checked": true,
    "id": "475af20b4207156cb822df420545a24ae33138a2",
    "semantic_title": "integration-free training for spatio-temporal multimodal covariate deep kernel point processes",
    "citation_count": 0,
    "authors": [
      "YIXUAN ZHANG",
      "Quyu Kong",
      "Feng Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4ec43957eda1126ad4887995d05fae3b-Abstract-Conference.html": {
    "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought",
    "volume": "main",
    "abstract": "Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments.In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the \"Chain of Thoughts\" mode for effective embodied planning.(ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control.Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering.Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset",
    "keywords": [],
    "checked": true,
    "id": "00cb69a9f280317d1c59ac5827551ee9b10642b8",
    "semantic_title": "embodiedgpt: vision-language pre-training via embodied chain of thought",
    "citation_count": 59,
    "authors": [
      "Yao Mu",
      "Qinglong Zhang",
      "Mengkang Hu",
      "Wenhai Wang",
      "Mingyu Ding",
      "Jun Jin",
      "Bin Wang",
      "Jifeng Dai",
      "Yu Qiao",
      "Ping Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4eef8829319316d0b552328715c836c3-Abstract-Conference.html": {
    "title": "Conditional Matrix Flows for Gaussian Graphical Models",
    "volume": "main",
    "abstract": "Studying conditional independence among many variables with few observations is a challenging task.Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\\leq1$.However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\\lambda$.In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\\lambda$ requires repeated runs of expensive Gibbs samplers.Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.Within one model we thus have access to (i) the evolution of the posterior for any $\\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit",
    "keywords": [],
    "checked": true,
    "id": "48502eca6eaecd7d3e100990c9eefba36338cd33",
    "semantic_title": "conditional matrix flows for gaussian graphical models",
    "citation_count": 0,
    "authors": [
      "Marcello Massimo Negri",
      "Fabricio Arend Torres",
      "Volker Roth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4ef594af0d9a519db8fb292452c461fa-Abstract-Conference.html": {
    "title": "Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation",
    "volume": "main",
    "abstract": "We tackle the problems of latent variables identification and \"out-of-support'' image generation in representation learning. We show that both are possible for a class of decoders that we call additive, which are reminiscent of decoders used for object-centric representation learning (OCRL) and well suited for images that can be decomposed as a sum of object-specific images. We provide conditions under which exactly solving the reconstruction problem using an additive decoder is guaranteed to identify the blocks of latent variables up to permutation and block-wise invertible transformations. This guarantee relies only on very weak assumptions about the distribution of the latent factors, which might present statistical dependencies and have an almost arbitrarily shaped support. Our result provides a new setting where nonlinear independent component analysis (ICA) is possible and adds to our theoretical understanding of OCRL methods. We also show theoretically that additive decoders can generate novel images by recombining observed factors of variations in novel ways, an ability we refer to as Cartesian-product extrapolation. We show empirically that additivity is crucial for both identifiability and extrapolation on simulated data",
    "keywords": [],
    "checked": true,
    "id": "dfe6e29e0b441ab417e847f6a09a7db5b559f9a7",
    "semantic_title": "additive decoders for latent variables identification and cartesian-product extrapolation",
    "citation_count": 7,
    "authors": [
      "Sébastien Lachapelle",
      "Divyat Mahajan",
      "Ioannis Mitliagkas",
      "Simon Lacoste-Julien"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4f31327e046913c7238d5b671f5d820e-Abstract-Conference.html": {
    "title": "How2comm: Communication-Efficient and Collaboration-Pragmatic Multi-Agent Perception",
    "volume": "main",
    "abstract": "Multi-agent collaborative perception has recently received widespread attention as an emerging application in driving scenarios. Despite the advancements in previous efforts, challenges remain due to various noises in the perception procedure, including communication redundancy, transmission delay, and collaboration heterogeneity. To tackle these issues, we propose \\textit{How2comm}, a collaborative perception framework that seeks a trade-off between perception performance and communication bandwidth. Our novelties lie in three aspects. First, we devise a mutual information-aware communication mechanism to maximally sustain the informative features shared by collaborators. The spatial-channel filtering is adopted to perform effective feature sparsification for efficient communication. Second, we present a flow-guided delay compensation strategy to predict future characteristics from collaborators and eliminate feature misalignment due to temporal asynchrony. Ultimately, a pragmatic collaboration transformer is introduced to integrate holistic spatial semantics and temporal context clues among agents. Our framework is thoroughly evaluated on several LiDAR-based collaborative detection datasets in real-world and simulated scenarios. Comprehensive experiments demonstrate the superiority of How2comm and the effectiveness of all its vital components. The code will be released at https://github.com/ydk122024/How2comm",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingkang Yang",
      "Kun Yang",
      "Yuzheng Wang",
      "Jing Liu",
      "Zhi Xu",
      "Rongbin Yin",
      "Peng Zhai",
      "Lihua Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4f3820576130a8f796ddbf204c841487-Abstract-Conference.html": {
    "title": "LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images",
    "volume": "main",
    "abstract": "We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pre-trained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet. Code is available at https://github.com/virajprabhu/lance",
    "keywords": [],
    "checked": true,
    "id": "5d36d8817fb0ab69ee396250aed204dea70da9f8",
    "semantic_title": "lance: stress-testing visual models by generating language-guided counterfactual images",
    "citation_count": 5,
    "authors": [
      "Viraj Prabhu",
      "Sriram Yenamandra",
      "Prithvijit Chattopadhyay",
      "Judy Hoffman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4f693c15f189efd888b6782a5f4eccb1-Abstract-Conference.html": {
    "title": "Refined Mechanism Design for Approximately Structured Priors via Active Regression",
    "volume": "main",
    "abstract": "We consider the problem of a revenue-maximizing seller with a large number of items $m$ for sale to $n$ strategic bidders, whose valuations are drawn independently from high-dimensional, unknown prior distributions. It is well-known that optimal and even approximately-optimal mechanisms for this setting are notoriously difficult to characterize or compute, and, even when they can be found, are often rife with various counter-intuitive properties. In this paper, following a model introduced recently by Cai and Daskalakis [CD22], we consider the case that bidders' prior distributions can be well-approximated by a topic model. We design an active learning component, responsible for interacting with the bidders and outputting low-dimensional approximations of their types, and a mechanism design component, responsible for robustifying mechanisms for the low-dimensional model to work for the approximate types of the former component. On the active learning front, we cast our problem in the framework of Randomized Linear Algebra (RLA) for regression problems, allowing us to import several breakthrough results from that line of research, and adapt them to our setting. On the mechanism design front, we remove many restrictive assumptions of prior work on the type of access needed to the underlying distributions and the associated mechanisms. To the best of our knowledge, our work is the first to formulate connections between mechanism design, and RLA for active learning of regression problems, opening the door for further applications of randomized linear algebra primitives to mechanism design",
    "keywords": [],
    "checked": true,
    "id": "540595e6c7a476602f7a68431cebb899a09450e2",
    "semantic_title": "refined mechanism design for approximately structured priors via active regression",
    "citation_count": 0,
    "authors": [
      "Christos Boutsikas",
      "Petros Drineas",
      "Marios Mertzanidis",
      "Alexandros Psomas",
      "Paritosh Verma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4f7b1f51ef415f09e171f632172284ff-Abstract-Conference.html": {
    "title": "Most Neural Networks Are Almost Learnable",
    "volume": "main",
    "abstract": "We present a PTAS for learning random constant-depth networks. We show that for any fixed $\\epsilon>0$ and depth $i$, there is a poly-time algorithm that for any distribution on $\\sqrt{d} \\cdot \\mathbb{S}^{d-1}$ learns random Xavier networks of depth $i$, up to an additive error of $\\epsilon$. The algorithm runs in time and sample complexity of $(\\bar{d})^{\\mathrm{poly}(\\epsilon^{-1})}$, where $\\bar d$ is the size of the network. For some cases of sigmoid and ReLU-like activations the bound can be improved to $(\\bar{d})^{\\mathrm{polylog}(\\epsilon^{-1})}$, resulting in a quasi-poly-time algorithm for learning constant depth random networks",
    "keywords": [],
    "checked": true,
    "id": "6852b8c38f955366b96ccdd06a9ae64d72d7c500",
    "semantic_title": "most neural networks are almost learnable",
    "citation_count": 0,
    "authors": [
      "Amit Daniely",
      "Nati Srebro",
      "Gal Vardi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4f82088872dc8a91085f426f90bdd7dc-Abstract-Conference.html": {
    "title": "Bounded rationality in structured density estimation",
    "volume": "main",
    "abstract": "Learning to accurately represent environmental uncertainty is crucial for adaptive and optimal behaviors in various cognitive tasks. However, it remains unclear how the human brain, constrained by finite cognitive resources, constructs an internal model from an infinite space of probability distributions. In this study, we explore how these learned distributions deviate from the ground truth, resulting in observable inconsistency in a novel structured density estimation task. During each trial, human participants were asked to form and report the latent probability distribution functions underlying sequentially presented independent observations. As the number of observations increased, the reported predictive density became closer to the ground truth. Nevertheless, we observed an intriguing inconsistency in human structure estimation, specifically a large error in the number of reported clusters. Such inconsistency is invariant to the scale of the distribution and persists across stimulus modalities. We modeled uncertainty learning as approximate Bayesian inference in a nonparametric mixture prior of distributions. Human reports were best explained under resource rationality embodied in a decaying tendency towards model expansion. Our study offers insights into human cognitive processes under uncertainty and lays the groundwork for further exploration of resource-rational representations in the brain under more complex tasks",
    "keywords": [],
    "checked": true,
    "id": "69f9bc469cd4342eeff9dcb1359e89562a71eac3",
    "semantic_title": "bounded rationality in structured density estimation",
    "citation_count": 0,
    "authors": [
      "Tianyuan Teng",
      "Kevin Li",
      "Hang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4f86833d5cc98ec32e470ef1c8cb82e3-Abstract-Conference.html": {
    "title": "RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency",
    "volume": "main",
    "abstract": "In this paper, we study the problem of continuous 3D shape representations. The majority of existing successful methods are coordinate-based implicit neural representations. However, they are inefficient to render novel views or recover explicit surface points. A few works start to formulate 3D shapes as ray-based neural functions, but the learned structures are inferior due to the lack of multi-view geometry consistency. To tackle these challenges, we propose a new framework called RayDF. It consists of three major components: 1) the simple ray-surface distance field, 2) the novel dual-ray visibility classifier, and 3) a multi-view consistency optimization module to drive the learned ray-surface distances to be multi-view geometry consistent. We extensively evaluate our method on three public datasets, demonstrating remarkable performance in 3D surface point reconstruction on both synthetic and challenging real-world 3D scenes, clearly surpassing existing coordinate-based and ray-based baselines. Most notably, our method achieves a 1000x faster speed than coordinate-based methods to render an 800x800 depth image, showing the superiority of our method for 3D shape representation. Our code and data are available at https://github.com/vLAR-group/RayDF",
    "keywords": [],
    "checked": true,
    "id": "b17f5381d09444f15a9740cd4ed28dc9c329cfec",
    "semantic_title": "raydf: neural ray-surface distance fields with multi-view consistency",
    "citation_count": 2,
    "authors": [
      "Zhuoman Liu",
      "Bo Yang",
      "Yan Luximon",
      "Ajay Kumar",
      "Jinxi Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4feccf7f781e1844f3a5d70eb779147a-Abstract-Conference.html": {
    "title": "Blocked Collaborative Bandits: Online Collaborative Filtering with Per-Item Budget Constraints",
    "volume": "main",
    "abstract": "We consider the problem of \\emph{blocked} collaborative bandits where there are multiple users, each with an associated multi-armed bandit problem. These users are grouped into \\emph{latent} clusters such that the mean reward vectors of users within the same cluster are identical. Our goal is to design algorithms that maximize the cumulative reward accrued by all the users over time, under the \\emph{constraint} that no arm of a user is pulled more than $\\mathsf{B}$ times. This problem has been originally considered by \\cite{Bresler:2014}, and designing regret-optimal algorithms for it has since remained an open problem.In this work, we propose an algorithm called B-LATTICE (Blocked Latent bAndiTs via maTrIx ComplEtion) that collaborates across users, while simultaneously satisfying the budget constraints, to maximize their cumulative rewards. Theoretically, under certain reasonable assumptions on the latent structure, with $\\mathsf{M}$ users, $\\mathsf{N}$ arms, $\\mathsf{T}$ rounds per user, and $\\mathsf{C}=O(1)$ latent clusters, B-LATTICE achieves a per-user regret of $\\widetilde{O}(\\sqrt{\\mathsf{T}(1 + \\mathsf{N}\\mathsf{M}^{-1})})$ under a budget constraint of $\\mathsf{B}=\\Theta(\\log \\mathsf{T})$. These are the first sub-linear regret bounds for this problem, and match the minimax regret bounds when $\\mathsf{B}=\\mathsf{T}$. Empirically, we demonstrate that our algorithm has superior performance over baselines even when $\\mathsf{B}=1$. B-LATTICE is a phased algorithm where in each phase it clusters users into groups and collaborates across users within a group to quickly learn their reward models",
    "keywords": [],
    "checked": true,
    "id": "b468ee222ecc0e7d2101df65312d24346ad0f0ba",
    "semantic_title": "blocked collaborative bandits: online collaborative filtering with per-item budget constraints",
    "citation_count": 0,
    "authors": [
      "Soumyabrata Pal",
      "Arun Suggala",
      "Karthikeyan Shanmugam",
      "Prateek Jain"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4ff08be7b0105049ff3e0ce3d70658c5-Abstract-Conference.html": {
    "title": "Efficient Online Clustering with Moving Costs",
    "volume": "main",
    "abstract": "In this work we consider an online learning problem, called Online $k$-Clustering with Moving Costs, at which a learner maintains a set of $k$ facilities over $T$ rounds so as to minimize the connection cost of an adversarially selected sequence of clients. The learner is informed on the positions of the clients at each round $t$ only after its facility-selection and can use this information to update its decision in the next round. However, updating the facility positions comes with an additional moving cost based on the moving distance of the facilities. We present the first $\\mathcal{O}(\\log n)$-regret polynomial-time online learning algorithm guaranteeing that the overall cost (connection $+$ moving) is at most $\\mathcal{O}(\\log n)$ times the time-averaged connection cost of the best fixed solution. Our work improves on the recent result of (Fotakis et al., 2021) establishing $\\mathcal{O}(k)$-regret guarantees only on the connection cost",
    "keywords": [],
    "checked": false,
    "id": "2fa36c01b9a494ce9abbd905268a3161f736141d",
    "semantic_title": "a fast incremental spectral clustering algorithm with cosine similarity",
    "citation_count": 0,
    "authors": [
      "Dimitrios Christou",
      "Stratis Skoulakis",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4ff83037e8d97b2171b2d3e96cb8e677-Abstract-Conference.html": {
    "title": "SEGA: Instructing Text-to-Image Models using Semantic Guidance",
    "volume": "main",
    "abstract": "Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) generalizes to any generative architecture using classifier-free guidance. More importantly, it allows for subtle and extensive edits, composition and style changes, and optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of tasks, thus providing strong evidence for its versatility and flexibility",
    "keywords": [],
    "checked": true,
    "id": "ac4e9536df4a6ec1cb5840766d058a5b893bf60e",
    "semantic_title": "sega: instructing text-to-image models using semantic guidance",
    "citation_count": 0,
    "authors": [
      "Manuel Brack",
      "Felix Friedrich",
      "Dominik Hintersdorf",
      "Lukas Struppek",
      "Patrick Schramowski",
      "Kristian Kersting"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/50251f54848a433f3e47ae3b7cbded53-Abstract-Conference.html": {
    "title": "Learning Sample Difficulty from Pre-trained Models for Reliable Prediction",
    "volume": "main",
    "abstract": "Large-scale pre-trained models have achieved remarkable success in many applications, but how to leverage them to improve the prediction reliability of downstream models is undesirably under-explored. Moreover, modern neural networks have been found to be poorly calibrated and make overconfident predictions regardless of inherent sample difficulty and data uncertainty. To address this issue, we propose to utilize large-scale pre-trained models to guide downstream model training with sample difficulty-aware entropy regularization. Pre-trained models that have been exposed to large-scale datasets and do not overfit the downstream training classes enable us to measure each training sample's difficulty via feature-space Gaussian modeling and relative Mahalanobis distance computation. Importantly, by adaptively penalizing overconfident prediction based on the sample difficulty, we simultaneously improve accuracy and uncertainty calibration across challenging benchmarks (e.g., +0.55% ACC and −3.7% ECE on ImageNet1k using ResNet34), consistently surpassing competitive baselines for reliable prediction. The improved uncertainty estimate further improves selective classification (abstaining from erroneous predictions) and out-of-distribution detection",
    "keywords": [],
    "checked": true,
    "id": "f8d7ce62ad988915b070136351328ccb2fdc27d3",
    "semantic_title": "learning sample difficulty from pre-trained models for reliable prediction",
    "citation_count": 2,
    "authors": [
      "Peng Cui",
      "Dan Zhang",
      "Zhijie Deng",
      "Yinpeng Dong",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5049acb0d5d976130388f3e8edcae183-Abstract-Conference.html": {
    "title": "Asynchronous Proportional Response Dynamics: Convergence in Markets with Adversarial Scheduling",
    "volume": "main",
    "abstract": "We study Proportional Response Dynamics (PRD) in linear Fisher markets, where participants act asynchronously. We model this scenario as a sequential process in which at each step, an adversary selects a subset of the players to update their bids, subject to liveness constraints. We show that if every bidder individually applies the PRD update rule whenever they are included in the group of bidders selected by the adversary, then, in the generic case, the entire dynamic converges to a competitive equilibrium of the market. Our proof technique reveals additional properties of linear Fisher markets, such as the uniqueness of the market equilibrium for generic parameters and the convergence of associated no swap regret dynamics and best response dynamics under certain conditions",
    "keywords": [],
    "checked": false,
    "id": "a693518831be43a5cc53e79663fa6cb3550281b3",
    "semantic_title": "asynchronous proportional response dynamics in markets with adversarial scheduling",
    "citation_count": 0,
    "authors": [
      "Yoav Kolumbus",
      "Menahem Levy",
      "Noam Nisan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/50a057e9fe79ffa3f4120fb6fb88071a-Abstract-Conference.html": {
    "title": "Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP",
    "volume": "main",
    "abstract": "Vision-language pre-training methods, e.g., CLIP, demonstrate an impressive zero-shot performance on visual categorizations with the class proxy from the text embedding of the class name. However, the modality gap between the text and vision space can result in a sub-optimal performance. We theoretically show that the gap cannot be reduced sufficiently by minimizing the contrastive loss in CLIP and the optimal proxy for vision tasks may reside only in the vision space. Therefore, given unlabeled target vision data, we propose to learn the vision proxy directly with the help from the text proxy for zero-shot transfer. Moreover, according to our theoretical analysis, strategies are developed to further refine the pseudo label obtained by the text proxy to facilitate the intra-modal proxy learning (InMaP) for vision. Experiments on extensive downstream tasks confirm the effectiveness and efficiency of our proposal. Concretely, InMaP can obtain the vision proxy within one minute on a single GPU while improving the zero-shot accuracy from $77.02\\%$ to $80.21\\%$ on ImageNet with ViT-L/14@336 pre-trained by CLIP",
    "keywords": [],
    "checked": true,
    "id": "43abdd72a50706ca76e77a2d613a7435324f4997",
    "semantic_title": "intra-modal proxy learning for zero-shot visual categorization with clip",
    "citation_count": 0,
    "authors": [
      "Qi Qian",
      "Yuanhong Xu",
      "Juhua Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/50ca96a1a9ebe0b5e5688a504feb6107-Abstract-Conference.html": {
    "title": "Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation",
    "volume": "main",
    "abstract": "Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages. By implementing BN in a continual learning fashion, our approach achieves significant performance gains across a wide range of benchmarks, particularly for the challenging yet realistic online scenarios (e.g., up to 7.68\\%, 6.86\\% and 4.26\\% on Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively). Our code is available at https://github.com/lvyilin/AdaB2N",
    "keywords": [],
    "checked": true,
    "id": "1d97512534ef34fbba6e313fdf3070276ae41802",
    "semantic_title": "overcoming recency bias of normalization statistics in continual learning: balance and adaptation",
    "citation_count": 0,
    "authors": [
      "Yilin Lyu",
      "Liyuan Wang",
      "Xingxing Zhang",
      "Zicheng Sun",
      "Hang Su",
      "Jun Zhu",
      "Liping Jing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/50d6dbc809b0dc96f7f1090810537acc-Abstract-Conference.html": {
    "title": "The Simplicity Bias in Multi-Task RNNs: Shared Attractors, Reuse of Dynamics, and Geometric Representation",
    "volume": "main",
    "abstract": "How does a single interconnected neural population perform multiple tasks, each with its own dynamical requirements? The relation between task requirements and neural dynamics in Recurrent Neural Networks (RNNs) has been investigated for single tasks. The forces shaping joint dynamics of multiple tasks, however, are largely unexplored. In this work, we first construct a systematic framework to study multiple tasks in RNNs, minimizing interference from input and output correlations with the hidden representation. This allows us to reveal how RNNs tend to share attractors and reuse dynamics, a tendency we define as the \"simplicity bias\".We find that RNNs develop attractors sequentially during training, preferentially reusing existing dynamics and opting for simple solutions when possible. This sequenced emergence and preferential reuse encapsulate the simplicity bias. Through concrete examples, we demonstrate that new attractors primarily emerge due to task demands or architectural constraints, illustrating a balance between simplicity bias and external factors.We examine the geometry of joint representations within a single attractor, by constructing a family of tasks from a set of functions. We show that the steepness of the associated functions controls their alignment within the attractor. This arrangement again highlights the simplicity bias, as points with similar input spacings undergo comparable transformations to reach the shared attractor.Our findings propose compelling applications. The geometry of shared attractors might allow us to infer the nature of unknown tasks. Furthermore, the simplicity bias implies that without specific incentives, modularity in RNNs may not spontaneously emerge, providing insights into the conditions required for network specialization",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elia Turner",
      "Omri Barak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/50ea4dbd1cff6bd3daef939eff10c092-Abstract-Conference.html": {
    "title": "Optimize Planning Heuristics to Rank, not to Estimate Cost-to-Goal",
    "volume": "main",
    "abstract": "In imitation learning for planning, parameters of heuristic functions are optimized against a set of solved problem instances. This work revisits the necessary and sufficient conditions of strictly optimally efficient heuristics for forward search algorithms, mainly A* and greedy best-first search, which expand only states on the returned optimal path. It then proposes a family of loss functions based on ranking tailored for a given variant of the forward search algorithm. Furthermore, from a learning theory point of view, it discusses why optimizing cost-to-goal h* is unnecessarily difficult. The experimental comparison on a diverse set of problems unequivocally supports the derived theory",
    "keywords": [],
    "checked": true,
    "id": "758305d53c3c956df6671c7b5fc4f8ff3131983e",
    "semantic_title": "optimize planning heuristics to rank, not to estimate cost-to-goal",
    "citation_count": 1,
    "authors": [
      "Leah Chrestien",
      "Stefan Edelkamp",
      "Antonin Komenda",
      "Tomas Pevny"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/51053d7b8473df7d5a2165b2a8ee9629-Abstract-Conference.html": {
    "title": "Goal-Conditioned Predictive Coding for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Recent work has demonstrated the effectiveness of formulating decision making as supervised learning on offline-collected trajectories. Powerful sequence models, such as GPT or BERT, are often employed to encode the trajectories. However, the benefits of performing sequence modeling on trajectory data remain unclear. In this work, we investigate whether sequence modeling has the ability to condense trajectories into useful representations that enhance policy learning. We adopt a two-stage framework that first leverages sequence models to encode trajectory-level representations, and then learns a goal-conditioned policy employing the encoded representations as its input. This formulation allows us to consider many existing supervised offline RL methods as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predictive Coding (GCPC), a sequence modeling objective that yields powerful trajectory representations and leads to performant policies. Through extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, we observe that sequence modeling can have a significant impact on challenging decision making tasks. Furthermore, we demonstrate that GCPC learns a goal-conditioned latent representation encoding the future trajectory, which enables competitive performance on all three benchmarks",
    "keywords": [],
    "checked": true,
    "id": "dea2e300e007902c43c75897c2fad14a730711d9",
    "semantic_title": "goal-conditioned predictive coding for offline reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Zilai Zeng",
      "Ce Zhang",
      "Shijie Wang",
      "Chen Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/510ad3018bbdc5b6e3b10646e2e35771-Abstract-Conference.html": {
    "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
    "volume": "main",
    "abstract": "Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of attention glitches, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce flip-flop language modeling (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs",
    "keywords": [],
    "checked": true,
    "id": "d40dbe668d5b68419e934dfa4c5851ffa1c24aa2",
    "semantic_title": "exposing attention glitches with flip-flop language modeling",
    "citation_count": 12,
    "authors": [
      "Bingbin Liu",
      "Jordan Ash",
      "Surbhi Goel",
      "Akshay Krishnamurthy",
      "Cyril Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/511d7c4e61878cf08ece6351ea3c529e-Abstract-Conference.html": {
    "title": "Information Design in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "Reinforcement learning (RL) is inspired by the way human infants and animals learn from the environment. The setting is somewhat idealized because, in actual tasks, other agents in the environment have their own goals and behave adaptively to the ego agent. To thrive in those environments, the agent needs to influence other agents so their actions become more helpful and less harmful. Research in computational economics distills two ways to influence others directly: by providing tangible goods (mechanism design) and by providing information (information design). This work investigates information design problems for a group of RL agents. The main challenges are two-fold. One is the information provided will immediately affect the transition of the agent trajectories, which introduces additional non-stationarity. The other is the information can be ignored, so the sender must provide information that the receiver is willing to respect. We formulate the Markov signaling game, and develop the notions of signaling gradient and the extended obedience constraints that address these challenges. Our algorithm is efficient on various mixed-motive tasks and provides further insights into computational economics. Our code is publicly available at https://github.com/YueLin301/InformationDesignMARL",
    "keywords": [],
    "checked": true,
    "id": "a1ff5404fa9ae2353043c106eaf2f6d6649fb81f",
    "semantic_title": "information design in multi-agent reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Yue Lin",
      "Wenhao Li",
      "Hongyuan Zha",
      "Baoxiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/51373b6499708b6fcc38f1e8f8f5b376-Abstract-Conference.html": {
    "title": "Gaussian Mixture Solvers for Diffusion Models",
    "volume": "main",
    "abstract": "Recently, diffusion models have achieved great success in generative tasks. Sampling from diffusion models is equivalent to solving the reverse diffusion stochastic differential equations (SDEs) or the corresponding probability flow ordinary differential equations (ODEs). In comparison, SDE-based solvers can generate samples of higher quality and are suited for image translation tasks like stroke-based synthesis. During inference, however, existing SDE-based solvers are severely constrained by the efficiency-effectiveness dilemma. Our investigation suggests that this is because the Gaussian assumption in the reverse transition kernel is frequently violated (even in the case of simple mixture data) given a limited number of discretization steps. To overcome this limitation, we introduce a novel class of SDE-based solvers called \\emph{Gaussian Mixture Solvers (GMS)} for diffusion models. Our solver estimates the first three-order moments and optimizes the parameters of a Gaussian mixture transition kernel using generalized methods of moments in each step during sampling. Empirically, our solver outperforms numerous SDE-based solvers in terms of sample quality in image generation and stroke-based synthesis in various diffusion models, which validates the motivation and effectiveness of GMS. Our code is available at https://github.com/Guohanzhong/GMS",
    "keywords": [],
    "checked": true,
    "id": "8e6d2f5bd4bd5a33ba2f1ebe3575b8b6d85ca6fd",
    "semantic_title": "gaussian mixture solvers for diffusion models",
    "citation_count": 1,
    "authors": [
      "Hanzhong Guo",
      "Cheng Lu",
      "Fan Bao",
      "Tianyu Pang",
      "Shuicheng Yan",
      "Chao Du",
      "Chongxuan LI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/51484744337f4bf5fea0e4dd92ddab0b-Abstract-Conference.html": {
    "title": "Trade-off Between Efficiency and Consistency for Removal-based Explanations",
    "volume": "main",
    "abstract": "In the current landscape of explanation methodologies, most predominant approaches, such as SHAP and LIME, employ removal-based techniques to evaluate the impact of individual features by simulating various scenarios with specific features omitted. Nonetheless, these methods primarily emphasize efficiency in the original context, often resulting in general inconsistencies. In this paper, we demonstrate that such inconsistency is an inherent aspect of these approaches by establishing the Impossible Trinity Theorem, which posits that interpretability, efficiency, and consistency cannot hold simultaneously. Recognizing that the attainment of an ideal explanation remains elusive, we propose the utilization of interpretation error as a metric to gauge inefficiencies and inconsistencies. To this end, we present two novel algorithms founded on the standard polynomial basis, aimed at minimizing interpretation error. Our empirical findings indicate that the proposed methods achieve a substantial reduction in interpretation error, up to 31.8 times lower when compared to alternative techniques",
    "keywords": [],
    "checked": true,
    "id": "5ed9d7fc307b9817bce423ff089e2c29e4d8160f",
    "semantic_title": "trade-off between efficiency and consistency for removal-based explanations",
    "citation_count": 2,
    "authors": [
      "Yifan Zhang",
      "Haowei He",
      "Zhiquan Tan",
      "Yang Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5159aaee380391c366b27994ed225e4f-Abstract-Conference.html": {
    "title": "QuACK: Accelerating Gradient-Based Quantum Optimization with Koopman Operator Learning",
    "volume": "main",
    "abstract": "Quantum optimization, a key application of quantum computing, has traditionally been stymied by the linearly increasing complexity of gradient calculations with an increasing number of parameters. This work bridges the gap between Koopman operator theory, which has found utility in applications because it allows for a linear representation of nonlinear dynamical systems, and natural gradient methods in quantum optimization, leading to a significant acceleration of gradient-based quantum optimization. We present Quantum-circuit Alternating Controlled Koopman learning (QuACK), a novel framework that leverages an alternating algorithm for efficient prediction of gradient dynamics on quantum computers. We demonstrate QuACK's remarkable ability to accelerate gradient-based optimization across a range of applications in quantum optimization and machine learning. In fact, our empirical studies, spanning quantum chemistry, quantum condensed matter, quantum machine learning, and noisy environments, have shown accelerations of more than 200x speedup in the overparameterized regime, 10x speedup in the smooth regime, and 3x speedup in the non-smooth regime. With QuACK, we offer a robust advancement that harnesses the advantage of gradient-based quantum optimization for practical benefits",
    "keywords": [],
    "checked": true,
    "id": "09b85dd390b8e4e738ad884e7dc468ab0bdc9f5c",
    "semantic_title": "quack: accelerating gradient-based quantum optimization with koopman operator learning",
    "citation_count": 0,
    "authors": [
      "Di Luo",
      "Jiayu Shen",
      "Rumen Dangovski",
      "Marin Soljacic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/516ca2e9e7bffbb4027a25d9f8838bc9-Abstract-Conference.html": {
    "title": "Provably Robust Temporal Difference Learning for Heavy-Tailed Rewards",
    "volume": "main",
    "abstract": "In a broad class of reinforcement learning applications, stochastic rewards have heavy-tailed distributions, which lead to infinite second-order moments for stochastic (semi)gradients in policy evaluation and direct policy optimization. In such instances, the existing RL methods may fail miserably due to frequent statistical outliers. In this work, we establish that temporal difference (TD) learning with a dynamic gradient clipping mechanism, and correspondingly operated natural actor-critic (NAC), can be provably robustified against heavy-tailed reward distributions. It is shown in the framework of linear function approximation that a favorable tradeoff between bias and variability of the stochastic gradients can be achieved with this dynamic gradient clipping mechanism. In particular, we prove that robust versions of TD learning achieve sample complexities of order $\\mathcal{O}(\\varepsilon^{-\\frac{1}{p}})$ and $\\mathcal{O}(\\varepsilon^{-1-\\frac{1}{p}})$ with and without the full-rank assumption on the feature matrix, respectively, under heavy-tailed rewards with finite moments of order $(1+p)$ for some $p\\in(0,1]$, both in expectation and with high probability. We show that a robust variant of NAC based on Robust TD learning achieves $\\tilde{\\mathcal{O}}(\\varepsilon^{-4-\\frac{2}{p}})$ sample complexity. We corroborate our theoretical results with numerical experiments",
    "keywords": [],
    "checked": true,
    "id": "4c0333c745f3a15c4a801215ea22f1123d27486a",
    "semantic_title": "provably robust temporal difference learning for heavy-tailed rewards",
    "citation_count": 0,
    "authors": [
      "Semih Cayci",
      "Atilla Eryilmaz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/516fd05dc408fd6d6374940a83930193-Abstract-Conference.html": {
    "title": "Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models",
    "volume": "main",
    "abstract": "Despite their prevalence in deep-learning communities, over-parameterized models convey high demands of computational costs for proper training. This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy. Empirical evidence reveals that when scaling down into network modules, such as heads in self-attention models, we can observe varying learning patterns implicitly associated with each module's trainability. To describe such modular-level learning capabilities, we introduce a novel concept dubbed modular neural tangent kernel (mNTK), and we demonstrate that the quality of a module's learning is tightly associated with its mNTK's principal eigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module learns features with better convergence, while those miniature ones may impact generalization negatively. Inspired by the discovery, we propose a novel training strategy termed Modular Adaptive Training (MAT) to update those modules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively, concentrating the model on learning common features and ignoring those inconsistent ones. Unlike most existing training schemes with a complete BP cycle across all network modules, MAT can significantly save computations by its partially-updating strategy and can further improve performance. Experiments show that MAT nearly halves the computational cost of model training and outperforms the accuracy of baselines",
    "keywords": [],
    "checked": false,
    "id": "2516652a00f5620ff27a1a7c99a99a7926466ca2",
    "semantic_title": "mlaas : a framework for exposing machine learning as a service on cloud platforms",
    "citation_count": 0,
    "authors": [
      "Yubin Shi",
      "Yixuan Chen",
      "Mingzhi Dong",
      "Xiaochen Yang",
      "Dongsheng Li",
      "Yujiang Wang",
      "Robert Dick",
      "Qin Lv",
      "Yingying Zhao",
      "Fan Yang",
      "Tun Lu",
      "Ning Gu",
      "Li Shang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5195825ee60d7efc1e42b7f3f3137040-Abstract-Conference.html": {
    "title": "Class-Distribution-Aware Pseudo-Labeling for Semi-Supervised Multi-Label Learning",
    "volume": "main",
    "abstract": "Pseudo-labeling has emerged as a popular and effective approach for utilizing unlabeled data. However, in the context of semi-supervised multi-label learning (SSMLL), conventional pseudo-labeling methods encounter difficulties when dealing with instances associated with multiple labels and an unknown label count. These limitations often result in the introduction of false positive labels or the neglect of true positive ones. To overcome these challenges, this paper proposes a novel solution called Class-Aware Pseudo-Labeling (CAP) that performs pseudo-labeling in a class-aware manner. The proposed approach introduces a regularized learning framework incorporating class-aware thresholds, which effectively control the assignment of positive and negative pseudo-labels for each class. Notably, even with a small proportion of labeled examples, our observations demonstrate that the estimated class distribution serves as a reliable approximation. Motivated by this finding, we develop a class-distribution-aware thresholding strategy to ensure the alignment of pseudo-label distribution with the true distribution. The correctness of the estimated class distribution is theoretically verified, and a generalization error bound is provided for our proposed method. Extensive experiments on multiple benchmark datasets confirm the efficacy of CAP in addressing the challenges of SSMLL problems",
    "keywords": [],
    "checked": false,
    "id": "8140e51258eb15c52a328689cec1318d00e5209e",
    "semantic_title": "class-distribution-aware pseudo labeling for semi-supervised multi-label learning",
    "citation_count": 0,
    "authors": [
      "Ming-Kun Xie",
      "Jiahao Xiao",
      "Hao-Zhe Liu",
      "Gang Niu",
      "Masashi Sugiyama",
      "Sheng-Jun Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/51ba8a68f471d952af625d1faf55e6c6-Abstract-Conference.html": {
    "title": "Adaptive Data Analysis in a Balanced Adversarial Model",
    "volume": "main",
    "abstract": "In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from an unknown distribution $\\cal{D}$, andis required to provide accurate estimations to a sequence of adaptively chosen statistical queries with respect to $\\cal{D}$.Hardt and Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that in general, it is computationally hard to answer more than $\\Theta(n^2)$ adaptive queries, assuming the existence of one-way functions. However, these negative results strongly rely on an adversarial model that significantly advantages the adversarial analyst over the mechanism, as the analyst, who chooses the adaptive queries, also chooses the underlying distribution $\\cal{D}$. This imbalance raises questions with respect to the applicability of the obtained hardness results -- an analyst who has complete knowledge of the underlying distribution $\\cal{D}$ would have little need, if at all, to issue statistical queries to a mechanism which only holds a finite number of samples from $\\cal{D}$.We consider more restricted adversaries, called \\emph{balanced}, where each such adversary consists of two separated algorithms: The \\emph{sampler} who is the entity that chooses the distribution and provides the samples to the mechanism, and the \\emph{analyst} who chooses the adaptive queries, but has no prior knowledge of the underlying distribution (and hence has no a priori advantage with respect to the mechanism). We improve the quality of previous lower bounds by revisiting them using an efficient \\emph{balanced} adversary, under standard public-key cryptography assumptions. We show that these stronger hardness assumptions are unavoidable in the sense that any computationally bounded \\emph{balanced} adversary that has the structure of all known attacks, implies the existence of public-key cryptography",
    "keywords": [],
    "checked": true,
    "id": "7feee0515f59fc684c7bff17b6f0605d9c873c16",
    "semantic_title": "adaptive data analysis in a balanced adversarial model",
    "citation_count": 0,
    "authors": [
      "Kobbi Nissim",
      "Uri Stemmer",
      "Eliad Tsfadia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/51ec452ca04d8ec7160e5bbaf76153f6-Abstract-Conference.html": {
    "title": "Accelerating Molecular Graph Neural Networks via Knowledge Distillation",
    "volume": "main",
    "abstract": "Recent advances in graph neural networks (GNNs) have enabled more comprehensive modeling of molecules and molecular systems, thereby enhancing the precision of molecular property prediction and molecular simulations. Nonetheless, as the field has been progressing to bigger and more complex architectures, state-of-the-art GNNs have become largely prohibitive for many large-scale applications. In this paper, we explore the utility of knowledge distillation (KD) for accelerating molecular GNNs. To this end, we devise KD strategies that facilitate the distillation of hidden representations in directional and equivariant GNNs, and evaluate their performance on the regression task of energy and force prediction. We validate our protocols across different teacher-student configurations and datasets, and demonstrate that they can consistently boost the predictive accuracy of student models without any modifications to their architecture. Moreover, we conduct comprehensive optimization of various components of our framework, and investigate the potential of data augmentation to further enhance performance. All in all, we manage to close the gap in predictive accuracy between teacher and student models by as much as 96.7\\% and 62.5\\% for energy and force prediction respectively, while fully preserving the inference throughput of the more lightweight models",
    "keywords": [],
    "checked": true,
    "id": "0aa2cdc4c1f7c8d2aedc9b739272ba1d3bd4b9e7",
    "semantic_title": "accelerating molecular graph neural networks via knowledge distillation",
    "citation_count": 1,
    "authors": [
      "Filip Ekström Kelvinius",
      "Dimitar Georgiev",
      "Artur Toshev",
      "Johannes Gasteiger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/51f3d6252706100325ddc435ba0ade0e-Abstract-Conference.html": {
    "title": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models",
    "volume": "main",
    "abstract": "The computation necessary for training Transformer-based language models has skyrocketed in recent years.This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop., RHO-loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain",
    "keywords": [],
    "checked": true,
    "id": "881883842c2661b41bbfc999d56c763b1ceef0bd",
    "semantic_title": "no train no gain: revisiting efficient training algorithms for transformer-based language models",
    "citation_count": 17,
    "authors": [
      "Jean Kaddour",
      "Oscar Key",
      "Piotr Nawrot",
      "Pasquale Minervini",
      "Matt J. Kusner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/51f9036d5e7ae822da8f6d4adda1fb39-Abstract-Conference.html": {
    "title": "Layer-Neighbor Sampling --- Defusing Neighborhood Explosion in GNNs",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have received significant attention recently, but training them at a large scale remains a challenge.Mini-batch training coupled with sampling is used to alleviate this challenge.However, existing approaches either suffer from the neighborhood explosion phenomenon or have suboptimal performance. To address these issues, we propose a new sampling algorithm called LAyer-neighBOR sampling (LABOR). It is designed to be a direct replacement for Neighbor Sampling (NS) with the same fanout hyperparameter while sampling up to 7 times fewer vertices, without sacrificing quality.By design, the variance of the estimator of each vertex matches NS from the point of view of a single vertex.Moreover, under the same vertex sampling budget constraints, LABOR converges faster than existing layer sampling approaches and can use up to 112 times larger batch sizes compared to NS",
    "keywords": [],
    "checked": false,
    "id": "81d633a4724eb5410f3c2e2402efe8fbeb4d24f9",
    "semantic_title": "layer-neighbor sampling -- defusing neighborhood explosion in gnns",
    "citation_count": 1,
    "authors": [
      "Muhammed Fatih Balin",
      "Ümit Çatalyürek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/51f9d542dea8bed1f66c8add6ec23c69-Abstract-Conference.html": {
    "title": "Undirected Probabilistic Model for Tensor Decomposition",
    "volume": "main",
    "abstract": "Tensor decompositions (TDs) serve as a powerful tool for analyzing multiway data. Traditional TDs incorporate prior knowledge about the data into the model, such as a directed generative process from latent factors to observations. In practice, selecting proper structural or distributional assumptions beforehand is crucial for obtaining a promising TD representation. However, since such prior knowledge is typically unavailable in real-world applications, choosing an appropriate TD model can be challenging. This paper aims to address this issue by introducing a flexible TD framework that discards the structural and distributional assumptions, in order to learn as much information from the data. Specifically, we construct a TD model that captures the joint probability of the data and latent tensor factors through a deep energy-based model (EBM). Neural networks are then employed to parameterize the joint energy function of tensor factors and tensor entries. The flexibility of EBM and neural networks enables the learning of underlying structures and distributions. In addition, by designing the energy function, our model unifies the learning process of different types of tensors, such as static tensors and dynamic tensors with time stamps. The resulting model presents a doubly intractable nature due to the presence of latent tensor factors and the unnormalized probability function. To efficiently train the model, we derive a variational upper bound of the conditional noise-contrastive estimation objective that learns the unnormalized joint probability by distinguishing data from conditional noises. We show advantages of our model on both synthetic and several real-world datasets",
    "keywords": [],
    "checked": false,
    "id": "23ca8dfa61866a926aaed66a873c5cbd29b965a6",
    "semantic_title": "probabilistic tensor decomposition of neural population spiking activity",
    "citation_count": 4,
    "authors": [
      "Zerui Tao",
      "Toshihisa Tanaka",
      "Qibin Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/51fd9a7d1706023cb9f8210cc6ac357c-Abstract-Conference.html": {
    "title": "Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules",
    "volume": "main",
    "abstract": "Masked graph modeling excels in the self-supervised representation learning of molecular graphs. Scrutinizing previous studies, we can reveal a common scheme consisting of three key components: (1) graph tokenizer, which breaks a molecular graph into smaller fragments (\\ie subgraphs) and converts them into tokens; (2) graph masking, which corrupts the graph with masks; (3) graph autoencoder, which first applies an encoder on the masked graph to generate the representations, and then employs a decoder on the representations to recover the tokens of the original graph. However, the previous MGM studies focus extensively on graph masking and encoder, while there is limited understanding of tokenizer and decoder. To bridge the gap, we first summarize popular molecule tokenizers at the granularity of node, edge, motif, and Graph Neural Networks (GNNs), and then examine their roles as the MGM's reconstruction targets. Further, we explore the potential of adopting an expressive decoder in MGM. Our results show that a subgraph-level tokenizer and a sufficiently expressive decoder with remask decoding have a \\yuan{large impact on the encoder's representation learning}. Finally, we propose a novel MGM method SimSGT, featuring a Simple GNN-based Tokenizer (SGT) and an effective decoding strategy. We empirically validate that our method outperforms the existing molecule self-supervised learning methods. Our codes and checkpoints are available at https://github.com/syr-cn/SimSGT",
    "keywords": [],
    "checked": true,
    "id": "fbebc512a0471446fd87406067463ea4d70ac1fa",
    "semantic_title": "rethinking tokenizer and decoder in masked graph modeling for molecules",
    "citation_count": 2,
    "authors": [
      "ZHIYUAN LIU",
      "Yaorui Shi",
      "An Zhang",
      "Enzhi Zhang",
      "Kenji Kawaguchi",
      "Xiang Wang",
      "Tat-Seng Chua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/520425a5a4c2fb7f7fc345078b188201-Abstract-Conference.html": {
    "title": "Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples",
    "volume": "main",
    "abstract": "Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense. The code is available at https://github.com/SCLBD/BackdoorBench (PyTorch) and https://github.com/shawkui/MindTrojan (MindSpore)",
    "keywords": [],
    "checked": true,
    "id": "92018bf2418a2276cdadff9b81b740c8b13d89c4",
    "semantic_title": "shared adversarial unlearning: backdoor mitigation by unlearning shared adversarial examples",
    "citation_count": 5,
    "authors": [
      "Shaokui Wei",
      "Mingda Zhang",
      "Hongyuan Zha",
      "Baoyuan Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/52493d82db00e73abb2858a5a5f28717-Abstract-Conference.html": {
    "title": "Calibration by Distribution Matching: Trainable Kernel Calibration Metrics",
    "volume": "main",
    "abstract": "Calibration ensures that probabilistic forecasts meaningfully capture uncertainty by requiring that predicted probabilities align with empirical frequencies. However, many existing calibration methods are specialized for post-hoc recalibration, which can worsen the sharpness of forecasts. Drawing on the insight that calibration can be viewed as a distribution matching task, we introduce kernel-based calibration metrics that unify and generalize popular forms of calibration for both classification and regression. These metrics admit differentiable sample estimates, making it easy to incorporate a calibration objective into empirical risk minimization. Furthermore, we provide intuitive mechanisms to tailor calibration metrics to a decision task, and enforce accurate loss estimation and no regret decisions. Our empirical evaluation demonstrates that employing these metrics as regularizers enhances calibration, sharpness, and decision-making across a range of regression and classification tasks, outperforming methods relying solely on post-hoc recalibration",
    "keywords": [],
    "checked": true,
    "id": "7e8f43302006e5e8771e2445af7b06f1c8e7c9c9",
    "semantic_title": "calibration by distribution matching: trainable kernel calibration metrics",
    "citation_count": 0,
    "authors": [
      "Charlie Marx",
      "Sofian Zalouk",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/525338e0d98401a62950bc7c454eb83d-Abstract-Conference.html": {
    "title": "Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets",
    "volume": "main",
    "abstract": "The Strong Lottery Ticket Hypothesis (SLTH) states that randomly-initialised neural networks likely contain subnetworks that perform well without any training. Although unstructured pruning has been extensively studied in this context, its structured counterpart, which can deliver significant computational and memory efficiency gains, has been largely unexplored. One of the main reasons for this gap is the limitations of the underlying mathematical tools used in formal analyses of the SLTH.In this paper, we overcome these limitations: we leverage recent advances in the multidimensional generalisation of the Random Subset-Sum Problem and obtain a variant that admits the stochastic dependencies that arise when addressing structured pruning in the SLTH. We apply this result to prove, for a wide class of random Convolutional Neural Networks, the existence of structured subnetworks that can approximate any sufficiently smaller network.This result provides the first sub-exponential bound around the SLTH for structured pruning, opening up new avenues for further research on the hypothesis and contributing to the understanding of the role of over-parameterization in deep learning",
    "keywords": [],
    "checked": true,
    "id": "b6bc1e5cf7589abc259e9eb18597564121b41364",
    "semantic_title": "polynomially over-parameterized convolutional neural networks contain structured strong winning lottery tickets",
    "citation_count": 0,
    "authors": [
      "Arthur da Cunha",
      "Francesco D'Amore",
      " Natale"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/525c95ffca1f57a10e3527d3584f3cf1-Abstract-Conference.html": {
    "title": "Robustifying Generalizable Implicit Shape Networks with a Tunable Non-Parametric Model",
    "volume": "main",
    "abstract": "Feedforward generalizable models for implicit shape reconstruction from unoriented point cloud present multiple advantages, including high performance and inference speed. However, they still suffer from generalization issues, ranging from underfitting the input point cloud, to misrepresenting samples outside of the training data distribution, or with toplogies unseen at training. We propose here an efficient mechanism to remedy some of these limitations at test time. We combine the inter-shape data prior of the network with an intra-shape regularization prior of a Nyström Kernel Ridge Regression, that we further adapt by fitting its hyperprameters to the current shape. The resulting shape function defined in a shape specific Reproducing Kernel Hilbert Space benefits from desirable stability and efficiency properties and grants a shape adaptive expressiveness-robustness trade-off. We demonstrate the improvement obtained through our method with respect to baselines and the state-of-the-art using synthetic and real data",
    "keywords": [],
    "checked": true,
    "id": "271ec984da74d45b02f0fb1dfa41f348aaa36553",
    "semantic_title": "robustifying generalizable implicit shape networks with a tunable non-parametric model",
    "citation_count": 1,
    "authors": [
      "Amine Ouasfi",
      "Adnane Boukhayma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/525d24400247f884c3419b0b7b1c4829-Abstract-Conference.html": {
    "title": "Segment Anything in 3D with NeRFs",
    "volume": "main",
    "abstract": "Recently, the Segment Anything Model (SAM) emerged as a powerful vision foundation model which is capable to segment anything in 2D images. This paper aims to generalize SAM to segment 3D objects. Rather than replicating the data acquisition and annotation procedure which is costly in 3D, we design an efficient solution, leveraging the Neural Radiance Field (NeRF) as a cheap and off-the-shelf prior that connects multi-view 2D images to the 3D space. We refer to the proposed solution as SA3D, for Segment Anything in 3D. It is only required to provide a manual segmentation prompt (e.g., rough points) for the target object in a single view, which is used to generate its 2D mask in this view with SAM. Next, SA3D alternately performs mask inverse rendering and cross-view self-prompting across various views to iteratively complete the 3D mask of the target object constructed with voxel grids. The former projects the 2D mask obtained by SAM in the current view onto 3D mask with guidance of the density distribution learned by the NeRF; The latter extracts reliable prompts automatically as the input to SAM from the NeRF-rendered 2D mask in another view. We show in experiments that SA3D adapts to various scenes and achieves 3D segmentation within minutes. Our research offers a generic and efficient methodology to lift a 2D vision foundation model to 3D, as long as the 2D model can steadily address promptable segmentation across multiple views",
    "keywords": [],
    "checked": true,
    "id": "99306dec69473a73b0f72c33fd5d757bcb665c33",
    "semantic_title": "segment anything in 3d with nerfs",
    "citation_count": 45,
    "authors": [
      "Jiazhong Cen",
      "Zanwei Zhou",
      "Jiemin Fang",
      "chen yang",
      "Wei Shen",
      "Lingxi Xie",
      "Dongsheng Jiang",
      "XIAOPENG ZHANG",
      "Qi Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/526356453b7301c9b29aa0533f62bdef-Abstract-Conference.html": {
    "title": "Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction",
    "volume": "main",
    "abstract": "Cross-device Federated Learning (FL) faces significant challenges where low-end clients that could potentially make unique contributions are excluded from training large models due to their resource bottlenecks. Recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. This paper presents a unifying framework for heterogeneous FL algorithms with online model extraction and provides a general convergence analysis for the first time. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we introduce the concept of minimum coverage index, together with model reduction noise, which will determine the convergence of heterogeneous federated learning, and therefore we advocate for a holistic approach that considers both factors to enhance the efficiency of heterogeneous federated learning",
    "keywords": [],
    "checked": true,
    "id": "56a868d761bb8cc35625d354df427aa1a625cd92",
    "semantic_title": "every parameter matters: ensuring the convergence of federated learning with dynamic heterogeneous models reduction",
    "citation_count": 2,
    "authors": [
      "Hanhan Zhou",
      "Tian Lan",
      "Guru Prasadh Venkataramani",
      "Wenbo Ding"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/528388f1ad3a481249a97cbb698d2fe6-Abstract-Conference.html": {
    "title": "Small batch deep reinforcement learning",
    "volume": "main",
    "abstract": "In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests reducing the batch size can result in a number of significant performance gains; this is surprising, as the general tendency when training neural networks is towards larger batch sizes for improved performance. We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon",
    "keywords": [],
    "checked": true,
    "id": "339e4b4aaa5c43660bf48756066433e8a2045187",
    "semantic_title": "small batch deep reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Johan Obando Ceron",
      "Marc Bellemare",
      "Pablo Samuel Castro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5297e56ac65ba2bfa70ee9fc4818c042-Abstract-Conference.html": {
    "title": "A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability",
    "volume": "main",
    "abstract": "In the past few years, there has been an explosive surge in the use of machine learning (ML) techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear programs (MILPs). Despite the achievements, the limited availability of real-world instances often leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic MILP instance generation techniques. However, existing methods either rely heavily on expert-designed formulations or struggle to capture the rich features of real-world instances. To tackle this problem, we propose G2MILP, the first deep generative framework for MILP instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instances without prior expert-designed formulations, while preserving the structures and computational hardness of real-world datasets, simultaneously. Thus the generated instances can facilitate downstream tasks for enhancing MILP solvers under limited data availability. We design a suite of benchmarks to evaluate the quality of the generated MILP instances. Experiments demonstrate that our method can produce instances that closely resemble real-world datasets in terms of both structures and computational hardness. The deliverables are released at https://miralab-ustc.github.io/L2O-G2MILP",
    "keywords": [],
    "checked": true,
    "id": "abb6e9dc000fbc094e1598ae5e81c545340656ea",
    "semantic_title": "a deep instance generative framework for milp solvers under limited data availability",
    "citation_count": 1,
    "authors": [
      "Zijie Geng",
      "Xijun Li",
      "Jie Wang",
      "Xiao Li",
      "Yongdong Zhang",
      "Feng Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/52d63f9e4b81f866bf69fb3c834aad47-Abstract-Conference.html": {
    "title": "Multi-Swap k-Means++",
    "volume": "main",
    "abstract": "The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular $k$-means clustering objective and is known to give an $O(\\log k)$-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting $k$-means++ with $O(k \\log \\log k)$ local-search steps obtained through the $k$-means++ sampling distribution to yield a $c$-approximation to the $k$-means clustering problem, where $c$ is a large absolute constant. Here we generalize and extend their local-search algorithm by considering larger and more sophisticated local-search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a $9 + \\varepsilon$ approximation ratio, which is the best possible for local search. Importantly we show that our algorithm is practical, namely easy to implement and fast enough to run on a variety of classic datasets, and outputs solutions of better cost",
    "keywords": [],
    "checked": true,
    "id": "4d19140e0da85728bb9d6b91da2287a0618c4843",
    "semantic_title": "multi-swap k-means++",
    "citation_count": 1,
    "authors": [
      "Lorenzo Beretta",
      "Vincent Cohen-Addad",
      "Silvio Lattanzi",
      "Nikos Parotsidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/52da50b1ef221e4b1793e3bf44dd973d-Abstract-Conference.html": {
    "title": "A Unified Discretization Framework for Differential Equation Approach with Lyapunov Arguments for Convex Optimization",
    "volume": "main",
    "abstract": "The differential equation (DE) approach for convex optimization, which relates optimization methods to specific continuous DEs with rate-revealing Lyapunov functionals, has gained increasing interest since the seminal paper by Su--Boyd--Candès (2014).However, the approach still lacks a crucial component to make it truly useful: there is no general, consistent way to transition back to discrete optimization methods. Consequently, even if we derive insights from continuous DEs, we still need to perform individualized and tedious calculations for the analysis of each method.This paper aims to bridge this gap by introducing a new concept called ``weak discrete gradient'' (wDG), which consolidates the conditions required for discrete versions of gradients in the DE approach arguments.We then define abstract optimization methods using wDG and provide abstract convergence theories that parallel those in continuous DEs.We demonstrate that many typical optimization methods and their convergence rates can be derived as special cases of this abstract theory.The proposed unified discretization framework for the differential equation approach to convex optimization provides an easy environment for developing new optimization methods and achieving competitive convergence rates with state-of-the-art methods, such as Nesterov's accelerated gradient",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kansei Ushiyama",
      "Shun Sato",
      "Takayasu Matsuo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/52f050499cf82fa8efb588e263f6f3a7-Abstract-Conference.html": {
    "title": "Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator",
    "volume": "main",
    "abstract": "Text-to-video is a rapidly growing research area that aims to generate a semantic, identical, and temporal coherence sequence of frames that accurately align with the input text prompt. This study focuses on zero-shot text-to-video generation considering the data- and cost-efficient. To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of ``moving images'', we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Furthermore, to ensure temporal and identical coherence while maintaining semantic coherence, we propose a series of annotative modifications to adapting LDMs in the reverse process, including joint noise sampling, step-aware attention shift, and dual-path interpolation. Without any video data and training requirements, Free-Bloom generates vivid and high-quality videos, awe-inspiring in generating complex scenes with semantic meaningful frame sequences. In addition, Free-Bloom is naturally compatible with LDMs-based extensions",
    "keywords": [],
    "checked": true,
    "id": "120aca3e415b6641a0b0cd20695ab85ed7789612",
    "semantic_title": "free-bloom: zero-shot text-to-video generator with llm director and ldm animator",
    "citation_count": 21,
    "authors": [
      "Hanzhuo Huang",
      "Yufan Feng",
      "Cheng Shi",
      "Lan Xu",
      "Jingyi Yu",
      "Sibei Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5301c49207917c5c870131959971851c-Abstract-Conference.html": {
    "title": "NeRF Revisited: Fixing Quadrature Instability in Volume Rendering",
    "volume": "main",
    "abstract": "Neural radiance fields (NeRF) rely on volume rendering to synthesize novel views. Volume rendering requires evaluating an integral along each ray, which is numerically approximated with a finite sum that corresponds to the exact integral along the ray under piecewise constant volume density. As a consequence, the rendered result is unstable w.r.t. the choice of samples along the ray, a phenomenon that we dub quadrature instability. We propose a mathematically principled solution by reformulating the sample-based rendering equation so that it corresponds to the exact integral under piecewise linear volume density. This simultaneously resolves multiple issues: conflicts between samples along different rays, imprecise hierarchical sampling, and non-differentiability of quantiles of ray termination distances w.r.t. model parameters. We demonstrate several benefits over the classical sample-based rendering equation, such as sharper textures, better geometric reconstruction, and stronger depth supervision. Our proposed formulation can be also be used as a drop-in replacement to the volume rendering equation of existing NeRF-based methods. Our project page can be found at pl-nerf.github.io",
    "keywords": [],
    "checked": true,
    "id": "3efbcdfb1db5cda74edb017e74df6daef7b6017a",
    "semantic_title": "nerf revisited: fixing quadrature instability in volume rendering",
    "citation_count": 2,
    "authors": [
      "Mikaela Angelina Uy",
      "Kiyohiro Nakayama",
      "Guandao Yang",
      "Rahul Thomas",
      "Leonidas J. Guibas",
      "Ke Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5305b7891e1098dd9773d35cd9333180-Abstract-Conference.html": {
    "title": "Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima",
    "volume": "main",
    "abstract": "Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \\rho \\frac{\\nabla f(x_t)}{\\lVert \\nabla f(x_t) \\rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions.Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points.For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\\tilde \\Theta(\\frac{1}{T^2})$, the convergence bound of stochastic SAM suffers an inevitable additive term $\\mathcal O(\\rho^2)$, indicating convergence only up to neighborhoods of optima.In fact, such $\\mathcal O(\\rho^2)$ factors arise for stochastic SAM in all the settings we consider, and also for deterministic SAM in nonconvex cases; importantly, we prove by examples that such terms are unavoidable.Our results highlight vastly different characteristics of SAM with vs. without decaying perturbation size or gradient normalization, and suggest that the intuitions gained from one version may not apply to the other",
    "keywords": [],
    "checked": true,
    "id": "c5ee9fc8bd862efc8afdf0866142ff572dd34c73",
    "semantic_title": "practical sharpness-aware minimization cannot converge all the way to optima",
    "citation_count": 3,
    "authors": [
      "Dongkuk Si",
      "Chulhee Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/531230cfac80c65017ad0f85d3031edc-Abstract-Conference.html": {
    "title": "Online Convex Optimization with Unbounded Memory",
    "volume": "main",
    "abstract": "Online convex optimization (OCO) is a widely used framework in online learning. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. However, in many applications the learner's loss depends not only on the current decision but on the entire history of decisions until that point. The OCO framework and its existing generalizations do not capture this, and they can only be applied to many settings of interest after a long series of approximation arguments. They also leave open the question of whether the dependence on memory is tight because there are no non-trivial lower bounds. In this work we introduce a generalization of the OCO framework, ``Online Convex Optimization with Unbounded Memory'', that captures long-term dependence on past decisions. We introduce the notion of $p$-effective memory capacity, $H_p$, that quantifies the maximum influence of past decisions on present losses. We prove an $O(\\sqrt{H_p T})$ upper bound on the policy regret and a matching (worst-case) lower bound. As a special case, we prove the first non-trivial lower bound for OCO with finite memory~\\citep{anavaHM2015online}, which could be of independent interest, and also improve existing upper bounds. We demonstrate the broad applicability of our framework by using it to derive regret bounds, and to improve and simplify existing regret bound derivations, for a variety of online learning problems including online linear control and an online variant of performative prediction",
    "keywords": [],
    "checked": true,
    "id": "0357499656bea3c723b73a6ae9ad1d42589e2dde",
    "semantic_title": "online convex optimization with unbounded memory",
    "citation_count": 2,
    "authors": [
      "Raunak Kumar",
      "Sarah Dean",
      "Robert Kleinberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/531998dc1fc858b5857a90b74d96ecab-Abstract-Conference.html": {
    "title": "Making Scalable Meta Learning Practical",
    "volume": "main",
    "abstract": "Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e.,\\ learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains",
    "keywords": [],
    "checked": true,
    "id": "a815c3209e7baff4466dbf6e129129511f842b7e",
    "semantic_title": "making scalable meta learning practical",
    "citation_count": 1,
    "authors": [
      "Sang Choe",
      "Sanket Vaibhav Mehta",
      "Hwijeen Ahn",
      "Willie Neiswanger",
      "Pengtao Xie",
      "Emma Strubell",
      "Eric Xing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5321b1dabcd2be188d796c21b733e8c7-Abstract-Conference.html": {
    "title": "Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing",
    "volume": "main",
    "abstract": "Large-scale text-to-image generative models have been a ground-breaking development in generative AI, with diffusion models showing their astounding ability to synthesize convincing images following an input text prompt. The goal of image editing research is to give users control over the generated images by modifying the text prompt. Current image editing techniques are susceptible to unintended modifications of regions outside the targeted area, such as on the background or on distractor objects which have some semantic or visual relationship with the targeted object. According to our experimental findings, inaccurate cross-attention maps are at the root of this problem. Based on this observation, we propose $\\textit{Dynamic Prompt Learning}$ ($DPL$) to force cross-attention maps to focus on correct $\\textit{noun}$ words in the text prompt. By updating the dynamic tokens for nouns in the textual input with the proposed leakage repairment losses, we achieve fine-grained image editing over particular objects while preventing undesired changes to other image regions. Our method $DPL$, based on the publicly available $\\textit{Stable Diffusion}$, is extensively evaluated on a wide range of images, and consistently obtains superior results both quantitatively (CLIP score, Structure-Dist) and qualitatively (on user-evaluation). We show improved prompt editing results for Word-Swap, Prompt Refinement, and Attention Re-weighting, especially for complex multi-object scenes",
    "keywords": [],
    "checked": true,
    "id": "8d531cb8cf51eec3b8f1106d189295fa3c81c02a",
    "semantic_title": "dynamic prompt learning: addressing cross-attention leakage for text-based image editing",
    "citation_count": 7,
    "authors": [
      "kai wang",
      "Fei Yang",
      "Shiqi Yang",
      "Muhammad Atif Butt",
      "Joost van de Weijer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/535915d26859036410b0533804cee788-Abstract-Conference.html": {
    "title": "Brant: Foundation Model for Intracranial Neural Signal",
    "volume": "main",
    "abstract": "We propose a foundation model named Brant for modeling intracranial recordings, which learns powerful representations of intracranial neural signals by pre-training, providing a large-scale, off-the-shelf model for medicine. Brant is the largest model in the field of brain signals and is pre-trained on a large corpus of intracranial data collected by us. The design of Brant is to capture long-term temporal dependency and spatial correlation from neural signals, combining the information in both time and frequency domains. As a foundation model, Brant achieves SOTA performance on various downstream tasks (i.e. neural signal forecasting, frequency-phase forecasting, imputation and seizure detection), showing the generalization ability to a broad range of tasks. The low-resource label analysis and representation visualization further illustrate the effectiveness of our pre-training strategy. In addition, we explore the effect of model size to show that a larger model with a higher capacity can lead to performance improvements on our dataset. The source code and pre-trained weights are available at: https://zju-brainnet.github.io/Brant.github.io/",
    "keywords": [],
    "checked": true,
    "id": "fdbd1c11a0cd390309ad147d0273b5d8721def12",
    "semantic_title": "brant: foundation model for intracranial neural signal",
    "citation_count": 1,
    "authors": [
      "Daoze Zhang",
      "Zhizhang Yuan",
      "YANG YANG",
      "Junru Chen",
      "Jingjing Wang",
      "Yafeng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/53be3798fcc46e68ca0819c29a004652-Abstract-Conference.html": {
    "title": "Wasserstein distributional robustness of neural networks",
    "volume": "main",
    "abstract": "Deep neural networks are known to be vulnerable to adversarial attacks (AA). For an image recognition task, this means that a small perturbation of the original can result in the image being misclassified. Design of such attacks as well as methods of adversarial training against them are subject of intense research. We re-cast the problem using techniques of Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. We consider a set of distributional threat models. Unlike the traditional pointwise attacks, which assume a uniform bound on perturbation of each input data point, distributional threat models allow attackers to perturb inputs in a non-uniform way. We link these more general attacks with questions of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, we propose a first-order AA algorithm and its multistep version. Our attack algorithms include Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) as special cases. Furthermore, we provide a new asymptotic estimate of the adversarial accuracy against distributional threat models. The bound is fast to compute and first-order accurate, offering new insights even for the pointwise AA. It also naturally yields out-of-sample performance guarantees. We conduct numerical experiments on CIFAR-10, CIFAR-100, ImageNet datasets using DNNs on RobustBench to illustrate our theoretical results. Our code is available at https://github.com/JanObloj/W-DRO-Adversarial-Methods",
    "keywords": [],
    "checked": true,
    "id": "63c24262fc0741179d9d4e002d8eebbb764cc3d5",
    "semantic_title": "wasserstein distributional robustness of neural networks",
    "citation_count": 1,
    "authors": [
      "Xingjian Bai",
      "Guangyi He",
      "Yifan Jiang",
      "Jan Obloj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/53dd219b6b11abc8ce523921c18c7a3e-Abstract-Conference.html": {
    "title": "High dimensional, tabular deep learning with an auxiliary knowledge graph",
    "volume": "main",
    "abstract": "Machine learning models exhibit strong performance on datasets with abundant labeled samples. However, for tabular datasets with extremely high $d$-dimensional features but limited $n$ samples (i.e. $d \\gg n$), machine learning models struggle to achieve strong performance due to the risk of overfitting. Here, our key insight is that there is often abundant, auxiliary domain information describing input features which can be structured as a heterogeneous knowledge graph (KG). We propose PLATO, a method that achieves strong performance on tabular data with $d \\gg n$ by using an auxiliary KG describing input features to regularize a multilayer perceptron (MLP). In PLATO, each input feature corresponds to a node in the auxiliary KG. In the MLP's first layer, each input feature also corresponds to a weight vector. PLATO is based on the inductive bias that two input features corresponding to similar nodes in the auxiliary KG should have similar weight vectors in the MLP's first layer. PLATO captures this inductive bias by inferring the weight vector for each input feature from its corresponding node in the KG via a trainable message-passing function. Across 6 $d \\gg n$ datasets, PLATO outperforms 13 state-of-the-art baselines by up to 10.19%",
    "keywords": [],
    "checked": false,
    "id": "c779710db42fa7dc0ebc6bf98a5d019cb9e8737c",
    "semantic_title": "enabling tabular deep learning when d ≫ n with an auxiliary knowledge graph",
    "citation_count": 1,
    "authors": [
      "Camilo Ruiz",
      "Hongyu Ren",
      "Kexin Huang",
      "Jure Leskovec"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/53e9b4152ca09d5f1228157e752651dd-Abstract-Conference.html": {
    "title": "Learning Space-Time Continuous Latent Neural PDEs from Partially Observed States",
    "volume": "main",
    "abstract": "We introduce a novel grid-independent model for learning partial differential equations (PDEs) from noisy and partial observations on irregular spatiotemporal grids. We propose a space-time continuous latent neural PDE model with an efficient probabilistic framework and a novel encoder design for improved data efficiency and grid independence. The latent state dynamics are governed by a PDE model that combines the collocation method and the method of lines. We employ amortized variational inference for approximate posterior estimation and utilize a multiple shooting technique for enhanced training speed and stability. Our model demonstrates state-of-the-art performance on complex synthetic and real-world datasets, overcoming limitations of previous approaches and effectively handling partially-observed data. The proposed model outperforms recent methods, showing its potential to advance data-driven PDE modeling and enabling robust, grid-independent modeling of complex partially-observed dynamic processes across various domains",
    "keywords": [],
    "checked": false,
    "id": "18fc14bf5e659e56f4c5cc3ccd593462dd28c383",
    "semantic_title": "learning space-time continuous neural pdes from partially observed states",
    "citation_count": 0,
    "authors": [
      "Valerii Iakovlev",
      "Markus Heinonen",
      "Harri Lähdesmäki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/540eb9e0ee35d525231c3fd22d1dcbf2-Abstract-Conference.html": {
    "title": "Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction",
    "volume": "main",
    "abstract": "The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, two issues remain unsolved in this line of work. First, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al.), this approach results in slower convergence rates under interpolation. Second, intuitive line-search methods equipped with variance-reduction (VR) fail to converge (Dubois-Taine et al.). So far, no VR methods successfully accelerate these two stepsizes with a convergence guarantee.In this work, we make two contributions:Firstly, we propose two new robust variants of SPS and SLS, called AdaSPS and AdaSLS, which achieve optimal asymptotic rates in both strongly-convex or convex and interpolation or non-interpolation settings, except for the case when we have both strong convexity and non-interpolation. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value as input. Secondly, we propose a novel VR method that can use Polyak stepsizes or line-search to achieve acceleration. When it is equipped with AdaSPS or AdaSLS, the resulting algorithms obtain the optimal ratefor optimizing convex smooth functions. Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms",
    "keywords": [],
    "checked": true,
    "id": "d1a06508395919e9f7d8064dd478e4ea8721142b",
    "semantic_title": "adaptive sgd with polyak stepsize and line-search: robust convergence and variance reduction",
    "citation_count": 5,
    "authors": [
      "Xiaowen Jiang",
      "Sebastian U. Stich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/542c14ff4622e45384df40dc97b9cf90-Abstract-Conference.html": {
    "title": "SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "This paper studies referring video object segmentation (RVOS) by boosting video-level visual-linguistic alignment. Recent approaches model the RVOS task as a sequence prediction problem and perform multi-modal interaction as well as segmentation for each frame separately. However, the lack of a global view of video content leads to difficulties in effectively utilizing inter-frame relationships and understanding textual descriptions of object temporal variations. To address this issue, we propose Semantic-assisted Object Cluster (SOC), which aggregates video content and textual guidance for unified temporal modeling and cross-modal alignment. By associating a group of frame-level object embeddings with language tokens, SOC facilitates joint space learning across modalities and time steps. Moreover, we present multi-modal contrastive supervision to help construct well-aligned joint space at the video level. We conduct extensive experiments on popular RVOS benchmarks, and our method outperforms state-of-the-art competitors on all benchmarks by a remarkable margin. Besides, the emphasis on temporal coherence enhances the segmentation stability and adaptability of our method in processing text expressions with temporal variations. Code is available at https://github.com/RobertLuo1/NeurIPS2023_SOC",
    "keywords": [],
    "checked": true,
    "id": "9fc5dddf5a5431629e00d35a68dc583226352edb",
    "semantic_title": "soc: semantic-assisted object cluster for referring video object segmentation",
    "citation_count": 3,
    "authors": [
      "Zhuoyan Luo",
      "Yicheng Xiao",
      "Yong Liu",
      "Shuyan Li",
      "Yitong Wang",
      "Yansong Tang",
      "Xiu Li",
      "Yujiu Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5433b79562b9fa85bd5da0c95a78c907-Abstract-Conference.html": {
    "title": "Posthoc privacy guarantees for collaborative inference with modified Propose-Test-Release",
    "volume": "main",
    "abstract": "Cloud-based machine learning inference is an emerging paradigm where users query by sending their data through a service provider who runs an ML model on that data and returns back the answer. Due to increased concerns over data privacy, recent works have proposed Collaborative Inference (CI) to learn a privacy-preserving encoding of sensitive user data before it is shared with an untrusted service provider. Existing works so far evaluate the privacy of these encodings through empirical reconstruction attacks. In this work, we develop a new framework that provides formal privacy guarantees for an arbitrarily trained neural network by linking its local Lipschitz constant with its local sensitivity. To guarantee privacy using local sensitivity, we extend the Propose-Test-Release (PTR) framework to make it tractable for neural network queries. We verify the efficacy of our framework experimentally on real-world datasets and elucidate the role of Adversarial Representation Learning (ARL) in improving the privacy-utility trade-off",
    "keywords": [],
    "checked": true,
    "id": "b35359847414451137a0352deed729807f6b1706",
    "semantic_title": "posthoc privacy guarantees for collaborative inference with modified propose-test-release",
    "citation_count": 0,
    "authors": [
      "Abhishek Singh",
      "Praneeth Vepakomma",
      "Vivek Sharma",
      "Ramesh Raskar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/543924fdf260ba990f2ef84f940f3db2-Abstract-Conference.html": {
    "title": "Strategic Classification under Unknown Personalized Manipulation",
    "volume": "main",
    "abstract": "We study the fundamental mistake bound and sample complexity in the strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive. For example, given a classifier determining college admission, student candidates may try to take easier classes to improve their GPA, retake SAT and change schools in an effort to fool the classifier. *Ball manipulations* are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. Unlike most prior work, our work consider manipulations to be *personalized*, meaning that agents can have different levels of manipulation abilities (e.g., varying radii for ball manipulations), and *unknown* to the learner.We formalize the learning problem in an interaction model where the learner first deploys a classifier and the agent manipulates the feature vector within their manipulation set to game the deployed classifier. We investigate various scenarios in terms of the information available to the learner during the interaction, such as observing the original feature vector before or after deployment, observing the manipulated feature vector, or not seeing either the original or the manipulated feature vector. We begin by providing online mistake bounds and PAC sample complexity in these scenarios for ball manipulations. We also explore non-ball manipulations and show that, even in the simplest scenario where both the original and the manipulated feature vectors are revealed, the mistake bounds and sample complexity are lower bounded by $\\Omega(|\\mathcal H|)$ when the target function belongs to a known class $\\mathcal H$",
    "keywords": [],
    "checked": true,
    "id": "af269ebec60237ecf1420055f10d74255fd9cda3",
    "semantic_title": "strategic classification under unknown personalized manipulation",
    "citation_count": 2,
    "authors": [
      "Han Shao",
      "Avrim Blum",
      "Omar Montasser"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/543ec10715d964122ab7cb15f648772b-Abstract-Conference.html": {
    "title": "On the Ability of Graph Neural Networks to Model Interactions Between Vertices",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as separation rank, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's walk index --- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named Walk Index Sparsification (WIS), which preserves the ability of a GNN to model interactions when input edges are removed. WIS is simple, computationally efficient, and in our experiments has markedly outperformed alternative methods in terms of induced prediction accuracy. More broadly, it showcases the potential of improving GNNs by theoretically analyzing the interactions they can model",
    "keywords": [],
    "checked": true,
    "id": "509cd4f647c14196f0b7fa426b5f7875534250e6",
    "semantic_title": "on the ability of graph neural networks to model interactions between vertices",
    "citation_count": 3,
    "authors": [
      "Noam Razin",
      "Tom Verbin",
      "Nadav Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/544242770e8333875325d013328b2079-Abstract-Conference.html": {
    "title": "Learning from Both Structural and Textual Knowledge for Inductive Knowledge Graph Completion",
    "volume": "main",
    "abstract": "Learning rule-based systems plays a pivotal role in knowledge graph completion (KGC). Existing rule-based systems restrict the input of the system to structural knowledge only, which may omit some useful knowledge for reasoning, e.g., textual knowledge. In this paper, we propose a two-stage framework that imposes both structural and textual knowledge to learn rule-based systems. In the first stage, we compute a set of triples with confidence scores (called \\emph{soft triples}) from a text corpus by distant supervision, where a textual entailment model with multi-instance learning is exploited to estimate whether a given triple is entailed by a set of sentences. In the second stage, these soft triples are used to learn a rule-based model for KGC. To mitigate the negative impact of noise from soft triples, we propose a new formalism for rules to be learnt, named \\emph{text enhanced rules} or \\emph{TE-rules} for short. To effectively learn TE-rules, we propose a neural model that simulates the inference of TE-rules. We theoretically show that any set of TE-rules can always be interpreted by a certain parameter assignment of the neural model. We introduce three new datasets to evaluate the effectiveness of our method. Experimental results demonstrate that the introduction of soft triples and TE-rules results in significant performance improvements in inductive link prediction",
    "keywords": [],
    "checked": false,
    "id": "8aa60d7802659dd18c80a3411a905599fc431226",
    "semantic_title": "unifying structure and language semantic for efficient contrastive knowledge graph completion with structured entity anchors",
    "citation_count": 0,
    "authors": [
      "Kunxun Qi",
      "Jianfeng Du",
      "Hai Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/544696ef4847c903376ed6ec58f3a703-Abstract-Conference.html": {
    "title": "Sorting with Predictions",
    "volume": "main",
    "abstract": "We explore the fundamental problem of sorting through the lens of learning-augmented algorithms, where algorithms can leverage possibly erroneous predictions to improve their efficiency. We consider two different settings: In the first setting, each item is provided a prediction of its position in the sorted list. In the second setting, we assume there is a ``quick-and-dirty'' way of comparing items, in addition to slow-and-exact comparisons. For both settings, we design new and simple algorithms using only $O(\\sum_i \\log \\eta_i)$ exact comparisons, where $\\eta_i$ is a suitably defined prediction error for the $i$th element. In particular, as the quality of predictions deteriorates, the number of comparisons degrades smoothly from $O(n)$ to $O(n\\log n)$. We prove that this comparison complexity is theoretically optimal with respect to the examined error measures. An experimental evaluation against existing adaptive and non-adaptive sorting algorithms demonstrates the potential of applying learning-augmented algorithms in sorting tasks",
    "keywords": [],
    "checked": true,
    "id": "15f8b247570b9381aa2b38c4360d02ae81834517",
    "semantic_title": "sorting with predictions",
    "citation_count": 3,
    "authors": [
      "Xingjian Bai",
      "Christian Coester"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/545a674417b8c4bcae96eceffad1c4f0-Abstract-Conference.html": {
    "title": "Posterior Sampling for Competitive RL: Function Approximation and Partial Observation",
    "volume": "main",
    "abstract": "This paper investigates posterior sampling algorithms for competitive reinforcement learning (RL) in the context of general function approximations. Focusing on zero-sum Markov games (MGs) under two critical settings, namely self-play and adversarial learning, we first propose the self-play and adversarial generalized eluder coefficient (GEC) as complexity measures for function approximation, capturing the exploration-exploitation trade-off in MGs. Based on self-play GEC, we propose a model-based self-play posterior sampling method to control both players to learn Nash equilibrium, which can successfully handle the partial observability of states. Furthermore, we identify a set of partially observable MG models fitting MG learning with the adversarial policies of the opponent. Incorporating the adversarial GEC, we propose a model-based posterior sampling method for learning adversarial MG with potential partial observability. We further provide low regret bounds for proposed algorithms that can scale sublinearly with the proposed GEC and the number of episodes $T$. To the best of our knowledge, we for the first time develop generic model-based posterior sampling algorithms for competitive RL that can be applied to a majority of tractable zero-sum MG classes in both fully observable and partially observable MGs with self-play and adversarial learning",
    "keywords": [],
    "checked": true,
    "id": "b24ba4c315e82c080357f20e1c578ad66e9dfa2c",
    "semantic_title": "posterior sampling for competitive rl: function approximation and partial observation",
    "citation_count": 0,
    "authors": [
      "Shuang Qiu",
      "Ziyu Dai",
      "Han Zhong",
      "Zhaoran Wang",
      "Zhuoran Yang",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54801e196796134a2b0ae5e8adef502f-Abstract-Conference.html": {
    "title": "Towards Test-Time Refusals via Concept Negation",
    "volume": "main",
    "abstract": "Generative models produce unbounded outputs, necessitating the use of refusal techniques to confine their output space. Employing generative refusals is crucial in upholding the ethical and copyright integrity of synthesized content, particularly when working with widely adopted diffusion models. \"Concept negation'' presents a promising paradigm to achieve generative refusals, as it effectively defines and governs the model's output space based on concepts, utilizing natural language interfaces that are readily comprehensible to humans. However, despite the valuable contributions of prior research to the field of concept negation, it still suffers from significant limitations. The existing concept negation methods, which operate based on the composition of score or noise predictions from the diffusion process, are limited to independent concepts (e.g., ``a blonde girl`` without ``glasses``) and fail to consider the interconnected nature of concepts in reality (e.g., ``Mickey mouse eats ice cream`` without ``Disney characters``). Keeping the limitations in mind, we propose a novel framework, called $ProtoRe$, to improve the flexibility of concept negation via test-time negative concept identification along with purification in the feature space. $ProtoRe$ works by incorporating CLIP's language-contrastive knowledge to identify the prototype of negative concepts, extract the negative features from outputs using the prototype as a prompt, and further refine the attention maps by retrieving negative features. Our evaluation on multiple benchmarks shows that $ProtoRe$ outperforms state-of-the-art methods under various settings, in terms of the effectiveness of purification and the fidelity of generative images",
    "keywords": [],
    "checked": false,
    "id": "85110935299ae6888bea78ed90a2001669d6cee7",
    "semantic_title": "towards a smart and sustainable industry : cycle time optimization",
    "citation_count": 1,
    "authors": [
      "Peiran Dong",
      "Song Guo",
      "Junxiao Wang",
      "Bingjie WANG",
      "Jiewei Zhang",
      "Ziming Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5491280797f3192b895bce84eb83df8d-Abstract-Conference.html": {
    "title": "Likelihood Ratio Confidence Sets for Sequential Decision Making",
    "volume": "main",
    "abstract": "Certifiable, adaptive uncertainty estimates for unknown quantities are an essential ingredient of sequential decision-making algorithms. Standard approaches rely on problem-dependent concentration results and are limited to a specific combination of parameterization, noise family, and estimator. In this paper, we revisit the likelihood-based inference principle and propose to use \\emph{likelihood ratios} to construct \\emph{any-time valid} confidence sequences without requiring specialized treatment in each application scenario. Our method is especially suitable for problems with well-specified likelihoods, and the resulting sets always maintain the prescribed coverage in a model-agnostic manner. The size of the sets depends on a choice of estimator sequence in the likelihood ratio. We discuss how to provably choose the best sequence of estimators and shed light on connections to online convex optimization with algorithms such as Follow-the-Regularized-Leader. To counteract the initially large bias of the estimators, we propose a reweighting scheme that also opens up deployment in non-parametric settings such as RKHS function classes. We provide a \\emph{non-asymptotic} analysis of the likelihood ratio confidence sets size for generalized linear models, using insights from convex duality and online learning. We showcase the practical strength of our method on generalized linear bandit problems, survival analysis, and bandits with various additive noise distributions",
    "keywords": [],
    "checked": true,
    "id": "29d9ce0650ba6614207affcbb6329d7d4777b8d0",
    "semantic_title": "likelihood ratio confidence sets for sequential decision making",
    "citation_count": 1,
    "authors": [
      "Nicolas Emmenegger",
      "Mojmir Mutny",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54a1495b06c4ee2f07184afb9a37abda-Abstract-Conference.html": {
    "title": "Uncertainty Quantification over Graph with Conformalized Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) are powerful machine learning prediction models on graph-structured data. However, GNNs lack rigorous uncertainty estimates, limiting their reliable deployment in settings where the cost of errors is significant. We propose conformalized GNN (CF-GNN), extending conformal prediction (CP) to graph-based models for guaranteed uncertainty estimates. Given an entity in the graph, CF-GNN produces a prediction set/interval that provably contains the true label with pre-defined coverage probability (e.g. 90%). We establish a permutation invariance condition that enables the validity of CP on graph data and provide an exact characterization of the test-time coverage. Moreover, besides valid coverage, it is crucial to reduce the prediction set size/interval length for practical use. We observe a key connection between non-conformity scores and network structures, which motivates us to develop a topology-aware output correction model that learns to update the prediction and produces more efficient prediction sets/intervals. Extensive experiments show that CF-GNN achieves any pre-defined target marginal coverage while significantly reducing the prediction set/interval size by up to 74% over the baselines. It also empirically achieves satisfactory conditional coverage over various raw and network features",
    "keywords": [],
    "checked": true,
    "id": "569140ad11310f71c5fcc0ecaa6810d12bee3416",
    "semantic_title": "uncertainty quantification over graph with conformalized graph neural networks",
    "citation_count": 3,
    "authors": [
      "Kexin Huang",
      "Ying Jin",
      "Emmanuel Candes",
      "Jure Leskovec"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54d2d38a56a74387d5916ee40e462295-Abstract-Conference.html": {
    "title": "VPP: Efficient Conditional 3D Generation via Voxel-Point Progressive Representation",
    "volume": "main",
    "abstract": "Conditional 3D generation is undergoing a significant advancement, enabling the free creation of 3D content from inputs such as text or 2D images. However, previous approaches have suffered from low inference efficiency, limited generation categories, and restricted downstream applications. In this work, we revisit the impact of different 3D representations on generation quality and efficiency. We propose a progressive generation method through Voxel-Point Progressive Representation (VPP). VPP leverages structured voxel representation in the proposed Voxel Semantic Generator and the sparsity of unstructured point representation in the Point Upsampler, enabling efficient generation of multi-category objects. VPP can generate high-quality 8K point clouds within 0.2 seconds. Additionally, the masked generation Transformer allows for various 3D downstream tasks, such as generation, editing, completion, and pre-training. Extensive experiments demonstrate that VPP efficiently generates high-fidelity and diverse 3D shapes across different categories, while also exhibiting excellent representation transfer performance. Codes will be released at https://github.com/qizekun/VPP",
    "keywords": [],
    "checked": true,
    "id": "7da6bec621f443d33ba1ef7625cd8abb57d35629",
    "semantic_title": "vpp: efficient conditional 3d generation via voxel-point progressive representation",
    "citation_count": 3,
    "authors": [
      "Zekun Qi",
      "Muzhou Yu",
      "Runpei Dong",
      "Kaisheng Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54d8aab579b5a9ed3395764c7341ebec-Abstract-Conference.html": {
    "title": "Multi Time Scale World Models",
    "volume": "main",
    "abstract": "Intelligent agents use internal world models to reason and make predictions about different courses of their actions at many scales. Devising learning paradigms and architectures that allow machines to learn world models that operate at multiple levels of temporal abstractions while dealing with complex uncertainty predictions is a major technical hurdle. In this work, we propose a probabilistic formalism to learn multi-time scale world models which we call the Multi Time Scale State Space (MTS3) model. Our model uses a computationally efficient inference scheme on multiple time scales for highly accurate long-horizon predictions and uncertainty estimates over several seconds into the future. Our experiments, which focus on action conditional long horizon future predictions, show that MTS3 outperforms recent methods on several system identification benchmarks including complex simulated and real-world dynamical systems",
    "keywords": [],
    "checked": true,
    "id": "f6dc0af5a7d38e8cb8adb944d8ecfbddc2ba77b1",
    "semantic_title": "multi time scale world models",
    "citation_count": 0,
    "authors": [
      "Vaisakh Shaj Kumar",
      "SALEH GHOLAM ZADEH",
      "Ozan Demir",
      "Luiz Douat",
      "Gerhard Neumann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54dcf25318f9de5a7a01f0a4125c541e-Abstract-Conference.html": {
    "title": "How many samples are needed to leverage smoothness?",
    "volume": "main",
    "abstract": "A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function seems to require enough samples close to one another to get meaningful estimate of high-order derivatives, which would be hard in machine learning problems where the ratio between number of data and input dimension is relatively small. By deriving new lower bounds on the generalization error, this paper formalizes such an intuition, before investigating the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while they play a dominant role in practice",
    "keywords": [],
    "checked": true,
    "id": "6ce644bed0e2465ffe372d403779c630f5c9f689",
    "semantic_title": "how many samples are needed to leverage smoothness?",
    "citation_count": 2,
    "authors": [
      "Vivien Cabannes",
      "Stefano Vigogna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54e13b23fa2f399cea6e67acf9063c40-Abstract-Conference.html": {
    "title": "Causal Imitability Under Context-Specific Independence Relations",
    "volume": "main",
    "abstract": "Drawbacks of ignoring the causal mechanisms when performing imitation learning have recently been acknowledged. Several approaches both to assess the feasibility of imitation and to circumvent causal confounding and causal misspecifications have been proposed in the literature.However, the potential benefits of the incorporation of additional information about the underlying causal structure are left unexplored.An example of such overlooked information is context-specific independence (CSI), i.e., independence that holds only in certain contexts.We consider the problem of causal imitation learning when CSI relations are known.We prove that the decision problem pertaining to the feasibility of imitation in this setting is NP-hard.Further, we provide a necessary graphical criterion for imitation learning under CSI and show that under a structural assumption, this criterion is also sufficient.Finally, we propose a sound algorithmic approach for causal imitation learning which takes both CSI relations and data into account",
    "keywords": [],
    "checked": true,
    "id": "00fe94a6d7097b860a33ec0c1c85eba2e2e74f14",
    "semantic_title": "causal imitability under context-specific independence relations",
    "citation_count": 0,
    "authors": [
      "Fateme Jamshidi",
      "Sina Akbari",
      "Negar Kiyavash"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54e5d7af6250ccab796ad7fe75663ba5-Abstract-Conference.html": {
    "title": "A Finite-Particle Convergence Rate for Stein Variational Gradient Descent",
    "volume": "main",
    "abstract": "We provide the first finite-particle convergence rate for Stein variational gradient descent (SVGD), a popular algorithm for approximating a probability distribution with a collection of particles. Specifically, whenever the target distribution is sub-Gaussian with a Lipschitz score, SVGD with $n$ particles and an appropriate step size sequence drives the kernel Stein discrepancy to zero at an order ${1/}{\\sqrt{\\log\\log n}}$ rate. We suspect that the dependence on $n$ can be improved, and we hope that our explicit, non-asymptotic proof strategy will serve as a template for future refinements",
    "keywords": [],
    "checked": true,
    "id": "a7204bde93700156e9c828758425736cd203f36d",
    "semantic_title": "a finite-particle convergence rate for stein variational gradient descent",
    "citation_count": 9,
    "authors": [
      "Jiaxin Shi",
      "Lester Mackey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54f7125dee9b8b3dc798bb9a082b09e2-Abstract-Conference.html": {
    "title": "Bringing regularized optimal transport to lightspeed: a splitting method adapted for GPUs",
    "volume": "main",
    "abstract": "We present an efficient algorithm for regularized optimal transport. In contrast toprevious methods, we use the Douglas-Rachford splitting technique to developan efficient solver that can handle a broad class of regularizers. The algorithmhas strong global convergence guarantees, low per-iteration cost, and can exploitGPU parallelization, making it considerably faster than the state-of-the-art formany problems. We illustrate its competitiveness in several applications, includingdomain adaptation and learning of generative models",
    "keywords": [],
    "checked": true,
    "id": "7d5320b30a2610c47d79fcb405cd86aaca872dfa",
    "semantic_title": "bringing regularized optimal transport to lightspeed: a splitting method adapted for gpus",
    "citation_count": 1,
    "authors": [
      "Jacob Lindbäck",
      "Zesen Wang",
      "Mikael Johansson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54f82cdae821aad5c2888d61a6515170-Abstract-Conference.html": {
    "title": "Use perturbations when learning from explanations",
    "volume": "main",
    "abstract": "Machine learning from explanations (MLX) is an approach to learning that uses human-provided explanations of relevant or irrelevant features for each input to ensure that model predictions are right for the right reasons. Existing MLX approaches rely on local model interpretation methods and require strong model smoothing to align model and human explanations, leading to sub-optimal performance. We recast MLX as a robustness problem, where human explanations specify a lower dimensional manifold from which perturbations can be drawn, and show both theoretically and empirically how this approach alleviates the need for strong model smoothing. We consider various approaches to achieving robustness, leading to improved performance over prior MLX methods. Finally, we show how to combine robustness with an earlier MLX method, yielding state-of-the-art results on both synthetic and real-world benchmarks",
    "keywords": [],
    "checked": true,
    "id": "0fa49a6a6ae4935218d5ca831d3c4c7a2ce8210b",
    "semantic_title": "use perturbations when learning from explanations",
    "citation_count": 0,
    "authors": [
      "Juyeon Heo",
      "Vihari Piratla",
      "Matthew Wicker",
      "Adrian Weller"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/550ab405d0addd3de5b70e57b44878df-Abstract-Conference.html": {
    "title": "The Memory-Perturbation Equation: Understanding Model's Sensitivity to Data",
    "volume": "main",
    "abstract": "Understanding model's sensitivity to its training data is crucial but can also be challenging and costly, especially during training. To simplify such issues, we present the Memory-Perturbation Equation (MPE) which relates model's sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data. The proposed equation is expected to be useful for future research on robust and adaptive learning",
    "keywords": [],
    "checked": false,
    "id": "12cce6a8af34ff35286b92959f1cd49e38b600bf",
    "semantic_title": "the memory perturbation equation: understanding model's sensitivity to data",
    "citation_count": 0,
    "authors": [
      "Peter Nickl",
      "Lu Xu",
      "Dharmesh Tailor",
      "Thomas Möllenhoff",
      "Mohammad Emtiyaz E. Khan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5526c73e3ff4f2a34009e13d15f52fcb-Abstract-Conference.html": {
    "title": "Contextual Gaussian Process Bandits with Neural Networks",
    "volume": "main",
    "abstract": "Contextual decision-making problems have witnessed extensive applications in various fields such as online content recommendation, personalized healthcare, and autonomous vehicles, where a core practical challenge is to select a suitable surrogate model for capturing unknown complicated reward functions. It is often the case that both high approximation accuracy and explicit uncertainty quantification are desired. In this work, we propose a neural network-accompanied Gaussian process (NN-AGP) model, which leverages neural networks to approximate the unknown and potentially complicated reward function regarding the contextual variable, and maintains a Gaussian process surrogate model with respect to the decision variable. Our model is shown to outperform existing approaches by offering better approximation accuracy thanks to the use of neural networks and possessing explicit uncertainty quantification from the Gaussian process. We also analyze the maximum information gain of the NN-AGP model and prove the regret bounds for the corresponding algorithms. Moreover, we conduct the experiments on both synthetic and practical problems, illustrating the effectiveness of our approach",
    "keywords": [],
    "checked": false,
    "id": "474b95ebceb8ccdcb982a2a9377deee2ec8cead6",
    "semantic_title": "combinatorial gaussian process bandits in bayesian settings: theory and application for energy-efficient navigation",
    "citation_count": 0,
    "authors": [
      "Haoting Zhang",
      "Jinghai He",
      "Rhonda Righter",
      "Zuo-Jun Shen",
      "Zeyu Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/555479a201da27c97aaeed842d16ca49-Abstract-Conference.html": {
    "title": "Auxiliary Losses for Learning Generalizable Concept-based Models",
    "volume": "main",
    "abstract": "The increasing use of neural networks in various applications has lead to increasing apprehensions, underscoring the necessity to understand their operations beyond mere final predictions. As a solution to enhance model transparency, Concept Bottleneck Models (CBMs) have gained popularity since their introduction. CBMs essentially limit the latent space of a model to human-understandable high-level concepts. While beneficial, CBMs have been reported to often learn irrelevant concept representations that consecutively damage model performance. To overcome the performance trade-off, we propose a cooperative-Concept Bottleneck Model (coop-CBM). The concept representation of our model is particularly meaningful when fine-grained concept labels are absent. Furthermore, we introduce the concept orthogonal loss (COL) to encourage the separation between the concept representations and to reduce the intra-concept distance. This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL. We also study the performance of coop-CBM models under various distributional shift settings. We show that our proposed method achieves higher accuracy in all distributional shift settings even compared to the black-box models with the highest concept accuracy",
    "keywords": [],
    "checked": true,
    "id": "c2f929c9c8d5caa3ac12fd2f7b134c3c1bc646c0",
    "semantic_title": "auxiliary losses for learning generalizable concept-based models",
    "citation_count": 2,
    "authors": [
      "Ivaxi Sheth",
      "Samira Ebrahimi Kahou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5555cc3fb226ed067fa946e35355f938-Abstract-Conference.html": {
    "title": "Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "Domain Adaptation (DA) is always challenged by the spurious correlation between the domain-invariant features (e.g., class identity) and the domain-specific ones (e.g., environment) that does not generalize to the target domain. Unfortunately, even enriched with additional unsupervised target domains, existing Unsupervised DA (UDA) methods still suffer from it. This is because the source domain supervision only considers the target domain samples as auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the target domain---where the valuable de-correlation clues hide---is disregarded. We propose to make the U in UDA matter by giving equal status to the two domains. Specifically, we learn an invariant classifier whose prediction is simultaneously consistent with the labels in the source domain and clusters in the target domain, hence the spurious correlation inconsistent in the target domain is removed. We dub our approach \"Invariant CONsistency learning\" (ICON). Extensive experiments show that ICON achieves the state-of-the-art performance on the classic UDA benchmarks: Office-Home and VisDA-2017, and outperforms all the conventional methods on the challenging WILDS 2.0 benchmark. Codes are in https://github.com/yue-zhongqi/ICON",
    "keywords": [],
    "checked": true,
    "id": "31496aa700dc812572fc42ce0fffb5a365dc739a",
    "semantic_title": "make the u in uda matter: invariant consistency learning for unsupervised domain adaptation",
    "citation_count": 0,
    "authors": [
      "Zhongqi Yue",
      "QIANRU SUN",
      "Hanwang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/558a100caa93422df215fadb9e9b1dd7-Abstract-Conference.html": {
    "title": "Hyper-HMM: aligning human brains and semantic features in a common latent event space",
    "volume": "main",
    "abstract": "Naturalistic stimuli evoke complex neural responses with spatial and temporal properties that differ across individuals. Current alignment methods focus on either spatial hyperalignment (assuming exact temporal correspondence) or temporal alignment (assuming exact spatial correspondence). Here, we propose a hybrid model, the Hyper-HMM, that simultaneously aligns both temporal and spatial features across brains. The model learns to linearly project voxels to a reduced-dimension latent space, in which timecourses are segmented into corresponding temporal events. This approach allows tracking of each individual's mental trajectory through an event sequence, and also allows for alignment with other feature spaces such as stimulus content. Using an fMRI dataset in which students watch videos of class lectures, we demonstrate that the Hyper-HMM can be used to map all participants and the semantic content of the videos into a common low-dimensional space, and that these mappings generalize to held-out data. Our model provides a new window into individual cognitive dynamics evoked by complex naturalistic stimuli",
    "keywords": [],
    "checked": true,
    "id": "dcf55e24173614b50ca359c58cada4af16a643d8",
    "semantic_title": "hyper-hmm: aligning human brains and semantic features in a common latent event space",
    "citation_count": 0,
    "authors": [
      "Caroline Lee",
      "Jane Han",
      "Ma Feilong",
      "Guo Jiahui",
      "James Haxby",
      "Christopher Baldassano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/559a0998fab1d19b80e7e43a5852401c-Abstract-Conference.html": {
    "title": "Active Learning for Semantic Segmentation with Multi-class Label Query",
    "volume": "main",
    "abstract": "This paper proposes a new active learning method for semantic segmentation. The core of our method lies in a new annotation query design. It samples informative local image regions ($\\textit{e.g.}$, superpixels), and for each of such regions, asks an oracle for a multi-hot vector indicating all classes existing in the region. This multi-class labeling strategy is substantially more efficient than existing ones like segmentation, polygon, and even dominant class labeling in terms of annotation time per click. However, it introduces the class ambiguity issue in training as it assigns partial labels ($\\textit{i.e.}$, a set of candidate classes) to individual pixels. We thus propose a new algorithm for learning semantic segmentation while disambiguating the partial labels in two stages. In the first stage, it trains a segmentation model directly with the partial labels through two new loss functions motivated by partial label learning and multiple instance learning. In the second stage, it disambiguates the partial labels by generating pixel-wise pseudo labels, which are used for supervised learning of the model. Equipped with a new acquisition function dedicated to the multi-class labeling, our method outperforms previous work on Cityscapes and PASCAL VOC 2012 while spending less annotation cost. Our code and results are available at [https://github.com/sehyun03/MulActSeg](https://github.com/sehyun03/MulActSeg)",
    "keywords": [],
    "checked": true,
    "id": "1cd472f40342576b1301d870f712fdf5189f81cd",
    "semantic_title": "active learning for semantic segmentation with multi-class label query",
    "citation_count": 0,
    "authors": [
      "Sehyun Hwang",
      "Sohyun Lee",
      "Hoyoung Kim",
      "Minhyeon Oh",
      "Jungseul Ok",
      "Suha Kwak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/55a49718689fdecef31b6a2386df6fe1-Abstract-Conference.html": {
    "title": "Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions",
    "volume": "main",
    "abstract": "Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate that state-of-the-art fairness interventions are effective at removing epistemic discrimination on standard (overused) tabular datasets. However, when data has missing values, there is still significant room for improvement in handling aleatoric discrimination",
    "keywords": [],
    "checked": true,
    "id": "2a3d82d3af02d8c893397c45d1bf14042dd9d679",
    "semantic_title": "aleatoric and epistemic discrimination: fundamental limits of fairness interventions",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Luxi He",
      "Rui Gao",
      "Flavio Calmon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/55aeba84b402008d3ed10440d906b4e1-Abstract-Conference.html": {
    "title": "DISCOVER: Making Vision Networks Interpretable via Competition and Dissection",
    "volume": "main",
    "abstract": "Modern deep networks are highly complex and their inferential outcome very hard to interpret. This is a serious obstacle to their transparent deployment in safety-critical or bias-aware applications. This work contributes to *post-hoc* interpretability, and specifically Network Dissection. Our goal is to present a framework that makes it easier to *discover* the individual functionality of each neuron in a network trained on a vision task; discovery is performed in terms of textual description generation. To achieve this objective, we leverage: (i) recent advances in multimodal vision-text models and (ii) network layers founded upon the novel concept of stochastic local competition between linear units. In this setting, only a *small subset* of layer neurons are activated *for a given input*, leading to extremely high activation sparsity (as low as only $\\approx 4\\%$). Crucially, our proposed method infers (sparse) neuron activation patterns that enables the neurons to activate/specialize to inputs with specific characteristics, diversifying their individual functionality. This capacity of our method supercharges the potential of dissection processes: human understandable descriptions are generated only for the very few active neurons, thus facilitating the direct investigation of the network's decision process. As we experimentally show, our approach: (i) yields Vision Networks that retain or improve classification performance, and (ii) realizes a principled framework for text-based description and examination of the generated neuronal representations",
    "keywords": [],
    "checked": true,
    "id": "961adb2787708bf530816c3498d90379783d0b97",
    "semantic_title": "discover: making vision networks interpretable via competition and dissection",
    "citation_count": 1,
    "authors": [
      "Konstantinos Panousis",
      "Sotirios Chatzis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/55c518a17bd17dcb69aa14d69d085994-Abstract-Conference.html": {
    "title": "Adapting Neural Link Predictors for Data-Efficient Complex Query Answering",
    "volume": "main",
    "abstract": "Answering complex queries on incomplete knowledge graphs is a challenging task where a model needs to answer complex logical queries in the presence of missing knowledge. Prior work in the literature has proposed to address this problem by designing architectures trained end-to-end for the complex query answering task with a reasoning process that is hard to interpret while requiring data and resource-intensive training. Other lines of research have proposed re-using simple neural link predictors to answer complex queries, reducing the amount of training data by orders of magnitude while providing interpretable answers. The neural link predictor used in such approaches is not explicitly optimised for the complex query answering task, implying that its scores are not calibrated to interact together. We propose to address these problems via CQD$^{\\mathcal{A}}$, a parameter-efficient score \\emph{adaptation} model optimised to re-calibrate neural link prediction scores for the complex query answering task. While the neural link predictor is frozen, the adaptation component -- which only increases the number of model parameters by $0.03\\%$ -- is trained on the downstream complex query answering task. Furthermore, the calibration component enables us to support reasoning over queries that include atomic negations, which was previously impossible with link predictors. In our experiments, CQD$^{\\mathcal{A}}$ produces significantly more accurate results than current state-of-the-art methods, improving from $34.4$ to $35.1$ Mean Reciprocal Rank values averaged across all datasets and query types while using $\\leq 30\\%$ of the available training query types. We further show that CQD$^{\\mathcal{A}}$ is data-efficient, achieving competitive results with only $1\\%$ of the complex training queries and robust in out-of-domain evaluations. Source code and datasets are available at https://github.com/EdinburghNLP/adaptive-cqd",
    "keywords": [],
    "checked": true,
    "id": "d3aa244f42c598d58d055c177b2ca1fdc1a172ad",
    "semantic_title": "adapting neural link predictors for data-efficient complex query answering",
    "citation_count": 3,
    "authors": [
      "Erik Arakelyan",
      "Pasquale Minervini",
      "Daniel Daza",
      "Michael Cochez",
      "Isabelle Augenstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/563d94819f68cb73d6a382809e587b54-Abstract-Conference.html": {
    "title": "$p$-value Adjustment for Monotonous, Unbiased, and Fast Clustering Comparison",
    "volume": "main",
    "abstract": "Popular metrics for clustering comparison, like the Adjusted Rand Index and the Adjusted Mutual Information, are type II biased. The Standardized Mutual Information removes this bias but suffers from counterintuitive non-monotonicity and poor computational efficiency. We introduce the $p$-value adjusted Rand Index ($\\operatorname{PMI}_2$), the first cluster comparison method that is type II unbiased and provably monotonous. The $\\operatorname{PMI}_2$ has fast approximations that outperform the Standardized Mutual information. We demonstrate its unbiased clustering selection, approximation quality, and runtime efficiency on synthetic benchmarks. In experiments on image and social network datasets, we show how the $\\operatorname{PMI}_2$ can help practitioners choose better clustering and community detection algorithms",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Klede",
      "Thomas Altstidl",
      "Dario Zanca",
      "Bjoern Eskofier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5642b9811a9ac5281be1cc84c275f251-Abstract-Conference.html": {
    "title": "On Computing Pairwise Statistics with Local Differential Privacy",
    "volume": "main",
    "abstract": "We study the problem of computing pairwise statistics, i.e., ones of the form $\\binom{n}{2}^{-1} \\sum_{i \\ne j} f(x_i, x_j)$, where $x_i$ denotes the input to the $i$th user, with differential privacy (DP) in the local model. This formulation captures important metrics such as Kendall's $\\tau$ coefficient, Area Under Curve, Gini's mean difference, Gini's entropy, etc. We give several novel and generic algorithms for the problem, leveraging techniques from DP algorithms for linear queries",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Badih Ghazi",
      "Pritish Kamath",
      "Ravi Kumar",
      "Pasin Manurangsi",
      "Adam Sealfon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5647763d4245b23e6a1cb0a8947b38c9-Abstract-Conference.html": {
    "title": "STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning",
    "volume": "main",
    "abstract": "Recently, model-based reinforcement learning algorithms have demonstrated remarkable efficacy in visual input environments. These approaches begin by constructing a parameterized simulation world model of the real environment through self-supervised learning. By leveraging the imagination of the world model, the agent's policy is enhanced without the constraints of sampling from the real environment. The performance of these algorithms heavily relies on the sequence modeling and generation capabilities of the world model. However, constructing a perfectly accurate model of a complex unknown environment is nearly impossible. Discrepancies between the model and reality may cause the agent to pursue virtual goals, resulting in subpar performance in the real environment. Introducing random noise into model-based reinforcement learning has been proven beneficial.In this work, we introduce Stochastic Transformer-based wORld Model (STORM), an efficient world model architecture that combines the strong sequence modeling and generation capabilities of Transformers with the stochastic nature of variational autoencoders. STORM achieves a mean human performance of $126.7\\%$ on the Atari $100$k benchmark, setting a new record among state-of-the-art methods that do not employ lookahead search techniques. Moreover, training an agent with $1.85$ hours of real-time interaction experience on a single NVIDIA GeForce RTX 3090 graphics card requires only $4.3$ hours, showcasing improved efficiency compared to previous methodologies",
    "keywords": [],
    "checked": true,
    "id": "c433c7f0a1c5fe3449274c7234ff5a8fc4f3c3ff",
    "semantic_title": "storm: efficient stochastic transformer based world models for reinforcement learning",
    "citation_count": 2,
    "authors": [
      "Weipu Zhang",
      "Gang Wang",
      "Jian Sun",
      "Yetian Yuan",
      "Gao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/565f995643da6329cec701f26f8579f5-Abstract-Conference.html": {
    "title": "Is Heterogeneity Notorious? Taming Heterogeneity to Handle Test-Time Shift in Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) is an effective machine learning paradigm where multiple clients can train models based on heterogeneous data in a decentralized manner without accessing their private data. However, existing FL systems undergo performance deterioration due to feature-level test-time shifts, which are well investigated in centralized settings but rarely studied in FL. The common non-IID issue in FL usually refers to inter-client heterogeneity during training phase, while the test-time shift refers to the intra-client heterogeneity during test phase. Although the former is always deemed to be notorious for FL, there is still a wealth of useful information delivered by heterogeneous data sources, which may potentially help alleviate the latter issue. To explore the possibility of using inter-client heterogeneity in handling intra-client heterogeneity, we firstly propose a contrastive learning-based FL framework, namely FedICON, to capture invariant knowledge among heterogeneous clients and consistently tune the model to adapt to test data. In FedICON, each client performs sample-wise supervised contrastive learning during the local training phase, which enhances sample-wise invariance encoding ability. Through global aggregation, the invariance extraction ability can be mutually boosted among inter-client heterogeneity. During the test phase, our test-time adaptation procedure leverages unsupervised contrastive learning to guide the model to smoothly generalize to test data under intra-client heterogeneity. Extensive experiments validate the effectiveness of the proposed FedICON in taming heterogeneity to handle test-time shift problems",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Tan",
      "Chen Chen",
      "Weiming Zhuang",
      "Xin Dong",
      "Lingjuan Lyu",
      "Guodong Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/56a225639da77e8f7c0409f6d5ba996b-Abstract-Conference.html": {
    "title": "Self-Predictive Universal AI",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) algorithms typically utilize learning and/or planning techniques to derive effective policies. The integration of both approaches has proven to be highly successful in addressing complex sequential decision-making challenges, as evidenced by algorithms such as AlphaZero and MuZero, which consolidate the planning process into a parametric search-policy. AIXI, the most potent theoretical universal agent, leverages planning through comprehensive search as its primary means to find an optimal policy. Here we define an alternative universal agent, which we call Self-AIXI, that on the contrary to AIXI, maximally exploits learning to obtain good policies. It does so by self-predicting its own stream of action data, which is generated, similarly to other TD(0) agents, by taking an action maximization step over the current on-policy (universal mixture-policy) Q-value estimates. We prove that Self-AIXI converges to AIXI, and inherits a series of properties like maximal Legg-Hutter intelligence and the self-optimizing property",
    "keywords": [],
    "checked": false,
    "id": "5cab50974b97872d2f8e9f4f486f0eb12ebfa67c",
    "semantic_title": "deep language models for interpretative and predictive materials science",
    "citation_count": 26,
    "authors": [
      "Elliot Catt",
      "Jordi Grau-Moya",
      "Marcus Hutter",
      "Matthew Aitchison",
      "Tim Genewein",
      "Grégoire Delétang",
      "Kevin Li",
      "Joel Veness"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/56a7b9a07ae01ddea762dcc51280298b-Abstract-Conference.html": {
    "title": "Addressing Negative Transfer in Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion-based generative models have achieved remarkable success in various domains. It trains a shared model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of \\textit{negative transfer}, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we first aim to analyze diffusion training from an MTL standpoint, presenting two key observations: \\textbf{(O1)} the task affinity between denoising tasks diminishes as the gap between noise levels widens, and \\textbf{(O2)} negative transfer can arise even in diffusion training. Building upon these observations, we aim to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL methods, but the presence of a huge number of denoising tasks makes this computationally expensive to calculate the necessary per-task loss or gradient. To address this challenge, we propose clustering the denoising tasks into small task clusters and applying MTL methods to them. Specifically, based on \\textbf{(O2)}, we employ interval clustering to enforce temporal proximity among denoising tasks within clusters. We show that interval clustering can be solved using dynamic programming, utilizing signal-to-noise ratio, timestep, and task affinity for clustering objectives. Through this, our approach addresses the issue of negative transfer in diffusion models by allowing for efficient computation of MTL methods. We validate the proposed clustering and its integration with MTL methods through various experiments, demonstrating improved sample quality of diffusion models. Our project page is available at https://gohyojun15.github.io/ANT_diffusion",
    "keywords": [],
    "checked": true,
    "id": "121b3e27f81004451e399cf6177cb03347d7976d",
    "semantic_title": "addressing negative transfer in diffusion models",
    "citation_count": 3,
    "authors": [
      "Hyojun Go",
      " Kim",
      "Yunsung Lee",
      "Seunghyun Lee",
      "Shinhyeok Oh",
      "Hyeongdon Moon",
      "Seungtaek Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/56cbfbf49937a0873d451343ddc8c57d-Abstract-Conference.html": {
    "title": "The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks",
    "volume": "main",
    "abstract": "Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms? Several recent studies, on tasks ranging from group operations to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex: small changes to model hyperparameters and initializations can induce discovery of qualitatively different algorithms from a fixed training set, and even learning of multiple different solutions in parallel. In modular addition, we specifically show that models learn a known Clock algorithm, a previously undescribed, less intuitive, but comprehensible procedure we term the Pizza algorithm, and a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for mechanistically characterizing the behavior of neural networks across the algorithmic phase space",
    "keywords": [],
    "checked": true,
    "id": "a72ba8dc49a6a842f69c312ac9a037a0f33b74f5",
    "semantic_title": "the clock and the pizza: two stories in mechanistic explanation of neural networks",
    "citation_count": 21,
    "authors": [
      "Ziqian Zhong",
      "Ziming Liu",
      "Max Tegmark",
      "Jacob Andreas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/56db53e53db1b29ae658e53fb764f067-Abstract-Conference.html": {
    "title": "Convergent Bregman Plug-and-Play Image Restoration for Poisson Inverse Problems",
    "volume": "main",
    "abstract": "Plug-and-Play (PnP) methods are efficient iterative algorithms for solving ill-posed image inverse problems. PnP methods are obtained by using deep Gaussian denoisers instead of the proximal operator or the gradient-descent step within proximal algorithms. Current PnP schemes rely on data-fidelity terms that have either Lipschitz gradients or closed-form proximal operators, which is not applicable to Poisson inverse problems. Based on the observation that the Gaussian noise is not the adequate noise model in this setting, we propose to generalize PnP using the Bregman Proximal Gradient (BPG) method. BPG replaces the Euclidean distance with a Bregman divergence that can better capture the smoothness properties of the problem. We introduce the Bregman Score Denoiser specifically parametrized and trained for the new Bregman geometry and prove that it corresponds to the proximal operator of a nonconvex potential. We propose two PnP algorithms based on the Bregman Score Denoiser for solving Poisson inverse problems. Extending the convergence results of BPG in the nonconvex settings, we show that the proposed methods converge, targeting stationary points of an explicit global functional. Experimental evaluations conducted on various Poisson inverse problems validate the convergence results and showcase effective restoration performance",
    "keywords": [],
    "checked": true,
    "id": "56f514024462ae498bf69a9d419f90fa90902b52",
    "semantic_title": "convergent bregman plug-and-play image restoration for poisson inverse problems",
    "citation_count": 1,
    "authors": [
      "Samuel Hurault",
      "Ulugbek Kamilov",
      "Arthur Leclaire",
      "Nicolas Papadakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/571fcbd2ad4273d9df51d7abc1172112-Abstract-Conference.html": {
    "title": "SQ Lower Bounds for Learning Mixtures of Linear Classifiers",
    "volume": "main",
    "abstract": "We study the problem of learning mixtures of linear classifiers under Gaussian covariates.Given sample access to a mixture of $r$ distributions on $\\mathbb{R}^n$ of the form $(\\mathbf{x},y_{\\ell})$, $\\ell \\in [r]$,where $\\mathbf{x}\\sim\\mathcal{N}(0,\\mathbf{I}_n)$ and$y_\\ell=\\mathrm{sign}(\\langle\\mathbf{v}_{\\ell},\\mathbf{x}\\rangle)$for an unknown unit vector $\\mathbf{v}_{\\ell}$,the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible,even for the special case of uniform mixtures.In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\\mathrm{poly}(1/\\Delta) \\log(r)}$,where $\\Delta$ is a lower bound on the pairwise $\\ell_2$-separation between the $\\mathbf{v}_{\\ell}$'s.The key technical ingredient underlying our result is a new construction of spherical designs on the unit sphere that may be of independent interest",
    "keywords": [],
    "checked": true,
    "id": "5d3ec46e3bae52a04761d0696dfeb01bca6cbe74",
    "semantic_title": "sq lower bounds for learning mixtures of linear classifiers",
    "citation_count": 1,
    "authors": [
      "Ilias Diakonikolas",
      "Daniel Kane",
      "Yuxin Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/572a6f16ec44f794fb3e0f8a310acbc6-Abstract-Conference.html": {
    "title": "Canonical normalizing flows for manifold learning",
    "volume": "main",
    "abstract": "Manifold learning flows are a class of generative modelling techniques that assume a low-dimensional manifold description of the data. The embedding of such a manifold into the high-dimensional space of the data is achieved via learnable invertible transformations. Therefore, once the manifold is properly aligned via a reconstruction loss, the probability density is tractable on the manifold and maximum likelihood can be used to optimize the network parameters. Naturally, the lower-dimensional representation of the data requires an injective-mapping. Recent approaches were able to enforce that the density aligns with the modelled manifold, while efficiently calculating the density volume-change term when embedding to the higher-dimensional space. However, unless the injective-mapping is analytically predefined, the learned manifold is not necessarily an \\emph{efficient representation} of the data. Namely, the latent dimensions of such models frequently learn an entangled intrinsic basis, with degenerate information being stored in each dimension. Alternatively, if a locally orthogonal and/or sparse basis is to be learned, here coined canonical intrinsic basis, it can serve in learning a more compact latent space representation. Toward this end, we propose a canonical manifold learning flow method, where a novel optimization objective enforces the transformation matrix to have few prominent and non-degenerate basis functions. We demonstrate that by minimizing the off-diagonal manifold metric elements $\\ell_1$-norm, we can achieve such a basis, which is simultaneously sparse and/or orthogonal. Canonical manifold flow yields a more efficient use of the latent space, automatically generating fewer prominent and distinct dimensions to represent data, and consequently a better approximation of target distributions than other manifold flow methods in most experiments we conducted, resulting in lower FID scores",
    "keywords": [],
    "checked": true,
    "id": "4860d11504e3aeaad96b5b128a1444969bc63f63",
    "semantic_title": "canonical normalizing flows for manifold learning",
    "citation_count": 0,
    "authors": [
      "Kyriakos Flouris",
      "Ender Konukoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/572cd21bd5dea96b065476b77d21b3c6-Abstract-Conference.html": {
    "title": "Regularizing Neural Networks with Meta-Learning Generative Models",
    "volume": "main",
    "abstract": "This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This can be caused by the synthetic samples not perfectly representing class categories in real data and uniform sampling not necessarily providing useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples for regularizing feature extractors instead of training classifiers. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR can avoid the performance degradation of naive generative data augmentation and boost the baselines. Experiments on six datasets showed that MGR is effective particularly when datasets are smaller and stably outperforms baselines by up to 7 percentage points on test accuracy",
    "keywords": [],
    "checked": true,
    "id": "36883d8d2fc0d039abd52433cabdbbdd7f92a49e",
    "semantic_title": "regularizing neural networks with meta-learning generative models",
    "citation_count": 1,
    "authors": [
      "Shin'ya Yamaguchi",
      "Daiki Chijiwa",
      "Sekitoshi Kanai",
      "Atsutoshi Kumagai",
      "Hisashi Kashima"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/574f145eac328cc4aaf9358e27120eb5-Abstract-Conference.html": {
    "title": "Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information",
    "volume": "main",
    "abstract": "Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\\circ \\mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable **Landscape Surrogate** $\\mathcal{M}$ as a replacement for $f\\circ \\mathbf{g}$. This surrogate, learnable by neural networks, can be computed faster than the solver $\\mathbf{g}$, provides dense and smooth gradients during training, can generalize to unseen optimization problems, and is efficiently learned via alternating optimization. We test our approach on both synthetic problems, including shortest path and multidimensional knapsack, and real-world problems such as portfolio optimization, achieving comparable or superior objective values compared to state-of-the-art baselines while reducing the number of calls to $\\mathbf{g}$. Notably, our approach outperforms existing methods for computationally expensive high-dimensional problems",
    "keywords": [],
    "checked": true,
    "id": "2f7d8963a4981b790be5afde1ddfc567460a38ba",
    "semantic_title": "landscape surrogate: learning decision losses for mathematical optimization under partial information",
    "citation_count": 2,
    "authors": [
      "Arman Zharmagambetov",
      "Brandon Amos",
      "Aaron Ferber",
      "Taoan Huang",
      "Bistra Dilkina",
      "Yuandong Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/575286a73f238b6516ce0467d67eadb2-Abstract-Conference.html": {
    "title": "Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework",
    "volume": "main",
    "abstract": "The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application",
    "keywords": [],
    "checked": false,
    "id": "0126f8d40bd682b1c4659f3f7d2a64fb6a515354",
    "semantic_title": "quantifying&modeling multimodal interactions: an information decomposition framework",
    "citation_count": 0,
    "authors": [
      "Paul Pu Liang",
      "Yun Cheng",
      "Xiang Fan",
      "Chun Kai Ling",
      "Suzanne Nie",
      "Richard Chen",
      "Zihao Deng",
      "Nicholas Allen",
      "Randy Auerbach",
      "Faisal Mahmood",
      "Russ R. Salakhutdinov",
      "Louis-Philippe Morency"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5752f9fd2d5c40174738d6f02c202e72-Abstract-Conference.html": {
    "title": "A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "The dominant paradigm in 3D human pose estimation that lifts a 2D pose sequence to 3D heavily relies on long-term temporal clues (i.e., using a daunting number of video frames) for improved accuracy, which incurs performance saturation, intractable computation and the non-causal problem. This can be attributed to their inherent inability to perceive spatial context as plain 2D joint coordinates carry no visual cues. To address this issue, we propose a straightforward yet powerful solution: leveraging the $\\textit{readily available}$ intermediate visual representations produced by off-the-shelf (pre-trained) 2D pose detectors -- no finetuning on the 3D task is even needed. The key observation is that, while the pose detector learns to localize 2D joints, such representations (e.g., feature maps) implicitly encode the joint-centric spatial context thanks to the regional operations in backbone networks. We design a simple baseline named $\\textbf{Context-Aware PoseFormer}$ to showcase its effectiveness. $\\textit{Without access to any temporal information}$, the proposed method significantly outperforms its context-agnostic counterpart, PoseFormer, and other state-of-the-art methods using up to $\\textit{hundreds of}$ video frames regarding both speed and precision. $\\textit{Project page:}$ https://qitaozhao.github.io/ContextAware-PoseFormer",
    "keywords": [],
    "checked": true,
    "id": "51fb9dac2d9b9fcf02149fe247388da0e71f5765",
    "semantic_title": "a single 2d pose with context is worth hundreds for 3d human pose estimation",
    "citation_count": 1,
    "authors": [
      "Qitao Zhao",
      "Ce Zheng",
      "Mengyuan Liu",
      "Chen Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57587d8d6a7ede0e5302fc22d0878c53-Abstract-Conference.html": {
    "title": "On the Stability-Plasticity Dilemma in Continual Meta-Learning: Theory and Algorithm",
    "volume": "main",
    "abstract": "We focus on Continual Meta-Learning (CML), which targets accumulating and exploiting meta-knowledge on a sequence of non-i.i.d. tasks. The primary challenge is to strike a balance between stability and plasticity, where a model should be stable to avoid catastrophic forgetting in previous tasks and plastic to learn generalizable concepts from new tasks. To address this, we formulate the CML objective as controlling the average excess risk upper bound of the task sequence, which reflects the trade-off between forgetting and generalization. Based on the objective, we introduce a unified theoretical framework for CML in both static and shifting environments, providing guarantees for various task-specific learning algorithms. Moreover, we first present a rigorous analysis of a bi-level trade-off in shifting environments. To approach the optimal trade-off, we propose a novel algorithm that dynamically adjusts the meta-parameter and its learning rate w.r.t environment change. Empirical evaluations on synthetic and real datasets illustrate the effectiveness of the proposed theory and algorithm",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi CHEN",
      "Changjian Shui",
      "Ligong Han",
      "Mario Marchand"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/575c450013d0e99e4b0ecf82bd1afaa4-Abstract-Conference.html": {
    "title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense",
    "volume": "main",
    "abstract": "The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics.To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data",
    "keywords": [],
    "checked": true,
    "id": "1c13af186d1e177b85ef1ec3fc7b8d33ec314cfd",
    "semantic_title": "paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
    "citation_count": 98,
    "authors": [
      "Kalpesh Krishna",
      "Yixiao Song",
      "Marzena Karpinska",
      "John Wieting",
      "Mohit Iyyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57a9b97477b67936298489e3c1417b0a-Abstract-Conference.html": {
    "title": "Energy Transformer",
    "volume": "main",
    "abstract": "Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion task, and obtain strong quantitative results on the graph anomaly detection and graph classification tasks",
    "keywords": [],
    "checked": true,
    "id": "09ec56d18667455fa86372c0645f331657bc3341",
    "semantic_title": "energy transformer",
    "citation_count": 8,
    "authors": [
      "Benjamin Hoover",
      "Yuchen Liang",
      "Bao Pham",
      "Rameswar Panda",
      "Hendrik Strobelt",
      "Duen Horng Chau",
      "Mohammed Zaki",
      "Dmitry Krotov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57bb27b9be6ad04019ae3cea2b540872-Abstract-Conference.html": {
    "title": "Theoretical and Practical Perspectives on what Influence Functions Do",
    "volume": "main",
    "abstract": "Influence functions (IF) have been seen as a technique for explaining model predictions through the lens of the training data. Their utility is assumed to be in identifying training examples \"responsible\" for a prediction so that, for example, correcting a prediction is possible by intervening on those examples (removing or editing them) and retraining the model. However, recent empirical studies have shown that the existing methods of estimating IF predict the leave-one-out-and-retrain effect poorly. In order to understand the mismatch between the theoretical promise and the practical results, we analyse five assumptions made by IF methods which are problematic for modern-scale deep neural networks and which concern convexity, numeric stability, training trajectory and parameter divergence. This allows us to clarify what can be expected theoretically from IF. We show that while most assumptions can be addressed successfully, the parameter divergence poses a clear limitation on the predictive power of IF: influence fades over training time even with deterministic training. We illustrate this theoretical result with BERT and ResNet models.Another conclusion from the theoretical analysis is that IF are still useful for model debugging and correcting even though some of the assumptions made in prior work do not hold: using natural language processing and computer vision tasks, we verify that mis-predictions can be successfully corrected by taking only a few fine-tuning steps on influential examples",
    "keywords": [],
    "checked": true,
    "id": "01a3ae8808dc61c9c1fffff370430c3a0059cba5",
    "semantic_title": "theoretical and practical perspectives on what influence functions do",
    "citation_count": 4,
    "authors": [
      "Andrea Schioppa",
      "Katja Filippova",
      "Ivan Titov",
      "Polina Zablotskaia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57bc0a850255e2041341bf74c7e2b9fa-Abstract-Conference.html": {
    "title": "On Sparse Modern Hopfield Model",
    "volume": "main",
    "abstract": "We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model.Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Theoretically, our key contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer.Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention.Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog.The conditions for the benefits of sparsity to arise are therefore identified and discussed.In addition, we show that the sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point convergence and exponential memory capacity.Empirically, we use both synthetic and real-world datasets to demonstrate that the sparse Hopfield model outperforms its dense counterpart in many situations",
    "keywords": [],
    "checked": true,
    "id": "abb79cc72fab35bfeb50585a121375b9bebafbb0",
    "semantic_title": "on sparse modern hopfield model",
    "citation_count": 3,
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Donglin Yang",
      "Dennis Wu",
      "Chenwei Xu",
      "Bo-Yu Chen",
      "Han Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57c30b677add9aa78e1745f0643104d0-Abstract-Conference.html": {
    "title": "D-CIPHER: Discovery of Closed-form Partial Differential Equations",
    "volume": "main",
    "abstract": "Closed-form differential equations, including partial differential equations and higher-order ordinary differential equations, are one of the most important tools used by scientists to model and better understand natural phenomena. Discovering these equations directly from data is challenging because it requires modeling relationships between various derivatives that are not observed in the data (equation-data mismatch) and it involves searching across a huge space of possible equations. Current approaches make strong assumptions about the form of the equation and thus fail to discover many well-known phenomena. Moreover, many of them resolve the equation-data mismatch by estimating the derivatives, which makes them inadequate for noisy and infrequent observations. To this end, we propose D-CIPHER, which is robust to measurement artifacts and can uncover a new and very general class of differential equations. We further design a novel optimization procedure, CoLLie, to help D-CIPHER search through this class efficiently. Finally, we demonstrate empirically that it can discover many well-known equations that are beyond the capabilities of current methods",
    "keywords": [],
    "checked": true,
    "id": "c6adc872ff1ca0ca353d0acc2d207fafe5e9bc9f",
    "semantic_title": "d-cipher: discovery of closed-form partial differential equations",
    "citation_count": 1,
    "authors": [
      "Krzysztof Kacprzyk",
      "Zhaozhi Qian",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57d7e7e1593ad1ab6818c258fa5654ce-Abstract-Conference.html": {
    "title": "Training neural operators to preserve invariant measures of chaotic attractors",
    "volume": "main",
    "abstract": "Chaotic systems make long-horizon forecasts difficult because small perturbations in initial conditions cause trajectories to diverge at an exponential rate. In this setting, neural operators trained to minimize squared error losses, while capable of accurate short-term forecasts, often fail to reproduce statistical or structural properties of the dynamics over longer time horizons and can yield degenerate results. In this paper, we propose an alternative framework designed to preserve invariant measures of chaotic attractors that characterize the time-invariant statistical properties of the dynamics. Specifically, in the multi-environment setting (where each sample trajectory is governed by slightly different dynamics), we consider two novel approaches to training with noisy data. First, we propose a loss based on the optimal transport distance between the observed dynamics and the neural operator outputs. This approach requires expert knowledge of the underlying physics to determine what statistical features should be included in the optimal transport loss. Second, we show that a contrastive learning framework, which does not require any specialized prior knowledge, can preserve statistical properties of the dynamics nearly as well as the optimal transport approach. On a variety of chaotic systems, our method is shown empirically to preserve invariant measures of chaotic attractors",
    "keywords": [],
    "checked": true,
    "id": "d0fa04f18c35d4633e5baea53ad90ed7b25f2271",
    "semantic_title": "training neural operators to preserve invariant measures of chaotic attractors",
    "citation_count": 3,
    "authors": [
      "Ruoxi Jiang",
      "Peter Y. Lu",
      "Elena Orlova",
      "Rebecca Willett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57d8ebf4c2f050a6485f370d47656a9e-Abstract-Conference.html": {
    "title": "Certification of Distributional Individual Fairness",
    "volume": "main",
    "abstract": "Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \\textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees",
    "keywords": [],
    "checked": true,
    "id": "16ab5911ca99c30800570035b1316dc864ca894f",
    "semantic_title": "certification of distributional individual fairness",
    "citation_count": 0,
    "authors": [
      "Matthew Wicker",
      "Vihari Piratla",
      "Adrian Weller"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57fabaa549352c52d5d312171b16970e-Abstract-Conference.html": {
    "title": "Leveraging sparse and shared feature activations for disentangled representation learning",
    "volume": "main",
    "abstract": "Recovering the latent factors of variation of high dimensional data has so far focused on simple synthetic settings. Mostly building on unsupervised and weakly-supervised objectives, prior work missed out on the positive implications for representation learning on real world data. In this work, we propose to leverage knowledge extracted from a diversified set of supervised tasks to learn a common disentangled representation. Assuming each supervised task only depends on an unknown subset of the factors of variation, we disentangle the feature space of a supervised multi-task model, with features activating sparsely across different tasks and information being shared as appropriate. Importantly, we never directly observe the factors of variations but establish that access to multiple tasks is sufficient for identifiability under sufficiency and minimality assumptions.We validate our approach on six real world distribution shift benchmarks, and different data modalities (images, text), demonstrating how disentangled representations can be transferred to real settings",
    "keywords": [],
    "checked": true,
    "id": "d32a0fdd974badf6bf5529e11c6776fe5ed88d00",
    "semantic_title": "leveraging sparse and shared feature activations for disentangled representation learning",
    "citation_count": 3,
    "authors": [
      "Marco Fumero",
      "Florian Wenzel",
      "Luca Zancato",
      "Alessandro Achille",
      "Emanuele Rodolà",
      "Stefano Soatto",
      "Bernhard Schölkopf",
      "Francesco Locatello"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/58575be50c9b47902359920a4d5d1ab4-Abstract-Conference.html": {
    "title": "A Unified Framework for U-Net Design and Analysis",
    "volume": "main",
    "abstract": "U-Nets are a go-to neural architecture across numerous tasks for continuous signals on a square such as images and Partial Differential Equations (PDE), however their design and architecture is understudied. In this paper, we provide a framework for designing and analysing general U-Net architectures. We present theoretical results which characterise the role of the encoder and decoder in a U-Net, their high-resolution scaling limits and their conjugacy to ResNets via preconditioning. We propose Multi-ResNets, U-Nets with a simplified, wavelet-based encoder without learnable parameters. Further, we show how to design novel U-Net architectures which encode function constraints, natural bases, or the geometry of the data. In diffusion models, our framework enables us to identify that high-frequency information is dominated by noise exponentially faster, and show how U-Nets with average pooling exploit this. In our experiments, we demonstrate how Multi-ResNets achieve competitive and often superior performance compared to classical U-Nets in image segmentation, PDE surrogate modelling, and generative modelling with diffusion models. Our U-Net framework paves the way to study the theoretical properties of U-Nets and design natural, scalable neural architectures for a multitude of problems beyond the square",
    "keywords": [],
    "checked": true,
    "id": "36a22d5028424d2429f602e2b92bb299eb639f60",
    "semantic_title": "a unified framework for u-net design and analysis",
    "citation_count": 2,
    "authors": [
      "Christopher Williams",
      "Fabian Falck",
      "George Deligiannidis",
      "Chris C Holmes",
      "Arnaud Doucet",
      "Saifuddin Syed"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/585e9cf25585612ac27b535457116513-Abstract-Conference.html": {
    "title": "On the Importance of Feature Separability in Predicting Out-Of-Distribution Error",
    "volume": "main",
    "abstract": "Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground-truth labels. While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability empirically and theoretically. Specifically, we propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency",
    "keywords": [],
    "checked": true,
    "id": "13bfdd929b91164c32643bddb566bcce02ccf6b5",
    "semantic_title": "on the importance of feature separability in predicting out-of-distribution error",
    "citation_count": 1,
    "authors": [
      "RENCHUNZI XIE",
      "Hongxin Wei",
      "Lei Feng",
      "Yuzhou Cao",
      "Bo An"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/58692a1701314e09cbd7a5f5f3871cc9-Abstract-Conference.html": {
    "title": "The Transient Nature of Emergent In-Context Learning in Transformers",
    "volume": "main",
    "abstract": "Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL), despite not being explicitly trained for it. Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g., through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data. However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically. Here, we show that the emergence of ICL during transformer training is, in fact, often transient. We train transformers on synthetic data designed so that both ICL or in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL first emerges, then disappears and gives way to IWL,all while the training loss decreases, indicating an asymptotic preference for IWL. The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to \"overtrain\" transformers when seeking compact, cheaper-to-run models. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks",
    "keywords": [],
    "checked": true,
    "id": "50714ad97ae3a65ff15771b8ec89de47aa8311ab",
    "semantic_title": "the transient nature of emergent in-context learning in transformers",
    "citation_count": 5,
    "authors": [
      "Aaditya Singh",
      "Stephanie Chan",
      "Ted Moskovitz",
      "Erin Grant",
      "Andrew Saxe",
      "Felix Hill"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/58a799d16fb0c1f2014e98f4ba972b25-Abstract-Conference.html": {
    "title": "When is Agnostic Reinforcement Learning Statistically Tractable?",
    "volume": "main",
    "abstract": "We study the problem of agnostic PAC reinforcement learning (RL): given a policy class $\\Pi$, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an $\\epsilon$-suboptimal policy with respect to \\(\\Pi\\)? Towards that end, we introduce a new complexity measure, called the \\emph{spanning capacity}, that depends solely on the set \\(\\Pi\\) and is independent of the MDP dynamics. With a generative model, we show that the spanning capacity characterizes PAC learnability for every policy class $\\Pi$. However, for online RL, the situation is more subtle. We show there exists a policy class $\\Pi$ with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \\emph{sunflower} structure which in conjunction with bounded spanning capacity enables statistically efficient online RL via a new algorithm called POPLER, which takes inspiration from classical importance sampling methods as well as recent developments for reachable-state identification and policy evaluation in reward-free exploration",
    "keywords": [],
    "checked": true,
    "id": "c9808b9785e51e8ff84abdf2e4d5cdd2adfc3202",
    "semantic_title": "when is agnostic reinforcement learning statistically tractable?",
    "citation_count": 0,
    "authors": [
      "Zeyu Jia",
      "Gene Li",
      "Alexander Rakhlin",
      "Ayush Sekhari",
      "Nati Srebro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/58be158bf831a706b1a66cffbc401cac-Abstract-Conference.html": {
    "title": "Convolutional Visual Prompt for Robust Visual Perception",
    "volume": "main",
    "abstract": "Vision models are often vulnerable to out-of-distribution (OOD) samples without adapting. While visual prompts offer a lightweight method of input-space adaptation for large-scale vision models, they rely on a high-dimensional additive vector and labeled data. This leads to overfitting when adapting models in a self-supervised test-time setting without labels. We introduce convolutional visual prompts (CVP) for label-free test-time adaptation for robust visual perception. The structured nature of CVP demands fewer trainable parameters, less than 1\\% compared to standard visual prompts, combating overfitting. Extensive experiments and analysis on a wide variety of OOD visual perception tasks show that our approach is effective, improving robustness by up to 5.87\\% over several large-scale models",
    "keywords": [],
    "checked": true,
    "id": "aa4e78c7853a5b286810f095a28b9619bba31486",
    "semantic_title": "convolutional visual prompt for robust visual perception",
    "citation_count": 3,
    "authors": [
      "Yun-Yun Tsai",
      "Chengzhi Mao",
      "Junfeng Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/58cc11cda2a2679e8af5c6317aed0af8-Abstract-Conference.html": {
    "title": "LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching",
    "volume": "main",
    "abstract": "Obtaining large pre-trained models that can be fine-tuned to new tasks with limited annotated samples has remained an open challenge for medical imaging data. While pre-trained networks on ImageNet and vision-language foundation models trained on web-scale data are the prevailing approaches, their effectiveness on medical tasks is limited due to the significant domain shift between natural and medical images. To bridge this gap, we introduce LVM-Med, the first family of deep networks trained on large-scale medical datasets. We have collected approximately 1.3 million medical images from 55 publicly available datasets, covering a large number of organs and modalities such as CT, MRI, X-ray, and Ultrasound. We benchmark several state-of-the-art self-supervised algorithms on this dataset and propose a novel self-supervised contrastive learning algorithm using a graph-matching formulation. The proposed approach makes three contributions: (i) it integrates prior pair-wise image similarity metrics based on local and global information; (ii) it captures the structural constraints of feature embeddings through a loss function constructed through a combinatorial graph-matching objective, and (iii) it can be trained efficiently end-to-end using modern gradient-estimation techniques for black-box solvers. We thoroughly evaluate the proposed LVM-Med on 15 downstream medical tasks ranging from segmentation and classification to object detection, and both for the in and out-of-distribution settings. LVM-Med empirically outperforms a number of state-of-the-art supervised, self-supervised, and foundation models. For challenging tasks such as Brain Tumor Classification or Diabetic Retinopathy Grading, LVM-Med improves previous vision-language models trained on 1 billion masks by 6-7% while using only a ResNet-50",
    "keywords": [],
    "checked": true,
    "id": "f51017a549c1d858ae7d0650e1bad0186cb17808",
    "semantic_title": "lvm-med: learning large-scale self-supervised vision models for medical imaging via second-order graph matching",
    "citation_count": 6,
    "authors": [
      "Duy M. H. Nguyen",
      "Hoang Nguyen",
      "Nghiem Diep",
      "Tan Ngoc Pham",
      "Tri Cao",
      "Binh Nguyen",
      "Paul Swoboda",
      "Nhat Ho",
      "Shadi Albarqouni",
      "Pengtao Xie",
      "Daniel Sonntag",
      "Mathias Niepert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/58cd3b02902d79aea4b3b603fb0d0941-Abstract-Conference.html": {
    "title": "Lending Interaction Wings to Recommender Systems with Conversational Agents",
    "volume": "main",
    "abstract": "An intelligent conversational agent (a.k.a., chat-bot) could embrace conversational technologies to obtain user preferences online, to overcome inherent limitations of recommender systems trained over the offline historical user behaviors. In this paper, we propose CORE, a new offline-training and online-checking framework to plug a COnversational agent into REcommender systems. Unlike most prior conversational recommendation approaches that systemically combine conversational and recommender parts through a reinforcement learning framework, CORE bridges the conversational agent and recommender system through a unified uncertainty minimization framework, which can be easily applied to any existing recommendation approach. Concretely, CORE treats a recommender system as an offline estimator to produce an estimated relevance score for each item, while CORE regards a conversational agent as an online checker that checks these estimated scores in each online session. We define uncertainty as the sum of unchecked relevance scores. In this regard, the conversational agent acts to minimize uncertainty via querying either attributes or items. Towards uncertainty minimization, we derive the certainty gain of querying each attribute and item, and develop a novel online decision tree algorithm to decide what to query at each turn. Our theoretical analysis reveals the bound of the expected number of turns of CORE in a cold-start setting. Experimental results demonstrate that CORE can be seamlessly employed on a variety of recommendation approaches, and can consistently bring significant improvements in both hot-start and cold-start settings",
    "keywords": [],
    "checked": true,
    "id": "e04c1ccf87d16fcf361ee3810dedca951d4589e6",
    "semantic_title": "lending interaction wings to recommender systems with conversational agents",
    "citation_count": 4,
    "authors": [
      "Jiarui Jin",
      "Xianyu Chen",
      "Fanghua Ye",
      "Mengyue Yang",
      "Yue Feng",
      "Weinan Zhang",
      "Yong Yu",
      "Jun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/58d0e78cf042af5876e12661087bea12-Abstract-Conference.html": {
    "title": "High-Fidelity Audio Compression with Improved RVQGAN",
    "volume": "main",
    "abstract": "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling",
    "keywords": [],
    "checked": true,
    "id": "cf8fced1f446c554ca0c5608ae0a1131184212f6",
    "semantic_title": "high-fidelity audio compression with improved rvqgan",
    "citation_count": 30,
    "authors": [
      "Rithesh Kumar",
      "Prem Seetharaman",
      "Alejandro Luebs",
      "Ishaan Kumar",
      "Kundan Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/59056767478c7df64e6250eadfeb0a04-Abstract-Conference.html": {
    "title": "Comparing Apples to Oranges: Learning Similarity Functions for Data Produced by Different Distributions",
    "volume": "main",
    "abstract": "Similarity functions measure how comparable pairs of elements are, and play a key role in a wide variety of applications, e.g., notions of Individual Fairness abiding by the seminal paradigm of Dwork et al., as well as Clustering problems. However, access to an accurate similarity function should not always be considered guaranteed, and this point was even raised by Dwork et al. For instance, it is reasonable to assume that when the elements to be compared are produced by different distributions, or in other words belong to different ``demographic'' groups, knowledge of their true similarity might be very difficult to obtain. In this work, we present an efficient sampling framework that learns these across-groups similarity functions, using only a limited amount of experts' feedback. We show analytical results with rigorous theoretical bounds, and empirically validate our algorithms via a large suite of experiments",
    "keywords": [],
    "checked": true,
    "id": "b97145f180fe7f90999bab18528b72f7476690a2",
    "semantic_title": "comparing apples to oranges: learning similarity functions for data produced by different distributions",
    "citation_count": 0,
    "authors": [
      "Leonidas Tsepenekas",
      "Ivan Brugere",
      "Freddy Lecue",
      "Daniele Magazzeni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/590daf74f99ee85df3d8c007df9c8187-Abstract-Conference.html": {
    "title": "Scalable Transformer for PDE Surrogate Modeling",
    "volume": "main",
    "abstract": "Transformer has shown state-of-the-art performance on various applications and has recently emerged as a promising tool for surrogate modeling of partial differential equations (PDEs). Despite the introduction of linear-complexity attention, applying Transformer to problems with a large number of grid points can be numerically unstable and computationally expensive. In this work, we propose Factorized Transformer (FactFormer), which is based on an axial factorized kernel integral. Concretely, we introduce a learnable projection operator that decomposes the input function into multiple sub-functions with one-dimensional domain. These sub-functions are then evaluated and used to compute the instance-based kernel with an axial factorized scheme. We showcase that the proposed model is able to simulate 2D Kolmogorov flow on a $256\\times 256$ grid and 3D smoke buoyancy on a $64\\times64\\times64$ grid with good accuracy and efficiency. The proposed factorized scheme can serve as a computationally efficient low-rank surrogate for the full attention scheme when dealing with multi-dimensional problems",
    "keywords": [],
    "checked": true,
    "id": "a4a2326cf7f16ea0988ffc65eabf48ffe47c432f",
    "semantic_title": "scalable transformer for pde surrogate modeling",
    "citation_count": 6,
    "authors": [
      "Zijie Li",
      "Dule Shu",
      "Amir Barati Farimani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5927edd18c5dd83aa8936a4610c72029-Abstract-Conference.html": {
    "title": "What is the Inductive Bias of Flatness Regularization? A Study of Deep Matrix Factorization Models",
    "volume": "main",
    "abstract": "Recent works on over-parameterized neural networks have shown that the stochasticity in optimizers has the implicit regularization effect of minimizing the sharpness of the loss function (in particular, the trace of its Hessian) over the family zero-loss solutions. More explicit forms of flatness regularization also empirically improve the generalization performance. However, it remains unclear why and when flatness regularization leads to better generalization. This work takes the first step towards understanding the inductive bias of the minimum trace of the Hessian solutions in an important setting: learning deep linear networks from linear measurements, also known as \\emph{deep matrix factorization}. We show that with the standard Restricted Isometry Property (RIP) on the measurements, minimizing the trace of Hessian is approximately equivalent to minimizing the Schatten 1-norm of the corresponding end-to-end matrix parameters (i.e., the product of all layer matrices), which in turn leads to better generalization",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khashayar Gatmiry",
      "Zhiyuan Li",
      "Tengyu Ma",
      "Sashank Reddi",
      "Stefanie Jegelka",
      "Ching-Yao Chuang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/592da1445a51e54a3987958b5831948f-Abstract-Conference.html": {
    "title": "Two Sides of The Same Coin: Bridging Deep Equilibrium Models and Neural ODEs via Homotopy Continuation",
    "volume": "main",
    "abstract": "Deep Equilibrium Models (DEQs) and Neural Ordinary Differential Equations (Neural ODEs) are two branches of implicit models that have achieved remarkable success owing to their superior performance and low memory consumption. While both are implicit models, DEQs and Neural ODEs are derived from different mathematical formulations. Inspired by homotopy continuation, we establish a connection between these two models and illustrate that they are actually two sides of the same coin. Homotopy continuation is a classical method of solving nonlinear equations based on a corresponding ODE. Given this connection, we proposed a new implicit model called HomoODE that inherits the property of high accuracy from DEQs and the property of stability from Neural ODEs. Unlike DEQs, which explicitly solve an equilibrium-point-finding problem via Newton's methods in the forward pass, HomoODE solves the equilibrium-point-finding problem implicitly using a modified Neural ODE via homotopy continuation. Further, we developed an acceleration method for HomoODE with a shared learnable initial point. It is worth noting that our model also provides a better understanding of why Augmented Neural ODEs work as long as the augmented part is regarded as the equilibrium point to find. Comprehensive experiments with several image classification tasks demonstrate that HomoODE surpasses existing implicit models in terms of both accuracy and memory consumption",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shutong Ding",
      "Tianyu Cui",
      "Jingya Wang",
      "Ye Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/59404fb89d6194641c69ae99ecdf8f6d-Abstract-Conference.html": {
    "title": "Emergent and Predictable Memorization in Large Language Models",
    "volume": "main",
    "abstract": "Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization in the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https://github.com/EleutherAI/pythia",
    "keywords": [],
    "checked": true,
    "id": "deb8f26509ae320fc975b32922416cb156c61bbd",
    "semantic_title": "emergent and predictable memorization in large language models",
    "citation_count": 39,
    "authors": [
      "Stella Biderman",
      "USVSN PRASHANTH",
      "Lintang Sutawika",
      "Hailey Schoelkopf",
      "Quentin Anthony",
      "Shivanshu Purohit",
      "Edward Raff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5951641ad71b0052cf776f9b71f18932-Abstract-Conference.html": {
    "title": "MultiMoDN—Multimodal, Multi-Task, Interpretable Modular Networks",
    "volume": "main",
    "abstract": "Predicting multiple real-world tasks in a single model often requires a particularly diverse feature space. Multimodal (MM) models aim to extract the synergistic predictive potential of multiple data types to create a shared feature space with aligned semantic meaning across inputs of drastically varying sizes (i.e. images, text, sound). Most current MM architectures fuse these representations in parallel, which not only limits their interpretability but also creates a dependency on modality availability. We present MultiModN, a multimodal, modular network that fuses latent representations in a sequence of any number, combination, or type of modality while providing granular real-time predictive feedback on any number or combination of predictive tasks. MultiModN's composable pipeline is interpretable-by-design, as well as innately multi-task and robust to the fundamental issue of biased missingness. We perform four experiments on several benchmark MM datasets across 10 real-world tasks (predicting medical diagnoses, academic performance, and weather), and show that MultiModN's sequential MM fusion does not compromise performance compared with a baseline of parallel fusion. By simulating the challenging bias of missing not-at-random (MNAR), this work shows that, contrary to MultiModN, parallel fusion baselines erroneously learn MNAR and suffer catastrophic failure when faced with different patterns of MNAR at inference. To the best of our knowledge, this is the first inherently MNAR-resistant approach to MM modeling. In conclusion, MultiModN provides granular insights, robustness, and flexibility without compromising performance",
    "keywords": [],
    "checked": false,
    "id": "5456000bbf4f202f95f384a7c97d7d0a56f5d905",
    "semantic_title": "multimodn- multimodal, multi-task, interpretable modular networks",
    "citation_count": 0,
    "authors": [
      "Vinitra Swamy",
      "Malika Satayeva",
      "Jibril Frej",
      "Thierry Bossy",
      "Thijs Vogels",
      "Martin Jaggi",
      "Tanja Käser",
      "Mary-Anne Hartley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5965f3a748a8d41415db2bfa44635cc3-Abstract-Conference.html": {
    "title": "Differentiable Neuro-Symbolic Reasoning on Large-Scale Knowledge Graphs",
    "volume": "main",
    "abstract": "Knowledge graph (KG) reasoning utilizes two primary techniques, i.e., rule-based and KG-embedding based. The former provides precise inferences, but inferring via concrete rules is not scalable. The latter enables efficient reasoning at the cost of ambiguous inference accuracy. Neuro-symbolic reasoning seeks to amalgamate the advantages of both techniques. The crux of this approach is replacing the predicted existence of all possible triples (i.e., truth scores inferred from rules) with a suitable approximation grounded in embedding representations. However, constructing an effective approximation of all possible triples' truth scores is a challenging task, because it needs to balance the tradeoff between accuracy and efficiency, while compatible with both the rule-based and KG-embedding models. To this end, we proposed a differentiable framework - DiffLogic. Instead of directly approximating all possible triples, we design a tailored filter to adaptively select essential triples based on the dynamic rules and weights. The truth scores assessed by KG-embedding are continuous, so we employ a continuous Markov logic network named probabilistic soft logic (PSL). It employs the truth scores of essential triples to assess the overall agreement among rules, weights, and observed triples. PSL enables end-to-end differentiable optimization, so we can alternately update embedding and weighted rules. On benchmark datasets, we empirically show that DiffLogic surpasses baselines in both effectiveness and efficiency",
    "keywords": [],
    "checked": false,
    "id": "81e51c5d677338208f94f19488d86e6658f927af",
    "semantic_title": "lprules: rule induction in knowledge graphs using linear programming",
    "citation_count": 3,
    "authors": [
      "CHEN SHENGYUAN",
      "Yunfeng Cai",
      "Huang Fang",
      "Xiao Huang",
      "Mingming Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/597254dc45be8c166d3ccf0ba2d56325-Abstract-Conference.html": {
    "title": "Topological Parallax: A Geometric Specification for Deep Perception Models",
    "volume": "main",
    "abstract": "For safety and robustness of AI systems, we introduce topological parallax as a theoretical and computational tool that compares a trained model to a reference dataset to determine whether they have similar multiscale geometric structure. Our proofs and examples show that this geometric similarity between dataset and model is essential to trustworthy interpolation and perturbation, and we conjecture that this new concept will add value to the current debate regarding the unclear relationship between \"overfitting\"' and \"generalization'' in applications of deep-learning. In typical deep-learning applications, an explicit geometric description of the model isimpossible, but parallax can estimate topological features (components, cycles, voids, etc.)in the model by examining the effect on the Rips complex of geodesic distortions using the reference dataset.Thus, parallax indicates whether the model shares similar multiscale geometric features with the dataset.Parallax presents theoretically via topological data analysis [TDA] as a bi-filtered persistence module,and the key properties of this module are stable under perturbation of the reference dataset",
    "keywords": [],
    "checked": true,
    "id": "8ac76f8921c69fd673d56f99b91fc445b12575ca",
    "semantic_title": "topological parallax: a geometric specification for deep perception models",
    "citation_count": 1,
    "authors": [
      "Abraham Smith",
      "Michael Catanzaro",
      "Gabrielle Angeloro",
      "Nirav Patel",
      "Paul Bendich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/599221d7ebf6b3403190f38a3f282a1c-Abstract-Conference.html": {
    "title": "Rewiring Neurons in Non-Stationary Environments",
    "volume": "main",
    "abstract": "The human brain rewires itself for neuroplasticity in the presence of new tasks. We are inspired to harness this key process in continual reinforcement learning, prioritizing adaptation to non-stationary environments. In distinction to existing rewiring approaches that rely on pruning or dynamic routing, which may limit network capacity and plasticity, this work presents a novel rewiring scheme by permuting hidden neurons. Specifically, the neuron permutation is parameterized to be end-to-end learnable and can rearrange all available synapses to explore a large span of weight space, thereby promoting adaptivity. In addition, we introduce two main designs to steer the rewiring process in continual reinforcement learning: first, a multi-mode rewiring strategy is proposed which diversifies the policy and encourages exploration when encountering new environments. Secondly, to ensure stability on history tasks, the network is devised to cache each learned wiring while subtly updating its weights, allowing for retrospective recovery of any previous state appropriate for the task. Meanwhile, an alignment mechanism is curated to achieve better plasticity-stability tradeoff by jointly optimizing cached wirings and weights. Our proposed method is comprehensively evaluated on 18 continual reinforcement learning scenarios ranging from locomotion to manipulation, demonstrating its advantages over state-of-the-art competitors in performance-efficiency tradeoffs. Code is available at https://github.com/feifeiobama/RewireNeuron",
    "keywords": [],
    "checked": true,
    "id": "938975891e19b10b61de3aca3e23c3ac33be9356",
    "semantic_title": "rewiring neurons in non-stationary environments",
    "citation_count": 0,
    "authors": [
      "Zhicheng Sun",
      "Yadong Mu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/59b7c1e1716c4feadefd6c70b1dd4630-Abstract-Conference.html": {
    "title": "TransHP: Image Classification with Hierarchical Prompting",
    "volume": "main",
    "abstract": "This paper explores a hierarchical prompting mechanism for the hierarchical image classification (HIC) task. Different from prior HIC methods, our hierarchical prompting is the first to explicitly inject ancestor-class information as a tokenized hint that benefits the descendant-class discrimination. We think it well imitates human visual recognition, i.e., humans may use the ancestor class as a prompt to draw focus on the subtle differences among descendant classes. We model this prompting mechanism into a Transformer with Hierarchical Prompting (TransHP). TransHP consists of three steps: 1) learning a set of prompt tokens to represent the coarse (ancestor) classes, 2) on-the-fly predicting the coarse class of the input image at an intermediate block, and 3) injecting the prompt token of the predicted coarse class into the intermediate feature. Though the parameters of TransHP maintain the same for all input images, the injected coarse-class prompt conditions (modifies) the subsequent feature extraction and encourages a dynamic focus on relatively subtle differences among the descendant classes. Extensive experiments show that TransHP improves image classification on accuracy (e.g., improving ViT-B/16 by +2.83% ImageNet classification accuracy), training data efficiency (e.g., +12.69% improvement under 10% ImageNet training data), and model explainability. Moreover, TransHP also performs favorably against prior HIC methods, showing that TransHP well exploits the hierarchical information",
    "keywords": [],
    "checked": true,
    "id": "d91886f3fed510e1fb8efde52538f06e1b64c13f",
    "semantic_title": "transhp: image classification with hierarchical prompting",
    "citation_count": 2,
    "authors": [
      "Wenhao Wang",
      "Yifan Sun",
      "Wei Li",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/59b9582cd35f555ea8415030073e7b22-Abstract-Conference.html": {
    "title": "Practical Differentially Private Hyperparameter Tuning with Subsampling",
    "volume": "main",
    "abstract": "Tuning the hyperparameters of differentially private (DP) machine learning (ML) algorithms often requires use of sensitive data and this may leak private information via hyperparameter values. Recently, Papernot and Steinke (2022) proposed a certain class of DP hyperparameter tuning algorithms, where the number of random search samples is randomized. Commonly, these algorithms still considerably increase the DP privacy parameter $\\varepsilon$ over non-tuned DP ML model training and can be computationally heavy as evaluating each hyperparameter candidate requires a new training run. We focus on lowering both the DP bounds and the compute cost of these methods by using only a random subset of the sensitive data for the hyperparameter tuning and by appropriately extrapolating the optimal values to a larger dataset. We carry out a Rényi differential privacy analysis for the proposed method and experimentally show that it consistently leads to better privacy-utility trade-off than the baseline method by Papernot and Steinke",
    "keywords": [],
    "checked": true,
    "id": "e73c2eb94d146cfc838e733a8c6a3e348479f2f8",
    "semantic_title": "practical differentially private hyperparameter tuning with subsampling",
    "citation_count": 5,
    "authors": [
      "Antti Koskela",
      "Tejas D. Kulkarni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/59d4e18a60490b9ed9913f3be2b14839-Abstract-Conference.html": {
    "title": "Learning to Discover Skills through Guidance",
    "volume": "main",
    "abstract": "In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE are available at https://mynsng.github.io/discodance/",
    "keywords": [],
    "checked": true,
    "id": "2892b19b1ac3c3e7d1bb95ceb39b2d4273b2939b",
    "semantic_title": "learning to discover skills through guidance",
    "citation_count": 0,
    "authors": [
      "HYUNSEUNG KIM",
      "BYUNG KUN LEE",
      "Hojoon Lee",
      "Dongyoon Hwang",
      "Sejik Park",
      "Kyushik Min",
      "Jaegul Choo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/59f6421e64707225fdf5b28840679a07-Abstract-Conference.html": {
    "title": "Polynomial-Time Linear-Swap Regret Minimization in Imperfect-Information Sequential Games",
    "volume": "main",
    "abstract": "No-regret learners seek to minimize the difference between the loss they cumulated through the actions they played, and the loss they would have cumulated in hindsight had they consistently modified their behavior according to some strategy transformation function. The size of the set of transformations considered by the learner determines a natural notion of rationality. As the set of transformations each learner considers grows, the strategies played by the learners recover more complex game-theoretic equilibria, including correlated equilibria in normal-form games and extensive-form correlated equilibria in extensive-form games. At the extreme, a no-swap-regret agent is one that minimizes regret against the set of all functions from the set of strategies to itself. While it is known that the no-swap-regret condition can be attained efficiently in nonsequential (normal-form) games, understanding what is the strongest notion of rationality that can be attained efficiently in the worst case in sequential (extensive-form) games is a longstanding open problem. In this paper we provide a positive result, by showing that it is possible, in any sequential game, to retain polynomial-time (in the game tree size) iterations while achieving sublinear regret with respect to all linear transformations of the mixed strategy space, a notion called no-linear-swap regret. This notion of hindsight rationality is as strong as no-swap-regret in nonsequential games, and stronger than no-trigger-regret in sequential games—thereby proving the existence of a subset of extensive-form correlated equilibria robust to linear deviations, which we call linear-deviation correlated equilibria, that can be approached efficiently",
    "keywords": [],
    "checked": true,
    "id": "cb50198dced629fad04f70ac2aa32461fd32975c",
    "semantic_title": "polynomial-time linear-swap regret minimization in imperfect-information sequential games",
    "citation_count": 3,
    "authors": [
      "Gabriele Farina",
      "Charilaos Pipis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/59fe467d5e71ba6b8d41bb3928da6f4c-Abstract-Conference.html": {
    "title": "Counterfactually Comparing Abstaining Classifiers",
    "volume": "main",
    "abstract": "Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stake decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions matter when they can eventually be utilized, either directly or as a backup option in a failure mode. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions under which the counterfactual score is identifiable: if the abstentions are stochastic, and if the evaluation data is independent of the training data (ensuring that the predictions are missing at random), then the score is identifiable. Note that, if abstentions are deterministic, then the score is unidentifiable because the classifier can perform arbitrarily poorly on its abstentions. Leveraging tools from observational causal inference, we then develop nonparametric and doubly robust methods to efficiently estimate this quantity under identification. Our approach is examined in both simulated and real data experiments",
    "keywords": [],
    "checked": true,
    "id": "bf7af0f9371f19493b5f08a742d9984c0bc3facc",
    "semantic_title": "counterfactually comparing abstaining classifiers",
    "citation_count": 0,
    "authors": [
      "Yo Joong Choe",
      "Aditya Gangrade",
      "Aaditya Ramdas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5a1667459d0cdeb2fe6b2f0dffc5cb9d-Abstract-Conference.html": {
    "title": "Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "While bisimulation-based approaches hold promise for learning robust state representations for Reinforcement Learning (RL) tasks, their efficacy in offline RL tasks has not been up to par. In some instances, their performance has even significantly underperformed alternative methods. We aim to understand why bisimulation methods succeed in online settings, but falter in offline tasks. Our analysis reveals that missing transitions in the dataset are particularly harmful to the bisimulation principle, leading to ineffective estimation. We also shed light on the critical role of reward scaling in bounding the scale of bisimulation measurements and of the value error they induce. Based on these findings, we propose to apply the expectile operator for representation learning to our offline RL setting, which helps to prevent overfitting to incomplete data. Meanwhile, by introducing an appropriate reward scaling strategy, we avoid the risk of feature collapse in representation space. We implement these recommendations on two state-of-the-art bisimulation-based algorithms, MICo and SimSR, and demonstrate performance gains on two benchmark suites: D4RL and Visual D4RL. Codes are provided at \\url{https://github.com/zanghyu/Offline_Bisimulation}",
    "keywords": [],
    "checked": true,
    "id": "2b386c5d013011f43179afa7f9893c8cad67a09d",
    "semantic_title": "understanding and addressing the pitfalls of bisimulation-based representations in offline reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Hongyu Zang",
      "Xin Li",
      "Leiji Zhang",
      "Yang Liu",
      "Baigui Sun",
      "Riashat Islam",
      "Remi Tachet des Combes",
      "Romain Laroche"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5a1a10c2c2c9b9af1514687bc24b8f3d-Abstract-Conference.html": {
    "title": "Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting",
    "volume": "main",
    "abstract": "Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally-trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the predictions of base forecasters with reduced computational overhead over reverse diffusion (refine). Notably, the generative performance of the model remains intact — downstream forecasters trained on synthetic samples from TSDiff outperform forecasters that are trained on samples from other state-of-the-art generative time series models, occasionally even outperforming models trained on real data (synthesize)",
    "keywords": [],
    "checked": true,
    "id": "7d9fcc512c425c2deb9c24be25518cfd3dfe0491",
    "semantic_title": "predict, refine, synthesize: self-guiding diffusion models for probabilistic time series forecasting",
    "citation_count": 2,
    "authors": [
      "Marcel Kollovieh",
      "Abdul Fatir Ansari",
      "Michael Bohlke-Schneider",
      "Jasper Zschiegner",
      "Hao Wang",
      "Yuyang (Bernie) Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5a33645b5c5b7a9882652526d30d0acc-Abstract-Conference.html": {
    "title": "Learning Layer-wise Equivariances Automatically using Gradients",
    "volume": "main",
    "abstract": "Convolutions encode equivariance symmetries into neural networks leading to better generalisation performance. However, symmetries provide fixed hard constraints on the functions a network can represent, need to be specified in advance, and can not be adapted. Our goal is to allow flexible symmetry constraints that can automatically be learned from data using gradients. Learning symmetry and associated weight connectivity structures from scratch is difficult for two reasons. First, it requires efficient and flexible parameterisations of layer-wise equivariances. Secondly, symmetries act as constraints and are therefore not encouraged by training losses measuring data fit. To overcome these challenges, we improve parameterisations of soft equivariance and learn the amount of equivariance in layers by optimising the marginal likelihood, estimated using differentiable Laplace approximations. The objective balances data fit and model complexity enabling layer-wise symmetry discovery in deep networks. We demonstrate the ability to automatically learn layer-wise equivariances on image classification tasks, achieving equivalent or improved performance over baselines with hard-coded symmetry",
    "keywords": [],
    "checked": true,
    "id": "a3ac0ef3a7687108e55903426b5d364ba65dc0c5",
    "semantic_title": "learning layer-wise equivariances automatically using gradients",
    "citation_count": 1,
    "authors": [
      "Tycho van der Ouderaa",
      "Alexander Immer",
      "Mark van der Wilk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5a3674849d6d6d23ac088b9a2552f323-Abstract-Conference.html": {
    "title": "PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning",
    "volume": "main",
    "abstract": "Classical federated learning (FL) enables training machine learning models without sharing data for privacy preservation, but heterogeneous data characteristic degrades the performance of the localized model. Personalized FL (PFL) addresses this by synthesizing personalized models from a global model via training on local data. Such a global model may overlook the specific information that the clients have been sampled. In this paper, we propose a novel scheme to inject personalized prior knowledge into the global model in each client, which attempts to mitigate the introduced incomplete information problem in PFL. At the heart of our proposed approach is a framework, the $\\textit{PFL with Bregman Divergence}$ (pFedBreD), decoupling the personalized prior from the local objective function regularized by Bregman divergence for greater adaptability in personalized scenarios. We also relax the mirror descent (RMD) to extract the prior explicitly to provide optional strategies. Additionally, our pFedBreD is backed up by a convergence analysis. Sufficient experiments demonstrate that our method reaches the $\\textit{state-of-the-art}$ performances on 5 datasets and outperforms other methods by up to 3.5% across 8 benchmarks. Extensive analyses verify the robustness and necessity of proposed designs. The code will be made public",
    "keywords": [],
    "checked": true,
    "id": "9873fbc01dea2ebcb018f79810e6f75eea2a4ee3",
    "semantic_title": "prior: personalized prior for reactivating the information overlooked in federated learning",
    "citation_count": 2,
    "authors": [
      "Mingjia Shi",
      "Yuhao Zhou",
      "Kai Wang",
      "Huaizheng Zhang",
      "Shudong Huang",
      "Qing Ye",
      "Jiancheng Lv"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5a5e9197ea547141b4977a5a198bbaac-Abstract-Conference.html": {
    "title": "Byzantine-Tolerant Methods for Distributed Variational Inequalities",
    "volume": "main",
    "abstract": "Robustness to Byzantine attacks is a necessity for various distributed training scenarios. When the training reduces to the process of solving a minimization problem, Byzantine robustness is relatively well-understood. However, other problem formulations, such as min-max problems or, more generally, variational inequalities, arise in many modern machine learning and, in particular, distributed learning tasks. These problems significantly differ from the standard minimization ones and, therefore, require separate consideration. Nevertheless, only one work [Abidi et al., 2022] addresses this important question in the context of Byzantine robustness. Our work makes a further step in this direction by providing several (provably) Byzantine-robust methods for distributed variational inequality, thoroughly studying their theoretical convergence, removing the limitations of the previous work, and providing numerical comparisons supporting the theoretical findings",
    "keywords": [],
    "checked": true,
    "id": "6651b72082d3c78bdd64a0bc24d08918f28765f8",
    "semantic_title": "byzantine-tolerant methods for distributed variational inequalities",
    "citation_count": 0,
    "authors": [
      "Nazarii Tupitsa",
      "Abdulla Jasem Almansoori",
      "Yanlin Wu",
      "Martin Takac",
      "Karthik Nandakumar",
      "Samuel Horváth",
      "Eduard Gorbunov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5a829e299ebc1c1615ddb09e98fb6ce8-Abstract-Conference.html": {
    "title": "Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow",
    "volume": "main",
    "abstract": "Collaborative perception can substantially boost each agent's perception ability by facilitating communication among multiple agents. However, temporal asynchrony among agents is inevitable in the real world due to communication delays, interruptions, and clock misalignments. This issue causes information mismatch during multi-agent fusion, seriously shaking the foundation of collaboration. To address this issue, we propose CoBEVFlow, an asynchrony-robust collaborative perception system based on bird's eye view (BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align asynchronous collaboration messages sent by multiple agents. To model the motion in a scene, we propose BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle asynchronous collaboration messages sent at irregular, continuous time stamps without discretization; and (ii) with BEV flow, CoBEVFlow only transports the original perceptual features, instead of generating new perceptual features, avoiding additional noises. To validate CoBEVFlow's efficacy, we create IRregular V2V(IRV2V), the first synthetic collaborative perception dataset with various temporal asynchronies that simulate different real-world scenarios. Extensive experiments conducted on both IRV2V and the real-world dataset DAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is robust in extremely asynchronous settings. The code is available at https://github.com/MediaBrain-SJTU/CoBEVFlow",
    "keywords": [],
    "checked": true,
    "id": "e4797aee1f423b449cdccf90cbd9c67b42b2202d",
    "semantic_title": "asynchrony-robust collaborative perception via bird's eye view flow",
    "citation_count": 2,
    "authors": [
      "Sizhe Wei",
      "Yuxi Wei",
      "Yue Hu",
      "Yifan Lu",
      "Yiqi Zhong",
      "Siheng Chen",
      "Ya Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5a9c1af5f76da0bd37903b6f23e96c74-Abstract-Conference.html": {
    "title": "Train 'n Trade: Foundations of Parameter Markets",
    "volume": "main",
    "abstract": "Organizations typically train large models individually. This is costly and time-consuming, particularly for large-scale foundation models. Such vertical production is known to be suboptimal. Inspired by this economic insight, we ask whether it is possible to leverage others' expertise by trading the constituent parts in models, i.e., sets of weights, as if they were market commodities. While recent advances in aligning and interpolating models suggest that doing so may be possible, a number of fundamental questions must be answered to create viable parameter markets. In this work, we address these basic questions, propose a framework containing the infrastructure necessary for market operations to take place, study strategies for exchanging parameters, and offer means for agents to monetize parameters. Excitingly, compared to agents who train siloed models from scratch, we show that it is possible to mutually gain by using the market, even in competitive settings. This suggests that the notion of parameter markets may be a useful paradigm for improving large-scale model training in the future",
    "keywords": [],
    "checked": true,
    "id": "d87f727a27b38646959b0b3b0257741d91ed90c9",
    "semantic_title": "train 'n trade: foundations of parameter markets",
    "citation_count": 0,
    "authors": [
      "Tzu-Heng Huang",
      "Harit Vishwakarma",
      "Frederic Sala"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5aad86aa2a3c00b70c71e19bc4780319-Abstract-Conference.html": {
    "title": "Relax, it doesn't matter how you get there: A new self-supervised approach for multi-timescale behavior analysis",
    "volume": "main",
    "abstract": "Unconstrained and natural behavior consists of dynamics that are complex and unpredictable, especially when trying to predict what will happen multiple steps into the future. While some success has been found in building representations of animal behavior under constrained or simplified task-based conditions, many of these models cannot be applied to free and naturalistic settings where behavior becomes increasingly hard to model. In this work, we develop a multi-task representation learning model for animal behavior that combines two novel components: (i) an action-prediction objective that aims to predict the distribution of actions over future timesteps, and (ii) a multi-scale architecture that builds separate latent spaces to accommodate short- and long-term dynamics. After demonstrating the ability of the method to build representations of both local and global dynamics in robots in varying environments and terrains, we apply our method to the MABe 2022 Multi-Agent Behavior challenge, where our model ranks first overall on both mice and fly benchmarks. In all of these cases, we show that our model can build representations that capture the many different factors that drive behavior and solve a wide range of downstream tasks",
    "keywords": [],
    "checked": true,
    "id": "ad0b83f7806bb242af7daccb8fe63d4d807d7298",
    "semantic_title": "relax, it doesn't matter how you get there: a new self-supervised approach for multi-timescale behavior analysis",
    "citation_count": 1,
    "authors": [
      "Mehdi Azabou",
      "Michael Mendelson",
      "Nauman Ahad",
      "Maks Sorokin",
      "Shantanu Thakoor",
      "Carolina Urzay",
      "Eva Dyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5aadf1e309cc03cab3ec35afb7c9d0c8-Abstract-Conference.html": {
    "title": "A Measure-Theoretic Axiomatisation of Causality",
    "volume": "main",
    "abstract": "Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of what happens when one intervenes on a system, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a causal space, consisting of a probability space along with a collection of transition probability kernels, called causal kernels, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes",
    "keywords": [],
    "checked": true,
    "id": "c5665e783aacea59b387cb9267fe98ac16b56920",
    "semantic_title": "a measure-theoretic axiomatisation of causality",
    "citation_count": 0,
    "authors": [
      "Junhyung Park",
      "Simon Buchholz",
      "Bernhard Schölkopf",
      "Krikamol Muandet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5aca18e0192b2c1300479e5b700c76a9-Abstract-Conference.html": {
    "title": "Networks are Slacking Off: Understanding Generalization Problem in Image Deraining",
    "volume": "main",
    "abstract": "Deep deraining networks consistently encounter substantial generalization issues when deployed in real-world applications, although they are successful in laboratory benchmarks. A prevailing perspective in deep learning encourages using highly complex data for training, with the expectation that richer image background content will facilitate overcoming the generalization problem. However, through comprehensive and systematic experimentation, we discover that this strategy does not enhance the generalization capability of these networks. On the contrary, it exacerbates the tendency of networks to overfit specific degradations. Our experiments reveal that better generalization in a deraining network can be achieved by simplifying the complexity of the training background images. This is because that the networks are ``slacking off'' during training, that is, learning the least complex elements in the image background and degradation to minimize training loss. When the background images are less complex than the rain streaks, the network will prioritize the background reconstruction, thereby suppressing overfitting the rain patterns and leading to improved generalization performance. Our research offers a valuable perspective and methodology for better understanding the generalization problem in low-level vision tasks and displays promising potential for practical application",
    "keywords": [],
    "checked": true,
    "id": "bf5361001178ab662f153e203c4c9c17315e4e31",
    "semantic_title": "networks are slacking off: understanding generalization problem in image deraining",
    "citation_count": 2,
    "authors": [
      "Jinjin Gu",
      "Xianzheng Ma",
      "Xiangtao Kong",
      "Yu Qiao",
      "Chao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5acf5a0ee5c17d372bfe7fdaeffd6e33-Abstract-Conference.html": {
    "title": "Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning",
    "volume": "main",
    "abstract": "With the prosperity of contrastive learning for visual representation learning (VCL), it is also adapted to the graph domain and yields promising performance. However, through a systematic study of various graph contrastive learning (GCL) methods, we observe that some common phenomena among existing GCL methods that are quite different from the original VCL methods, including 1) positive samples are not a must for GCL; 2) negative samples are not necessary for graph classification, neither for node classification when adopting specific normalization modules; 3) data augmentations have much less influence on GCL, as simple domain-agnostic augmentations (e.g., Gaussian noise) can also attain fairly good performance. By uncovering how the implicit inductive bias of GNNs works in contrastive learning, we theoretically provide insights into the above intriguing properties of GCL. Rather than directly porting existing VCL methods to GCL, we advocate for more attention toward the unique architecture of graph learning and consider its implicit influence when designing GCL methods. Code is available at https://github.com/PKU-ML/ArchitectureMattersGCL",
    "keywords": [],
    "checked": true,
    "id": "5c4d763b9aafa5d251cd87c3591df5163463ff7c",
    "semantic_title": "architecture matters: uncovering implicit mechanisms in graph contrastive learning",
    "citation_count": 0,
    "authors": [
      "Xiaojun Guo",
      "Yifei Wang",
      "Zeming Wei",
      "Yisen  Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5af741d487c5f0b08bfe56e11d1883e4-Abstract-Conference.html": {
    "title": "Text Promptable Surgical Instrument Segmentation with Vision-Language Models",
    "volume": "main",
    "abstract": "In this paper, we propose a novel text promptable surgical instrument segmentation approach to overcome challenges associated with diversity and differentiation of surgical instruments in minimally invasive surgeries. We redefine the task as text promptable, thereby enabling a more nuanced comprehension of surgical instruments and adaptability to new instrument types. Inspired by recent advancements in vision-language models, we leverage pretrained image and text encoders as our model backbone and design a text promptable mask decoder consisting of attention- and convolution-based prompting schemes for surgical instrument segmentation prediction. Our model leverages multiple text prompts for each surgical instrument through a new mixture of prompts mechanism, resulting in enhanced segmentation performance. Additionally, we introduce a hard instrument area reinforcement module to improve image feature comprehension and segmentation precision. Extensive experiments on several surgical instrument segmentation datasets demonstrate our model's superior performance and promising generalization capability. To our knowledge, this is the first implementation of a promptable approach to surgical instrument segmentation, offering significant potential for practical application in the field of robotic-assisted surgery",
    "keywords": [],
    "checked": true,
    "id": "cd51dda4f02b2294a0b61385862d4ed091076755",
    "semantic_title": "text promptable surgical instrument segmentation with vision-language models",
    "citation_count": 2,
    "authors": [
      "Zijian Zhou",
      "Oluwatosin Alabi",
      "Meng Wei",
      "Tom Vercauteren",
      "Miaojing Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5b0c0b2c2efdd736a53688ebfdc3bcdb-Abstract-Conference.html": {
    "title": "On the Consistency of Maximum Likelihood Estimation of Probabilistic Principal Component Analysis",
    "volume": "main",
    "abstract": "Probabilistic principal component analysis (PPCA) is currently one of the most used statistical tools to reduce the ambient dimension of the data. From multidimensional scaling to the imputation of missing data, PPCA has a broad spectrum of applications ranging from science and engineering to quantitative finance.\\Despite this wide applicability in various fields, hardly any theoretical guarantees exist to justify the soundness of the maximal likelihood (ML) solution for this model. In fact, it is well known that the maximum likelihood estimation (MLE) can only recover the true model parameters up to a rotation. The main obstruction is posed by the inherent identifiability nature of the PPCA model resulting from the rotational symmetry of the parameterization. To resolve this ambiguity, we propose a novel approach using quotient topological spaces and in particular, we show that the maximum likelihood solution is consistent in an appropriate quotient Euclidean space. Furthermore, our consistency results encompass a more general class of estimators beyond the MLE. Strong consistency of the ML estimate and consequently strong covariance estimation of the PPCA model have also been established under a compactness assumption",
    "keywords": [],
    "checked": true,
    "id": "fcebd1fca7c6e8894be31abba79d1da80f55c0cb",
    "semantic_title": "on the consistency of maximum likelihood estimation of probabilistic principal component analysis",
    "citation_count": 0,
    "authors": [
      "Arghya Datta",
      "Sayak Chakrabarty"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5b4a459db23e6db9be2a128380953d96-Abstract-Conference.html": {
    "title": "Similarity, Compression and Local Steps: Three Pillars of Efficient Communications for Distributed Variational Inequalities",
    "volume": "main",
    "abstract": "Variational inequalities are a broad and flexible class of problems that includes minimization, saddle point, and fixed point problems as special cases. Therefore, variational inequalities are used in various applications ranging from equilibrium search to adversarial learning. With the increasing size of data and models, today's instances demand parallel and distributed computing for real-world machine learning problems, most of which can be represented as variational inequalities. Meanwhile, most distributed approaches have a significant bottleneck -- the cost of communications. The three main techniques to reduce the total number of communication rounds and the cost of one such round are the similarity of local functions, compression of transmitted information, and local updates. In this paper, we combine all these approaches. Such a triple synergy did not exist before for variational inequalities and saddle problems, nor even for minimization problems. The methods presented in this paper have the best theoretical guarantees of communication complexity and are significantly ahead of other methods for distributed variational inequalities. The theoretical results are confirmed by adversarial learning experiments on synthetic and real datasets",
    "keywords": [],
    "checked": true,
    "id": "37d70bf3ab48ed6cf6a482ef4ac5b752bcf7591c",
    "semantic_title": "similarity, compression and local steps: three pillars of efficient communications for distributed variational inequalities",
    "citation_count": 2,
    "authors": [
      "Aleksandr Beznosikov",
      "Martin Takac",
      "Alexander Gasnikov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5b4b967d4222d87fa5b28b6ec7144058-Abstract-Conference.html": {
    "title": "Lookaround Optimizer: $k$ steps around, 1 step average",
    "volume": "main",
    "abstract": "Weight Average (WA) is an active research topic due to its simplicity in ensembling deep networks and the effectiveness in promoting generalization. Existing weight average approaches, however, are often carried out along only one training trajectory in a post-hoc manner (i.e., the weights are averaged after the entire training process is finished), which significantly degrades the diversity between networks and thus impairs the effectiveness. In this paper, inspired by weight average, we propose Lookaround, a straightforward yet effective SGD-based optimizer leading to flatter minima with better generalization. Specifically, Lookaround iterates two steps during the whole training period: the around step and the average step. In each iteration, 1) the around step starts from a common point and trains multiple networks simultaneously, each on transformed data by a different data augmentation, and 2) the average step averages these trained networks to get the averaged network, which serves as the starting point for the next iteration. The around step improves the functionality diversity while the average step guarantees the weight locality of these networks during the whole training, which is essential for WA to work. We theoretically explain the superiority of Lookaround by convergence analysis, and make extensive experiments to evaluate Lookaround on popular benchmarks including CIFAR and ImageNet with both CNNs and ViTs, demonstrating clear superiority over state-of-the-arts. Our code is available at https://github.com/Ardcy/Lookaround",
    "keywords": [],
    "checked": false,
    "id": "e35776788d031de23050caadbb22bcb6e469a526",
    "semantic_title": "lookaround optimizer: k steps around, 1 step average",
    "citation_count": 0,
    "authors": [
      "Jiangtao Zhang",
      "Shunyu Liu",
      "Jie Song",
      "Tongtian Zhu",
      "Zhengqi Xu",
      "Mingli Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5b6346a05a537d4cdb2f50323452a9fe-Abstract-Conference.html": {
    "title": "The Quantization Model of Neural Scaling",
    "volume": "main",
    "abstract": "We propose the Quantization Model of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the Quantization Hypothesis, where network knowledge and skills are \"quantized\" into discrete chunks (quanta). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model gradients, we automatically decompose model behavior into a diverse set of skills (quanta). We tentatively find that the frequency at which these quanta are used in the training distribution roughly follows a power law corresponding with the empirical scaling exponent for language models, a prediction of our theory",
    "keywords": [],
    "checked": true,
    "id": "9e4980cb927b803d375c5796f4a2eb3a7fe0555d",
    "semantic_title": "the quantization model of neural scaling",
    "citation_count": 23,
    "authors": [
      "Eric Michaud",
      "Ziming Liu",
      "Uzay Girit",
      "Max Tegmark"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5b755cf5598a4324d253025e1fbbba52-Abstract-Conference.html": {
    "title": "$\\varepsilon$-fractional core stability in Hedonic Games",
    "volume": "main",
    "abstract": "Hedonic Games (HGs) are a classical framework modeling coalition formation of strategic agents guided by their individual preferences. According to these preferences, it is desirable that a coalition structure (i.e. a partition of agents into coalitions) satisfies some form of stability. The most well-known and natural of such notions is arguably core-stability. Informally, a partition is core-stable if no subset of agents would like to deviate by regrouping in a so-called core-blocking coalition. Unfortunately, core-stable partitions seldom exist and even when they do, it is often computationally intractable to find one. To circumvent these problems, we propose the notion of $\\varepsilon$-fractional core-stability, where at most an $\\varepsilon$-fraction of all possible coalitions is allowed to core-block. It turns out that such a relaxation may guarantee both existence and polynomial-time computation. Specifically, we design efficient algorithms returning an $\\varepsilon$-fractional core-stable partition, with $\\varepsilon$ exponentially decreasing in the number of agents, for two fundamental classes of HGs: Simple Fractional and Anonymous. From a probabilistic point of view, being the definition of $\\varepsilon$-fractional core equivalent to requiring that uniformly sampled coalitions core-block with probability lower than $\\varepsilon$, we further extend the definition to handle more complex sampling distributions. Along this line, when valuations have to be learned from samples in a PAC-learning fashion, we give positive and negative results on which distributions allow the efficient computation of outcomes that are $\\varepsilon$-fractional core-stable with arbitrarily high confidence",
    "keywords": [],
    "checked": false,
    "id": "8b46e109a8ef73152a066913614d837f3b86bdae",
    "semantic_title": "ε-fractional core stability in hedonic games",
    "citation_count": 0,
    "authors": [
      "Simone Fioravanti",
      "Michele Flammini",
      "Bojana Kodric",
      "Giovanna Varricchio"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5b84864ff8474fd742c66f219b2eaac1-Abstract-Conference.html": {
    "title": "Semi-Supervised Domain Generalization with Known and Unknown Classes",
    "volume": "main",
    "abstract": "Semi-Supervised Domain Generalization (SSDG) aims to learn a model that is generalizable to an unseen target domain with only a few labels, and most existing SSDG methods assume that unlabeled training and testing samples are all known classes. However, a more realistic scenario is that known classes may be mixed with some unknown classes in unlabeled training and testing data. To deal with such a scenario, we propose the Class-Wise Adaptive Exploration and Exploitation (CWAEE) method. In particular, we explore unlabeled training data by using one-vs-rest classifiers and class-wise adaptive thresholds to detect known and unknown classes, and exploit them by adopting consistency regularization on augmented samples based on Fourier Transformation to improve the unseen domain generalization. The experiments conducted on real-world datasets verify the effectiveness and superiority of our method",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhang",
      "Ji-Fu Li",
      "Wei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5ba11de4c74548071899cf41dec078bf-Abstract-Conference.html": {
    "title": "When Do Graph Neural Networks Help with Node Classification? Investigating the Homophily Principle on Node Distinguishability",
    "volume": "main",
    "abstract": "Homophily principle, i.e., nodes with the same labels are more likely to be connected, has been believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks on node classification tasks. Recent research suggests that, even in the absence of homophily, the advantage of GNNs still exists as long as nodes from the same class share similar neighborhood patterns. However, this argument only considers intra-class Node Distinguishability (ND) but neglects inter-class ND, which provides incomplete understanding of homophily on GNNs. In this paper, we first demonstrate such deficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and study ND deeply, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and define two metrics, Probabilistic Bayes Error (PBE) and negative generalized Jeffreys divergence, to quantify ND. With the metrics, we visualize and analyze how graph filters, node degree distributions and class variances influence ND, and investigate the combined effect of intra- and inter-class ND. Besides, we discovered the mid-homophily pitfall, which occurs widely in graph datasets. Furthermore, we verified that, in real-work tasks, the superiority of GNNs is indeed closely related to both intra- and inter-class ND regardless of homophily levels. Grounded in this observation, we propose a new hypothesis-testing based performance metric beyond homophily, which is non-linear, feature-based and can provide statistical threshold value for GNNs' the superiority. Experiments indicate that it is significantly more effective than the existing homophily metrics on revealing the advantage and disadvantage of graph-aware modes on both synthetic and benchmark real-world datasets",
    "keywords": [],
    "checked": false,
    "id": "d6b5fa7e88c0740654529362695612a797df4b5a",
    "semantic_title": "when do graph neural networks help with node classification: investigating the homophily principle on node distinguishability",
    "citation_count": 14,
    "authors": [
      "Sitao Luan",
      "Chenqing Hua",
      "Minkai Xu",
      "Qincheng Lu",
      "Jiaqi Zhu",
      "Xiao-Wen Chang",
      "Jie Fu",
      "Jure Leskovec",
      "Doina Precup"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5bacb12bf81e98e2ee0eed953a23c656-Abstract-Conference.html": {
    "title": "(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy",
    "volume": "main",
    "abstract": "We derive a new, (almost) guaranteed upper bound on the error of deep neural networks under distribution shift using unlabeled test data. Prior methods are either vacuous in practice or accurate on average but heavily underestimate error for a sizeable fraction of shifts. In particular, the latter only give guarantees based on complex continuous measures such as test calibration, which cannot be identified without labels, and are therefore unreliable. Instead, our bound requires a simple, intuitive condition which is well justified by prior empirical works and holds in practice effectively 100\\% of the time. The bound is inspired by $\\mathcal{H}\\Delta\\mathcal{H}$-divergence but is easier to evaluate and substantially tighter, consistently providing non-vacuous test error upper bounds. Estimating the bound requires optimizing one multiclass classifier to disagree with another, for which some prior works have used sub-optimal proxy losses; we devise a \"disagreement loss\" which is theoretically justified and performs better in practice. We expect this loss can serve as a drop-in replacement for future methods which require maximizing multiclass disagreement. Across a wide range of natural and synthetic distribution shift benchmarks, our method gives valid error bounds while achieving average accuracy comparable to—though not better than—competitive estimation baselines",
    "keywords": [],
    "checked": true,
    "id": "0cd1becc6514591c04cbe38f1ad4896410810a31",
    "semantic_title": "(almost) provable error bounds under distribution shift via disagreement discrepancy",
    "citation_count": 1,
    "authors": [
      "Elan Rosenfeld",
      "Saurabh Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5bc3356e0fa1753fff7e8d6628e71b22-Abstract-Conference.html": {
    "title": "Schema-learning and rebinding as mechanisms of in-context learning and emergence",
    "volume": "main",
    "abstract": "In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at different levels of overparameterization, suggesting that overparameterization helps in learning more complex template (schema) circuits. By showing how ICL can be achieved with small models and datasets, we open up a path to novel architectures, and take a vital step towards a more general understanding of the mechanics behind this important capability",
    "keywords": [],
    "checked": true,
    "id": "787f3c102342efb14e3691d310e77ab3c07b5343",
    "semantic_title": "schema-learning and rebinding as mechanisms of in-context learning and emergence",
    "citation_count": 5,
    "authors": [
      "Sivaramakrishnan Swaminathan",
      "Antoine Dedieu",
      "Rajkumar Vasudeva Raju",
      "Murray Shanahan",
      "Miguel Lazaro-Gredilla",
      "Dileep George"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5bd9fbb3a5a985f80c16ddd0ec1dfc43-Abstract-Conference.html": {
    "title": "CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models",
    "volume": "main",
    "abstract": "Driven by significant improvements in architectural design and training pipelines, computer visionhas recently experienced dramatic progress in terms of accuracy on classic benchmarks such as ImageNet. These highly-accurate models are challenging to deploy, as they appear harder to compress using standard techniques such as pruning. We address this issue by introducing the Correlation Aware Pruner (CAP), a new unstructured pruning framework which significantly pushes the compressibility limits for state-of-the-art architectures.Our method is based on two technical advancements: a new theoretically-justified pruner, which can handle complex weight correlations accurately and efficiently during the pruning process itself, and an efficient finetuning procedure for post-compression recovery. We validate our approach via extensive experiments on several modern vision models such as Vision Transformers (ViT), modern CNNs, and ViT-CNN hybrids, showing for the first time that these can be pruned to high sparsity levels (e.g. $\\geq 75$%) with low impact on accuracy ($\\leq 1$% relative drop). Our approach is also compatible with structured pruning and quantization, and can lead to practical speedups of 1.5 to 2.4x without accuracy loss. To further showcase CAP's accuracy and scalability, we use it to show for the first time that extremely-accurate large vision models, trained via self-supervised techniques, can also be pruned to moderate sparsities, with negligible accuracy loss",
    "keywords": [],
    "checked": true,
    "id": "3137c3ae4d21cf19d1bec8648f5f2935b5a3378e",
    "semantic_title": "cap: correlation-aware pruning for highly-accurate sparse vision models",
    "citation_count": 1,
    "authors": [
      "Denis Kuznedelev",
      "Eldar Kurtić",
      "Elias Frantar",
      "Dan Alistarh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5be3783ea9d43d7add5409c101d87d83-Abstract-Conference.html": {
    "title": "Your representations are in the network: composable and parallel adaptation for large scale models",
    "volume": "main",
    "abstract": "We present a framework for transfer learning that efficiently adapts a large base-model by learning lightweight cross-attention modules attached to its intermediate activations.We name our approach InCA (Introspective-Cross-Attention) and show that it can efficiently survey a network's representations and identify strong performing adapter models for a downstream task.During training, InCA enables training numerous adapters efficiently and in parallel, isolated from the frozen base model. On the ViT-L/16 architecture, our experiments show that a single adapter, 1.3% of the full model, is able to reach full fine-tuning accuracy on average across 11 challenging downstream classification tasks.Compared with other forms of parameter-efficient adaptation, the isolated nature of the InCA adaptation is computationally desirable for large-scale models. For instance, we adapt ViT-G/14 (1.8B+ parameters) quickly with 20+ adapters in parallel on a single V100 GPU (76% GPU memory reduction) and exhaustively identify its most useful representations.We further demonstrate how the adapters learned by InCA can be incrementally modified or combined for flexible learning scenarios and our approach achieves state of the art performance on the ImageNet-to-Sketch multi-task benchmark",
    "keywords": [],
    "checked": true,
    "id": "9ea9236895c14af682e6422df05e55b6efd950f0",
    "semantic_title": "your representations are in the network: composable and parallel adaptation for large scale models",
    "citation_count": 1,
    "authors": [
      "Yonatan Dukler",
      "Alessandro Achille",
      "Hao Yang",
      "Varsha Vivek",
      "Luca Zancato",
      "Benjamin Bowman",
      "Avinash Ravichandran",
      "Charless Fowlkes",
      "Ashwin Swaminathan",
      "Stefano Soatto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5bed8703db85ab27dc32f6a42f8fbdb6-Abstract-Conference.html": {
    "title": "Learning Energy-based Model via Dual-MCMC Teaching",
    "volume": "main",
    "abstract": "This paper studies the fundamental learning problem of the energy-based model (EBM). Learning the EBM can be achieved using the maximum likelihood estimation (MLE), which typically involves the Markov Chain Monte Carlo (MCMC) sampling, such as the Langevin dynamics. However, the noise-initialized Langevin dynamics can be challenging in practice and hard to mix. This motivates the exploration of joint training with the generator model where the generator model serves as a complementary model to bypass MCMC sampling. However, such a method can be less accurate than the MCMC and result in biased EBM learning. While the generator can also serve as an initializer model for better MCMC sampling, its learning can be biased since it only matches the EBM and has no access to empirical training examples. Such biased generator learning may limit the potential of learning the EBM. To address this issue, we present a joint learning framework that interweaves the maximum likelihood learning algorithm for both the EBM and the complementary generator model. In particular, the generator model is learned by MLE to match both the EBM and the empirical data distribution, making it a more informative initializer for MCMC sampling of EBM. Learning generator with observed examples typically requires inference of the generator posterior. To ensure accurate and efficient inference, we adopt the MCMC posterior sampling and introduce a complementary inference model to initialize such latent MCMC sampling. We show that three separate models can be seamlessly integrated into our joint framework through two (dual-) MCMC teaching, enabling effective and efficient EBM learning",
    "keywords": [],
    "checked": true,
    "id": "73c65c359a47bbf9869336c792a52af0b7db8cc0",
    "semantic_title": "learning energy-based model via dual-mcmc teaching",
    "citation_count": 0,
    "authors": [
      "Jiali Cui",
      "Tian Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5bf234ecf83cd77bc5b77a24ba9338b0-Abstract-Conference.html": {
    "title": "Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex",
    "volume": "main",
    "abstract": "One of the most impactful findings in computational neuroscience over the past decade is that the object recognition accuracy of deep neural networks (DNNs) correlates with their ability to predict neural responses to natural images in the inferotemporal (IT) cortex. This discovery supported the long-held theory that object recognition is a core objective of the visual cortex, and suggested that more accurate DNNs would serve as better models of IT neuron responses to images. Since then, deep learning has undergone a revolution of scale: billion parameter-scale DNNs trained on billions of images are rivaling or outperforming humans at visual tasks including object recognition. Have today's DNNs become more accurate at predicting IT neuron responses to images as they have grown more accurate at object recognition?Surprisingly, across three independent experiments, we find that this is not the case. DNNs have become progressively worse models of IT as their accuracy has increased on ImageNet. To understand why DNNs experience this trade-off and evaluate if they are still an appropriate paradigm for modeling the visual system, we turn to recordings of IT that capture spatially resolved maps of neuronal activity elicited by natural images. These neuronal activity maps reveal that DNNs trained on ImageNet learn to rely on different visual features than those encoded by IT and that this problem worsens as their accuracy increases. We successfully resolved this issue with the neural harmonizer, a plug-and-play training routine for DNNs that aligns their learned representations with humans. Our results suggest that harmonized DNNs break the trade-off between ImageNet accuracy and neural prediction accuracy that assails current DNNs and offer a path to more accurate models of biological vision. Our work indicates that the standard approach for modeling IT with task-optimized DNNs needs revision, and other biological constraints, including human psychophysics data, are needed to accurately reverse-engineer the visual cortex",
    "keywords": [],
    "checked": true,
    "id": "c44bdf4ff085f577a7cdd9a1cebf4af0b4eb0c82",
    "semantic_title": "performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex",
    "citation_count": 5,
    "authors": [
      "Drew Linsley",
      "Ivan F Rodriguez Rodriguez",
      "Thomas FEL",
      "Michael Arcaro",
      "Saloni Sharma",
      "Margaret Livingstone",
      "Thomas Serre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5bf40077b2bac53399676d33d564ef58-Abstract-Conference.html": {
    "title": "Private Federated Frequency Estimation: Adapting to the Hardness of the Instance",
    "volume": "main",
    "abstract": "In federated frequency estimation (FFE), multiple clients work together to estimate the frequency of their local data by communicating with a server, while maintaining the security constraint of $\\mathtt{secsum}$ where the server can only access the sum of client-held vectors. For FFE with a single communication round, it is known that count sketch is nearly information-theoretically optimal [Chen et al., 2022]. However, when multiple communication rounds are allowed, we propose a new sketch algorithm that is provably more accurate than a naive adaptation of count sketch. Furthermore, we show that both our sketch algorithm and count sketch can achieve better accuracy when the problem instance is simpler. Therefore, we propose a two-phase approach to enable the use of a smaller sketch size for simpler problems. Finally, we provide mechanisms to make our proposed algorithm differentially private. We verify the performance of our methods through experiments conducted on real datasets",
    "keywords": [],
    "checked": true,
    "id": "25d5a53456282aad4e096f67847008f3d042f37c",
    "semantic_title": "private federated frequency estimation: adapting to the hardness of the instance",
    "citation_count": 2,
    "authors": [
      "Jingfeng Wu",
      "Wennan Zhu",
      "Peter Kairouz",
      "Vladimir Braverman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5c1863f711c721648387ac2ef745facb-Abstract-Conference.html": {
    "title": "On Class Distributions Induced by Nearest Neighbor Graphs for Node Classification of Tabular Data",
    "volume": "main",
    "abstract": "Researchers have used nearest neighbor graphs to transform classical machine learning problems on tabular data into node classification tasks to solve with graph representation learning methods. Such artificial structures often reflect the homophily assumption, believed to be a key factor in the performances of deep graph networks. In light of recent results demystifying these beliefs, we introduce a theoretical framework to understand the benefits of Nearest Neighbor (NN) graphs when a graph structure is missing. We formally analyze the Cross-Class Neighborhood Similarity (CCNS), used to empirically evaluate the usefulness of structures, in the context of nearest neighbor graphs. Moreover, we study the class separability induced by deep graph networks on a k-NN graph. Motivated by the theory, our quantitative experiments demonstrate that, under full supervision, employing a k-NN graph offers no benefits compared to a structure-agnostic baseline. Qualitative analyses suggest that our framework is good at estimating the CCNS and hint at k-NN graphs never being useful for such classification tasks under full supervision, thus advocating for the study of alternative graph construction techniques in combination with deep graph networks",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Errica"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5c20c00504e0c049ec2370d0cceaf3c4-Abstract-Conference.html": {
    "title": "VRA: Variational Rectified Activation for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is critical to building reliable machine learning systems in the open world. Researchers have proposed various strategies to reduce model overconfidence on OOD data. Among them, ReAct is a typical and effective technique to deal with model overconfidence, which truncates high activations to increase the gap between in-distribution and OOD. Despite its promising results, is this technique the best choice? To answer this question, we leverage the variational method to find the optimal operation and verify the necessity of suppressing abnormally low and high activations and amplifying intermediate activations in OOD detection, rather than focusing only on high activations like ReAct. This motivates us to propose a novel technique called ``Variational Rectified Activation (VRA)'', which simulates these suppression and amplification operations using piecewise functions. Experimental results on multiple benchmark datasets demonstrate that our method outperforms existing post-hoc strategies. Meanwhile, VRA is compatible with different scoring functions and network architectures. Our code is available at https://github.com/zeroQiaoba/VRA",
    "keywords": [],
    "checked": true,
    "id": "f911f3b51fcc88f2240def8f38ed8dff1da2e605",
    "semantic_title": "vra: variational rectified activation for out-of-distribution detection",
    "citation_count": 2,
    "authors": [
      "Mingyu Xu",
      "Zheng Lian",
      "Bin Liu",
      "Jianhua Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5c25c15b5b2fd386ab188a918e54c7d5-Abstract-Conference.html": {
    "title": "Variational Gaussian processes for linear inverse problems",
    "volume": "main",
    "abstract": "By now Bayesian methods are routinely used in practice for solving inverse problems. In inverse problems the parameter or signal of interest is observed only indirectly, as an image of a given map, and the observations are typically further corrupted with noise. Bayes offers a natural way to regularize these problems via the prior distribution and provides a probabilistic solution, quantifying the remaining uncertainty in the problem. However, the computational costs of standard, sampling based Bayesian approaches can be overly large in such complex models. Therefore, in practice variational Bayes is becoming increasingly popular. Nevertheless, the theoretical understanding of these methods is still relatively limited, especially in context of inverse problems.In our analysis we investigate variational Bayesian methods for Gaussian process priors to solve linear inverse problems. We consider both mildly and severely ill-posed inverse problems and work with the popular inducing variable variational Bayes approach proposed by Titsias [Titsias, 2009]. We derive posterior contraction rates for the variational posterior in general settings and show that the minimax estimation rate can be attained by correctly tunned procedures. As specific examples we consider a collection of inverse problems including the heat equation, Volterra operator and Radon transform and inducing variable methods based on population and empirical spectral features",
    "keywords": [],
    "checked": true,
    "id": "de22e455cacc73ebbbd560b2f2fa2ef555e27b5c",
    "semantic_title": "variational gaussian processes for linear inverse problems",
    "citation_count": 1,
    "authors": [
      "Thibault RANDRIANARISOA",
      "Botond Szabo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5c46ae130105fa012da0446126c01d1d-Abstract-Conference.html": {
    "title": "Self-Supervised Learning with Lie Symmetries for Partial Differential Equations",
    "volume": "main",
    "abstract": "Machine learning for differential equations paves the way for computationally efficient alternatives to numerical solvers, with potentially broad impacts in science and engineering. Though current algorithms typically require simulated training data tailored to a given setting, one may instead wish to learn useful information from heterogeneous sources, or from real dynamical systems observations that are messy or incomplete. In this work, we learn general-purpose representations of PDEs from heterogeneous data by implementing joint embedding methods for self-supervised learning (SSL), a framework for unsupervised representation learning that has had notable success in computer vision. Our representation outperforms baseline approaches to invariant tasks, such as regressing the coefficients of a PDE, while also improving the time-stepping performance of neural solvers. We hope that our proposed methodology will prove useful in the eventual development of general-purpose foundation models for PDEs",
    "keywords": [],
    "checked": true,
    "id": "5dc01119b2e4911b0c1ae61233e36efe78588aed",
    "semantic_title": "self-supervised learning with lie symmetries for partial differential equations",
    "citation_count": 5,
    "authors": [
      "Grégoire Mialon",
      "Quentin Garrido",
      "Hannah Lawrence",
      "Danyal Rehman",
      "Yann LeCun",
      "Bobak Kiani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5c5bc3553815adb4d1a8a5b8701e41a9-Abstract-Conference.html": {
    "title": "TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery",
    "volume": "main",
    "abstract": "Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs",
    "keywords": [],
    "checked": true,
    "id": "b40863427d4d2f5440ece176f4b8e6cec75b7d6e",
    "semantic_title": "tempme: towards the explainability of temporal graph neural networks via motif discovery",
    "citation_count": 1,
    "authors": [
      "Jialin Chen",
      "Rex Ying"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5c7c66dfc9f93f0c738947f3b1c13832-Abstract-Conference.html": {
    "title": "Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation",
    "volume": "main",
    "abstract": "Whittle index policy is a heuristic to the intractable restless multi-armed bandits (RMAB) problem. Although it is provably asymptotically optimal, finding Whittle indices remains difficult. In this paper, we present Neural-Q-Whittle, a Whittle index based Q-learning algorithm for RMAB with neural network function approximation, which is an example of nonlinear two-timescale stochastic approximation with Q-function values updated on a faster timescale and Whittle indices on a slower timescale. Despite the empirical success of deep Q-learning, the non-asymptotic convergence rate of Neural-Q-Whittle, which couples neural networks with two-timescale Q-learning largely remains unclear. This paper provides a finite-time analysis of Neural-Q-Whittle, where data are generated from a Markov chain, and Q-function is approximated by a ReLU neural network. Our analysis leverages a Lyapunov drift approach to capture the evolution of two coupled parameters, and the nonlinearity in value function approximation further requires us to characterize the approximation error. Combing these provide Neural-Q-Whittle with $\\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations",
    "keywords": [],
    "checked": true,
    "id": "4e240c739148a0dcf286dce5604f08247294347c",
    "semantic_title": "finite-time analysis of whittle index based q-learning for restless multi-armed bandits with neural network function approximation",
    "citation_count": 3,
    "authors": [
      "GUOJUN XIONG",
      "Jian Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5cad96c4433955a2e76749ee74a424f5-Abstract-Conference.html": {
    "title": "Adapting to Continuous Covariate Shift via Online Density Ratio Estimation",
    "volume": "main",
    "abstract": "Dealing with distribution shifts is one of the central challenges for modern machine learning. One fundamental situation is the covariate shift, where the input distributions of data change from the training to testing stages while the input-conditional output distribution remains unchanged. In this paper, we initiate the study of a more challenging scenario --- continuous covariate shift --- in which the test data appear sequentially, and their distributions can shift continuously. Our goal is to adaptively train the predictor such that its prediction risk accumulated over time can be minimized. Starting with the importance-weighted learning, we theoretically show the method works effectively if the time-varying density ratios of test and train inputs can be accurately estimated. However, existing density ratio estimation methods would fail due to data scarcity at each time step. To this end, we propose an online density ratio estimation method that can appropriately reuse historical information. Our method is proven to perform well by enjoying a dynamic regret bound, which finally leads to an excess risk guarantee for the predictor. Empirical results also validate the effectiveness",
    "keywords": [],
    "checked": true,
    "id": "905f8c3324ff1867b1edbd76737b29b08aecf35d",
    "semantic_title": "adapting to continuous covariate shift via online density ratio estimation",
    "citation_count": 2,
    "authors": [
      "Yu-Jie Zhang",
      "Zhen-Yu Zhang",
      "Peng Zhao",
      "Masashi Sugiyama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5cebc89b113920dbff7c79854ba765a3-Abstract-Conference.html": {
    "title": "Hierarchical Integration Diffusion Model for Realistic Image Deblurring",
    "volume": "main",
    "abstract": "Diffusion models (DMs) have recently been introduced in image deblurring and exhibited promising performance, particularly in terms of details reconstruction. However, the diffusion model requires a large number of inference iterations to recover the clean image from pure Gaussian noise, which consumes massive computational resources. Moreover, the distribution synthesized by the diffusion model is often misaligned with the target results, leading to restrictions in distortion-based metrics. To address the above issues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), for realistic image deblurring. Specifically, we perform the DM in a highly compacted latent space to generate the prior feature for the deblurring process. The deblurring process is implemented by a regression-based method to obtain better distortion accuracy. Meanwhile, the highly compact latent space ensures the efficiency of the DM. Furthermore, we design the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios. Comprehensive experiments on synthetic and real-world blur datasets demonstrate that our HI-Diff outperforms state-of-the-art methods. Code and trained models are available at https://github.com/zhengchen1999/HI-Diff",
    "keywords": [],
    "checked": true,
    "id": "ea2feb30a758519672e876bd1ff6f05b859e308e",
    "semantic_title": "hierarchical integration diffusion model for realistic image deblurring",
    "citation_count": 11,
    "authors": [
      "Zheng Chen",
      "Yulun Zhang",
      "Ding Liu",
      "bin xia",
      "Jinjin Gu",
      "Linghe Kong",
      "Xin Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5cf93940e37f7a7877cd57b6dba6b7ab-Abstract-Conference.html": {
    "title": "Efficient Beam Tree Recursion",
    "volume": "main",
    "abstract": "Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as an extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although better than previous approaches in terms of memory usage, BT-RvNN can be still exorbitantly expensive. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10-16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$ into a token contextualizer of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models. Our code is available at the link: https://github.com/JRC1995/BeamRecursionFamily",
    "keywords": [],
    "checked": true,
    "id": "9e3e56957e249cdebdd8673fd1174980ed694560",
    "semantic_title": "efficient beam tree recursion",
    "citation_count": 3,
    "authors": [
      "Jishnu Ray Chowdhury",
      "Cornelia Caragea"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d087955ee13fe9a7402eedec879b9c3-Abstract-Conference.html": {
    "title": "Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data",
    "volume": "main",
    "abstract": "We propose a learning problem involving adapting a pre-trained source model to the target domain for classifying all classes that appeared in the source data, using target data that covers only a partial label space. This problem is practical, as it is unrealistic for the target end-users to collect data for all classes prior to adaptation. However, it has received limited attention in the literature. To shed light on this issue, we construct benchmark datasets and conduct extensive experiments to uncover the inherent challenges. We found a dilemma --- on the one hand, adapting to the new target domain is important to claim better performance; on the other hand, we observe that preserving the classification accuracy of classes missing in the target adaptation data is highly challenging, let alone improving them. To tackle this, we identify two key directions: 1) disentangling domain gradients from classification gradients, and 2) preserving class relationships. We present several effective solutions that maintain the accuracy of the missing classes and enhance the overall performance, establishing solid baselines for holistic transfer of pre-trained models with partial target data",
    "keywords": [],
    "checked": true,
    "id": "1ee91bda73e023983678058a43134527934d688c",
    "semantic_title": "holistic transfer: towards non-disruptive fine-tuning with partial target data",
    "citation_count": 0,
    "authors": [
      "Cheng-Hao Tu",
      "Hong-You Chen",
      "Zheda Mai",
      "Jike Zhong",
      "Vardaan Pahuja",
      "Tanya Berger-Wolf",
      "Song Gao",
      "Charles Stewart",
      "Yu Su",
      "Wei-Lun (Harry) Chao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d1233f819202ade06023346df80a6d2-Abstract-Conference.html": {
    "title": "Rethinking Semi-Supervised Imbalanced Node Classification from Bias-Variance Decomposition",
    "volume": "main",
    "abstract": "This paper introduces a new approach to address the issue of class imbalance in graph neural networks (GNNs) for learning on graph-structured data. Our approach integrates imbalanced node classification and Bias-Variance Decomposition, establishing a theoretical framework that closely relates data imbalance to model variance. We also leverage graph augmentation technique to estimate the variance and design a regularization term to alleviate the impact of imbalance. Exhaustive tests are conducted on multiple benchmarks, including naturally imbalanced datasets and public-split class-imbalanced datasets, demonstrating that our approach outperforms state-of-the-art methods in various imbalanced scenarios. This work provides a novel theoretical perspective for addressing the problem of imbalanced node classification in GNNs",
    "keywords": [],
    "checked": true,
    "id": "091a1eb4e38911ce97dee2c88e5fbe18f42da6b8",
    "semantic_title": "rethinking semi-supervised imbalanced node classification from bias-variance decomposition",
    "citation_count": 0,
    "authors": [
      "Divin Yan",
      "Gengchen Wei",
      "Chen Yang",
      "Shengzhong Zhang",
      "zengfeng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d1a382162cb5ed326f1d3dbbfac4c82-Abstract-Conference.html": {
    "title": "Practical Equivariances via Relational Conditional Neural Processes",
    "volume": "main",
    "abstract": "Conditional Neural Processes (CNPs) are a class of metalearning models popular for combining the runtime efficiency of amortized inference with reliable uncertainty quantification. Many relevant machine learning tasks, such as in spatio-temporal modeling, Bayesian Optimization and continuous control, inherently contain equivariances – for example to translation – which the model can exploit for maximal performance. However, prior attempts to include equivariances in CNPs do not scale effectively beyond two input dimensions. In this work, we propose Relational Conditional Neural Processes (RCNPs), an effective approach to incorporate equivariances into any neural process model. Our proposed method extends the applicability and impact of equivariant neural processes to higher dimensions. We empirically demonstrate the competitive performance of RCNPs on a large array of tasks naturally containing equivariances",
    "keywords": [],
    "checked": true,
    "id": "0899e419cf8af94bfa2954d5227cf026f52573bf",
    "semantic_title": "practical equivariances via relational conditional neural processes",
    "citation_count": 0,
    "authors": [
      "Daolang Huang",
      "Manuel Haussmann",
      "Ulpu Remes",
      "ST John",
      "Grégoire Clarté",
      "Kevin Sebastian Luck",
      "Samuel Kaski",
      "Luigi Acerbi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d4834a159f1547b267a05a4e2b7cf5e-Abstract-Conference.html": {
    "title": "FourierHandFlow: Neural 4D Hand Representation Using Fourier Query Flow",
    "volume": "main",
    "abstract": "Recent 4D shape representations model continuous temporal evolution of implicit shapes by (1) learning query flows without leveraging shape and articulation priors or (2) decoding shape occupancies separately for each time value. Thus, they do not effectively capture implicit correspondences between articulated shapes or regularize jittery temporal deformations. In this work, we present FourierHandFlow, which is a spatio-temporally continuous representation for human hands that combines a 3D occupancy field with articulation-aware query flows represented as Fourier series. Given an input RGB sequence, we aim to learn a fixed number of Fourier coefficients for each query flow to guarantee smooth and continuous temporal shape dynamics. To effectively model spatio-temporal deformations of articulated hands, we compose our 4D representation based on two types of Fourier query flow: (1) pose flow that models query dynamics influenced by hand articulation changes via implicit linear blend skinning and (2) shape flow that models query-wise displacement flow. In the experiments, our method achieves state-of-the-art results on video-based 4D reconstruction while being computationally more efficient than the existing 3D/4D implicit shape representations. We additionally show our results on motion inter- and extrapolation and texture transfer using the learned correspondences of implicit shapes. To the best of our knowledge, FourierHandFlow is the first neural 4D continuous hand representation learned from RGB videos. The code will be publicly accessible",
    "keywords": [],
    "checked": true,
    "id": "f44efccc0ea660a424ba305aaaa2ddf45c61fb70",
    "semantic_title": "fourierhandflow: neural 4d hand representation using fourier query flow",
    "citation_count": 0,
    "authors": [
      "Jihyun Lee",
      "Junbong Jang",
      "Donghwan Kim",
      "Minhyuk Sung",
      "Tae-Kyun Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d4cd12ef6efedbf26b69b410f1f7d67-Abstract-Conference.html": {
    "title": "Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms",
    "volume": "main",
    "abstract": "Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations before actual safety violation to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian process to ensure safety with a deep RL algorithm to maximize the reward. Finally, we demonstrate that our proposed algorithm achieves better performance than state-of-the-art algorithms on grid-world and Safety Gym benchmarks without violating any safety constraints, even during training",
    "keywords": [],
    "checked": true,
    "id": "d3d7fa13b3f9b533647ac84fc2e446ef98211b08",
    "semantic_title": "safe exploration in reinforcement learning: a generalized formulation and algorithms",
    "citation_count": 2,
    "authors": [
      "Akifumi Wachi",
      "Wataru Hashimoto",
      "Xun Shen",
      "Kazumune Hashimoto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d56e69c317429945785ede86c00b44e-Abstract-Conference.html": {
    "title": "RevColV2: Exploring Disentangled Representations in Masked Image Modeling",
    "volume": "main",
    "abstract": "Masked image modeling (MIM) has become a prevalent pre-training setup for vision foundation models and attains promising performance. Despite its success, existing MIM methods discard the decoder network during downstream applica- tions, resulting in inconsistent representations between pre-training and fine-tuning and can hamper downstream task performance. In this paper, we propose a new architecture, RevColV2, which tackles this issue by keeping the entire autoen- coder architecture during both pre-training and fine-tuning. The main body of RevColV2 contains bottom-up columns and top-down columns, between which information is reversibly propagated and gradually disentangled. Such design enables our architecture with the nice property: maintaining disentangled low-level and semantic information at the end of the network in MIM pre-training. Our experimental results suggest that a foundation model with decoupled features can achieve competitive performance across multiple downstream vision tasks such as image classification, semantic segmentation and object detection. For exam- ple, after intermediate fine-tuning on ImageNet-22K dataset, RevColV2-L attains 88.4\\% top-1 accuracy on ImageNet-1K classification and 58.6 mIoU on ADE20K semantic segmentation. With extra teacher and large scale dataset, RevColv2-L achieves 62.1 APbox on COCO detection and 60.4 mIoU on ADE20K semantic segmentation",
    "keywords": [],
    "checked": true,
    "id": "2d3ee3327e7c70196a0ea5e271e1e30da5551588",
    "semantic_title": "revcolv2: exploring disentangled representations in masked image modeling",
    "citation_count": 2,
    "authors": [
      "Qi Han",
      "Yuxuan Cai",
      "Xiangyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d570ed1708bbe19cb60f7a7aff60575-Abstract-Conference.html": {
    "title": "Mass-Producing Failures of Multimodal Systems with Language Models",
    "volume": "main",
    "abstract": "Deployed multimodal models can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures---generalizable, natural-language descriptions that describe categories of individual failures. To uncover systematic failures, MultiMon scrapes for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model to identify common categories and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g.\"ignores quantifiers'') of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g.\"a shelf with a few/many books''). Because CLIP is the backbone for most state-of-the-art multimodal models, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-driving cars. We see MultiMon as a step towards evaluation that autonomously explores the long-tail of potential system failures",
    "keywords": [],
    "checked": true,
    "id": "0aaee2b99ec6f659657658416e88ad7f4161ac7f",
    "semantic_title": "mass-producing failures of multimodal systems with language models",
    "citation_count": 9,
    "authors": [
      "Shengbang Tong",
      "Erik Jones",
      "Jacob Steinhardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d8c01de2dc698c54201c1c7d0b86974-Abstract-Conference.html": {
    "title": "STXD: Structural and Temporal Cross-Modal Distillation for Multi-View 3D Object Detection",
    "volume": "main",
    "abstract": "3D object detection (3DOD) from multi-view images is an economically appealing alternative to expensive LiDAR-based detectors, but also an extremely challenging task due to the absence of precise spatial cues. Recent studies have leveraged the teacher-student paradigm for cross-modal distillation, where a strong LiDAR-modality teacher transfers useful knowledge to a multi-view-based image-modality student. However, prior approaches have only focused on minimizing global distances between cross-modal features, which may lead to suboptimal knowledge distillation results. Based on these insights, we propose a novel structural and temporal cross-modal knowledge distillation (STXD) framework for multi-view 3DOD. First, STXD reduces redundancy of the feature components of the student by regularizing the cross-correlation of cross-modal features, while maximizing their similarities. Second, to effectively transfer temporal knowledge, STXD encodes temporal relations of features across a sequence of frames via similarity maps. Lastly, STXD also adopts a response distillation method to further enhance the quality of knowledge distillation at the output-level. Our extensive experiments demonstrate that STXD significantly improves the NDS and mAP of the based student detectors by 2.8%~4.5% on the nuScenes testing dataset",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sujin Jang",
      "Dae Ung Jo",
      "Sung Ju Hwang",
      "Dongwook Lee",
      "Daehyun Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d97b7e62022c859347397f6c1e8d0f9-Abstract-Conference.html": {
    "title": "Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift",
    "volume": "main",
    "abstract": "Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or underconfident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, where training from scratch is prohibitively expensive. Finally, given the recent success of Deep Ensembles, we extend popular single-mode posterior approximations to multiple modes by the use of ensembles. While we find that ensembling single-mode approximations generally improves the generalization capability and calibration of the models by a significant margin, we also identify a failure mode of ensembles when finetuning large transformer-based language models. In this setting, variational inference based approaches such as last-layer Bayes By Backprop outperform other methods in terms of accuracy by a large margin, while modern approximate inference algorithms such as SWAG achieve the best calibration",
    "keywords": [],
    "checked": true,
    "id": "90e0eeef01318a4f20c452ec4d2f6bfbaf1a2b84",
    "semantic_title": "beyond deep ensembles: a large-scale evaluation of bayesian deep learning under distribution shift",
    "citation_count": 1,
    "authors": [
      "Florian Seligmann",
      "Philipp Becker",
      "Michael Volpp",
      "Gerhard Neumann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5da6ce80e97671b70c01a2e703b868b3-Abstract-Conference.html": {
    "title": "(S)GD over Diagonal Linear Networks: Implicit bias, Large Stepsizes and Edge of Stability",
    "volume": "main",
    "abstract": "In this paper, we investigate the impact of stochasticity and large stepsizes on the implicit regularisation of gradient descent (GD) and stochastic gradient descent (SGD) over $2$-layer diagonal linear networks. We prove the convergence of GD and SGD with macroscopic stepsizes in an overparametrised regression setting and characterise their solutions through an implicit regularisation problem. Our crisp characterisation leads to qualitative insights about the impact of stochasticity and stepsizes on the recovered solution. Specifically, we show that large stepsizes consistently benefit SGD for sparse regression problems, while they can hinder the recovery of sparse solutions for GD. These effects are magnified for stepsizes in a tight window just below the divergence threshold, in the ``edge of stability'' regime. Our findings are supported by experimental results",
    "keywords": [],
    "checked": false,
    "id": "1f1b58e92d7a5a6f70eb5b406d30183675e2a732",
    "semantic_title": "(s)gd over diagonal linear networks: implicit regularisation, large stepsizes and edge of stability",
    "citation_count": 16,
    "authors": [
      "Mathieu Even",
      "Scott Pesme",
      "Suriya Gunasekar",
      "Nicolas Flammarion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5da6d5818a156791090c875abeca3cf8-Abstract-Conference.html": {
    "title": "Optimal Preconditioning and Fisher Adaptive Langevin Sampling",
    "volume": "main",
    "abstract": "We define an optimal preconditioning for the Langevin diffusion by analytically optimizing the expected squared jumped distance. This yields as the optimal preconditioning an inverse Fisher information covariance matrix, where the covariance matrix is computed as the outer product of log target gradients averaged under the target. We apply this result to the Metropolis adjusted Langevin algorithm (MALA) and derive a computationally efficient adaptive MCMC scheme that learns the preconditioning from the history of gradients produced as the algorithm runs. We show in several experiments that the proposed algorithm is very robust in high dimensions and significantly outperforms other methods, including a closely related adaptive MALA scheme that learns the preconditioning with standard adaptive MCMC as well as the position-dependent Riemannian manifold MALA sampler",
    "keywords": [],
    "checked": true,
    "id": "0952b553f497983cbff480463dc2b6e9c8cba3f9",
    "semantic_title": "optimal preconditioning and fisher adaptive langevin sampling",
    "citation_count": 2,
    "authors": [
      "Michalis Titsias"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5e0da5da69b71349ae0bd7ad716e4bc9-Abstract-Conference.html": {
    "title": "AdaptSSR: Pre-training User Model with Augmentation-Adaptive Self-Supervised Ranking",
    "volume": "main",
    "abstract": "User modeling, which aims to capture users' characteristics or interests, heavily relies on task-specific labeled data and suffers from the data sparsity issue. Several recent studies tackled this problem by pre-training the user model on massive user behavior sequences with a contrastive learning task. Generally, these methods assume different views of the same behavior sequence constructed via data augmentation are semantically consistent, i.e., reflecting similar characteristics or interests of the user, and thus maximizing their agreement in the feature space. However, due to the diverse interests and heavy noise in user behaviors, existing augmentation methods tend to lose certain characteristics of the user or introduce noisy behaviors. Thus, forcing the user model to directly maximize the similarity between the augmented views may result in a negative transfer. To this end, we propose to replace the contrastive learning task with a new pretext task: Augmentation-Adaptive SelfSupervised Ranking (AdaptSSR), which alleviates the requirement of semantic consistency between the augmented views while pre-training a discriminative user model. Specifically, we adopt a multiple pairwise ranking loss which trains the user model to capture the similarity orders between the implicitly augmented view, the explicitly augmented view, and views from other users. We further employ an in-batch hard negative sampling strategy to facilitate model training. Moreover, considering the distinct impacts of data augmentation on different behavior sequences, we design an augmentation-adaptive fusion mechanism to automatically adjust the similarity order constraint applied to each sample based on the estimated similarity between the augmented views. Extensive experiments on both public and industrial datasets with six downstream tasks verify the effectiveness of AdaptSSR",
    "keywords": [],
    "checked": true,
    "id": "e8104e570db4699c0c85341ac5358fc3c3b3d64b",
    "semantic_title": "adaptssr: pre-training user model with augmentation-adaptive self-supervised ranking",
    "citation_count": 0,
    "authors": [
      "Yang Yu",
      "Qi Liu",
      "Kai Zhang",
      "Yuren Zhang",
      "Chao Song",
      "Min Hou",
      "Yuqing Yuan",
      "Zhihao Ye",
      "ZAIXI ZHANG",
      "Sanshi Lei Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5e11d23b18261d1b76d14da7a285fd0c-Abstract-Conference.html": {
    "title": "Anytime Model Selection in Linear Bandits",
    "volume": "main",
    "abstract": "Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\\mathrm{poly}M$) with the number of models $M$ in terms of their regret.Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\\log M$) dependence on $M$ for its regret.ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage.Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics",
    "keywords": [],
    "checked": true,
    "id": "3cdf1b5b4c758d56c44c229ce83ed5af4141318d",
    "semantic_title": "anytime model selection in linear bandits",
    "citation_count": 0,
    "authors": [
      "Parnian Kassraie",
      "Nicolas Emmenegger",
      "Andreas Krause",
      "Aldo Pacchiano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5e2217482fa75556f1970be809acd3f8-Abstract-Conference.html": {
    "title": "Towards Personalized Federated Learning via Heterogeneous Model Reassembly",
    "volume": "main",
    "abstract": "This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of using different public data and dynamically generates diverse personalized models in an automated manner",
    "keywords": [],
    "checked": true,
    "id": "99a0f0f6ada73216389307133e04e9468639fbb2",
    "semantic_title": "towards personalized federated learning via heterogeneous model reassembly",
    "citation_count": 6,
    "authors": [
      "Jiaqi Wang",
      "Xingyi Yang",
      "Suhan Cui",
      "Liwei Che",
      "Lingjuan Lyu",
      "Dongkuan (DK) Xu",
      "Fenglong Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5e5fd18f863cbe6d8ae392a93fd271c9-Abstract-Conference.html": {
    "title": "Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning",
    "volume": "main",
    "abstract": "Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction performance of event sequence models. We design LAMP, a framework that integrates a large language model in event prediction. Particularly, the language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on several challenging real-world datasets, we demonstrate that our framework---thanks to the reasoning capabilities of large language models---could significantly outperform the state-of-the-art event sequence models",
    "keywords": [],
    "checked": true,
    "id": "aefbd1cf0837b989785096921f9f04c275b88d7d",
    "semantic_title": "language models can improve event prediction by few-shot abductive reasoning",
    "citation_count": 11,
    "authors": [
      "Xiaoming Shi",
      "Siqiao Xue",
      "Kangrui Wang",
      "Fan Zhou",
      "James Zhang",
      "Jun Zhou",
      "Chenhao Tan",
      "Hongyuan Mei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5e8023f07625374c6fdf3aa08bb38e0e-Abstract-Conference.html": {
    "title": "Complexity Matters: Rethinking the Latent Space for Generative Modeling",
    "volume": "main",
    "abstract": "In generative modeling, numerous successful approaches leverage a low-dimensional latent space, e.g., Stable Diffusion models the latent space induced by an encoder and generates images through a paired decoder. Although the selection of the latent space is empirically pivotal, determining the optimal choice and the process of identifying it remain unclear. In this study, we aim to shed light on this under-explored topic by rethinking the latent space from the perspective of model complexity. Our investigation starts with the classic generative adversarial networks (GANs). Inspired by the GAN training objective, we propose a novel \"distance\" between the latent and data distributions, whose minimization coincides with that of the generator complexity. The minimizer of this distance is characterized as the optimal data-dependent latent that most effectively capitalizes on the generator's capacity. Then, we consider parameterizing such a latent distribution by an encoder network and propose a two-stage training strategy called Decoupled Autoencoder (DAE), where the encoder is only updated in the first stage with an auxiliary decoder and then frozen in the second stage while the actual decoder is being trained. DAE can improve the latent distribution and as a result, improve the generative performance. Our theoretical analyses are corroborated by comprehensive experiments on various models such as VQGAN and Diffusion Transformer, where our modifications yield significant improvements in sample quality with decreased model complexity",
    "keywords": [],
    "checked": true,
    "id": "655ce703ece0565139fccb1325f8ad30d8917c57",
    "semantic_title": "complexity matters: rethinking the latent space for generative modeling",
    "citation_count": 1,
    "authors": [
      "Tianyang Hu",
      "Fei Chen",
      "Haonan Wang",
      "Jiawei Li",
      "Wenjia Wang",
      "Jiacheng Sun",
      "Zhenguo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5e809ba53f34d9170386ebfc8b60300f-Abstract-Conference.html": {
    "title": "Riemannian stochastic optimization methods avoid strict saddle points",
    "volume": "main",
    "abstract": "Many modern machine learning applications - from online principal component analysis to covariance matrix identification and dictionary learning - can be formulated as minimization problems on Riemannian manifolds, typically solved with a Riemannian stochastic gradient method (or some variant thereof). However, in many cases of interest, the resulting minimization problem is _not_ geodesically convex, so the convergence of the chosen solver to a desirable solution - i.e., a local minimizer - is by no means guaranteed. In this paper, we study precisely this question, that is, whether stochastic Riemannian optimization algorithms are guaranteed to avoid saddle points with probability $1$. For generality, we study a family of retraction-based methods which, in addition to having a potentially much lower per-iteration cost relative to Riemannian gradient descent, include other widely used algorithms, such as natural policy gradient methods and mirror descent in ordinary convex spaces. In this general setting, we show that, under mild assumptions for the ambient manifold and the oracle providing gradient information, the policies under study avoid strict saddle points / submanifolds with probability $1$, from any initial condition. This result provides an important sanity check for the use of gradient methods on manifolds as it shows that, almost always, the end state of a stochastic Riemannian algorithm can only be a local minimizer",
    "keywords": [],
    "checked": true,
    "id": "0a0e53a0c8dfe647cfab76f8c3e5413f759a7811",
    "semantic_title": "riemannian stochastic optimization methods avoid strict saddle points",
    "citation_count": 1,
    "authors": [
      "Ya-Ping Hsieh",
      "Mohammad Reza Karimi Jaghargh",
      "Andreas Krause",
      "Panayotis Mertikopoulos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5e8309c9ca683e11672e3dbcd4b87776-Abstract-Conference.html": {
    "title": "Toward Better PAC-Bayes Bounds for Uniformly Stable Algorithms",
    "volume": "main",
    "abstract": "We give sharper bounds for uniformly stable randomized algorithms in a PAC-Bayesian framework, which improve the existing results by up to a factor of $\\sqrt{n}$ (ignoring a log factor), where $n$ is the sample size. The key idea is to bound the moment generating function of the generalization gap using concentration of weakly dependent random variables due to Bousquet et al (2020). We introduce an assumption of sub-exponential stability parameter, which allows a general treatment that we instantiate in two applications: stochastic gradient descent and randomized coordinate descent. Our results eliminate the requirement of strong convexity from previous results, and hold for non-smooth convex problems",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijia Zhou",
      "Yunwen Lei",
      "Ata Kaban"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5e84e4413268b713f0d4a1b23a9dae57-Abstract-Conference.html": {
    "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models",
    "volume": "main",
    "abstract": "Recently, growing interest has been aroused in extending the multimodal capability of large language models (LLMs), e.g., vision-language (VL) learning, which is regarded as the next milestone of artificial general intelligence. However, existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning. In this paper, we propose a novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA). Instead of using large neural networks to connect the image encoder and LLM, MMA adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models. Meanwhile, MMA is also equipped with a routing algorithm to help LLMs achieve an automatic shift between single- and multi-modal instructions without compromising their ability of natural language understanding. To validate MMA, we apply it to a recent LLM called LLaMA and term this formed large vision-language instructed model as LaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two setups, namely multimodal science question answering and multimodal dialogue. The experimental results not only demonstrate the competitive performance and the superior training efficiency of LaVIN than existing multimodal LLMs, but also confirm its great potential as a general-purpose chatbot. More importantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4 training hours with 3.8M trainable parameters, greatly confirming the effectiveness of MMA. Our code is anonymously released at: https://anonymous.4open.science/r/LaVIN--1067",
    "keywords": [],
    "checked": true,
    "id": "9c3a9b4821daa03cb5369041d59d2714329a3811",
    "semantic_title": "cheap and quick: efficient vision-language instruction tuning for large language models",
    "citation_count": 33,
    "authors": [
      "Gen Luo",
      "Yiyi Zhou",
      "Tianhe Ren",
      "Shengxin Chen",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5ebbbac62b968254093023f1c95015d3-Abstract-Conference.html": {
    "title": "Brain encoding models based on multimodal transformers can transfer across language and vision",
    "volume": "main",
    "abstract": "Encoding models have been used to assess how the human brain represents concepts in language and vision. While language and vision rely on similar concept representations, current encoding models are typically trained and tested on brain responses to each modality in isolation. Recent advances in multimodal pretraining have produced transformers that can extract aligned representations of concepts in language and vision. In this work, we used representations from multimodal transformers to train encoding models that can transfer across fMRI responses to stories and movies. We found that encoding models trained on brain responses to one modality can successfully predict brain responses to the other modality, particularly in cortical regions that represent conceptual meaning. Further analysis of these encoding models revealed shared semantic dimensions that underlie concept representations in language and vision. Comparing encoding models trained using representations from multimodal and unimodal transformers, we found that multimodal transformers learn more aligned representations of concepts in language and vision. Our results demonstrate how multimodal transformers can provide insights into the brain's capacity for multimodal processing",
    "keywords": [],
    "checked": true,
    "id": "32d0ad59162b07bf469b6889930ef1d10e66f7f8",
    "semantic_title": "brain encoding models based on multimodal transformers can transfer across language and vision",
    "citation_count": 5,
    "authors": [
      "Jerry Tang",
      "Meng Du",
      "Vy Vo",
      "VASUDEV LAL",
      "Alexander Huth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5ed5c3c846f684a54975ad7a2525199f-Abstract-Conference.html": {
    "title": "PointGPT: Auto-regressively Generative Pre-training from Point Clouds",
    "volume": "main",
    "abstract": "Large language models (LLMs) based on the generative pre-training transformer (GPT) have demonstrated remarkable effectiveness across a diverse range of downstream tasks. Inspired by the advancements of the GPT, we present PointGPT, a novel approach that extends the concept of GPT to point clouds, addressing the challenges associated with disorder properties, low information density, and task gaps. Specifically, a point cloud auto-regressive generation task is proposed to pre-train transformer models. Our method partitions the input point cloud into multiple point patches and arranges them in an ordered sequence based on their spatial proximity. Then, an extractor-generator based transformer decode, with a dual masking strategy, learns latent representations conditioned on the preceding point patches, aiming to predict the next one in an auto-regressive manner. To explore scalability and enhance performance, a larger pre-training dataset is collected. Additionally, a subsequent post-pre-training stage is introduced, incorporating a labeled hybrid dataset. Our scalable approach allows for learning high-capacity models that generalize well, achieving state-of-the-art performance on various downstream tasks. In particular, our approach achieves classification accuracies of 94.9% on the ModelNet40 dataset and 93.4% on the ScanObjectNN dataset, outperforming all other transformer models. Furthermore, our method also attains new state-of-the-art accuracies on all four few-shot learning benchmarks. Codes are available at https://github.com/CGuangyan-BIT/PointGPT",
    "keywords": [],
    "checked": true,
    "id": "128fc3e518a9616cd2780d974d32b4ef47ba5901",
    "semantic_title": "pointgpt: auto-regressively generative pre-training from point clouds",
    "citation_count": 8,
    "authors": [
      "Guangyan Chen",
      "Meiling Wang",
      "Yi Yang",
      "Kai Yu",
      "Li Yuan",
      "Yufeng Yue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5edb57c05c81d04beb716ef1d542fe9e-Abstract-Conference.html": {
    "title": "Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning",
    "volume": "main",
    "abstract": "Human reasoning can be understood as a cooperation between the intuitive, associative \"System-1'' and the deliberative, logical \"System-2''. For existing System-1-like methods in visual activity understanding, it is crucial to integrate System-2 processing to improve explainability, generalization, and data efficiency. One possible path of activity reasoning is building a symbolic system composed of symbols and rules, where one rule connects multiple symbols, implying human knowledge and reasoning abilities.Previous methods have made progress, but are defective with limited symbols from handcraft and limited rules from visual-based annotations, failing to cover the complex patterns of activities and lacking compositional generalization. To overcome the defects, we propose a new symbolic system with two ideal important properties: broad-coverage symbols and rational rules. Collecting massive human knowledge via manual annotations is expensive to instantiate this symbolic system. Instead, we leverage the recent advancement of LLMs (Large Language Models) as an approximation of the two ideal properties, i.e., Symbols from Large Language Models (Symbol-LLM). Then, given an image, visual contents from the images are extracted andchecked as symbols and activity semantics are reasoned out based on rules via fuzzy logic calculation.Our method shows superiority in extensive activity understanding tasks. Code and data are available at https://mvig-rhos.com/symbol_llm",
    "keywords": [],
    "checked": true,
    "id": "a35b7855d6dba196dd45db329afcbdec698ae718",
    "semantic_title": "symbol-llm: leverage language models for symbolic system in visual human activity reasoning",
    "citation_count": 0,
    "authors": [
      "Xiaoqian Wu",
      "Yong-Lu Li",
      "Jianhua Sun",
      "Cewu Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5ee60ca5686bbcf756e56a6c75e66f32-Abstract-Conference.html": {
    "title": "Non-Convex Bilevel Optimization with Time-Varying Objective Functions",
    "volume": "main",
    "abstract": "Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for OBO, we develop a novel analytical technique that disentangles the complex couplings between decision variables, and carefully controls the hypergradient estimation error. We show that SOBOW can achieve a sublinear bilevel local regret under mild conditions. Extensive experiments across multiple domains corroborate the effectiveness of SOBOW",
    "keywords": [],
    "checked": true,
    "id": "6f6e2139eb762f857ca1093e699de243fa0613dd",
    "semantic_title": "non-convex bilevel optimization with time-varying objective functions",
    "citation_count": 0,
    "authors": [
      "Sen Lin",
      "Daouda Sow",
      "Kaiyi Ji",
      "Yingbin Liang",
      "Ness Shroff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5eee634cb9729b8bcc2ec9f2a46a74ae-Abstract-Conference.html": {
    "title": "Online Pricing for Multi-User Multi-Item Markets",
    "volume": "main",
    "abstract": "Online pricing has been the focus of extensive research in recent years, particularly in the context of selling an item to sequentially arriving users. However, what if a provider wants to maximize revenue by selling multiple items to multiple users in each round? This presents a complex problem, as the provider must intelligently offer the items to those users who value them the most without exceeding their highest acceptable prices. In this study, we tackle this challenge by designing online algorithms that can efficiently offer and price items while learning user valuations from accept/reject feedback. We focus on three user valuation models (fixed valuations, random experiences, and random valuations) and provide algorithms with nearly-optimal revenue regret guarantees. In particular, for any market setting with $N$ users, $M$ items, and load $L$ (which roughly corresponds to the maximum number of simultaneous allocations possible), our algorithms achieve regret of order $O(NM\\log\\log(LT))$ under fixed valuations model, $\\widetilde{O}(\\sqrt{NMLT})$ under random experiences model and $\\widetilde{O}(\\sqrt{NMLT})$ under random valuations model in $T$ rounds",
    "keywords": [],
    "checked": false,
    "id": "9a8b698f0d51f875765f245ee0009ba7842c0b18",
    "semantic_title": "interactive recommendations for optimal allocations in markets with constraints",
    "citation_count": 1,
    "authors": [
      "Yigit Efe Erginbas",
      "Thomas Courtade",
      "Kannan Ramchandran",
      "Soham Phade"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5ef04392708bb2340cb9b7da41225660-Abstract-Conference.html": {
    "title": "Online (Multinomial) Logistic Bandit: Improved Regret and Constant Computation Cost",
    "volume": "main",
    "abstract": "This paper investigates the logistic bandit problem, a variant of the generalized linear bandit model that utilizes a logistic model to depict the feedback from an action. While most existing research focuses on the binary logistic bandit problem, the multinomial case, which considers more than two possible feedback values, offers increased practical relevance and adaptability for use in complex decision-making problems such as reinforcement learning. In this paper, we provide an algorithm that enjoys both statistical and computational efficiency for the logistic bandit problem. In the binary case, our method improves the state-of-the-art binary logistic bandit method by reducing the per-round computation cost from $\\mathcal{O}(\\log T)$ to $\\mathcal{O}(1)$ with respect to the time horizon $T$, while still preserving the minimax optimal guarantee up to logarithmic factors. In the multinomial case, with $K+1$ potential feedback values, our algorithm achieves an $\\tilde{\\mathcal{O}}(K\\sqrt{T})$ regret bound with $\\mathcal{O}(1)$ computational cost per round. The result not only improves the $\\tilde{\\mathcal{O}}(K\\sqrt{\\kappa T})$ bound for the best-known tractable algorithm—where the large constant $\\kappa$ increases exponentially with the diameter of the parameter domain—but also reduces the $\\mathcal{O}(T)$ computational complexity demanded by the previous method",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Jie Zhang",
      "Masashi Sugiyama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f02c76bc411a6f7c9a8bb2cbf981260-Abstract-Conference.html": {
    "title": "Transfer learning for atomistic simulations using GNNs and kernel mean embeddings",
    "volume": "main",
    "abstract": "Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, accurate models require large training datasets, while generating reference calculations is computationally demanding. To bypass this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) to represent chemical environments together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by incorporating into the kernel the chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression alone, as well as similar fine-tuning approaches",
    "keywords": [],
    "checked": true,
    "id": "615543d0a38a0f2e33c268486775607da48f7c35",
    "semantic_title": "transfer learning for atomistic simulations using gnns and kernel mean embeddings",
    "citation_count": 0,
    "authors": [
      "John Falk",
      "Luigi Bonati",
      "Pietro Novelli",
      "Michele Parrinello",
      "Massimiliano Pontil"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f0a4cd23e1c6eedd3edebba674ab877-Abstract-Conference.html": {
    "title": "Statistical Knowledge Assessment for Large Language Models",
    "volume": "main",
    "abstract": "Given varying prompts regarding a factoid question, can a large language model (LLM) reliably generate factually correct answers? Existing LLMs may generate distinct responses for different prompts. In this paper, we study the problem of quantifying knowledge contained in an LLM regarding a given set of facts. We propose KaRR, a statistical approach to assess factual knowledge for LLMs. The main idea is to estimate the ratio of LLM generating text corresponding to the answer entity given diverse prompts of the subject and the querying relation, versus it generating by random chances. Our assessment suite contains a comprehensive set of 994,123 entities and 600 relations, with 1,395,905 text aliases. We use our method to evaluate 20 LLMs of various sizes, including LLaMA, Alpaca, OPT, etc. Experiments show that our results have a strong correlation (0.43 Kendall's $\\tau$) with the results of human assessment on LLMs. Our results reveal that the knowledge in LLMs with the same backbone architecture adheres to the scaling law, while tuning on instruction-following data sometimes compromises the model's capability to generate factually correct text reliably",
    "keywords": [],
    "checked": true,
    "id": "ce994a2e8ece97e3314383c634473da4f90fbdd9",
    "semantic_title": "statistical knowledge assessment for large language models",
    "citation_count": 1,
    "authors": [
      "Qingxiu Dong",
      "Jingjing Xu",
      "Lingpeng Kong",
      "Zhifang Sui",
      "Lei Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f173562e7662b14fb5c5695f225ea46-Abstract-Conference.html": {
    "title": "Color Equivariant Convolutional Networks",
    "volume": "main",
    "abstract": "Color is a crucial visual cue readily exploited by Convolutional Neural Networks (CNNs) for object recognition. However, CNNs struggle if there is data imbalance between color variations introduced by accidental recording conditions. Color invariance addresses this issue but does so at the cost of removing all color information, which sacrifices discriminative power. In this paper, we propose Color Equivariant Convolutions (CEConvs), a novel deep learning building block that enables shape feature sharing across the color spectrum while retaining important color information. We extend the notion of equivariance from geometric to photometric transformations by incorporating parameter sharing over hue-shifts in a neural network. We demonstrate the benefits of CEConvs in terms of downstream performance to various tasks and improved robustness to color changes, including train-test distribution shifts. Our approach can be seamlessly integrated into existing architectures, such as ResNets, and offers a promising solution for addressing color-based domain shifts in CNNs",
    "keywords": [],
    "checked": true,
    "id": "7aa596994ab0c70f3a5f816e8afb01038dd0afe6",
    "semantic_title": "color equivariant convolutional networks",
    "citation_count": 0,
    "authors": [
      "Attila Lengyel",
      "Ombretta Strafforello",
      "Robert-Jan Bruintjes",
      "Alexander Gielisse",
      "Jan van Gemert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f5f7b6080dcadced61cf5d96f7c6dde-Abstract-Conference.html": {
    "title": "DiffVL: Scaling Up Soft Body Manipulation using Vision-Language Driven Differentiable Physics",
    "volume": "main",
    "abstract": "Combining gradient-based trajectory optimization with differentiable physics simulation is an efficient technique for solving soft-body manipulation problems.Using a well-crafted optimization objective, the solver can quickly converge onto a valid trajectory.However, writing the appropriate objective functions requires expert knowledge, making it difficult to collect a large set of naturalistic problems from non-expert users.We introduce DiffVL, a method that enables non-expert users to communicate soft-body manipulation tasks -- a combination of vision and natural language, given in multiple stages -- that can be readily leveraged by a differential physics solver. We have developed GUI tools that enable non-expert users to specify 100 tasks inspired by real-life soft-body manipulations from online videos, which we'll make public.We leverage large language models to translate task descriptions into machine-interpretable optimization objectives. The optimization objectives can help differentiable physics solvers to solve these long-horizon multistage tasks that are challenging for previous baselines",
    "keywords": [],
    "checked": true,
    "id": "1b29326d5c4ae8ba833b8571e8dfa5c8471bd29e",
    "semantic_title": "diffvl: scaling up soft body manipulation using vision-language driven differentiable physics",
    "citation_count": 0,
    "authors": [
      "Zhiao Huang",
      "Feng Chen",
      "Yewen Pu",
      "Chunru Lin",
      "Hao Su",
      "Chuang Gan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f6fae52f3b62c3334e288e3bc58230d-Abstract-Conference.html": {
    "title": "Label-efficient Segmentation via Affinity Propagation",
    "volume": "main",
    "abstract": "Weakly-supervised segmentation with label-efficient sparse annotations has attracted increasing research attention to reduce the cost of laborious pixel-wise labeling process, while the pairwise affinity modeling techniques play an essential role in this task. Most of the existing approaches focus on using the local appearance kernel to model the neighboring pairwise potentials. However, such a local operation fails to capture the long-range dependencies and ignores the topology of objects. In this work, we formulate the affinity modeling as an affinity propagation process, and propose a local and a global pairwise affinity terms to generate accurate soft pseudo labels. An efficient algorithm is also developed to reduce significantly the computational cost. The proposed approach can be conveniently plugged into existing segmentation networks. Experiments on three typical label-efficient segmentation tasks, i.e. box-supervised instance segmentation, point/scribble-supervised semantic segmentation and CLIP-guided semantic segmentation, demonstrate the superior performance of the proposed approach",
    "keywords": [],
    "checked": true,
    "id": "3860820637b81a6b69478d9ce96cf1ce5279b872",
    "semantic_title": "label-efficient segmentation via affinity propagation",
    "citation_count": 0,
    "authors": [
      "Wentong Li",
      "Yuqian Yuan",
      "Song Wang",
      "Wenyu Liu",
      "Dongqi Tang",
      "Jian liu",
      "Jianke Zhu",
      "Lei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f828e38160f31935cfe9f67503ad17c-Abstract-Conference.html": {
    "title": "Segment Anything in High Quality",
    "volume": "main",
    "abstract": "The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting. Despite being trained with 1.1 billion masks, SAM's mask prediction quality falls short in many cases, particularly when dealing with objects that have intricate structures. We propose HQ-SAM, equipping SAM with the ability to accurately segment any object, while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability. Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation. We design a learnable High-Quality Output Token, which is injected into SAM's mask decoder and is responsible for predicting the high-quality mask. Instead of only applying it on mask-decoder features, we first fuse them with early and final ViT features for improved mask details. To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of HQ-SAM in a suite of 10 diverse segmentation datasets across different downstream tasks, where 8 out of them are evaluated in a zero-shot transfer protocol. Our code and pretrained models are at https://github.com/SysCV/SAM-HQ",
    "keywords": [],
    "checked": true,
    "id": "36fff28902cfb6c99c7c98f284639ad8e0133c44",
    "semantic_title": "segment anything in high quality",
    "citation_count": 57,
    "authors": [
      "Lei Ke",
      "Mingqiao Ye",
      "Martin Danelljan",
      "Yifan liu",
      "Yu-Wing Tai",
      "Chi-Keung Tang",
      "Fisher Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f9453c4848b89d4d8c5d6041f5fb9ec-Abstract-Conference.html": {
    "title": "Variational Inference with Gaussian Score Matching",
    "volume": "main",
    "abstract": "Variational inference (VI) is a method to approximate the computationally intractable posterior distributions that arise in Bayesian statistics. Typically, VI fits a simple parametric distribution to be close to the target posterior, optimizing an appropriate objective such as the evidence lower bound (ELBO). In this work, we present a new approach to VI. Our method is based on the principle of score matching---namely, that if two distributions are equal then their score functions (i.e., gradients of the log density) are equal at every point on their support. With this principle, we develop score-matching VI, an iterative algorithm that seeks to match the scores between the variational approximation and the exact posterior. At each iteration, score-matching VI solves an inner optimization, one that minimally adjusts the current variational estimate to match the scores at a newly sampled value of the latent variables. We show that when the variational family is a Gaussian, this inner optimization enjoys a closed-form solution, which we call Gaussian score matching VI (GSM-VI). GSM-VI is a ``black box'' variational algorithm in that it only requires a differentiable joint distribution, and as such it can be applied to a wide class of models. We compare GSM-VI to black box variational inference (BBVI), which has similar requirements but instead optimizes the ELBO. We first study how GSM-VI behaves as a function of the problem dimensionality, the condition number of the target covariance matrix (when the target is Gaussian), and the degree of mismatch between the approximating and exact posterior distribution. We then study GSM-VI on a collection of real-world Bayesian inference problems from the posteriorDB database of datasets and models. We find that GSM-VI is faster than BBVI and equally or more accurate. Specifically, over a wide range of target posteriors, GSM-VI requires 10-100x fewer gradient evaluations than BBVI to obtain a comparable quality of approximation",
    "keywords": [],
    "checked": true,
    "id": "4707587fc67fd590c0f8c767869bb7ba73f3e56e",
    "semantic_title": "variational inference with gaussian score matching",
    "citation_count": 0,
    "authors": [
      "Chirag Modi",
      "Robert Gower",
      "Charles Margossian",
      "Yuling Yao",
      "David Blei",
      "Lawrence Saul"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f999632c48f87cffb214e575581e4a9-Abstract-Conference.html": {
    "title": "Feature Adaptation for Sparse Linear Regression",
    "volume": "main",
    "abstract": "Sparse linear regression is a central problem in high-dimensional statistics. We study the correlated random design setting, where the covariates are drawn from a multivariate Gaussian $N(0,\\Sigma)$, and we seek an estimator with small excess risk. If the true signal is $t$-sparse, information-theoretically, it is possible to achieve strong recovery guarantees with only $O(t\\log n)$ samples. However, computationally efficient algorithms have sample complexity linear in (some variant of) the *condition number* of $\\Sigma$. Classical algorithms such as the Lasso can require significantly more samples than necessary even if there is only a single sparse approximate dependency among the covariates.We provide a polynomial-time algorithm that, given $\\Sigma$, automatically adapts the Lasso to tolerate a small number of approximate dependencies. In particular, we achieve near-optimal sample complexity for constant sparsity and if $\\Sigma$ has few ``outlier'' eigenvalues.Our algorithm fits into a broader framework of *feature adaptation* for sparse linear regression with ill-conditioned covariates. With this framework, we additionally provide the first polynomial-factor improvement over brute-force search for constant sparsity $t$ and arbitrary covariance $\\Sigma$",
    "keywords": [],
    "checked": true,
    "id": "e469bf972efeb5c46f1ee0f9e6490a85d5315ad3",
    "semantic_title": "feature adaptation for sparse linear regression",
    "citation_count": 3,
    "authors": [
      "Jonathan Kelner",
      "Frederic Koehler",
      "Raghu Meka",
      "Dhruv Rohatgi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f9bfdfe3685e4ccdbc0e7fb29cccf2a-Abstract-Conference.html": {
    "title": "SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling",
    "volume": "main",
    "abstract": "Time series analysis is widely used in extensive areas. Recently, to reduce labeling expenses and benefit various tasks, self-supervised pre-training has attracted immense interest. One mainstream paradigm is masked modeling, which successfully pre-trains deep models by learning to reconstruct the masked content based on the unmasked part. However, since the semantic information of time series is mainly contained in temporal variations, the standard way of randomly masking a portion of time points will seriously ruin vital temporal variations of time series, making the reconstruction task too difficult to guide representation learning. We thus present SimMTM, a Simple pre-training framework for Masked Time-series Modeling. By relating masked modeling to manifold learning, SimMTM proposes to recover masked time points by the weighted aggregation of multiple neighbors outside the manifold, which eases the reconstruction task by assembling ruined but complementary temporal variations from multiple masked series. SimMTM further learns to uncover the local structure of the manifold, which is helpful for masked modeling. Experimentally, SimMTM achieves state-of-the-art fine-tuning performance compared to the most advanced time series pre-training methods in two canonical time series analysis tasks: forecasting and classification, covering both in- and cross-domain settings",
    "keywords": [],
    "checked": true,
    "id": "581921989e50637b1f9930670e623fbd18e42c5d",
    "semantic_title": "simmtm: a simple pre-training framework for masked time-series modeling",
    "citation_count": 19,
    "authors": [
      "Jiaxiang Dong",
      "Haixu Wu",
      "Haoran Zhang",
      "Li Zhang",
      "Jianmin Wang",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5fba70900a84a8fb755c48ba99420c95-Abstract-Conference.html": {
    "title": "CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs",
    "volume": "main",
    "abstract": "Controllable scene synthesis aims to create interactive environments for various industrial use cases. Scene graphs provide a highly suitable interface to facilitate these applications by abstracting the scene context in a compact manner. Existing methods, reliant on retrieval from extensive databases or pre-trained shape embeddings, often overlook scene-object and object-object relationships, leading to inconsistent results due to their limited generation capacity. To address this issue, we present CommonScenes, a fully generative model that converts scene graphs into corresponding controllable 3D scenes, which are semantically realistic and conform to commonsense. Our pipeline consists of two branches, one predicting the overall scene layout via a variational auto-encoder and the other generating compatible shapes via latent diffusion, capturing global scene-object and local inter-object relationships while preserving shape diversity. The generated scenes can be manipulated by editing the input scene graph and sampling the noise in the diffusion model. Due to lacking a scene graph dataset offering high-quality object-level meshes with relations, we also construct SG-FRONT, enriching the off-the-shelf indoor dataset 3D-FRONT with additional scene graph labels. Extensive experiments are conducted on SG-FRONT where CommonScenes shows clear advantages over other methods regarding generation consistency, quality, and diversity. Codes and the dataset will be released upon acceptance",
    "keywords": [],
    "checked": true,
    "id": "3572436b7601ab9c6af302c702bdebc4440a83ff",
    "semantic_title": "commonscenes: generating commonsense 3d indoor scenes with scene graphs",
    "citation_count": 6,
    "authors": [
      "Guangyao Zhai",
      "Evin Pınar Örnek",
      "Shun-Cheng Wu",
      "Yan Di",
      "Federico Tombari",
      "Nassir Navab",
      "Benjamin Busam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5fc47800ee5b30b8777fdd30abcaaf3b-Abstract-Conference.html": {
    "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
    "volume": "main",
    "abstract": "Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well.Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these bottlenecks with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM based simulator for human feedback that is 45x cheaper than crowdworkers and displays high agreement with humans. Second, we identify an evaluation dataset representative of real-world instructions and propose an automatic evaluation procedure. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, among others) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% win-rate improvement against Davinci003",
    "keywords": [],
    "checked": true,
    "id": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa",
    "semantic_title": "alpacafarm: a simulation framework for methods that learn from human feedback",
    "citation_count": 160,
    "authors": [
      "Yann Dubois",
      "Chen Xuechen Li",
      "Rohan Taori",
      "Tianyi Zhang",
      "Ishaan Gulrajani",
      "Jimmy Ba",
      "Carlos Guestrin",
      "Percy S. Liang",
      "Tatsunori B. Hashimoto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5fe1b43c882d746c187456eb4c8cdf52-Abstract-Conference.html": {
    "title": "Beta Diffusion",
    "volume": "main",
    "abstract": "We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, further supports the efficacy of KLUBs for optimization. Experimental results on both synthetic data and natural images demonstrate the unique capabilities of beta diffusion in generative modeling of range-bounded data and validate the effectiveness of KLUBs in optimizing diffusion models, thereby making them valuable additions to the family of diffusion-based generative models and the optimization techniques used to train them",
    "keywords": [],
    "checked": true,
    "id": "5c1d5d8bfc930b0f51f068d47f8ad301e5de2f28",
    "semantic_title": "beta diffusion",
    "citation_count": 3,
    "authors": [
      "Mingyuan Zhou",
      "Tianqi Chen",
      "Zhendong Wang",
      "Huangjie Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5fed79713d97df88f9912c8d886fccb3-Abstract-Conference.html": {
    "title": "Minimax Optimal Rate for Parameter Estimation in Multivariate Deviated Models",
    "volume": "main",
    "abstract": "We study the maximum likelihood estimation (MLE) in the multivariate deviated model where the data are generated from the density function $(1-\\lambda^{\\ast})h_{0}(x)+\\lambda^{\\ast}f(x|\\mu^{\\ast}, \\Sigma^{\\ast})$ in which $h_{0}$ is a known function, $\\lambda^{\\ast} \\in [0,1]$ and $(\\mu^{\\ast}, \\Sigma^{\\ast})$ are unknown parameters to estimate. The main challenges in deriving the convergence rate of the MLE mainly come from two issues: (1) The interaction between the function $h_{0}$ and the density function $f$; (2) The deviated proportion $\\lambda^{\\ast}$ can go to the extreme points of $[0,1]$ as the sample size tends to infinity. To address these challenges, we develop the \\emph{distinguishability condition} to capture the linear independent relation between the function $h_{0}$ and the density function $f$. We then provide comprehensive convergence rates of the MLE via the vanishing rate of $\\lambda^{\\ast}$ to zero as well as the distinguishability of two functions $h_{0}$ and $f$",
    "keywords": [],
    "checked": true,
    "id": "d946b5ccf263ec78cd70c06c3496dd9348f1f6da",
    "semantic_title": "minimax optimal rate for parameter estimation in multivariate deviated models",
    "citation_count": 1,
    "authors": [
      "Dat Do",
      "Huy Nguyen",
      "Khai Nguyen",
      "Nhat Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5ff7b1f30e0caf3cc0b2fbfd4d7ebdd4-Abstract-Conference.html": {
    "title": "Partial Matrix Completion",
    "volume": "main",
    "abstract": "The matrix completion problem involves reconstructing a low-rank matrix by using a given set of revealed (and potentially noisy) entries. Although existing methods address the completion of the entire matrix, the accuracy of the completed entries can vary significantly across the matrix, due to differences in the sampling distribution. For instance, users may rate movies primarily from their country or favorite genres, leading to inaccurate predictions for the majority of completed entries.We propose a novel formulation of the problem as Partial Matrix Completion, where the objective is to complete a substantial subset of the entries with high confidence. Our algorithm efficiently handles the unknown and arbitrarily complex nature of the sampling distribution, ensuring high accuracy for all completed entries and sufficient coverage across the matrix. Additionally, we introduce an online version of the problem and present a low-regret efficient algorithm based on iterative gradient updates. Finally, we conduct a preliminary empirical evaluation of our methods",
    "keywords": [],
    "checked": true,
    "id": "d7c39e86100764d02dc87e56d0b9a614642ec3d0",
    "semantic_title": "partial matrix completion",
    "citation_count": 1,
    "authors": [
      "Elad Hazan",
      "Adam Tauman Kalai",
      "Varun Kanade",
      "Clara Mohri",
      "Y. Jennifer Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/602e1a5de9c47df34cae39353a7f5bb1-Abstract-Conference.html": {
    "title": "BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing",
    "volume": "main",
    "abstract": "Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text.Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with up to 20x speedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Implementations are available at: https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion",
    "keywords": [],
    "checked": true,
    "id": "dc0c132b273456b288a785414db2fa72edf87b1a",
    "semantic_title": "blip-diffusion: pre-trained subject representation for controllable text-to-image generation and editing",
    "citation_count": 56,
    "authors": [
      "DONGXU LI",
      "Junnan Li",
      "Steven Hoi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/602f5c1b803c53b2aaf0b3864bf3383a-Abstract-Conference.html": {
    "title": "Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data",
    "volume": "main",
    "abstract": "The implicit bias towards solutions with favorable properties is believed to be a key reason why neural networks trained by gradient-based optimization can generalize well. While the implicit bias of gradient flow has been widely studied for homogeneous neural networks (including ReLU and leaky ReLU networks), the implicit bias of gradient descent is currently only understood for smooth neural networks. Therefore, implicit bias in non-smooth neural networks trained by gradient descent remains an open question. In this paper, we aim to answer this question by studying the implicit bias of gradient descent for training two-layer fully connected (leaky) ReLU neural networks. We showed that when the training data are nearly-orthogonal, for leaky ReLU activation function, gradient descent will find a network with a stable rank that converges to $1$, whereas for ReLU activation function, gradient descent will find a neural network with a stable rank that is upper bounded by a constant. Additionally, we show that gradient descent will find a neural network such that all the training data points have the same normalized margin asymptotically. Experiments on both synthetic and real data backup our theoretical findings",
    "keywords": [],
    "checked": true,
    "id": "95f5b6d14d5f7f96aa794dd929c5f0d0bf9ec78c",
    "semantic_title": "implicit bias of gradient descent for two-layer relu and leaky relu networks on nearly-orthogonal data",
    "citation_count": 1,
    "authors": [
      "Yiwen Kou",
      "Zixiang Chen",
      "Quanquan Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6034a661584af6c28fd97a6f23e56c0a-Abstract-Conference.html": {
    "title": "SpecTr: Fast Speculative Decoding via Optimal Transport",
    "volume": "main",
    "abstract": "Autoregressive sampling from large language models has led to state-of-the-art results in several natural language tasks.However, autoregressive sampling generates tokens one at a time making it slow, and even prohibitive in certain tasks. One way to speed up sampling is *speculative decoding*: use a small model to sample a *draft* (block or sequence of tokens), and then score all tokens in the draft by the large language model in parallel. A subset of the tokens in the draft are accepted (and the rest rejected) based on a statistical method to guarantee that the final output follows the distribution of the large model. In this work, we provide a principled understanding of speculative decoding through the lens of optimal transport (OT) with *membership cost*. This framework can be viewed as an extension of the well-known *maximal-coupling* problem. This new formulation enables us to generalize the speculative decoding method to allow for a set of $k$ candidates at the token-level, which leads to an improved optimal membership cost. We show that the optimal draft selection algorithm (transport plan) can be computed via linear programming, whose best-known runtime is exponential in $k$. We then propose a valid draft selection algorithm whose acceptance probability is $(1-1/e)$-optimal multiplicatively. Moreover, it can be computed in time almost linear with size of domain of a single token.Using this new draft selection algorithm, we develop a new autoregressive sampling algorithm called *SpecTr*, which provides speedup in decoding while ensuring that there is no quality degradation in the decoded output.We experimentally demonstrate that for state-of-the-art large language models, the proposed approach achieves a wall clock speedup of 2.13X, a further 1.37X speedup over speculative decoding on standard benchmarks",
    "keywords": [],
    "checked": true,
    "id": "ea1f648988c632a6dbab6d8b88432456aa021cfb",
    "semantic_title": "spectr: fast speculative decoding via optimal transport",
    "citation_count": 12,
    "authors": [
      "Ziteng Sun",
      "Ananda Theertha Suresh",
      "Jae Hun Ro",
      "Ahmad Beirami",
      "Himanshu Jain",
      "Felix Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6058d0c628a03fd95dfe5c72cbdf9e64-Abstract-Conference.html": {
    "title": "Proportional Response: Contextual Bandits for Simple and Cumulative Regret Minimization",
    "volume": "main",
    "abstract": "In many applications, e.g. in healthcare and e-commerce, the goal of a contextual bandit may be to learn an optimal treatment assignment policy at the end of the experiment. That is, to minimize simple regret. However, this objective remains understudied. We propose a new family of computationally efficient bandit algorithms for the stochastic contextual bandit setting, where a tuning parameter determines the weight placed on cumulative regret minimization (where we establish near-optimal minimax guarantees) versus simple regret minimization (where we establish state-of-the-art guarantees). Our algorithms work with any function class, are robust to model misspecification, and can be used in continuous arm settings. This flexibility comes from constructing and relying on \"conformal arm sets\" (CASs). CASs provide a set of arms for every context, encompassing the context-specific optimal arm with a certain probability across the context distribution. Our positive results on simple and cumulative regret guarantees are contrasted with a negative result, which shows that no algorithm can achieve instance-dependent simple regret guarantees while simultaneously achieving minimax optimal cumulative regret guarantees",
    "keywords": [],
    "checked": true,
    "id": "e5bfbad37b68da6835d8eaa50edddb9a241bb657",
    "semantic_title": "proportional response: contextual bandits for simple and cumulative regret minimization",
    "citation_count": 2,
    "authors": [
      "Sanath Kumar Krishnamurthy",
      "Ruohan Zhan",
      "Susan Athey",
      "Emma Brunskill"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/605e02ae04cba1ebf6a08206299e76b9-Abstract-Conference.html": {
    "title": "Higher-Order Uncoupled Dynamics Do Not Lead to Nash Equilibrium - Except When They Do",
    "volume": "main",
    "abstract": "The framework of multi-agent learning explores the dynamics of how an agent's strategies evolve in response to the evolving strategies of other agents. Of particular interest is whether or not agent strategies converge to well known solution concepts such as Nash Equilibrium (NE). In \"higher order'' learning, agent dynamics include auxiliary states that can capture phenomena such as path dependencies. We introduce higher-order gradient play dynamics that resemble projected gradient ascent with auxiliary states. The dynamics are \"payoff based'' and \"uncoupled'' in that each agent's dynamics depend on its own evolving payoff and has no explicit dependence on the utilities of other agents. We first show that for any specific game with an isolated completely mixed-strategy NE, there exist higher-order gradient play dynamics that lead (locally) to that NE, both for the specific game and nearby games with perturbed utility functions. Conversely, we show that for any higher-order gradient play dynamics, there exists a game with a unique isolated completely mixed-strategy NE for which the dynamics do not lead to NE. Finally, we show that convergence to the mixed-strategy equilibrium in coordination games, comes at the expense of the dynamics being inherently internally unstable",
    "keywords": [],
    "checked": true,
    "id": "9215c3a3cc22e79c2351e262431159571a6e0e83",
    "semantic_title": "higher-order uncoupled dynamics do not lead to nash equilibrium - except when they do",
    "citation_count": 0,
    "authors": [
      "Sarah Toonsi",
      "Jeff Shamma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6091bf1542b118287db4088bc16be8d9-Abstract-Conference.html": {
    "title": "Subject-driven Text-to-Image Generation via Apprenticeship Learning",
    "volume": "main",
    "abstract": "Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples.However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with {in-context} learning.Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization.SuTI is powered by {apprenticeship learning}, where a single apprentice model is learned from data generated by a massive number of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train a massive number of expert models, each specializing in a different subject. The apprentice model SuTI then learns to imitate the behavior of these fine-tuned experts. SuTI can generate high-quality and customized subject-specific images 20x faster than optimization-based SoTA methods. On the challenging DreamBench and DreamBench-v2, our human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth",
    "keywords": [],
    "checked": true,
    "id": "83b8e18488d8f31dd017ec0b26531cef4b635b36",
    "semantic_title": "subject-driven text-to-image generation via apprenticeship learning",
    "citation_count": 55,
    "authors": [
      "Wenhu Chen",
      "Hexiang Hu",
      "Yandong Li",
      "Nataniel Ruiz",
      "Xuhui Jia",
      "Ming-Wei Chang",
      "William W. Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/60dc7fa827f5f761ad481e2ad40b5573-Abstract-Conference.html": {
    "title": "Large Language Models can Implement Policy Iteration",
    "volume": "main",
    "abstract": "In this work, we demonstrate a method for implementing policy iteration using a large language model. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the \"few-shot\" quality that makes in-context learning attractive to begin with. Our method demonstrates that a large language model can be used to implement policy iteration using the machinery of in-context learning, enabling it to learn to perform RL tasks without expert demonstrations or gradients. Our approach iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our method using Codex (M. Chen et al. 2021b), a language model with no prior knowledge of the domains on which we evaluate it",
    "keywords": [],
    "checked": true,
    "id": "c4fff3bdbbdcff01545f1f53ec6290b3556c41ac",
    "semantic_title": "large language models can implement policy iteration",
    "citation_count": 3,
    "authors": [
      "Ethan Brooks",
      "Logan Walls",
      "Richard L Lewis",
      "Satinder Singh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/60f9118a849e8e9a0c67e2a36ad80ebf-Abstract-Conference.html": {
    "title": "ForkMerge: Mitigating Negative Transfer in Auxiliary-Task Learning",
    "volume": "main",
    "abstract": "Auxiliary-Task Learning (ATL) aims to improve the performance of the target task by leveraging the knowledge obtained from related tasks. Occasionally, learning multiple tasks simultaneously results in lower accuracy than learning only the target task, which is known as negative transfer. This problem is often attributed to the gradient conflicts among tasks, and is frequently tackled by coordinating the task gradients in previous works. However, these optimization-based methods largely overlook the auxiliary-target generalization capability. To better understand the root cause of negative transfer, we experimentally investigate it from both optimization and generalization perspectives. Based on our findings, we introduce ForkMerge, a novel approach that periodically forks the model into multiple branches, automatically searches the varying task weights by minimizing target validation errors, and dynamically merges all branches to filter out detrimental task-parameter updates. On a series of auxiliary-task learning benchmarks, ForkMerge outperforms existing methods and effectively mitigates negative transfer",
    "keywords": [],
    "checked": true,
    "id": "10b752a369ff01c748f2ec54b41326ba862a65fd",
    "semantic_title": "forkmerge: mitigating negative transfer in auxiliary-task learning",
    "citation_count": 1,
    "authors": [
      "Junguang Jiang",
      "Baixu Chen",
      "Junwei Pan",
      "Ximei Wang",
      "Dapeng Liu",
      "Jie Jiang",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6111371a868af8dcfba0f96ad9e25ae3-Abstract-Conference.html": {
    "title": "Revisiting Adversarial Robustness Distillation from the Perspective of Robust Fairness",
    "volume": "main",
    "abstract": "Adversarial Robustness Distillation (ARD) aims to transfer the robustness of large teacher models to small student models, facilitating the attainment of robust performance on resource-limited devices. However, existing research on ARD primarily focuses on the overall robustness of student models, overlooking the crucial aspect of $\\textit{robust fairness}$. Specifically, these models may demonstrate strong robustness on some classes of data while exhibiting high vulnerability on other classes. Unfortunately, the \"buckets effect\" implies that the robustness of the deployed model depends on the classes with the lowest level of robustness. In this paper, we first investigate the inheritance of robust fairness during ARD and reveal that student models only partially inherit robust fairness from teacher models. We further validate this issue through fine-grained experiments with various model capacities and find that it may arise due to the gap in capacity between teacher and student models, as well as the existing methods treating each class equally during distillation. Based on these observations, we propose $\\textbf{Fair}$ $\\textbf{A}$dversarial $\\textbf{R}$obustness $\\textbf{D}$istillation (Fair-ARD), a novel framework for enhancing the robust fairness of student models by increasing the weights of difficult classes, and design a geometric perspective-based method to quantify the difficulty of different classes for determining the weights. Extensive experiments show that Fair-ARD surpasses both state-of-the-art ARD methods and existing robust fairness algorithms in terms of robust fairness (e.g., the worst-class robustness under AutoAttack is improved by at most 12.3\\% and 5.3\\% using ResNet18 on CIFAR10, respectively), while also slightly improving overall robustness. Our code is available at: [https://github.com/NISP-official/Fair-ARD](https://github.com/NISP-official/Fair-ARD)",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinli Yue",
      "Mou Ningping",
      "Qian Wang",
      "Lingchen Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61202bb341e7e0a6026ea134a5057abf-Abstract-Conference.html": {
    "title": "Differentiable Sampling of Categorical Distributions Using the CatLog-Derivative Trick",
    "volume": "main",
    "abstract": "Categorical random variables can faithfully represent the discrete and uncertain aspects of data as part of a discrete latent variable model. Learning in such models necessitates taking gradients with respect to the parameters of the categorical probability distributions, which is often intractable due to their combinatorial nature. A popular technique to estimate these otherwise intractable gradients is the Log-Derivative trick. This trick forms the basis of the well-known REINFORCE gradient estimator and its many extensions. While the Log-Derivative trick allows us to differentiate through samples drawn from categorical distributions, it does not take into account the discrete nature of the distribution itself. Our first contribution addresses this shortcoming by introducing the CatLog-Derivative trick -- a variation of the Log-Derivative trick tailored towards categorical distributions. Secondly, we use the CatLog-Derivative trick to introduce IndeCateR, a novel and unbiased gradient estimator for the important case of products of independent categorical distributions with provably lower variance than REINFORCE. Thirdly, we empirically show that IndeCateR can be efficiently implemented and that its gradient estimates have significantly lower bias and variance for the same number of samples compared to the state of the art",
    "keywords": [],
    "checked": true,
    "id": "db5463470183203d89f35983bd56ff265344538c",
    "semantic_title": "differentiable sampling of categorical distributions using the catlog-derivative trick",
    "citation_count": 0,
    "authors": [
      "Lennert De Smet",
      "Emanuele Sansone",
      "Pedro Zuidberg Dos Martires"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/612a56f193d031687683445cd0001083-Abstract-Conference.html": {
    "title": "Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing",
    "volume": "main",
    "abstract": "Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets show that our VIR can outperform state-of-the-art imbalanced regression models in terms of both accuracy and uncertainty estimation. Code will soon be available at https://github.com/Wang-ML-Lab/variational-imbalanced-regression",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyan Wang",
      "Hao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/612a7948f3294a02a63d970566ca8536-Abstract-Conference.html": {
    "title": "Towards A Richer 2D Understanding of Hands at Scale",
    "volume": "main",
    "abstract": "As humans, we learn a lot about how to interact with the world by observing others interacting with their hands. To help AI systems obtain a better understanding of hand interactions, we introduce a new model that produces a rich understanding of hand interaction. Our system produces a richer output than past systems at a larger scale. Our outputs include boxes and segments for hands, in-contact objects, and second objects touched by tools as well as contact and grasp type. Supporting this method are annotations of 257K images, 401K hands, 288K objects, and 19K second objects spanning four datasets. We show that our method provides rich information and performs and generalizes well",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Cheng",
      "Dandan Shan",
      "Ayda Hassen",
      "Richard Higgins",
      "David Fouhey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61355b9c218505505d1bedede9da56b2-Abstract-Conference.html": {
    "title": "Effective Human-AI Teams via Learned Natural Language Rules and Onboarding",
    "volume": "main",
    "abstract": "People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately",
    "keywords": [],
    "checked": true,
    "id": "6389144ee8896409af54cf70c10b5cc4dd64f22f",
    "semantic_title": "effective human-ai teams via learned natural language rules and onboarding",
    "citation_count": 0,
    "authors": [
      "Hussein Mozannar",
      "Jimin Lee",
      "Dennis Wei",
      "Prasanna Sattigeri",
      "Subhro Das",
      "David Sontag"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61674667d642ae52f6bb281bea90ee29-Abstract-Conference.html": {
    "title": "On Certified Generalization in Structured Prediction",
    "volume": "main",
    "abstract": "In structured prediction, target objects have rich internal structure which does not factorize into independent components and violates common i.i.d. assumptions. This challenge becomes apparent through the exponentially large output space in applications such as image segmentation or scene graph generation.We present a novel PAC-Bayesian risk bound for structured prediction wherein the rate of generalization scales not only with the number of structured examples but also with their size.The underlying assumption, conforming to ongoing research on generative models, is that data are generated by the Knothe-Rosenblatt rearrangement of a factorizing reference measure. This allows to explicitly distill the structure between random output variables into a Wasserstein dependency matrix. Our work makes a preliminary step towards leveraging powerful generative models to establish generalization bounds for discriminative downstream tasks in the challenging setting of structured prediction",
    "keywords": [],
    "checked": true,
    "id": "b74cfe40d4b50693f4f34585e9a00aad2b2ff8fe",
    "semantic_title": "on certified generalization in structured prediction",
    "citation_count": 0,
    "authors": [
      "Bastian Boll",
      "Christoph Schnörr"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6171c9e600432a42688ad61a525951bf-Abstract-Conference.html": {
    "title": "SutraNets: Sub-series Autoregressive Networks for Long-Sequence, Probabilistic Forecasting",
    "volume": "main",
    "abstract": "We propose SutraNets, a novel method for neural probabilistic forecasting of long-sequence time series. SutraNets use an autoregressive generative model to factorize the likelihood of long sequences into products of conditional probabilities. When generating long sequences, most autoregressive approaches suffer from harmful error accumulation, as well as challenges in modeling long-distance dependencies. SutraNets treat long, univariate prediction as multivariate prediction over lower-frequency sub-series. Autoregression proceeds across time and across sub-series in order to ensure coherent multivariate (and, hence, high-frequency univariate) outputs. Since sub-series can be generated using fewer steps, SutraNets effectively reduce error accumulation and signal path distances. We find SutraNets to significantly improve forecasting accuracy over competitive alternatives on six real-world datasets, including when we vary the number of sub-series and scale up the depth and width of the underlying sequence models",
    "keywords": [],
    "checked": true,
    "id": "407af34a332b800a31f7ecd792e1755b02a7e576",
    "semantic_title": "sutranets: sub-series autoregressive networks for long-sequence, probabilistic forecasting",
    "citation_count": 0,
    "authors": [
      "Shane Bergsma",
      "Tim Zeyl",
      "Lei Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6174c67b136621f3f2e4a6b1d3286f6b-Abstract-Conference.html": {
    "title": "Complex Query Answering on Eventuality Knowledge Graph with Implicit Logical Constraints",
    "volume": "main",
    "abstract": "Querying knowledge graphs (KGs) using deep learning approaches can naturally leverage the reasoning and generalization ability to learn to infer better answers. Traditional neural complex query answering (CQA) approaches mostly work on entity-centric KGs. However, in the real world, we also need to make logical inferences about events, states, and activities (i.e., eventualities or situations) to push learning systems from System I to System II, as proposed by Yoshua Bengio. Querying logically from an EVentuality-centric KG (EVKG) can naturally provide references to such kind of intuitive and logical inference. Thus, in this paper, we propose a new framework to leverage neural methods to answer complex logical queries based on an EVKG, which can satisfy not only traditional first-order logic constraints but also implicit logical constraints over eventualities concerning their occurrences and orders. For instance, if we know that Food is bad happens before PersonX adds soy sauce, then PersonX adds soy sauce is unlikely to be the cause of Food is bad due to implicit temporal constraint. To facilitate consistent reasoning on EVKGs, we propose Complex Eventuality Query Answering (CEQA), a more rigorous definition of CQA that considers the implicit logical constraints governing the temporal order and occurrence of eventualities. In this manner, we propose to leverage theorem provers for constructing benchmark datasets to ensure the answers satisfy implicit logical constraints. We also propose a Memory-Enhanced Query Encoding (MEQE) approach to significantly improve the performance of state-of-the-art neural query encoders on the CEQA task",
    "keywords": [],
    "checked": true,
    "id": "da9bbfb3d3c3ddd76251a9a1069def181ef3128f",
    "semantic_title": "complex query answering on eventuality knowledge graph with implicit logical constraints",
    "citation_count": 9,
    "authors": [
      "Jiaxin Bai",
      "Xin Liu",
      "Weiqi Wang",
      "Chen Luo",
      "Yangqiu Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61779e9b0c26a31c5f36bd3e8c180dcf-Abstract-Conference.html": {
    "title": "Fast Bellman Updates for Wasserstein Distributionally Robust MDPs",
    "volume": "main",
    "abstract": "Markov decision processes (MDPs) often suffer from the sensitivity issue under model ambiguity. In recent years, robust MDPs have emerged as an effective framework to overcome this challenge. Distributionally robust MDPs extend the robust MDP framework by incorporating distributional information of the uncertain model parameters to alleviate the conservative nature of robust MDPs. This paper proposes a computationally efficient solution framework for solving distributionally robust MDPs with Wasserstein ambiguity sets. By exploiting the specific problem structure, the proposed framework decomposes the optimization problems associated with distributionally robust Bellman updates into smaller subproblems, which can be solved efficiently. The overall complexity of the proposed algorithm is quasi-linear in both the numbers of states and actions when the distance metric of the Wasserstein distance is chosen to be $L_1$, $L_2$, or $L_{\\infty}$ norm, and so the computational cost of distributional robustness is substantially reduced. Our numerical experiments demonstrate that the proposed algorithms outperform other state-of-the-art solution methods",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuodong Yu",
      "Ling Dai",
      "Shaohang Xu",
      "Siyang Gao",
      "Chin Pang Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/618c95f4557c15b253fb0e6f548ea0c0-Abstract-Conference.html": {
    "title": "Practical Contextual Bandits with Feedback Graphs",
    "volume": "main",
    "abstract": "While contextual bandit has a mature theory, effectively leveraging different feedback patterns to enhance the pace of learning remains unclear. Bandits with feedback graphs, which interpolates between the full information and bandit regimes, provides a promising framework to mitigate the statistical complexity of learning. In this paper, we propose and analyze an approach to contextual bandits with feedback graphs based upon reduction to regression. The resulting algorithms are computationally practical and achieve established minimax rates, thereby reducing the statistical complexity in real-world applications",
    "keywords": [],
    "checked": true,
    "id": "0a820808217feffbe8ba180c46b79c706c226e7a",
    "semantic_title": "practical contextual bandits with feedback graphs",
    "citation_count": 3,
    "authors": [
      "Mengxiao Zhang",
      "Yuheng Zhang",
      "Olga Vrousgou",
      "Haipeng Luo",
      "Paul Mineiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6191ab7080c840f67eaf5dff7d5edfcb-Abstract-Conference.html": {
    "title": "Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control",
    "volume": "main",
    "abstract": "Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time. In this work, we provide a fresh perspective on these behaviors by studying the return landscape: the mapping between a policy and a return. We find that popular algorithms traverse noisy neighborhoods of this landscape, in which a single update to the policy parameters leads to a wide range of returns. By taking a distributional view of these returns, we map the landscape, characterizing failure-prone regions of policy space and revealing a hidden dimension of policy quality. We show that the landscape exhibits surprising structure by finding simple paths in parameter space which improve the stability of a policy. To conclude, we develop a distribution-aware procedure which finds such paths, navigating away from noisy neighborhoods in order to improve the robustness of a policy. Taken together, our results provide new insight into the optimization, evaluation, and design of agents",
    "keywords": [],
    "checked": true,
    "id": "3009c368bba4e27acb8af50112a9d0b0e32cf445",
    "semantic_title": "policy optimization in a noisy neighborhood: on return landscapes in continuous control",
    "citation_count": 2,
    "authors": [
      "Nate Rahn",
      "Pierluca D&#x27;Oro",
      "Harley Wiltzer",
      "Pierre-Luc Bacon",
      "Marc Bellemare"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61960fdfda4d4e95fa1c1f6e64bfe8bc-Abstract-Conference.html": {
    "title": "Real-World Image Variation by Aligning Diffusion Inversion Chain",
    "volume": "main",
    "abstract": "Recent diffusion model advancements have enabled high-fidelity images to be generated using text prompts. However, a domain gap exists between generated images and real-world images, which poses a challenge in generating high-quality variations of real-world images. Our investigation uncovers that this domain gap originates from a latents' distribution gap in different diffusion processes. To address this issue, we propose a novel inference pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes diffusion models to generate image variations from a single image exemplar. Our pipeline enhances the generation quality of image variations by aligning the image generation process to the source image's inversion chain. Specifically, we demonstrate that step-wise latent distribution alignment is essential for generating high-quality variations. To attain this, we design a cross-image self-attention injection for feature interaction and a step-wise distribution normalization to align the latent features. Incorporating these alignment processes into a diffusion model allows RIVAL to generate high-quality image variations without further parameter optimization. Our experimental results demonstrate that our proposed approach outperforms existing methods concerning semantic similarity and perceptual quality. This generalized inference pipeline can be easily applied to other diffusion-based generation tasks, such as image-conditioned text-to-image generation and stylization. Project page: https://rival-diff.github.io",
    "keywords": [],
    "checked": true,
    "id": "83ce9f983bf0d3691cdea9bb5beffa7e4f970a4d",
    "semantic_title": "real-world image variation by aligning diffusion inversion chain",
    "citation_count": 9,
    "authors": [
      "Yuechen Zhang",
      "Jinbo Xing",
      "Eric Lo",
      "Jiaya Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/619cbddb92b8c6fecaf2b86463153be9-Abstract-Conference.html": {
    "title": "Vocabulary-free Image Classification",
    "volume": "main",
    "abstract": "Recent advances in large vision-language models have revolutionized the image classification paradigm. Despite showing impressive zero-shot capabilities, a pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time for composing the textual prompts. However, such assumption can be impractical when the semantic context is unknown and evolving. We thus formalize a novel task, termed as Vocabulary-free Image Classification (VIC), where we aim to assign to an input image a class that resides in an unconstrained language-induced semantic space, without the prerequisite of a known vocabulary. VIC is a challenging task as the semantic space is extremely large, containing millions of concepts, with hard-to-discriminate fine-grained categories. In this work, we first empirically verify that representing this semantic space by means of an external vision-language database is the most effective way to obtain semantically relevant content for classifying the image. We then propose Category Search from External Databases (CaSED), a method that exploits a pre-trained vision-language model and an external vision-language database to address VIC in a training-free manner. CaSED first extracts a set of candidate categories from captions retrieved from the database based on their semantic similarity to the image, and then assigns to the image the best matching candidate category according to the same vision-language model. Experiments on benchmark datasets validate that CaSED outperforms other complex vision-language frameworks, while being efficient with much fewer parameters, paving the way for future research in this direction",
    "keywords": [],
    "checked": true,
    "id": "77424562deba33e94ea5ca3c662ccfdc2b95fb5c",
    "semantic_title": "vocabulary-free image classification",
    "citation_count": 6,
    "authors": [
      "Alessandro Conti",
      "Enrico Fini",
      "Massimiliano Mancini",
      "Paolo Rota",
      "Yiming Wang",
      "Elisa Ricci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61a9278dfef5f871b5e472389f8d6fa1-Abstract-Conference.html": {
    "title": "A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence",
    "volume": "main",
    "abstract": "Modern policy optimization methods in reinforcement learning, such as TRPO and PPO, owe their success to the use of parameterized policies. However, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parameterization schemes remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parameterizations. The policy class induced by our scheme recovers known classes, e.g., softmax, and generates new ones depending on the choice of mirror map. Using our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization. To demonstrate the ability of our framework to accommodate general parameterization schemes, we provide its sample complexity when using shallow neural networks, show that it represents an improvement upon the previous best results, and empirically validate the effectiveness of our theoretical claims on classic control tasks",
    "keywords": [],
    "checked": false,
    "id": "0f71a43a5cd29368bcc2f2ff5e8769f69c20fff6",
    "semantic_title": "a novel framework for policy mirror descent with general parametrization and linear convergence",
    "citation_count": 6,
    "authors": [
      "Carlo Alfano",
      "Rui Yuan",
      "Patrick Rebeschini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61aa557643ae8709b6a4f41140b2234a-Abstract-Conference.html": {
    "title": "Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping",
    "volume": "main",
    "abstract": "Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to segment objects well blended with surrounding environments using sparsely-annotated data for model training. It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. To tackle the intrinsic similarity challenge, we design a multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results. By grouping similar features together, it encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images. For the weak supervision challenge, we utilize the recently-proposed vision foundation model, ``Segment Anything Model (SAM)'', and use the provided sparse annotations as prompts to generate segmentation masks, which are used to train the model. To alleviate the impact of low-quality segmentation masks, we further propose a series of strategies, including multi-augmentation result ensemble, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help provide more reliable supervision to train the segmentation model. We verify the effectiveness of our method on various WSCOS tasks, and experiments demonstrate that our method achieves state-of-the-art performance on these tasks",
    "keywords": [],
    "checked": true,
    "id": "ed67192cff934d21428fa0e2ae071eaa476dc086",
    "semantic_title": "weakly-supervised concealed object segmentation with sam-based pseudo labeling and multi-scale feature grouping",
    "citation_count": 29,
    "authors": [
      "Chunming He",
      "Kai Li",
      "Yachao Zhang",
      "Guoxia Xu",
      "Longxiang Tang",
      "Yulun Zhang",
      "Zhenhua Guo",
      "Xiu Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61c00c07e6d27285e4b952e96cc65666-Abstract-Conference.html": {
    "title": "Ordering-based Conditions for Global Convergence of Policy Gradient Methods",
    "volume": "main",
    "abstract": "We prove that, for finite-arm bandits with linear function approximation, the global convergence of policy gradient (PG) methods depends on inter-related properties between the policy update and the representation. textcolor{blue}{First}, we establish a few key observations that frame the study: \\textbf{(i)} Global convergence can be achieved under linear function approximation without policy or reward realizability, both for the standard Softmax PG and natural policy gradient (NPG). \\textbf{(ii)} Approximation error is not a key quantity for characterizing global convergence in either algorithm. \\textbf{(iii)} The conditions on the representation that imply global convergence are different between these two algorithms. Overall, these observations call into question approximation error as an appropriate quantity for characterizing the global convergence of PG methods under linear function approximation. \\textcolor{blue}{Second}, motivated by these observations, we establish new general results: \\textbf{(i)} NPG with linear function approximation achieves global convergence \\emph{if and only if} the projection of the reward onto the representable space preserves the optimal action's rank, a quantity that is not strongly related to approximation error. \\textbf{(ii)} The global convergence of Softmax PG occurs if the representation satisfies a non-domination condition and can preserve the ranking of rewards, which goes well beyond policy or reward realizability. We provide experimental results to support these theoretical findings",
    "keywords": [],
    "checked": true,
    "id": "366f0c69b673a72e450be6d5ff4ce396b80d34df",
    "semantic_title": "ordering-based conditions for global convergence of policy gradient methods",
    "citation_count": 0,
    "authors": [
      "Jincheng Mei",
      "Bo Dai",
      "Alekh Agarwal",
      "Mohammad Ghavamzadeh",
      "Csaba Szepesvari",
      "Dale Schuurmans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61c2c6338033da68885e0226881cbe71-Abstract-Conference.html": {
    "title": "Finding Order in Chaos: A Novel Data Augmentation Method for Time Series in Contrastive Learning",
    "volume": "main",
    "abstract": "The success of contrastive learning is well known to be dependent on data augmentation.Although the degree of data augmentations has been well controlled by utilizing pre-defined techniques in some domains like vision, time-series data augmentation is less explored and remains a challenging problem due to the complexity of the data generation mechanism, such as the intricate mechanism involved in the cardiovascular system.Moreover, there is no widely recognized and general time-series augmentation method that can be applied across different tasks.In this paper, we propose a novel data augmentation method for time-series tasks that aims to connect intra-class samples together, and thereby find order in the latent space.Our method builds upon the well-known data augmentation technique of mixup by incorporating a novel approach that accounts for the non-stationary nature of time-series data.Also, by controlling the degree of chaos created by data augmentation, our method leads to improved feature representations and performance on downstream tasks.We evaluate our proposed method on three time-series tasks, including heart rate estimation, human activity recognition, and cardiovascular disease detection. Extensive experiments against the state-of-the-art methods show that the proposed method outperforms prior works on optimal data generation and known data augmentation techniques in three tasks, reflecting the effectiveness of the presented method. The source code is available at double-blind policy",
    "keywords": [],
    "checked": true,
    "id": "76314ab64dc3b2f195389b3c87d555b24692fc34",
    "semantic_title": "finding order in chaos: a novel data augmentation method for time series in contrastive learning",
    "citation_count": 2,
    "authors": [
      "Berken Utku Demirel",
      "Christian Holz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61d0a96d4a73b626367310b3ad32579d-Abstract-Conference.html": {
    "title": "List and Certificate Complexities in Replicable Learning",
    "volume": "main",
    "abstract": "We investigate replicable learning algorithms. Informally a learning algorithm is replicable if the algorithm outputs the same canonical hypothesis over multiple runs with high probability, even when different runs observe a different set of samples from the unknown data distribution. In general, such a strong notion of replicability is not achievable. Thus we consider two feasible notions of replicability called {\\em list replicability} and {\\em certificate replicability}. Intuitively, these notions capture the degree of (non) replicability. The goal is to design learning algorithms with optimal list and certificate complexities while minimizing the sample complexity. Our contributions are the following.1. We first study the learning task of estimating the biases of $d$ coins, up to an additive error of $\\varepsilon$, by observing samples. For this task, we design a $(d+1)$-list replicable algorithm. To complement this result, we establish that the list complexity is optimal, i.e there are no learning algorithms with a list size smaller than $d+1$ for this task. We also design learning algorithms with certificate complexity $\\tilde{O}(\\log d)$. The sample complexity of both these algorithms is $\\tilde{O}(\\frac{d^2}{\\varepsilon^2})$ where $\\varepsilon$ is the approximation error parameter (for a constant error probability). 2. In the PAC model, we show that any hypothesis class that is learnable with $d$-nonadaptive statistical queries can be learned via a $(d+1)$-list replicable algorithm and also via a $\\tilde{O}(\\log d)$-certificate replicable algorithm. The sample complexity of both these algorithms is $\\tilde{O}(\\frac{d^2}{\\nu^2})$ where $\\nu$ is the approximation error of the statistical query. We also show that for the concept class \\dtep, the list complexity is exactly $d+1$ with respect to the uniform distribution. To establish our upper bound results we use rounding schemes induced by geometric partitions with certain properties. We use Sperner/KKM Lemma to establish the lower bound results",
    "keywords": [],
    "checked": true,
    "id": "49c60c7e69b70a44ed63f40ced4f8e5766ef2ace",
    "semantic_title": "list and certificate complexities in replicable learning",
    "citation_count": 4,
    "authors": [
      "Peter Dixon",
      "A. Pavan",
      "Jason Vander Woude",
      "N. V.  Vinodchandran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/61f4e5747b1b753cb35546b15d981f76-Abstract-Conference.html": {
    "title": "Flat Seeking Bayesian Neural Networks",
    "volume": "main",
    "abstract": "Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferring a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with lower sharpness have better generalization ability. However, existing posterior inferences are not aware of sharpness/flatness in terms of formulation, possibly leading to high sharpness for the models sampled from them. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior, and the optimal approximate posterior estimating this sharpness-aware posterior, have better flatness, hence possibly possessing higher generalization ability. We conduct experiments by leveraging the sharpness-aware posterior with state-of-the-art Bayesian Neural Networks, showing that the flat-seeking counterparts outperform their baselines in all metrics of interest",
    "keywords": [],
    "checked": true,
    "id": "6c11fb8fb7084547ff4be63d8ddb6f0dfee71765",
    "semantic_title": "flat seeking bayesian neural networks",
    "citation_count": 2,
    "authors": [
      "Van-Anh Nguyen",
      "Tung-Long Vuong",
      "Hoang Phan",
      "Thanh-Toan Do",
      "Dinh Phung",
      "Trung Le"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/621d0fd41c720ab252e178b77c200d90-Abstract-Conference.html": {
    "title": "Enhancing User Intent Capture in Session-Based Recommendation with Attribute Patterns",
    "volume": "main",
    "abstract": "The goal of session-based recommendation in E-commerce is to predict the next item that an anonymous user will purchase based on the browsing and purchase history. However, constructing global or local transition graphs to supplement session data can lead to noisy correlations and user intent vanishing. In this work, we propose the Frequent Attribute Pattern Augmented Transformer (FAPAT) that characterizes user intents by building attribute transition graphs and matching attribute patterns. Specifically, the frequent and compact attribute patterns are served as memory to augment session representations, followed by a gate and a transformer block to fuse the whole session information. Through extensive experiments on two public benchmarks and 100 million industrial data in three domains, we demonstrate that FAPAT consistently outperforms state-of-the-art methods by an average of 4.5% across various evaluation metrics (Hits, NDCG, MRR). Besides evaluating the next-item prediction, we estimate the models' capabilities to capture user intents via predicting items' attributes and period-item recommendations",
    "keywords": [],
    "checked": true,
    "id": "e23cafac4fefdf7fd80e920224f264ccade4fac2",
    "semantic_title": "enhancing user intent capture in session-based recommendation with attribute patterns",
    "citation_count": 0,
    "authors": [
      "Xin Liu",
      "Zheng Li",
      "Yifan Gao",
      "Jingfeng Yang",
      "Tianyu Cao",
      "Zhengyang Wang",
      "Bing Yin",
      "Yangqiu Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/622afc4edf2824a1b6aaf5afe153fa93-Abstract-Conference.html": {
    "title": "Can Language Models Solve Graph Problems in Natural Language?",
    "volume": "main",
    "abstract": "Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question",
    "keywords": [],
    "checked": true,
    "id": "df2beaae63e4d68ef8e762bcd4704c9f11f856d9",
    "semantic_title": "can language models solve graph problems in natural language?",
    "citation_count": 46,
    "authors": [
      "Heng Wang",
      "Shangbin Feng",
      "Tianxing He",
      "Zhaoxuan Tan",
      "Xiaochuang Han",
      "Yulia Tsvetkov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/623e5a86fcedca573d33390dd1173e6b-Abstract-Conference.html": {
    "title": "Language-driven Scene Synthesis using Multi-conditional Diffusion Model",
    "volume": "main",
    "abstract": "Scene synthesis is a challenging problem with several industrial applications. Recently, substantial efforts have been directed to synthesize the scene using human motions, room layouts, or spatial graphs as the input. However, few studies have addressed this problem from multiple modalities, especially combining text prompts. In this paper, we propose a language-driven scene synthesis task, which is a new task that integrates text prompts, human motion, and existing objects for scene synthesis. Unlike other single-condition synthesis tasks, our problem involves multiple conditions and requires a strategy for processing and encoding them into a unified space. To address the challenge, we present a multi-conditional diffusion model, which differs from the implicit unification approach of other diffusion literature by explicitly predicting the guiding points for the original data distribution. We demonstrate that our approach is theoretically supportive. The intensive experiment results illustrate that our method outperforms state-of-the-art benchmarks and enables natural scene editing applications. The source code and dataset can be accessed at https://lang-scene-synth.github.io/",
    "keywords": [],
    "checked": true,
    "id": "a71678abb2e5e3aae7b3f2a08a5bffc87036d8f1",
    "semantic_title": "language-driven scene synthesis using multi-conditional diffusion model",
    "citation_count": 1,
    "authors": [
      "An Dinh Vuong",
      "Minh Nhat VU",
      "Toan Nguyen",
      "Baoru Huang",
      "Dzung Nguyen",
      "Thieu Vo",
      "Anh Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6274172f7d981a8d58bbfd52342a9d1f-Abstract-Conference.html": {
    "title": "Implicit Contrastive Representation Learning with Guided Stop-gradient",
    "volume": "main",
    "abstract": "In self-supervised representation learning, Siamese networks are a natural architecture for learning transformation-invariance by bringing representations of positive pairs closer together. But it is prone to collapse into a degenerate solution. To address the issue, in contrastive learning, a contrastive loss is used to prevent collapse by moving representations of negative pairs away from each other. But it is known that algorithms with negative sampling are not robust to a reduction in the number of negative samples. So, on the other hand, there are algorithms that do not use negative pairs. Many positive-only algorithms adopt asymmetric network architecture consisting of source and target encoders as a key factor in coping with collapse. By exploiting the asymmetric architecture, we introduce a methodology to implicitly incorporate the idea of contrastive learning. As its implementation, we present a novel method guided stop-gradient. We apply our method to benchmark algorithms SimSiam and BYOL and show that our method stabilizes training and boosts performance. We also show that the algorithms with our method work well with small batch sizes and do not collapse even when there is no predictor. The code is available in the supplementary material",
    "keywords": [],
    "checked": false,
    "id": "fccada3fc530ea98d612126399f13ecb0844fc21",
    "semantic_title": "contrast with reconstruct: contrastive 3d representation learning guided by generative pretraining",
    "citation_count": 33,
    "authors": [
      "Byeongchan Lee",
      "Sehyun Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/62bf42cc047db5b290e7d5737c1f6a8d-Abstract-Conference.html": {
    "title": "Improved Best-of-Both-Worlds Guarantees for Multi-Armed Bandits: FTRL with General Regularizers and Multiple Optimal Arms",
    "volume": "main",
    "abstract": "We study the problem of designing adaptive multi-armed bandit algorithms that perform optimally in both the stochastic setting and the adversarial setting simultaneously (often known as a best-of-both-world guarantee). A line of recent works shows that when configured and analyzed properly, the Follow-the-Regularized-Leader (FTRL) algorithm, originally designed for the adversarial setting, can in fact optimally adapt to the stochastic setting as well. Such results, however, critically rely on an assumption that there exists one unique optimal arm. Recently, Ito [2021] took the first step to remove such an undesirable uniqueness assumption for one particular FTRL algorithm withthe 1/2-Tsallis entropy regularizer. In this work, we significantly improve and generalize this result, showing that uniqueness is unnecessary for FTRL with a broad family of regularizers and a new learning rate schedule. For some regularizers, our regret bounds also improve upon prior results even when uniqueness holds. We further provide an application of our results to the decoupled exploration and exploitation problem, demonstrating that our techniques are broadly applicable",
    "keywords": [],
    "checked": true,
    "id": "165bf26ca3c0c05f5b24d711c36afa03b89be4ec",
    "semantic_title": "improved best-of-both-worlds guarantees for multi-armed bandits: ftrl with general regularizers and multiple optimal arms",
    "citation_count": 5,
    "authors": [
      "Tiancheng Jin",
      "Junyan Liu",
      "Haipeng Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/62c9aa4d48329a85d1e36d5b6d0a6a32-Abstract-Conference.html": {
    "title": "AiluRus: A Scalable ViT Framework for Dense Prediction",
    "volume": "main",
    "abstract": "Vision transformers (ViTs) have emerged as a prevalent architecture for vision tasks owing to their impressive performance. However, their complexity dramatically increases when handling long token sequences, particularly for dense prediction tasks that require high-resolution input. Notably, dense prediction tasks, such as semantic segmentation or object detection, emphasize more on the contours or shapes of objects, while the texture inside objects is less informative. Motivated by this observation, we propose to apply adaptive resolution for different regions in the image according to their importance. Specifically, at the intermediate layer of the ViT, we select anchors from the token sequence using the proposed spatial-aware density-based clustering algorithm. Tokens that are adjacent to anchors are merged to form low-resolution regions, while others are preserved independently as high-resolution. This strategy could significantly reduce the number of tokens, and the following layers only handle the reduced token sequence for acceleration. At the output end, the resolution of the feature map is recovered by unfolding merged tokens for task prediction. Consequently, we can considerably accelerate ViTs for dense prediction tasks. The proposed method is evaluated across three different datasets and demonstrates promising performance. For instance, \"Segmenter ViT-L\" can be accelerated by 48\\% FPS without fine-tuning, while maintaining the performance. Moreover, our method can also be applied to accelerate fine-tuning. Experiments indicate that we can save 52\\% training time while accelerating 2.46$\\times$ FPS with only a 0.09\\% performance drop",
    "keywords": [],
    "checked": true,
    "id": "8061423f63cd8712fb9bfbc3d6a07501895ca504",
    "semantic_title": "ailurus: a scalable vit framework for dense prediction",
    "citation_count": 1,
    "authors": [
      "Jin Li",
      "Yaoming Wang",
      "XIAOPENG ZHANG",
      "Bowen Shi",
      "Dongsheng Jiang",
      "Chenglin Li",
      "Wenrui Dai",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/631ad9ae3174bf4d6c0f6fdca77335a4-Abstract-Conference.html": {
    "title": "LightSpeed: Light and Fast Neural Light Fields on Mobile Devices",
    "volume": "main",
    "abstract": "Real-time novel-view image synthesis on mobile devices is prohibitive due to the limited computational power and storage. Using volumetric rendering methods, such as NeRF and its derivatives, on mobile devices is not suitable due to the high computational cost of volumetric rendering. On the other hand, recent advances in neural light field representations have shown promising real-time view synthesis results on mobile devices. Neural light field methods learn a direct mapping from a ray representation to the pixel color. The current choice of ray representation is either stratified ray sampling or Plücker coordinates, overlooking the classic light slab (two-plane) representation, the preferred representation to interpolate between light field views. In this work, we find that using the light slab representation is an efficient representation for learning a neural light field. More importantly, it is a lower-dimensional ray representation enabling us to learn the 4D ray space using feature grids which are significantly faster to train and render. Although mostly designed for frontal views, we show that the light-slab representation can be further extended to non-frontal scenes using a divide-and-conquer strategy. Our method provides better rendering quality than prior light field methods and a significantly better trade-off between rendering quality and speed than prior light field methods",
    "keywords": [],
    "checked": true,
    "id": "092797a2735fdd36089f52d11ca49bc6bf90cee2",
    "semantic_title": "lightspeed: light and fast neural light fields on mobile devices",
    "citation_count": 1,
    "authors": [
      "Aarush Gupta",
      "Junli Cao",
      "Chaoyang Wang",
      "Ju Hu",
      "Sergey Tulyakov",
      "Jian Ren",
      "László Jeni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/631bb9434d718ea309af82566347d607-Abstract-Conference.html": {
    "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models",
    "volume": "main",
    "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijing Jin",
      "Yuen Chen",
      "Felix Leeb",
      "Luigi Gresele",
      "Ojasv Kamal",
      "Zhiheng LYU",
      "Kevin Blin",
      "Fernando Gonzalez Adauto",
      "Max Kleiman-Weiner",
      "Mrinmaya Sachan",
      "Bernhard Schölkopf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/631f99d8e860054410c239fc90d18270-Abstract-Conference.html": {
    "title": "Riemannian Laplace approximations for Bayesian neural networks",
    "volume": "main",
    "abstract": "Bayesian neural networks often approximate the weight-posterior with a Gaussian distribution. However, practical posteriors are often, even locally, highly non-Gaussian, and empirical performance deteriorates. We propose a simple parametric approximate posterior that adapts to the shape of the true posterior through a Riemannian metric that is determined by the log-posterior gradient. We develop a Riemannian Laplace approximation where samples naturally fall into weight-regions with low negative log-posterior. We show that these samples can be drawn by solving a system of ordinary differential equations, which can be done efficiently by leveraging the structure of the Riemannian metric and automatic differentiation. Empirically, we demonstrate that our approach consistently improves over the conventional Laplace approximation across tasks. We further show that, unlike the conventional Laplace approximation, our method is not overly sensitive to the choice of prior, which alleviates a practical pitfall of current approaches",
    "keywords": [],
    "checked": true,
    "id": "57ff0410cfabd77d2846bda1aef9744da15e475e",
    "semantic_title": "riemannian laplace approximations for bayesian neural networks",
    "citation_count": 4,
    "authors": [
      "Federico Bergamin",
      "Pablo Moreno-Muñoz",
      "Søren Hauberg",
      "Georgios Arvanitidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6357d6d068622c962391081d296bed69-Abstract-Conference.html": {
    "title": "Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL",
    "volume": "main",
    "abstract": "In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec",
    "keywords": [],
    "checked": true,
    "id": "86acc5ad9c3476630555b442d1d10d9c75de8c6a",
    "semantic_title": "contrastive retrospection: honing in on critical steps for rapid learning and generalization in rl",
    "citation_count": 0,
    "authors": [
      "Chen Sun",
      "Wannan Yang",
      "Thomas Jiralerspong",
      "Dane Malenfant",
      "Benjamin Alsbury-Nealy",
      "Yoshua Bengio",
      "Blake Richards"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/636d57c09a5baacd83722639265802f6-Abstract-Conference.html": {
    "title": "A Hierarchical Training Paradigm for Antibody Structure-sequence Co-design",
    "volume": "main",
    "abstract": "Therapeutic antibodies are an essential and rapidly flourishing drug modality. The binding specificity between antibodies and antigens is decided by complementarity-determining regions (CDRs) at the tips of these Y-shaped proteins. In this paper, we propose a \\textbf{h}ierarchical \\textbf{t}raining \\textbf{p}aradigm (HTP) for the antibody sequence-structure co-design. HTP consists of four levels of training stages, each corresponding to a specific protein modality within a particular protein domain. Through carefully crafted tasks in different stages, HTP seamlessly and effectively integrates geometric graph neural networks (GNNs) with large-scale protein language models to excavate evolutionary information from not only geometric structures but also vast antibody and non-antibody sequence databases, which determines ligand binding pose and strength. Empirical experiments show HTP sets the new state-of-the-art performance in the co-design problem as well as the fix-backbone design. Our research offers a hopeful path to unleash the potential of deep generative architectures and seeks to illuminate the way forward for the antibody sequence and structure co-design challenge",
    "keywords": [],
    "checked": true,
    "id": "9fb64fe39161eb42a88fbbb7b0010f630bc0da8e",
    "semantic_title": "a hierarchical training paradigm for antibody structure-sequence co-design",
    "citation_count": 1,
    "authors": [
      "Fang Wu",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/637a456d89289769ac1ab29617ef7213-Abstract-Conference.html": {
    "title": "Differentiable Clustering with Perturbed Spanning Forests",
    "volume": "main",
    "abstract": "We introduce a differentiable clustering method based on stochastic perturbations of minimum-weight spanning forests. This allows us to include clustering in end-to-end trainable pipelines, with efficient gradients. We show that our method performs well even in difficult settings, such as data sets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several data sets for supervised and semi-supervised tasks",
    "keywords": [],
    "checked": true,
    "id": "56750799e0f0fe956b63554a53993a7aa7960351",
    "semantic_title": "differentiable clustering with perturbed spanning forests",
    "citation_count": 2,
    "authors": [
      "Lawrence Stewart",
      "Francis Bach",
      "Felipe Llinares-Lopez",
      "Quentin Berthet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/637df18481a6aa74238bd2cafff94cb9-Abstract-Conference.html": {
    "title": "Logarithmic-Regret Quantum Learning Algorithms for Zero-Sum Games",
    "volume": "main",
    "abstract": "We propose the first online quantum algorithm for zero-sum games with $\\widetilde O(1)$ regret under the game setting. Moreover, our quantum algorithm computes an $\\varepsilon$-approximate Nash equilibrium of an $m \\times n$ matrix zero-sum game in quantum time $\\widetilde O(\\sqrt{m+n}/\\varepsilon^{2.5})$. Our algorithm uses standard quantum inputs and generates classical outputs with succinct descriptions, facilitating end-to-end applications. As an application, we obtain a fast quantum linear programming solver. Technically, our online quantum algorithm \"quantizes\" classical algorithms based on the optimistic multiplicative weight update method. At the heart of our algorithm is a fast quantum multi-sampling procedure for the Gibbs sampling problem, which may be of independent interest",
    "keywords": [],
    "checked": true,
    "id": "5987e98865ca6c7b31cae802d77c5c8ba4b19a6f",
    "semantic_title": "logarithmic-regret quantum learning algorithms for zero-sum games",
    "citation_count": 3,
    "authors": [
      "Minbo Gao",
      "Zhengfeng Ji",
      "Tongyang Li",
      "Qisheng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html": {
    "title": "Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network",
    "volume": "main",
    "abstract": "Generative Flow Networks (GFlowNets), a class of generative models over discrete and structured sample spaces, have been previously applied to the problem of inferring the marginal posterior distribution over the directed acyclic graph (DAG) of a Bayesian Network, given a dataset of observations. Based on recent advances extending this framework to non-discrete sample spaces, we propose in this paper to approximate the joint posterior over not only the structure of a Bayesian Network, but also the parameters of its conditional probability distributions. We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known. Since the parameters are included in the posterior distribution, this leaves more flexibility for the local probability models of the Bayesian Network, making our approach applicable even to non-linear models parametrized by neural networks. We show that our method, called JSP-GFN, offers an accurate approximation of the joint posterior, while comparing favorably against existing methods on both simulated and real data",
    "keywords": [],
    "checked": true,
    "id": "6e5e65ec544bc7ec1c18954f678ae586dc553ea1",
    "semantic_title": "joint bayesian inference of graphical structure and parameters with a single generative flow network",
    "citation_count": 13,
    "authors": [
      "Tristan Deleu",
      "Mizu Nishikawa-Toomey",
      "Jithendaraa Subramanian",
      "Nikolay Malkin",
      "Laurent Charlin",
      "Yoshua Bengio"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/63d4316315900a62e610e5c17bab900a-Abstract-Conference.html": {
    "title": "Three Towers: Flexible Contrastive Learning with Pretrained Image Models",
    "volume": "main",
    "abstract": "We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT (Zhai et al., 2022) has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits from training the image tower contrastively. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperforms relative to LiT for JFT-pretrained models, it outperforms LiT for ImageNet-21k and Places365 pretraining",
    "keywords": [],
    "checked": true,
    "id": "5554b54916fc110e23a0bed6e8af1e816e59c734",
    "semantic_title": "three towers: flexible contrastive learning with pretrained image models",
    "citation_count": 1,
    "authors": [
      "Jannik Kossen",
      "Mark Collier",
      "Basil Mustafa",
      "Xiao Wang",
      "Xiaohua Zhai",
      "Lucas Beyer",
      "Andreas Steiner",
      "Jesse Berent",
      "Rodolphe Jenatton",
      "Effrosyni Kokiopoulou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/63e8bc7bbf1cfea36d1d1b6538aecce5-Abstract-Conference.html": {
    "title": "Practical and Asymptotically Exact Conditional Sampling in Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have been successful on a range of conditional generation tasks including molecular design and text-to-image generation. However, these achievements have primarily depended on task-specific conditional training or error-prone heuristic approximations. Ideally, a conditional generation method should provide exact samples for a broad range of conditional distributions without requiring taskspecific training. To this end, we introduce the Twisted Diffusion Sampler, or TDS. TDS is a sequential Monte Carlo (SMC) algorithm that targets the conditional distributions of diffusion models. The main idea is to use twisting, an SMC technique that enjoys good computational efficiency, to incorporate heuristic approximations without compromising asymptotic exactness. We first find in simulation and in conditional image generation tasks that TDS provides a computational statistical trade-off, yielding more accurate approximations with many particles but with empirical improvements over heuristics with as few as two particles. We then turn to motif-scaffolding, a core task in protein design, using a TDS extension to Riemannian diffusion models; on benchmark test cases, TDS allows flexible conditioning criteria and often outperforms the state-of-the-art, conditionally trained model",
    "keywords": [],
    "checked": true,
    "id": "79531b47bb27cb18022891eb2ab1fcb41745fca6",
    "semantic_title": "practical and asymptotically exact conditional sampling in diffusion models",
    "citation_count": 15,
    "authors": [
      "Luhuan Wu",
      "Brian Trippe",
      "Christian Naesseth",
      "David Blei",
      "John P. Cunningham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/63ef323523f3be8b58ed9277cc747485-Abstract-Conference.html": {
    "title": "Actively Testing Your Model While It Learns: Realizing Label-Efficient Learning in Practice",
    "volume": "main",
    "abstract": "In active learning (AL), we focus on reducing the data annotation cost from the model training perspective. However, \"testing'', which often refers to the model evaluation process of using empirical risk to estimate the intractable true generalization risk, also requires data annotations. The annotation cost for \"testing'' (model evaluation) is under-explored. Even in works that study active model evaluation or active testing (AT), the learning and testing ends are disconnected. In this paper, we propose a novel active testing while learning (ATL) framework that integrates active learning with active testing. ATL provides an unbiased sample-efficient estimation of the model risk during active learning. It leverages test samples annotated from different periods of a dynamic active learning process to achieve fair model evaluations based on a theoretically guaranteed optimal integration of different test samples. Periodic testing also enables effective early-stopping to further save the total annotation cost. ATL further integrates an \"active feedback'' mechanism, which is inspired by human learning, where the teacher (active tester) provides immediate guidance given by the prior performance of the student (active learner). Our theoretical result reveals that active feedback maintains the label complexity of the integrated learning-testing objective, while improving the model's generalization capability. We study the realistic setting where we maximize the performance gain from choosing \"testing'' samples for feedback without sacrificing the risk estimation accuracy. An agnostic-style analysis and empirical evaluations on real-world datasets demonstrate that the ATL framework can effectively improve the annotation efficiency of both active learning and evaluation tasks",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayou Yu",
      "Weishi Shi",
      "Qi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/642a321fba8a0f03765318e629cb93ea-Abstract-Conference.html": {
    "title": "Causal Interpretation of Self-Attention in Pre-Trained Transformers",
    "volume": "main",
    "abstract": "We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks: sentiment classification (NLP) and recommendation",
    "keywords": [],
    "checked": true,
    "id": "085cd67ae1a544b5aced388d3c42e5675cbd62b4",
    "semantic_title": "causal interpretation of self-attention in pre-trained transformers",
    "citation_count": 0,
    "authors": [
      "Raanan Y. Rohekar",
      "Yaniv Gurwicz",
      "Shami Nisimov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6445dd88ebb9a6a3afa0b126ad87fe41-Abstract-Conference.html": {
    "title": "Parsel🐍: Algorithmic Reasoning with Language Models by Composing Decompositions",
    "volume": "main",
    "abstract": "Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel",
    "keywords": [],
    "checked": false,
    "id": "e325fe41c8c1d547ccd102ac82be3ec8b23960f2",
    "semantic_title": "parsel: algorithmic reasoning with language models by composing decompositions",
    "citation_count": 6,
    "authors": [
      "Eric Zelikman",
      "Qian Huang",
      "Gabriel Poesia",
      "Noah Goodman",
      "Nick Haber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6447714b83edcbed61dbe10371dd7ae5-Abstract-Conference.html": {
    "title": "Focus on Query: Adversarial Mining Transformer for Few-Shot Segmentation",
    "volume": "main",
    "abstract": "Few-shot segmentation (FSS) aims to segment objects of new categories given only a handful of annotated samples. Previous works focus their efforts on exploring the support information while paying less attention to the mining of the critical query branch. In this paper, we rethink the importance of support information and propose a new query-centric FSS model Adversarial Mining Transformer (AMFormer), which achieves accurate query image segmentation with only rough support guidance or even weak support labels. The proposed AMFormer enjoys several merits. First, we design an object mining transformer (G) that can achieve the expansion of incomplete region activated by support clue, and a detail mining transformer (D) to discriminate the detailed local difference between the expanded mask and the ground truth. Second, we propose to train G and D via an adversarial process, where G is optimized to generate more accurate masks approaching ground truth to fool D. We conduct extensive experiments on commonly used Pascal-5i and COCO-20i benchmarks and achieve state-of-the-art results across all settings. In addition, the decent performance with weak support labels in our query-centric paradigm may inspire the development of more general FSS models",
    "keywords": [],
    "checked": true,
    "id": "e6a71d661dc15346d140530ee40b61249c0b3829",
    "semantic_title": "focus on query: adversarial mining transformer for few-shot segmentation",
    "citation_count": 0,
    "authors": [
      "Yuan Wang",
      "Naisong Luo",
      "Tianzhu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6450ea28ebbc8437bc38775157818172-Abstract-Conference.html": {
    "title": "Improving Language Plasticity via Pretraining with Active Forgetting",
    "volume": "main",
    "abstract": "Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation, but also outperform standard ones in a low-data regime, particularly for languages that are distant from English. Code will be available at https://github.com/facebookresearch/language-model-plasticity",
    "keywords": [],
    "checked": true,
    "id": "9a2f47777b99a92effb4e998b7082e1e92ae13bc",
    "semantic_title": "improving language plasticity via pretraining with active forgetting",
    "citation_count": 4,
    "authors": [
      "Yihong Chen",
      "Kelly Marchisio",
      "Roberta Raileanu",
      "David Adelani",
      "Pontus Lars Erik Saito Stenetorp",
      "Sebastian Riedel",
      "Mikel Artetxe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6452474601429509f3035dc81c233226-Abstract-Conference.html": {
    "title": "Stability of Random Forests and Coverage of Random-Forest Prediction Intervals",
    "volume": "main",
    "abstract": "We establish stability of random forests under the mild condition that the squared response ($Y^2$) does not have a heavy tail. In particular, our analysis holds for the practical version of random forests that is implemented in popular packages like \\texttt{randomForest} in \\texttt{R}. Empirical results show that stability may persist even beyond our assumption and hold for heavy-tailed $Y^2$. Using the stability property, we prove a non-asymptotic lower bound for the coverage probability of prediction intervals constructed from the out-of-bag error of random forests. With another mild condition that is typically satisfied when $Y$ is continuous, we also establish a complementary upper bound, which can be similarly established for the jackknife prediction interval constructed from an arbitrary stable algorithm. We also discuss the asymptotic coverage probability under assumptions weaker than those considered in previous literature. Our work implies that random forests, with its stability property, is an effective machine learning method that can provide not only satisfactory point prediction but also justified interval prediction at almost no extra computational cost",
    "keywords": [],
    "checked": true,
    "id": "e3597414d6cc5f741636f98eb0181ba7616c7762",
    "semantic_title": "stability of random forests and coverage of random-forest prediction intervals",
    "citation_count": 0,
    "authors": [
      "Yan Wang",
      "Huaiqing Wu",
      "Dan Nettleton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/64602b87c31db70a3ef060f6c5d5b01d-Abstract-Conference.html": {
    "title": "Multi-Fidelity Multi-Armed Bandits Revisited",
    "volume": "main",
    "abstract": "We study the multi-fidelity multi-armed bandit ($\\texttt{MF-MAB}$), an extension of the canonical multi-armed bandit (MAB) problem.$\\texttt{MF-MAB}$ allows each arm to be pulled with different costs (fidelities) and observation accuracy.We study both the best arm identification with fixed confidence ($\\texttt{BAI}$) and the regret minimization objectives.For $\\texttt{BAI}$, we present (a) a cost complexity lower bound, (b) an algorithmic framework with two alternative fidelity selection procedures,and (c) both procedures' cost complexity upper bounds.From both cost complexity bounds of $\\texttt{MF-MAB}$,one can recover the standard sample complexity bounds of the classic (single-fidelity) MAB.For regret minimization of $\\texttt{MF-MAB}$, we propose a new regret definition, prove its problem-independent regret lower bound $\\Omega(K^{1/3}\\Lambda^{2/3})$ and problem-dependent lower bound $\\Omega(K\\log \\Lambda)$, where $K$ is the number of arms and $\\Lambda$ is the decision budget in terms of cost, and devise an elimination-based algorithm whose worst-cost regret upper bound matches its corresponding lower bound up to some logarithmic terms and, whose problem-dependent bound matches its corresponding lower bound in terms of $\\Lambda$",
    "keywords": [],
    "checked": true,
    "id": "ae5ccb6689b00fc7d342460fb17a3bca60f88f26",
    "semantic_title": "multi-fidelity multi-armed bandits revisited",
    "citation_count": 0,
    "authors": [
      "Xuchuang Wang",
      "Qingyun Wu",
      "Wei Chen",
      "John C.S. Lui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6464638c2472e4cae607f0c96a6fe774-Abstract-Conference.html": {
    "title": "Augmentation-Aware Self-Supervision for Data-Efficient GAN Training",
    "volume": "main",
    "abstract": "Training generative adversarial networks (GANs) with limited data is challenging because the discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversarially learn from the self-supervised discriminator by generating augmentation-predictable real and not fake data. This formulation connects the learning objective of the generator and the arithmetic $-$ harmonic mean divergence under certain assumptions. We compare our method with state-of-the-art (SOTA) methods using the class-conditional BigGAN and unconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100, FFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstrate significant improvements of our method over SOTA methods in training data-efficient GANs",
    "keywords": [],
    "checked": true,
    "id": "ea12d7594d62d84ff4095226c66fba655d74e7a2",
    "semantic_title": "augmentation-aware self-supervision for data-efficient gan training",
    "citation_count": 1,
    "authors": [
      "Liang Hou",
      "Qi Cao",
      "Yige Yuan",
      "Songtao Zhao",
      "Chongyang Ma",
      "Siyuan Pan",
      "Pengfei Wan",
      "Zhongyuan Wang",
      "Huawei Shen",
      "Xueqi Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/64792f7bd5d400c9ac310c6fef97ef2d-Abstract-Conference.html": {
    "title": "Template-free Articulated Neural Point Clouds for Reposable View Synthesis",
    "volume": "main",
    "abstract": "Dynamic Neural Radiance Fields (NeRFs) achieve remarkable visual quality when synthesizing novel views of time-evolving 3D scenes. However, the common reliance on backward deformation fields makes reanimation of the captured object poses challenging. Moreover, the state of the art dynamic models are often limited by low visual fidelity, long reconstruction time or specificity to narrow application domains. In this paper, we present a novel method utilizing a point-based representation and Linear Blend Skinning (LBS) to jointly learn a Dynamic NeRF and an associated skeletal model from even sparse multi-view video. Our forward-warping approach achieves state-of-the-art visual fidelity when synthesizing novel views and poses while significantly reducing the necessary learning time when compared to existing work. We demonstrate the versatility of our representation on a variety of articulated objects from common datasets and obtain reposable 3D reconstructions without the need of object-specific skeletal templates",
    "keywords": [],
    "checked": true,
    "id": "1956e9c389f8d3a765fccc297f9e2bb87cef9685",
    "semantic_title": "template-free articulated neural point clouds for reposable view synthesis",
    "citation_count": 2,
    "authors": [
      "Lukas Uzolas",
      "Elmar Eisemann",
      "Petr Kellnhofer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/647e122fc406573c51276692f20379b5-Abstract-Conference.html": {
    "title": "Demystifying the Optimal Performance of Multi-Class Classification",
    "volume": "main",
    "abstract": "Classification is a fundamental task in science and engineering on which machine learning methods have shown outstanding performances. However, it is challenging to determine whether such methods have achieved the Bayes error rate, that is, the lowest error rate attained by any classifier. This is mainly due to the fact that the Bayes error rate is not known in general and hence, effectively estimating it is paramount. Inspired by the work by Ishida et al. (2023), we propose an estimator for the Bayes error rate of supervised multi-class classification problems. We analyze several theoretical aspects of such estimator, including its consistency, unbiasedness, convergence rate, variance, and robustness. We also propose a denoising method that reduces the noise that potentially corrupts the data labels, and we improve the robustness of the proposed estimator to outliers by incorporating the median-of-means estimator. Our analysis demonstrates the consistency, asymptotic unbiasedness, convergence rate, and robustness of the proposed estimators. Finally, we validate the effectiveness of our theoretical results via experiments both on synthetic data under various noise settings and on real data",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minoh Jeong",
      "Martina Cardone",
      "Alex Dytso"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/64ae05e3f1a88ebac7f9263b69f4e702-Abstract-Conference.html": {
    "title": "Structured Voronoi Sampling",
    "volume": "main",
    "abstract": "Gradient-based sampling algorithms have demonstrated their effectiveness in text generation, especially in the context of controlled text generation. However, there exists a lack of theoretically grounded and principled approaches for this task. In this paper, we take an important step toward building a principled approach for sampling from language models with gradient-based methods. We use discrete distributions given by language models to define densities and develop an algorithm based on Hamiltonian Monte Carlo to sample from them. We name our gradient-based technique Structured Voronoi Sampling (SVS). In an experimental setup where the reference distribution is known, we show that the empirical distribution of SVS samples is closer to the reference distribution compared to alternative sampling schemes. Furthermore, in a controlled generation task, SVS is able to generate fluent and diverse samples while following the control targets significantly better than other methods",
    "keywords": [],
    "checked": true,
    "id": "b0b293312d0314b56f1e59e00441f3f5873e2389",
    "semantic_title": "structured voronoi sampling",
    "citation_count": 1,
    "authors": [
      "Afra Amini",
      "Li Du",
      "Ryan Cotterell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/64e2449d74f84e5b1a5c96ba7b3d308e-Abstract-Conference.html": {
    "title": "Stability and Generalization of the Decentralized Stochastic Gradient Descent Ascent Algorithm",
    "volume": "main",
    "abstract": "The growing size of available data has attracted increasing interest in solving minimax problems in a decentralized manner for various machine learning tasks. Previous theoretical research has primarily focused on the convergence rate and communication complexity of decentralized minimax algorithms, with little attention given to their generalization. In this paper, we investigate the primal-dual generalization bound of the decentralized stochastic gradient descent ascent (D-SGDA) algorithm using the approach of algorithmic stability under both convex-concave and nonconvex-nonconcave settings. Our theory refines the algorithmic stability in a decentralized manner and demonstrates that the decentralized structure does not destroy the stability and generalization of D-SGDA, implying that it can generalize as well as the vanilla SGDA in certain situations. Our results analyze the impact of different topologies on the generalization bound of the D-SGDA algorithm beyond trivial factors such as sample sizes, learning rates, and iterations. We also evaluate the optimization error and balance it with the generalization gap to obtain the optimal population risk of D-SGDA in the convex-concave setting. Additionally, we perform several numerical experiments which validate our theoretical findings",
    "keywords": [],
    "checked": true,
    "id": "bc7e1c25cacedd20d965e86cfb071eda16da0310",
    "semantic_title": "stability and generalization of the decentralized stochastic gradient descent ascent algorithm",
    "citation_count": 0,
    "authors": [
      "Miaoxi Zhu",
      "Li Shen",
      "Bo Du",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6521937507d78f327cd402401be73bf2-Abstract-Conference.html": {
    "title": "Hierarchical clustering with dot products recovers hidden tree structure",
    "volume": "main",
    "abstract": "In this paper we offer a new perspective on the well established agglomerative clustering algorithm, focusing on recovery of hierarchical structure. We recommend a simple variant of the standard algorithm, in which clusters are merged by maximum average dot product and not, for example, by minimum distance or within-cluster variance. We demonstrate that the tree output by this algorithm provides a bona fide estimate of generative hierarchical structure in data, under a generic probabilistic graphical model. The key technical innovations are to understand how hierarchical information in this model translates into tree geometry which can be recovered from data, and to characterise the benefits of simultaneously growing sample size and data dimension. We demonstrate superior tree recovery performance with real data over existing approaches such as UPGMA, Ward's method, and HDBSCAN",
    "keywords": [],
    "checked": true,
    "id": "e7a5ee90c6c79710d517a187f20f776b769081c6",
    "semantic_title": "hierarchical clustering with dot products recovers hidden tree structure",
    "citation_count": 0,
    "authors": [
      "Annie Gray",
      "Alexander Modell",
      "Patrick Rubin-Delanchy",
      "Nick Whiteley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6521bd47ebaa28228cd6c74cb85afb65-Abstract-Conference.html": {
    "title": "Latent Field Discovery in Interacting Dynamical Systems with Neural Fields",
    "volume": "main",
    "abstract": "Systems of interacting objects often evolve under the influence of underlying field effects that govern their dynamics, yet previous works have abstracted away from such effects, and assume that systems evolve in a vacuum. In this work, we focus on discovering these fields, and infer them from the observed dynamics alone, without directly observing them. We theorize the presence of latent force fields, and propose neural fields to learn them. Since the observed dynamics constitute the net effect of local object interactions and global field effects, recently popularized equivariant networks are inapplicable, as they fail to capture global information. To address this, we propose to disentangle local object interactions --which are SE(3) equivariant and depend on relative states-- from external global field effects --which depend on absolute states. We model the interactions with equivariant graph networks, and combine them with neural fields in a novel graph network that integrates field forces. Our experiments show that we can accurately discover the underlying fields in charged particles settings, traffic scenes, and gravitational n-body problems, and effectively use them to learn the system and forecast future trajectories",
    "keywords": [],
    "checked": true,
    "id": "5a367c798aa989555b502cee603885459a00e97a",
    "semantic_title": "latent field discovery in interacting dynamical systems with neural fields",
    "citation_count": 1,
    "authors": [
      "Miltiadis (Miltos) Kofinas",
      "Erik Bekkers",
      "Naveen Nagaraja",
      "Efstratios Gavves"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6530db249c161fe9254db2667453952c-Abstract-Conference.html": {
    "title": "Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration",
    "volume": "main",
    "abstract": "A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then maximizes their average. By only considering the visited states with similar value estimates for computing the intrinsic bonus, our method prevents the distribution of low-value states from affecting exploration around high-value states, and vice versa. We demonstrate that the proposed alternative to the state entropy baseline significantly accelerates various reinforcement learning algorithms across a variety of tasks within MiniGrid, DeepMind Control Suite, and Meta-World benchmarks. Source code is available at https://sites.google.com/view/rl-vcse",
    "keywords": [],
    "checked": true,
    "id": "13baa03290e1575ccdff08ecf582f82837ac8121",
    "semantic_title": "accelerating reinforcement learning with value-conditional state entropy exploration",
    "citation_count": 3,
    "authors": [
      "Dongyoung Kim",
      "Jinwoo Shin",
      "Pieter Abbeel",
      "Younggyo Seo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65496a4902252d301cdf219339bfbf9e-Abstract-Conference.html": {
    "title": "Learning World Models with Identifiable Factorization",
    "volume": "main",
    "abstract": "Extracting a stable and compact representation of the environment is crucial for efficient reinforcement learning in high-dimensional, noisy, and non-stationary environments. Different categories of information coexist in such environments -- how to effectively extract and disentangle the information remains a challenging problem. In this paper, we propose IFactor, a general framework to model four distinct categories of latent state variables that capture various aspects of information within the RL system, based on their interactions with actions and rewards. Our analysis establishes block-wise identifiability of these latent variables, which not only provides a stable and compact representation but also discloses that all reward-relevant factors are significant for policy learning. We further present a practical approach to learning the world model with identifiable blocks, ensuring the removal of redundancies but retaining minimal and sufficient information for policy optimization. Experiments in synthetic worlds demonstrate that our method accurately identifies the ground-truth latent variables, substantiating our theoretical findings. Moreover, experiments in variants of the DeepMind Control Suite and RoboDesk showcase the superior performance of our approach over baselines",
    "keywords": [],
    "checked": true,
    "id": "18276220e1183e7dbd6729d296d51b5d0ae0d70c",
    "semantic_title": "learning world models with identifiable factorization",
    "citation_count": 0,
    "authors": [
      "Yuren Liu",
      "Biwei Huang",
      "Zhengmao Zhu",
      "Honglong Tian",
      "Mingming Gong",
      "Yang Yu",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/654f61ecd998c9095d30d42c03b832aa-Abstract-Conference.html": {
    "title": "Online Map Vectorization for Autonomous Driving: A Rasterization Perspective",
    "volume": "main",
    "abstract": "High-definition (HD) vectorized map is essential for autonomous driving, providing detailed and precise environmental information for advanced perception and planning. However, current map vectorization methods often exhibit deviations, and the existing evaluation metric for map vectorization lacks sufficient sensitivity to detect these deviations. To address these limitations, we propose integrating the philosophy of rasterization into map vectorization. Specifically, we introduce a new rasterization-based evaluation metric, which has superior sensitivity and is better suited to real-world autonomous driving scenarios. Furthermore, we propose MapVR (Map Vectorization via Rasterization), a novel framework that applies differentiable rasterization to vectorized outputs and then performs precise and geometry-aware supervision on rasterized HD maps. Notably, MapVR designs tailored rasterization strategies for various geometric shapes, enabling effective adaptation to a wide range of map elements. Experiments show that incorporating rasterization into map vectorization greatly enhances performance with no extra computational cost during inference, leading to more accurate map perception and ultimately promoting safer autonomous driving. Codes are available at https://github.com/ZhangGongjie/MapVR. A standalone map vectorization evaluation toolkit is available at https://github.com/jiahaoLjh/MapVectorizationEvalToolkit",
    "keywords": [],
    "checked": true,
    "id": "812349ad59e259c2c566b08033baf288aa3bd01a",
    "semantic_title": "online map vectorization for autonomous driving: a rasterization perspective",
    "citation_count": 5,
    "authors": [
      "Gongjie Zhang",
      "Jiahao Lin",
      "Shuang Wu",
      "yilin song",
      "Zhipeng Luo",
      "Yang Xue",
      "Shijian Lu",
      "Zuoguan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/655846cc914cb7ff977a1ada40866441-Abstract-Conference.html": {
    "title": "NAP: Neural 3D Articulated Object Prior",
    "volume": "main",
    "abstract": "We propose Neural 3D Articulated object Prior (NAP), the first 3D deep generative model to synthesize 3D articulated object models. Despite the extensive research on generating 3D static objects, compositions, or scenes, there are hardly any approaches on capturing the distribution of articulated objects, a common object category for human and robot interaction. To generate articulated objects, we first design a novel articulation tree/graph parameterization and then apply a diffusion-denoising probabilistic model over this representation where articulated objects can be generated via denoising from random complete graphs. In order to capture both the geometry and the motion structure whose distribution will affect each other, we design a graph denoising network for learning the reverse diffusion process. We propose a novel distance that adapts widely used 3D generation metrics to our novel task to evaluate generation quality. Experiments demonstrate our high performance in articulated object generation as well as its applications on conditioned generation, including Part2Motion, PartNet-Imagination, Motion2Part, and GAPart2Object",
    "keywords": [],
    "checked": false,
    "id": "08a533c6f36f146adadd1edcc66162e88c93991b",
    "semantic_title": "nap: neural 3d articulation prior",
    "citation_count": 5,
    "authors": [
      "Jiahui Lei",
      "Congyue Deng",
      "William B Shen",
      "Leonidas J. Guibas",
      "Kostas Daniilidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/656678aa961a99a6a3d59bfbf88daf77-Abstract-Conference.html": {
    "title": "Compressed Video Prompt Tuning",
    "volume": "main",
    "abstract": "Compressed videos offer a compelling alternative to raw videos, showing the possibility to significantly reduce the on-line computational and storage cost. However, current approaches to compressed video processing generally follow the resource-consuming pre-training and fine-tuning paradigm, which does not fully take advantage of such properties, making them not favorable enough for widespread applications. Inspired by recent successes of prompt tuning techniques in computer vision, this paper presents the first attempt to build a prompt based representation learning framework, which enables effective and efficient adaptation of pre-trained raw video models to compressed video understanding tasks. To this end, we propose a novel prompt tuning approach, namely Compressed Video Prompt Tuning (CVPT), emphatically dealing with the challenging issue caused by the inconsistency between pre-training and downstream data modalities. Specifically, CVPT replaces the learnable prompts with compressed modalities (\\emph{e.g.} Motion Vectors and Residuals) by re-parameterizing them into conditional prompts followed by layer-wise refinement. The conditional prompts exhibit improved adaptability and generalizability to instances compared to conventional individual learnable ones, and the Residual prompts enhance the noisy motion cues in the Motion Vector prompts for further fusion with the visual cues from I-frames. Additionally, we design Selective Cross-modal Complementary Prompt (SCCP) blocks. After inserting them into the backbone, SCCP blocks leverage semantic relations across diverse levels and modalities to improve cross-modal interactions between prompts and input flows. Extensive evaluations on HMDB-51, UCF-101 and Something-Something v2 demonstrate that CVPT remarkably outperforms the state-of-the-art counterparts, delivering a much better balance between accuracy and efficiency",
    "keywords": [],
    "checked": false,
    "id": "a45fd3a9eb9bbd4f265800bb0b46c9ae4bfe3cd7",
    "semantic_title": "vop: text-video co-operative prompt tuning for cross-modal retrieval",
    "citation_count": 7,
    "authors": [
      "Bing Li",
      "Jiaxin Chen",
      "Xiuguo Bao",
      "Di Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/656faa09eb6e82dd86de9a417111c3b0-Abstract-Conference.html": {
    "title": "Sampling from Structured Log-Concave Distributions via a Soft-Threshold Dikin Walk",
    "volume": "main",
    "abstract": "Given a Lipschitz or smooth convex function $f:K \\to \\mathbb{R}^d$ for a bounded polytope $K:=${ $\\theta \\in \\mathbb{R}^d: A\\theta \\leq b$}, where $A\\in \\mathbb{R}^{m\\times d}$ and $b \\in \\mathbb{R}^m$, we consider the problem of sampling from the log-concave distribution $\\pi(\\theta) \\propto e^{-f(\\theta)}$ constrained to $K$. Interest in this problem derives from its applications to Bayesian inference and differential privacy. We present a generalization of the Dikin walk to this setting that requires at most $O((md + d L^2 R^2) \\times md^{\\omega-1} \\log(\\frac{w}{\\delta}))$ arithmetic operations to sample from $\\pi$ within error $\\delta>0$ in the total variation distance from a $w$-warm start. Here $L$ is the Lipschitz constant of $f$, $K$ is contained in a ball of radius $R$ and contains a ball of smaller radius $r$, and $\\omega \\approx 2.37$ is the matrix-multiplication constant. This improves on the running time of prior works for a range of structured settings important for the aforementioned inference and privacy applications. Technically, we depart from previous Dikin walks by adding a soft-threshold regularizer derived from the Lipschitz or smoothness properties of $f$ to a barrier function for $K$ that allows our version of the Dikin walk to propose updates that have a high Metropolis acceptance ratio for $f$, while at the same time remaining inside the polytope $K$",
    "keywords": [],
    "checked": false,
    "id": "3d8f8bd7ea0440ded36ecb73a925b62ff18049d1",
    "semantic_title": "sampling from log-concave distributions over polytopes via a soft-threshold dikin walk",
    "citation_count": 1,
    "authors": [
      "Oren Mangoubi",
      "Nisheeth K. Vishnoi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/659e07806dc17bd69d0d9aed47f85e7c-Abstract-Conference.html": {
    "title": "Implicit Regularization in Over-Parameterized Support Vector Machine",
    "volume": "main",
    "abstract": "In this paper, we design a regularization-free algorithm for high-dimensional support vector machines (SVMs) by integrating over-parameterization with Nesterov's smoothing method, and provide theoretical guarantees for the induced implicit regularization phenomenon. In particular, we construct an over-parameterized hinge loss function and estimate the true parameters by leveraging regularization-free gradient descent on this loss function. The utilization of Nesterov's method enhances the computational efficiency of our algorithm, especially in terms of determining the stopping criterion and reducing computational complexity. With appropriate choices of initialization, step size, and smoothness parameter, we demonstrate that unregularized gradient descent achieves a near-oracle statistical convergence rate. Additionally, we verify our theoretical findings through a variety of numerical experiments and compare the proposed method with explicit regularization. Our results illustrate the advantages of employing implicit regularization via gradient descent in conjunction with over-parameterization in sparse SVMs",
    "keywords": [],
    "checked": true,
    "id": "1cc3d262ec0781f86814edcc933bdfb3ba1341f1",
    "semantic_title": "implicit regularization in over-parameterized support vector machine",
    "citation_count": 0,
    "authors": [
      "Yang Sui",
      "Xin HE",
      "Yang Bai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65a39213d7d0e1eb5d192aa77e77eeb7-Abstract-Conference.html": {
    "title": "Large Language Models as Commonsense Knowledge for Large-Scale Task Planning",
    "volume": "main",
    "abstract": "Large-scale task planning is a major challenge. Recent work exploits large language models (LLMs) directly as a policy and shows surprisingly interesting results. This paper shows that LLMs provide a commonsense model of the world in addition to a policy that acts on it. The world model and the policy can be combined in a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task planning. In our new LLM-MCTS algorithm, the LLM-induced world model provides a commonsense prior belief for MCTS to achieve effective reasoning; the LLM-induced policy acts as a heuristic to guide the search, vastly improving search efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin, for complex, novel tasks. Further experiments and analyses on multiple tasks -- multiplication, travel planning, object rearrangement -- suggest minimum description length (MDL) as a general guiding principle: if the description length of the world model is substantially smaller than that of the policy, using LLM as a world model for model-based planning is likely better than using LLM solely as a policy",
    "keywords": [],
    "checked": true,
    "id": "5d32c364088733c6e8dadc9cf0baa26e10506d61",
    "semantic_title": "large language models as commonsense knowledge for large-scale task planning",
    "citation_count": 25,
    "authors": [
      "Zirui Zhao",
      "Wee Sun Lee",
      "David Hsu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65a925049647eab0aa06a9faf1cd470b-Abstract-Conference.html": {
    "title": "Uncovering Meanings of Embeddings via Partial Orthogonality",
    "volume": "main",
    "abstract": "Machine learning tools often rely on embedding text as vectors of real numbers.In this paper, we study how the semantic structure of language is encoded in the algebraic structure of such embeddings.Specifically, we look at a notion of \"semantic independence\" capturing the idea that, e.g., \"eggplant\" and \"tomato\" are independent given \"vegetable\". Although such examples are intuitive, it is difficult to formalize such a notion of semantic independence. The key observation here is that any sensible formalization should obey a set of so-called independence axioms, and thus any algebraic encoding of this structure should also obey these axioms. This leads us naturally to use partial orthogonality as the relevant algebraic structure. We develop theory and methods that allow us to demonstrate that partial orthogonality does indeed capture semantic independence.Complementary to this, we also introduce the concept of independence preserving embeddings where embeddings preserve the conditional independence structures of a distribution, and we prove the existence of such embeddings and approximations to them",
    "keywords": [],
    "checked": true,
    "id": "6f2297349ee671c7431cb32a83b3f278f4fc84f0",
    "semantic_title": "uncovering meanings of embeddings via partial orthogonality",
    "citation_count": 2,
    "authors": [
      "Yibo Jiang",
      "Bryon Aragam",
      "Victor Veitch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65b721a1df04c1098567f70d483d6468-Abstract-Conference.html": {
    "title": "Federated Learning with Bilateral Curation for Partially Class-Disjoint Data",
    "volume": "main",
    "abstract": "Partially class-disjoint data (PCDD), a common yet under-explored data formation where each client contributes a part of classes (instead of all classes) of samples, severely challenges the performance of federated algorithms. Without full classes, the local objective will contradict the global objective, yielding the angle collapse problem for locally missing classes and the space waste problem for locally existing classes. As far as we know, none of the existing methods can intrinsically mitigate PCDD challenges to achieve holistic improvement in the bilateral views (both global view and local view) of federated learning. To address this dilemma, we are inspired by the strong generalization of simplex Equiangular Tight Frame (ETF) on the imbalanced data, and propose a novel approach called FedGELA where the classifier is globally fixed as a simplex ETF while locally adapted to the personal distributions. Globally, FedGELA provides fair and equal discrimination for all classes and avoids inaccurate updates of the classifier, while locally it utilizes the space of locally missing classes for locally existing classes. We conduct extensive experiments on a range of datasets to demonstrate that our FedGELA achieves promising performance (averaged improvement of 3.9% to FedAvg and 1.5% to best baselines) and provide both local and global convergence guarantees",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqing Fan",
      "ruipeng zhang",
      "Jiangchao Yao",
      "Bo Han",
      "Ya Zhang",
      "Yanfeng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65cbe3e21ac62553111d9ecf7d60c18e-Abstract-Conference.html": {
    "title": "Partial Counterfactual Identification of Continuous Outcomes with a Curvature Sensitivity Model",
    "volume": "main",
    "abstract": "Counterfactual inference aims to answer retrospective \"what if\" questions and thus belongs to the most fine-grained type of inference in Pearl's causality ladder. Existing methods for counterfactual inference with continuous outcomes aim at point identification and thus make strong and unnatural assumptions about the underlying structural causal model. In this paper, we relax these assumptions and aim at partial counterfactual identification of continuous outcomes, i.e., when the counterfactual query resides in an ignorance interval with informative bounds. We prove that, in general, the ignorance interval of the counterfactual queries has non-informative bounds, already when functions of structural causal models are continuously differentiable. As a remedy, we propose a novel sensitivity model called Curvature Sensitivity Model. This allows us to obtain informative bounds by bounding the curvature of level sets of the functions. We further show that existing point counterfactual identification methods are special cases of our Curvature Sensitivity Model when the bound of the curvature is set to zero. We then propose an implementation of our Curvature Sensitivity Model in the form of a novel deep generative model, which we call Augmented Pseudo-Invertible Decoder. Our implementation employs (i) residual normalizing flows with (ii) variational augmentations. We empirically demonstrate the effectiveness of our Augmented Pseudo-Invertible Decoder. To the best of our knowledge, ours is the first partial identification model for Markovian structural causal models with continuous outcomes",
    "keywords": [],
    "checked": true,
    "id": "6cc1f2f6a02ad5cd6822e99ed80bf469ba0fefef",
    "semantic_title": "partial counterfactual identification of continuous outcomes with a curvature sensitivity model",
    "citation_count": 4,
    "authors": [
      "Valentyn Melnychuk",
      "Dennis Frauen",
      "Stefan Feuerriegel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65ccdfe02045fa0b823c5fa7ffd56b66-Abstract-Conference.html": {
    "title": "Training biologically plausible recurrent neural networks on cognitive tasks with long-term dependencies",
    "volume": "main",
    "abstract": "Training recurrent neural networks (RNNs) has become a go-to approach for generating and evaluating mechanistic neural hypotheses for cognition. The ease and efficiency of training RNNs with backpropagation through time and the availability of robustly supported deep learning libraries has made RNN modeling more approachable and accessible to neuroscience. Yet, a major technical hindrance remains. Cognitive processes such as working memory and decision making involve neural population dynamics over a long period of time within a behavioral trial and across trials. It is difficult to train RNNs to accomplish tasks where neural representations and dynamics have long temporal dependencies without gating mechanisms such as LSTMs or GRUs which currently lack experimental support and prohibit direct comparison between RNNs and biological neural circuits. We tackled this problem based on the idea of specialized skip-connections through time to support the emergence of task-relevant dynamics, and subsequently reinstitute biological plausibility by reverting to the original architecture. We show that this approach enables RNNs to successfully learn cognitive tasks that prove impractical if not impossible to learn using conventional methods. Over numerous tasks considered here, we achieve less training steps and shorter wall-clock times, particularly in tasks that require learning long-term dependencies via temporal integration over long timescales or maintaining a memory of past events in hidden-states. Our methods expand the range of experimental tasks that biologically plausible RNN models can learn, thereby supporting the development of theory for the emergent neural mechanisms of computations involving long-term dependencies",
    "keywords": [],
    "checked": true,
    "id": "2dad09b9eb927126e7ee9afb03f68431b92549ad",
    "semantic_title": "training biologically plausible recurrent neural networks on cognitive tasks with long-term dependencies",
    "citation_count": 0,
    "authors": [
      "Wayne Soo",
      "Vishwa Goudar",
      "Xiao-Jing Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65d32185f73cbf4535449a792c63926f-Abstract-Conference.html": {
    "title": "Diffusion Probabilistic Models for Structured Node Classification",
    "volume": "main",
    "abstract": "This paper studies structured node classification on graphs, where the predictions should consider dependencies between the node labels. In particular, we focus on solving the problem for partially labeled graphs where it is essential to incorporate the information in the known label for predicting the unknown labels. To address this issue, we propose a novel framework leveraging the diffusion probabilistic model for structured node classification (DPM-SNC). At the heart of our framework is the extraordinary capability of DPM-SNC to (a) learn a joint distribution over the labels with an expressive reverse diffusion process and (b) make predictions conditioned on the known labels utilizing manifold-constrained sampling. Since the DPMs lack training algorithms for partially labeled data, we design a novel training algorithm to apply DPMs, maximizing a new variational lower bound. We also theoretically analyze how DPMs benefit node classification by enhancing the expressive power of GNNs based on proposing AGG-WL, which is strictly more powerful than the classic 1-WL test. We extensively verify the superiority of our DPM-SNC in diverse scenarios, which include not only the transductive setting on partially labeled graphs but also the inductive setting and unlabeled graphs",
    "keywords": [],
    "checked": true,
    "id": "54bef272e2eca69c006d4f7458ce5dcb386818b2",
    "semantic_title": "diffusion probabilistic models for structured node classification",
    "citation_count": 0,
    "authors": [
      "Hyosoon Jang",
      "Seonghyun Park",
      "Sangwoo Mo",
      "Sungsoo Ahn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65e837e76a5308df3d5544aab6196e21-Abstract-Conference.html": {
    "title": "Non-stationary Experimental Design under Linear Trends",
    "volume": "main",
    "abstract": "Experimentation has been critical and increasingly popular across various domains, such as clinical trials and online platforms, due to its widely recognized benefits. One of the primary objectives of classical experiments is to estimate the average treatment effect (ATE) to inform future decision-making. However, in healthcare and many other settings, treatment effects may be non-stationary, meaning that they can change over time, rendering the traditional experimental design inadequate and the classical static ATE uninformative. In this work, we address the problem of non-stationary experimental design under linear trends by considering two objectives: estimating the dynamic treatment effect and minimizing welfare loss within the experiment. We propose an efficient design that can be customized for optimal estimation error rate, optimal regret rate, or the Pareto optimal trade-off between the two objectives. We establish information-theoretical lower bounds that highlight the inherent challenge in estimating dynamic treatment effects and minimizing welfare loss, and also statistically reveal the fundamental trade-off between them",
    "keywords": [],
    "checked": false,
    "id": "9a3c98e3f0904922af53e0038099887a0d339f49",
    "semantic_title": "non-stationary experimental design under structured trends",
    "citation_count": 1,
    "authors": [
      "David Simchi-Levi",
      "Chonghuan Wang",
      "Zeyu Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65e876f6a98c6799d0b3145966dd73e2-Abstract-Conference.html": {
    "title": "EICIL: Joint Excitatory Inhibitory Cycle Iteration Learning for Deep Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) have undergone continuous development and extensive study for decades, leading to increased biological plausibility and optimal energy efficiency. However, traditional training methods for deep SNNs have some limitations, as they rely on strategies such as pre-training and fine-tuning, indirect coding and reconstruction, and approximate gradients. These strategies lack a complete training model and require gradient approximation. To overcome these limitations, we propose a novel learning method named Joint Excitatory Inhibitory Cycle Iteration learning for Deep Spiking Neural Networks (EICIL) that integrates both excitatory and inhibitory behaviors inspired by the signal transmission of biological neurons.By organically embedding these two behavior patterns into one framework, the proposed EICIL significantly improves the bio-mimicry and adaptability of spiking neuron models, as well as expands the representation space of spiking neurons. Extensive experiments based on EICIL and traditional learning methods demonstrate that EICIL outperforms traditional methods on various datasets, such as CIFAR10 and CIFAR100, revealing the crucial role of the learning approach that integrates both behaviors during training",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihang Shao",
      "Xuanye Fang",
      "Yaxin Li",
      "Chaoran Feng",
      "Jiangrong Shen",
      "Qi Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/65ea878cb90b440e8b4cd34fe0959914-Abstract-Conference.html": {
    "title": "Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency",
    "volume": "main",
    "abstract": "Interpreting time series models is uniquely challenging because it requires identifying both the location of time series signals that drive model predictions and their matching to an interpretable temporal pattern. While explainers from other modalities can be applied to time series, their inductive biases do not transfer well to the inherently challenging interpretation of time series. We present TimeX, a time series consistency model for training explainers. TimeX trains an interpretable surrogate to mimic the behavior of a pretrained time series model. It addresses the issue of model faithfulness by introducing model behavior consistency, a novel formulation that preserves relations in the latent space induced by the pretrained model with relations in the latent space induced by TimeX. TimeX provides discrete attribution maps and, unlike existing interpretability methods, it learns a latent space of explanations that can be used in various ways, such as to provide landmarks to visually aggregate similar explanations and easily recognize temporal patterns. We evaluate TimeX on eight synthetic and real-world datasets and compare its performance against state-of-the-art interpretability methods. We also conduct case studies using physiological time series. Quantitative evaluations demonstrate that TimeX achieves the highest or second-highest performance in every metric compared to baselines across all datasets. Through case studies, we show that the novel components of TimeX show potential for training faithful, interpretable models that capture the behavior of pretrained time series models",
    "keywords": [],
    "checked": true,
    "id": "4337200e8063bddc97c508efeb1ef81f118abe30",
    "semantic_title": "encoding time-series explanations through self-supervised model behavior consistency",
    "citation_count": 3,
    "authors": [
      "Owen Queen",
      "Tom Hartvigsen",
      "Teddy Koker",
      "Huan He",
      "Theodoros Tsiligkaridis",
      "Marinka Zitnik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/66178beae8f12fcd48699de95acc1152-Abstract-Conference.html": {
    "title": "HyTrel: Hypergraph-enhanced Tabular Data Representation Learning",
    "volume": "main",
    "abstract": "Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks.However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HyTrel, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs--where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show thatHyTrel is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HyTreliff the two tables are identical up to permutation. Our empirical results demonstrate that HyTrel consistently outperforms other competitive baselines on four downstream tasks with minimal pretraining, illustrating the advantages of incorporating inductive biases associated with tabular data into the representations. Finally, our qualitative analyses showcase that HyTrel can assimilate the table structure to generate robust representations for the cells, rows, columns, and the entire table",
    "keywords": [],
    "checked": true,
    "id": "a7e58dc03d029100fd437e229f7ee80e976fc842",
    "semantic_title": "hytrel: hypergraph-enhanced tabular data representation learning",
    "citation_count": 3,
    "authors": [
      "Pei Chen",
      "Soumajyoti Sarkar",
      "Leonard Lausen",
      "Balasubramaniam Srinivasan",
      "Sheng Zha",
      "Ruihong Huang",
      "George Karypis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/661c37f3b098bdee53fd7d9c4ef6964a-Abstract-Conference.html": {
    "title": "PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance",
    "volume": "main",
    "abstract": "Exploiting pre-trained diffusion models for restoration has recently become a favored alternative to the traditional task-specific training approach. Previous works have achieved noteworthy success by limiting the solution space using explicit degradation models. However, these methods often fall short when faced with complex degradations as they generally cannot be precisely modeled. In this paper, we introduce $\\textit{partial guidance}$, a fresh perspective that is more adaptable to real-world degradations compared to existing works. Rather than specifically defining the degradation process, our approach models the desired properties, such as image structure and color statistics of high-quality images, and applies this guidance during the reverse diffusion process. These properties are readily available and make no assumptions about the degradation process. When combined with a diffusion prior, this partial guidance can deliver appealing results across a range of restoration tasks. Additionally, our method can be extended to handle composite tasks by consolidating multiple high-quality image properties, achieved by integrating the guidance from respective tasks. Experimental results demonstrate that our method not only outperforms existing diffusion-prior-based approaches but also competes favorably with task-specific models",
    "keywords": [],
    "checked": true,
    "id": "815efe975e4ee136a1aebbdc2b71dc4e5d718216",
    "semantic_title": "pgdiff: guiding diffusion models for versatile face restoration via partial guidance",
    "citation_count": 1,
    "authors": [
      "Peiqing Yang",
      "Shangchen Zhou",
      "Qingyi Tao",
      "Chen Change Loy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/661caac7729aa7d8c6b8ac0d39ccbc6a-Abstract-Conference.html": {
    "title": "Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP",
    "volume": "main",
    "abstract": "Open-vocabulary segmentation is a challenging task requiring segmenting and recognizing objects from an open set of categories in diverse environments. One way to address this challenge is to leverage multi-modal models, such as CLIP, to provide image and text features in a shared embedding space, which effectively bridges the gap between closed-vocabulary and open-vocabulary recognition.Hence, existing methods often adopt a two-stage framework to tackle the problem, where the inputs first go through a mask generator and then through the CLIP model along with the predicted masks. This process involves extracting features from raw images multiple times, which can be ineffective and inefficient. By contrast, we propose to build everything into a single-stage framework using a shared Frozen Convolutional CLIP backbone, which not only significantly simplifies the current two-stage pipeline, but also remarkably yields a better accuracy-cost trade-off. The resulting single-stage system, called FC-CLIP, benefits from the following observations: the frozen CLIP backbone maintains the ability of open-vocabulary classification and can also serve as a strong mask generator, and the convolutional CLIP generalizes well to a larger input resolution than the one used during contrastive image-text pretraining. Surprisingly, FC-CLIP advances state-of-the-art results on various benchmarks, while running practically fast. Specifically, when training on COCO panoptic data only and testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1 mIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2 mIoU on Cityscapes, outperforming the prior art under the same setting by +4.2 PQ, +2.4 AP, +4.2 mIoU on ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes, respectively. Additionally, the training and testing time of FC-CLIP is 7.5x and 6.6x significantly faster than the same prior art, while using 5.9x fewer total model parameters. Meanwhile, FC-CLIP also sets a new state-of-the-art performance across various open-vocabulary semantic segmentation datasets. Code and models are available at https://github.com/bytedance/fc-clip",
    "keywords": [],
    "checked": true,
    "id": "2b26b17fe3a909bc0f5408b3328308153b31f22e",
    "semantic_title": "convolutions die hard: open-vocabulary segmentation with single frozen convolutional clip",
    "citation_count": 25,
    "authors": [
      "Qihang Yu",
      "Ju He",
      "Xueqing Deng",
      "Xiaohui Shen",
      "Liang-Chieh Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/661d4fda173120a2f119e0319e6bcf97-Abstract-Conference.html": {
    "title": "CLIP-OGD: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments",
    "volume": "main",
    "abstract": "From clinical development of cancer therapies to investigations into partisan bias, adaptive sequential designs have become increasingly popular method for causal inference, as they offer the possibility of improved precision over their non-adaptive counterparts. However, even in simple settings (e.g. two treatments) the extent to which adaptive designs can improve precision is not sufficiently well understood. In this work, we study the problem of Adaptive Neyman Allocation in a design-based potential outcomes framework, where the experimenter seeks to construct an adaptive design which is nearly as efficient as the optimal (but infeasible) non-adaptive Neyman design, which has access to all potential outcomes. Motivated by connections to online optimization, we propose Neyman Ratio and Neyman Regret as two (equivalent) performance measures of adaptive designs for this problem. We present Clip-OGD, an adaptive design which achieves $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ expected Neyman regret and thereby recovers the optimal Neyman variance in large samples. Finally, we construct a conservative variance estimator which facilitates the development of asymptotically valid confidence intervals. To complement our theoretical results, we conduct simulations using data from a microeconomic experiment",
    "keywords": [],
    "checked": true,
    "id": "e16f00646b2bdfb8fbd1e95817e91798c74c103c",
    "semantic_title": "clip-ogd: an experimental design for adaptive neyman allocation in sequential experiments",
    "citation_count": 4,
    "authors": [
      "Jessica Dai",
      "Paula Gradu",
      "Christopher Harshaw"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/662b1774ba8845fc1fa3d1fc0177ceeb-Abstract-Conference.html": {
    "title": "Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context",
    "volume": "main",
    "abstract": "Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation rate and next-identifier match than the much larger text-davinci-003 model.We also conduct a generalizability study to evaluate the ability of MGD to generalize to multiple programming languages (Java, C# and Rust), coding scenarios (e.g., correct number of arguments to method calls), and to enforce richer semantic constraints (e.g., stateful API protocols). Our data and implementation are available at https://github.com/microsoft/monitors4codegen",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lakshya A Agrawal",
      "Aditya Kanade",
      "Navin Goyal",
      "Shuvendu Lahiri",
      "Sriram Rajamani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/663bce02a0050c4a11f1eb8a7f1429d3-Abstract-Conference.html": {
    "title": "Sharp Spectral Rates for Koopman Operator Learning",
    "volume": "main",
    "abstract": "Non-linear dynamical systems can be handily described by the associated Koopman operator, whose action evolves every observable of the system forward in time. Learning the Koopman operator and its spectral decomposition from data is enabled by a number of algorithms. In this work we present for the first time non-asymptotic learning bounds for the Koopman eigenvalues and eigenfunctions. We focus on time-reversal-invariant stochastic dynamical systems, including the important example of Langevin dynamics. We analyze two popular estimators: Extended Dynamic Mode Decomposition (EDMD) and Reduced Rank Regression (RRR). Our results critically hinge on novel {minimax} estimation bounds for the operator norm error, that may be of independent interest. Our spectral learning bounds are driven by the simultaneous control of the operator norm error and a novel metric distortion functional of the estimated eigenfunctions. The bounds indicates that both EDMD and RRR have similar variance, but EDMD suffers from a larger bias which might be detrimental to its learning rate. Our results shed new light on the emergence of spurious eigenvalues, an issue which is well known empirically. Numerical experiments illustrate the implications of the bounds in practice",
    "keywords": [],
    "checked": true,
    "id": "1bc9fd4f2f36fb1355d34c2ec8b15ed1be98b8ec",
    "semantic_title": "sharp spectral rates for koopman operator learning",
    "citation_count": 5,
    "authors": [
      "Vladimir Kostic",
      "Karim Lounici",
      "Pietro Novelli",
      "Massimiliano Pontil"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/666cccc6376058e251315b4de7e085b9-Abstract-Conference.html": {
    "title": "Continual Learning for Instruction Following from Realtime Feedback",
    "volume": "main",
    "abstract": "We propose and deploy an approach to continually train an instruction-following agent from feedback provided by users during collaborative interactions. During interaction, human users instruct an agent using natural language, and provide realtime binary feedback as they observe the agent following their instructions. We design a contextual bandit learning approach, converting user feedback to immediate reward. We evaluate through thousands of human-agent interactions, demonstrating 15.4% absolute improvement in instruction execution accuracy over time. We also show our approach is robust to several design variations, and that the feedback signal is roughly equivalent to the learning signal of supervised demonstration data",
    "keywords": [],
    "checked": true,
    "id": "6ba3e4172e5c22c8c3ace05a31e9b119a2e3c33c",
    "semantic_title": "continual learning for instruction following from realtime feedback",
    "citation_count": 8,
    "authors": [
      "Alane Suhr",
      "Yoav Artzi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/66738d21d3cddb8717ca52deff5a5546-Abstract-Conference.html": {
    "title": "Embracing the chaos: analysis and diagnosis of numerical instability in variational flows",
    "volume": "main",
    "abstract": "In this paper, we investigate the impact of numerical instability on the reliability of sampling, density evaluation, and evidence lower bound (ELBO) estimation in variational flows. We first empirically demonstrate that common flows can exhibit a catastrophic accumulation of error: the numerical flow map deviates significantly from the exact map---which affects sampling---and the numerical inverse flow map does not accurately recover the initial input---which affects density and ELBO computations. Surprisingly though, we find that results produced by flows are often accurate enough for applications despite the presence of serious numerical instability. In this work, we treat variational flows as chaotic dynamical systems, and leverage shadowing theory to elucidate this behavior via theoretical guarantees on the error of sampling, density evaluation, and ELBO estimation. Finally, we develop and empirically test a diagnostic procedure that can be used to validate results produced by numerically unstable flows in practice",
    "keywords": [],
    "checked": true,
    "id": "c0f5d9e904c7a9cb865f94909476d8104cca6bc5",
    "semantic_title": "embracing the chaos: analysis and diagnosis of numerical instability in variational flows",
    "citation_count": 1,
    "authors": [
      "Zuheng Xu",
      "Trevor Campbell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/66772e6aa61e54ae16443ae1d78a7319-Abstract-Conference.html": {
    "title": "Masked Two-channel Decoupling Framework for Incomplete Multi-view Weak Multi-label Learning",
    "volume": "main",
    "abstract": "Multi-view learning has become a popular research topic in recent years, but research on the cross-application of classic multi-label classification and multi-view learning is still in its early stages. In this paper, we focus on the complex yet highly realistic task of incomplete multi-view weak multi-label learning and propose a masked two-channel decoupling framework based on deep neural networks to solve this problem. The core innovation of our method lies in decoupling the single-channel view-level representation, which is common in deep multi-view learning methods, into a shared representation and a view-proprietary representation. We also design a cross-channel contrastive loss to enhance the semantic property of the two channels. Additionally, we exploit supervised information to design a label-guided graph regularization loss, helping the extracted embedding features preserve the geometric structure among samples. Inspired by the success of masking mechanisms in image and text analysis, we develop a random fragment masking strategy for vector features to improve the learning ability of encoders. Finally, it is important to emphasize that our model is fully adaptable to arbitrary view and label absences while also performing well on the ideal full data. We have conducted sufficient and convincing experiments to confirm the effectiveness and advancement of our model",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengliang Liu",
      "Jie Wen",
      "Yabo Liu",
      "Chao Huang",
      "Zhihao Wu",
      "Xiaoling Luo",
      "Yong Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/66820ab16b817d8a6b00d60b3d24b83a-Abstract-Conference.html": {
    "title": "Exponential Lower Bounds for Fictitious Play in Potential Games",
    "volume": "main",
    "abstract": "Fictitious Play (FP) is a simple and natural dynamic for repeated play with many applications in game theory and multi-agent reinforcement learning. It was introduced by Brown and its convergence properties for two-player zero-sum games was established later by Robinson. Potential games [Monderer and Shapley 1996] is another class of games which exhibit the FP property [Monderer and Shapley 1996], i.e., FP dynamics converges to a Nash equilibrium if all agents follows it. Nevertheless, except for two-player zero-sum games and for specific instances of payoff matrices [Abernethy et. al. 2021] or for adversarial tie-breaking rules [Daskalakis and Pan, 2014], the \\textit{convergence rate} of FP is unknown. In this work, we focus on the rate of convergence of FP when applied to potential games and more specifically identical payoff games. We prove that FP can take exponential time (in the number of strategies) to reach a Nash equilibrium, even if the game is restricted to \\textit{two agents}. To prove this, we recursively construct a two-player coordination game with a unique Nash equilibrium. Moreover, every approximate Nash equilibrium in the constructed game must be close to the pure Nash equilibrium in $\\ell_1$-distance",
    "keywords": [],
    "checked": true,
    "id": "7f69a58d6a9148ad4b547937b752b3055313fa62",
    "semantic_title": "exponential lower bounds for fictitious play in potential games",
    "citation_count": 0,
    "authors": [
      "Ioannis Panageas",
      "Nikolas Patris",
      "Stratis Skoulakis",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/668563ef18fbfef0b66af491ea334d5f-Abstract-Conference.html": {
    "title": "Cocktail: Mixing Multi-Modality Control for Text-Conditional Image Generation",
    "volume": "main",
    "abstract": "Text-conditional diffusion models are able to generate high-fidelity images with diverse contents.However, linguistic representations frequently exhibit ambiguous descriptions of the envisioned objective imagery, requiring the incorporation of additional control signals to bolster the efficacy of text-guided diffusion models. In this work, we propose Cocktail, a pipeline to mix various modalities into one embedding, amalgamated with a generalized ControlNet (gControlNet), a controllable normalisation (ControlNorm), and a spatial guidance sampling method, to actualize multi-modal and spatially-refined control for text-conditional diffusion models. Specifically, we introduce a hyper-network gControlNet, dedicated to the alignment and infusion of the control signals from disparate modalities into the pre-trained diffusion model. gControlNet is capable of accepting flexible modality signals, encompassing the simultaneous reception of any combination of modality signals, or the supplementary fusion of multiple modality signals. The control signals are then fused and injected into the backbone model according to our proposed ControlNorm.Furthermore, our advanced spatial guidance sampling methodology proficiently incorporates the control signal into the designated region, thereby circumventing the manifestation of undesired objects within the generated image.We demonstrate the results of our method in controlling various modalities, proving high-quality synthesis and fidelity to multiple external signals",
    "keywords": [],
    "checked": false,
    "id": "95d7c841d9db6e02a237a6382decaa62bcc5825f",
    "semantic_title": "cocktail: mixing multi-modality controls for text-conditional image generation",
    "citation_count": 3,
    "authors": [
      "Minghui Hu",
      "Jianbin Zheng",
      "Daqing Liu",
      "Chuanxia Zheng",
      "Chaoyue Wang",
      "Dacheng Tao",
      "Tat-Jen Cham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6692e1b0e8a31e8de84bd90ad4d8d9e0-Abstract-Conference.html": {
    "title": "RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability",
    "volume": "main",
    "abstract": "Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to spurious variations -- changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is able to operate in dynamic environments, it is not invariant under significant distribution shift. To address this, we propose a simple reward-free alignment procedure that enables test time adaptation of the encoder. This allows for quick adaptation to widely differing environments without having to relearn the dynamics and policy. Our effort is a step towards making model-based RL a practical and useful tool for dynamic, diverse domains and we show its effectiveness in simulation tasks with significant spurious variations",
    "keywords": [],
    "checked": true,
    "id": "b3d213883226192fd2f59f0413ceb4610249362a",
    "semantic_title": "repo: resilient model-based reinforcement learning by regularizing posterior predictability",
    "citation_count": 2,
    "authors": [
      "Chuning Zhu",
      "Max Simchowitz",
      "Siri Gadipudi",
      "Abhishek Gupta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6697bb267dc517379bc8aa326e844f8d-Abstract-Conference.html": {
    "title": "Circuit as Set of Points",
    "volume": "main",
    "abstract": "As the size of circuit designs continues to grow rapidly, artificial intelligence technologies are being extensively used in Electronic Design Automation (EDA) to assist with circuit design.Placement and routing are the most time-consuming parts of the physical design process, and how to quickly evaluate the placement has become a hot research topic. Prior works either transformed circuit designs into images using hand-crafted methods and then used Convolutional Neural Networks (CNN) to extract features, which are limited by the quality of the hand-crafted methods and could not achieve end-to-end training, or treated the circuit design as a graph structure and used Graph Neural Networks (GNN) to extract features, which require time-consuming preprocessing.In our work, we propose a novel perspective for circuit design by treating circuit components as point clouds and using Transformer-based point cloud perception methods to extract features from the circuit. This approach enables direct feature extraction from raw data without any preprocessing, allows for end-to-end training, and results in high performance.Experimental results show that our method achieves state-of-the-art performance in congestion prediction tasks on both the CircuitNet and ISPD2015 datasets, as well as in design rule check (DRC) violation prediction tasks on the CircuitNet dataset.Our method establishes a bridge between the relatively mature point cloud perception methods and the fast-developing EDA algorithms, enabling us to leverage more collective intelligence to solve this task. To facilitate the research of open EDA design, source codes and pre-trained models are released at https://github.com/hustvl/circuitformer",
    "keywords": [],
    "checked": true,
    "id": "a5c96657d02b89b548aeed3d1721ef9ae70a8e7a",
    "semantic_title": "circuit as set of points",
    "citation_count": 0,
    "authors": [
      "Jialv Zou",
      "Xinggang Wang",
      "Jiahao Guo",
      "Wenyu Liu",
      "Qian Zhang",
      "Chang Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67089958e98b243d5cc1881ad60418b8-Abstract-Conference.html": {
    "title": "Causal Component Analysis",
    "volume": "main",
    "abstract": "Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a corollary, this interventional perspective also leads to new identifiability results for nonlinear ICA—a special case of CauCA with an empty graph—requiring strictly fewer datasets than previous results. We introduce a likelihood-based approach using normalizing flows to estimate both the unmixing function and the causal mechanisms, and demonstrate its effectiveness through extensive synthetic experiments in the CauCA and ICA setting",
    "keywords": [],
    "checked": true,
    "id": "263041108b4e045e6459aa437bbd0d869decb332",
    "semantic_title": "causal component analysis",
    "citation_count": 13,
    "authors": [
      "Liang Wendong",
      "Armin Kekić",
      "Julius von Kügelgen",
      "Simon Buchholz",
      "Michel Besserve",
      "Luigi Gresele",
      "Bernhard Schölkopf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67101f97dc23fcc10346091181fff6cb-Abstract-Conference.html": {
    "title": "Latent Graph Inference with Limited Supervision",
    "volume": "main",
    "abstract": "Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of supervision starvation, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to restore the corrupted affinities and replenish the missed supervision for better LGI. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as k-hop starved nodes, which can be identified based on a given adjacency matrix. Considering the high computational burden, we further present a more efficient alternative inspired by CUR matrix decomposition. Subsequently, we eliminate the starved nodes by reconstructing the destroyed connections. Extensive experiments on representative benchmarks demonstrate that reducing the starved nodes consistently improves the performance of state-of-the-art LGI methods, especially under extremely limited supervision (6.12% improvement on Pubmed with a labeling rate of only 0.3%)",
    "keywords": [],
    "checked": true,
    "id": "b1c91d3d7ad6dbd26f313254ce00124bd1c31e8b",
    "semantic_title": "latent graph inference with limited supervision",
    "citation_count": 0,
    "authors": [
      "Jianglin Lu",
      "Yi Xu",
      "Huan Wang",
      "Yue Bai",
      "Yun Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67159f1c0cab15dd34c76a5dd830a389-Abstract-Conference.html": {
    "title": "Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows",
    "volume": "main",
    "abstract": "Achieving a balance between image quality (precision) and diversity (recall) is a significant challenge in the domain of generative models. Current state-of-the-art models primarily rely on optimizing heuristics, such as the Fr\\'echet Inception Distance. While recent developments have introduced principled methods for evaluating precision and recall, they have yet to be successfully integrated into the training of generative models. Our main contribution is a novel training method for generative models, such as Generative Adversarial Networks and Normalizing Flows, which explicitly optimizes a user-defined trade-off between precision and recall. More precisely, we show that achieving a specified precision-recall trade-off corresponds to minimizing a unique $f$-divergence from a family we call the \\mbox{\\em PR-divergences}. Conversely, any $f$-divergence can be written as a linear combination of PR-divergences and corresponds to a weighted precision-recall trade-off. Through comprehensive evaluations, we show that our approach improves the performance of existing state-of-the-art models like BigGAN in terms of either precision or recall when tested on datasets such as ImageNet",
    "keywords": [],
    "checked": true,
    "id": "c618c17c46ce9bac65cbbf192f802ca252faa00b",
    "semantic_title": "precision-recall divergence optimization for generative modeling with gans and normalizing flows",
    "citation_count": 3,
    "authors": [
      "Alexandre Verine",
      "Benjamin Negrevergne",
      "Muni Sreenivas Pydi",
      "Yann Chevaleyre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67226725b09ca9363637f63f85ed4bba-Abstract-Conference.html": {
    "title": "Energy Guided Diffusion for Generating Neurally Exciting Images",
    "volume": "main",
    "abstract": "In recent years, most exciting inputs (MEIs) synthesized from encoding models of neuronal activity have become an established method for studying tuning properties of biological and artificial visual systems. However, as we move up the visual hierarchy, the complexity of neuronal computations increases. Consequently, it becomes more challenging to model neuronal activity, requiring more complex models. In this study, we introduce a novel readout architecture inspired by the mechanism of visual attention. This new architecture, which we call attention readout, together with a data-driven convolutional core outperforms previous task-driven models in predicting the activity of neurons in macaque area V4. However, as our predictive network becomes deeper and more complex, synthesizing MEIs via straightforward gradient ascent (GA) can struggle to produce qualitatively good results and overfit to idiosyncrasies of a more complex model, potentially decreasing the MEI's model-to-brain transferability. To solve this problem, we propose a diffusion-based method for generating MEIs via Energy Guidance (EGG). We show that for models of macaque V4, EGG generates single neuron MEIs that generalize better across varying model architectures than the state-of-the-art GA, while at the same time reducing computational costs by a factor of 4.7x, facilitating experimentally challenging closed-loop experiments. Furthermore, EGG diffusion can be used to generate other neurally exciting images, like most exciting naturalistic images that are on par with a selection of highly activating natural images, or image reconstructions that generalize better across architectures. Finally, EGG is simple to implement, requires no retraining of the diffusion model, and can easily be generalized to provide other characterizations of the visual system, such as invariances. Thus, EGG provides a general and flexible framework to study the coding properties of the visual system in the context of natural images",
    "keywords": [],
    "checked": true,
    "id": "b3eedc72773aec00803312b04b4fbd485e08cb0e",
    "semantic_title": "energy guided diffusion for generating neurally exciting images",
    "citation_count": 4,
    "authors": [
      "Pawel Pierzchlewicz",
      "Konstantin Willeke",
      "Arne Nix",
      "Pavithra Elumalai",
      "Kelli Restivo",
      "Tori Shinn",
      "Cate Nealley",
      "Gabrielle Rodriguez",
      "Saumil Patel",
      "Katrin Franke",
      "Andreas Tolias",
      "Fabian Sinz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67390075fe466276797f489115582cdc-Abstract-Conference.html": {
    "title": "An active learning framework for multi-group mean estimation",
    "volume": "main",
    "abstract": "We consider a fundamental problem where there are multiple groups whose data distributions are unknown, and an analyst would like to learn the mean of each group. We consider an active learning framework to sequentially collect $T$ samples with bandit, each period observing a sample from a chosen group. After observing a sample, the analyst may update their estimate of the mean and variance of that group and choose the next group accordingly. The objective is to dynamically collect samples to minimize the $p$-norm of the vector of variances of our mean estimators after $T$ rounds. We propose an algorithm, Variance-UCB, that selects groups according to a an upper bound on the variance estimate adjusted to the $p$-norm chosen. We show that the regret of Variance-UCB is $O(T^{-2})$ for finite $p$, and prove that no algorithm can do better. When $p$ is infinite, we recover the $O(T^{-1.5})$ obtained in \\cite{activelearning, carpentier2011upper} and provide a new lower bound showing that no algorithm can do better",
    "keywords": [],
    "checked": false,
    "id": "f5079f73589b3b07160ffbe5d9fe88bffd3e3b22",
    "semantic_title": "activity report project-team models and algorithms for artiﬁcial intelligence",
    "citation_count": 0,
    "authors": [
      "Abdellah Aznag",
      "Rachel Cummings",
      "Adam N. Elmachtoub"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6739d8df16b5bce3587ca5f18662a6aa-Abstract-Conference.html": {
    "title": "CAT-Walk: Inductive Hypergraph Learning via Set Walks",
    "volume": "main",
    "abstract": "Temporal hypergraphs provide a powerful paradigm for modeling time-dependent, higher-order interactions in complex systems. Representation learning for hypergraphs is essential for extracting patterns of the higher-order interactions that are critically important in real-world problems in social network analysis, neuroscience, finance, etc. However, existing methods are typically designed only for specific tasks or static hypergraphs. We present CAT-Walk, an inductive method that learns the underlying dynamic laws that govern the temporal and structural processes underlying a temporal hypergraph. CAT-Walk introduces a temporal, higher-order walk on hypergraphs, SetWalk, that extracts higher-order causal patterns. CAT-Walk uses a novel adaptive and permutation invariant pooling strategy, SetMixer, along with a set-based anonymization process that hides the identity of hyperedges. Finally, we present a simple yet effective neural network model to encode hyperedges. Our evaluation on 10 hypergraph benchmark datasets shows that CAT-Walk attains outstanding performance on temporal hyperedge prediction benchmarks in both inductive and transductive settings. It also shows competitive performance with state-of-the-art methods for node classification. (https://github.com/ubc-systopia/CATWalk)",
    "keywords": [],
    "checked": true,
    "id": "83cb6e9d3de6c02e6f97b923e2d0a8019f43fb68",
    "semantic_title": "cat-walk: inductive hypergraph learning via set walks",
    "citation_count": 4,
    "authors": [
      "Ali Behrouz",
      "Farnoosh Hashemi",
      "Sadaf Sadeghian",
      "Margo Seltzer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6745cb9889cc213bda803535f2d3902e-Abstract-Conference.html": {
    "title": "Unbiased constrained sampling with Self-Concordant Barrier Hamiltonian Monte Carlo",
    "volume": "main",
    "abstract": "In this paper, we propose Barrier Hamiltonian Monte Carlo (BHMC), a version of the HMC algorithm which aims at sampling from a Gibbs distribution $\\pi$ on a manifold $\\mathsf{M}$, endowed with a Hessian metric $\\mathfrak{g}$ derived from a self-concordant barrier. Our method relies on Hamiltonian dynamics which comprises $\\mathfrak{g}$. Therefore, it incorporates the constraints defining $\\mathsf{M}$ and is able to exploit its underlying geometry. However, the corresponding Hamiltonian dynamics is defined via non separable Ordinary Differential Equations (ODEs) in contrast to the Euclidean case. It implies unavoidable bias in existing generalization of HMC to Riemannian manifolds. In this paper, we propose a new filter step, called ``involution checking step'', to address this problem. This step is implemented in two versions of BHMC, coined continuous BHMC (c-bHMC) and numerical BHMC (n-BHMC) respectively. Our main results establish that these two new algorithms generate reversible Markov chains with respect to $\\pi$ and do not suffer from any bias in comparison to previous implementations. Our conclusions are supported by numerical experiments where we consider target distributions defined on polytopes",
    "keywords": [],
    "checked": true,
    "id": "7d9c23f4f1ac60d09957097d4ab811dab7160848",
    "semantic_title": "unbiased constrained sampling with self-concordant barrier hamiltonian monte carlo",
    "citation_count": 2,
    "authors": [
      "Maxence Noble",
      "Valentin De Bortoli",
      "Alain Durmus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6751ee6546b31ceb7d4ee12276b9f4d9-Abstract-Conference.html": {
    "title": "Directional diffusion models for graph representation learning",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in diverse domains such as image synthesis, super-resolution, and 3D molecule generation. Surprisingly, the application of diffusion models in graph learning has garnered little attention. In this paper, we aim to bridge this gap by exploring the use of diffusion models for unsupervised graph representation learning. Our investigation commences with the identification of anisotropic structures within graphs and the recognition of a crucial limitation in the vanilla forward diffusion process when dealing with these anisotropic structures. The original forward diffusion process continually adds isotropic Gaussian noise to the data, which may excessively dilute anisotropic signals, leading to rapid signal-to-noise conversion. This rapid conversion poses challenges for training denoising neural networks and obstructs the acquisition of semantically meaningful representations during the reverse process. To overcome this challenge, we introduce a novel class of models termed {\\it directional diffusion models}. These models adopt data-dependent, anisotropic, and directional noises in the forward diffusion process. In order to assess the effectiveness of our proposed models, we conduct extensive experiments on 12 publicly available datasets, with a particular focus on two distinct graph representation learning tasks. The experimental results unequivocally establish the superiority of our models over state-of-the-art baselines, underscoring their effectiveness in capturing meaningful graph representations. Our research not only sheds light on the intricacies of the forward process in diffusion models but also underscores the vast potential of these models in addressing a wide spectrum of graph-related tasks. Our code is available at \\url{https://github.com/statsle/DDM}",
    "keywords": [],
    "checked": true,
    "id": "5dcdf4d9609d8ae3c0501861595d0def1401511f",
    "semantic_title": "directional diffusion models for graph representation learning",
    "citation_count": 2,
    "authors": [
      "Run Yang",
      "Yuling Yang",
      "Fan Zhou",
      "Qiang Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6776737cd11cf4afa3af226898474418-Abstract-Conference.html": {
    "title": "UniTSFace: Unified Threshold Integrated Sample-to-Sample Loss for Face Recognition",
    "volume": "main",
    "abstract": "Sample-to-class-based face recognition models can not fully explore the cross-sample relationship among large amounts of facial images, while sample-to-sample-based models require sophisticated pairing processes for training. Furthermore, neither method satisfies the requirements of real-world face verification applications, which expect a unified threshold separating positive from negative facial pairs. In this paper, we propose a unified threshold integrated sample-to-sample based loss (USS loss), which features an explicit unified threshold for distinguishing positive from negative pairs. Inspired by our USS loss, we also derive the sample-to-sample based softmax and BCE losses, and discuss their relationship. Extensive evaluation on multiple benchmark datasets, including MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace, demonstrates that the proposed USS loss is highly efficient and can work seamlessly with sample-to-class-based losses. The embedded loss (USS and sample-to-class Softmax loss) overcomes the pitfalls of previous approaches and the trained facial model UniTSFace exhibits exceptional performance, outperforming state-of-the-art methods, such as CosFace, ArcFace, VPL, AnchorFace, and UNPG. Our code is available at https://github.com/CVI-SZU/UniTSFace",
    "keywords": [],
    "checked": true,
    "id": "4ac3c4d6491421f94d0d6d6937c7662bf2211eb4",
    "semantic_title": "unitsface: unified threshold integrated sample-to-sample loss for face recognition",
    "citation_count": 0,
    "authors": [
      "qiufu li",
      "Xi Jia",
      "Jiancan Zhou",
      "Linlin Shen",
      "Jinming Duan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/677c8dc72c99482507323f313faf4738-Abstract-Conference.html": {
    "title": "Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks",
    "volume": "main",
    "abstract": "Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using benchmark datasets and representative attacks validates the efficacy of MDP. The code of MDP is publicly available",
    "keywords": [],
    "checked": true,
    "id": "41cc3338635cde85316a0bcb934bffc73008761a",
    "semantic_title": "defending pre-trained language models as few-shot learners against backdoor attacks",
    "citation_count": 3,
    "authors": [
      "Zhaohan Xi",
      "Tianyu Du",
      "Changjiang Li",
      "Ren Pang",
      "Shouling Ji",
      "Jinghui Chen",
      "Fenglong Ma",
      "Ting Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/678594bcff6f99f3b7a8ff459989b1a3-Abstract-Conference.html": {
    "title": "On the Power of SVD in the Stochastic Block Model",
    "volume": "main",
    "abstract": "A popular heuristic method for improving clustering results is to apply dimensionality reduction before running clustering algorithms.It has been observed that spectral-based dimensionality reduction tools, such as PCA or SVD, improve the performance of clustering algorithms in many applications. This phenomenon indicates that spectral method not only serves as a dimensionality reduction tool, but also contributes to the clustering procedure in some sense. It is an interesting question to understand the behavior of spectral steps in clustering problems.As an initial step in this direction, this paper studies the power of vanilla-SVD algorithm in the stochastic block model (SBM). We show that, in the symmetric setting, vanilla-SVD algorithm recovers all clusters correctly. This result answers an open question posed by Van Vu (Combinatorics Probability and Computing, 2018) in the symmetric setting",
    "keywords": [],
    "checked": true,
    "id": "e71ddcfb8661a03c86a0b0107d55518eb5f427e9",
    "semantic_title": "on the power of svd in the stochastic block model",
    "citation_count": 0,
    "authors": [
      "Xinyu Mao",
      "Jiapeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/678cffc05549fdabda971127602084c6-Abstract-Conference.html": {
    "title": "Continuous-time Analysis of Anchor Acceleration",
    "volume": "main",
    "abstract": "Recently, the anchor acceleration, an acceleration mechanism distinct from Nesterov's, has been discovered for minimax optimization and fixed-point problems, but its mechanism is not understood well, much less so than Nesterov acceleration. In this work, we analyze continuous-time models of anchor acceleration. We provide tight, unified analyses for characterizing the convergence rate as a function of the anchor coefficient $\\beta(t)$, thereby providing insight into the anchor acceleration mechanism and its accelerated $\\mathcal{O}(1/k^2)$-convergence rate. Finally, we present an adaptive method inspired by the continuous-time analyses and establish its effectiveness through theoretical analyses and experiments",
    "keywords": [],
    "checked": true,
    "id": "bdfebfa663b31c02e3dfdb5bce37a723a09905f6",
    "semantic_title": "continuous-time analysis of anchor acceleration",
    "citation_count": 2,
    "authors": [
      "Jaewook Suh",
      "Jisun Park",
      "Ernest Ryu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html": {
    "title": "Self-supervised Object-Centric Learning for Videos",
    "volume": "main",
    "abstract": "Unsupervised multi-object segmentation has shown impressive results on images by utilizing powerful semantics learned from self-supervised pretraining. An additional modality such as depth or motion is often used to facilitate the segmentation in video sequences. However, the performance improvements observed in synthetic sequences, which rely on the robustness of an additional cue, do not translate to more challenging real-world scenarios. In this paper, we propose the first fully unsupervised method for segmenting multiple objects in real-world sequences. Our object-centric learning framework spatially binds objects to slots on each frame and then relates these slots across frames. From these temporally-aware slots, the training objective is to reconstruct the middle frame in a high-level semantic feature space. We propose a masking strategy by dropping a significant portion of tokens in the feature space for efficiency and regularization. Additionally, we address over-clustering by merging slots based on similarity. Our method can successfully segment multiple instances of complex and high-variety classes in YouTube videos",
    "keywords": [],
    "checked": true,
    "id": "dce93d147731495d0ec3eec8713f82cc67545056",
    "semantic_title": "self-supervised object-centric learning for videos",
    "citation_count": 4,
    "authors": [
      "Görkay Aydemir",
      "Weidi Xie",
      "Fatma Guney"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67b2e2e895380fa6acd537c2894e490e-Abstract-Conference.html": {
    "title": "Improving Adversarial Transferability via Intermediate-level Perturbation Decay",
    "volume": "main",
    "abstract": "Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneously. In-depth discussion verifies the effectiveness of our method. Experimental results show that it outperforms state-of-the-arts by large margins in attacking various victim models on ImageNet (+10.07% on average) and CIFAR-10 (+3.88% on average). Our code is at https://github.com/qizhangli/ILPD-attack",
    "keywords": [],
    "checked": false,
    "id": "333a8c09fee4dbf8fbd6d9291c9bc6aacd3cf7d9",
    "semantic_title": "improving adversarial transferability by intermediate-level perturbation decay",
    "citation_count": 1,
    "authors": [
      "Qizhang Li",
      "Yiwen Guo",
      "Wangmeng Zuo",
      "Hao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67d5c7dd7930dfce2725defdb0552b6e-Abstract-Conference.html": {
    "title": "GUST: Combinatorial Generalization by Unsupervised Grouping with Neuronal Coherence",
    "volume": "main",
    "abstract": "Dynamically grouping sensory information into structured entities is essential for understanding the world of combinatorial nature. However, the grouping ability and therefore combinatorial generalization are still challenging artificial neural networks. Inspired by the evidence that successful grouping is indicated by neuronal coherence in the human brain, we introduce GUST (Grouping Unsupervisely by Spike Timing network), an iterative network architecture with biological constraints to bias the network towards a dynamical state of neuronal coherence that softly reflects the grouping information in the temporal structure of its spiking activity. We evaluate and analyze the model on synthetic datasets. Interestingly, the segregation ability is directly learned from superimposed stimuli with a succinct unsupervised objective. Two learning stages are present, from coarsely perceiving global features to additionally capturing local features. Further, the learned symbol-like building blocks can be systematically composed to represent novel scenes in a bio-plausible manner",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zheng",
      "Hui Lin",
      "Rong Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67dd6a41bf9539cffc0fc0165e4d0616-Abstract-Conference.html": {
    "title": "State Regularized Policy Optimization on Data with Dynamics Shift",
    "volume": "main",
    "abstract": "In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy.However, these methods can be sample inefficient as data are used \\textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\\textbf{S}tate \\textbf{R}egularized \\textbf{P}olicy \\textbf{O}ptimization) algorithm. To conduct theoretical analyses, the intuition of similar environment structures is characterized by the notion of homomorphous MDPs. We then demonstrate a lower-bound performance guarantee on policies regularized by the stationary state distribution. In practice, SRPO can be an add-on module to context-based algorithms in both online and offline RL settings.Experimental results show that SRPO can make several context-based algorithms far more data efficient and significantly improve their overall performance",
    "keywords": [],
    "checked": true,
    "id": "8b87214afe121dc39c342fabc73014c4fefb31e3",
    "semantic_title": "state regularized policy optimization on data with dynamics shift",
    "citation_count": 2,
    "authors": [
      "Zhenghai Xue",
      "Qingpeng Cai",
      "Shuchang Liu",
      "Dong Zheng",
      "Peng Jiang",
      "Kun Gai",
      "Bo An"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67f30132d98e758f7b4e28c36091d86e-Abstract-Conference.html": {
    "title": "Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models",
    "volume": "main",
    "abstract": "We propose a conceptually simple and lightweight framework for improving the robustness of vision models through the combination of knowledge distillation and data augmentation. We address the conjecture that larger models do not make for better teachers by showing strong gains in out-of-distribution robustness when distilling from pretrained foundation models. Following this finding, we propose Discrete Adversarial Distillation (DAD), which leverages a robust teacher to generate adversarial examples and a VQGAN to discretize them, creating more informative samples than standard data augmentation techniques. We provide a theoretical framework for the use of a robust teacher in the knowledge distillation with data augmentation setting and demonstrate strong gains in out-of-distribution robustness and clean accuracy across different student architectures. Notably, our method adds minor computational overhead compared to similar techniques and can be easily combined with other data augmentations for further improvements",
    "keywords": [],
    "checked": true,
    "id": "f71ee484b9182cfe00ed260ae0c70014cabf9f59",
    "semantic_title": "distilling out-of-distribution robustness from vision-language foundation models",
    "citation_count": 0,
    "authors": [
      "Andy Zhou",
      "Jindong Wang",
      "Yu-Xiong Wang",
      "Haohan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6818dcc65fdf3cbd4b05770fb957803e-Abstract-Conference.html": {
    "title": "Factorized Contrastive Learning: Going Beyond Multi-view Redundancy",
    "volume": "main",
    "abstract": "In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks",
    "keywords": [],
    "checked": true,
    "id": "e1b2a35a000ca296c32284b323c7e36a28fe0693",
    "semantic_title": "factorized contrastive learning: going beyond multi-view redundancy",
    "citation_count": 6,
    "authors": [
      "Paul Pu Liang",
      "Zihao Deng",
      "Martin Q. Ma",
      "James Y. Zou",
      "Louis-Philippe Morency",
      "Ruslan Salakhutdinov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/683464f40aa1a6b7c939c3e9cd64b1fd-Abstract-Conference.html": {
    "title": "Semantic Image Synthesis with Unconditional Generator",
    "volume": "main",
    "abstract": "Semantic image synthesis (SIS) aims to generate realistic images according to semantic masks given by a user. Although recent methods produce high quality results with fine spatial control, SIS requires expensive pixel-level annotation of the training images. On the other hand, manipulating intermediate feature maps in a pretrained unconditional generator such as StyleGAN supports coarse spatial control without heavy annotation. In this paper, we introduce a new approach, for reflecting user's detailed guiding masks on a pretrained unconditional generator. Our method converts a user's guiding mask to a proxy mask through a semantic mapper. Then the proxy mask conditions the resulting image through a rearranging network based on cross-attention mechanism. The proxy mask is simple clustering of intermediate feature maps in the generator. The semantic mapper and the rearranging network are easy to train (less than half an hour). Our method is useful for many tasks: semantic image synthesis, spatially editing real images, and unaligned local transplantation. Last but not least, it is generally applicable to various datasets such as human faces, animal faces, and churches",
    "keywords": [],
    "checked": false,
    "id": "9481914391856d7ded7d487f1c9c058cfaca25f3",
    "semantic_title": "semantic image synthesis with vision graph neural network",
    "citation_count": 0,
    "authors": [
      "JungWoo Chae",
      "Hyunin Cho",
      "Sooyeon Go",
      "Kyungmook Choi",
      "Youngjung Uh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/68637ee6b30276f900bc67320466b69f-Abstract-Conference.html": {
    "title": "Learning Neural Implicit through Volume Rendering with Attentive Depth Fusion Priors",
    "volume": "main",
    "abstract": "Learning neural implicit representations has achieved remarkable performance in 3D reconstruction from multi-view images. Current methods use volume rendering to render implicit representations into either RGB or depth images that are supervised by the multi-view ground truth. However, rendering a view each time suffers from incomplete depth at holes and unawareness of occluded structures from the depth supervision, which severely affects the accuracy of geometry inference via volume rendering. To resolve this issue, we propose to learn neural implicit representations from multi-view RGBD images through volume rendering with an attentive depth fusion prior. Our prior allows neural networks to sense coarse 3D structures from the Truncated Signed Distance Function (TSDF) fused from all available depth images for rendering. The TSDF enables accessing the missing depth at holes on one depth image and the occluded parts that are invisible from the current view. By introducing a novel attention mechanism, we allow neural networks to directly use the depth fusion prior with the inferred occupancy as the learned implicit function. Our attention mechanism works with either a one-time fused TSDF that represents a whole scene or an incrementally fused TSDF that represents a partial scene in the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on widely used benchmarks including synthetic and real-world scans show our superiority over the latest neural implicit methods",
    "keywords": [],
    "checked": true,
    "id": "6a69bd5f31eb523d63b9839c137f36516ded71f0",
    "semantic_title": "learning neural implicit through volume rendering with attentive depth fusion priors",
    "citation_count": 2,
    "authors": [
      "Pengchong Hu",
      "Zhizhong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/686a3f32067838c8dbb68da6e9e3cf69-Abstract-Conference.html": {
    "title": "SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning",
    "volume": "main",
    "abstract": "Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demonstrate the effectiveness of the proposed methods over existing FBO algorithms",
    "keywords": [],
    "checked": true,
    "id": "e90ee5b4566c2c7012c6044039057535476248a8",
    "semantic_title": "simfbo: towards simple, flexible and communication-efficient federated bilevel learning",
    "citation_count": 3,
    "authors": [
      "Yifan Yang",
      "Peiyao Xiao",
      "Kaiyi Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/68730224bbf35ffac7a4fbf9b1ea4bfe-Abstract-Conference.html": {
    "title": "PICProp: Physics-Informed Confidence Propagation for Uncertainty Quantification",
    "volume": "main",
    "abstract": "Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost.This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem.That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees.We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions.We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning. Code is available at https://github.com/ShenQianli/PICProp",
    "keywords": [],
    "checked": true,
    "id": "49f5066cf3a4e7a3a15712ab6d2868a80d330286",
    "semantic_title": "picprop: physics-informed confidence propagation for uncertainty quantification",
    "citation_count": 0,
    "authors": [
      "Qianli Shen",
      "Wai Hoh Tang",
      "Zhun Deng",
      "Apostolos Psaros",
      "Kenji Kawaguchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/687b7b2bdcc2ced577c0a989b44e7078-Abstract-Conference.html": {
    "title": "Foundation Model is Efficient Multimodal Multitask Model Selector",
    "volume": "main",
    "abstract": "This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering.A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability, they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee. Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models, making it the first model selection method in the multi-task scenario. For instance, compared with the state- of-the-art method LogME enhanced by our label embeddings, EMMS achieves 9.0%, 26.3%, 20.1%, 54.8%, 12.2% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13×, 6.29×, 3.59×, 6.19×, and 5.66× speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector",
    "keywords": [],
    "checked": true,
    "id": "a9a05fdbbc7d469bb4a308c3af39135225a3acba",
    "semantic_title": "foundation model is efficient multimodal multitask model selector",
    "citation_count": 3,
    "authors": [
      "fanqing meng",
      "Wenqi Shao",
      "zhanglin peng",
      "Chonghe Jiang",
      "Kaipeng Zhang",
      "Yu Qiao",
      "Ping Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/68b138608ef80b08d65b1bd9594d9559-Abstract-Conference.html": {
    "title": "Feature Likelihood Score: Evaluating the Generalization of Generative Models Using Samples",
    "volume": "main",
    "abstract": "The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Score (FLS), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLS to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLS on various image datasets and model classes, demonstrating its ability to match intuitions of previous metrics like FID while offering a more comprehensive evaluation of generative models",
    "keywords": [],
    "checked": true,
    "id": "580365279131e18cbe3e06d842cbb460da168b0d",
    "semantic_title": "feature likelihood score: evaluating the generalization of generative models using samples",
    "citation_count": 0,
    "authors": [
      "Marco Jiralerspong",
      "Joey Bose",
      "Ian Gemp",
      "Chongli Qin",
      "Yoram Bachrach",
      "Gauthier Gidel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/68efc144ad3b41108f779b51b9fb1300-Abstract-Conference.html": {
    "title": "Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks",
    "volume": "main",
    "abstract": "Quantum neural networks (QNNs) have been a promising framework in pursuing near-term quantum advantage in various fields, where many applications can be viewed as learning a quantum state that encodes useful data. As a quantum analog of probability distribution learning, quantum state learning is theoretically and practically essential in quantum machine learning. In this paper, we develop a no-go theorem for learning an unknown quantum state with QNNs even starting from a high-fidelity initial state. We prove that when the loss value is lower than a critical threshold, the probability of avoiding local minima vanishes exponentially with the qubit count, while only grows polynomially with the circuit depth. The curvature of local minima is concentrated to the quantum Fisher information times a loss-dependent constant, which characterizes the sensibility of the output state with respect to parameters in QNNs. These results hold for any circuit structures, initialization strategies, and work for both fixed ansatzes and adaptive methods. Extensive numerical simulations are performed to validate our theoretical results. Our findings place generic limits on good initial guesses and adaptive methods for improving the learnability and scalability of QNNs, and deepen the understanding of prior information's role in QNNs",
    "keywords": [],
    "checked": true,
    "id": "f1e85bee7ee2ddbe09bb6e5df919486bed40e6bc",
    "semantic_title": "statistical analysis of quantum state learning process in quantum neural networks",
    "citation_count": 1,
    "authors": [
      "Hao-Kai Zhang",
      "Chenghong Zhu",
      "Mingrui Jing",
      "Xin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/690b93e9ab0cc3b1d88b32f6f473ce69-Abstract-Conference.html": {
    "title": "SOL: Sampling-based Optimal Linear bounding of arbitrary scalar functions",
    "volume": "main",
    "abstract": "Finding tight linear bounds for activation functions in neural networksis an essential part of several state of the art neural network robustness certification tools. An activation function is an arbitrary, nonlinear,scalar function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$. In the existing work on robustness certification, such bounds have been computed using human ingenuity for a handful of the most popular activation functions. While a number of heuristics have been proposed for bounding arbitrary functions,no analysis of the tightness optimality for general scalar functions has been offered yet, to the best of our knowledge. We fill this gap by formulating a concise optimality criterion for tightness of the approximation which allows us tobuild optimal bounds for any function convex in the region of interest $R$. Fora more general class of functions Lipshitz-continuous in $R$ we propose a sampling-based approach (SOL) which, given an instance of the bounding problem, efficiently computes the tightest linear bounds within a given $\\varepsilon > 0$ threshold. We leverage an adaptive sampling technique to iteratively build a setof sample points suitable for representing the target activation function. While the theoretical worst case time complexity of our approach is$O(\\varepsilon^{-2d})$,it typically only takes $O(\\log^{\\beta} \\frac{1}{\\varepsilon})$ time for some $\\beta \\ge 1$ and isthus sufficiently fast in practice. We provide empirical evidence of SOL's practicalityby incorporating it into a robustness certifier and observing that itproduces similar or higher certification rates while taking as low as quarter of the time compared to the other methods",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuriy Biktairov",
      "Jyotirmoy Deshmukh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/690e82a09bcb3f101831962bf3cb54ec-Abstract-Conference.html": {
    "title": "Opening the Vocabulary of Egocentric Actions",
    "volume": "main",
    "abstract": "Human actions in egocentric videos often feature hand-object interactions composed of a verb (performed by the hand) applied to an object. Despite their extensive scaling up, egocentric datasets still face two limitations — sparsity of action compositions and a closed set of interacting objects. This paper proposes a novel open vocabulary action recognition task. Given a set of verbs and objects observed during training, the goal is to generalize the verbs to an open vocabulary of actions with seen and novel objects. To this end, we decouple the verb and object predictions via an object-agnostic verb encoder and a prompt-based object encoder. The prompting leverages CLIP representations to predict an open vocabulary of interacting objects. We create open vocabulary benchmarks on the EPIC-KITCHENS-100 and Assembly101 datasets; whereas closed-action methods fail to generalize, our proposed method is effective. In addition, our object encoder significantly outperforms existing open-vocabulary visual recognition methods in recognizing novel interacting objects",
    "keywords": [],
    "checked": true,
    "id": "4d4caf0d8aa589bfa2f836ea30076c8b080a0cd8",
    "semantic_title": "opening the vocabulary of egocentric actions",
    "citation_count": 1,
    "authors": [
      "Dibyadip Chatterjee",
      "Fadime Sener",
      "Shugao Ma",
      "Angela Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/690eb240baf1180b69dac48fc905c918-Abstract-Conference.html": {
    "title": "On the Pareto Front of Multilingual Neural Machine Translation",
    "volume": "main",
    "abstract": "In this work, we study how the performance of a given direction changes with its sampling ratio in Multilingual Neural Machine Translation (MNMT). By training over 200 multilingual models with various model sizes, data sizes, and language directions, we find it interesting that the performance of certain translation direction does not always improve with the increase of its weight in the multi-task optimization objective. Accordingly, scalarization method leads to a multitask trade-off front that deviates from the traditional Pareto front when there exists data imbalance in the training corpus, which poses a great challenge to improve the overall performance of all directions. Based on our observations, we propose the Double Power Law to predict the unique performance trade-off front in MNMT, which is robust across various languages, data adequacy, and the number of tasks. Finally, we formulate the sample ratio selection problem in MNMT as an optimization problem based on the Double Power Law. Extensive experiments show that it achieves better performance than temperature searching and gradient manipulation methods with only 1/5 to 1/2 of the total training budget. We release the code at https://github.com/pkunlp-icler/ParetoMNMT for reproduction",
    "keywords": [],
    "checked": true,
    "id": "a05e67039798a9b7280a929c17a8600d1b456ac0",
    "semantic_title": "on the pareto front of multilingual neural machine translation",
    "citation_count": 0,
    "authors": [
      "Liang Chen",
      "Shuming Ma",
      "Dongdong Zhang",
      "Furu Wei",
      "Baobao Chang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/694be3548697e9cc8999d45e8d16fe1e-Abstract-Conference.html": {
    "title": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling",
    "volume": "main",
    "abstract": "Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling.Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling.These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN",
    "keywords": [],
    "checked": true,
    "id": "434d751d355d7a7c20efa570e785c76286245e77",
    "semantic_title": "hierarchically gated recurrent neural network for sequence modeling",
    "citation_count": 10,
    "authors": [
      "Zhen Qin",
      "Songlin Yang",
      "Yiran Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/694ec0018b9fd0ebe863ec29fa5a89b9-Abstract-Conference.html": {
    "title": "Why Did This Model Forecast This Future? Information-Theoretic Saliency for Counterfactual Explanations of Probabilistic Regression Models",
    "volume": "main",
    "abstract": "We propose a post hoc saliency-based explanation framework for counterfactual reasoning in probabilistic multivariate time-series forecasting (regression) settings. Building upon Miller's framework of explanations derived from research in multiple social science disciplines, we establish a conceptual link between counterfactual reasoning and saliency-based explanation techniques. To address the lack of a principled notion of saliency, we leverage a unifying definition of information-theoretic saliency grounded in preattentive human visual cognition and extend it to forecasting settings. Specifically, we obtain a closed-form expression for commonly used density functions to identify which observed timesteps appear salient to an underlying model in making its probabilistic forecasts. We empirically validate our framework in a principled manner using synthetic data to establish ground-truth saliency that is unavailable for real-world data. Finally, using real-world data and forecasting models, we demonstrate how our framework can assist domain experts in forming new data-driven hypotheses about the causal relationships between features in the wild",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chirag Raman",
      "Alec Nonnemaker",
      "Amelia Villegas-Morcillo",
      "Hayley Hung",
      "Marco Loog"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/695b6f9490d27d852e439e35c56e73e3-Abstract-Conference.html": {
    "title": "Category-Extensible Out-of-Distribution Detection via Hierarchical Context Descriptions",
    "volume": "main",
    "abstract": "The key to OOD detection has two aspects: generalized feature representation and precise category description. Recently, vision-language models such as CLIP provide significant advances in both two issues, but constructing precise category descriptions is still in its infancy due to the absence of unseen categories. This work introduces two hierarchical contexts, namely perceptual context and spurious context, to carefully describe the precise category boundary through automatic prompt tuning. Specifically, perceptual contexts perceive the inter-category difference (e.g., cats vs apples) for current classification tasks, while spurious contexts further identify spurious (similar but exactly not) OOD samples for every single category (e.g., cats vs panthers, apples vs peaches). The two contexts hierarchically construct the precise description for a certain category, which is, first roughly classifying a sample to the predicted category and then delicately identifying whether it is truly an ID sample or actually OOD. Moreover, the precise descriptions for those categories within the vision-language framework present a novel application: CATegory-EXtensible OOD detection (CATEX). One can efficiently extend the set of recognizable categories by simply merging the hierarchical contexts learned under different sub-task settings. And extensive experiments are conducted to demonstrate CATEX's effectiveness, robustness, and category-extensibility. For instance, CATEX consistently surpasses the rivals by a large margin with several protocols on the challenging ImageNet-1K dataset. In addition, we offer new insights on how to efficiently scale up the prompt engineering in vision-language models to recognize thousands of object categories, as well as how to incorporate large language models (like GPT-3) to boost zero-shot applications",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Liu",
      "Zhihang Fu",
      "Chao Chen",
      "Sheng Jin",
      "Ze Chen",
      "Mingyuan Tao",
      "Rongxin Jiang",
      "Jieping Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/697200c9d1710c2799720b660abd11bb-Abstract-Conference.html": {
    "title": "Online Corrupted User Detection and Regret Minimization",
    "volume": "main",
    "abstract": "In real-world online web systems, multiple users usually arrive sequentially into the system. For applications like click fraud and fake reviews, some users can maliciously perform corrupted (disrupted) behaviors to trick the system. Therefore, it is crucial to design efficient online learning algorithms to robustly learn from potentially corrupted user behaviors and accurately identify the corrupted users in an online manner. Existing works propose bandit algorithms robust to adversarial corruption. However, these algorithms are designed for a single user, and cannot leverage the implicit social relations among multiple users for more efficient learning. Moreover, none of them consider how to detect corrupted users online in the multiple-user scenario. In this paper, we present an important online learning problem named LOCUD to learn and utilize unknown user relations from disrupted behaviors to speed up learning, and identify the corrupted users in an online setting. To robustly learn and utilize the unknown relations among potentially corrupted users, we propose a novel bandit algorithm RCLUB-WCU. To detect the corrupted users, we devise a novel online detection algorithm OCCUD based on RCLUB-WCU's inferred user relations. We prove a regret upper bound for RCLUB-WCU, which asymptotically matches the lower bound with respect to $T$ up to logarithmic factors, and matches the state-of-the-art results in degenerate cases. We also give a theoretical guarantee for the detection accuracy of OCCUD. With extensive experiments, our methods achieve superior performance over previous bandit algorithms and high corrupted user detection accuracy",
    "keywords": [],
    "checked": true,
    "id": "f1827e73c44a67346664f69a57ca6d6b48a55178",
    "semantic_title": "online corrupted user detection and regret minimization",
    "citation_count": 1,
    "authors": [
      "Zhiyong Wang",
      "Jize Xie",
      "Tong Yu",
      "Shuai Li",
      "John C.S. Lui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/69bf9fd8d3b7b792b6c8c19149024d22-Abstract-Conference.html": {
    "title": "Nash Regret Guarantees for Linear Bandits",
    "volume": "main",
    "abstract": "We obtain essentially tight upper bounds for a strengthened notion of regret in the stochastic linear bandits framework. The strengthening---referred to as Nash regret---is defined as the difference between the (a priori unknown) optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithm. Since the geometric mean corresponds to the well-studied Nash social welfare (NSW) function, this formulation quantifies the performance of a bandit algorithm as the collective welfare it generates across rounds. NSW is known to satisfy fairness axioms and, hence, an upper bound on Nash regret provides a principled fairness guarantee. We consider the stochastic linear bandits problem over a horizon of $\\mathsf{T}$ rounds and with a set of arms ${\\cal X}$ in ambient dimension $d$. Furthermore, we focus on settings in which the stochastic reward---associated with each arm in ${\\cal X}$---is a non-negative, sub-Poisson random variable. For this setting, we develop an algorithm that achieves a Nash regret of $O\\left( \\sqrt{\\frac{d}{\\mathsf{T}}} \\log(\\mathsf{T} |{\\cal X}|)\\right)$. In addition, addressing linear bandit instances in which the set of arms ${\\cal X}$ is not necessarily finite, we obtain a Nash regret upper bound of $O\\left( \\frac{d^\\frac{5}{4}}{\\sqrt{\\mathsf{T}}} \\log(\\mathsf{T})\\right)$. Since bounded random variables are sub-Poisson, these results hold for bounded, non-negative rewards. Our linear bandit algorithm is built upon the successive elimination method with novel technical insights, including tailored concentration bounds and the use of sampling via John ellipsoid in conjunction with the Kiefer–Wolfowitz optimal design",
    "keywords": [],
    "checked": true,
    "id": "7a9a2460079ab272a91da5f93c4dc83fa5156705",
    "semantic_title": "nash regret guarantees for linear bandits",
    "citation_count": 0,
    "authors": [
      "Ayush Sawarni",
      "Soumyabrata Pal",
      "Siddharth Barman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/69c49f75ca31620f1f0d38093d9f3d9b-Abstract-Conference.html": {
    "title": "ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. However, both the attention mechanism and multi-layer perceptrons (MLPs) in ViTs are not sufficiently efficient due to dense multiplications, leading to costly training and inference. To this end, we propose to reparameterize pre-trained ViTs with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\\textbf{ShiftAddViT}$, which aims to achieve end-to-end inference speedups on GPUs without requiring training from scratch. Specifically, all $\\texttt{MatMuls}$ among queries, keys, and values are reparameterized using additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized with shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameterization on (quadratic or linear) attention maintains model accuracy, while inevitably leading to accuracy drops when being applied to MLPs. To marry the best of both worlds, we further propose a new mixture of experts (MoE) framework to reparameterize MLPs by taking multiplication or its primitives as experts, e.g., multiplication and shift, and designing a new latency-aware load-balancing loss. Such a loss helps to train a generic router for assigning a dynamic amount of input tokens to different experts according to their latency. In principle, the faster the experts run, the more input tokens they are assigned. Extensive experiments on various 2D/3D Transformer-based vision tasks consistently validate the effectiveness of our proposed ShiftAddViT, achieving up to $\\textbf{5.18$\\times$}$ latency reductions on GPUs and $\\textbf{42.9}$% energy savings, while maintaining a comparable accuracy as original or efficient ViTs. Codes and models are available at https://github.com/GATECH-EIC/ShiftAddViT",
    "keywords": [],
    "checked": true,
    "id": "92a95c5d3ea87e08ac527d8ce25383ff8c1015be",
    "semantic_title": "shiftaddvit: mixture of multiplication primitives towards efficient vision transformer",
    "citation_count": 2,
    "authors": [
      "Haoran You",
      "Huihong Shi",
      "Yipin Guo",
      "Yingyan Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/69ce18ad9f53f28e8e7ac1649ae02337-Abstract-Conference.html": {
    "title": "Optimal Extragradient-Based Algorithms for Stochastic Variational Inequalities with Separable Structure",
    "volume": "main",
    "abstract": "We consider the problem of solving stochastic monotone variational inequalities with a separable structure using a stochastic first-order oracle. Building on standard extragradient for variational inequalities we propose a novel algorithm---stochastic \\emph{accelerated gradient-extragradient} (AG-EG)---for strongly monotone variational inequalities (VIs). Our approach combines the strengths of extragradient and Nesterov acceleration. By showing that its iterates remain in a bounded domain and applying scheduled restarting, we prove that AG-EG has an optimal convergence rate for strongly monotone VIs. Furthermore, when specializing to the particular case of bilinearly coupled strongly-convex-strongly-concave saddle-point problems, including bilinear games, our algorithm achieves fine-grained convergence rates that match the respective lower bounds, with the stochasticity being characterized by an additive statistical error term that is optimal up to a constant prefactor",
    "keywords": [],
    "checked": true,
    "id": "8830327b53f816d26193c56ce864231db8f5e475",
    "semantic_title": "optimal extragradient-based algorithms for stochastic variational inequalities with separable structure",
    "citation_count": 0,
    "authors": [
      "Angela Yuan",
      "Chris Junchi Li",
      "Gauthier Gidel",
      "Michael Jordan",
      "Quanquan Gu",
      "Simon S. Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/69f98acf161316ed896047e45da3bc0c-Abstract-Conference.html": {
    "title": "Combinatorial Group Testing with Selfish Agents",
    "volume": "main",
    "abstract": "We study the Combinatorial Group Testing (CGT) problem in a novel game-theoretic framework, with a solution concept of Adversarial Equilibrium (AE). In this new framework, we have $n$ selfish agents corresponding to the elements of the universe $[n] =\\{0,1,\\ldots,n-1\\}$ and a hidden set $K \\subseteq [n]$ of active agents of size $|K| = k \\ll n$. In each round of the game, each active agent decides if it is present in a query $Q \\subseteq [n]$, and all agents receive feedback on $Q \\cap K$. The goal of each active agent is to assure that its id could be learned from the feedback as early as possible. We present a comprehensive set of results in this new game, where we design and analyze adaptive algorithmic strategies of agents which are AE's. In particular, if $k$ is known to the agents, then we design adaptive AE strategies with provably near optimal learning time of $O(k \\log(n/k))$. In the case of unknown $k$, we design an adaptive AE strategies with learning time of order $n^k$, and we prove a lower bound of $\\Omega(n)$ on the learning time of any such algorithmic strategies. This shows a strong separations between the two models of known and unknown $k$, as well as between the classic CGT, i.e., without selfish agents, and our game theoretic CGT model",
    "keywords": [],
    "checked": false,
    "id": "13775e7d59aedcfe993d86f68ea2f644aadecaa7",
    "semantic_title": "piecewise-stationary combinatorial semi-bandit with causally related rewards",
    "citation_count": 0,
    "authors": [
      "Georgios Chionas",
      "Dariusz Kowalski",
      "Piotr Krysta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a0480190bbe6b622c7f1d3aa9be9c0f-Abstract-Conference.html": {
    "title": "A Hierarchical Spatial Transformer for Massive Point Samples in Continuous Space",
    "volume": "main",
    "abstract": "Transformers are widely used deep learning architectures. Existing transformers are mostly designed for sequences (texts or time series), images or videos, and graphs. This paper proposes a novel transformer model for massive (up to a million) point samples in continuous space. Such data are ubiquitous in environment sciences (e.g., sensor observations), numerical simulations (e.g., particle-laden flow, astrophysics), and location-based services (e.g., POIs and trajectories). However, designing a transformer for massive spatial points is non-trivial due to several challenges, including implicit long-range and multi-scale dependency on irregular points in continuous space, a non-uniform point distribution, the potential high computational costs of calculating all-pair attention across massive points, and the risks of over-confident predictions due to varying point density. To address these challenges, we propose a new hierarchical spatial transformer model, which includes multi-resolution representation learning within a quad-tree hierarchy and efficient spatial attention via coarse approximation. We also design an uncertainty quantification branch to estimate prediction confidence related to input feature noise and point sparsity. We provide a theoretical analysis of computational time complexity and memory costs. Extensive experiments on both real-world and synthetic datasets show that our method outperforms multiple baselines in prediction accuracy and our model can scale up to one million points on one NVIDIA A100 GPU. The code is available at https://github.com/spatialdatasciencegroup/HST",
    "keywords": [],
    "checked": true,
    "id": "6480d65a03dc789204e6c84b627a1764432869b8",
    "semantic_title": "a hierarchical spatial transformer for massive point samples in continuous space",
    "citation_count": 3,
    "authors": [
      "Wenchong He",
      "Zhe Jiang",
      "Tingsong Xiao",
      "Zelin Xu",
      "Shigang Chen",
      "Ronald Fick",
      "MILES MEDINA",
      "Christine Angelini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a14c7f9fb3f42645cfa6bd5aa446819-Abstract-Conference.html": {
    "title": "Rethinking Gauss-Newton for learning over-parameterized models",
    "volume": "main",
    "abstract": "This work studies the global convergence and implicit bias of Gauss Newton's (GN) when optimizing over-parameterized one-hidden layer networks in the mean-field regime. We first establish a global convergence result for GN in the continuous-time limit exhibiting a faster convergence rate compared to GD due to improved conditioning. We then perform an empirical study on a synthetic regression task to investigate the implicit bias of GN's method.While GN is consistently faster than GD in finding a global optimum, the learned model generalizes well on test data when starting from random initial weights with a small variance and using a small step size to slow down convergence. Specifically, our study shows that such a setting results in a hidden learning phenomenon, where the dynamics are able to recover features with good generalization properties despite the model having sub-optimal training and test performances due to an under-optimized linear layer. This study exhibits a trade-off between the convergence speed of GN and the generalization ability of the learned solution",
    "keywords": [],
    "checked": true,
    "id": "7a468482707a48cc1fb69a020cbd35691432d020",
    "semantic_title": "rethinking gauss-newton for learning over-parameterized models",
    "citation_count": 2,
    "authors": [
      "Michael Arbel",
      "Romain Menegaux",
      "Pierre Wolinski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a15378acabd1aef017ec79a3ed744d2-Abstract-Conference.html": {
    "title": "Knowledge Distillation for High Dimensional Search Index",
    "volume": "main",
    "abstract": "Lightweight compressed models are prevalent in Approximate Nearest Neighbor Search (ANNS) and Maximum Inner Product Search (MIPS) owing to their superiority of retrieval efficiency in large-scale datasets. However, results given by compressed methods are less accurate due to the curse of dimension and the limitations of optimization objectives (e.g., lacking interactions between queries and documents). Thus, we are encouraged to design a new learning algorithm for the compressed search index on high dimensions to improve retrieval performance. In this paper, we propose a novel KnowledgeDistillation for high dimensional search index framework (KDindex), with the aim of efficiently learning lightweight indexes by distilling knowledge from high-precision ANNS and MIPS models such as graph-based indexes. Specifically, the student is guided to keep the same ranking order of the top-k relevant results yielded by the teacher model, which acts as the additional supervision signals between queries and documents to learn the similarities between documents. Furthermore, to avoid the trivial solutions that all candidates are partitioned to the same centroid, the reconstruction loss that minimizes the compressed error, and the posting list balance strategy that equally allocates the candidates, are integrated into the learning objective. Experiment results demonstrate that KDindex outperforms existing learnable quantization-based indexes and is 40× lighter than the state-of-the-art non-exhaustive methods while achieving comparable recall quality",
    "keywords": [],
    "checked": false,
    "id": "26dd3dd55bf7fb4c28c37115ddd8063f5f6b5959",
    "semantic_title": "survey on exact knn queries over high-dimensional data space",
    "citation_count": 9,
    "authors": [
      "Zepu Lu",
      "Jin Chen",
      "Defu Lian",
      "ZAIXI ZHANG",
      "Yong Ge",
      "Enhong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a27ee6f66d13557f15f070274c51721-Abstract-Conference.html": {
    "title": "Exploring the Optimal Choice for Generative Processes in Diffusion Models: Ordinary vs Stochastic Differential Equations",
    "volume": "main",
    "abstract": "The diffusion model has shown remarkable success in computer vision, but it remains unclear whether the ODE-based probability flow or the SDE-based diffusion model is more superior and under what circumstances. Comparing the two is challenging due to dependencies on data distributions, score training, and other numerical issues. In this paper, we study the problem mathematically for two limiting scenarios: the zero diffusion (ODE) case and the large diffusion case. We first introduce a pulse-shape error to perturb the score function and analyze error accumulation of sampling quality, followed by a thorough analysis for generalization to arbitrary error. Our findings indicate that when the perturbation occurs at the end of the generative process, the ODE model outperforms the SDE model with a large diffusion coefficient. However, when the perturbation occurs earlier, the SDE model outperforms the ODE model. We demonstrate that the error of sample generation due to the pulse-shape perturbation is exponentially suppressed as the diffusion term's magnitude increases to infinity. Numerical validation of this phenomenon is provided using Gaussian, Gaussian mixture, and Swiss roll distribution, as well as realistic datasets like MNIST and CIFAR-10",
    "keywords": [],
    "checked": true,
    "id": "2f78b7cbce3c4ee4b42b5cba9ab2165f15e5c627",
    "semantic_title": "exploring the optimal choice for generative processes in diffusion models: ordinary vs stochastic differential equations",
    "citation_count": 3,
    "authors": [
      "Yu Cao",
      "Jingrun Chen",
      "Yixin Luo",
      "Xiang ZHOU"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a412f0037b0df295a39a198666ea6a6-Abstract-Conference.html": {
    "title": "EgoDistill: Egocentric Head Motion Distillation for Efficient Video Understanding",
    "volume": "main",
    "abstract": "Recent advances in egocentric video understanding models are promising, but their heavy computational expense is a barrier for many real-world applications. To address this challenge, we propose EgoDistill, a distillation-based approach that learns to reconstruct heavy ego-centric video clip features by combining the semantics from a sparse set of video frames with head motion from lightweight IMU readings. We further devise a novel IMU-based self-supervised pretraining strategy. Our method leads to significant improvements in efficiency, requiring 200× fewer GFLOPs than equivalent video models. We demonstrate its effectiveness on the Ego4D and EPIC- Kitchens datasets, where our method outperforms state-of-the-art efficient video understanding methods",
    "keywords": [],
    "checked": true,
    "id": "6f83eb44dbfbc2ff55c66d40c2a94e8729aa0e85",
    "semantic_title": "egodistill: egocentric head motion distillation for efficient video understanding",
    "citation_count": 5,
    "authors": [
      "Shuhan Tan",
      "Tushar Nagarajan",
      "Kristen Grauman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a45a1b0697ee086bd8bf494cacc6567-Abstract-Conference.html": {
    "title": "Unsupervised Protein-Ligand Binding Energy Prediction via Neural Euler's Rotation Equation",
    "volume": "main",
    "abstract": "Protein-ligand binding prediction is a fundamental problem in AI-driven drug discovery. Previous work focused on supervised learning methods for small molecules where binding affinity data is abundant, but it is hard to apply the same strategy to other ligand classes like antibodies where labelled data is limited. In this paper, we explore unsupervised approaches and reformulate binding energy prediction as a generative modeling task. Specifically, we train an energy-based model on a set of unlabelled protein-ligand complexes using SE(3) denoising score matching (DSM) and interpret its log-likelihood as binding affinity. Our key contribution is a new equivariant rotation prediction network called Neural Euler's Rotation Equations (NERE) for SE(3) DSM. It predicts a rotation by modeling the force and torque between protein and ligand atoms, where the force is defined as the gradient of an energy function with respect to atom coordinates. Using two protein-ligand and antibody-antigen binding affinity prediction benchmarks, we show that NERE outperforms all unsupervised baselines (physics-based potentials and protein language models) in both cases and surpasses supervised baselines in the antibody case",
    "keywords": [],
    "checked": true,
    "id": "2f534901ac9b3616de5581fea82789afbb22722b",
    "semantic_title": "unsupervised protein-ligand binding energy prediction via neural euler's rotation equation",
    "citation_count": 5,
    "authors": [
      "Wengong Jin",
      "Siranush Sarkizova",
      "Xun Chen",
      "Nir HaCohen",
      "Caroline Uhler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a4d5d85f7a52f062d23d98d544a5578-Abstract-Conference.html": {
    "title": "ProteinNPT: Improving protein property prediction and design with non-parametric transformers",
    "volume": "main",
    "abstract": "Protein design holds immense potential for optimizing naturally occurring sequences, with broad applications in drug discovery, material design, and sustainability. However, computational methods for protein engineering are confronted with significant challenges, including an expansive design space, sparse functional regions, and scarcity of available labels. Furthermore, real-life design scenarios often necessitate the simultaneous optimization of multiple properties, exacerbating label sparsity issues. In this paper, we present ProteinNPT, a non-parametric transformer variant tailored for protein sequences and particularly suited to label-scarce and multi-task optimization settings. We first expand the ProteinGym benchmark to evaluate models in supervised settings and develop several cross-validation schemes for robust assessment. Subsequently, we reimplement existing top-performing baselines, introduce several extensions of these baselines by integrating diverse branches of protein engineering literature, and demonstrate that ProteinNPT consistently outperforms all of them across a diverse set of protein property prediction tasks. Finally, we demonstrate the value of our approach for iterative protein design in several in silico Bayesian optimization experiments",
    "keywords": [],
    "checked": true,
    "id": "646053255a924d51e0b6c8727d8e165dde0fc1f1",
    "semantic_title": "proteinnpt: improving protein property prediction and design with non-parametric transformers",
    "citation_count": 1,
    "authors": [
      "Pascal Notin",
      "Ruben Weitzman",
      "Debora Marks",
      "Yarin Gal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a5181cfe76f67b37a7e1bb19837abdf-Abstract-Conference.html": {
    "title": "Mitigating Source Bias for Fairer Weak Supervision",
    "volume": "main",
    "abstract": "Weak supervision enables efficient development of training sets by reducing the need for ground truth labels. However, the techniques that make weak supervision attractive---such as integrating any source of signal to estimate unknown labels---also entail the danger that the produced pseudolabels are highly biased. Surprisingly, given everyday use and the potential for increased bias, weak supervision has not been studied from the point of view of fairness. We begin such a study, starting with the observation that even when a fair model can be built from a dataset with access to ground-truth labels, the corresponding dataset labeled via weak supervision can be arbitrarily unfair. To address this, we propose and empirically validate a model for source unfairness in weak supervision, then introduce a simple counterfactual fairness-based technique that can mitigate these biases. Theoretically, we show that it is possible for our approach to simultaneously improve both accuracy and fairness---in contrast to standard fairness approaches that suffer from tradeoffs. Empirically, we show that our technique improves accuracy on weak supervision baselines by as much as 32\\% while reducing demographic parity gap by 82.5\\%. A simple extension of our method aimed at maximizing performance produces state-of-the-art performance in five out of ten datasets in the WRENCH benchmark",
    "keywords": [],
    "checked": true,
    "id": "9c05e357dc539f408e76a073e3200caef53c3395",
    "semantic_title": "mitigating source bias for fairer weak supervision",
    "citation_count": 1,
    "authors": [
      "Changho Shin",
      "Sonia Cromp",
      "Dyah Adila",
      "Frederic Sala"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a55f024db3f771194bdadc8f3a35381-Abstract-Conference.html": {
    "title": "GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels",
    "volume": "main",
    "abstract": "Evaluating the performance of graph neural networks (GNNs) is an essential task for practical GNN model deployment and serving, as deployed GNNs face significant performance uncertainty when inferring on unseen and unlabeled test graphs, due to mismatched training-test graph distributions. In this paper, we study a new problem, GNN model evaluation, that aims to assess the performance of a specific GNN model trained on labeled and observed graphs, by precisely estimating its performance (e.g., node classification accuracy) on unseen graphs without labels. Concretely, we propose a two-stage GNN model evaluation framework, including (1) DiscGraph set construction and (2) GNNEvaluator training and inference. The DiscGraph set captures wide-range and diverse graph data distribution discrepancies through a discrepancy measurement function, which exploits the GNN outputs of latent node embeddings and node class predictions. Under the effective training supervision from the DiscGraph set, GNNEvaluator learns to precisely estimate node classification accuracy of the to-be-evaluated GNN model and makes an accurate inference for evaluating GNN model performance. Extensive experiments on real-world unseen and unlabeled test graphs demonstrate the effectiveness of our proposed method for GNN model evaluation",
    "keywords": [],
    "checked": true,
    "id": "80568b425b09e53debcfd23929ba68595874c8de",
    "semantic_title": "gnnevaluator: evaluating gnn performance on unseen graphs without labels",
    "citation_count": 3,
    "authors": [
      "Xin Zheng",
      "Miao Zhang",
      "Chunyang Chen",
      "Soheila Molaei",
      "Chuan Zhou",
      "Shirui Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a6679e3d5b9f7d5f09cdb79a5fc3fd8-Abstract-Conference.html": {
    "title": "Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures",
    "volume": "main",
    "abstract": "The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with *weight-sharing*. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- *expand* and *reduce*. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between these two K-FAC variations when using them to train both a graph neural network and a vision transformer. However, both variations are able to reach a fixed validation metric target in $50$-$75$\\% of the number of steps of a first-order reference run, which translates into a comparable improvement in wall-clock time. This highlights the potential of applying K-FAC to modern neural network architectures",
    "keywords": [],
    "checked": true,
    "id": "35bfd3eaa9760f04af07199faab4972f42b0fde4",
    "semantic_title": "kronecker-factored approximate curvature for modern neural network architectures",
    "citation_count": 1,
    "authors": [
      "Runa Eschenhagen",
      "Alexander Immer",
      "Richard Turner",
      "Frank Schneider",
      "Philipp Hennig"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a69d44b3386e50c06f7107ef4f29302-Abstract-Conference.html": {
    "title": "Tanimoto Random Features for Scalable Molecular Machine Learning",
    "volume": "main",
    "abstract": "The Tanimoto coefficient is commonly used to measure the similarity between molecules represented as discrete fingerprints,either as a distance metric or a positive definite kernel. While many kernel methods can be accelerated using random feature approximations, at present there is a lack of such approximations for the Tanimoto kernel. In this paper we propose two kinds of novel random features to allow this kernel to scale to large datasets, and in the process discover a novel extension of the kernel to real-valued vectors. We theoretically characterize these random features, and provide error bounds on the spectral norm of the Gram matrix. Experimentally, we show that these random features are effective at approximating the Tanimoto coefficient of real-world datasetsand are useful for molecular property prediction and optimization tasks. Future updates to this work will be available at http://arxiv.org/abs/2306.14809",
    "keywords": [],
    "checked": true,
    "id": "b04b2bdef4acfca2f70f657e87729b511e16edc9",
    "semantic_title": "tanimoto random features for scalable molecular machine learning",
    "citation_count": 1,
    "authors": [
      "Austin Tripp",
      "Sergio Bacallado",
      "Sukriti Singh",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a6e010edde1b8f2812f558b67a1974e-Abstract-Conference.html": {
    "title": "Probabilistic Inference in Reinforcement Learning Done Right",
    "volume": "main",
    "abstract": "A popular perspective in Reinforcement learning (RL) casts the problem as probabilistic inference on a graphical model of the Markov decision process (MDP). The core object of study is the probability of each state-action pair being visited under the optimal policy. Previous approaches to approximate this quantity can be arbitrarily poor, leading to algorithms that do not implement genuine statistical inference and consequently do not perform well in challenging problems. In this work, we undertake a rigorous Bayesian treatment of the posterior probability of state-action optimality and clarify how it flows through the MDP. We first reveal that this quantity can indeed be used to generate a policy that explores efficiently, as measured by regret. Unfortunately, computing it is intractable, so we derive a new variational Bayesian approximation yielding a tractable convex optimization problem and establish that the resulting policy also explores efficiently. We call our approach VAPOR and show that it has strong connections to Thompson sampling, K-learning, and maximum entropy exploration. We conclude with some experiments demonstrating the performance advantage of a deep RL version of VAPOR",
    "keywords": [],
    "checked": true,
    "id": "1675bce7fca4eb2171f68755e79c399060087f23",
    "semantic_title": "probabilistic inference in reinforcement learning done right",
    "citation_count": 1,
    "authors": [
      "Jean Tarbouriech",
      "Tor Lattimore",
      "Brendan O'Donoghue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a6ecedac816a24f92ad1f444b1edcb0-Abstract-Conference.html": {
    "title": "Scale-teaching: Robust Multi-scale Training for Time Series Classification with Noisy Labels",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) have been criticized because they easily overfit noisy (incorrect) labels. To improve the robustness of DNNs, existing methods for image data regard samples with small training losses as correctly labeled data (small-loss criterion). Nevertheless, time series' discriminative patterns are easily distorted by external noises (i.e., frequency perturbations) during the recording process. This results in training losses of some time series samples that do not meet the small-loss criterion. Therefore, this paper proposes a deep learning paradigm called Scale-teaching to cope with time series noisy labels. Specifically, we design a fine-to-coarse cross-scale fusion mechanism for learning discriminative patterns by utilizing time series at different scales to train multiple DNNs simultaneously. Meanwhile, each network is trained in a cross-teaching manner by using complementary information from different scales to select small-loss samples as clean labels. For unselected large-loss samples, we introduce multi-scale embedding graph learning via label propagation to correct their labels by using selected clean samples. Experiments on multiple benchmark time series datasets demonstrate the superiority of the proposed Scale-teaching paradigm over state-of-the-art methods in terms of effectiveness and robustness",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Liu",
      "ma peitian",
      "Dongliang Chen",
      "Wenbin Pei",
      "Qianli Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a7c2a320f5f36bb98f8eb878c6f1180-Abstract-Conference.html": {
    "title": "VOCE: Variational Optimization with Conservative Estimation for Offline Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline safe reinforcement learning (RL) algorithms promise to learn policies that satisfy safety constraints directly in offline datasets without interacting with the environment. This arrangement is particularly important in scenarios with high sampling costs and potential dangers, such as autonomous driving and robotics. However, the influence of safety constraints and out-of-distribution (OOD) actions have made it challenging for previous methods to achieve high reward returns while ensuring safety. In this work, we propose a Variational Optimization with Conservative Eestimation algorithm (VOCE) to solve the problem of optimizing safety policies in the offline dataset. Concretely, we reframe the problem of offline safe RL using probabilistic inference, which introduces variational distributions to make the optimization of policies more flexible. Subsequently, we utilize pessimistic estimation methods to estimate the Q-value of cost and reward, which mitigates the extrapolation errors induced by OOD actions. Finally, extensive experiments demonstrate that the VOCE algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Guan",
      "Guang Chen",
      "Jiaming Ji",
      "Long Yang",
      "ao zhou",
      "Zhijun Li",
      "changjun jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ac807c9b296964409b277369e55621a-Abstract-Conference.html": {
    "title": "Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis",
    "volume": "main",
    "abstract": "How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits",
    "keywords": [],
    "checked": true,
    "id": "b437d4398b443234aa253156404e12326ba899a5",
    "semantic_title": "beyond geometry: comparing the temporal structure of computation in neural circuits with dynamical similarity analysis",
    "citation_count": 3,
    "authors": [
      "Mitchell Ostrow",
      "Adam Eisen",
      "Leo Kozachkov",
      "Ila Fiete"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ad5d39b10e37915d7dfda2893d8e924-Abstract-Conference.html": {
    "title": "H-nobs: Achieving Certified Fairness and Robustness in Distributed Learning on Heterogeneous Datasets",
    "volume": "main",
    "abstract": "Fairness and robustness are two important goals in the design of modern distributed learning systems. Despite a few prior works attempting to achieve both fairness and robustness, some key aspects of this direction remain underexplored. In this paper, we try to answer three largely unnoticed and unaddressed questions that are of paramount significance to this topic: (i) What makes jointly satisfying fairness and robustness difficult? (ii) Is it possible to establish theoretical guarantee for the dual property of fairness and robustness? (iii) How much does fairness have to sacrifice at the expense of robustness being incorporated into the system? To address these questions, we first identify data heterogeneity as the key difficulty of combining fairness and robustness. Accordingly, we propose a fair and robust framework called H-nobs which can offer certified fairness and robustness through the adoption of two key components, a fairness-promoting objective function and a simple robust aggregation scheme called norm-based screening (NBS). We explain in detail why NBS is the suitable scheme in our algorithm in contrast to other robust aggregation measures. In addition, we derive three convergence theorems for H-nobs in cases of the learning model being nonconvex, convex, and strongly convex respectively, which provide theoretical guarantees for both fairness and robustness. Further, we empirically investigate the influence of the robust mechanism (NBS) on the fairness performance of H-nobs, the very first attempt of such exploration",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanqiang Zhou",
      "Ping Xu",
      "Yue Wang",
      "Zhi Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ae7df1f40f5faeda474b36b61197822-Abstract-Conference.html": {
    "title": "A Randomized Approach to Tight Privacy Accounting",
    "volume": "main",
    "abstract": "Bounding privacy leakage over compositions, i.e., privacy accounting, is a key challenge in differential privacy (DP). However, the privacy parameter ($\\varepsilon$ or $\\delta$) is often easy to estimate but hard to bound. In this paper, we propose a new differential privacy paradigm called estimate-verify-release (EVR), which tackles the challenges of providing a strict upper bound for the privacy parameter in DP compositions by converting an *estimate* of privacy parameter into a formal guarantee. The EVR paradigm first verifies whether the mechanism meets the *estimated* privacy guarantee, and then releases the query output based on the verification result. The core component of the EVR is privacy verification. We develop a randomized privacy verifier using Monte Carlo (MC) technique. Furthermore, we propose an MC-based DP accountant that outperforms existing DP accounting techniques in terms of accuracy and efficiency. MC-based DP verifier and accountant is applicable to an important and commonly used class of DP algorithms, including the famous DP-SGD. An empirical evaluation shows the proposed EVR paradigm improves the utility-privacy tradeoff for privacy-preserving machine learning",
    "keywords": [],
    "checked": false,
    "id": "e231843966472fa60117403f49f2ac5f252ed1d9",
    "semantic_title": "a randomized approach for tight privacy accounting",
    "citation_count": 3,
    "authors": [
      "Jiachen (Tianhao) Wang",
      "Saeed Mahloujifar",
      "Tong Wu",
      "Ruoxi Jia",
      "Prateek Mittal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6af779991368999ab3da0d366c208fba-Abstract-Conference.html": {
    "title": "Triple Eagle: Simple, Fast and Practical Budget-Feasible Mechanisms",
    "volume": "main",
    "abstract": "We revisit the classical problem of designing Budget-Feasible Mechanisms (BFMs) for submodular valuation functions, which has been extensively studied since the seminal paper of Singer [FOCS'10] due to its wide applications in crowdsourcing and social marketing. We propose TripleEagle, a novel algorithmic framework for designing BFMs, based on which we present several simple yet effective BFMs that achieve better approximation ratios than the state-of-the-art work. Moreover, our BFMs are the first in the literature to achieve linear complexities while ensuring obvious strategyproofness, making them more practical than the previous BFMs. We conduct extensive experiments to evaluate the empirical performance of our BFMs, and the experimental results strongly demonstrate the efficiency and effectiveness of our approach",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Han",
      "You Wu",
      "He Huang",
      "Shuang Cui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b055b95d689b1f704d8f92191cdb788-Abstract-Conference.html": {
    "title": "VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs",
    "keywords": [],
    "checked": true,
    "id": "5ac5cf1e0c2186c43c067ca04a24d80d6194a56a",
    "semantic_title": "villandiffusion: a unified backdoor attack framework for diffusion models",
    "citation_count": 6,
    "authors": [
      "Sheng-Yen Chou",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b1d4c03391b0aa6ddde0b807a78c950-Abstract-Conference.html": {
    "title": "An Information Theory Perspective on Variance-Invariance-Covariance Regularization",
    "volume": "main",
    "abstract": "Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised learning (SSL) method that has shown promising results on a variety of tasks. However, the fundamental mechanisms underlying VICReg remain unexplored. In this paper, we present an information-theoretic perspective on the VICReg objective. We begin by deriving information-theoretic quantities for deterministic networks as an alternative to unrealistic stochastic network assumptions. We then relate the optimization of the VICReg objective to mutual information optimization, highlighting underlying assumptions and facilitating a constructive comparison with other SSL algorithms and derive a generalization bound for VICReg, revealing its inherent advantages for downstream tasks. Building on these results, we introduce a family of SSL methods derived from information-theoretic principles that outperform existing SSL techniques",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ravid Shwartz-Ziv",
      "Randall Balestriero",
      "Kenji Kawaguchi",
      "Tim G. J. Rudner",
      "Yann LeCun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b241c515433caae3051266668d808b7-Abstract-Conference.html": {
    "title": "Learning and processing the ordinal information of temporal sequences in recurrent neural circuits",
    "volume": "main",
    "abstract": "Temporal sequence processing is fundamental in brain cognitive functions. Experimental data has indicated that the representations of ordinal information and contents of temporal sequences are disentangled in the brain, but the neural mechanism underlying this disentanglement remains largely unclear. Here, we investigate how recurrent neural circuits learn to represent the abstract order structure of temporal sequences, and how this disentangled representation of order structure from that of contents facilitates the processing of temporal sequences. We show that with an appropriate learn protocol, a recurrent neural circuit can learn a set of tree-structured attractor states to encode the corresponding tree-structured orders of given temporal sequences. This abstract temporal order template can then be bound with different contents, allowing for flexible and robust temporal sequence processing. Using a transfer learning task, we demonstrate that the reuse of a temporal order template facilitates the acquisition of new temporal sequences of the same or similar ordinal structure. Using a key-word spotting task, we demonstrate that the attractor representation of order structure improves the robustness of temporal sequence discrimination, if the ordinal information is the key to differentiate different sequences. We hope this study gives us insights into the neural mechanism of representing the ordinal information of temporal sequences in the brain, and helps us to develop brain-inspired temporal sequence processing algorithms",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "xiaolong zou",
      "Zhikun Chu",
      "Qinghai Guo",
      "Jie Cheng",
      "Bo Ho",
      "Si Wu",
      "Yuanyuan Mi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b44765c9201730a27f7931afb4d7434-Abstract-Conference.html": {
    "title": "UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures",
    "volume": "main",
    "abstract": "In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\underline{u}$nsupervised $\\underline{n}$eural $\\underline{s}$peech $\\underline{s}$eparation by leveraging $\\underline{o}$ver-determined training mixtu$\\underline{r}$es. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR",
    "keywords": [],
    "checked": true,
    "id": "25c399a231364f4a77d1dc4b59927585e63f5f11",
    "semantic_title": "unssor: unsupervised neural speech separation by leveraging over-determined training mixtures",
    "citation_count": 2,
    "authors": [
      "Zhong-Qiu Wang",
      "Shinji Watanabe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b555e8552240d6dfe0767146c9ebf36-Abstract-Conference.html": {
    "title": "Improving Self-supervised Molecular Representation Learning using Persistent Homology",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has great potential for molecular representation learning given the complexity of molecular graphs, the large amounts of unlabelled data available, the considerable cost of obtaining labels experimentally, and the hence often only small training datasets. The importance of the topic is reflected in the variety of paradigms and architectures that have been investigated recently. Yet the differences in performance seem often minor and are barely understood to date. In this paper, we study SSL based on persistent homology (PH), a mathematical tool for modeling topological features of data that persist across multiple scales. It has several unique features which particularly suit SSL, naturally offering: different views of the data, stability in terms of distance preservation, and the opportunity to flexibly incorporate domain knowledge.We (1) investigate an autoencoder, which shows the general representational power of PH, and (2) propose a contrastive loss that complements existing approaches. We rigorously evaluate our approach for molecular property prediction and demonstrate its particular features in improving the embedding space:after SSL, the representations are better and offer considerably more predictive power than the baselines over different probing tasks; our loss increases baseline performance, sometimes largely; and we often obtain substantial improvements over very small datasets, a common scenario in practice",
    "keywords": [],
    "checked": true,
    "id": "1a529ea63606f0507832b99a5a26f09d68e9d69c",
    "semantic_title": "improving self-supervised molecular representation learning using persistent homology",
    "citation_count": 0,
    "authors": [
      "Yuankai Luo",
      "Lei Shi",
      "Veronika Thost"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b61c278e483954fee502b49fe71cd14-Abstract-Conference.html": {
    "title": "Characteristic Circuits",
    "volume": "main",
    "abstract": "In many real-world scenarios it is crucial to be able to reliably and efficiently reason under uncertainty while capturing complex relationships in data. Probabilistic circuits (PCs), a prominent family of tractable probabilistic models, offer a remedy to this challenge by composing simple, tractable distributions into a high-dimensional probability distribution. However, learning PCs on heterogeneous data is challenging and densities of some parametric distributions are not available in closed form, limiting their potential use. We introduce characteristic circuits (CCs), a family of tractable probabilistic models providing a unified formalization of distributions over heterogeneous data in the spectral domain. The one-to-one relationship between characteristic functions and probability measures enables us to learn high-dimensional distributions on heterogeneous data domains and facilitates efficient probabilistic inference even when no closed-form density function is available. We show that the structure and parameters of CCs can be learned efficiently from the data and find that CCs outperform state-of-the-art density estimators for heterogeneous data domains on common benchmark data sets",
    "keywords": [],
    "checked": true,
    "id": "4ba9784d73dfe8d2eb3f1126138d3815f8341121",
    "semantic_title": "characteristic circuits",
    "citation_count": 0,
    "authors": [
      "Zhongjie Yu",
      "Martin Trapp",
      "Kristian Kersting"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b7676588c33d344485eeba1b5653ab1-Abstract-Conference.html": {
    "title": "Posterior Contraction Rates for Matérn Gaussian Processes on Riemannian Manifolds",
    "volume": "main",
    "abstract": "Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into $\\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Matérn Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of examples, which, mirroring prior work, show that intrinsic processes can achieve better performance in practice. Therefore, our work shows that finer-grained analyses are needed to distinguish between different levels of data-efficiency of geometric Gaussian processes, particularly in settings which involve small data set sizes and non-asymptotic behavior",
    "keywords": [],
    "checked": true,
    "id": "154e5cc26ae0dc82264d6ece09748fe3f76b0513",
    "semantic_title": "posterior contraction rates for matérn gaussian processes on riemannian manifolds",
    "citation_count": 1,
    "authors": [
      "Paul Rosa",
      "Slava Borovitskiy",
      "Alexander Terenin",
      "Judith Rousseau"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b7e1e96243c9edc378f85e7d232e415-Abstract-Conference.html": {
    "title": "Causal Context Connects Counterfactual Fairness to Robust Prediction and Group Fairness",
    "volume": "main",
    "abstract": "Counterfactual fairness requires that a person would have been classified in the same way by an AI or other algorithmic system if they had a different protected class, such as a different race or gender. This is an intuitive standard, as reflected in the U.S. legal system, but its use is limited because counterfactuals cannot be directly observed in real-world data. On the other hand, group fairness metrics (e.g., demographic parity or equalized odds) are less intuitive but more readily observed. In this paper, we use \\textit{causal context} to bridge the gaps between counterfactual fairness, robust prediction, and group fairness. First, we motivate counterfactual fairness by showing that there is not necessarily a fundamental trade-off between fairness and accuracy because, under plausible conditions, the counterfactually fair predictor is in fact accuracy-optimal in an unbiased target distribution. Second, we develop a correspondence between the causal graph of the data-generating process and which, if any, group fairness metrics are equivalent to counterfactual fairness. Third, we show that in three common fairness contexts—measurement error, selection on label, and selection on predictors—counterfactual fairness is equivalent to demographic parity, equalized odds, and calibration, respectively. Counterfactual fairness can sometimes be tested by measuring relatively simple group fairness metrics",
    "keywords": [],
    "checked": true,
    "id": "e204c4a15d1d06fabb760f41fa6603bbe64a2714",
    "semantic_title": "causal context connects counterfactual fairness to robust prediction and group fairness",
    "citation_count": 1,
    "authors": [
      "Jacy Anthis",
      "Victor Veitch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b8c6f846c3575e1d1ad496abea28826-Abstract-Conference.html": {
    "title": "Banana: Banach Fixed-Point Network for Pointcloud Segmentation with Inter-Part Equivariance",
    "volume": "main",
    "abstract": "Equivariance has gained strong interest as a desirable network property that inherently ensures robust generalization. However, when dealing with complex systems such as articulated objects or multi-object scenes, effectively capturing inter-part transformations poses a challenge, as it becomes entangled with the overall structure and local transformations. The interdependence of part assignment and per-part group action necessitates a novel equivariance formulation that allows for their co-evolution. In this paper, we present Banana, a Banach fixed-point network for equivariant segmentation with inter-part equivariance by construction. Our key insight is to iteratively solve a fixed-point problem, where point-part assignment labels and per-part SE(3)-equivariance co-evolve simultaneously. We provide theoretical derivations of both per-step equivariance and global convergence, which induces an equivariant final convergent state. Our formulation naturally provides a strict definition of inter-part equivariance that generalizes to unseen inter-part configurations. Through experiments conducted on both articulated objects and multi-object scans, we demonstrate the efficacy of our approach in achieving strong generalization under inter-part transformations, even when confronted with substantial changes in pointcloud geometry and topology",
    "keywords": [],
    "checked": true,
    "id": "6c1064cbb45259732ef8032b105f5121a67d27ef",
    "semantic_title": "banana: banach fixed-point network for pointcloud segmentation with inter-part equivariance",
    "citation_count": 6,
    "authors": [
      "Congyue Deng",
      "Jiahui Lei",
      "William B Shen",
      "Kostas Daniilidis",
      "Leonidas J. Guibas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Abstract-Conference.html": {
    "title": "Describe, Explain, Plan and Select: Interactive Planning with LLMs Enables Open-World Multi-Task Agents",
    "volume": "main",
    "abstract": "In this paper, we study the problem of planning in Minecraft, a popular, democratized yet challenging open-ended environment for developing multi-task embodied agents. We've found two primary challenges of empowering such agents with planning: 1) planning in an open-ended world like Minecraft requires precise and multi-step reasoning due to the long-term nature of the tasks, and 2) as vanilla planners do not consider the achievability of the current agent when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient. To this end, we propose ``$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect'' ($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). Our approach helps with better error correction from the feedback during the long-haul planning, while also bringing the sense of proximity via goal $\\textbf{Selector}$, a learnable module that ranks parallel sub-goals based on the estimated steps of completion and improves the original plan accordingly. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach",
    "keywords": [],
    "checked": false,
    "id": "0935ce0adad57e1b24c50d793d46a407c3f563f3",
    "semantic_title": "describe, explain, plan and select: interactive planning with large language models enables open-world multi-task agents",
    "citation_count": 134,
    "authors": [
      "Zihao Wang",
      "Shaofei Cai",
      "Guanzhou Chen",
      "Anji Liu",
      "Xiaojian (Shawn) Ma",
      "Yitao Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b97236d90d945be7c58268207a14f4f-Abstract-Conference.html": {
    "title": "Partial Label Learning with Dissimilarity Propagation guided Candidate Label Shrinkage",
    "volume": "main",
    "abstract": "In partial label learning (PLL), each sample is associated with a group of candidate labels, among which only one label is correct. The key of PLL is to disambiguate the candidate label set to find the ground-truth label. To this end, we first construct a constrained regression model to capture the confidence of the candidate labels, and multiply the label confidence matrix by its transpose to build a second-order similarity matrix, whose elements indicate the pairwise similarity relationships of samples globally. Then we develop a semantic dissimilarity matrix by considering the complement of the intersection of the candidate label set, and further propagate the initial dissimilarity relationships to the whole data set by leveraging the local geometric structure of samples. The similarity and dissimilarity matrices form an adversarial relationship, which is further utilized to shrink the solution space of the label confidence matrix and promote the dissimilarity matrix. We finally extend the proposed model to a kernel version to exploit the non-linear structure of samples and solve the proposed model by the inexact augmented Lagrange multiplier method. By exploiting the adversarial prior, the proposed method can significantly outperformstate-of-the-art PLL algorithms when evaluated on 10 artificial and 7 real-world partial label data sets. We also prove the effectiveness of our method with some theoretical guarantees. The code is publicly available at https://github.com/Yangfc-ML/DPCLS",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Jia",
      "Fuchao Yang",
      "Yongqiang Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6b9aa8f418bde2840d5f4ab7a02f663b-Abstract-Conference.html": {
    "title": "Data Selection for Language Models via Importance Resampling",
    "volume": "main",
    "abstract": "Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given some unlabeled target samples. Due to the large scale and dimensionality of the raw text data, existing methods use simple heuristics or use experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. To determine an appropriate feature space, we show that KL reduction, a data metric that measures the proximity between selected pretraining data and the target in a feature space, has high correlation with average downstream accuracy (r=0.89) when computed with simple n-gram features. This motivates our instantiation of DSIR using n-gram features. When performing continued pretraining towards a specific domain, DSIR performs comparably to expert curation across 8 target distributions. When pretraining general-domain models (target is Wikipedia + books), DSIR improves over random selection and heuristic filtering baselines by 2-2.5% on the GLUE benchmark",
    "keywords": [],
    "checked": true,
    "id": "74013b7cfa0fc524803350fca51341004565eb22",
    "semantic_title": "data selection for language models via importance resampling",
    "citation_count": 37,
    "authors": [
      "Sang Michael Xie",
      "Shibani Santurkar",
      "Tengyu Ma",
      "Percy S. Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ba85c6f1c7656a6a647bc4d63b90bf0-Abstract-Conference.html": {
    "title": "Video Dynamics Prior: An Internal Learning Approach for Robust Video Enhancements",
    "volume": "main",
    "abstract": "In this paper, we present a novel robust framework for low-level vision tasks, including denoising, object removal, frame interpolation, and super-resolution, that does not require any external training data corpus. Our proposed approach directly learns the weights of neural modules by optimizing over the corrupted test sequence, leveraging the spatio-temporal coherence and internal statistics of videos. Furthermore, we introduce a novel spatial pyramid loss that leverages the property of spatio-temporal patch recurrence in a video across the different scales of the video. This loss enhances robustness to unstructured noise in both the spatial and temporal domains. This further results in our framework being highly robust to degradation in input frames and yields state-of-the-art results on downstream tasks such as denoising, object removal, and frame interpolation. To validate the effectiveness of our approach, we conduct qualitative and quantitative evaluations on standard video datasets such as DAVIS, UCF-101, and VIMEO90K-T",
    "keywords": [],
    "checked": true,
    "id": "1fdf712a08b595fa2e81fddfcc696bbe53772ce2",
    "semantic_title": "video dynamics prior: an internal learning approach for robust video enhancements",
    "citation_count": 1,
    "authors": [
      "Gaurav Shrivastava",
      "Ser Nam Lim",
      "Abhinav Shrivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6baec7c4ba0a8734ccbd528a8090cb1f-Abstract-Conference.html": {
    "title": "Glance and Focus: Memory Prompting for Multi-Event Video Question Answering",
    "volume": "main",
    "abstract": "Video Question Answering (VideoQA) has emerged as a vital tool to evaluate agents' ability to understand human daily behaviors. Despite the recent success of large vision language models in many multi-modal tasks, complex situation reasoning over videos involving multiple human-object interaction events still remains challenging. In contrast, humans can easily tackle it by using a series of episode memories as anchors to quickly locate question-related key moments for reasoning. To mimic this effective reasoning strategy, we propose the Glance- Focus model. One simple way is to apply an action detection model to predict a set of actions as key memories. However, these actions within a closed set vocabulary are hard to generalize to various video domains. Instead of that, we train an Encoder-Decoder to generate a set of dynamic event memories at the glancing stage. Apart from using supervised bipartite matching to obtain the event memories, we further design an unsupervised memory generation method to get rid of dependence on event annotations. Next, at the focusing stage, these event memories act as a bridge to establish the correlation between the questions with high-level event concepts and low-level lengthy video content. Given the question, the model first focuses on the generated key event memory, then focuses on the most relevant moment for reasoning through our designed multi-level cross- attention mechanism. We conduct extensive experiments on four Multi-Event VideoQA benchmarks including STAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves state-of-the-art results, surpassing current large models in various challenging reasoning tasks. The code and models are available at https://github.com/ByZ0e/Glance-Focus",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Bai",
      "Ruiping Wang",
      "Xilin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6bbda0824bcc20749f21510fd8b28de5-Abstract-Conference.html": {
    "title": "Learning To Dive In Branch And Bound",
    "volume": "main",
    "abstract": "Primal heuristics are important for solving mixed integer linear programs, because they find feasible solutions that facilitate branch and bound search. A prominent group of primal heuristics are diving heuristics. They iteratively modify and resolve linear programs to conduct a depth-first search from any node in the search tree. Existing divers rely on generic decision rules that fail to exploit structural commonality between similar problem instances that often arise in practice. Therefore, we propose L2Dive to learn specific diving heuristics with graph neural networks: We train generative models to predict variable assignments and leverage the duality of linear programs to make diving decisions based on the model's predictions. L2Dive is fully integrated into the open-source solver SCIP. We find that L2Dive outperforms standard divers to find better feasible solutions on a range of combinatorial optimization problems. For real-world applications from server load balancing and neural network verification, L2Dive improves the primal-dual integral by up to 7% (35%) on average over a tuned (default) solver baseline and reduces average solving time by 20% (29%)",
    "keywords": [],
    "checked": true,
    "id": "6441b0ef0588eb5d90a3cc88423ee364f982ba06",
    "semantic_title": "learning to dive in branch and bound",
    "citation_count": 1,
    "authors": [
      "Max Paulus",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c0ff499edc529c7d8c9f05c7c0ccb82-Abstract-Conference.html": {
    "title": "Intriguing Properties of Quantization at Scale",
    "volume": "main",
    "abstract": "Emergent properties have been widely adopted as a term to describe behavior not present in smaller models but observed in larger models (Wei et al., 2022a). Recent work suggests that the trade-off incurred by quantization is also an emergent property, with sharp drops in performance in models over 6B parameters. In this work, we ask are quantization cliffs in performance solely a factor of scale? Against a backdrop of increased research focus on why certain emergent properties surface at scale, this work provides a useful counter-example. We posit that it is possible to optimize for a quantization friendly training recipe that suppresses large activation magnitude outliers. Here, we find that outlier dimensions are not an inherent product of scale, but rather sensitive to the optimization conditions present during pre-training. This both opens up directions for more efficient quantization, and poses the question of whether other emergent properties are inherent or can be altered and conditioned by optimization and architecture design choices. We successfully quantize models ranging in size from 410M to 52B with minimal degradation in performance",
    "keywords": [],
    "checked": true,
    "id": "17dfa45f14fcc1b861bc06b7f0b4678d870628d9",
    "semantic_title": "intriguing properties of quantization at scale",
    "citation_count": 10,
    "authors": [
      "Arash Ahmadian",
      "Saurabh Dash",
      "Hongyu Chen",
      "Bharat Venkitesh",
      "Zhen Stephen Gou",
      "Phil Blunsom",
      "Ahmet Üstün",
      "Sara Hooker"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c33e4ea4ddfb05a78541022ab5a1fb9-Abstract-Conference.html": {
    "title": "Self-supervised Graph Neural Networks via Low-Rank Decomposition",
    "volume": "main",
    "abstract": "Self-supervised learning is introduced to train graph neural networks (GNNs) by employing propagation-based GNNs designed for semi-supervised learning tasks. Unfortunately, this common choice tends to cause two serious issues. Firstly, global parameters cause the model lack the ability to capture the local property. Secondly, it is difficult to handle networks beyond homophily without label information.This paper tends to break through the common choice of employing propagation-based GNNs, which aggregate representations of nodes belonging to different classes and tend to lose discriminative information. If the propagation in each ego-network is just between the nodes from the same class, the obtained representation matrix should follow the low-rank characteristic. To meet this requirement, this paper proposes the Low-Rank Decomposition-based GNNs (LRD-GNN-Matrix) by employing Low-Rank Decomposition to the attribute matrix. Furthermore, to incorporate long-distance information, Low-Rank Tensor Decomposition-based GNN (LRD-GNN-Tensor) is proposed by constructing the node attribute tensor from selected similar ego-networks and performing Low-Rank Tensor Decomposition. The employed tensor nuclear norm facilitates the capture of the long-distance relationship between original and selected similar ego-networks. Extensive experiments demonstrate the superior performance and the robustness of LRD-GNNs",
    "keywords": [],
    "checked": true,
    "id": "6d35b28af908c1f6e2c9de2f30e12a9573480ddc",
    "semantic_title": "self-supervised graph neural networks via low-rank decomposition",
    "citation_count": 0,
    "authors": [
      "Liang Yang",
      "Runjie Shi",
      "Qiuliang Zhang",
      "bingxin niu",
      "Zhen Wang",
      "Xiaochun Cao",
      "Chuan Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c473e69ba261200dd595d07494c1a73-Abstract-Conference.html": {
    "title": "Cookie Consent Has Disparate Impact on Estimation Accuracy",
    "volume": "main",
    "abstract": "Cookies are designed to enable more accurate identification and tracking of user behavior, in turn allowing for more personalized ads and better performing ad campaigns. Given the additional information that is recorded, questions related to privacy and fairness naturally arise. How does a user's consent decision influence how much the system can learn about their demographic and tastes? Is the impact of a user's consent decision on the recommender system's ability to learn about their latent attributes uniform across demographics? We investigate these questions in the context of an engagement-driven recommender system using simulation. We empirically demonstrate that when consent rates exhibit demographic-dependence, user consent has a disparate impact on the recommender agent's ability to estimate users' latent attributes. In particular, we find that when consent rates are demographic-dependent, a user disagreeing to share their cookie may counter-intuitively cause the recommender agent to know more about the user than if the user agreed to share their cookie. Furthermore, the gap in base consent rates across demographics serves as an amplifier: users from the lower consent rate demographic who agree to cookie sharing generally experience higher estimation errors than the same users from the higher consent rate demographic, and conversely for users who choose to disagree to cookie sharing, with these differences increasing in consent rate gap. We discuss the need for new notions of fairness that encourage consistency between a user's privacy decisions and the system's ability to estimate their latent attributes",
    "keywords": [],
    "checked": true,
    "id": "19650e5574016e69105d08e213fb45c3e3a3be8c",
    "semantic_title": "cookie consent has disparate impact on estimation accuracy",
    "citation_count": 0,
    "authors": [
      "Erik Miehling",
      "Rahul Nair",
      "Elizabeth Daly",
      "Karthikeyan Natesan Ramamurthy",
      "Robert Redmond"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c4a1a3cbe70ef36d7d6332166bba77d-Abstract-Conference.html": {
    "title": "NPCL: Neural Processes for Uncertainty-Aware Continual Learning",
    "volume": "main",
    "abstract": "Continual learning (CL) aims to train deep neural networks efficiently on streaming data while limiting the forgetting caused by new tasks. However, learning transferable knowledge with less interference between tasks is difficult, and real-world deployment of CL models is limited by their inability to measure predictive uncertainties. To address these issues, we propose handling CL tasks with neural processes (NPs), a class of meta-learners that encode different tasks into probabilistic distributions over functions all while providing reliable uncertainty estimates. Specifically, we propose an NP-based CL approach (NPCL) with task-specific modules arranged in a hierarchical latent variable model. We tailor regularizers on the learned latent distributions to alleviate forgetting. The uncertainty estimation capabilities of the NPCL can also be used to handle the task head/module inference challenge in CL. Our experiments show that the NPCL outperforms previous CL approaches. We validate the effectiveness of uncertainty estimation in the NPCL for identifying novel data and evaluating instance-level model confidence. Code is available at https://github.com/srvCodes/NPCL",
    "keywords": [],
    "checked": true,
    "id": "f20140ed835172a476810c479fb3ed368d8e6c91",
    "semantic_title": "npcl: neural processes for uncertainty-aware continual learning",
    "citation_count": 2,
    "authors": [
      "Saurav Jha",
      "Dong Gong",
      "He Zhao",
      "Lina Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c52a8a4fadc9129c6e1d1745f2dfd0f-Abstract-Conference.html": {
    "title": "From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces",
    "volume": "main",
    "abstract": "Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces. This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use — via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks",
    "keywords": [],
    "checked": true,
    "id": "ee7020fc413590878dca60dcf41896bbe6a6c628",
    "semantic_title": "from pixels to ui actions: learning to follow instructions via graphical user interfaces",
    "citation_count": 13,
    "authors": [
      "Peter Shaw",
      "Mandar Joshi",
      "James Cohan",
      "Jonathan Berant",
      "Panupong Pasupat",
      "Hexiang Hu",
      "Urvashi Khandelwal",
      "Kenton Lee",
      "Kristina N Toutanova"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c59ace4fc4872a14df13d91762ad4f0-Abstract-Conference.html": {
    "title": "Robust Mean Estimation Without Moments for Symmetric Distributions",
    "volume": "main",
    "abstract": "We study the problem of robustly estimating the mean or location parameter without moment assumptions.Known computationally efficient algorithms rely on strong distributional assumptions, such as sub-Gaussianity, or (certifiably) bounded moments.Moreover, the guarantees that they achieve in the heavy-tailed setting are weaker than those for sub-Gaussian distributions with known covariance.In this work, we show that such a tradeoff, between error guarantees and heavy-tails, is not necessary for symmetric distributions.We show that for a large class of symmetric distributions, the same error as in the Gaussian setting can be achieved efficiently.The distributions we study include products of arbitrary symmetric one-dimensional distributions, such as product Cauchy distributions, as well as elliptical distributions, a vast generalization of the Gaussian distribution.For product distributions and elliptical distributions with known scatter (covariance) matrix, we show that given an $\\varepsilon$-corrupted sample, we can with probability at least $1-\\delta$ estimate its location up to error $O(\\varepsilon \\sqrt{\\log(1/\\varepsilon)})$ using $\\tfrac{d\\log(d) + \\log(1/\\delta)}{\\varepsilon^2 \\log(1/\\varepsilon)}$ samples.This result matches the best-known guarantees for the Gaussian distribution and known SQ lower bounds (up to the $\\log(d)$ factor).For elliptical distributions with unknown scatter (covariance) matrix, we propose a sequence of efficient algorithms that approaches this optimal error.Specifically, for every $k \\in \\mathbb{N}$, we design an estimator using time and samples $\\tilde{O}({d^k})$ achieving error $O(\\varepsilon^{1-\\frac{1}{2k}})$.This matches the error and running time guarantees when assuming certifiably bounded moments of order up to $k$.For unknown covariance, such error bounds of $o(\\sqrt{\\varepsilon})$ are not even known for (general) sub-Gaussian distributions.Our algorithms are based on a generalization of the well-known filtering technique [DK22].More specifically, we show how this machinery can be combined with Huber-loss-based techniques to work with projections of the noise that behave more nicely than the initial noise.Moreover, we show how sum-of-squares proofs can be used to obtain algorithmic guarantees even for distributions without a first moment.We believe that this approach may find other applications in future works",
    "keywords": [],
    "checked": true,
    "id": "4a8db7531a405faa5bd29301ca6993469a55a25d",
    "semantic_title": "robust mean estimation without moments for symmetric distributions",
    "citation_count": 0,
    "authors": [
      "Gleb Novikov",
      "David Steurer",
      "Stefan Tiegel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c5b82193c5d8e6aa5806239676ddc97-Abstract-Conference.html": {
    "title": "Fast and Simple Spectral Clustering in Theory and Practice",
    "volume": "main",
    "abstract": "Spectral clustering is a popular and effective algorithm designed to find $k$ clusters in a graph $G$.In the classical spectral clustering algorithm, the vertices of $G$ are embedded into $\\mathbb{R}^k$ using $k$ eigenvectors of the graph Laplacian matrix.However, computing this embedding is computationally expensive and dominates the running time of the algorithm.In this paper, we present a simple spectral clustering algorithm based on a vertex embedding with $O(\\log(k))$ vectors computed by the power method.The vertex embedding is computed in nearly-linear time with respect to the size of the graph, andthe algorithm provably recovers the ground truth clusters under natural assumptions on the input graph.We evaluate the new algorithm on several synthetic and real-world datasets, finding that it is significantly faster than alternative clustering algorithms,while producing results with approximately the same clustering accuracy",
    "keywords": [],
    "checked": true,
    "id": "8625c3a2c6f7015cb79cd739da170aca712323da",
    "semantic_title": "fast and simple spectral clustering in theory and practice",
    "citation_count": 0,
    "authors": [
      "Peter Macgregor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c7154e394e24c69409256ccf8bf0804-Abstract-Conference.html": {
    "title": "CL-NeRF: Continual Learning of Neural Radiance Fields for Evolving Scene Representation",
    "volume": "main",
    "abstract": "Existing methods for adapting Neural Radiance Fields (NeRFs) to scene changes require extensive data capture and model retraining, which is both time-consuming and labor-intensive. In this paper, we tackle the challenge of efficiently adapting NeRFs to real-world scene changes over time using a few new images while retaining the memory of unaltered areas, focusing on the continual learning aspect of NeRFs. To this end, we propose CL-NeRF, which consists of two key components: a lightweight expert adaptor for adapting to new changes and evolving scene representations and a conflict-aware knowledge distillation learning objective for memorizing unchanged parts. We also present a new benchmark for evaluating Continual Learning of NeRFs with comprehensive metrics. Our extensive experiments demonstrate that CL-NeRF can synthesize high-quality novel views of both changed and unchanged regions with high training efficiency, surpassing existing methods in terms of reducing forgetting and adapting to changes. Code and benchmark will be made available",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuzhe Wu",
      "Peng Dai",
      "Weipeng DENG",
      "Handi Chen",
      "Yang Wu",
      "Yan-Pei Cao",
      "Ying Shan",
      "Xiaojuan Qi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c78ae0c1140902bf3a430b1725bcc4e-Abstract-Conference.html": {
    "title": "Generalised f-Mean Aggregation for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Network (GNN) architectures are defined by their implementations of update and aggregation modules. While many works focus on new ways to parametrise the update modules, the aggregation modules receive comparatively little attention. Because it is difficult to parametrise aggregation functions, currently most methods select a ``standard aggregator'' such as mean, sum, or max. While this selection is often made without any reasoning, it has been shown that the choice in aggregator has a significant impact on performance, and the best choice in aggregator is problem-dependent. Since aggregation is a lossy operation, it is crucial to select the most appropriate aggregator in order to minimise information loss. In this paper, we present GenAgg, a generalised aggregation operator, which parametrises a function space that includes all standard aggregators. In our experiments, we show that GenAgg is able to represent the standard aggregators with much higher accuracy than baseline methods. We also show that using GenAgg as a drop-in replacement for an existing aggregator in a GNN often leads to a significant boost in performance across various tasks",
    "keywords": [],
    "checked": true,
    "id": "92da627ceed3a344a180934fb783bc038e17c5ae",
    "semantic_title": "generalised f-mean aggregation for graph neural networks",
    "citation_count": 0,
    "authors": [
      "Ryan Kortvelesy",
      "Steven Morad",
      "Amanda Prorok"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c7ca1889f01a9b767c631686fb5fd24-Abstract-Conference.html": {
    "title": "Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization",
    "volume": "main",
    "abstract": "To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. In this paper, we propose a differentiable regularizer that is a lower bound on the distance of the data points to the classification boundary. The proposed regularizer requires knowledge of the model's Lipschitz constant along certain directions. To this end, we develop a scalable method for calculating guaranteed differentiable upper bounds on the Lipschitz constant of neural networks accurately and efficiently. The relative accuracy of the bounds prevents excessive regularization and allows for more direct manipulation of the decision boundary. Furthermore, our Lipschitz bounding algorithm exploits the monotonicity and Lipschitz continuity of the activation layers, and the resulting bounds can be used to design new layers with controllable bounds on their Lipschitz constant. Experiments on the MNIST, CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm obtains competitively improved results compared to the state-of-the-art",
    "keywords": [],
    "checked": true,
    "id": "d84e86788d688e47de6b33c7d52b418037ef9f05",
    "semantic_title": "certified robustness via dynamic margin maximization and improved lipschitz regularization",
    "citation_count": 0,
    "authors": [
      "Mahyar Fazlyab",
      "Taha Entesari",
      "Aniket Roy",
      "Rama Chellappa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6c8985579293e0209bdaa4f21bb1d237-Abstract-Conference.html": {
    "title": "Unpaired Multi-Domain Causal Representation Learning",
    "volume": "main",
    "abstract": "The goal of causal representation learning is to find a representation of data that consists of causally related latent variables. We consider a setup where one has access to data from multiple domains that potentially share a causal representation. Crucially, observations in different domains are assumed to be unpaired, that is, we only observe the marginal distribution in each domain but not their joint distribution. In this paper, we give sufficient conditions for identifiability of the joint distribution and the shared causal graph in a linear setup. Identifiability holds if we can uniquely recover the joint distribution and the shared causal representation from the marginal distributions in each domain. We transform our results into a practical method to recover the shared latent causal graph",
    "keywords": [],
    "checked": true,
    "id": "bd9fd8c7a6cc8aa3e1f4333454858e49e731cf3c",
    "semantic_title": "unpaired multi-domain causal representation learning",
    "citation_count": 4,
    "authors": [
      "Nils Sturma",
      "Chandler Squires",
      "Mathias Drton",
      "Caroline Uhler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ca5d2665de83394f437dad0c3746907-Abstract-Conference.html": {
    "title": "Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection",
    "volume": "main",
    "abstract": "Cooperatively utilizing both ego-vehicle and infrastructure sensor data can significantly enhance autonomous driving perception abilities. However, the uncertain temporal asynchrony and limited communication conditions that are present in traffic environments can lead to fusion misalignment and constrain the exploitation of infrastructure data. To address these issues in vehicle-infrastructure cooperative 3D (VIC3D) object detection, we propose the Feature Flow Net (FFNet), a novel cooperative detection framework. FFNet is a flow-based feature fusion framework that uses a feature flow prediction module to predict future features and compensate for asynchrony. Instead of transmitting feature maps extracted from still-images, FFNet transmits feature flow, leveraging the temporal coherence of sequential infrastructure frames. Furthermore, we introduce a self-supervised training approach that enables FFNet to generate feature flow with feature prediction ability from raw infrastructure sequences. Experimental results demonstrate that our proposed method outperforms existing cooperative detection methods while only requiring about 1/100 of the transmission cost of raw data and covers all latency in one model on the DAIR-V2X dataset. The code is available https://github.com/haibao-yu/FFNet-VIC3D",
    "keywords": [],
    "checked": true,
    "id": "b015c405a15b44f6edab3abad796813c67d67078",
    "semantic_title": "flow-based feature fusion for vehicle-infrastructure cooperative 3d object detection",
    "citation_count": 0,
    "authors": [
      "Haibao Yu",
      "Yingjuan Tang",
      "Enze Xie",
      "Jilei Mao",
      "Ping Luo",
      "Zaiqing Nie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6cb7246003d556c4d1cbf9c17c392ee3-Abstract-Conference.html": {
    "title": "Subspace Identification for Multi-Source Domain Adaptation",
    "volume": "main",
    "abstract": "Multi-source domain adaptation (MSDA) methods aim to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Although current methods achieve target joint distribution identifiability by enforcing minimal changes across domains, they often necessitate stringent conditions, such as an adequate number of domains, monotonic transformation of latent variables, and invariant label distributions. These requirements are challenging to satisfy in real-world applications. To mitigate the need for these strict assumptions, we propose a subspace identification theory that guarantees the disentanglement of domain-invariant and domain-specific variables under less restrictive constraints regarding domain numbers and transformation properties and thereby facilitating domain adaptation by minimizing the impact of domain shifts on invariant variables. Based on this theory, we develop a Subspace Identification Guarantee (SIG) model that leverages variational inference. Furthermore, the SIG model incorporates class-aware conditional alignment to accommodate target shifts where label distributions change with the domain. Experimental results demonstrate that our SIG model outperforms existing MSDA techniques on various benchmark datasets, highlighting its effectiveness in real-world applications",
    "keywords": [],
    "checked": true,
    "id": "7ab6c5a5f9d83d04a147ee152aff944a16eca6ce",
    "semantic_title": "subspace identification for multi-source domain adaptation",
    "citation_count": 2,
    "authors": [
      "Zijian Li",
      "Ruichu Cai",
      "Guangyi Chen",
      "Boyang Sun",
      "Zhifeng Hao",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6cbd0a1251f41b41aa68e728bcc1ee40-Abstract-Conference.html": {
    "title": "Optimistic Exploration in Reinforcement Learning Using Symbolic Model Estimates",
    "volume": "main",
    "abstract": "There has been an increasing interest in using symbolic models along with reinforcement learning (RL) problems, where these coarser abstract models are used as a way to provide RL agents with higher level guidance. However, most of these works are inherently limited by their assumption of having an access to a symbolic approximation of the underlying problem. To address this issue, we introduce a new method for learning optimistic symbolic approximations of the underlying world model. We will see how these representations, coupled with fast diverse planners developed by the automated planning community, provide us with a new paradigm for optimistic exploration in sparse reward settings. We investigate the possibility of speeding up the learning process by generalizing learned model dynamics across similar actions with minimal human input. Finally, we evaluate the method, by testing it on multiple benchmark domains and compare it with other RL strategies",
    "keywords": [],
    "checked": false,
    "id": "e66d55565f6cd6336f92be01b4efc7e6a1eb2381",
    "semantic_title": "optimistic active exploration of dynamical systems",
    "citation_count": 0,
    "authors": [
      "Sarath Sreedharan",
      "Michael Katz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6cc321baf0a8611b1d1bdbd18822667b-Abstract-Conference.html": {
    "title": "Feature learning via mean-field Langevin dynamics: classifying sparse parities and beyond",
    "volume": "main",
    "abstract": "Neural network in the mean-field regime is known to be capable of \\textit{feature learning}, unlike the kernel (NTK) counterpart. Recent works have shown that mean-field neural networks can be globally optimized by a noisy gradient descent update termed the \\textit{mean-field Langevin dynamics} (MFLD). However, all existing guarantees for MFLD only considered the \\textit{optimization} efficiency, and it is unclear if this algorithm leads to improved \\textit{generalization} performance and sample complexity due to the presence of feature learning. To fill this gap, in this work we study the statistical and computational complexity of MFLD in learning a class of binary classification problems. Unlike existing margin bounds for neural networks, we avoid the typical norm control by utilizing the perspective that MFLD optimizes the \\textit{distribution} of parameters rather than the parameter itself; this leads to an improved analysis of the sample complexity and convergence rate. We apply our general framework to the learning of $k$-sparse parity functions, where we prove that unlike kernel methods, two-layer neural networks optimized by MFLD achieves a sample complexity where the degree $k$ is ``decoupled'' from the exponent in the dimension dependence",
    "keywords": [],
    "checked": false,
    "id": "a3ebed9b1e75a528f8e943275bdb99ffe515ab2c",
    "semantic_title": "optimal mass transport meets thermodynamics: on power and efficiency of finite-time thermodynamic engines",
    "citation_count": 0,
    "authors": [
      "Taiji Suzuki",
      "Denny Wu",
      "Kazusato Oko",
      "Atsushi Nitanda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6cd3ac24cdb789beeaa9f7145670fcae-Abstract-Conference.html": {
    "title": "Improving Graph Matching with Positional Reconstruction Encoder-Decoder Network",
    "volume": "main",
    "abstract": "Deriving from image matching and understanding, semantic keypoint matching aims at establishing correspondence between keypoint sets in images. As graphs are powerful tools to represent points and their complex relationships, graph matching provides an effective way to find desired semantic keypoint correspondences. Recent deep graph matching methods have shown excellent performance, but there is still a lack of exploration and utilization of spatial information of keypoints as nodes in graphs. More specifically, existing methods are insufficient to capture the relative spatial relations through current graph construction approaches from the locations of semantic keypoints. To address these issues, we introduce a positional reconstruction encoder-decoder (PR-EnDec) to model intrinsic graph spatial structure, and present an end-to-end graph matching network PREGM based on PR-EnDec. Our PR-EnDec consists of a positional encoder that learns effective node spatial embedding with the affine transformation invariance, and a spatial relation decoder that further utilizes the high-order spatial information by reconstructing the locational structure of graphs contained in the node coordinates. Extensive experimental results on three public keypoint matching datasets demonstrate the effectiveness of our proposed PREGM",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiao Zhou",
      "Ruiqi Jia",
      "Hongxiang Lin",
      "Hefeng Quan",
      "Yumeng Zhao",
      "Xiaoqing Lyu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6cda6dae05ae5e42ea78be85d5a26f77-Abstract-Conference.html": {
    "title": "A Causal Framework for Decomposing Spurious Variations",
    "volume": "main",
    "abstract": "One of the fundamental challenges found throughout the data sciences is to explain why things happen in specific ways, or through which mechanisms a certain variable $X$ exerts influences over another variable $Y$. In statistics and machine learning, significant efforts have been put into developing machinery to estimate correlations across variables efficiently. In causal inference, a large body of literature is concerned with the decomposition of causal effects under the rubric of mediation analysis. However, many variations are spurious in nature, including different phenomena throughout the applied sciences. Despite the statistical power to estimate correlations and the identification power to decompose causal effects, there is still little understanding of the properties of spurious associations and how they can be decomposed in terms of the underlying causal mechanisms. In this manuscript, we develop formal tools for decomposing spurious variations in both Markovian and Semi-Markovian models. We prove the first results that allow a non-parametric decomposition of spurious effects and provide sufficient conditions for the identification of such decompositions. The described approach has several applications, ranging from explainable and fair AI to questions in epidemiology and medicine, and we empirically demonstrate its use on a real-world dataset",
    "keywords": [],
    "checked": true,
    "id": "46d1820bfd416887cdfef804a259d09e47188606",
    "semantic_title": "a causal framework for decomposing spurious variations",
    "citation_count": 0,
    "authors": [
      "Drago Plecko",
      "Elias Bareinboim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6cdb2cbb2083477cca5243843d6dad06-Abstract-Conference.html": {
    "title": "Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification",
    "volume": "main",
    "abstract": "Meta-learning has demonstrated promising results in few-shot classification (FSC) by learning to solve new problems using prior knowledge. Bayesian methods are effective at characterizing uncertainty in FSC, which is crucial in high-risk fields. In this context, the logistic-softmax likelihood is often employed as an alternative to the softmax likelihood in multi-class Gaussian process classification due to its conditional conjugacy property. However, the theoretical property of logistic-softmax is not clear and previous research indicated that the inherent uncertainty of logistic-softmax leads to suboptimal performance. To mitigate these issues, we revisit and redesign the logistic-softmax likelihood, which enables control of the \\textit{a priori} confidence level through a temperature parameter. Furthermore, we theoretically and empirically show that softmax can be viewed as a special case of logistic-softmax and logistic-softmax induces a larger family of data distribution than softmax. Utilizing modified logistic-softmax, we integrate the data augmentation technique into the deep kernel based Gaussian process meta-learning framework, and derive an analytical mean-field approximation for task-specific updates. Our approach yields well-calibrated uncertainty estimates and achieves comparable or superior results on standard benchmark datasets. Code is publicly available at \\url{https://github.com/keanson/revisit-logistic-softmax}",
    "keywords": [],
    "checked": true,
    "id": "15282d063c77f1709ac53ea14734e81ffdebabc1",
    "semantic_title": "revisiting logistic-softmax likelihood in bayesian meta-learning for few-shot classification",
    "citation_count": 0,
    "authors": [
      "Tianjun Ke",
      "Haoqun Cao",
      "Zenan Ling",
      "Feng Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6cdd4ce9330025967dd1ed0bed3010f5-Abstract-Conference.html": {
    "title": "Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration",
    "volume": "main",
    "abstract": "In recent years, AI-assisted drug design methods have been proposed to generate molecules given the pockets' structures of target proteins. Most of them are {\\em atom-level-based} methods, which consider atoms as basic components and generate atom positions and types. In this way, however, it is hard to generate realistic fragments with complicated structures. To solve this, we propose \\textsc{D3FG}, a {\\em functional-group-based} diffusion model for pocket-specific molecule generation and elaboration. \\textsc{D3FG} decomposes molecules into two categories of components: functional groups defined as rigid bodies and linkers as mass points. And the two kinds of components can together form complicated fragments that enhance ligand-protein interactions. To be specific, in the diffusion process, \\textsc{D3FG} diffuses the data distribution of the positions, orientations, and types of the components into a prior distribution; In the generative process, the noise is gradually removed from the three variables by denoisers parameterized with designed equivariant graph neural networks. In the experiments, our method can generate molecules with more realistic 3D structures, competitive affinities toward the protein targets, and better drug properties. Besides, \\textsc{D3FG} as a solution to a new task of molecule elaboration, could generate molecules with high affinities based on existing ligands and the hotspots of target proteins",
    "keywords": [],
    "checked": true,
    "id": "e499649e26e12971a6d62a23491d64620913af72",
    "semantic_title": "functional-group-based diffusion for pocket-specific molecule generation and elaboration",
    "citation_count": 5,
    "authors": [
      "Haitao Lin",
      "Yufei Huang",
      "Odin Zhang",
      "Yunfan Liu",
      "Lirong Wu",
      "Siyuan Li",
      "Zhiyuan Chen",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6cde6435e111671b04f4574006cf3c47-Abstract-Conference.html": {
    "title": "Approximately Equivariant Graph Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are commonly described as being permutation equivariant with respect to node relabeling in the graph. This symmetry of GNNs is often compared to the translation equivariance of Euclidean convolution neural networks (CNNs). However, these two symmetries are fundamentally different: The translation equivariance of CNNs corresponds to symmetries of the fixed domain acting on the image signals (sometimes known as active symmetries), whereas in GNNs any permutation acts on both the graph signals and the graph domain (sometimes described as passive symmetries). In this work, we focus on the active symmetries of GNNs, by considering a learning setting where signals are supported on a fixed graph. In this case, the natural symmetries of GNNs are the automorphisms of the graph. Since real-world graphs tend to be asymmetric, we relax the notion of symmetries by formalizing approximate symmetries via graph coarsening. We present a bias-variance formula that quantifies the tradeoff between the loss in expressivity and the gain in the regularity of the learned estimator, depending on the chosen symmetry group. To illustrate our approach, we conduct extensive experiments on image inpainting, traffic flow prediction, and human pose estimation with different choices of symmetries. We show theoretically and empirically that the best generalization performance can be achieved by choosing a suitably larger group than the graph automorphism, but smaller than the permutation group",
    "keywords": [],
    "checked": true,
    "id": "3024ec5a5a45a057ffd7c1ff6222ddaf7e7f7c8c",
    "semantic_title": "approximately equivariant graph networks",
    "citation_count": 1,
    "authors": [
      "Ningyuan Huang",
      "Ron Levie",
      "Soledad Villar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html": {
    "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models",
    "volume": "main",
    "abstract": "Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the $\\mathsf{KV}$ $\\mathsf{cache}$, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the $\\mathsf{KV}$ $\\mathsf{cache}$ which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters ($\\mathsf{H_2}$). Through a comprehensive investigation, we find that ($i$) the emergence of $\\mathsf{H_2}$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and ($ii$) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle ($\\mathsf{H_2O}$), a $\\mathsf{KV}$ $\\mathsf{cache}$ eviction policy that dynamically retains a balance of recent and $\\mathsf{H_2}$ tokens. We formulate the $\\mathsf{KV}$ $\\mathsf{cache}$ eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of $\\mathsf{H_2O}$ with 20\\% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to $29\\times$, $29\\times$, and $3\\times$ on OPT-6.7B and OPT-30B. With the same batch size, $\\mathsf{H_2O}$ can reduce the latency by up to $1.9\\times$",
    "keywords": [],
    "checked": true,
    "id": "e586a4591ba0303b769f2c07cbddaf1899cb72e4",
    "semantic_title": "h2o: heavy-hitter oracle for efficient generative inference of large language models",
    "citation_count": 10,
    "authors": [
      "Zhenyu Zhang",
      "Ying Sheng",
      "Tianyi Zhou",
      "Tianlong Chen",
      "Lianmin Zheng",
      "Ruisi Cai",
      "Zhao Song",
      "Yuandong Tian",
      "Christopher Ré",
      "Clark Barrett",
      "Zhangyang \"Atlas\" Wang",
      "Beidi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6cf7a37e761f55b642cf0939b4c64bb8-Abstract-Conference.html": {
    "title": "Uncovering motifs of concurrent signaling across multiple neuronal populations",
    "volume": "main",
    "abstract": "Modern recording techniques now allow us to record from distinct neuronal populations in different brain networks. However, especially as we consider multiple (more than two) populations, new conceptual and statistical frameworks are needed to characterize the multi-dimensional, concurrent flow of signals among these populations. Here, we develop a dimensionality reduction framework that determines (1) the subset of populations described by each latent dimension, (2) the direction of signal flow among those populations, and (3) how those signals evolve over time within and across experimental trials. We illustrate these features in simulation, and further validate the method by applying it to previously studied recordings from neuronal populations in macaque visual areas V1 and V2. Then we study interactions across select laminar compartments of areas V1, V2, and V3d, recorded simultaneously with multiple Neuropixels probes. Our approach uncovered signatures of selective communication across these three areas that related to their retinotopic alignment. This work advances the study of concurrent signaling across multiple neuronal populations",
    "keywords": [],
    "checked": false,
    "id": "96be0e34f1b6e2142a287a12acb0c101f73dcd0e",
    "semantic_title": "dual-polarity voltage imaging of the concurrent dynamics of multiple neuron types",
    "citation_count": 22,
    "authors": [
      "Evren Gokcen",
      "Anna Jasper",
      "Alison Xu",
      "Adam Kohn",
      "Christian K. Machens",
      "Byron M Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6d0942e288ce41db8d4ebd041e7d1100-Abstract-Conference.html": {
    "title": "NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos",
    "volume": "main",
    "abstract": "In this paper, we aim to model 3D scene dynamics from multi-view videos. Unlike the majority of existing works which usually focus on the common task of novel view synthesis within the training time period, we propose to simultaneously learn the geometry, appearance, and physical velocity of 3D scenes only from video frames, such that multiple desirable applications can be supported, including future frame extrapolation, unsupervised 3D semantic scene decomposition, and dynamic motion transfer. Our method consists of three major components, 1) the keyframe dynamic radiance field, 2) the interframe velocity field, and 3) a joint keyframe and interframe optimization module which is the core of our framework to effectively train both networks. To validate our method, we further introduce two dynamic 3D datasets: 1) Dynamic Object dataset, and 2) Dynamic Indoor Scene dataset. We conduct extensive experiments on multiple datasets, demonstrating the superior performance of our method over all baselines, particularly in the critical tasks of future frame extrapolation and unsupervised 3D semantic scene decomposition",
    "keywords": [],
    "checked": true,
    "id": "d5ec28803ea982668093b622262633ec20289925",
    "semantic_title": "nvfi: neural velocity fields for 3d physics learning from dynamic videos",
    "citation_count": 0,
    "authors": [
      "Jinxi Li",
      "Ziyang Song",
      "Bo Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6d0bf1265ea9635fb4f9d56f16d7efb2-Abstract-Conference.html": {
    "title": "Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models",
    "volume": "main",
    "abstract": "Recent works have shown that line search methods can speed up Stochastic Gradient Descent (SGD) and Adam in modern over-parameterized settings. However, existing line searches may take steps that are smaller than necessary since they require a monotone decrease of the (mini-)batch objective function. We explore nonmonotone line search methods to relax this condition and possibly accept larger step sizes. Despite the lack of a monotonic decrease, we prove the same fast rates of convergence as in the monotone case. Our experiments show that nonmonotone methods improve the speed of convergence and generalization properties of SGD/Adam even beyond the previous monotone line searches. We propose a POlyak NOnmonotone Stochastic (PoNoS) method, obtained by combining a nonmonotone line search with a Polyak initial step size. Furthermore, we develop a new resetting technique that in the majority of the iterations reduces the amount of backtracks to zero while still maintaining a large initial step size. To the best of our knowledge, a first runtime comparison shows that the epoch-wise advantage of line-search-based methods gets reflected in the overall computational time",
    "keywords": [],
    "checked": true,
    "id": "d4792ed80f2d2d330841b84c0d9ee77104ddcf87",
    "semantic_title": "don't be so monotone: relaxing stochastic line search in over-parameterized models",
    "citation_count": 2,
    "authors": [
      "Leonardo Galli",
      "Holger Rauhut",
      "Mark Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6d13e085b79d454da5910e4ca82a3d9d-Abstract-Conference.html": {
    "title": "On Private and Robust Bandits",
    "volume": "main",
    "abstract": "We study private and robust multi-armed bandits (MABs), where the agent receives Huber's contaminated heavy-tailed rewards and meanwhile needs to ensure differential privacy. We consider both the finite $k$-th raw moment and the finite $k$-th central moment settings for heavy-tailed rewards distributions with $k\\ge 2$. We first present its minimax lower bound, characterizing the information-theoretic limit of regret with respect to privacy budget, contamination level, and heavy-tailedness. Then, we propose a meta-algorithm that builds on a private and robust mean estimation sub-routine \\texttt{PRM} that essentially relies on reward truncation and the Laplace mechanism. For the above two different heavy-tailed settings, we give corresponding schemes of \\texttt{PRM}, which enable us to achieve nearly-optimal regrets. Moreover, our two proposed truncation-based or histogram-based \\texttt{PRM} schemes achieve the optimal trade-off between estimation accuracy, privacy and robustness. Finally, we support our theoretical results and show the effectiveness of our algorithms with experimental studies",
    "keywords": [],
    "checked": true,
    "id": "40d9a132059f2d2c6ab76a9ae3f10f2434c13603",
    "semantic_title": "on private and robust bandits",
    "citation_count": 1,
    "authors": [
      "Yulian Wu",
      "Xingyu Zhou",
      "Youming Tao",
      "Di Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6d3040941a2d57ead4043556a70dd728-Abstract-Conference.html": {
    "title": "RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization",
    "volume": "main",
    "abstract": "Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeling quantiles of it as weighted quantile mixtures of per-agent return distribution utilities. RiskQ satisfies the RIGM principle for the VaR and distorted risk metrics. We show that RiskQ can obtain promising performance through extensive experiments. The source code of RiskQ is available in https://github.com/xmu-rl-3dv/RiskQ",
    "keywords": [],
    "checked": true,
    "id": "87fbcfffdd5c1a3cca52deed98df0a8bb8d5fc94",
    "semantic_title": "riskq: risk-sensitive multi-agent reinforcement learning value factorization",
    "citation_count": 0,
    "authors": [
      "Siqi Shen",
      "Chennan Ma",
      "Chao Li",
      "Weiquan Liu",
      "Yongquan Fu",
      "Songzhu Mei",
      "Xinwang Liu",
      "Cheng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6d5f304fb4ed0243851e41699dca4287-Abstract-Conference.html": {
    "title": "Learning Exponential Families from Truncated Samples",
    "volume": "main",
    "abstract": "Missing data problems have many manifestations across many scientific fields. A fundamental type of missing data problem arises when samples are \\textit{truncated}, i.e., samples that lie in a subset of the support are not observed. Statistical estimation from truncated samples is a classical problem in statistics which dates back to Galton, Pearson, and Fisher. A recent line of work provides the first efficient estimation algorithms for the parameters of a Gaussian distribution and for linear regression with Gaussian noise.In this paper we generalize these results to log-concave exponential families. We provide an estimation algorithm that shows that \\textit{extrapolation} is possible for a much larger class of distributions while it maintains a polynomial sample and time complexity on average. Our algorithm is based on Projected Stochastic Gradient Descent and is not only applicable in a more general setting but is also simpler and more efficient than recent algorithms. Our work also has interesting implications for learning general log-concave distributions and sampling given only access to truncated data",
    "keywords": [],
    "checked": true,
    "id": "e0f40b6aca0ae3ba76f8c5f423f93cbbf442021a",
    "semantic_title": "learning exponential families from truncated samples",
    "citation_count": 0,
    "authors": [
      "Jane Lee",
      "Andre Wibisono",
      "Emmanouil Zampetakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6d6f9908ea35313dd7566f5ce8c6e815-Abstract-Conference.html": {
    "title": "XAGen: 3D Expressive Human Avatars Generation",
    "volume": "main",
    "abstract": "Recent advances in 3D-aware GAN models have enabled the generation of realistic and controllable human body images. However, existing methods focus on the control of major body joints, neglecting the manipulation of expressive attributes, such as facial expressions, jaw poses, hand poses, and so on. In this work, we present XAGen, the first 3D generative model for human avatars capable of expressive control over body, face, and hands. To enhance the fidelity of small-scale regions like face and hands, we devise a multi-scale and multi-part 3D representation that models fine details. Based on this representation, we propose a multi-part rendering technique that disentangles the synthesis of body, face, and hands to ease model training and enhance geometric quality. Furthermore, we design multi-part discriminators that evaluate the quality of the generated avatars with respect to their appearance and fine-grained control capabilities. Experiments show that XAGen surpasses state-of-the-art methods in terms of realism, diversity, and expressive control abilities. Code and data will be made available at https://showlab.github.io/xagen",
    "keywords": [],
    "checked": true,
    "id": "28008ea664a6cf0d4d4fe37105bfb279f73a755b",
    "semantic_title": "xagen: 3d expressive human avatars generation",
    "citation_count": 2,
    "authors": [
      "Zhongcong XU",
      "Jianfeng Zhang",
      "Jun Hao Liew",
      "Jiashi Feng",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6d7c4a0727e089ed6cdd3151cbe8d8ba-Abstract-Conference.html": {
    "title": "HIQL: Offline Goal-Conditioned RL with Latent States as Actions",
    "volume": "main",
    "abstract": "Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/",
    "keywords": [],
    "checked": true,
    "id": "f18587247e4769ad0efd96a0286b012d856ba214",
    "semantic_title": "hiql: offline goal-conditioned rl with latent states as actions",
    "citation_count": 4,
    "authors": [
      "Seohong Park",
      "Dibya Ghosh",
      "Benjamin Eysenbach",
      "Sergey Levine"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html": {
    "title": "Visual Instruction Tuning",
    "volume": "main",
    "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general-purpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available",
    "keywords": [],
    "checked": true,
    "id": "a5036f31f0e629dc661f120b8c3b1f374d479ab8",
    "semantic_title": "visual instruction tuning",
    "citation_count": 732,
    "authors": [
      "Haotian Liu",
      "Chunyuan Li",
      "Qingyang Wu",
      "Yong Jae Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6de668dab370194fa304a08be5aacd85-Abstract-Conference.html": {
    "title": "A Fast and Accurate Estimator for Large Scale Linear Model via Data Averaging",
    "volume": "main",
    "abstract": "This work is concerned with the estimation problem of linear model when thesample size is extremely large and the data dimension can vary with the samplesize. In this setting, the least square estimator based on the full data is not feasiblewith limited computational resources. Many existing methods for this problem arebased on the sketching technique which uses the sketched data to perform leastsquare estimation. We derive fine-grained lower bounds of the conditional meansquared error for sketching methods. For sampling methods, our lower boundprovides an attainable optimal convergence rate. Our result implies that when thedimension is large, there is hardly a sampling method can have a faster convergencerate than the uniform sampling method. To achieve a better statistical performance,we propose a new sketching method based on data averaging. The proposedmethod reduces the original data to a few averaged observations. These averagedobservations still satisfy the linear model and are used to estimate the regressioncoefficients. The asymptotic behavior of the proposed estimation procedure isstudied. Our theoretical results show that the proposed method can achieve afaster convergence rate than the optimal convergence rate for sampling methods.Theoretical and numerical results show that the proposed estimator has goodstatistical performance as well as low computational cost",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Wang",
      "Yanyan Ouyang",
      "Yu Panpan",
      "Wangli Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6dea02c16a492682d66c6f626c306db2-Abstract-Conference.html": {
    "title": "Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry",
    "volume": "main",
    "abstract": "The backpropagation algorithm has experienced remarkable success in training large-scale artificial neural networks; however, its biological plausibility has been strongly criticized, and it remains an open question whether the brain employs supervised learning mechanisms akin to it. Here, we propose correlative information maximization between layer activations as an alternative normative approach to describe the signal propagation in biological neural networks in both forward and backward directions. This new framework addresses many concerns about the biological-plausibility of conventional artificial neural networks and the backpropagation algorithm. The coordinate descent-based optimization of the corresponding objective, combined with the mean square error loss function for fitting labeled supervision data, gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Furthermore, our approach provides a natural resolution to the weight symmetry problem between forward and backward signal propagation paths, a significant critique against the plausibility of the conventional backpropagation algorithm. This is achieved by leveraging two alternative, yet equivalent forms of the correlative mutual information objective. These alternatives intrinsically lead to forward and backward prediction networks without weight symmetry issues, providing a compelling solution to this long-standing challenge",
    "keywords": [],
    "checked": true,
    "id": "3f3458d01f05139a86cc36d1b4954045dfb6c9a5",
    "semantic_title": "correlative information maximization: a biologically plausible approach to supervised deep neural networks without weight symmetry",
    "citation_count": 0,
    "authors": [
      "Bariscan Bozkurt",
      "Cengiz Pehlevan",
      "Alper Erdogan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e2986deda273d8fb903342841fcc4dc-Abstract-Conference.html": {
    "title": "What Distributions are Robust to Indiscriminate Poisoning Attacks for Linear Learners?",
    "volume": "main",
    "abstract": "We study indiscriminate poisoning for linear learners where an adversary injects a few crafted examples into the training data with the goal of forcing the induced model to incur higher test error. Inspired by the observation that linear learners on some datasets are able to resist the best known attacks even without any defenses, we further investigate whether datasets can be inherently robust to indiscriminate poisoning attacks for linear learners. For theoretical Gaussian distributions, we rigorously characterize the behavior of an optimal poisoning attack, defined as the poisoning strategy that attains the maximum risk of the induced model at a given poisoning budget. Our results prove that linear learners can indeed be robust to indiscriminate poisoning if the class-wise data distributions are well-separated with low variance and the size of the constraint set containing all permissible poisoning points is also small. These findings largely explain the drastic variation in empirical attack performance of the state-of-the-art poisoning attacks on linear learners across benchmark datasets, making an important initial step towards understanding the underlying reasons some learning tasks are vulnerable to data poisoning attacks",
    "keywords": [],
    "checked": true,
    "id": "f63541ae25b88d5b715d64a18a10242c0fd37eb3",
    "semantic_title": "what distributions are robust to indiscriminate poisoning attacks for linear learners?",
    "citation_count": 1,
    "authors": [
      "Fnu Suya",
      "Xiao Zhang",
      "Yuan Tian",
      "David Evans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e2a1a8a037f9a06004fe651054e8938-Abstract-Conference.html": {
    "title": "Expressive probabilistic sampling in recurrent neural networks",
    "volume": "main",
    "abstract": "In sampling-based Bayesian models of brain function, neural activities are assumed to be samples from probability distributions that the brain uses for probabilistic computation. However, a comprehensive understanding of how mechanistic models of neural dynamics can sample from arbitrary distributions is still lacking. We use tools from functional analysis and stochastic differential equations to explore the minimum architectural requirements for $\\textit{recurrent}$ neural circuits to sample from complex distributions. We first consider the traditional sampling model consisting of a network of neurons whose outputs directly represent the samples ($\\textit{sampler-only}$ network). We argue that synaptic current and firing-rate dynamics in the traditional model have limited capacity to sample from a complex probability distribution. We show that the firing rate dynamics of a recurrent neural circuit with a separate set of output units can sample from an arbitrary probability distribution. We call such circuits $\\textit{reservoir-sampler networks}$ (RSNs). We propose an efficient training procedure based on denoising score matching that finds recurrent and output weights such that the RSN implements Langevin sampling. We empirically demonstrate our model's ability to sample from several complex data distributions using the proposed neural dynamics and discuss its applicability to developing the next generation of sampling-based Bayesian brain models",
    "keywords": [],
    "checked": true,
    "id": "e5069d753493ed876767f127a197bf6e067bf9c6",
    "semantic_title": "expressive probabilistic sampling in recurrent neural networks",
    "citation_count": 2,
    "authors": [
      "Shirui Chen",
      "Linxing Jiang",
      "Rajesh PN Rao",
      "Eric Shea-Brown"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e32c247076c2c0fb381e022c02d2c78-Abstract-Conference.html": {
    "title": "Counting Distinct Elements Under Person-Level Differential Privacy",
    "volume": "main",
    "abstract": "We study the problem of counting the number of distinct elements in a dataset subject to the constraint of differential privacy. We consider the challenging setting of person-level DP (a.k.a. user-level DP) where each person may contribute an unbounded number of items and hence the sensitivity is unbounded.Our approach is to compute a bounded-sensitivity version of this query, which reduces to solving a max-flow problem. The sensitivity bound is optimized to balance the noise we must add to privatize the answer against the error of the approximation of the bounded-sensitivity query to the true number of unique elements",
    "keywords": [],
    "checked": true,
    "id": "c50521a79d9080d4dca9bc62ab44937ce1ec247e",
    "semantic_title": "counting distinct elements under person-level differential privacy",
    "citation_count": 0,
    "authors": [
      "Thomas Steinke",
      "Alexander Knop"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e4432b912599d11609b9cdf98c823c5-Abstract-Conference.html": {
    "title": "Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks",
    "volume": "main",
    "abstract": "In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler (sparse or low-rank) subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss. We observe empirically the existence of attractive invariant sets in trained deep neural networks, implying that SGD dynamics often collapses to simple subnetworks with either vanishing or redundant neurons. We further demonstrate how this simplifying process of stochastic collapse benefits generalization in a linear teacher-student framework. Finally, through this analysis, we mechanistically explain why early training with large learning rates for extended periods benefits subsequent generalization",
    "keywords": [],
    "checked": true,
    "id": "b488223a9422d43ed8c0e0da04c9c0592eb07829",
    "semantic_title": "stochastic collapse: how gradient noise attracts sgd dynamics towards simpler subnetworks",
    "citation_count": 7,
    "authors": [
      "Feng Chen",
      "Daniel Kunin",
      "Atsushi Yamamura",
      "Surya Ganguli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e469fbdc43ade121170f61096f4458b-Abstract-Conference.html": {
    "title": "Conservative State Value Estimation for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning faces a significant challenge of value over-estimation due to the distributional drift between the dataset and the current learned policy, leading to learning failure in practice. The common approach is to incorporate a penalty term to reward or value estimation in the Bellman iterations. Meanwhile, to avoid extrapolation on out-of-distribution (OOD) states and actions, existing methods focus on conservative Q-function estimation. In this paper, we propose Conservative State Value Estimation (CSVE), a new approach that learns conservative V-function via directly imposing penalty on OOD states. Compared to prior work, CSVE allows more effective state value estimation with conservative guarantees and further better policy optimization. Further, we apply CSVE and develop a practical actor-critic algorithm in which the critic does the conservative value estimation by additionally sampling and penalizing the states around the dataset, and the actor applies advantage weighted updates extended with state exploration to improve the policy. We evaluate in classic continual control tasks of D4RL, showing that our method performs better than the conservative Q-function learning methods and is strongly competitive among recent SOTA methods",
    "keywords": [],
    "checked": true,
    "id": "8c4a2558851522b0344c7d605aac7d2a36b740aa",
    "semantic_title": "conservative state value estimation for offline reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Liting Chen",
      "Jie Yan",
      "Zhengdao Shao",
      "Lu Wang",
      "Qingwei Lin",
      "Saravanakumar Rajmohan",
      "Thomas Moscibroda",
      "Dongmei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e4cdfdd909ea4e34bfc85a12774cba0-Abstract-Conference.html": {
    "title": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks",
    "volume": "main",
    "abstract": "Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU",
    "keywords": [],
    "checked": true,
    "id": "8b9f01585a679dffe92261ecdec56425db9ef97f",
    "semantic_title": "demystifying oversmoothing in attention-based graph neural networks",
    "citation_count": 3,
    "authors": [
      "Xinyi Wu",
      "Amir Ajorlou",
      "Zihui Wu",
      "Ali Jadbabaie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e60a9023d2c63f7f0856910129ae753-Abstract-Conference.html": {
    "title": "Sample Complexity for Quadratic Bandits: Hessian Dependent Bounds and Optimal Algorithms",
    "volume": "main",
    "abstract": "In stochastic zeroth-order optimization, a problem of practical relevance is understanding how to fully exploit the local geometry of the underlying objective function. We consider a fundamental setting in which the objective function is quadratic, and provide the first tight characterization of the optimal Hessian-dependent sample complexity. Our contribution is twofold. First, from an information-theoretic point of view, we prove tight lower bounds on Hessian-dependent complexities by introducing a concept called \\emph{energy allocation}, which captures the interaction between the searching algorithm and the geometry of objective functions. A matching upper bound is obtained by solving the optimal energy spectrum. Then, algorithmically, we show the existence of a Hessian-independent algorithm that universally achieves the asymptotic optimal sample complexities for all Hessian instances. The optimal sample complexities achieved by our algorithm remain valid for heavy-tailed noise distributions, which are enabled by a truncation method",
    "keywords": [],
    "checked": true,
    "id": "b953069761ba1b75e09323b2c9f1f80a68d6dbd9",
    "semantic_title": "sample complexity for quadratic bandits: hessian dependent bounds and optimal algorithms",
    "citation_count": 0,
    "authors": [
      "Qian Yu",
      "Yining Wang",
      "Baihe Huang",
      "Qi Lei",
      "Jason D. Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e73c39cc428c7d264d9820319f31e79-Abstract-Conference.html": {
    "title": "Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?",
    "volume": "main",
    "abstract": "We study benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. In particular, we consider linearly separable data for which a relatively small proportion of labels are corrupted or flipped. We identify conditions on the margin of the clean data that give rise to three distinct training outcomes: benign overfitting, in which zero loss is achieved and with high probability test data is classified correctly; overfitting, in which zero loss is achieved but test data is misclassified with probability lower bounded by a constant; and non-overfitting, in which clean points, but not corrupt points, achieve zero loss and again with high probability test data is classified correctly. Our analysis provides a fine-grained description of the dynamics of neurons throughout training and reveals two distinct phases: in the first phase clean points achieve close to zero loss, in the second phase clean points oscillate on the boundary of zero loss while corrupt points either converge towards zero loss or are eventually zeroed by the network. We prove these results using a combinatorial approach that involves bounding the number of clean versus corrupt updates during these phases of training",
    "keywords": [],
    "checked": true,
    "id": "bf4418dfdf31f4f8d2eeab634699a7d8b4a3365a",
    "semantic_title": "training shallow relu networks on noisy data using hinge loss: when do we overfit and is it benign?",
    "citation_count": 1,
    "authors": [
      "Erin George",
      "Michael Murray",
      "William Swartworth",
      "Deanna Needell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e976e7930460b5c3167a104ba8cc39c-Abstract-Conference.html": {
    "title": "Adaptive Algorithms for Relaxed Pareto Set Identification",
    "volume": "main",
    "abstract": "In this paper we revisit the fixed-confidence identification of the Pareto optimal set in a multi-objective multi-armed bandit model. As the sample complexity to identify the exact Pareto set can be very large, a relaxation allowing to output some additional near-optimal arms has been studied. In this work we also tackle alternative relaxations that allow instead to identify a relevant \\emph{subset} of the Pareto set. Notably, we propose a single sampling strategy, called Adaptive Pareto Exploration, that can be used in conjunction with different stopping rules to take into account different relaxations of the Pareto Set Identification problem. We analyze the sample complexity of these different combinations, quantifying in particular the reduction in sample complexity that occurs when one seeks to identify at most $k$ Pareto optimal arms. We showcase the good practical performance of Adaptive Pareto Exploration on a real-world scenario, in which we adaptively explore several vaccination strategies against Covid-19 in order to find the optimal ones when multiple immunogenicity criteria are taken into account",
    "keywords": [],
    "checked": true,
    "id": "8601d8dd272343add81f489c0a1338f7e85ee195",
    "semantic_title": "adaptive algorithms for relaxed pareto set identification",
    "citation_count": 1,
    "authors": [
      "Cyrille KONE",
      "Emilie Kaufmann",
      "Laura Richert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e9a0a72da9b76c3ebc8cc33ff10ac29-Abstract-Conference.html": {
    "title": "PHOTOSWAP: Personalized Subject Swapping in Images",
    "volume": "main",
    "abstract": "In an era where images and visual content dominate our digital landscape, the ability to manipulate and personalize these images has become a necessity.Envision seamlessly substituting a tabby cat lounging on a sunlit window sill in a photograph with your own playful puppy, all while preserving the original charm and composition of the image. We present \\emph{Photoswap}, a novel approach that enables this immersive image editing experience through personalized subject swapping in existing images.\\emph{Photoswap} first learns the visual concept of the subject from reference images and then swaps it into the target image using pre-trained diffusion models in a training-free manner. We establish that a well-conceptualized visual subject can be seamlessly transferred to any image with appropriate self-attention and cross-attention manipulation, maintaining the pose of the swapped subject and the overall coherence of the image. Comprehensive experiments underscore the efficacy and controllability of \\emph{Photoswap} in personalized subject swapping. Furthermore, \\emph{Photoswap} significantly outperforms baseline methods in human ratings across subject swapping, background preservation, and overall quality, revealing its vast application potential, from entertainment to professional editing",
    "keywords": [],
    "checked": true,
    "id": "477ae5324c206c35d4bde4fe3fad21c74349a723",
    "semantic_title": "photoswap: personalized subject swapping in images",
    "citation_count": 11,
    "authors": [
      "Jing Gu",
      "Yilin Wang",
      "Nanxuan Zhao",
      "Tsu-Jui Fu",
      "Wei Xiong",
      "Qing Liu",
      "Zhifei Zhang",
      "HE Zhang",
      "Jianming Zhang",
      "HyunJoon Jung",
      "Xin Eric Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ea69f8116b7c01e3c3e43b62e6868fc-Abstract-Conference.html": {
    "title": "Simplifying Neural Network Training Under Class Imbalance",
    "volume": "main",
    "abstract": "Real-world datasets are often highly class-imbalanced, which can adversely impact the performance of deep learning models. The majority of research on training neural networks under class imbalance has focused on specialized loss functions and sampling techniques. Notably, we demonstrate that simply tuning existing components of standard deep learning pipelines, such as the batch size, data augmentation, architecture size, pre-training, optimizer, and label smoothing, can achieve state-of-the-art performance without any specialized loss functions or samplers. We also provide key prescriptions and considerations for training under class imbalance, and an understanding of why imbalance methods succeed or fail",
    "keywords": [],
    "checked": true,
    "id": "ce957722ad79e2913d82d1b6d4949e7e8efea822",
    "semantic_title": "simplifying neural network training under class imbalance",
    "citation_count": 1,
    "authors": [
      "Ravid Shwartz-Ziv",
      "Micah Goldblum",
      "Yucen Li",
      "C. Bayan Bruss",
      "Andrew G. Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6eaf8c729af4fbeb18006dc2e6a41d9b-Abstract-Conference.html": {
    "title": "Regret Minimization via Saddle Point Optimization",
    "volume": "main",
    "abstract": "A long line of works characterizes the sample complexity of regret minimization in sequential decision-making by min-max programs. In the corresponding saddle-point game, the min-player optimizes the sampling distribution against an adversarial max-player that chooses confusing models leading to large regret. The most recent instantiation of this idea is the decision-estimation coefficient (DEC), which was shown to provide nearly tight lower and upper bounds on the worst-case expected regret in structured bandits and reinforcement learning. By re-parametrizing the offset DEC with the confidence radius and solving the corresponding min-max program, we derive an anytime variant of the Estimation-To-Decisions algorithm (Anytime-E2D). Importantly, the algorithm optimizes the exploration-exploitation trade-off online instead of via the analysis. Our formulation leads to a practical algorithm for finite model classes and linear feedback models. We illustrate the results by deriving improved rates for high-dimensional linear bandits. Lastly, we point out connections to the information ratio, decoupling coefficient and PAC-DEC, and numerically evaluate the performance of E2D on simple examples",
    "keywords": [],
    "checked": true,
    "id": "50845b94a141637814ba90772f5efd1073fea972",
    "semantic_title": "regret minimization via saddle point optimization",
    "citation_count": 0,
    "authors": [
      "Johannes Kirschner",
      "Alireza Bakhtiari",
      "Kushagra Chandak",
      "Volodymyr Tkachuk",
      "Csaba Szepesvari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ec2be0bb10be9a0e5db4cc2a921f301-Abstract-Conference.html": {
    "title": "On the Sublinear Regret of GP-UCB",
    "volume": "main",
    "abstract": "In the kernelized bandit problem, a learner aims to sequentially compute the optimum of a function lying in a reproducing kernel Hilbert space given only noisy evaluations at sequentially chosen points. In particular, the learner aims to minimize regret, which is a measure of the suboptimality of the choices made.Arguably the most popular algorithm is the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, which involves acting based on a simple linear estimator of the unknown function.Despite its popularity, existing analyses of GP-UCB give a suboptimal regret rate, which fails to be sublinear for many commonly used kernels such as the Matern kernel. This has led to a longstanding open question: are existing regret analyses for GP-UCB tight, or can bounds be improved by using more sophisticated analytical techniques?In this work, we resolve this open question and show that GP-UCB enjoys nearly optimal regret. In particular, our results yield sublinear regret rates for the Matern kernel, improving over the state-of-the-art analyses and partially resolving a COLT open problem posed by Vakili et al. Our improvements rely on a key technical contribution --- regularizing kernel ridge estimators in proportion to the smoothness of the underlying kernel $k$. Applying this key idea together with a largely overlooked concentration result in separable Hilbert spaces (for which we provide an independent, simplified derivation), we are able to provide a tighter analysis of the GP-UCB algorithm",
    "keywords": [],
    "checked": true,
    "id": "6cc825b25d30c01264783e4fba8ce5aedfa032a5",
    "semantic_title": "on the sublinear regret of gp-ucb",
    "citation_count": 2,
    "authors": [
      "Justin Whitehouse",
      "Aaditya Ramdas",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ecd51685e2d765bc0ad32a2e73faf62-Abstract-Conference.html": {
    "title": "Train Once and Explain Everywhere: Pre-training Interpretable Graph Neural Networks",
    "volume": "main",
    "abstract": "Intrinsic interpretable graph neural networks aim to provide transparent predictions by identifying the influential fraction of the input graph that guides the model prediction, i.e., the explanatory subgraph. However, current interpretable GNNs mostly are dataset-specific and hard to generalize to different graphs. A more generalizable GNN interpretation model which can effectively distill the universal structural patterns of different graphs is until-now unexplored. Motivated by the great success of recent pre-training techniques, we for the first time propose the Pre-training Interpretable Graph Neural Network ($\\pi$-GNN) to distill the universal interpretability of GNNs by pre-training over synthetic graphs with ground-truth explanations. Specifically, we introduce a structural pattern learning module to extract diverse universal structure patterns and integrate them together to comprehensively represent the graphs of different types. Next, a hypergraph refining module is proposed to identify the explanatory subgraph by incorporating the universal structure patterns with local edge interactions. Finally, the task-specific predictor is cascaded with the pre-trained $\\pi$-GNN model and fine-tuned over downstream tasks. Extensive experiments demonstrate that $\\pi$-GNN significantly surpasses the leading interpretable GNN baselines with up to 9.98\\% interpretation improvement and 16.06\\% classification accuracy improvement. Meanwhile, $\\pi$-GNN pre-trained on graph classification task also achieves the top-tier interpretation performance on node classification task, which further verifies its promising generalization performance among different downstream tasks. Our code and datasets are available at https://anonymous.4open.science/r/PI-GNN-F86C",
    "keywords": [],
    "checked": false,
    "id": "f726f137c73bf173357b3b6cdb10e57658829e06",
    "semantic_title": "asdexplainer : an interpretable graph neural network framework for brain network based autism spectrum disorder analysis",
    "citation_count": 0,
    "authors": [
      "Jun Yin",
      "Chaozhuo Li",
      "Hao Yan",
      "Jianxun Lian",
      "Senzhang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ed9931d6e1fb6a85efa1b2c014a47e1-Abstract-Conference.html": {
    "title": "Quantum speedups for stochastic optimization",
    "volume": "main",
    "abstract": "We consider the problem of minimizing a continuous function given given access to a natural quantum generalization of a stochastic gradient oracle. We provide two new methods for the special case of minimizing a Lipschitz convex function. Each method obtains a dimension versus accuracy trade-off which is provably unachievable classically and we prove that one method is asymptotically optimal in low-dimensional settings. Additionally, we provide quantum algorithms for computing a critical point of a smooth non-convex function at rates not known to be achievable classically. To obtain these results we build upon the quantum multivariate mean estimation result of Cornelissen et al. and provide a general quantum variance reduction technique of independent interest",
    "keywords": [],
    "checked": true,
    "id": "23540cc01ce11f686cd3eb854ceff3ba4b3f2ec6",
    "semantic_title": "quantum speedups for stochastic optimization",
    "citation_count": 0,
    "authors": [
      "Aaron Sidford",
      "Chenyi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6f125214c86439d107ccb58e549e828f-Abstract-Conference.html": {
    "title": "Concept Algebra for (Score-Based) Text-Controlled Generative Models",
    "volume": "main",
    "abstract": "This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. A key property of such models is that they can compose disparate concepts in a 'disentangled' manner.This suggests these models have internal representations that encode concepts in a 'disentangled' manner. Here, we focus on the idea that concepts are encoded as subspaces of some representation space. We formalize what this means, show there's a natural choice for the representation, and develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples using Stable Diffusion",
    "keywords": [],
    "checked": true,
    "id": "574114328240db66ae3f2d689b2fde97ba5d9c6e",
    "semantic_title": "concept algebra for (score-based) text-controlled generative models",
    "citation_count": 3,
    "authors": [
      "Zihao Wang",
      "Lin Gui",
      "Jeffrey Negrea",
      "Victor Veitch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6f1346bac8b02f76a631400e2799b24b-Abstract-Conference.html": {
    "title": "Fast Optimal Transport through Sliced Generalized Wasserstein Geodesics",
    "volume": "main",
    "abstract": "Wasserstein distance (WD) and the associated optimal transport plan have been proven useful in many applications where probability measures are at stake. In this paper, we propose a new proxy of the squared WD, coined $\\textnormal{min-SWGG}$, that is based on the transport map induced by an optimal one-dimensional projection of the two input distributions. We draw connections between $\\textnormal{min-SWGG}$, and Wasserstein generalized geodesics in which the pivot measure is supported on a line. We notably provide a new closed form for the exact Wasserstein distance in the particular case of one of the distributions supported on a line allowing us to derive a fast computational scheme that is amenable to gradient descent optimization. We show that $\\textnormal{min-SWGG}$, is an upper bound of WD and that it has a complexity similar to as Sliced-Wasserstein, with the additional feature of providing an associated transport plan. We also investigate some theoretical properties such as metricity, weak convergence, computational and topological properties. Empirical evidences support the benefits of $\\textnormal{min-SWGG}$, in various contexts, from gradient flows, shape matching and image colorization, among others",
    "keywords": [],
    "checked": false,
    "id": "1df217771f2c52e9211bd6c95298196885812830",
    "semantic_title": "fast optimal transport through sliced wasserstein generalized geodesics",
    "citation_count": 3,
    "authors": [
      "Guillaume Mahey",
      "Laetitia Chapel",
      "Gilles Gasso",
      "Clément Bonet",
      "Nicolas Courty"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6f43166f50f26e8d8f3edc5545b0749f-Abstract-Conference.html": {
    "title": "Aggregating Capacity in FL through Successive Layer Training for Computationally-Constrained Devices",
    "volume": "main",
    "abstract": "Federated learning (FL) is usually performed on resource-constrained edge devices, e.g., with limited memory for the computation. If the required memory to train a model exceeds this limit, the device will be excluded from the training. This can lead to a lower accuracy as valuable data and computation resources are excluded from training, also causing bias and unfairness. The FL training process should be adjusted to such constraints. The state-of-the-art techniques propose training subsets of the FL model at constrained devices, reducing their resource requirements for training. However, these techniques largely limit the co-adaptation among parameters of the model and are highly inefficient, as we show: it is actually better to train a smaller (less accurate) model by the system where all the devices can train the model end-to-end than applying such techniques. We propose a new method that enables successive freezing and training of the parameters of the FL model at devices, reducing the training's resource requirements at the devices while still allowing enough co-adaptation between parameters. We show through extensive experimental evaluation that our technique greatly improves the accuracy of the trained model (by 52.4 p.p. ) compared with the state of the art, efficiently aggregating the computation capacity available on distributed devices",
    "keywords": [],
    "checked": true,
    "id": "7f58a2897ff3bdf1f389b76ae948abbb0ed1d4db",
    "semantic_title": "aggregating capacity in fl through successive layer training for computationally-constrained devices",
    "citation_count": 0,
    "authors": [
      "Kilian Pfeiffer",
      "Ramin Khalili",
      "Joerg Henkel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6f479ea488e0908ac8b1b37b27fd134c-Abstract-Conference.html": {
    "title": "FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations",
    "volume": "main",
    "abstract": "Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe, achieves an average gain of up to 4.4\\%, compared to the state-of-the-art unsupervised models, across all datasets in consideration, both homophilic and heterophilic. Our code can be found at: https://github.com/Microsoft/figure",
    "keywords": [],
    "checked": true,
    "id": "d0c199b4e56ab22146be76a620ea74690b3f70bd",
    "semantic_title": "figure: simple and efficient unsupervised node representations with filter augmentations",
    "citation_count": 0,
    "authors": [
      "Chanakya Ekbote",
      "Ajinkya Deshpande",
      "Arun Iyer",
      "SUNDARARAJAN SELLAMANICKAM",
      "Ramakrishna Bairi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6f5288d7059cbe3f5a19dad1b3bf17e1-Abstract-Conference.html": {
    "title": "Mixed-Initiative Multiagent Apprenticeship Learning for Human Training of Robot Teams",
    "volume": "main",
    "abstract": "Extending recent advances in Learning from Demonstration (LfD) frameworks to multi-robot settings poses critical challenges such as environment non-stationarity due to partial observability which is detrimental to the applicability of existing methods. Although prior work has shown that enabling communication among agents of a robot team can alleviate such issues, creating inter-agent communication under existing Multi-Agent LfD (MA-LfD) frameworks requires the human expert to provide demonstrations for both environment actions and communication actions, which necessitates an efficient communication strategy on a known message spaces. To address this problem, we propose Mixed-Initiative Multi-Agent Apprenticeship Learning (MixTURE). MixTURE enables robot teams to learn from a human expert-generated data a preferred policy to accomplish a collaborative task, while simultaneously learning emergent inter-agent communication to enhance team coordination. The key ingredient to MixTURE's success is automatically learning a communication policy, enhanced by a mutual-information maximizing reverse model that rationalizes the underlying expert demonstrations without the need for human generated data or an auxiliary reward function. MixTURE outperforms a variety of relevant baselines on diverse data generated by human experts in complex heterogeneous domains. MixTURE is the first MA-LfD framework to enable learning multi-robot collaborative policies directly from real human data, resulting in ~44% less human workload, and ~46% higher usability score",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Esmaeil Seraj",
      "Jerry Xiong",
      "Mariah Schrum",
      "Matthew Gombolay"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6f627c706a7d9961cc1ff55f37f07f97-Abstract-Conference.html": {
    "title": "Meta-Learning Adversarial Bandit Algorithms",
    "volume": "main",
    "abstract": "We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two levels of low-dimensional hyperparameter tuning is enough to learn a sequence of affine functions of non-Lipschitz and sometimes non-convex Bregman divergences bounding the regret of OMD",
    "keywords": [],
    "checked": true,
    "id": "06179a2bf769d8b32da5d84c8e4a0d98557229f0",
    "semantic_title": "meta-learning adversarial bandit algorithms",
    "citation_count": 0,
    "authors": [
      "Misha Khodak",
      "Ilya Osadchiy",
      "Keegan Harris",
      "Maria-Florina F. Balcan",
      "Kfir Y. Levy",
      "Ron Meir",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6f6dd92b03ff9be7468a6104611c9187-Abstract-Conference.html": {
    "title": "Geometric Algebra Transformer",
    "volume": "main",
    "abstract": "Problems involving geometric data arise in physics, chemistry, robotics, computer vision, and many other fields. Such data can take numerous forms, for instance points, direction vectors, translations, or rotations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric (or Clifford) algebra, which offers an efficient 16-dimensional vector-space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile, efficient, and scalable. We demonstrate GATr in problems from n-body modeling to wall-shear-stress estimation on large arterial meshes to robotic motion planning. GATr consistently outperforms both non-geometric and equivariant baselines in terms of error, data efficiency, and scalability",
    "keywords": [],
    "checked": true,
    "id": "4689f6603587e64e87ae36a385e9aab34af2966a",
    "semantic_title": "geometric algebra transformer",
    "citation_count": 4,
    "authors": [
      "Johann Brehmer",
      "Pim de Haan",
      "Sönke Behrends",
      "Taco S. Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6f7fa4df2c8a79c164d3697898a32bd9-Abstract-Conference.html": {
    "title": "Top-Ambiguity Samples Matter: Understanding Why Deep Ensemble Works in Selective Classification",
    "volume": "main",
    "abstract": "Selective classification allows a machine learning model to reject some hard inputs and thus improve the reliability of its predictions. In this area, the ensemble method is powerful in practice, but there has been no solid analysis on why the ensemble method works. Inspired by an interesting empirical result that the improvement of the ensemble largely comes from top-ambiguity samples where its member models diverge, we prove that, based on some assumptions, the ensemble has a lower selective risk than the member model for any coverage within a range. The proof is nontrivial since the selective risk is a non-convex function of the model prediction. The assumptions and the theoretical results are supported by systematic experiments on both computer vision and natural language processing tasks",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Ding",
      "Yixuan Cao",
      "Ping Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6f9806a5adc72b5b834b27e4c7c0df9b-Abstract-Conference.html": {
    "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input",
    "volume": "main",
    "abstract": "Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single $k$-nearest-neighbor ($k$NN) index, while the returned $k$NN distances are the attention dot-product scores. This $k$NN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even **500k** token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. Our code and models are publicly available at https://github.com/abertsch72/unlimiformer , and support LLaMA-2 as well",
    "keywords": [],
    "checked": true,
    "id": "dbc368bc8b49347dd27679894524fa62f88492c9",
    "semantic_title": "unlimiformer: long-range transformers with unlimited length input",
    "citation_count": 38,
    "authors": [
      "Amanda Bertsch",
      "Uri Alon",
      "Graham Neubig",
      "Matthew Gormley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6fa4d985e7c434002fb6289ab9b2d654-Abstract-Conference.html": {
    "title": "Improving CLIP Training with Language Rewrites",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) stands as one of the most effective and scalable methods for training transferable vision models using paired image and text data. CLIP models are trained using contrastive loss, which typically relies on data augmentations to prevent overfitting and shortcuts. However, in the CLIP training paradigm, data augmentations are exclusively applied to image inputs, while language inputs remain unchanged throughout the entire training process, limiting the exposure of diverse texts to the same image. In this paper, we introduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach to enhance CLIP training through language rewrites. Leveraging the in-context learning capability of large language models, we rewrite the text descriptions associated with each image. These rewritten texts exhibit diversity in sentence structure and vocabulary while preserving the original key concepts and meanings. During training, LaCLIP randomly selects either the original texts or the rewritten versions as text augmentations for each image. Extensive experiments on CC3M, CC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with language rewrites significantly improves the transfer performance without computation or memory overhead during training. Specifically for ImageNet zero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on LAION-400M",
    "keywords": [],
    "checked": true,
    "id": "d4af12327385260116dfd68ed1ec6d0602d26d1f",
    "semantic_title": "improving clip training with language rewrites",
    "citation_count": 28,
    "authors": [
      "Lijie Fan",
      "Dilip Krishnan",
      "Phillip Isola",
      "Dina Katabi",
      "Yonglong Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6fcbfb3721c1781728b10c6685cc2f6c-Abstract-Conference.html": {
    "title": "Extensible Prompts for Language Models on Zero-shot Language Style Customization",
    "volume": "main",
    "abstract": "We propose eXtensible Prompt (X-Prompt) for prompting a large language model (LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words. Registering new imaginary words allows us to instruct the LLM to comprehend concepts that are difficult to describe with NL words, thereby making a prompt more descriptive. Also, these imaginary words are designed to be out-of-distribution (OOD) robust so that they can be (re)used like NL words in various prompts, distinguishing X-Prompt from soft prompt that is for fitting in-distribution data. We propose context-augmented learning (CAL) to learn imaginary words for general usability, enabling them to work properly in OOD (unseen) prompts. We experiment X-Prompt for zero-shot language style customization as a case study. The promising results of X-Prompt demonstrate its potential to facilitate advanced interaction beyond the natural language interface, bridging the communication gap between humans and LLMs",
    "keywords": [],
    "checked": true,
    "id": "d6f5fdf345b9cea58ecee3753e93f6c110831286",
    "semantic_title": "extensible prompts for language models on zero-shot language style customization",
    "citation_count": 4,
    "authors": [
      "Tao Ge",
      "Hu Jing",
      "Li Dong",
      "Shaoguang Mao",
      "Yan Xia",
      "Xun Wang",
      "Si-Qing Chen",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6fe10a4c0d680609f0560920bd9ade4a-Abstract-Conference.html": {
    "title": "MIMEx: Intrinsic Rewards from Masked Input Modeling",
    "volume": "main",
    "abstract": "Exploring in environments with high-dimensional observations is hard. One promising approach for exploration is to use intrinsic rewards, which often boils down to estimating \"novelty\" of states, transitions, or trajectories with deep networks. Prior works have shown that conditional prediction objectives such as masked autoencoding can be seen as stochastic estimation of pseudo-likelihood. We show how this perspective naturally leads to a unified view on existing intrinsic reward approaches: they are special cases of conditional prediction, where the estimation of novelty can be seen as pseudo-likelihood estimation with different mask distributions. From this view, we propose a general framework for deriving intrinsic rewards -- Masked Input Modeling for Exploration (MIMEx) -- where the mask distribution can be flexibly tuned to control the difficulty of the underlying conditional prediction task. We demonstrate that MIMEx can achieve superior results when compared against competitive baselines on a suite of challenging sparse-reward visuomotor tasks",
    "keywords": [],
    "checked": true,
    "id": "af9e69ec120e94b5542edf88b392ee3769c1379b",
    "semantic_title": "mimex: intrinsic rewards from masked input modeling",
    "citation_count": 1,
    "authors": [
      "Toru Lin",
      "Allan Jabri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6feb9b30798abcfae937760d183605e1-Abstract-Conference.html": {
    "title": "RGMIL: Guide Your Multiple-Instance Learning Model with Regressor",
    "volume": "main",
    "abstract": "In video analysis, an important challenge is insufficient annotated data due to the rare occurrence of the critical patterns, and we need to provide discriminative frame-level representation with limited annotation in some applications. Multiple Instance Learning (MIL) is suitable for this scenario. However, many MIL models paid attention to analyzing the relationships between instance representations and aggregating them, but neglecting the critical information from the MIL problem itself, which causes difficultly achieving ideal instance-level performance compared with the supervised model.To address this issue, we propose the $\\textbf{\\textit{Regressor-Guided MIL network} (RGMIL)}$, which effectively produces discriminative instance-level representations in a general multi-classification scenario. In the proposed method, we make full use of the $\\textit{regressor}$ through our newly introduced $\\textit{aggregator}$, $\\textbf{\\textit{Regressor-Guided Pooling} (RGP)}$. RGP focuses on simulating the correct inference process of humans while facing similar problems without introducing new parameters, and the MIL problem can be accurately described through the critical information from the $\\textit{regressor}$ in our method. In experiments, RGP shows dominance on more than 20 MIL benchmark datasets, with the average bag-level classification accuracy close to 1. We also perform a series of comprehensive experiments on the MMNIST dataset. Experimental results illustrate that our $\\textit{aggregator}$ outperforms existing methods under different challenging circumstances. Instance-level predictions are even possible under the guidance of RGP information table in a long sequence. RGMIL also presents comparable instance-level performance with S-O-T-A supervised models in complicated applications. Statistical results demonstrate the assumption that a MIL model can compete with a supervised model at the instance level, as long as a structure that accurately describes the MIL problem is provided. The codes are available on $\\url{https://github.com/LMBDA-design/RGMIL}$",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaolong Du",
      "Shasha Mao",
      "Yimeng Zhang",
      "Shuiping Gou",
      "Licheng Jiao",
      "Lin Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ffa1f5ad26addef897dcb938e525db7-Abstract-Conference.html": {
    "title": "Stochastic Multi-armed Bandits: Optimal Trade-off among Optimality, Consistency, and Tail Risk",
    "volume": "main",
    "abstract": "We consider the stochastic multi-armed bandit problem and fully characterize the interplays among three desired properties for policy design: worst-case optimality, instance-dependent consistency, and light-tailed risk. We show how the order of expected regret exactly affects the decaying rate of the regret tail probability for both the worst-case and instance-dependent scenario. A novel policy is proposed to achieve the optimal regret tail risk for any regret threshold. Concretely, for any given $\\alpha\\in[1/2, 1)$ and $\\beta\\in[0, 1)$, our policy achieves a worst-case expected regret of $\\tilde O(T^\\alpha)$ and instance-dependent expected regret of $\\tilde O(T^\\beta)$, while enjoys a probability of incurring an $\\Omega(T^\\delta)$ regret that decays exponentially with a polynomial $T$ term. Such decaying rate is proved to be best achievable. We also generalize our analysis to the stochastic multi-armed bandit problem with non-stationary baseline rewards, where in each time period $t$, the decision maker pulls one of $K$ arms and collects a reward which is the sum of three terms: the mean of the pulled arm, an independent noise, and a non-stationary baseline reward as a function of $t$. Our results reveal insights on the trade-off between expected regret and tail risk for both worst-case and instance-dependent scenario, indicating that more sub-optimality and inconsistency leaves space for more light-tailed risk of incurring a large regret",
    "keywords": [],
    "checked": false,
    "id": "0cc29ca9d95dda98535c1e979e661bc6654be57b",
    "semantic_title": "regret distribution in stochastic bandits: optimal trade-off between expectation and tail risk",
    "citation_count": 1,
    "authors": [
      "David Simchi-Levi",
      "Zeyu Zheng",
      "Feng Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6ffe484a646db13891bb6435ca39d667-Abstract-Conference.html": {
    "title": "Learning Mask-aware CLIP Representations for Zero-Shot Segmentation",
    "volume": "main",
    "abstract": "Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically, Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, mask-aware loss and self-distillation loss are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\\% (+ 8.2\\%) on COCO, 81.8\\% (+ 3.2\\%) on Pascal-VOC, and 8.7\\% (+4.3\\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git",
    "keywords": [],
    "checked": true,
    "id": "6a627c258084054b2648058a78c579539d7f7bc3",
    "semantic_title": "learning mask-aware clip representations for zero-shot segmentation",
    "citation_count": 2,
    "authors": [
      "Siyu Jiao",
      "Yunchao Wei",
      "Yaowei Wang",
      "Yao Zhao",
      "Humphrey Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7016d7b7b6e3c05b2128ac5b3aae492d-Abstract-Conference.html": {
    "title": "Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks",
    "volume": "main",
    "abstract": "The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend.Specific nonlinear behaviors can also be precisely identified and captured theoretically, such asinitial condensation, saddle-to-plateau dynamics, plateau escape, changes of activation patterns, learning with increasing complexity, etc",
    "keywords": [],
    "checked": true,
    "id": "a55a6887c27d5c8effaec38f9c4b8dd78231c800",
    "semantic_title": "understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks",
    "citation_count": 2,
    "authors": [
      "Mingze Wang",
      "Chao Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/70255afc962aca0930327c090eb7d8c5-Abstract-Conference.html": {
    "title": "Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy",
    "volume": "main",
    "abstract": "We study gradient descent under linearly correlated noise. Our work is motivated by recent practical methods for optimization with differential privacy (DP), such as DP-FTRL, which achieve strong performance in settings where privacy amplification techniques are infeasible (such as in federated learning). These methods inject privacy noise through a matrix factorization mechanism, making the noise linearly correlated over iterations. We propose a simplified setting that distills key facets of these methods and isolates the impact of linearly correlated noise. We analyze the behavior of gradient descent in this setting, for both convex and non-convex functions. Our analysis is demonstrably tighter than prior work and recovers multiple important special cases exactly (including anticorrelated perturbed gradient descent). We use our results to develop new, effective matrix factorizations for differentially private optimization, and highlight the benefits of these factorizations theoretically and empirically",
    "keywords": [],
    "checked": true,
    "id": "94d7c11cfba5264611a59ca65dbeac51d92a4fa9",
    "semantic_title": "gradient descent with linearly correlated noise: theory and applications to differential privacy",
    "citation_count": 0,
    "authors": [
      "Anastasiia Koloskova",
      "Ryan McKenna",
      "Zachary Charles",
      "John Rush",
      "H. Brendan McMahan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/702b67152ec4435795f681865b67999c-Abstract-Conference.html": {
    "title": "A Framework for Fast and Stable Representations of Multiparameter Persistent Homology Decompositions",
    "volume": "main",
    "abstract": "Topological data analysis (TDA) is an area of data science that focuses on using invariants from algebraic topology to provide multiscale shape descriptors for geometric data sets such as point clouds. One of the most important such descriptors is persistent homology, which encodes the change in shape as a filtration parameter changes; a typical parameter is the feature scale. For many data sets, it is useful to simultaneously vary multiple filtration parameters, for example feature scale and density. While the theoretical properties of single parameter persistent homology are well understood, less is known about the multiparameter case. A central question is the problem of representing multiparameter persistent homology by elements of a vector space for integration with standard machine learning algorithms. Existing approaches to this problem either ignore most of the multiparameter information to reduce to the one-parameter case or are heuristic and potentially unstable in the face of noise. In this article, we introduce a new general representation framework that leverages recent results on decompositions of multiparameter persistent homology. This framework is rich in information, fast to compute, and encompasses previous approaches. Moreover, we establish theoretical stability guarantees under this framework as well as efficient algorithms for practical computation, making this framework an applicable and versatile tool for analyzing geometric and point cloud data. We validate our stability results and algorithms with numerical experiments that demonstrate statistical convergence, prediction accuracy, and fast running times on several real data sets",
    "keywords": [],
    "checked": true,
    "id": "0fddca6b52859c6dbe8dd3ca260253479ae7ee30",
    "semantic_title": "a framework for fast and stable representations of multiparameter persistent homology decompositions",
    "citation_count": 1,
    "authors": [
      "David Loiseaux",
      "Mathieu Carrière",
      "Andrew Blumberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/703f727ec10190b2fddcf8e24f52df48-Abstract-Conference.html": {
    "title": "Should We Learn Most Likely Functions or Parameters?",
    "volume": "main",
    "abstract": "Standard regularized training procedures correspond to maximizing a posterior distribution over parameters, known as maximum a posteriori (MAP) estimation. However, model parameters are of interest only insomuch as they combine with the functional form of a model to provide a function that can make good predictions. Moreover, the most likely parameters under the parameter posterior do not generally correspond to the most likely function induced by the parameter posterior. In fact, we can re-parametrize a model such that any setting of parameters can maximize the parameter posterior. As an alternative, we investigate the benefits and drawbacks of directly estimating the most likely function implied by the model and the data. We show that this procedure leads to pathological solutions when using neural networks and prove conditions under which the procedure is well-behaved, as well as a scalable approximation. Under these conditions, we find that function-space MAP estimation can lead to flatter minima, better generalization, and improved robustness to overfitting",
    "keywords": [],
    "checked": true,
    "id": "3da722c8ba0e65311ca53f101725efe84fbf309d",
    "semantic_title": "should we learn most likely functions or parameters?",
    "citation_count": 1,
    "authors": [
      "Shikai Qiu",
      "Tim G. J. Rudner",
      "Sanyam Kapoor",
      "Andrew G. Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/70518ea42831f02afc3a2828993935ad-Abstract-Conference.html": {
    "title": "Geometry-Informed Neural Operator for Large-Scale 3D PDEs",
    "volume": "main",
    "abstract": "We propose the geometry-informed neural operator (GINO), a highly efficient approach to learning the solution operator of large-scale partial differential equations with varying geometries. GINO uses a signed distance function (SDF) representation of the input shape and neural operators based on graph and Fourier architectures to learn the solution operator. The graph neural operator handles irregular grids and transforms them into and from regular latent grids on which Fourier neural operator can be efficiently applied. We provide an efficient implementation of GINO using an optimized hashing approach, which allows efficient learning in a shared, compressed latent space with reduced computation and memory costs. GINO is discretization-invariant, meaning the trained model can be applied to arbitrary discretizations of the continuous domain and applies to any shape or resolution. To empirically validate the performance of our method on large-scale simulation, we generate the industry-standard aerodynamics dataset of 3D vehicle geometries with Reynolds numbers as high as five million. For this large-scale 3D fluid simulation, numerical methods are expensive to compute surface pressure. We successfully trained GINO to predict the pressure on car surfaces using only five hundred data points. The cost-accuracy experiments show a 26,000x speed-up compared to optimized GPU-based computational fluid dynamics (CFD) simulators on computing the drag coefficient. When tested on new combinations of geometries and boundary conditions (inlet velocities), GINO obtains a one-fourth reduction in error rate compared to deep neural network approaches",
    "keywords": [],
    "checked": true,
    "id": "55a59dfff3e57ec551814db82225a3677411e092",
    "semantic_title": "geometry-informed neural operator for large-scale 3d pdes",
    "citation_count": 15,
    "authors": [
      "Zongyi Li",
      "Nikola Kovachki",
      "Chris Choy",
      "Boyi Li",
      "Jean Kossaifi",
      "Shourya Otta",
      "Mohammad Amin Nabian",
      "Maximilian Stadler",
      "Christian Hundt",
      "Kamyar Azizzadenesheli",
      "Animashree Anandkumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7058bc192a37f5e5a57398887b05f6f6-Abstract-Conference.html": {
    "title": "Differentially Private Image Classification by Learning Priors from Random Processes",
    "volume": "main",
    "abstract": "In privacy-preserving machine learning, differentially private stochastic gradient descent (DP-SGD) performs worse than SGD due to per-sample gradient clipping and noise addition.A recent focus in private learning research is improving the performance of DP-SGD on private data by incorporating priors that are learned on real-world public data.In this work, we explore how we can improve the privacy-utility tradeoff of DP-SGD by learning priors from images generated by random processes and transferring these priors to private data. We propose DP-RandP, a three-phase approach. We attain new state-of-the-art accuracy when training from scratch on CIFAR10, CIFAR100, MedMNIST and ImageNet for a range of privacy budgets $\\\\varepsilon \\\\in [1, 8]$. In particular, we improve the previous best reported accuracy on CIFAR10 from $60.6 \\\\%$ to $72.3 \\\\%$ for $\\\\varepsilon=1$",
    "keywords": [],
    "checked": true,
    "id": "b40b72ccfa4eb12ecf97ad9a65543dec02bceab2",
    "semantic_title": "differentially private image classification by learning priors from random processes",
    "citation_count": 3,
    "authors": [
      "Xinyu Tang",
      "Ashwinee Panda",
      "Vikash Sehwag",
      "Prateek Mittal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/707a2d58641b2192203b4bf4c532cfe1-Abstract-Conference.html": {
    "title": "NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA",
    "volume": "main",
    "abstract": "Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves retrieving nodes from a knowledge graph (KG) to answer natural language questions. Recent GNN-based approaches formulate this task as a KG path searching problem, where messages are sequentially propagated from the seed node towards the answer nodes. However, these messages are past-oriented, and they do not consider the full KG context. To make matters worse, KG nodes often represent pronoun entities and are sometimes encrypted, being uninformative in selecting between paths. To address these problems, we propose Neural Tree Search (NuTrea), a tree search-based GNN model that incorporates the broader KG context. Our model adopts a message-passing scheme that probes the unreached subtree regions to boost the past-oriented embeddings. In addition, we introduce the Relation Frequency-Inverse Entity Frequency (RF-IEF) node embedding that considers the global KG context to better characterize ambiguous KG nodes. The general effectiveness of our approach is demonstrated through experiments on three major multi-hop KGQA benchmark datasets, and our extensive analyses further validate its expressiveness and robustness. Overall, NuTrea provides a powerful means to query the KG with complex natural language questions. Code is available at https://github.com/mlvlab/NuTrea",
    "keywords": [],
    "checked": true,
    "id": "8f0c0de1ec55529e8b11779178730602ab6ef618",
    "semantic_title": "nutrea: neural tree search for context-guided multi-hop kgqa",
    "citation_count": 0,
    "authors": [
      "Hyeong Kyu Choi",
      "Seunghun Lee",
      "Jaewon Chu",
      "Hyunwoo J. Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/70899a5d74f83317c78f1a7d413d1baa-Abstract-Conference.html": {
    "title": "Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization",
    "volume": "main",
    "abstract": "While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a top concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called \"look-alike clustering\", which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster centers acts as a regularization and improves generalization error of the trained models. Finally, we corroborate our asymptotic theory with finite-sample numerical experiments where we observe a perfect match when the sample size is only of order of a few hundreds",
    "keywords": [],
    "checked": true,
    "id": "7d52df6b1593a4e9cbb8d084f69be46c5d432220",
    "semantic_title": "anonymous learning via look-alike clustering: a precise analysis of model generalization",
    "citation_count": 0,
    "authors": [
      "Adel Javanmard",
      "Vahab Mirrokni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/708e0d691a22212e1e373dc8779cbe53-Abstract-Conference.html": {
    "title": "Saving 100x Storage: Prototype Replay for Reconstructing Training Sample Distribution in Class-Incremental Semantic Segmentation",
    "volume": "main",
    "abstract": "Existing class-incremental semantic segmentation (CISS) methods mainly tackle catastrophic forgetting and background shift, but often overlook another crucial issue. In CISS, each step focuses on different foreground classes, and the training set for a single step only includes images containing pixels of the current foreground classes, excluding images without them. This leads to an overrepresentation of these foreground classes in the single-step training set, causing the classification biased towards these classes. To address this issue, we present STAR, which preserves the main characteristics of each past class by storing a compact prototype and necessary statistical data, and aligns the class distribution of single-step training samples with the complete dataset by replaying these prototypes and repeating background pixels with appropriate frequency. Compared to the previous works that replay raw images, our method saves over 100 times the storage while achieving better performance. Moreover, STAR incorporates an old-class features maintaining (OCFM) loss, keeping old-class features unchanged while preserving sufficient plasticity for learning new classes. Furthermore, a similarity-aware discriminative (SAD) loss is employed to specifically enhance the feature diversity between similar old-new class pairs. Experiments on two public datasets, Pascal VOC 2012 and ADE20K, reveal that our model surpasses all previous state-of-the-art methods",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Chen",
      "Runmin Cong",
      "Yuxuan LUO",
      "Horace Ip",
      "Sam Kwong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/70b8505ac79e3e131756f793cd80eb8d-Abstract-Conference.html": {
    "title": "Skill-it! A data-driven skills framework for understanding and training language models",
    "volume": "main",
    "abstract": "The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 37.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens",
    "keywords": [],
    "checked": true,
    "id": "4b474c1f42eefbf14ca85c951f2a22ce031b6cb7",
    "semantic_title": "skill-it! a data-driven skills framework for understanding and training language models",
    "citation_count": 11,
    "authors": [
      "Mayee Chen",
      "Nicholas Roberts",
      "Kush Bhatia",
      "Jue WANG",
      "Ce Zhang",
      "Frederic Sala",
      "Christopher Ré"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/70c6d82d27cd96c501c4def4803d5782-Abstract-Conference.html": {
    "title": "Strategic Behavior in Two-sided Matching Markets with Prediction-enhanced Preference-formation",
    "volume": "main",
    "abstract": "Two-sided matching markets have long existed to pair agents in the absence of regulated exchanges. A common example is school choice, where a matching mechanism uses student and school preferences to assign students to schools. In such settings, forming preferences is both difficult and critical. Prior work has suggested various prediction mechanisms that help agents make decisions about their preferences. Although often deployed together, these matching and prediction mechanisms are almost always analyzed separately. The present work shows that at the intersection of the two lies a previously unexplored type of strategic behavior: agents returning to the market (e.g., schools) can attack future predictions by interacting short-term non-optimally with their matches. Here, we first introduce this type of strategic behavior, which we call an adversarial interaction attack. Next, we construct a formal economic model that captures the feedback loop between prediction mechanisms designed to assist agents and the matching mechanism used to pair them. Finally, in a simplified setting, we prove that returning agents can benefit from using adversarial interaction attacks and gain progressively more as the trust in and accuracy of predictions increases. We also show that this attack increases inequality in the student population",
    "keywords": [],
    "checked": false,
    "id": "580e930c08be764f99a56bc0e73f224537f34e02",
    "semantic_title": "incentives in two-sided matching markets with prediction-enhanced preference-formation",
    "citation_count": 1,
    "authors": [
      "Stefania Ionescu",
      "Yuhao Du",
      "Kenneth Joseph",
      "Ancsa Hannak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/70de9e3948645a1be2de657f14d85c6d-Abstract-Conference.html": {
    "title": "Sample Complexity of Forecast Aggregation",
    "volume": "main",
    "abstract": "We consider a Bayesian forecast aggregation model where $n$ experts, after observing private signals about an unknown binary event, report their posterior beliefs about the event to a principal, who then aggregates the reports into a single prediction for the event. The signals of the experts and the outcome of the event follow a joint distribution that is unknown to the principal, but the principal has access to i.i.d. \"samples\" from the distribution, where each sample is a tuple of the experts' reports (not signals) and the realization of the event. Using these samples, the principal aims to find an $\\varepsilon$-approximately optimal aggregator, where optimality is measured in terms of the expected squared distance between the aggregated prediction and the realization of the event. We show that the sample complexity of this problem is at least $\\tilde \\Omega(m^{n-2} / \\varepsilon)$ for arbitrary discrete distributions, where $m$ is the size of each expert's signal space. This sample complexity grows exponentially in the number of experts $n$. But, if the experts' signals are independent conditioned on the realization of the event, then the sample complexity is significantly reduced, to $\\tilde O(1 / \\varepsilon^2)$, which does not depend on $n$. Our results can be generalized to non-binary events. The proof of our results uses a reduction from the distribution learning problem and reveals the fact that forecast aggregation is almost as difficult as distribution learning",
    "keywords": [],
    "checked": true,
    "id": "e238aa29e7fce808f60899363764937fe4c43606",
    "semantic_title": "sample complexity of forecast aggregation",
    "citation_count": 0,
    "authors": [
      "Tao Lin",
      "Yiling Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/70f286e0fc977c0a3a64ef96849c8d7d-Abstract-Conference.html": {
    "title": "Learning to Influence Human Behavior with Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "When interacting with people, AI agents do not just influence the state of the world -- they also influence the actions people take in response to the agent, and even their underlying intentions and strategies. Accounting for and leveraging this influence has mostly been studied in settings where it is sufficient to assume that human behavior is near-optimal: competitive games, or general-sum settings like autonomous driving alongside human drivers. Instead, we focus on influence in settings where there is a need to capture human suboptimality. For instance, imagine a collaborative task in which, due either to cognitive biases or lack of information, people do not perform very well -- how could an agent influence them towards more optimal behavior? Assuming near-optimal human behavior will not work here, and so the agent needs to learn from real human data. But experimenting online with humans is potentially unsafe, and creating a high-fidelity simulator of the environment is often impractical. Hence, we focus on learning from an offline dataset of human-human interactions. Our observation is that offline reinforcement learning (RL) can learn to effectively influence suboptimal humans by extending and combining elements of observed human-human behavior. We demonstrate that offline RL can solve two challenges with effective influence. First, we show that by learning from a dataset of suboptimal human-human interaction on a variety of tasks -- none of which contains examples of successful influence -- an agent can learn influence strategies to steer humans towards better performance even on new tasks. Second, we show that by also modeling and conditioning on human behavior, offline RL can learn to affect not just the human's actions but also their underlying strategy, and adapt to changes in their strategy",
    "keywords": [],
    "checked": true,
    "id": "2b62e91ab690a5fbc46fec6e8c39fb9fdb59b579",
    "semantic_title": "learning to influence human behavior with offline reinforcement learning",
    "citation_count": 4,
    "authors": [
      "Joey Hong",
      "Sergey Levine",
      "Anca Dragan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7103cd82de95a7b30983fcf74ba499ac-Abstract-Conference.html": {
    "title": "Discriminative Calibration: Check Bayesian Computation from Simulations and Flexible Classifier",
    "volume": "main",
    "abstract": "To check the accuracy of Bayesian computations, it is common to use rank-based simulation-based calibration (SBC). However, SBC has drawbacks: The test statistic is somewhat ad-hoc, interactions are difficult to examine, multiple testing is a challenge, and the resulting p-value is not a divergence metric. We propose to replace the marginal rank test with a flexible classification approach that learns test statistics from data. This measure typically has a higher statistical power than the SBC test and returns an interpretable divergence measure of miscalibration, computed from classification accuracy. This approach can be used with different data generating processes to address simulation-based inference or traditional inference methods like Markov chain Monte Carlo or variational inference. We illustrate an automated implementation using neural networks and statistically-inspired features, and validate the method with numerical and real data experiments",
    "keywords": [],
    "checked": false,
    "id": "ad2061063d0c9bd36e75a74b2f8d385483e8f21a",
    "semantic_title": "discriminative calibration",
    "citation_count": 1,
    "authors": [
      "Yuling Yao",
      "Justin Domke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7172e147d916eef4cb1eb30016ce725f-Abstract-Conference.html": {
    "title": "Epidemic Learning: Boosting Decentralized Learning with Randomized Communication",
    "volume": "main",
    "abstract": "We present Epidemic Learning (EL), a simple yet powerful decentralized learning (DL) algorithm that leverages changing communication topologies to achieve faster model convergence compared to conventional DL approaches. At each round of EL, each node sends its model updates to a random sample of $s$ other nodes (in a system of $n$ nodes). We provide an extensive theoretical analysis of EL, demonstrating that its changing topology culminates in superior convergence properties compared to the state-of-the-art (static and dynamic) topologies. Considering smooth non-convex loss functions, the number of transient iterations for EL, i.e., the rounds required to achieve asymptotic linear speedup, is in $O(n^3/s^2)$ which outperforms the best-known bound $O(n^3)$ by a factor of $s^2$, indicating the benefit of randomized communication for DL. We empirically evaluate EL in a 96-node network and compare its performance with state-of-the-art DL approaches. Our results illustrate that EL converges up to $ 1.7\\times$ quicker than baseline DL algorithms and attains $2.2 $\\% higher accuracy for the same communication volume",
    "keywords": [],
    "checked": true,
    "id": "7bc4857e9256adf472a2d95e7f487a7c10631b67",
    "semantic_title": "epidemic learning: boosting decentralized learning with randomized communication",
    "citation_count": 0,
    "authors": [
      "Martijn De Vos",
      "Sadegh Farhadkhani",
      "Rachid Guerraoui",
      "Anne-marie Kermarrec",
      "Rafael Pires",
      "Rishi Sharma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/717b9fd2ede6b8a9971a296d5179df89-Abstract-Conference.html": {
    "title": "Global Identifiability of $\\ell_1$-based Dictionary Learning via Matrix Volume Optimization",
    "volume": "main",
    "abstract": "We propose a novel formulation for dictionary learning that minimizes the determinant of the dictionary matrix, also known as its volume, subject to the constraint that each row of the sparse coefficient matrix has unit $\\ell_1$ norm. The main motivation for the proposed formulation is that it provides global identifiability guarantee of the groundtruth dictionary and sparse coefficient matrices, up to the inherent and inconsequential permutation and scaling ambiguity, if a set of vectors obtained from the coefficient matrix lies inside the $\\ell_\\infty$ norm ball but contains the $\\ell_2$ norm ball in their convex hull. Unlike existing work on identifiability of dictionary learning, our result is global, meaning that a globally optimal solution to our proposed formulation has to be a permuted and rescaled version of the groundtruth factors. Another major improvement in our result is that there is no additional assumption on the dictionary matrix other than it is nonsingular, unlike most other work that require the atoms of the dictionary to be mutually incoherent. We also provide a probabilistic analysis and show that if the sparse coefficient matrix is generated from the widely adopted Bernoulli-Gaussian model, then it is globally identifiable if the sample size is bigger than a constant times $k\\log k$, where $k$ is the number atoms in the dictionary, with overwhelming probability. The bound is essentially the same as those local identifiability results, but we show that it is also global. Finally, we propose algorithms to solve the new proposed formulation, specifically one based on the linearized-ADMM with efficient per-iteration updates. The proposed algorithms exhibit surprisingly effective performance in correctly and efficiently recovering the dictionary, as demonstrated in the numerical experiments",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingzhou Hu",
      "Kejun Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7183f4fc87598f6c6e947b96714acbd6-Abstract-Conference.html": {
    "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization",
    "volume": "main",
    "abstract": "Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase.To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) – a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage.We employ PEQA-tuning for task-specific adaptation on LLMs with up to $65$ billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA",
    "keywords": [],
    "checked": true,
    "id": "a10843d1349fff8d2a7d9722f800802187fef67f",
    "semantic_title": "memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization",
    "citation_count": 21,
    "authors": [
      "Jeonghoon Kim",
      "Jung Hyun Lee",
      "Sungdong Kim",
      "Joonsuk Park",
      "Kang Min Yoo",
      "Se Jung Kwon",
      "Dongsoo Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/71b52a5b3fe2e9303433a174b60e160d-Abstract-Conference.html": {
    "title": "Corruption-Robust Offline Reinforcement Learning with General Function Approximation",
    "volume": "main",
    "abstract": "We investigate the problem of corruption robustness in offline reinforcement learning (RL) with general function approximation, where an adversary can corrupt each sample in the offline dataset, and the corruption level $\\zeta\\geq0$ quantifies the cumulative corruption amount over $n$ episodes and $H$ steps. Our goal is to find a policy that is robust to such corruption and minimizes the suboptimality gap with respect to the optimal policy for the uncorrupted Markov decision processes (MDPs). Drawing inspiration from the uncertainty-weighting technique from the robust online RL setting \\citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight iteration procedure to efficiently compute on batched samples and propose a corruption-robust algorithm for offline RL. Notably, under the assumption of single policy coverage and the knowledge of $\\zeta$, our proposed algorithm achieves a suboptimality bound that is worsened by an additive factor of $\\mathcal O(\\zeta \\cdot (\\text CC(\\lambda,\\hat{\\mathcal F},\\mathcal Z_n^H))^{1/2} (C(\\hat{\\mathcal F},\\mu))^{-1/2} n^{-1})$ due to the corruption. Here $\\text CC(\\lambda,\\hat{\\mathcal F},\\mathcal Z_n^H)$ is the coverage coefficient that depends on the regularization parameter $\\lambda$, the confidence set $\\hat{\\mathcal F}$, and the dataset $\\mathcal Z_n^H$, and $C(\\hat{\\mathcal F},\\mu)$ is a coefficient that depends on $\\hat{\\mathcal F}$ and the underlying data distribution $\\mu$. When specialized to linear MDPs, the corruption-dependent error term reduces to $\\mathcal O(\\zeta d n^{-1})$ with $d$ being the dimension of the feature map, which matches the existing lower bound for corrupted linear MDPs. This suggests that our analysis is tight in terms of the corruption-dependent term",
    "keywords": [],
    "checked": true,
    "id": "a1dea7d843d369d6317e8ce5783c0011943c4c99",
    "semantic_title": "corruption-robust offline reinforcement learning with general function approximation",
    "citation_count": 5,
    "authors": [
      "Chenlu Ye",
      "Rui Yang",
      "Quanquan Gu",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/71c31ebf577ffdad5f4a74156daad518-Abstract-Conference.html": {
    "title": "Training Fully Connected Neural Networks is $\\exists\\mathbb{R}$-Complete",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Bertschinger",
      "Christoph Hertrich",
      "Paul Jungeblut",
      "Tillmann Miltzow",
      "Simon Weber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/71ec377d5df1fc61ee7770857820519b-Abstract-Conference.html": {
    "title": "Uncertainty Estimation for Safety-critical Scene Segmentation via Fine-grained Reward Maximization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongzheng Yang",
      "Cheng Chen",
      "Yueyao CHEN",
      " Scheppach",
      "Hon Chi Yip",
      "DOU QI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/71ed042903ed67c7f6355e5dd0539eec-Abstract-Conference.html": {
    "title": "GLIME: General, Stable and Local LIME Explanation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeren Tan",
      "Yang Tian",
      "Jian Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7207ffb9888068c0ee13ae3be023cada-Abstract-Conference.html": {
    "title": "Efficient Symbolic Policy Learning with Differentiable Symbolic Expression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Guo",
      "Rui Zhang",
      "Shaohui Peng",
      "Qi Yi",
      "Xing Hu",
      "Ruizhi Chen",
      "Zidong Du",
      "xishan zhang",
      "Ling Li",
      "Qi Guo",
      "Yunji Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/720991812855c99df50bc8b36966cd81-Abstract-Conference.html": {
    "title": "PAC-Bayesian Spectrally-Normalized Bounds for Adversarially Robust Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiancong Xiao",
      "Ruoyu Sun",
      "Zhi-Quan Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/72153267883fbcafdb6e4662382696c5-Abstract-Conference.html": {
    "title": "Neural Graph Generation from Graph Statistics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiarash Zahirnia",
      "Yaochen Hu",
      "Mark Coates",
      "Oliver Schulte"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/72223cc66f63ca1aa59edaec1b3670e6-Abstract-Conference.html": {
    "title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadreza Pourreza",
      "Davood Rafiei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/72235260ae8d57ac42638a26d3b7d089-Abstract-Conference.html": {
    "title": "Scaling Up Differentially Private LASSO Regularized Logistic Regression via Faster Frank-Wolfe Iterations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edward Raff",
      "Amol Khanna",
      "Fred Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/722fcbc1a6667f2075d75ea79a1b3552-Abstract-Conference.html": {
    "title": "Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov Games with Bandit Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Cai",
      "Haipeng Luo",
      "Chen-Yu Wei",
      "Weiqiang Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/72393bd47a35f5b3bee4c609e7bba733-Abstract-Conference.html": {
    "title": "Deductive Verification of Chain-of-Thought Reasoning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhan Ling",
      "Yunhao Fang",
      "Xuanlin Li",
      "Zhiao Huang",
      "Mingu Lee",
      "Roland Memisevic",
      "Hao Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/72416ded78a439907ff72165ac9c56e0-Abstract-Conference.html": {
    "title": "Rigorous Runtime Analysis of MOEA/D for Solving Multi-Objective Minimum Weight Base Problems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Viet Do",
      "Aneta Neumann",
      "Frank Neumann",
      "Andrew Sutton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7274ed909a312d4d869cc328ad1c5f04-Abstract-Conference.html": {
    "title": "Implicit Transfer Operator Learning: Multiple Time-Resolution Models for Molecular Dynamics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathias Schreiner",
      "Ole Winther",
      "Simon Olsson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7279908471a7dd4898d2715f7c6a7413-Abstract-Conference.html": {
    "title": "Causal de Finetti: On the Identification of Invariant Causal Structure in Exchangeable Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Guo",
      "Viktor Toth",
      "Bernhard Schölkopf",
      "Ferenc Huszar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/727a5a5c77be15d053b47b7c391800c2-Abstract-Conference.html": {
    "title": "Batch Bayesian Optimization For Replicable Experimental Design",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongxiang Dai",
      "Quoc Phong Nguyen",
      "Sebastian Tay",
      "Daisuke Urano",
      "Richalynn Leong",
      "Bryan Kian Hsiang Low",
      "Patrick Jaillet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/72802bef5cf1a3449e909b20c2ae18d5-Abstract-Conference.html": {
    "title": "Contrastive Modules with Temporal Attention for Multi-Task Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siming Lan",
      "Rui Zhang",
      "Qi Yi",
      "Jiaming Guo",
      "Shaohui Peng",
      "Yunkai Gao",
      "Fan Wu",
      "Ruizhi Chen",
      "Zidong Du",
      "Xing Hu",
      "xishan zhang",
      "Ling Li",
      "Yunji Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/72a1ec14aed36985ffba175e0bba3fec-Abstract-Conference.html": {
    "title": "Scalable Primal-Dual Actor-Critic Method for Safe Multi-Agent RL with General Utilities",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghao Ying",
      "Yunkai Zhang",
      "Yuhao Ding",
      "Alec Koppel",
      "Javad Lavaei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/72c12e48c6135762f56bf188cd2479d2-Abstract-Conference.html": {
    "title": "Optimal Transport-Guided Conditional Score-Based Diffusion Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Gu",
      "Liwei Yang",
      "Jian Sun",
      "Zongben Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/72d32f4fe0b7af03732bd227bf1c4a5f-Abstract-Conference.html": {
    "title": "GNeSF: Generalizable Neural Semantic Fields",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Chen",
      "Chen Li",
      "Mengqi Guo",
      "Zhiwen Yan",
      "Gim Hee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/730ce0ae730f39e4d77b0f04a8afe4be-Abstract-Conference.html": {
    "title": "When can Regression-Adjusted Control Variate Help? Rare Events, Sobolev Embedding and Minimax Optimality",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jose Blanchet",
      "Haoxuan Chen",
      "Yiping Lu",
      "Lexing Ying"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7319b7561ffe5e2f6419acd4a2f52d6b-Abstract-Conference.html": {
    "title": "Sharp Calibrated Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Capone",
      "Sandra Hirche",
      "Geoff Pleiss"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/732c5757aa5577de9b103332cf7ac0bf-Abstract-Conference.html": {
    "title": "GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takahiro Mimori",
      "Michiaki Hamada"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/73950f0eb4ac0925dc71ba2406893320-Abstract-Conference.html": {
    "title": "The Learnability of In-Context Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noam Wies",
      "Yoav Levine",
      "Amnon Shashua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/73aacd8b3b05b4b503d58310b523553c-Abstract-Conference.html": {
    "title": "Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuval Kirstain",
      "Adam Polyak",
      "Uriel Singer",
      "Shahbuland Matiana",
      "Joe Penna",
      "Omer Levy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/73af055566f5514b9863315133b84eda-Abstract-Conference.html": {
    "title": "Decorate3D: Text-Driven High-Quality Texture Generation for Mesh Decoration in the Wild",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanhui Guo",
      "Xinxin Zuo",
      "Peng Dai",
      "Juwei Lu",
      "Xiaolin Wu",
      "Li cheng",
      "Youliang Yan",
      "Songcen Xu",
      "Xiaofei Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/73bf692447f174984f30499ec9b20e04-Abstract-Conference.html": {
    "title": "Representational Strengths and Limitations of Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clayton Sanford",
      "Daniel J. Hsu",
      "Matus Telgarsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/73d6c3e4b214deebbbf8256e26d2cf45-Abstract-Conference.html": {
    "title": "On the Relationship Between Relevance and Conflict in Online Social Link Recommendations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbang Wang",
      "Jon Kleinberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/74088c68894b99383c12399c9c637be9-Abstract-Conference.html": {
    "title": "Mobilizing Personalized Federated Learning in Infrastructure-Less and Heterogeneous Environments via Random Walk Stochastic ADMM",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziba Parsons",
      "Fei Dou",
      "Houyi Du",
      "Zheng Song",
      "Jin Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7429f4c1b267cf619f28c4d4f1532f99-Abstract-Conference.html": {
    "title": "An Optimal Structured Zeroth-order Algorithm for Non-smooth Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Rando",
      "Cesare Molinari",
      "Lorenzo Rosasco",
      "Silvia Villa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/745b7e084d5ca5afc07fb454ab2be522-Abstract-Conference.html": {
    "title": "Online Control for Meta-optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Chen",
      "Elad Hazan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/746cf1bc2337700f7f0c35c7b02638cc-Abstract-Conference.html": {
    "title": "Emergent Communication in Interactive Sketch Question Answering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixing Lei",
      "Yiming Zhang",
      "Yuxin Xiong",
      "Siheng Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/746d2254f6892f3badadb07cc9c0f0da-Abstract-Conference.html": {
    "title": "Computing Approximate $\\ell_p$ Sensitivities",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Swati Padmanabhan",
      "David Woodruff",
      "Richard Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7480ed13740773505262791131c12b89-Abstract-Conference.html": {
    "title": "Automated Classification of Model Errors on ImageNet",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Momchil Peychev",
      "Mark Müller",
      "Marc Fischer",
      "Martin Vechev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7482e8ce4139df1a2d8195a0746fa713-Abstract-Conference.html": {
    "title": "Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihao Andreas Lin",
      "Javier Antorán",
      "Shreyas Padhy",
      "David Janz",
      "José Miguel Hernández-Lobato",
      "Alexander Terenin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/749252feedd44f7f10d47ec1d674a2f8-Abstract-Conference.html": {
    "title": "Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihua Zhang",
      "Yimeng Zhang",
      "Aochuan Chen",
      "jinghan jia",
      "Jiancheng Liu",
      "Gaowen Liu",
      "Mingyi Hong",
      "Shiyu Chang",
      "Sijia Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/749b64078a64fa5734a49fb40bc9fd65-Abstract-Conference.html": {
    "title": "On Slicing Optimality for Mutual Information",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ammar Fayad",
      "Majd Ibrahim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/74bb24dca8334adce292883b4b651eda-Abstract-Conference.html": {
    "title": "Language Model Tokenizers Introduce Unfairness Between Languages",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Petrov",
      "Emanuele La Malfa",
      "Philip Torr",
      "Adel Bibi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/74f11936d6144eae43730e1a49365479-Abstract-Conference.html": {
    "title": "Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaolu Liu",
      "Robert Peach",
      "Pedro A.M Mediano",
      "Mauricio Barahona"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/74f1edadbdf495e7258ee8db7b1d3acd-Abstract-Conference.html": {
    "title": "Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitao Mao",
      "Zhikai Chen",
      "Wei Jin",
      "Haoyu Han",
      "Yao Ma",
      "Tong Zhao",
      "Neil Shah",
      "Jiliang Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/74fa3651b41560e1c7555e0958c70333-Abstract-Conference.html": {
    "title": "Deciphering Spatio-Temporal Graph Forecasting: A Causal Lens and Treatment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Xia",
      "Yuxuan Liang",
      "Haomin Wen",
      "Xu Liu",
      "Kun Wang",
      "Zhengyang Zhou",
      "Roger Zimmermann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/74fb3d526c7d8bd0c3e4b71704bb5abf-Abstract-Conference.html": {
    "title": "Greedy Poisson Rejection Sampling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gergely Flamich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/74fc5575632191d96881d8015f79dde3-Abstract-Conference.html": {
    "title": "Uncertainty Quantification via Neural Posterior Principal Components",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elias Nehme",
      "Omer Yair",
      "Tomer Michaeli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/75101364dc3aa7772d27528ea504472b-Abstract-Conference.html": {
    "title": "Deep Reinforcement Learning with Plasticity Injection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgenii Nikishin",
      "Junhyuk Oh",
      "Georg Ostrovski",
      "Clare Lyle",
      "Razvan Pascanu",
      "Will Dabney",
      "Andre Barreto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7526508f11bbe0a123af62b9dab1fbe1-Abstract-Conference.html": {
    "title": "StreamNet: Memory-Efficient Streaming Tiny Deep Learning Inference on the Microcontroller",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong-Sheng Zheng",
      "Yu-Yuan Liu",
      "Chen-Fong Hsu",
      "Tsung Tai Yeh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/752820c79b4ebb72809014bdfdedd603-Abstract-Conference.html": {
    "title": "Estimating and Controlling for Equalized Odds via Sensitive Attribute Predictors",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beepul Bharti",
      "Paul Yi",
      "Jeremias Sulam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/753d9584b57ba01a10482f1ea7734a89-Abstract-Conference.html": {
    "title": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youquan Liu",
      "Lingdong Kong",
      "Jun CEN",
      "Runnan Chen",
      "Wenwei Zhang",
      "Liang Pan",
      "Kai Chen",
      "Ziwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/754612bde73a8b65ad8743f1f6d8ddf6-Abstract-Conference.html": {
    "title": "Time Series Kernels based on Nonlinear Vector AutoRegressive Delay Embeddings",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni De Felice",
      "John Goulermas",
      "Vladimir Gusev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/754e80f98b2a141942f45a0eeb258a3c-Abstract-Conference.html": {
    "title": "SPA: A Graph Spectral Alignment Perspective for Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqing Xiao",
      "Haobo Wang",
      "Ying Jin",
      "Lei Feng",
      "Gang Chen",
      "Fei Huang",
      "Junbo Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/756d74cd58592849c904421e3b2ec7a4-Abstract-Conference.html": {
    "title": "CosNet: A Generalized Spectral Kernel Network",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfang Xue",
      "Pengfei Fang",
      "Jinyue Tian",
      "Shipeng Zhu",
      "hui xue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7571c9d44179c7988178593c5b62a9b6-Abstract-Conference.html": {
    "title": "A Theory of Unsupervised Translation Motivated by Understanding Animal Communication",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shafi Goldwasser",
      "David Gruber",
      "Adam Tauman Kalai",
      "Orr Paradise"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/75b0edb869e2cd509d64d0e8ff446bc1-Abstract-Conference.html": {
    "title": "Adversarial Self-Training Improves Robustness and Generalization for Gradual Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianghe Shi",
      "Weiwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/75c2ec5f98d7b2f50ad68033d2c07086-Abstract-Conference.html": {
    "title": "TensorNet: Cartesian Tensor Representations for Efficient Learning of Molecular Potentials",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillem Simeon",
      "Gianni De Fabritiis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/75c411b0a06fa9e78f2a516b57b2ce62-Abstract-Conference.html": {
    "title": "Multi-Player Zero-Sum Markov Games with Networked Separable Interactions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chanwoo Park",
      "Kaiqing Zhang",
      "Asuman Ozdaglar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/75cd262a3fd8e76e37bb7941db141a1d-Abstract-Conference.html": {
    "title": "Continuous-Time Functional Diffusion Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giulio Franzese",
      "Giulio Corallo",
      "Simone Rossi",
      "Markus Heinonen",
      "Maurizio Filippone",
      "Pietro Michiardi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/75f1a165c7561e028c41d42fa6286a76-Abstract-Conference.html": {
    "title": "Is Distance Matrix Enough for Geometric Deep Learning?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zian Li",
      "Xiyuan Wang",
      "Yinan Huang",
      "Muhan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/760b5def8dcb1156aac454e9c0f5f406-Abstract-Conference.html": {
    "title": "Optimized Covariance Design for AB Test on Social Network under Interference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianyi Chen",
      "Bo Li",
      "Lu Deng",
      "Yong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/760dff0f9c0e9ed4d7e22918c73351d4-Abstract-Conference.html": {
    "title": "AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Susan Liang",
      "Chao Huang",
      "Yapeng Tian",
      "Anurag Kumar",
      "Chenliang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/760e8857c7660fe50bac933161b14f41-Abstract-Conference.html": {
    "title": "Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Voronov",
      "Mikhail Khoroshikh",
      "Artem Babenko",
      "Max Ryabinin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/761c3284ee4859bff3c7e5d9299a45ee-Abstract-Conference.html": {
    "title": "DesCo: Learning Object Recognition with Rich Language Descriptions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liunian Li",
      "Zi-Yi Dou",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7644353d580a9e027e0069d6480d971b-Abstract-Conference.html": {
    "title": "On the Variance, Admissibility, and Stability of Empirical Risk Minimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gil Kur",
      "Eli Putterman",
      "Alexander Rakhlin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/764ba7236fb63743014fafbd87dd4f0e-Abstract-Conference.html": {
    "title": "NetHack is Hard to Hack",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ulyana Piterbarg",
      "Lerrel Pinto",
      "Rob Fergus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7664a7e946a84ac5e97649a967717cf2-Abstract-Conference.html": {
    "title": "Improving Diffusion-Based Image Synthesis with Context Prediction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Yang",
      "Jingwei Liu",
      "Shenda Hong",
      "Zhilong Zhang",
      "Zhilin Huang",
      "Zheming Cai",
      "Wentao Zhang",
      "Bin CUI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/766f407b7b4a82135da23b32f0cbaff3-Abstract-Conference.html": {
    "title": "Adversarial Robustness through Random Weight Sampling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanxiang Ma",
      "Minjing Dong",
      "Chang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/767c1b5f7c03d9299e493bc9e1feeba6-Abstract-Conference.html": {
    "title": "PyNeRF: Pyramidal Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haithem Turki",
      "Michael Zollhöfer",
      "Christian Richardt",
      "Deva Ramanan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/76818d8d85e05e45ce3a16a8468619d1-Abstract-Conference.html": {
    "title": "Universal Online Learning with Gradient Variations: A Multi-layer Online Ensemble Approach",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Hu Yan",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/768396006e9214568dba5aae9dd312c5-Abstract-Conference.html": {
    "title": "Information Theoretic Lower Bounds for Information Theoretic Upper Bounds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roi Livni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7691484a7a35d5e2742279c1d926b778-Abstract-Conference.html": {
    "title": "CoDrug: Conformal Drug Property Prediction with Density Estimation under Covariate Shift",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddhartha Laghuvarapu",
      "Zhen Lin",
      "Jimeng Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/76bea0a1cf7bf9b78f842009f6de15a1-Abstract-Conference.html": {
    "title": "Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Berivan Isik",
      "Wei-Ning Chen",
      "Ayfer Ozgur",
      "Tsachy Weissman",
      "Albert No"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/76c6f9f2475b275b92d03a83ea270af4-Abstract-Conference.html": {
    "title": "Score-based Generative Modeling through Stochastic Evolution Equations in Hilbert Spaces",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungbin Lim",
      "EUN BI YOON",
      "Taehyun Byun",
      "Taewon Kang",
      "Seungwoo Kim",
      "Kyungjae Lee",
      "Sungjoon Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/76d2f8e328e1081c22a77ca0fa330ca5-Abstract-Conference.html": {
    "title": "Unlocking Feature Visualization for Deep Network with MAgnitude Constrained Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas FEL",
      "Thibaut Boissin",
      "Victor Boutin",
      "Agustin PICARD",
      "Paul Novello",
      "Julien Colin",
      "Drew Linsley",
      "Tom ROUSSEAU",
      "Remi Cadene",
      "Lore Goetschalckx",
      "Laurent Gardes",
      "Thomas Serre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/770b3ecb70147a2d2f18d2964fafcdd5-Abstract-Conference.html": {
    "title": "Exact recovery and Bregman hard clustering of node-attributed Stochastic Block Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilien Dreveton",
      "Felipe Fernandes",
      "Daniel Figueiredo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/770cabd044c4eacb6dc5924d9a686dce-Abstract-Conference.html": {
    "title": "Learning to Receive Help: Intervention-Aware Concept Embedding Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateo Espinosa Zarlenga",
      "Katie Collins",
      "Krishnamurthy Dvijotham",
      "Adrian Weller",
      "Zohreh Shams",
      "Mateja Jamnik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/771155abaae744e08576f1f3b4b7ac0d-Abstract-Conference.html": {
    "title": "Tracr: Compiled Transformers as a Laboratory for Interpretability",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Lindner",
      "Janos Kramar",
      "Sebastian Farquhar",
      "Matthew Rahtz",
      "Tom McGrath",
      "Vladimir Mikulik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7712b1075f5e0eae297702845714098f-Abstract-Conference.html": {
    "title": "KAKURENBO: Adaptively Hiding Samples in Deep Neural Network Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Truong Thao Nguyen",
      "Balazs Gerofi",
      "Edgar Josafat Martinez-Noriega",
      "François Trahay",
      "Mohamed Wahib"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7721f1fea280e9ffae528dc78c732576-Abstract-Conference.html": {
    "title": "Mixed Samples as Probes for Unsupervised Model Selection in Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dapeng Hu",
      "Jian Liang",
      "Jun Hao Liew",
      "Chuhui Xue",
      "Song Bai",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/77307e2e3f326335dfeb94ab47f7a6c0-Abstract-Conference.html": {
    "title": "Payoff-based Learning with Matrix Multiplicative Weights in Quantum Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyriakos Lotidis",
      "Panayotis Mertikopoulos",
      "Nicholas Bambos",
      "Jose Blanchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7749f9c0d5ff109231be21e910a3ced2-Abstract-Conference.html": {
    "title": "Deep Stochastic Processes via Functional Markov Transition Operators",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Xu",
      "Emilien Dupont",
      "Kaspar Märtens",
      "Thomas Rainforth",
      "Yee Whye Teh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/778ff1fcfb6d6707fc015908a1845b62-Abstract-Conference.html": {
    "title": "A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Tyurin",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/77b5aaf2826c95c98e5eb4ab830073de-Abstract-Conference.html": {
    "title": "Optimistic Active Exploration of Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavya ",
      "Lenart Treven",
      "Cansu Sancaktar",
      "Sebastian Blaes",
      "Stelian Coros",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/77c33e6a367922d003ff102ffb92b658-Abstract-Conference.html": {
    "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongliang Shen",
      "Kaitao Song",
      "Xu Tan",
      "Dongsheng Li",
      "Weiming Lu",
      "Yueting Zhuang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/77c7faab15002432ba1151e8d5cc389a-Abstract-Conference.html": {
    "title": "Multi-Step Generalized Policy Improvement by Leveraging Approximate Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas N. Alegre",
      "Ana Bazzan",
      "Ann Nowe",
      "Bruno C. da Silva"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/77cf940349218069bbc230fc2c9c8a21-Abstract-Conference.html": {
    "title": "GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sima Behpour",
      "Thang Long Doan",
      "Xin Li",
      "Wenbin He",
      "Liang Gou",
      "Liu Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/77e59fafe99e94f822e79bf9308ec377-Abstract-Conference.html": {
    "title": "Learning to Modulate pre-trained Models in RL",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Schmied",
      "Markus Hofmarcher",
      "Fabian Paischer",
      "Razvan Pascanu",
      "Sepp Hochreiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/77fa0e7d45c6687f1958de0b31e9fc05-Abstract-Conference.html": {
    "title": "Injecting Multimodal Information into Rigid Protein Docking via Bi-level Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijia Wang",
      "YiWu Sun",
      "Yujie Luo",
      "Shaochuan Li",
      "Cheng Yang",
      "Xingyi Cheng",
      "Hui Li",
      "Chuan Shi",
      "Le Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/78526d7ad4a2532bd91416e948b9644c-Abstract-Conference.html": {
    "title": "Uncertainty-Aware Alignment Network for Cross-Domain Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoshuai Hao",
      "Wanqian Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7866ff509c822c2e58d20d00154a15a2-Abstract-Conference.html": {
    "title": "Can Pre-Trained Text-to-Image Models Generate Visual Goals for Reinforcement Learning?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialu Gao",
      "Kaizhe Hu",
      "Guowei Xu",
      "Huazhe Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7886b89aced4d37dd25a6f32854bf3f9-Abstract-Conference.html": {
    "title": "H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhong Wang",
      "Xu Han",
      "Weilin Zhao",
      "Guoyang Zeng",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/788e086c07b8d6fa6b279df56e512312-Abstract-Conference.html": {
    "title": "Binarized Spectral Compressive Imaging",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Cai",
      "Yuxin Zheng",
      "Jing Lin",
      "Xin Yuan",
      "Yulun Zhang",
      "Haoqian Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/78ccee9dfbcf84840165ab4093715969-Abstract-Conference.html": {
    "title": "When Can We Track Significant Preference Shifts in Dueling Bandits?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joe Suk",
      "Arpit Agarwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/78efbc5386c5a7c241e7fcc482d3c3dc-Abstract-Conference.html": {
    "title": "Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitz Sáez de Ocáriz Borde",
      "Alvaro Arroyo",
      "Ismael Morales",
      "Ingmar Posner",
      "Xiaowen Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7900318ffaf5e9bc60250f134c6cc3c7-Abstract-Conference.html": {
    "title": "Beyond Confidence: Reliable Models Should Also Consider Atypicality",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mert Yuksekgonul",
      "Linjun Zhang",
      "James Y. Zou",
      "Carlos Guestrin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7903af0a1cffb43dbb2f8160d110a5f3-Abstract-Conference.html": {
    "title": "Reversible and irreversible bracket-based dynamics for deep graph neural networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anthony Gruber",
      "Kookjin Lee",
      "Nathaniel Trask"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/791d3337291b2c574545aeecfa75484c-Abstract-Conference.html": {
    "title": "In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhou Cao",
      "Hussein Mozannar",
      "Lei Feng",
      "Hongxin Wei",
      "Bo An"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/79206ac5b7e88eeeed74997f3b6f4c7f-Abstract-Conference.html": {
    "title": "Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyan Huang",
      "Zhiqi Li",
      "Chonghao Sima",
      "Wenhai Wang",
      "Jingdong Wang",
      "Yu Qiao",
      "Hongyang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/79358587d84628728199059f648824e6-Abstract-Conference.html": {
    "title": "No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiancheng Jin",
      "Junyan Liu",
      "Chloé Rouyer",
      "William Chang",
      "Chen-Yu Wei",
      "Haipeng Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/796455f65fd2cbe049112a2d2d4488cb-Abstract-Conference.html": {
    "title": "Generalizable Lightweight Proxy for Robust NAS against Diverse Perturbations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonjeong Ha",
      "Minseon Kim",
      "Sung Ju Hwang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/797be96e4481c3fe5d675c1ba5352969-Abstract-Conference.html": {
    "title": "Ignorance is Bliss: Robust Control via Information Gating",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manan Tomar",
      "Riashat Islam",
      "Matthew  Taylor ",
      "Sergey Levine",
      "Philip Bachman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7984e22a06eb5f0e35d745cb38345983-Abstract-Conference.html": {
    "title": "Reduced Policy Optimization for Continuous Control with Hard Constraints",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shutong Ding",
      "Jingya Wang",
      "Yali Du",
      "Ye Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7988e9b3876ad689e921ce05d711442f-Abstract-Conference.html": {
    "title": "ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Xu",
      "Zheng Lian",
      "Lei Feng",
      "Bin Liu",
      "Jianhua Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/799f81cfa0611f93586c007024041460-Abstract-Conference.html": {
    "title": "Conditional Score Guidance for Text-Driven Image-to-Image Translation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsoo Lee",
      "Minsoo Kang",
      "Bohyung Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/79a0c8e7ae8e403e39341ea6b0ba4c21-Abstract-Conference.html": {
    "title": "A Unified Approach to Count-Based Weakly Supervised Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinay Shukla",
      "Zhe Zeng",
      "Kareem Ahmed",
      "Guy Van den Broeck"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/79ba1b827d3fc58e129d1cbfc8ff69f2-Abstract-Conference.html": {
    "title": "Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyue Wen",
      "Yuchen Li",
      "Bingbin Liu",
      "Andrej Risteski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/79cab89b43ac21c6941ad9735df95d30-Abstract-Conference.html": {
    "title": "GEQ: Gaussian Kernel Inspired Equilibrium Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjie Li",
      "Yisen  Wang",
      "Zhouchen Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/79f7f00cbe3003cea4d0c2326b4c0b42-Abstract-Conference.html": {
    "title": "Efficient Potential-based Exploration in Reinforcement Learning using Inverse Dynamic Bisimulation Metric",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Wang",
      "Ming Yang",
      "Renzhi Dong",
      "Binbin Sun",
      "Furui Liu",
      "Leong Hou U"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/79fea214543ba263952ac3f4e5452b14-Abstract-Conference.html": {
    "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joy Hsu",
      "Jiayuan Mao",
      "Josh Tenenbaum",
      "Jiajun Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7a0f7e9d9b42b26e5bfc9ba4c6e5287c-Abstract-Conference.html": {
    "title": "Recovering from Out-of-sample States via Inverse Dynamics in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Jiang",
      "Jia-Yu Yao",
      "Xiaoyang Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7a62d9a4c03377d1175b8859b4cc16d4-Abstract-Conference.html": {
    "title": "TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongkun Zheng",
      "Lu Qi",
      "Xi Chen",
      "Yi Wang",
      "Kun Wang",
      "Yu Qiao",
      "Hengshuang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7a69ab48efcbb0153e72d458fb091969-Abstract-Conference.html": {
    "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "AJAY JAISWAL",
      "Shiwei Liu",
      "Tianlong Chen",
      "Zhangyang \"Atlas\" Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7a7f6cc5dc2a84fb4edf0feb8e5cfd50-Abstract-Conference.html": {
    "title": "Hypervolume Maximization: A Geometric View of Pareto Set Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyuan Zhang",
      "Xi Lin",
      "Bo Xue",
      "Yifan Chen",
      "Qingfu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7a8d388b7a17df480856dff1cc079b08-Abstract-Conference.html": {
    "title": "Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koen Minartz",
      "Yoeri Poels",
      "Simon Koop",
      "Vlado Menkovski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7a8fa1382ea068f3f402b72081df16be-Abstract-Conference.html": {
    "title": "Collaborative Alignment of NLP Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fereshte Khani",
      "Marco Tulio Ribeiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7abbcb05a5d55157ede410bb718e32d7-Abstract-Conference.html": {
    "title": "Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yule Wang",
      "Zijing Wu",
      "Chengrui Li",
      "Anqi Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7ac19fdcdf4f311f3e3ef2e7ef4784d7-Abstract-Conference.html": {
    "title": "Closing the gap between the upper bound and lower bound of Adam's iteration complexity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Wang",
      "Jingwen Fu",
      "Huishuai Zhang",
      "Nanning Zheng",
      "Wei Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7ac484b0f1a1719ad5be9aa8c8455fbb-Abstract-Conference.html": {
    "title": "Deep Patch Visual Odometry",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Teed",
      "Lahav Lipson",
      "Jia Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7af8e3dfefe6e3141144197b8fa44f79-Abstract-Conference.html": {
    "title": "Isometric Quotient Variational Auto-Encoders for Structure-Preserving Representation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "In Huh",
      "changwook jeong",
      "Jae Myung Choe",
      "YOUNGGU KIM",
      "Daesin Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7b2e844c52349134268e819a9b56b9e8-Abstract-Conference.html": {
    "title": "Fast Partitioned Learned Bloom Filter",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsuki Sato",
      "Yusuke Matsui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7b35a69f434b5eb07ed1b1ef16ace52c-Abstract-Conference.html": {
    "title": "Instructing Goal-Conditioned Reinforcement Learning Agents with Temporal Logic Objectives",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjie Qiu",
      "Wensen Mao",
      "He Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7b5ae891000049b91b3b62de596b1560-Abstract-Conference.html": {
    "title": "Neural Multi-Objective Combinatorial Optimization with Diversity Enhancement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinbiao Chen",
      "Zizhen Zhang",
      "Zhiguang Cao",
      "Yaoxin Wu",
      "Yining Ma",
      "Te Ye",
      "Jiahai Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7b64c47dcb067efd6be5eee854c14835-Abstract-Conference.html": {
    "title": "Multi-Agent First Order Constrained Optimization in Policy Space",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youpeng Zhao",
      "Yaodong Yang",
      "Zhenbo Lu",
      "Wengang Zhou",
      "Houqiang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7b76eea0c3683e440c3d362620f578cd-Abstract-Conference.html": {
    "title": "This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiyu Ma",
      "Brandon Zhao",
      "Chaofan Chen",
      "Cynthia Rudin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7b97adeafa1c51cf65263459ca9d0d7c-Abstract-Conference.html": {
    "title": "Speculative Decoding with Big Little Decoder",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sehoon Kim",
      "Karttikeya Mangalam",
      "Suhong Moon",
      "Jitendra Malik",
      "Michael W. Mahoney",
      "Amir Gholami",
      "Kurt Keutzer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7baa48bc166aa2013d78cbdc15010530-Abstract-Conference.html": {
    "title": "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduard Tulchinskii",
      "Kristian Kuznetsov",
      "Laida Kushnareva",
      "Daniil Cherniavskii",
      "Sergey Nikolenko",
      "Evgeny Burnaev",
      "Serguei Barannikov",
      "Irina Piontkovskaya"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7bc3fe234454107149fa9d44faacaa64-Abstract-Conference.html": {
    "title": "Replicable Clustering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Esfandiari",
      "Amin Karbasi",
      "Vahab Mirrokni",
      "Grigoris Velegkas",
      "Felix Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7bc4f74e35bcfe8cfe43b0a860786d6a-Abstract-Conference.html": {
    "title": "Counterfactual Memorization in Neural Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiyuan Zhang",
      "Daphne Ippolito",
      "Katherine Lee",
      "Matthew Jagielski",
      "Florian Tramer",
      "Nicholas Carlini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7bd4a7d0e6773072c2e3c77b11d93065-Abstract-Conference.html": {
    "title": "Learning Generalizable Agents via Saliency-guided Features Decorrelation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sili Huang",
      "Yanchao Sun",
      "Jifeng Hu",
      "Siyuan Guo",
      "Hechang Chen",
      "Yi Chang",
      "Lichao Sun",
      "Bo Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7bdd36a198a8408f444834039b09f518-Abstract-Conference.html": {
    "title": "You Only Condense Once: Two Rules for Pruning Condensed Datasets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang He",
      "Lingao Xiao",
      "Joey Tianyi Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7bf3e93543a612b75b6373178ba1faa4-Abstract-Conference.html": {
    "title": "Provably Efficient Offline Reinforcement Learning in Regular Decision Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roberto Cipollone",
      "Anders Jonsson",
      "Alessandro Ronca",
      "Mohammad Sadegh Talebi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7c10e259c7e56fa218ee03d9ae7d728e-Abstract-Conference.html": {
    "title": "CP-SLAM: Collaborative Neural Point-based SLAM System",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Hu",
      "Mao Mao",
      "Hujun Bao",
      "Guofeng Zhang",
      "Zhaopeng Cui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7c119415672ae2186e17d492e1d5da2f-Abstract-Conference.html": {
    "title": "The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurabh Saxena",
      "Charles Herrmann",
      "Junhwa Hur",
      "Abhishek Kar",
      "Mohammad Norouzi",
      "Deqing Sun",
      "David J. Fleet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7c319b62e2257b34cb0e1040ced2e007-Abstract-Conference.html": {
    "title": "Efficient Testable Learning of Halfspaces with Adversarial Label Noise",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Daniel Kane",
      "Vasilis Kontonis",
      "Sihan Liu",
      "Nikos Zarifis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7c3a8d20ceadb7c519e9ac1bb77a15ff-Abstract-Conference.html": {
    "title": "Achieving $\\mathcal{O}(\\epsilon^{-1.5})$ Complexity in Hessian/Jacobian-free Stochastic Bilevel Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Yang",
      "Peiyao Xiao",
      "Kaiyi Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7c5a4b7a31dffef8ce296deedb6214a9-Abstract-Conference.html": {
    "title": "Robust and Actively Secure Serverless Collaborative Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Franzese",
      "Adam Dziedzic",
      "Christopher A. Choquette-Choo",
      "Mark R Thomas",
      "Muhammad Ahmad Kaleem",
      "Stephan Rabanser",
      "Congyu Fang",
      "Somesh Jha",
      "Nicolas Papernot",
      "Xiao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7c72fcd7b6bffc3864c7152ab5a2dd83-Abstract-Conference.html": {
    "title": "Birder: Communication-Efficient 1-bit Adaptive Optimizer for Practical Distributed DNN Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Peng",
      "Shuang Qin",
      "Yue Yu",
      "Jin Wang",
      "Hui Wang",
      "Ge Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7c7a12559be4501f70d221352514397c-Abstract-Conference.html": {
    "title": "MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Menet",
      "Michael Hersche",
      "Geethan Karunaratne",
      "Luca Benini",
      "Abu Sebastian",
      "Abbas Rahimi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7ca55c8276acf1f0aa996cd3622d1df4-Abstract-Conference.html": {
    "title": "C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Liu",
      "Jiaxin Yuan",
      "Bang An",
      "Yuancheng Xu",
      "Yifan Yang",
      "Furong Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7caf9d251b546bc78078b35b4a6f3b7e-Abstract-Conference.html": {
    "title": "Representation Learning via Consistent Assignment of Views over Random Partitions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thalles Santos Silva",
      "Adín Ramírez Rivera"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7cb2c2a8d35576c00078b6591ec26a7d-Abstract-Conference.html": {
    "title": "Federated Multi-Objective Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haibo Yang",
      "Zhuqing Liu",
      "Jia Liu",
      "Chaosheng Dong",
      "Michinari Momma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7cc1005ec73cfbaac9fa21192b622507-Abstract-Conference.html": {
    "title": "Language Models can Solve Computer Tasks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geunwoo Kim",
      "Pierre Baldi",
      "Stephen McAleer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7cde9bd7774c9f5056cb6e5474fbadff-Abstract-Conference.html": {
    "title": "Robustness Guarantees for Adversarially Trained Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Poorya Mianjy",
      "Raman Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7ce1cbededb4b0d6202847ac1b484ee8-Abstract-Conference.html": {
    "title": "Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialong Wu",
      "Haoyu Ma",
      "Chaoyi Deng",
      "Mingsheng Long"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7cefded8659ccc899196860af674b596-Abstract-Conference.html": {
    "title": "Strategyproof Voting under Correlated Beliefs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Halpern",
      "Rachel Li",
      "Ariel D. Procaccia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d0e867582cdc156fd280d5a6aa1be08-Abstract-Conference.html": {
    "title": "PCF-GAN: generating sequential data via the characteristic function of measures on the path space",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Lou",
      "Siran Li",
      "Hao Ni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d25b1db211d99d5750ec45d65fd6e4e-Abstract-Conference.html": {
    "title": "A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Veit David Wild",
      "Sahra Ghalebikesabi",
      "Dino Sejdinovic",
      "Jeremias Knoblauch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d2b770c3ccd35b41c9453ef6f8765a3-Abstract-Conference.html": {
    "title": "Markovian Sliced Wasserstein Distances: Beyond Independent Projections",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khai Nguyen",
      "Tongzheng Ren",
      "Nhat Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d4c0094ae32530494c71468558ab5b1-Abstract-Conference.html": {
    "title": "Generative Modelling of Stochastic Actions with Arbitrary Constraints in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changyu CHEN",
      "Ramesha Karunasena",
      "Thanh Nguyen",
      "Arunesh Sinha",
      "Pradeep Varakantham"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d535a224c8ae54ba75bac0457b6b279-Abstract-Conference.html": {
    "title": "On the impact of activation and normalization in obtaining isometric embeddings at initialization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Joudaki",
      "Hadi Daneshmand",
      "Francis Bach"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d60bfd8458b67acbbaf18b892338d00-Abstract-Conference.html": {
    "title": "Uni3DETR: Unified 3D Detection Transformer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wang",
      "Ya-Li Li",
      "Xi Chen",
      "Hengshuang Zhao",
      "Shengjin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d62a85ebfed2f680eb5544beae93191-Abstract-Conference.html": {
    "title": "SceneScape: Text-Driven Consistent Scene Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafail Fridman",
      "Amit Abecasis",
      "Yoni Kasten",
      "Tali Dekel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d640f377893fc5f22b5610e175ef7c3-Abstract-Conference.html": {
    "title": "RDumb: A simple approach that questions our progress in continual test-time adaptation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ori Press",
      "Steffen Schneider",
      "Matthias Kümmerer",
      "Matthias Bethge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d693203215325902ff9dbdd067a50ac-Abstract-Conference.html": {
    "title": "Swap Agnostic Learning, or Characterizing Omniprediction via Multicalibration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parikshit Gopalan",
      "Michael Kim",
      "Omer Reingold"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html": {
    "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wu",
      "Zhihao Fan",
      "Xiao Liu",
      "Hai-Tao Zheng",
      "Yeyun Gong",
      "yelong shen",
      "Jian Jiao",
      "Juntao Li",
      "zhongyu wei",
      "Jian Guo",
      "Nan Duan",
      "Weizhu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7da558c6bd476ba77f5ba712626bba1a-Abstract-Conference.html": {
    "title": "Adaptive Uncertainty Estimation via High-Dimensional Testing on Latent Representations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsai Hor Chan",
      "Kin Wai Lau",
      "Jiajun Shen",
      "Guosheng Yin",
      "Lequan Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7db2348b5bfeca620aa7327df815adcc-Abstract-Conference.html": {
    "title": "Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziye Ma",
      "Javad Lavaei",
      "Somayeh Sojoudi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7dc7793c89b93887e126a86f22ef63c6-Abstract-Conference.html": {
    "title": "A General Theory of Correct, Incorrect, and Extrinsic Equivariance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dian Wang",
      "Xupeng Zhu",
      "Jung Yeon Park",
      "Mingxi Jia",
      "Guanang Su",
      "Robert Platt",
      "Robin Walters"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7dd309df03d37643b96f5048b44da798-Abstract-Conference.html": {
    "title": "Analyzing Vision Transformers for Image Classification in Class Embedding Space",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martina G. Vilas",
      "Timothy Schaumlöffel",
      "Gemma Roig"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7df69dbf39705c7a39b40f2d70e806c1-Abstract-Conference.html": {
    "title": "Toward Re-Identifying Any Animal",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingliang Jiao",
      "Lingqiao Liu",
      "Liying Gao",
      "Ruiqi Wu",
      "Guosheng Lin",
      "PENG WANG",
      "Yanning Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e02f2910ea7911a37c4691f4201c878-Abstract-Conference.html": {
    "title": "Critical Initialization of Wide and Deep Neural Networks using Partial Jacobians: General Theory and Applications",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darshil Doshi",
      "Tianyu He",
      "Andrey Gromov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e0af0d1bc0ec2a90fc294be2e00447e-Abstract-Conference.html": {
    "title": "Trading-off price for data quality to achieve fair online allocation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Molina",
      "Nicolas Gast",
      "Patrick Loiseau",
      "Vianney Perchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e16384b94a1c7e4462a70bb8fb93ca9-Abstract-Conference.html": {
    "title": "Asymptotics of Bayesian Uncertainty Estimation in Random Features Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngsoo Baek",
      "Samuel Berchuck",
      "Sayan Mukherjee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e6f445a74cdb71931aac64f1e3f49c9-Abstract-Conference.html": {
    "title": "Decentralized Matrix Sensing: Statistical Guarantees and Fast Convergence",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marie Maros",
      "Gesualdo Scutari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e7b768198d24d883d69704eee57efb0-Abstract-Conference.html": {
    "title": "Dynamics Generalisation in Reinforcement Learning via Adaptive Context-Aware Policies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Beukman",
      "Devon Jarvis",
      "Richard Klein",
      "Steven James",
      "Benjamin Rosman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e810b2c75d69be186cadd2fe3febeab-Abstract-Conference.html": {
    "title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Zhong",
      "Peter Zhang",
      "Steve Li",
      "Jinwoo Ahn",
      "Dan Klein",
      "Jacob Steinhardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e8bb8d17bb1cb24dfe972a2f8ff2500-Abstract-Conference.html": {
    "title": "Convex and Non-convex Optimization Under Generalized Smoothness",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochuan Li",
      "Jian Qian",
      "Yi Tian",
      "Alexander Rakhlin",
      "Ali Jadbabaie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e966a12c2d6307adb8809aaa9acf057-Abstract-Conference.html": {
    "title": "DOSE: Diffusion Dropout with Adaptive Prior for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxin Tai",
      "Yue Lei",
      "Fan Zhou",
      "Goce Trajcevski",
      "Ting Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e976afe805026f7d378a583af5ea9a2-Abstract-Conference.html": {
    "title": "ISP: Multi-Layered Garment Draping with Implicit Sewing Patterns",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ren Li",
      "Benoit Guillard",
      "Pascal Fua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7e991aa4cd2fdf0014fba2f000f542d0-Abstract-Conference.html": {
    "title": "Optimality of Message-Passing Architectures for Sparse Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aseem Baranwal",
      "Kimon Fountoulakis",
      "Aukosh Jagannath"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7ea46207ec9bda974b140fe11d8dd727-Abstract-Conference.html": {
    "title": "Distribution-Free Statistical Dispersion Control for Societal Applications",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhun Deng",
      "Thomas Zollo",
      "Jake Snell",
      "Toniann Pitassi",
      "Richard Zemel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7eeb42802d3750ca59e8a0523068e9e6-Abstract-Conference.html": {
    "title": "Switching Temporary Teachers for Semi-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemin Na",
      "Jung-Woo Ha",
      "Hyung Jin Chang",
      "Dongyoon Han",
      "Wonjun Hwang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7eed2822411dc37b3768ae04561caafa-Abstract-Conference.html": {
    "title": "Extremal Domain Translation with Neural Optimal Transport",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milena Gazdieva",
      "Alexander Korotin",
      "Daniil Selikhanovych",
      "Evgeny Burnaev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f05193e5487287a890df7fbc3554427-Abstract-Conference.html": {
    "title": "Recaptured Raw Screen Image and Video Demoiréing via Channel and Spatial Modulations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijia Cheng",
      "Xin Liu",
      "Jingyu Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f2223201858b6ff4cc1832d8856459b-Abstract-Conference.html": {
    "title": "On Imitation in Mean-field Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giorgia Ramponi",
      "Pavel Kolev",
      "Olivier Pietquin",
      "Niao He",
      "Mathieu Lauriere",
      "Matthieu Geist"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f2fc4053a66edfa430bcdf9a6ff3b17-Abstract-Conference.html": {
    "title": "CluB: Cluster Meets BEV for LiDAR-Based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingjie Wang",
      "Jiajun Deng",
      "Yuenan Hou",
      "Yao Li",
      "Yu Zhang",
      "Jianmin Ji",
      "Wanli Ouyang",
      "Yanyong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f64034009f4a5fa417a57e1a987c5cd-Abstract-Conference.html": {
    "title": "Probabilistic Exponential Integrators",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathanael Bosch",
      "Philipp Hennig",
      "Filip Tronarp"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f70331dbe58ad59d83941dfa7d975aa-Abstract-Conference.html": {
    "title": "Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Lu",
      "Yaoliang Yu",
      "Xinlin Li",
      "Vahid Partovi Nia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f77492bb8070a5c825a87c0c5181da2-Abstract-Conference.html": {
    "title": "CorresNeRF: Image Correspondence Priors for Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixing Lao",
      "Xiaogang Xu",
      "zhipeng cai",
      "Xihui Liu",
      "Hengshuang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f7fa581cc8a1970a4332920cdf87395-Abstract-Conference.html": {
    "title": "Score-based Data Assimilation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "François Rozet",
      "Gilles Louppe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f8b8bc8ebac661c442c4dafd5d98c08-Abstract-Conference.html": {
    "title": "Sharp Bounds for Generalized Causal Sensitivity Analysis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dennis Frauen",
      "Valentyn Melnychuk",
      "Stefan Feuerriegel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7fa46657df480226112d5be3faf096c4-Abstract-Conference.html": {
    "title": "Supported Value Regularization for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixiu Mao",
      "Hongchang Zhang",
      "Chen Chen",
      "Yi Xu",
      "Xiangyang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7fbae0a0885d3d688840bd34e4a8a698-Abstract-Conference.html": {
    "title": "Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingying Fan",
      "Yu Wu",
      "Bo Du",
      "Yutian Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7fe3170d88a8310ca86df2843f54236c-Abstract-Conference.html": {
    "title": "Maximum Independent Set: Self-Training through Dynamic Programming",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Brusca",
      "Lars C.P.M. Quaedvlieg",
      "Stratis Skoulakis",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7ffb2b550ff6a75c536b279348a93fb0-Abstract-Conference.html": {
    "title": "Reference-Based POMDPs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edward Kim",
      "Yohan Karunanayake",
      "Hanna Kurniawati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7ffb9f1b57628932518505b532301603-Abstract-Conference.html": {
    "title": "Siamese Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Agrim Gupta",
      "Jiajun Wu",
      "Jia Deng",
      "Fei-Fei Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8011b23e1dc3f57e1b6211ccad498919-Abstract-Conference.html": {
    "title": "Score-based Generative Models with Lévy Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "EUN BI YOON",
      "Keehun Park",
      "Sungwoong Kim",
      "Sungbin Lim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/801750bc49fdc3d498e9ee63479f315e-Abstract-Conference.html": {
    "title": "3D Indoor Instance Segmentation in an Open-World",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed El Amine Boudjoghra",
      "Salwa Al Khatib",
      "Jean Lahoud",
      "Hisham Cholakkal",
      "Rao Anwer",
      "Salman H. Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/801ec05b0aae9fcd2ef35c168bd538e0-Abstract-Conference.html": {
    "title": "AbDiffuser: full-atom generation of in-vitro functioning antibodies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karolis Martinkus",
      "Jan Ludwiczak",
      "WEI-CHING LIANG",
      "Julien Lafrance-Vanasse",
      "Isidro Hotzel",
      "Arvind Rajpal",
      "Yan Wu",
      "Kyunghyun Cho",
      "Richard Bonneau",
      "Vladimir Gligorijevic",
      "Andreas Loukas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8027ace571384361920665f1d1b69758-Abstract-Conference.html": {
    "title": "Structure Learning with Adaptive Random Neighborhood Informed MCMC",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xitong Liang",
      "Alberto Caron",
      "Samuel Livingstone",
      "Jim Griffin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/802a4350ca4fced76b13b8b320af1543-Abstract-Conference.html": {
    "title": "Reining Generalization in Offline Reinforcement Learning via Representation Distinction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Ma",
      "Hongyao Tang",
      "Dong Li",
      "Zhaopeng Meng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/802e90325f4c8546e13e5763b2ecab88-Abstract-Conference.html": {
    "title": "BIRD: Generalizable Backdoor Detection and Removal for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Chen",
      "Wenbo Guo",
      "Guanhong Tao",
      "Xiangyu Zhang",
      "Dawn Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8037f47a6254eb60899a644bd90b4f6a-Abstract-Conference.html": {
    "title": "Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijun Dong",
      "Kevin Miller",
      "Qi Lei",
      "Rachel Ward"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8039ca1e9860daab3a79e45d010d5398-Abstract-Conference.html": {
    "title": "Bicriteria Multidimensional Mechanism Design with Side Information",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddharth Prasad",
      "Maria-Florina F. Balcan",
      "Tuomas Sandholm"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/804b5e300c9ed4e3ea3b073f186f4adc-Abstract-Conference.html": {
    "title": "Exploring Diverse In-Context Configurations for Image Captioning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Yang",
      "Yongliang Wu",
      "Mingzhuo Yang",
      "Haokun Chen",
      "Xin Geng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/805c06617d2b643278936daadfde4280-Abstract-Conference.html": {
    "title": "DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngjoong Kwon",
      "Lingjie Liu",
      "Henry Fuchs",
      "Marc Habermann",
      "Christian Theobalt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8078e8c3055303a884ffae2d3ea00338-Abstract-Conference.html": {
    "title": "Zero-Shot Anomaly Detection via Batch Normalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aodong Li",
      "Chen Qiu",
      "Marius Kloft",
      "Padhraic Smyth",
      "Maja Rudolph",
      "Stephan Mandt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/808a79d149c9dd8338d789881c9dab4c-Abstract-Conference.html": {
    "title": "What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "‪Yotam Alexander‬‏",
      "Nimrod De La Vega",
      "Noam Razin",
      "Nadav Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/80e354fdac2c7fbf439a51f4853edbac-Abstract-Conference.html": {
    "title": "Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiong Wu",
      "Wei Yu",
      "Yiyi Zhou",
      "Shubin Huang",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/80f253dcb51cd2af7ce54e9379fb3521-Abstract-Conference.html": {
    "title": "A Dynamical System View of Langevin-Based Non-Convex Sampling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Reza Karimi Jaghargh",
      "Ya-Ping Hsieh",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/80f48ffa8022773973a4a5cec7cce19c-Abstract-Conference.html": {
    "title": "OKRidge: Scalable Optimal k-Sparse Ridge Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachang Liu",
      "Sam Rosen",
      "Chudi Zhong",
      "Cynthia Rudin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/80fe51a7d8d0c73ff7439c2a2554ed53-Abstract-Conference.html": {
    "title": "Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arpit Bansal",
      "Eitan Borgnia",
      "Hong-Min Chu",
      "Jie Li",
      "Hamid Kazemi",
      "Furong Huang",
      "Micah Goldblum",
      "Jonas Geiping",
      "Tom Goldstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8143b8c73073a9a23b9c18e400066471-Abstract-Conference.html": {
    "title": "Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongrui Liu",
      "Huiqi Deng",
      "Xu Cheng",
      "Qihan Ren",
      "Kangrui Wang",
      "Quanshi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8154c89c8d3612d39fd1ed6a20f4bab1-Abstract-Conference.html": {
    "title": "Limits, approximation and size transferability for GNNs on sparse graphs via graphops",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thien Le",
      "Stefanie Jegelka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/81858558b55a8c63763cfe088090242a-Abstract-Conference.html": {
    "title": "The Adversarial Consistency of Surrogate Risks for Binary Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natalie Frank",
      "Jonathan Niles-Weed"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/819cebb05f993840e8a52d7564c5c282-Abstract-Conference.html": {
    "title": "Large Language Models of Code Fail at Completing Code with Potential Bugs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan Dinh",
      "Jinman Zhao",
      "Samson Tan",
      "Renato Negrinho",
      "Leonard Lausen",
      "Sheng Zha",
      "George Karypis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/819f426947c27eb5067bb6fdbdde93dd-Abstract-Conference.html": {
    "title": "Doubly-Robust Self-Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Banghua Zhu",
      "Mingyu Ding",
      "Philip Jacobson",
      "Ming Wu",
      "Wei Zhan",
      "Michael Jordan",
      "Jiantao Jiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/81a12aed87eb9c75dfdf91ed99d5519d-Abstract-Conference.html": {
    "title": "FairLISA: Fair User Modeling with Limited Sensitive Attributes Information",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zheng zhang",
      "Qi Liu",
      "Hao Jiang",
      "Fei Wang",
      "Yan Zhuang",
      "Le Wu",
      "Weibo Gao",
      "Enhong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html": {
    "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenneth Li",
      "Oam Patel",
      "Fernanda Viégas",
      "Hanspeter Pfister",
      "Martin Wattenberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/81c565e605161fcf25d08aa230431eba-Abstract-Conference.html": {
    "title": "Composable Coresets for Determinant Maximization: Greedy is Almost Optimal",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siddharth Gollapudi",
      "Sepideh Mahabadi",
      "Varun Sivashankar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/81cca94f16f20d5548c76c3344b27dea-Abstract-Conference.html": {
    "title": "Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mazda Moayeri",
      "Wenxiao Wang",
      "Sahil Singla",
      "Soheil Feizi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/81e1cdaa570954321d8b06be386cc3d4-Abstract-Conference.html": {
    "title": "NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokil Ham",
      "Jungwuk Park",
      "Dong-Jun Han",
      "Jaekyun Moon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/81fde95c4dc79188a69ce5b24d63010b-Abstract-Conference.html": {
    "title": "Self-Evaluation Guided Beam Search for Reasoning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Xie",
      "Kenji Kawaguchi",
      "Yiran Zhao",
      "James Xu Zhao",
      "Min-Yen Kan",
      "Junxian He",
      "Michael Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8203a5156918d467328d5a90147ab307-Abstract-Conference.html": {
    "title": "ViSt3D: Video Stylization with 3D CNN",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Pande",
      "Gaurav Sharma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/82096f4f6f897529ecd3eabea603e9cc-Abstract-Conference.html": {
    "title": "Smoothed Online Learning for Prediction in Piecewise Affine Systems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Block",
      "Max Simchowitz",
      "Russ Tedrake"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/820e42f39773c6cbbd875553db45658f-Abstract-Conference.html": {
    "title": "Adversarial Attacks on Online Learning to Rank with Click Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhang Zuo",
      "Zhiyao Zhang",
      "Zhiyong Wang",
      "Shuai Li",
      "Mohammad Hajiesmaili",
      "Adam Wierman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/821655c7dc4836838cd8524d07f9d6fd-Abstract-Conference.html": {
    "title": "RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyue Xue",
      "Guanglu Song",
      "Qiushan Guo",
      "Boxiao Liu",
      "Zhuofan Zong",
      "Yu Liu",
      "Ping Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/823e43f5537d8c1894afd1f6ab00a927-Abstract-Conference.html": {
    "title": "Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhang Li",
      "Yiwen Guo",
      "Wangmeng Zuo",
      "Hao Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8249b30d877c91611fd8c7aa6ac2b5fe-Abstract-Conference.html": {
    "title": "Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Bu",
      "Yu-Xiang Wang",
      "Sheng Zha",
      "George Karypis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8278a2e5f9db8489cd908d20c43f1f87-Abstract-Conference.html": {
    "title": "Error Discovery By Clustering Influence Embeddings",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fulton Wang",
      "Julius Adebayo",
      "Sarah Tan",
      "Diego Garcia-Olano",
      "Narine Kokhlikyan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/828bb8f42d4ab15322b9315151959c61-Abstract-Conference.html": {
    "title": "BadTrack: A Poison-Only Backdoor Attack on Visual Object Tracking",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Huang",
      "Jiaqian Yu",
      "Yiwei Chen",
      "Siyang Pan",
      "Qiang Wang",
      "Zhi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8296d5800a8e68e58ad0472b393be80e-Abstract-Conference.html": {
    "title": "Spiking PointNet: Spiking Neural Networks for Point Clouds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayong Ren",
      "Zhe Ma",
      "Yuanpei Chen",
      "Weihang Peng",
      "Xiaode Liu",
      "Yuhan Zhang",
      "Yufei Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/82aec8518602748540a42b783468c94d-Abstract-Conference.html": {
    "title": "A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing Time",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ranran Shen",
      "Pan Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/82d3258eb58ceac31744a88005b7ddef-Abstract-Conference.html": {
    "title": "Boosting with Tempered Exponential Measures",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Nock",
      "Ehsan Amid",
      "Manfred Warmuth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/82d7d58cba24731c0ca952dff1de46ae-Abstract-Conference.html": {
    "title": "DP-HyPO: An Adaptive Private Framework for Hyperparameter Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hua Wang",
      "Sheng Gao",
      "Huanyu Zhang",
      "Weijie Su",
      "Milan Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/82eec786fdfbbfa53450c5feb7d1ac92-Abstract-Conference.html": {
    "title": "Active Learning-Based Species Range Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Lange",
      "Elijah Cole",
      "Grant Horn",
      "Oisin Mac Aodha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/82f05a105c928c10706213952bf0c8b7-Abstract-Conference.html": {
    "title": "One-Step Diffusion Distillation via Deep Equilibrium Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyang Geng",
      "Ashwini Pokle",
      "J. Zico Kolter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/82f0dae85424eb743017c90380e7ab9b-Abstract-Conference.html": {
    "title": "Discrete-Smoothness in Online Algorithms with Predictions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yossi Azar",
      "Debmalya Panigrahi",
      "Noam Touitou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8305a0049227f7dd2bb91e11090f8cfa-Abstract-Conference.html": {
    "title": "Riemannian Projection-free Online Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Hu",
      "Guanghui Wang",
      "Jacob D. Abernethy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8313b1920ee9c78d846c5798c1ce48be-Abstract-Conference.html": {
    "title": "SwiFT: Swin 4D fMRI Transformer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Kim",
      "Junbeom Kwon",
      "Sunghwan Joo",
      "Sangyoon Bae",
      "Donggyu Lee",
      "Yoonho Jung",
      "Shinjae Yoo",
      "Jiook Cha",
      "Taesup Moon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/831406cfe7e4a0aed5ac5c8a8389d1f5-Abstract-Conference.html": {
    "title": "Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giannis Daras",
      "Yuval Dagan",
      "Alex Dimakis",
      "Constantinos Daskalakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8342218a4ec08b8c19661725e9cd6c0b-Abstract-Conference.html": {
    "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsoo Kim",
      "Sihwa Lee",
      "Janghwan Lee",
      "Sukjin Hong",
      "Du-Seong Chang",
      "Wonyong Sung",
      "Jungwook Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/836012122f3de08aeeae67369b087964-Abstract-Conference.html": {
    "title": "Efficient Exploration in Continuous-time Model-based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lenart Treven",
      "Jonas Hübotter",
      "Bhavya ",
      "Florian Dorfler",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/837b396039248acb08c385bebb6291b4-Abstract-Conference.html": {
    "title": "On Learning Necessary and Sufficient Causal Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengrui Cai",
      "Yixin Wang",
      "Michael Jordan",
      "Rui Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8393d955a00c463a982cefe77d0404e1-Abstract-Conference.html": {
    "title": "Slimmed Asymmetrical Contrastive Learning and Cross Distillation for Lightweight Model Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Meng",
      "Li Yang",
      "Kyungmin Lee",
      "Jinwoo Shin",
      "Deliang Fan",
      "Jae-sun Seo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/839e23e5b1c52cfd1268f4023a3af0d6-Abstract-Conference.html": {
    "title": "Beyond Unimodal: Generalising Neural Processes for Multimodal Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myong Chol Jung",
      "He Zhao",
      "Joanna Dipnall",
      "Lan Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/83a10a480fbec91c88f6a9293b4d2b05-Abstract-Conference.html": {
    "title": "Trans-Dimensional Generative Modeling via Jump Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Campbell",
      "William Harvey",
      "Christian Weilbach",
      "Valentin De Bortoli",
      "Thomas Rainforth",
      "Arnaud Doucet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/83a14a36de4502bac5b580db36e81858-Abstract-Conference.html": {
    "title": "Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaofei Fan",
      "Nick Hahn",
      "Foram Kamdar",
      "Donald Avansino",
      "Guy Wilson",
      "Leigh Hochberg",
      "Krishna V Shenoy",
      "Jaimie Henderson",
      "Francis Willett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/83c637c3bc0ca88eda6cf4f5f45bdced-Abstract-Conference.html": {
    "title": "Towards robust and generalizable representations of extracellular data using contrastive learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Vishnubhotla",
      "Charlotte Loh",
      "Akash Srivastava",
      "Liam Paninski",
      "Cole Hurwitz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/83ca9e252329e7b0704ead93893e6b1b-Abstract-Conference.html": {
    "title": "Rethinking Conditional Diffusion Sampling with Progressive Guidance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh-Dung Dinh",
      "Daochang Liu",
      "Chang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/83dc5747870ea454cab25e30bef4eb8a-Abstract-Conference.html": {
    "title": "State-Action Similarity-Based Representations for Off-Policy Evaluation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brahma Pavse",
      "Josiah Hanna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8407d254b5baacf69ee977aa34f0e521-Abstract-Conference.html": {
    "title": "Explore In-Context Learning for 3D Point Cloud Understanding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongbin Fang",
      "Xiangtai Li",
      "Xia Li",
      "Joachim M Buhmann",
      "Chen Change Loy",
      "Mengyuan Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8434e0db3227276c00ef2b18c7f01c65-Abstract-Conference.html": {
    "title": "Learning Re-sampling Methods with Parameter Attribution for Image Super-resolution",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotong Luo",
      "Yuan Xie",
      "Yanyun Qu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/84706cdfc192cd0351daf48f379847e6-Abstract-Conference.html": {
    "title": "Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaemoo Choi",
      "Jaewoong Choi",
      "Myungjoo Kang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/847bb9bb1351f557a52d2ecdacb7e2d3-Abstract-Conference.html": {
    "title": "Generalized Belief Transport",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junqi Wang",
      "PEI WANG",
      "Patrick Shafto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8493c860bec41705f7743d5764301b94-Abstract-Conference.html": {
    "title": "Lie Point Symmetry and Physics-Informed Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tara Akhound-Sadegh",
      "Laurence Perreault-Levasseur",
      "Johannes Brandstetter",
      "Max Welling",
      "Siamak Ravanbakhsh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8493e190ff1bbe3837eca821190b61ff-Abstract-Conference.html": {
    "title": "Norm-based Generalization Bounds for Sparse Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomer Galanti",
      "Mengjia Xu",
      "Liane Galanti",
      "Tomaso Poggio"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/84b686f7cc7b7751e9aaac0da74f755a-Abstract-Conference.html": {
    "title": "Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Amir",
      "Steven Gortler",
      "Ilai Avni",
      "Ravina Ravina",
      "Nadav Dym"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/84bad835faaf48f24d990072bb5b80ee-Abstract-Conference.html": {
    "title": "Online Ad Procurement in Non-stationary Autobidding Worlds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Cheuk Nam Liang",
      "Haihao Lu",
      "Baoyu Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/84f44b36ceb4fbc9bb269959f4796eed-Abstract-Conference.html": {
    "title": "Precise asymptotic generalization for multiclass classification with overparameterized linear models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Wu",
      "Anant Sahai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/85069585133c4c168c865e65d72e9775-Abstract-Conference.html": {
    "title": "Break It Down: Evidence for Structural Compositionality in Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Lepori",
      "Thomas Serre",
      "Ellie Pavlick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8511d06d5590f4bda24d42087802cc81-Abstract-Conference.html": {
    "title": "Focused Transformer: Contrastive Training for Context Scaling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Szymon Tworkowski",
      "Konrad Staniszewski",
      "Mikołaj Pacek",
      "Yuhuai Wu",
      "Henryk Michalewski",
      "Piotr Miłoś"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8514a5203b87cba5e440bd62ab18f2b4-Abstract-Conference.html": {
    "title": "Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyinzi Jiang",
      "Chaojie Mao",
      "Ziyuan Huang",
      "Ao Ma",
      "Yiliang Lv",
      "Yujun Shen",
      "Deli Zhao",
      "Jingren Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/85381f4549b5ddf1d48e2e287d7d3d15-Abstract-Conference.html": {
    "title": "Learning Energy-Based Prior Model with Diffusion-Amortized MCMC",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiyu Yu",
      "Yaxuan Zhu",
      "Sirui Xie",
      "Xiaojian (Shawn) Ma",
      "Ruiqi Gao",
      "Song-Chun Zhu",
      "Ying Nian Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/854a9ab0f323b841955e70ca383b27d1-Abstract-Conference.html": {
    "title": "Directed Cyclic Graph for Causal Discovery from Multivariate Functional Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saptarshi Roy",
      "Raymond K. W. Wong",
      "Yang Ni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/854b6ec839294bf332db0d86e2f83c3f-Abstract-Conference.html": {
    "title": "Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Casey Meehan",
      "Florian Bordes",
      "Pascal Vincent",
      "Kamalika Chaudhuri",
      "Chuan Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/85b2ff7574ef265f3a4800db9112ce14-Abstract-Conference.html": {
    "title": "Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Younis",
      "Erik Sudderth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/85d456fd41f3eec83bd3b0c337037a0e-Abstract-Conference.html": {
    "title": "Learning Curves for Deep Structured Gaussian Feature Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Zavatone-Veth",
      "Cengiz Pehlevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/85f5c7372625d1e0df0e3996f85062d6-Abstract-Conference.html": {
    "title": "Mirror Diffusion Models for Constrained and Watermarked Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guan-Horng Liu",
      "Tianrong Chen",
      "Evangelos Theodorou",
      "Molei Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/860a092bb4d9d81d3133a01c50c01578-Abstract-Conference.html": {
    "title": "Training Transitive and Commutative Multimodal Transformers with LoReTTa",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Tran",
      "Yashin Dicente Cid",
      "Amal Lahiani",
      "Fabian Theis",
      "Tingying Peng",
      "Eldad Klaiman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/860c1c657deafe09f64c013c2888bd7b-Abstract-Conference.html": {
    "title": "SaVeNet: A Scalable Vector Network for Enhanced Molecular Representation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarp Aykent",
      "Tian Xia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8629b0fff229b8a27efb1422e990605f-Abstract-Conference.html": {
    "title": "Beyond Pretrained Features: Noisy Image Modeling Provides Adversarial Defense",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zunzhi You",
      "Daochang Liu",
      "Bohyung Han",
      "Chang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/862f45ccecb2275851bc8acebb8b4d65-Abstract-Conference.html": {
    "title": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Qin",
      "Shu Zhang",
      "Ning Yu",
      "Yihao Feng",
      "Xinyi Yang",
      "Yingbo Zhou",
      "Huan Wang",
      "Juan Carlos Niebles",
      "Caiming Xiong",
      "Silvio Savarese",
      "Stefano Ermon",
      "Yun Fu",
      "Ran Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/863da9d40547f1d1b18859519ce2dee4-Abstract-Conference.html": {
    "title": "Unlocking Deterministic Robustness Certification on ImageNet",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Hu",
      "Andy Zou",
      "Zifan Wang",
      "Klas Leino",
      "Matt Fredrikson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86419aba4e5eafd2b1009a2e3c540bb0-Abstract-Conference.html": {
    "title": "Bayesian Optimisation of Functions on Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingchen Wan",
      "Pierre Osselin",
      "Henry Kenlay",
      "Binxin Ru",
      "Michael A Osborne",
      "Xiaowen Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8644b61a9bc87bf7844750a015feb600-Abstract-Conference.html": {
    "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Lee",
      "Annie Xie",
      "Aldo Pacchiano",
      "Yash Chandak",
      "Chelsea Finn",
      "Ofir Nachum",
      "Emma Brunskill"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8667f264f88c7938a73a53ab01eb1327-Abstract-Conference.html": {
    "title": "L2T-DLN: Learning to Teach with Dynamic Loss Network",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Hai",
      "Liyuan Pan",
      "Xiabi Liu",
      "Zhengzheng Liu",
      "Mirna Yunita"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8668fdc7b2ddf55a0e235824c66f2eee-Abstract-Conference.html": {
    "title": "Structured Federated Learning through Clustered Additive Modeling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Ma",
      "Tianyi Zhou",
      "Guodong Long",
      "Jing Jiang",
      "Chengqi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8671b6dffc08b4fcf5b8ce26799b2bef-Abstract-Conference.html": {
    "title": "An Inductive Bias for Tabular Deep Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ege Beyazit",
      "Jonathan Kozaczuk",
      "Bo Li",
      "Vanessa Wallace",
      "Bilal Fadlallah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8678da90126aa58326b2fc0254b33a8c-Abstract-Conference.html": {
    "title": "Fairness-guided Few-shot Prompting for Large Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Ma",
      "Changqing Zhang",
      "Yatao Bian",
      "Lemao Liu",
      "Zhirui Zhang",
      "Peilin Zhao",
      "Shu Zhang",
      "Huazhu Fu",
      "Qinghua Hu",
      "Bingzhe Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/868f2f9a9950f7b0538b3ce7eb4c8eb8-Abstract-Conference.html": {
    "title": "Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Yang",
      "Junlong Lyu",
      "Wenlong Lyu",
      "Zhitang Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86ab6927ee4ae9bde4247793c46797c7-Abstract-Conference.html": {
    "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Nguyen",
      "Michael Poli",
      "Marjan Faizi",
      "Armin Thomas",
      "Michael Wornow",
      "Callum Birch-Sykes",
      "Stefano Massaroli",
      "Aman Patel",
      "Clayton Rabideau",
      "Yoshua Bengio",
      "Stefano Ermon",
      "Christopher Ré",
      "Stephen Baccus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86b8ad667206fb9a52ae575fbf1cd6be-Abstract-Conference.html": {
    "title": "Learning a 1-layer conditional generative model in total variation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ajil Jalal",
      "Justin Kang",
      "Ananya Uppal",
      "Kannan Ramchandran",
      "Eric Price"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86bcae6da75c72e32f30a5553f094c06-Abstract-Conference.html": {
    "title": "Model Shapley: Equitable Model Valuation with Black-box Access",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Xu",
      "Thanh Lam",
      "Chuan Sheng Foo",
      "Bryan Kian Hsiang Low"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86bd650f85480c595ecab29081a3774e-Abstract-Conference.html": {
    "title": "Robust Concept Erasure via Kernelized Rate-Distortion Maximization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Nicholas Monath",
      "Kumar Avinava Dubey",
      "Amr Ahmed",
      "Snigdha Chaturvedi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86c070ce724102ee876d1935590e111a-Abstract-Conference.html": {
    "title": "BiMatting: Efficient Video Matting via Binarization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotong Qin",
      "Lei Ke",
      "Xudong Ma",
      "Martin Danelljan",
      "Yu-Wing Tai",
      "Chi-Keung Tang",
      "Xianglong Liu",
      "Fisher Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86c17de05579cde52025f9984e6e2ebb-Abstract-Conference.html": {
    "title": "One Fits All: Power General Time Series Analysis by Pretrained LM",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Zhou",
      "Peisong Niu",
      "xue wang",
      "Liang Sun",
      "Rong Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86c1fd74fa25bd6be0072937803e0bd1-Abstract-Conference.html": {
    "title": "Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyoung Lee",
      "Myungsik Cho",
      "Youngchul Sung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86c283920335ed1fec3edee227e05fbf-Abstract-Conference.html": {
    "title": "Near-Optimal Algorithms for Gaussians with Huber Contamination: Mean Estimation and Linear Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Daniel Kane",
      "Ankit Pensia",
      "Thanasis Pittas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/86cba2b31d17f237866a2e6c52c7878a-Abstract-Conference.html": {
    "title": "Causal Discovery from Subsampled Time Series with Proxy Variables",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhou Liu",
      "Xinwei Sun",
      "Lingjing Hu",
      "Yizhou Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/870c1e0589822bf37590b84984c345c4-Abstract-Conference.html": {
    "title": "Why Not Looking backward?\" A Robust Two-Step Method to Automatically Terminate Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Li",
      "Ke Li",
      "Wei Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/871ed095b734818cfba48db6aeb25a62-Abstract-Conference.html": {
    "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pan Lu",
      "Baolin Peng",
      "Hao Cheng",
      "Michel Galley",
      "Kai-Wei Chang",
      "Ying Nian Wu",
      "Song-Chun Zhu",
      "Jianfeng Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8735753cc18f6baa92d1f069fd8b14a0-Abstract-Conference.html": {
    "title": "Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zebin You",
      "Yong Zhong",
      "Fan Bao",
      "Jiacheng Sun",
      "Chongxuan LI",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/873c86d9a979ab80d8e2919510d4446b-Abstract-Conference.html": {
    "title": "Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuobai Zhang",
      "Minghao Xu",
      "Aurelie C. Lozano",
      "Vijil Chenthamarakshan",
      "Payel Das",
      "Jian Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/87425754bcc35f2bc62ef4a421a772d6-Abstract-Conference.html": {
    "title": "Progressive Ensemble Distillation: Building Ensembles for Efficient Inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Don Dennis",
      "Abhishek Shetty",
      "Anish Prasad Sevekari",
      "Kazuhito Koishida",
      "Virginia Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/87571720167f7e88827c40e468e3101f-Abstract-Conference.html": {
    "title": "Differentially Private Approximate Near Neighbor Counting in High Dimensions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandr Andoni",
      "Piotr Indyk",
      "Sepideh Mahabadi",
      "Shyam Narayanan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8797d13e5998acfab387d4bf0a5b9b00-Abstract-Conference.html": {
    "title": "Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjie Wang",
      "Meng Xiao",
      "Min Wu",
      "pengfei wang",
      "Yuanchun Zhou",
      "Yanjie Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/87cf37e2085655bad7bad0a014e0edad-Abstract-Conference.html": {
    "title": "FD-Align: Feature Discrimination Alignment for Fine-tuning Pre-Trained Models in Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Song",
      "Huimin Ma",
      "Bochao Zou",
      "Huishuai Zhang",
      "Weiran Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/87e82678c0d6e5b729398426f82e9af6-Abstract-Conference.html": {
    "title": "D-Separation for Causal Self-Explanation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liu",
      "Jun Wang",
      "Haozhao Wang",
      "Ruixuan Li",
      "Zhiying Deng",
      "YuanKai Zhang",
      "Yang Qiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/87ee1bbac4635e7c948f3eea83c1f262-Abstract-Conference.html": {
    "title": "History Filtering in Imperfect Information Games: Algorithms and Complexity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Solinas",
      "Doug Rebstock",
      "Nathan Sturtevant",
      "Michael Buro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/881259965dacb9f42967aae84a157283-Abstract-Conference.html": {
    "title": "Constant Approximation for Individual Preference Stable Clustering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anders Aamand",
      "Justin Chen",
      "Allen Liu",
      "Sandeep Silwal",
      "Pattara Sukprasert",
      "Ali Vakilian",
      "Fred Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/88139fdcc82fc597090620d77b023282-Abstract-Conference.html": {
    "title": "Intervention Generalization: A View from Factor Graph Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gecia Bravo-Hermsdorff",
      "David Watson",
      "Jialin Yu",
      "Jakob Zeitler",
      "Ricardo Silva"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/88237ac4e9941b1be5c6d3c1ad408184-Abstract-Conference.html": {
    "title": "Decision Tree for Locally Private Estimation with Public Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Ma",
      "Han Zhang",
      "Yuchao Cai",
      "Hanfang Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/883105b282fe15275991b411e6b200c5-Abstract-Conference.html": {
    "title": "DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Ye",
      "Jiarui Wang",
      "Zhiguang Cao",
      "Helan Liang",
      "Yong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8833c8aa10542d24d693bbaf6a4598f5-Abstract-Conference.html": {
    "title": "Offline Imitation Learning with Variational Counterfactual Reasoning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexu Sun",
      "Bowei He",
      "Jinxin Liu",
      "Xu Chen",
      "Chen Ma",
      "Shuai Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/88593e16b09104fb6010d370c081d7bc-Abstract-Conference.html": {
    "title": "LART: Neural Correspondence Learning with Latent Regularization Transformer for 3D Motion Transfer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Chen",
      "Hao Tang",
      "Radu Timofte",
      "Luc V Gool",
      "Guoying Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/886ed40d7882c9f891824e42a452c228-Abstract-Conference.html": {
    "title": "Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jang-Hyun Kim",
      "Sangdoo Yun",
      "Hyun Oh Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/887262aeb3eafb01ef0fd0e3a87a8831-Abstract-Conference.html": {
    "title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Cheng",
      "Di Luo",
      "Xiuying Chen",
      "Lemao Liu",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/887932131fddf943e8fe3310b62c0147-Abstract-Conference.html": {
    "title": "Front-door Adjustment Beyond Markov Equivalence with Limited Graph Knowledge",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhin Shah",
      "Karthikeyan Shanmugam",
      "Murat Kocaoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8882d370cdafec9885b918a8cfac642e-Abstract-Conference.html": {
    "title": "Training Energy-Based Normalizing Flow with Score-Matching Objectives",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Hao Chao",
      "Wei-Fang Sun",
      "Yen-Chang Hsu",
      "Zsolt Kira",
      "Chun-Yi Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/88a129e44f25a571ae8b838057c46855-Abstract-Conference.html": {
    "title": "LayoutPrompter: Awaken the Design Ability of Large Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Lin",
      "Jiaqi Guo",
      "Shizhao Sun",
      "Zijiang Yang",
      "Jian-Guang Lou",
      "Dongmei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/88be023075a5a3ff3dc3b5d26623fa22-Abstract-Conference.html": {
    "title": "Classification of Heavy-tailed Features in High Dimensions: a Superstatistical Approach",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urte Adomaityte",
      "Gabriele Sicuro",
      "Pierpaolo Vivo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html": {
    "title": "CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andres Potapczynski",
      "Marc Finzi",
      "Geoff Pleiss",
      "Andrew G. Wilson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/88dddaf430b5bc38ab8228902bb61821-Abstract-Conference.html": {
    "title": "Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eghbal Hosseini",
      "Evelina Fedorenko"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8912b4892064a4f08a0c04f92913c134-Abstract-Conference.html": {
    "title": "Weakly Coupled Deep Q-Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim El Shar",
      "Daniel Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8936fa1691764912d9519e1b5673ea66-Abstract-Conference.html": {
    "title": "Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youbang Sun",
      "Tao Liu",
      "Ruida Zhou",
      "P. R. Kumar",
      "Shahin Shahrampour"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/89379d5fc6eb34ff98488202fb52b9d0-Abstract-Conference.html": {
    "title": "MMGP: a Mesh Morphing Gaussian Process-based machine learning method for regression of physical problems under nonparametrized geometrical variability",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabien Casenave",
      "Brian Staber",
      "Xavier Roynard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/893a5db6100028ec814cfd99fe92c31b-Abstract-Conference.html": {
    "title": "Adaptive Online Replanning with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Zhou",
      "Yilun Du",
      "Shun Zhang",
      "Mengdi Xu",
      "Yikang Shen",
      "Wei Xiao",
      "Dit-Yan Yeung",
      "Chuang Gan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/893ca2e5ff5bb258da30e0a82f4c8de9-Abstract-Conference.html": {
    "title": "SODA: Robust Training of Test-Time Data Adaptors",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zige Wang",
      "Yonggang Zhang",
      "Zhen Fang",
      "Long Lan",
      "Wenjing Yang",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8948a8d039ed52d1031db6c7c2373378-Abstract-Conference.html": {
    "title": "Training Neural Networks is NP-Hard in Fixed Dimension",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Froese",
      "Christoph Hertrich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8951bbdcf234132bcce680825e7cb354-Abstract-Conference.html": {
    "title": "GlyphControl: Glyph Conditional Control for Visual Text Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukang Yang",
      "Dongnan Gui",
      "YUHUI YUAN",
      "Weicong Liang",
      "Haisong Ding",
      "Han Hu",
      "Kai Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/899511e37a8e01e1bd6f6f1d377cc250-Abstract-Conference.html": {
    "title": "Domain Adaptive Imitation Learning with Visual Observation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungho Choi",
      "Seungyul Han",
      "Woojun Kim",
      "Jongseong Chae",
      "Whiyoung Jung",
      "Youngchul Sung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/89beb2a345269f3f9afe48cee35403aa-Abstract-Conference.html": {
    "title": "Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Usha Bhalla",
      "Suraj Srinivas",
      "Himabindu Lakkaraju"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/89e541b817ea043a971840a926e12b37-Abstract-Conference.html": {
    "title": "Detecting hidden confounding in observational data using multiple environments",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rickard Karlsson",
      "Jesse Krijthe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8a267516a7a697965c6ae4f48b908605-Abstract-Conference.html": {
    "title": "Information Geometry of the Retinal Representation Manifold",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuehao Ding",
      "Dongsoo Lee",
      "Joshua Melander",
      "George Sivulka",
      "Surya Ganguli",
      "Stephen Baccus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8a8b9c7f979e8819a7986b3ef825c08a-Abstract-Conference.html": {
    "title": "Sequential Memory with Temporal Predictive Coding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mufeng Tang",
      "Helen Barron",
      "Rafal Bogacz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8a8ce53beb3775522305e0a6033d4455-Abstract-Conference.html": {
    "title": "Transportability for Bandits with Data from Different Environments",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexis Bellot",
      "Alan Malek",
      "Silvia Chiappa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8b07d224a643b02e7571e083578a86d2-Abstract-Conference.html": {
    "title": "Students Parrot Their Teachers: Membership Inference on Model Distillation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Jagielski",
      "Milad Nasr",
      "Katherine Lee",
      "Christopher A. Choquette-Choo",
      "Nicholas Carlini",
      "Florian Tramer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8b1008098947ad59144c18a78337f937-Abstract-Conference.html": {
    "title": "DiffuseBot: Breeding Soft Robots With Physics-Augmented Generative Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsun-Hsuan Johnson Wang",
      "Juntian Zheng",
      "Pingchuan Ma",
      "Yilun Du",
      "Byungchul Kim",
      "Andrew Spielberg",
      "Josh Tenenbaum",
      "Chuang Gan",
      "Daniela Rus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8b4add8b0aa8749d80a34ca5d941c355-Abstract-Conference.html": {
    "title": "Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jimmy Di",
      "Jack Douglas",
      "Jayadev Acharya",
      "Gautam Kamath",
      "Ayush Sekhari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8b4ba64e549c410185c4d3eac3a81726-Abstract-Conference.html": {
    "title": "Thought Cloning: Learning to Think while Acting by Imitating Human Thinking",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengran Hu",
      "Jeff Clune"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8b54ecd9823fff6d37e61ece8f87e534-Abstract-Conference.html": {
    "title": "SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugues Van Assel",
      "Titouan Vayer",
      "Rémi Flamary",
      "Nicolas Courty"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8b6a8b010e9a266aad40a024c5976d5c-Abstract-Conference.html": {
    "title": "Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nurendra Choudhary",
      "Nikhil Rao",
      "Chandan Reddy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8ba80c47b9d3dced79ee835b7d3bf72a-Abstract-Conference.html": {
    "title": "On Separate Normalization in Self-supervised Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohui Chen",
      "Yinkai Wang",
      "Yuanqi Du",
      "Soha Hassoun",
      "Liping Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8bc74514d554a90c996576f6c373f5f3-Abstract-Conference.html": {
    "title": "Modulated Neural ODEs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilze Amanda Auzina",
      "Çağatay Yıldız",
      "Sara Magliacane",
      "Matthias Bethge",
      "Efstratios Gavves"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8bd31288ad8e9a31d519fdeede7ee47d-Abstract-Conference.html": {
    "title": "DrugCLIP: Contrasive Protein-Molecule Representation Learning for Virtual Screening",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Gao",
      "Bo Qiang",
      "Haichuan Tan",
      "Yinjun Jia",
      "Minsi Ren",
      "Minsi Lu",
      "Jingjing Liu",
      "Wei-Ying Ma",
      "Yanyan Lan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8bea36ac39e11ebe49e9eddbd4b8bd3a-Abstract-Conference.html": {
    "title": "On the Convergence of Black-Box Variational Inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyurae Kim",
      "Jisu Oh",
      "Kaiwen Wu",
      "Yian Ma",
      "Jacob Gardner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c086821724b99f4c756648bb0f165db-Abstract-Conference.html": {
    "title": "DRAUC: An Instance-wise Distributionally Robust AUC Optimization Framework",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siran Dai",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Xiaochun Cao",
      "Qingming Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c1d92835eb4e601f396c97ec60439fe-Abstract-Conference.html": {
    "title": "iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive Noise Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Chen",
      "Kevin Bello",
      "Bryon Aragam",
      "Pradeep Ravikumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c22e5e918198702765ecff4b20d0a90-Abstract-Conference.html": {
    "title": "Optimal Learners for Realizable Regression: PAC Learning and Online Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idan Attias",
      "Steve Hanneke",
      "Alkis Kalavasis",
      "Amin Karbasi",
      "Grigoris Velegkas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c234d9c7e738a793947e0282c36eb95-Abstract-Conference.html": {
    "title": "Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong XU",
      "Dejan Markovic",
      "Jacob Sandakly",
      "Todd Keebler",
      "Steven Krenn",
      "Alexander Richard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c2df4c35cdbee764ebb9e9d0acd5197-Abstract-Conference.html": {
    "title": "Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Hollmann",
      "Samuel Müller",
      "Frank Hutter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c3caae2f725c8e2a55ecd600563d172-Abstract-Conference.html": {
    "title": "On quantum backpropagation, information reuse, and cheating measurement collapse",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amira Abbas",
      "Robbie King",
      "Hsin-Yuan Huang",
      "William J. Huggins",
      "Ramis Movassagh",
      "Dar Gilboa",
      "Jarrod McClean"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c3e38ce55a0fa44bc325bc6fdb7f4e5-Abstract-Conference.html": {
    "title": "First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandr Beznosikov",
      "Sergey Samsonov",
      "Marina Sheshukova",
      "Alexander Gasnikov",
      "Alexey Naumov",
      "Eric Moulines"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c533f53f76c4df9a7f08e7cb676d132-Abstract-Conference.html": {
    "title": "Fair, Polylog-Approximate Low-Cost Hierarchical Clustering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marina Knittel",
      "Max Springer",
      "John Dickerson",
      "MohammadTaghi Hajiaghayi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c64bc3f7796d31caa7c3e6b969bf7da-Abstract-Conference.html": {
    "title": "A Novel Approach for Effective Multi-View Clustering with Information-Theoretic Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhang Cui",
      "Yazhou Ren",
      "Jingyu Pu",
      "Jiawei Li",
      "Xiaorong Pu",
      "Tianyi Wu",
      "Yutao Shi",
      "Lifang He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c7304e77c832ddc70075dfee081ca6c-Abstract-Conference.html": {
    "title": "OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghua Liu",
      "Ruoxi Shi",
      "Kaiming Kuang",
      "Yinhao Zhu",
      "Xuanlin Li",
      "Shizhong Han",
      "Hong Cai",
      "Fatih Porikli",
      "Hao Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c8cd1b78cdae751265c88efc136e5bd-Abstract-Conference.html": {
    "title": "Optimizing over trained GNNs via symmetry breaking",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiqiang Zhang",
      "Juan Campos",
      "Christian Feldmann",
      "David Walz",
      "Frederik Sandfort",
      "Miriam Mathea",
      "Calvin Tsay",
      "Ruth Misener"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c96b559340daa7bb29f56ccfbbc9c2f-Abstract-Conference.html": {
    "title": "REx: Data-Free Residual Quantization Error Expansion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edouard YVINEC",
      "Arnaud Dapogny",
      "Matthieu Cord",
      "Kevin Bailly"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8ca113d122584f12a6727341aaf58887-Abstract-Conference.html": {
    "title": "A Unified, Scalable Framework for Neural Population Decoding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehdi Azabou",
      "Vinam Arora",
      "Venkataramana Ganesh",
      "Ximeng Mao",
      "Santosh Nachimuthu",
      "Michael Mendelson",
      "Blake Richards",
      "Matthew Perich",
      "Guillaume Lajoie",
      "Eva Dyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8d2c36836fb0e7d78fe68762ff8b5f1e-Abstract-Conference.html": {
    "title": "Adaptive Contextual Perception: How To Generalize To New Backgrounds and Ambiguous Objects",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuofan Ying",
      "Peter Hase",
      "Mohit Bansal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8d6b1d775014eff18256abeb207202ad-Abstract-Conference.html": {
    "title": "On the Gini-impurity Preservation For Privacy Random Forests",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XinRan Xie",
      "Man-Jie Yuan",
      "Xuetong Bai",
      "Wei Gao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8d7060b2ee6ff728692398783e3d59d1-Abstract-Conference.html": {
    "title": "Debiasing Pretrained Generative Models by Uniformly Sampling Semantic Attributes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Walter Gerych",
      "Kevin Hickey",
      "Luke Buquicchio",
      "Kavin Chandrasekaran",
      "Abdulaziz Alajaji",
      "Elke A. Rundensteiner",
      "Emmanuel Agu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8db0d67d22e0ec08c95b810be3a66907-Abstract-Conference.html": {
    "title": "Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamish Flynn",
      "David Reeb",
      "Melih Kandemir",
      "Jan R. Peters"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8db9279f593652ee9bb2223b4a2c43fa-Abstract-Conference.html": {
    "title": "Tame a Wild Camera: In-the-Wild Monocular Camera Calibration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Zhu",
      "Abhinav Kumar",
      "Masa Hu",
      "Xiaoming Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8dcc306a2522c60a78f047ab8739e631-Abstract-Conference.html": {
    "title": "ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhitong Gao",
      "Shipeng Yan",
      "Xuming He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8ddcba644f602835a52b962d9a119eea-Abstract-Conference.html": {
    "title": "Regression with Cost-based Rejection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Cheng",
      "Yuzhou Cao",
      "Haobo Wang",
      "Hongxin Wei",
      "Bo An",
      "Lei Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8df0fe2bba0f14208a10c1cb22e71552-Abstract-Conference.html": {
    "title": "A State Representation for Diminishing Rewards",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ted Moskovitz",
      "Samo Hromadka",
      "Ahmed Touati",
      "Diana Borsa",
      "Maneesh Sahani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8df705957a5262de3cb37ba9f1fb96f3-Abstract-Conference.html": {
    "title": "Unified Segment-to-Segment Framework for Simultaneous Sequence Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaolei Zhang",
      "Yang Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8df90a1440ce782d1f5607b7a38f2531-Abstract-Conference.html": {
    "title": "DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salva Rühling Cachay",
      "Bo Zhao",
      "Hailey Joren",
      "Rose Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e176ef071f00f1b233461c5ad5e1b24-Abstract-Conference.html": {
    "title": "Energy Discrepancies: A Score-Independent Loss for Energy-Based Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Schröder",
      "Zijing Ou",
      "Jen Lim",
      "Yingzhen Li",
      "Sebastian Vollmer",
      "Andrew Duncan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e2571d13f432b301d4c5e3cc70227a6-Abstract-Conference.html": {
    "title": "Learning to Group Auxiliary Datasets for Molecule",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tinglin Huang",
      "Ziniu Hu",
      "Rex Ying"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e2a75e0c7b579a6cf176dc0858cde55-Abstract-Conference.html": {
    "title": "Equivariant Spatio-Temporal Attentive Graph Networks to Simulate Physical Dynamics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liming Wu",
      "Zhichao Hou",
      "Jirui Yuan",
      "Yu Rong",
      "Wenbing Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e3db2040672d85fd12e6313945594fe-Abstract-Conference.html": {
    "title": "Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eli Chien",
      "Wei-Ning Chen",
      "Chao Pan",
      "Pan Li",
      "Ayfer Ozgur",
      "Olgica Milenkovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e4ccc9ca6ae2225c4cbb7782ab48daf-Abstract-Conference.html": {
    "title": "Team-PSRO for Learning Approximate TMECor in Large Team Games via Cooperative Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen McAleer",
      "Gabriele Farina",
      "Gaoyue Zhou",
      "Mingzhi Wang",
      "Yaodong Yang",
      "Tuomas Sandholm"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e5de4cb639ef718f44060dc257cb04f-Abstract-Conference.html": {
    "title": "Learning Linear Causal Representations from Interventions under General Nonlinear Mixing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Buchholz",
      "Goutham Rajendran",
      "Elan Rosenfeld",
      "Bryon Aragam",
      "Bernhard Schölkopf",
      "Pradeep Ravikumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e63972d4d9d81b31459d787466ce271-Abstract-Conference.html": {
    "title": "Disentanglement via Latent Quantization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyle Hsu",
      "William Dorrell",
      "James Whittington",
      "Jiajun Wu",
      "Chelsea Finn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e69a97cbdd91ac0808603fa589d6c17-Abstract-Conference.html": {
    "title": "Variance-Reduced Gradient Estimation via Noise-Reuse in Online Evolution Strategies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oscar Li",
      "James Harrison",
      "Jascha Sohl-Dickstein",
      "Virginia Smith",
      "Luke Metz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e806d3c56ed5f1dab85d601e13cbe38-Abstract-Conference.html": {
    "title": "Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongxin Li",
      "Yiheng Lin",
      "Shaolei Ren",
      "Adam Wierman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e9a6582caa59fda0302349702965171-Abstract-Conference.html": {
    "title": "Graph Contrastive Learning with Stable and Scalable Spectral Encoding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deyu Bo",
      "Yuan Fang",
      "Yang Liu",
      "Chuan Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e9bdc23f169a05ea9b72ccef4574551-Abstract-Conference.html": {
    "title": "A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Zhang",
      "Charles Herrmann",
      "Junhwa Hur",
      "Luisa Polania Cabrera",
      "Varun Jampani",
      "Deqing Sun",
      "Ming-Hsuan Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8e9c7d4a48bdac81a58f983a64aaf42b-Abstract-Conference.html": {
    "title": "SatLM: Satisfiability-Aided Language Models Using Declarative Prompting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Ye",
      "Qiaochu Chen",
      "Isil Dillig",
      "Greg Durrett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8ec61d4084443d29c9e47ac60f9aea31-Abstract-Conference.html": {
    "title": "A normative theory of social conflict",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergey Shuvaev",
      "Evgeny Amelchenko",
      "Dmitry Smagin",
      "Natalia Kudryavtseva",
      "Grigori Enikolopov",
      "Alex Koulakov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8ed2293e714b7692b63117e330e551e8-Abstract-Conference.html": {
    "title": "Learning Invariant Representations of Graph Neural Networks via Cluster Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donglin Xia",
      "Xiao Wang",
      "Nian Liu",
      "Chuan Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8ed3d610ea4b68e7afb30ea7d01422c6-Abstract-Conference.html": {
    "title": "Transformers learn to implement preconditioned gradient descent for in-context learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwangjun Ahn",
      "Xiang Cheng",
      "Hadi Daneshmand",
      "Suvrit Sra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8eec8d7bcecf034304174e6b57dbc19a-Abstract-Conference.html": {
    "title": "Linear Time Algorithms for k-means with Multi-Swap Local Search",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Huang",
      "Qilong Feng",
      "Ziyun Huang",
      "Jinhui Xu",
      "Jianxin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8eff4196f50c43eda7bcf0f0cf87a0d0-Abstract-Conference.html": {
    "title": "VaRT: Variational Regression Trees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Salazar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8f0d446441a938d9de420a8ab8d7fd36-Abstract-Conference.html": {
    "title": "STREAMER: Streaming Representation Learning and Event Segmentation in a Hierarchical Manner",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramy Mounir",
      "Sujal Vijayaraghavan",
      "Sudeep Sarkar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8f182e220092f7f1fc44f3313023f5a0-Abstract-Conference.html": {
    "title": "Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Allouah",
      "Rachid Guerraoui",
      "Nirupam Gupta",
      "Rafael Pinot",
      "Geovani Rizk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8f1bacee31caf990a4f08d84f0ccb322-Abstract-Conference.html": {
    "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Jiang",
      "Jiaqi Gu",
      "Hanqing Zhu",
      "David Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8f9d459c19b59b5400ce396e0f8c23e0-Abstract-Conference.html": {
    "title": "AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Yue",
      "Zhiling Ye",
      "Jiadi Jiang",
      "Yongchao Liu",
      "Ke Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8f9f4eb32b9081a90f2a0b2627eb2a24-Abstract-Conference.html": {
    "title": "PDP: Parameter-free Differentiable Pruning is All You Need",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsik Cho",
      "Saurabh Adya",
      "Devang Naik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8fab4407e1fe9006b39180525c0d323c-Abstract-Conference.html": {
    "title": "ExPT: Synthetic Pretraining for Few-Shot Experimental Design",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tung Nguyen",
      "Sudhanshu Agrawal",
      "Aditya Grover"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.html": {
    "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibo Hao",
      "Tianyang Liu",
      "Zhen Wang",
      "Zhiting Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8fd5bc08e744fe0dfe798c61d1575a22-Abstract-Conference.html": {
    "title": "CLIP4HOI: Towards Adapting CLIP for Practical Zero-Shot HOI Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunyao Mao",
      "Jiajun Deng",
      "Wengang Zhou",
      "Li Li",
      "Yao Fang",
      "Houqiang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8ffb4e3118280a66b192b6f06e0e2596-Abstract-Conference.html": {
    "title": "Transformer-based Planning for Symbolic Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parshin Shojaee",
      "Kazem Meidani",
      "Amir Barati Farimani",
      "Chandan Reddy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/90043ebd68500f9efe84fedf860a64f3-Abstract-Conference.html": {
    "title": "Exploring Geometry of Blind Spots in Vision models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sriram Balasubramanian",
      "Gaurang Sriramanan",
      "Vinu Sankar Sadasivan",
      "Soheil Feizi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/90080022263cddafddd4a0726f1fb186-Abstract-Conference.html": {
    "title": "Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omar Chehab",
      "Aapo Hyvarinen",
      "Andrej Risteski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/902c462e821e5e639ac3422b48b65932-Abstract-Conference.html": {
    "title": "Strategic Distribution Shift of Interacting Agents via Coupled Gradient Flows",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lauren Conger",
      "Franca Hoffmann",
      "Eric Mazumdar",
      "Lillian Ratliff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9032e5c9ec394ce768a2fa9bdc56af6c-Abstract-Conference.html": {
    "title": "Learning Time-Invariant Representations for Individual Neurons from Population Dynamics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Mi",
      "Trung Le",
      "Tianxing He",
      "Eli Shlizerman",
      "Uygar Sümbül"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/903c5eb12f2389c4847574df90503d63-Abstract-Conference.html": {
    "title": "GeoTMI: Predicting Quantum Chemical Property with Easy-to-Obtain Geometry via Positional Denoising",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonsu Kim",
      "Jeheon Woo",
      "SEONGHWAN KIM",
      "Seokhyun Moon",
      "Jun Hyeong Kim",
      "Woo Youn Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/903f778fe1341e5351b5b63e0e6b197f-Abstract-Conference.html": {
    "title": "PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yang",
      "Haiyang Wang",
      "Di Dai",
      "Liwei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9050e8d5b5de08d16e65dc79ad5c0146-Abstract-Conference.html": {
    "title": "Active Observing in Continuous-time Control",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Holt",
      "Alihan Hüyük",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9062b7d6e522dadf4f7d85d49b60d81e-Abstract-Conference.html": {
    "title": "Principled Weight Initialisation for Input-Convex Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pieter-Jan Hoedt",
      "Günter Klambauer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/906c860f1b7515a8ffec02dcdac74048-Abstract-Conference.html": {
    "title": "Automatic Grouping for Efficient Cooperative Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zang",
      "Jinmin He",
      "Kai Li",
      "Haobo Fu",
      "Qiang Fu",
      "Junliang Xing",
      "Jian Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/908f03779b5b063413fbf0247a46a403-Abstract-Conference.html": {
    "title": "On the Minimax Regret for Online Learning with Feedback Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khaled Eldowa",
      "Emmanuel Esposito",
      "Tom Cesari",
      "Nicolò Cesa-Bianchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9098e2901b4eb54772f83535f89cb8ac-Abstract-Conference.html": {
    "title": "DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Wang",
      "Junsong Fan",
      "Yuxi Wang",
      "Kaiyou Song",
      "Tong Wang",
      "ZHAO-XIANG ZHANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/909d6b6a7c6ac13ea51de4c4cace35db-Abstract-Conference.html": {
    "title": "Hierarchical VAEs provide a normative account of motion processing in the primate brain",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Vafaii",
      "Jacob Yates",
      "Daniel Butts"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/90bfd7201f6717b215e5dcfd987064da-Abstract-Conference.html": {
    "title": "Variational Gaussian Processes with Decoupled Conditionals",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Zhu",
      "Kaiwen Wu",
      "Natalie Maus",
      "Jacob Gardner",
      "David Bindel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/90debc7cedb5cac83145fc8d18378dc5-Abstract-Conference.html": {
    "title": "TabMT: Generating tabular data with masked transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manbir Gulati",
      "Paul Roysdon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/90e06fe49254204248cb12562528b952-Abstract-Conference.html": {
    "title": "Brain Dissection: fMRI-trained Networks Reveal Spatial Selectivity in the Processing of Natural Images",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Sarch",
      "Michael Tarr",
      "Katerina Fragkiadaki",
      "Leila Wehbe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/90e73f3cf1a6c84c723a2e8b7fb2b2c1-Abstract-Conference.html": {
    "title": "Action Inference by Maximising Evidence: Zero-Shot Imitation from Observation with World Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyuan Zhang",
      "Philip Becker-Ehmck",
      "Patrick van der Smagt",
      "Maximilian Karl"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/911dd89c81efc624c4e1c39381179505-Abstract-Conference.html": {
    "title": "ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided Diffusion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingjun Du",
      "Zehao Xiao",
      "Shengcai Liao",
      "Cees Snoek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/911fc798523e7d4c2e9587129fcf88fc-Abstract-Conference.html": {
    "title": "Synthetic Experience Replay",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Lu",
      "Philip Ball",
      "Yee Whye Teh",
      "Jack Parker-Holder"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/91228b942a4528cdae031c1b68b127e8-Abstract-Conference.html": {
    "title": "Learning to Tokenize for Generative Retrieval",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Sun",
      "Lingyong Yan",
      "Zheng Chen",
      "Shuaiqiang Wang",
      "Haichao Zhu",
      "Pengjie Ren",
      "Zhumin Chen",
      "Dawei Yin",
      "Maarten Rijke",
      "Zhaochun Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/915125efea950af378435518b3542e6a-Abstract-Conference.html": {
    "title": "A Reduction-based Framework for Sequential Decision Making with Delayed Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunchang Yang",
      "Han Zhong",
      "Tianhao Wu",
      "Bin Liu",
      "Liwei Wang",
      "Simon S. Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.html": {
    "title": "Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minshuo Chen",
      "Yu Bai",
      "H. Vincent Poor",
      "Mengdi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/916cb4e1aeafaa0757953c9bacd17337-Abstract-Conference.html": {
    "title": "Unified 3D Segmenter As Prototypical Classifiers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyun Qin",
      "Cheng Han",
      "Qifan Wang",
      "Xiushan Nie",
      "Yilong Yin",
      "Lu Xiankai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/917d55788726131e3bb21bf39d477f58-Abstract-Conference.html": {
    "title": "Estimating Causal Effects Identifiable from a Combination of Observations and Experiments",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonghan Jung",
      "Ivan Diaz",
      "Jin Tian",
      "Elias Bareinboim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/91813e5ddd9658b99be4c532e274b49c-Abstract-Conference.html": {
    "title": "LMC: Large Model Collaboration with Cross-assessment for Training-Free Open-Set Object Recognition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Qu",
      "Xiaofei Hui",
      "Yujun Cai",
      "Jun Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/91a5742235f70ae846436d9780e9f1d4-Abstract-Conference.html": {
    "title": "TaskMet: Task-driven Metric Learning for Model Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dishank Bansal",
      "Ricky T. Q. Chen",
      "Mustafa Mukadam",
      "Brandon Amos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/91b047c5f5bd41ef56bfaf4ad0bd19e3-Abstract-Conference.html": {
    "title": "Pairwise Causality Guided Transformers for Event Sequences",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Shou",
      "Debarun Bhattacharjya",
      "Tian Gao",
      "Dharmashankar Subramanian",
      "Oktie Hassanzadeh",
      "Kristin P Bennett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html": {
    "title": "Self-Refine: Iterative Refinement with Self-Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aman Madaan",
      "Niket Tandon",
      "Prakhar Gupta",
      "Skyler Hallinan",
      "Luyu Gao",
      "Sarah Wiegreffe",
      "Uri Alon",
      "Nouha Dziri",
      "Shrimai Prabhumoye",
      "Yiming Yang",
      "Shashank Gupta",
      "Bodhisattwa Prasad Majumder",
      "Katherine Hermann",
      "Sean Welleck",
      "Amir Yazdanbakhsh",
      "Peter Clark"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/91f9fb16b5679115a777ade51af87e48-Abstract-Conference.html": {
    "title": "Causal Discovery in Semi-Stationary Time Series",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanyun Gao",
      "Raghavendra Addanki",
      "Tong Yu",
      "Ryan Rossi",
      "Murat Kocaoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9200d97ca2bf3a26db7b591844014f00-Abstract-Conference.html": {
    "title": "Fine-grained Expressivity of Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Böker",
      "Ron Levie",
      "Ningyuan Huang",
      "Soledad Villar",
      "Christopher Morris"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9213010cbcd6ba8e1f1cf1533835d51c-Abstract-Conference.html": {
    "title": "Hierarchical Adaptive Value Estimation for Multi-modal Visual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangru Huang",
      "Peixi Peng",
      "Yifan Zhao",
      "Haoran Xu",
      "Mengyue Geng",
      "Yonghong Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/921dcb622bd0119c8f4f34644ce87ee0-Abstract-Conference.html": {
    "title": "Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgenii Chzhen",
      "Christophe Giraud",
      "Zhen LI",
      "Gilles Stoltz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/923285deb805c3e14e1aeebc9854d644-Abstract-Conference.html": {
    "title": "Neural Oscillators are Universal",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Lanthaler",
      "T. Konstantin Rusch",
      "Siddhartha Mishra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9235c376df778f1aaf486a882afb7471-Abstract-Conference.html": {
    "title": "PAC-Bayes Generalization Certificates for Learned Inductive Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Apoorva Sharma",
      "Sushant Veer",
      "Asher Hancock",
      "Heng Yang",
      "Marco Pavone",
      "Anirudha Majumdar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/92369a01fbe8046a093746389b2c413e-Abstract-Conference.html": {
    "title": "Image Captioners Are Scalable Vision Learners Too",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Tschannen",
      "Manoj Kumar",
      "Andreas Steiner",
      "Xiaohua Zhai",
      "Neil Houlsby",
      "Lucas Beyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9278abf072b58caf21d48dd670b4c721-Abstract-Conference.html": {
    "title": "CrossGNN: Confronting Noisy Multivariate Time Series Via Cross Interaction Refinement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihe Huang",
      "Lei Shen",
      "Ruixin Zhang",
      "Shouhong Ding",
      "Binwu Wang",
      "Zhengyang Zhou",
      "Yang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/927962d8866377a07ee3150d2d691319-Abstract-Conference.html": {
    "title": "Structured Prediction with Stronger Consistency Guarantees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Mao",
      "Mehryar Mohri",
      "Yutao Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/92bb2145c74b7d10fbb61aba315b5010-Abstract-Conference.html": {
    "title": "Explainable Brain Age Prediction using coVariance Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurabh Sihag",
      "Gonzalo Mateos",
      "Corey McMillan",
      "Alejandro Ribeiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/92d21245424f3898b7110f555a00e829-Abstract-Conference.html": {
    "title": "Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ambar Pal",
      "Jeremias Sulam",
      "Rene Vidal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/92d3d2a9801211ca3693ccb2faa1316f-Abstract-Conference.html": {
    "title": "Structured State Space Models for In-Context Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Lu",
      "Yannick Schroecker",
      "Albert Gu",
      "Emilio Parisotto",
      "Jakob Foerster",
      "Satinder Singh",
      "Feryal Behbahani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/92dd1adab39f362046f99dfe3c39d90f-Abstract-Conference.html": {
    "title": "Sharpness-Aware Minimization Leads to Low-Rank Features",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maksym Andriushchenko",
      "Dara Bahri",
      "Hossein Mobahi",
      "Nicolas Flammarion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9308d1b7d4ae2d3e2e67ae94b1078bf7-Abstract-Conference.html": {
    "title": "A Spectral Theory of Neural Prediction and Alignment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdulkadir Canatar",
      "Jenelle Feather",
      "Albert Wakhloo",
      "SueYeon Chung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9318763d049edf9a1f2779b2a59911d3-Abstract-Conference.html": {
    "title": "Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenzhi Wang",
      "Qisen Yang",
      "Jiawei Gao",
      "Matthieu Lin",
      "HAO CHEN",
      "Liwei Wu",
      "Ning Jia",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/931db0b5a61f9db6c97c7e4bf068147d-Abstract-Conference.html": {
    "title": "Test-Time Distribution Normalization for Contrastively Learned Visual-language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhou",
      "Juntao Ren",
      "Fengyu Li",
      "Ramin Zabih",
      "Ser Nam Lim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/932147114c48f8b04d41aebc0c631158-Abstract-Conference.html": {
    "title": "Propagating Knowledge Updates to LMs Through Distillation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shankar Padmanabhan",
      "Yasumasa Onoe",
      "Michael Zhang",
      "Greg Durrett",
      "Eunsol Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9328208f88ec69420031647e6ff97727-Abstract-Conference.html": {
    "title": "ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqi Chen",
      "Kan Ren",
      "Yansen Wang",
      "Yuchen Fang",
      "Weiwei Sun",
      "Dongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/933b5d002cf251b3e854d586e55ac58c-Abstract-Conference.html": {
    "title": "Differentiable Random Partition Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Sutter",
      "Alain Ryser",
      "Joram Liebeskind",
      "Julia Vogt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/93712c59f6a81bd92040facf04c8b308-Abstract-Conference.html": {
    "title": "Connecting Pre-trained Language Model and Downstream Task via Properties of Representation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenwei Wu",
      "Holden Lee",
      "Rong Ge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html": {
    "title": "Generalizable One-shot 3D Neural Head Avatar",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueting Li",
      "Shalini De Mello",
      "Sifei Liu",
      "Koki Nagano",
      "Umar Iqbal",
      "Jan Kautz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/93b3d975f9a2448964a906199db98a9d-Abstract-Conference.html": {
    "title": "Equivariant Single View Pose Prediction Via Induced and Restriction Representations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Owen Howell",
      "David Klee",
      "Ondrej Biza",
      "Linfeng Zhao",
      "Robin Walters"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/93b8618a9061f8a55825c13ecf28392b-Abstract-Conference.html": {
    "title": "Unsupervised Learning for Solving the Travelling Salesman Problem",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimeng Min",
      "Yiwei Bai",
      "Carla P. Gomes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/93cf20db85fabb0fd4bb89346510629c-Abstract-Conference.html": {
    "title": "ContinuAR: Continuous Autoregression For Infinite-Fidelity Fusion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "WEI XING",
      "Yuxin Wang",
      "Zheng Xing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/93e98ddf39a9beb0a97fbbe56a986c80-Abstract-Conference.html": {
    "title": "FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengzhong Liu",
      "Tomoyoshi Kimura",
      "Dongxin Liu",
      "Ruijie Wang",
      "Jinyang Li",
      "Suhas Diggavi",
      "Mani Srivastava",
      "Tarek Abdelzaher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/93ed74938a54a73b5e4c52bbaf42ca8e-Abstract-Conference.html": {
    "title": "Assumption violations in causal discovery and the robustness of score matching",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Montagna",
      "Atalanti Mastakouri",
      "Elias Eulig",
      "Nicoletta Noceti",
      "Lorenzo Rosasco",
      "Dominik Janzing",
      "Bryon Aragam",
      "Francesco Locatello"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/93fce71def4e3cf418918805455d436f-Abstract-Conference.html": {
    "title": "Normalizing flow neural networks by JKO scheme",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Xu",
      "Xiuyuan Cheng",
      "Yao Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9408564a4229f4a933ac9bd09a29ee96-Abstract-Conference.html": {
    "title": "Stability-penalty-adaptive follow-the-regularized-leader: Sparsity, game-dependency, and best-of-both-worlds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taira Tsuchiya",
      "Shinji Ito",
      "Junya Honda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/940a7634dab556b67af15bacd337f7db-Abstract-Conference.html": {
    "title": "Domain Agnostic Fourier Neural Operators",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Liu",
      "Siavash Jafarzadeh",
      "Yue Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/940f1d0760ca52c8b21ef3b661357ec2-Abstract-Conference.html": {
    "title": "$\\textbf{A}^2\\textbf{CiD}^2$: Accelerating Asynchronous Communication in Decentralized Deep Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adel Nabli",
      "Eugene Belilovsky",
      "Edouard Oyallon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9410d94d47adfb07b41a0b226270f068-Abstract-Conference.html": {
    "title": "MathNAS: If Blocks Have a Role in Mathematical Architecture Design",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinsi Wang",
      "Jinghan Ke",
      "Zhi Liang",
      "Sihai Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9417a5154519e370fd64e5a65e7dc59b-Abstract-Conference.html": {
    "title": "Block Broyden's Methods for Solving Nonlinear Equations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengchang Liu",
      "Cheng Chen",
      "Luo Luo",
      "John C.S. Lui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/942032b61720a3fd64897efe46237c81-Abstract-Conference.html": {
    "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grace Luo",
      "Lisa Dunlap",
      "Dong Huk Park",
      "Aleksander Holynski",
      "Trevor Darrell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/944ecf65a46feb578a43abfd5cddd960-Abstract-Conference.html": {
    "title": "No Change, No Gain: Empowering Graph Neural Networks with Expected Model Change Maximization for Active Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixing Song",
      "Yifei Zhang",
      "Irwin King"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/945c781d7194ea81026148838af95af7-Abstract-Conference.html": {
    "title": "Scaling Laws for Hyperparameter Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arlind Kadra",
      "Maciej Janowski",
      "Martin Wistuba",
      "Josif Grabocka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/94796017d01c5a171bdac520c199d9ed-Abstract-Conference.html": {
    "title": "A Robust and Opponent-Aware League Training Method for StarCraft II",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruozi Huang",
      "Xipeng Wu",
      "Hongsheng Yu",
      "Zhong Fan",
      "Haobo Fu",
      "Qiang Fu",
      "Wei Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/948552777302d3abf92415b1d7e9de70-Abstract-Conference.html": {
    "title": "Causal Fairness for Outcome Control",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Drago Plecko",
      "Elias Bareinboim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/948d8ba4e30c8c3a800cf436b31f376e-Abstract-Conference.html": {
    "title": "DeepPCR: Parallelizing Sequential Operations in Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Danieli",
      "Miguel Sarabia",
      "Xavier Suau Cuadros",
      "Pau Rodriguez",
      "Luca Zappella"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/949c57d30f8791e3ae42646081b3c102-Abstract-Conference.html": {
    "title": "DELTA: Diverse Client Sampling for Fasting Federated Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Wang",
      "Yongxin Guo",
      "Tao Lin",
      "Xiaoying Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/94ab02a30b0e4a692a42ccd0b4c55399-Abstract-Conference.html": {
    "title": "Conformal Meta-learners for Predictive Inference of Individual Treatment Effects",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmed M. Alaa",
      "Zaid Ahmad",
      "Mark van der Laan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html": {
    "title": "Simple and Controllable Music Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jade Copet",
      "Felix Kreuk",
      "Itai Gat",
      "Tal Remez",
      "David Kant",
      "Gabriel Synnaeve",
      "Yossi Adi",
      "Alexandre Defossez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/94bcb01789fccf15afe2764d8fe0f40e-Abstract-Conference.html": {
    "title": "Temporal Robustness against Data poisoning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxiao Wang",
      "Soheil Feizi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/94ccfdb2ca14f33a86a0b9b7d0c1bfb1-Abstract-Conference.html": {
    "title": "Optimal Treatment Regimes for Proximal Causal Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Shen",
      "Yifan Cui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/94d13c2401fe119e57ba325b6fe526e0-Abstract-Conference.html": {
    "title": "Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhong Yi Wan",
      "Ricardo Baptista",
      "Anudhyan Boral",
      "Yi-Fan Chen",
      "John Anderson",
      "Fei Sha",
      "Leonardo Zepeda-Núñez"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/94e85561a342de88b559b72c9b29f638-Abstract-Conference.html": {
    "title": "Transformers over Directed Acyclic Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuankai Luo",
      "Veronika Thost",
      "Lei Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9521b6e7f33e039e7d92e23f5e37bbf4-Abstract-Conference.html": {
    "title": "Understanding and Mitigating Copying in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gowthami Somepalli",
      "Vasu Singla",
      "Micah Goldblum",
      "Jonas Geiping",
      "Tom Goldstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/953390c834451505703c9da45de634d8-Abstract-Conference.html": {
    "title": "Credal Marginal MAP",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Radu Marinescu",
      "Debarun Bhattacharjya",
      "Junkyu Lee",
      "Fabio Cozman",
      "Alexander Gray"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/95464e2e49103dc560091ed2c64a5b12-Abstract-Conference.html": {
    "title": "Multi-task Representation Learning for Pure Exploration in Bilinear Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhojyoti Mukherjee",
      "Qiaomin Xie",
      "Josiah Hanna",
      "Robert Nowak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/955499a8e2860ed746717c1374224c43-Abstract-Conference.html": {
    "title": "Mechanic: A Learning Rate Tuner",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashok Cutkosky",
      "Aaron Defazio",
      "Harsh Mehta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/95827e011b9e899f189a01fe2f4ef316-Abstract-Conference.html": {
    "title": "Compositional Policy Learning in Stochastic Control Systems with Formal Guarantees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Đorđe Žikelić",
      "Mathias Lechner",
      "Abhinav Verma",
      "Krishnendu Chatterjee",
      "Thomas Henzinger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/959f70ee50044bed305e48e3484005a7-Abstract-Conference.html": {
    "title": "Fast Exact Leverage Score Sampling from Khatri-Rao Products with Applications to Tensor Decomposition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Bharadwaj",
      "Osman Asif Malik",
      "Riley Murray",
      "Laura Grigori",
      "Aydin Buluc",
      "James Demmel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/95a704bd2fdf8ef8242b4adcc7ce3c93-Abstract-Conference.html": {
    "title": "Online Performative Gradient Descent for Learning Nash Equilibria in Decision-Dependent Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Zhu",
      "Ethan Fang",
      "Zhuoran Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/95ab5c3e26fd82c7de3230bbad087d2d-Abstract-Conference.html": {
    "title": "AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiakang Yuan",
      "Bo Zhang",
      "Xiangchao Yan",
      "Botian Shi",
      "Tao Chen",
      "Yikang LI",
      "Yu Qiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/95b6e2ff961580e03c0a662a63a71812-Abstract-Conference.html": {
    "title": "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Hartvigsen",
      "Swami Sankaranarayanan",
      "Hamid Palangi",
      "Yoon Kim",
      "Marzyeh Ghassemi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/95b7a93e60fdfd10cc202f44fd6adf5f-Abstract-Conference.html": {
    "title": "On the Identifiability of Sparse ICA without Assuming Non-Gaussianity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ignavier Ng",
      "Yujia Zheng",
      "Xinshuai Dong",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9602d22a8c791f23f8e4d1398e3fb5be-Abstract-Conference.html": {
    "title": "Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong He",
      "Xinmeng Huang",
      "Kun Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/960573a3b797441aec39caa9f74bc793-Abstract-Conference.html": {
    "title": "Pareto Frontiers in Deep Feature Learning: Data, Compute, Width, and Luck",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Edelman",
      "Surbhi Goel",
      "Sham Kakade",
      "Eran Malach",
      "Cyril Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/96189e90e599ccc43f00434ff3ed0312-Abstract-Conference.html": {
    "title": "Reliable learning in challenging environments",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria-Florina F. Balcan",
      "Steve Hanneke",
      "Rattana Pukdee",
      "Dravyansh Sharma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/964b1c8dd5667fd647c09c8772829fd1-Abstract-Conference.html": {
    "title": "Retaining Beneficial Information from Detrimental Data for Neural Network Repair",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long-Kai Huang",
      "Peilin Zhao",
      "Junzhou Huang",
      "Sinno Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/96810b6d4752abe7bfb91f234c51e9e6-Abstract-Conference.html": {
    "title": "Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujie Xia",
      "Ziluo Ding",
      "Rui Zhao",
      "Jiyuan Zhang",
      "Lei Ma",
      "Zhaofei Yu",
      "Tiejun Huang",
      "Ruiqin Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/96842011407c2691ab4eefff48fc864d-Abstract-Conference.html": {
    "title": "No-regret Algorithms for Fair Resource Allocation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Sinha",
      "Ativ Joshi",
      "Rajarshi Bhattacharjee",
      "Cameron Musco",
      "Mohammad Hajiesmaili"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9690d4746230cfea3d067fca695ba648-Abstract-Conference.html": {
    "title": "Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josh Alman",
      "杰昊 梁",
      "Zhao Song",
      "Ruizhe Zhang",
      "Danyang Zhuo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/969c14957c0df5ce2db642b3a5fa985c-Abstract-Conference.html": {
    "title": "Online PCA in Converging Self-consistent Field Equations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xihan Li",
      "Xiang Chen",
      "Rasul Tutunov",
      "Haitham Bou Ammar",
      "Lei Wang",
      "Jun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/96a54c09569ebbdd9ecb22f5012e6b66-Abstract-Conference.html": {
    "title": "DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangtian Zhang",
      "Zuobai Zhang",
      "Bozitao Zhong",
      "Sanchit Misra",
      "Jian Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/96aca14d6c4dcd3adf54bc2c5ad7f138-Abstract-Conference.html": {
    "title": "ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Han Yao",
      "Amit Raj",
      "Wei-Chih Hung",
      "Michael Rubinstein",
      "Yuanzhen Li",
      "Ming-Hsuan Yang",
      "Varun Jampani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/96c6f409a374b5c81d2efa4bc5526f27-Abstract-Conference.html": {
    "title": "Noether Embedding: Efficient Learning of Temporal Regularities",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Gao",
      "Zidong Zhou",
      "Luping Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/96d00450ed65531ffe2996daed487536-Abstract-Conference.html": {
    "title": "$\\texttt{TACO}$: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Zheng",
      "Xiyao Wang",
      "Yanchao Sun",
      "Shuang Ma",
      "Jieyu Zhao",
      "Huazhe Xu",
      "Hal Daumé III",
      "Furong Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/96d328a1f6d8396d8c8a62f2beee252a-Abstract-Conference.html": {
    "title": "On the choice of Perception Loss Function for Learned Video Compression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadaf Salehkalaibar",
      "Truong Buu Phan",
      "Jun Chen",
      "Wei Yu",
      "Ashish Khisti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/96e35b532b4932a86cce8c929ff3f960-Abstract-Conference.html": {
    "title": "Imitation Learning from Vague Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin-Qiang Cai",
      "Yu-Jie Zhang",
      "Chao-Kai Chiang",
      "Masashi Sugiyama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9708c7d3a0fef3710f33ba05a74e10b3-Abstract-Conference.html": {
    "title": "Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen BAI",
      "Jean-Baptiste Durand",
      "Grégoire Vincent",
      "Florence Forbes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/970f59b22f4c72aec75174aae63c7459-Abstract-Conference.html": {
    "title": "Max-Margin Token Selection in Attention Mechanism",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davoud Ataee Tarzanagh",
      "Yingcong Li",
      "Xuechen Zhang",
      "Samet Oymak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9713d53ee4f31781304b1ca43266f8d1-Abstract-Conference.html": {
    "title": "Locality-Aware Generalizable Implicit Neural Representation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Doyup Lee",
      "Chiheon Kim",
      "Minsu Cho",
      "WOOK SHIN HAN"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/971f1e59cd956cc094da4e2f78c6ea7c-Abstract-Conference.html": {
    "title": "StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonglong Tian",
      "Lijie Fan",
      "Phillip Isola",
      "Huiwen Chang",
      "Dilip Krishnan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/972cd27c994a806e187ef1c2f5254059-Abstract-Conference.html": {
    "title": "DropCompute: simple and more robust distributed synchronous training via compute variance reduction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niv Giladi",
      "Shahar Gottlieb",
      "moran shkolnik",
      "Asaf Karnieli",
      "Ron Banner",
      "Elad Hoffer",
      "Kfir Y. Levy",
      "Daniel Soudry"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/973a0f50d43cf99118cdab456edcacda-Abstract-Conference.html": {
    "title": "A Unified Generalization Analysis of Re-Weighting and Logit-Adjustment for Imbalanced Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitai Wang",
      "Qianqian Xu",
      "Zhiyong Yang",
      "Yuan He",
      "Xiaochun Cao",
      "Qingming Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9768645621c2cd6c5b851a06205b92cf-Abstract-Conference.html": {
    "title": "Sketching Algorithms for Sparse Dictionary Learning: PTAS and Turnstile Streaming",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregory Dexter",
      "Petros Drineas",
      "David Woodruff",
      "Taisuke Yasuda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/976cc04f0cbaad7790ce0d665e44f90f-Abstract-Conference.html": {
    "title": "Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binhui Xie",
      "Shuang Li",
      "Qingju Guo",
      "Chi Liu",
      "Xinjing Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/97826456fb8c02fa368d673a49bbc563-Abstract-Conference.html": {
    "title": "Kissing to Find a Match: Efficient Low-Rank Permutation Representation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hannah Dröge",
      "Zorah Lähner",
      "Yuval Bahat",
      "Onofre Martorell Nadal",
      "Felix Heide",
      "Michael Moeller"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/97b73904e88cc1dc0a3485595eda3753-Abstract-Conference.html": {
    "title": "Creating Multi-Level Skill Hierarchies in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua B. Evans",
      "Özgür Şimşek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/97b983c974551153d20ddfabb62a5203-Abstract-Conference.html": {
    "title": "Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Grinsztajn",
      "Daniel Furelos-Blanco",
      "Shikha Surana",
      "Clément Bonnet",
      "Tom Barrett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/97c8a8eb0e5231d107d0da51b79e09cb-Abstract-Conference.html": {
    "title": "Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzheng Hu",
      "Ruicheng Xian",
      "Qilong Wu",
      "Qiuling Fan",
      "Lang Yin",
      "Han Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/97c903fbf21a7d863af2015d8803ca8f-Abstract-Conference.html": {
    "title": "Provable Guarantees for Generative Behavior Cloning: Bridging Low-Level Stability and High-Level Behavior",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Block",
      "Ali Jadbabaie",
      "Daniel Pfrommer",
      "Max Simchowitz",
      "Russ Tedrake"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/97d596ca21d0751ba2c633bad696cf7f-Abstract-Conference.html": {
    "title": "Prefix-Tree Decoding for Predicting Mass Spectra from Molecules",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Goldman",
      "John Bradshaw",
      "Jiayi Xin",
      "Connor Coley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/97faedc90260eae5c400f92d5831c3d7-Abstract-Conference.html": {
    "title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minki Kang",
      "Seanie Lee",
      "Jinheon Baek",
      "Kenji Kawaguchi",
      "Sung Ju Hwang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/97fe251c25b6f99a2a23b330a75b11d4-Abstract-Conference.html": {
    "title": "Nonparametric Identifiability of Causal Representations from Unknown Interventions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius von Kügelgen",
      "Michel Besserve",
      "Liang Wendong",
      "Luigi Gresele",
      "Armin Kekić",
      "Elias Bareinboim",
      "David Blei",
      "Bernhard Schölkopf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98143953a7fd1319175b491888fc8df5-Abstract-Conference.html": {
    "title": "Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Guo",
      "Shijie Ma",
      "Hu Su",
      "Zhiqing Wang",
      "Yuhao Zhao",
      "Wei Zou",
      "Siyang Sun",
      "Yun Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/982ca2640e64bf7a1908b028ebc8734a-Abstract-Conference.html": {
    "title": "Hierarchical Gaussian Mixture based Task Generative Model for Robust Meta-Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhou Zhang",
      "Jingchao Ni",
      "Wei Cheng",
      "Zhengzhang Chen",
      "Liang Tong",
      "Haifeng Chen",
      "Yan Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/983591c3e9a0dc94a99134b3238bbe52-Abstract-Conference.html": {
    "title": "Temporal Dynamic Quantization for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyuk So",
      "Jungwon Lee",
      "Daehyun Ahn",
      "Hyungjun Kim",
      "Eunhyeok Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9850e6a5410331290dc1deefb7514448-Abstract-Conference.html": {
    "title": "Learning Interpretable Low-dimensional Representation via Physical Symmetry",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanjie Liu",
      "Daniel Chin",
      "Yichen Huang",
      "Gus Xia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98530736e5d94e62b689dfc1fda89bd1-Abstract-Conference.html": {
    "title": "ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ya sheng sun",
      "Yifan Yang",
      "Houwen Peng",
      "Yifei Shen",
      "Yuqing Yang",
      "Han Hu",
      "Lili Qiu",
      "Hideki Koike"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/985786d06c1e45e9e8c65f7aca3547e4-Abstract-Conference.html": {
    "title": "Meek Separators and Their Applications in Targeted Causal Discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirankumar Shiragur",
      "Jiaqi Zhang",
      "Caroline Uhler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/986e0caad271b59417287737416d8594-Abstract-Conference.html": {
    "title": "CLeAR: Continual Learning on Algorithmic Reasoning for Human-like Intelligence",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bong Gyun Kang",
      "HyunGi Kim",
      "Dahuin Jung",
      "Sungroh Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/987bed997ab668f91c822a09bce3ea12-Abstract-Conference.html": {
    "title": "SLIBO-Net: Floorplan Reconstruction via Slicing Box Representation with Local Geometry Regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jheng-Wei Su",
      "Kuei-Yu Tung",
      "Chi-Han Peng",
      "Peter Wonka",
      "Hung-Kuo (James) Chu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98a5c0470e57d518ade4e56c6ee0b363-Abstract-Conference.html": {
    "title": "Fantastic Robustness Measures: The Secrets of Robust Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoki Kim",
      "Jinseong Park",
      "Yujin Choi",
      "Jaewook Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98b2b307aa4aa323df2ba3a83460f25e-Abstract-Conference.html": {
    "title": "A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilias Diakonikolas",
      "Daniel Kane",
      "Jasper Lee",
      "Ankit Pensia",
      "Thanasis Pittas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98c50f47a37f63477c01558600dd225a-Abstract-Conference.html": {
    "title": "Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simian Luo",
      "Chuanhao Yan",
      "Chenxu Hu",
      "Hang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98d0ad88db1e51bd0aa341a823290ece-Abstract-Conference.html": {
    "title": "Optimal Treatment Allocation for Efficient Policy Evaluation in Sequential Decision Making",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Li",
      "Chengchun Shi",
      "Jianing Wang",
      "Fan Zhou",
      "hongtu zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98e967164ae2f6811b975d686dece3eb-Abstract-Conference.html": {
    "title": "Advancing Bayesian Optimization via Learning Correlated Latent Space",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghun Lee",
      "Jaewon Chu",
      "Sihyeon Kim",
      "Juyeon Ko",
      "Hyunwoo J. Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98ed250b203d1ac6b24bbcf263e3d4a7-Abstract-Conference.html": {
    "title": "Generalization bounds for neural ordinary differential equations and deep residual networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Marion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98f8c89ae042c512e6c87e0e0c2a0f98-Abstract-Conference.html": {
    "title": "Global Update Tracking: A Decentralized Learning Algorithm for Heterogeneous Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Aparna Aketi",
      "Abolfazl Hashemi",
      "Kaushik Roy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9902a53031ebbbab73898028073d4790-Abstract-Conference.html": {
    "title": "QuadAttac$K$: A Quadratic Programming Approach to Learning Ordered Top-$K$ Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Paniagua",
      "Ryan Grainger",
      "Tianfu Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/99088dffd5eab0babebcda4bc58bbcea-Abstract-Conference.html": {
    "title": "Predicting mutational effects on protein-protein binding via a side-chain diffusion probabilistic model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwei Liu",
      "Tian Zhu",
      "Milong Ren",
      "Chungong Yu",
      "Dongbo Bu",
      "Haicang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/991c9324ca71aa85ab4dd11146b35fc3-Abstract-Conference.html": {
    "title": "PETAL: Physics Emulation Through Averaged Linearizations for Solving Inverse Problems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihui Jin",
      "Etienne Ollivier",
      "Richard Touret",
      "Matthew McKinley",
      "Karim Sabra",
      "Justin Romberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/995f693b73050f90977ed2828202645c-Abstract-Conference.html": {
    "title": "Learning Transformer Programs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Friedman",
      "Alexander Wettig",
      "Danqi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/996e2b446391fcb8bf32a3d1645cc799-Abstract-Conference.html": {
    "title": "An Inverse Scaling Law for CLIP Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianhang Li",
      "Zeyu Wang",
      "Cihang Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/99766cda865be123d55a1d9666c7b9fc-Abstract-Conference.html": {
    "title": "Sequential Preference Ranking for Efficient Reinforcement Learning from Human Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minyoung Hwang",
      "Gunmin Lee",
      "Hogun Kee",
      "Chan Woo Kim",
      "Kyungjae Lee",
      "Songhwai Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/99786eed5e16920f908572fb00e151c3-Abstract-Conference.html": {
    "title": "Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Ju Ho",
      "Chen-Hsuan Tai",
      "Yen-Yu Lin",
      "Ming-Hsuan Yang",
      "Yi-Hsuan Tsai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/99b419554537c66bf27e5eb7a74c7de4-Abstract-Conference.html": {
    "title": "Aligning Language Models with Human Preferences via a Bayesian Approach",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo WANG",
      "Haozhao Wang",
      "Shichao Sun",
      "Wenjie Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/99c41fb9fd53abfdd4a0259560ef1c9d-Abstract-Conference.html": {
    "title": "A Smooth Binary Mechanism for Efficient Private Continual Observation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Daniel Andersson",
      "Rasmus Pagh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/99fc8bc48b917c301a80cb74d91c0c06-Abstract-Conference.html": {
    "title": "Training Transformers with 4-bit Integers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haocheng Xi",
      "ChangHao Li",
      "Jianfei Chen",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9a08fbb992f15faa695c42b6a2c8e000-Abstract-Conference.html": {
    "title": "TD Convergence: An Optimization Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kavosh Asadi",
      "Shoham Sabach",
      "Yao Liu",
      "Omer Gottesman",
      "Rasool Fakoor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9a17c1eb808cf012065e9db47b7ca80d-Abstract-Conference.html": {
    "title": "Time Series as Images: Vision Transformer for Irregularly Sampled Time Series",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Li",
      "Shiyang Li",
      "Xifeng Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9a39b4925e35cf447ccba8757137d84f-Abstract-Conference.html": {
    "title": "Symbolic Discovery of Optimization Algorithms",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangning Chen",
      "Chen Liang",
      "Da Huang",
      "Esteban Real",
      "Kaiyuan Wang",
      "Hieu Pham",
      "Xuanyi Dong",
      "Thang Luong",
      "Cho-Jui Hsieh",
      "Yifeng Lu",
      "Quoc V Le"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9a645c38d4ec6f94633a35aeb2079596-Abstract-Conference.html": {
    "title": "On Calibrating Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Pang",
      "Cheng Lu",
      "Chao Du",
      "Min Lin",
      "Shuicheng Yan",
      "Zhijie Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9a6a435e75419a836fe47ab6793623e6-Abstract-Conference.html": {
    "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenliang Dai",
      "Junnan Li",
      "DONGXU LI",
      "Anthony Meng Huat Tiong",
      "Junqi Zhao",
      "Weisheng Wang",
      "Boyang Li",
      "Pascale N Fung",
      "Steven Hoi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9a6f6e0d6781d1cb8689192408946d73-Abstract-Conference.html": {
    "title": "Privacy Auditing with One (1) Training Run",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Steinke",
      "Milad Nasr",
      "Matthew Jagielski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9a8eb202c060b7d81f5889631cbcd47e-Abstract-Conference.html": {
    "title": "Kernel Stein Discrepancy thinning: a theoretical perspective of pathologies and a practical fix with regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clement Benard",
      "Brian Staber",
      "Sébastien Da Veiga"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9a9f4e15ad0d680429a3e0570a96f763-Abstract-Conference.html": {
    "title": "Punctuation-level Attack: Single-shot and Single Punctuation Can Fool Text Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "wenqiang wang",
      "Chongyang Du",
      "Tao Wang",
      "Kaihao Zhang",
      "Wenhan Luo",
      "Lin Ma",
      "Wei Liu",
      "Xiaochun Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9ab8da29b1eb3bec912a06e0879065cd-Abstract-Conference.html": {
    "title": "Towards Hybrid-grained Feature Interaction Selection for Deep Sparse Network",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuyuan Lyu",
      "Xing Tang",
      "Dugang Liu",
      "Chen Ma",
      "Weihong Luo",
      "Liang Chen",
      "xiuqiang He",
      "Xue (Steve) Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9adc8ada9183f4b9a007a02773fd8114-Abstract-Conference.html": {
    "title": "On the Asymptotic Learning Curves of Kernel Ridge Regression under Power-law Decay",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Li",
      "haobo Zhang",
      "Qian Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9af2b1d6acf561af9c4cf70d52c7a49d-Abstract-Conference.html": {
    "title": "Mechanism Design for Collaborative Normal Mean Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiding Chen",
      "Jerry Zhu",
      "Kirthevasan Kandasamy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9b01333262789ea3a65a5fab4c22feae-Abstract-Conference.html": {
    "title": "DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaipeng Zheng",
      "Huishuai Zhang",
      "Weiran Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9b35a0a20d617dc68ae98a7a57df2f51-Abstract-Conference.html": {
    "title": "High-dimensional Contextual Bandit Problem without Sparsity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junpei Komiyama",
      "Masaaki Imaizumi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9b6d7202750e8e32cd5270eb7fc131f7-Abstract-Conference.html": {
    "title": "Energy-Based Models for Anomaly Detection: A Manifold Diffusion Recovery Approach",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwoong Yoon",
      "Young-Uk Jin",
      "Yung-Kyun Noh",
      "Frank Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9b867f0e56c4c085ef1cfdad691db5f6-Abstract-Conference.html": {
    "title": "Characterizing the Optimal $0-1$ Loss for Multi-class Classification with a Test-time Attacker",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihui Dai",
      "Wenxin Ding",
      "Arjun Nitin Bhagoji",
      "Daniel Cullina",
      "Heather Zheng",
      "Ben Zhao",
      "Prateek Mittal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9b905031125e56a557db38dff4fa8d21-Abstract-Conference.html": {
    "title": "Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hezhe Qiao",
      "Guansong Pang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9b912f91a5e299472764377db6ca2431-Abstract-Conference.html": {
    "title": "Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiao Wang",
      "Yongyi Mao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9b91ee0da3bcd61905fcd89e770168fc-Abstract-Conference.html": {
    "title": "Exploiting Contextual Objects and Relations for 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Yang",
      "chunfeng yuan",
      "Ziqi Zhang",
      "Zhongang Qi",
      "Yan Xu",
      "Wei Liu",
      "Ying Shan",
      "Bing Li",
      "Weiping Yang",
      "Peng Li",
      "Yan Wang",
      "Weiming Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9bae70d354793a95fa18751888cea07d-Abstract-Conference.html": {
    "title": "Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yining Ma",
      "Zhiguang Cao",
      "Yeow Meng Chee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9bb93a3c1a424654aaea6f5b594e94d5-Abstract-Conference.html": {
    "title": "Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Zhu",
      "Paria Rashidinejad",
      "Jiantao Jiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9bbb3c3aa33616c55521e2f826c132bd-Abstract-Conference.html": {
    "title": "Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longlin Yu",
      "Tianyu Xie",
      "Yu Zhu",
      "Tong Yang",
      "Xiangyu Zhang",
      "Cheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9bbc8b6038603e6170e35f89e3c3e296-Abstract-Conference.html": {
    "title": "Geometry-Aware Adaptation for Pretrained Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Roberts",
      "Xintong Li",
      "Dyah Adila",
      "Sonia Cromp",
      "Tzu-Heng Huang",
      "Jitian Zhao",
      "Frederic Sala"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9be39b35906526b8d240056daac72c6f-Abstract-Conference.html": {
    "title": "A fast heuristic to optimize time-space tradeoff for large models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akifumi Imanishi",
      "Zijian Xu",
      "Masayuki Takagi",
      "Sixue Wang",
      "Emilio Castillo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9bf0810a4a1597a36d27ceea58667d92-Abstract-Conference.html": {
    "title": "A Unified Conditional Framework for Diffusion-based Image Restoration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Xiaoyu Shi",
      "Dasong Li",
      "Xiaogang Wang",
      "Jian Wang",
      "Hongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9bf12308ece130daa083fb21f7faf1b6-Abstract-Conference.html": {
    "title": "Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Yuan",
      "Qingyun Sun",
      "Xingcheng Fu",
      "Ziwei Zhang",
      "Cheng Ji",
      "Hao Peng",
      "Jianxin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9bf1962c5b65a243ee243bb03ff2c506-Abstract-Conference.html": {
    "title": "Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniket Das",
      "Dheeraj Nagaraj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9bfc2c20fa2f56a18397eafe1be8a50a-Abstract-Conference.html": {
    "title": "Flow Factorized Representation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Song",
      "Andy Keller",
      "Nicu Sebe",
      "Max Welling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c0efc0d84c263972af72bf70a2de533-Abstract-Conference.html": {
    "title": "Hierarchical Randomized Smoothing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Scholten",
      "Jan Schuchardt",
      "Aleksandar Bojchevski",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c256fa1965318b7fcb9ed104c265540-Abstract-Conference.html": {
    "title": "Stable Nonconvex-Nonconcave Training via Linear Interpolation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Pethick",
      "Wanyun Xie",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c2aa1e456ea543997f6927295196381-Abstract-Conference.html": {
    "title": "UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenliang Zhao",
      "Lujia Bai",
      "Yongming Rao",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c534edc7ac1d6438216311be6d42eb2-Abstract-Conference.html": {
    "title": "Coop: Memory is not a Commodity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhao Zhang",
      "Shihan Ma",
      "Peihong Liu",
      "Jinhui Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c537882044c8b5352c363e840872ddb-Abstract-Conference.html": {
    "title": "Learning with Explanation Constraints",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rattana Pukdee",
      "Dylan Sam",
      "J. Zico Kolter",
      "Maria-Florina F. Balcan",
      "Pradeep Ravikumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c6d29852a049218d70108bbf5c48dfe-Abstract-Conference.html": {
    "title": "On the Interplay between Social Welfare and Tractability of Equilibria",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ioannis Anagnostides",
      "Tuomas Sandholm"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c70cfa2e7d9328c649c94d50cbf8faf-Abstract-Conference.html": {
    "title": "Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Litu Rout",
      "Negin Raoof",
      "Giannis Daras",
      "Constantine Caramanis",
      "Alex Dimakis",
      "Sanjay Shakkottai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c7900fac04a701cbed83256b76dbaa3-Abstract-Conference.html": {
    "title": "Maximum State Entropy Exploration using Predecessor and Successor Representations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnav Kumar Jain",
      "Lucas Lehnert",
      "Irina Rish",
      "Glen Berseth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c93b3cd3bc60c0fe7b0c2d74a2da966-Abstract-Conference.html": {
    "title": "From Distribution Learning in Training to Gradient Search in Testing for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Li",
      "Jinpei Guo",
      "Runzhong Wang",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c940ba3be5bc9020ec74279d6e37c8a-Abstract-Conference.html": {
    "title": "Learning Curves for Noisy Heterogeneous Feature-Subsampled Ridge Ensembles",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Ruben",
      "Cengiz Pehlevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9ca825deb6ce588c96f880728d3b8aea-Abstract-Conference.html": {
    "title": "Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Li",
      "Jun Xiao",
      "Guikun Chen",
      "Jian Shao",
      "Yueting Zhuang",
      "Long Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9cf5fff2f85310e6ece5bc3a8489b6fa-Abstract-Conference.html": {
    "title": "BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Fan",
      "Gaspard Choné-Ducasse",
      "Mark Schmidt",
      "Christos Thrampoulidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d0f188c7947eacb0c07f709576824f6-Abstract-Conference.html": {
    "title": "Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maya Okawa",
      "Ekdeep S Lubana",
      "Robert Dick",
      "Hidenori Tanaka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d23562fcedc078e27a3be813ff6feb5-Abstract-Conference.html": {
    "title": "Extracting Reward Functions from Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Nuti",
      "Tim Franzmeyer",
      "João F. Henriques"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d276b0a087efdd2404f3295b26c24c1-Abstract-Conference.html": {
    "title": "Disentangling Voice and Content with Self-Supervision for Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "TIANCHI LIU",
      "Kong Aik Lee",
      "Qiongqiong Wang",
      "Haizhou Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d30c2def27b5c6a5fb21a9aa5c16f8f-Abstract-Conference.html": {
    "title": "Automatic Integration for Spatiotemporal Neural Point Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Zhou",
      "Rose Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d3a4cdf6f70559e8c6fe02170fba568-Abstract-Conference.html": {
    "title": "Identifiability Guarantees for Causal Disentanglement from Soft Interventions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Zhang",
      "Kristjan Greenewald",
      "Chandler Squires",
      "Akash Srivastava",
      "Karthikeyan Shanmugam",
      "Caroline Uhler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d5856318032ef3630cb580f4e24f823-Abstract-Conference.html": {
    "title": "Equivariant Adaptation of Large Pretrained Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnab Kumar Mondal",
      "Siba Smarak Panigrahi",
      "Oumar Kaba",
      "Sai Rajeswar Mudumba",
      "Siamak Ravanbakhsh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d75de47462ffe77addaa7b985fc6d8e-Abstract-Conference.html": {
    "title": "Provable Training for Graph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Yu",
      "Xiao Wang",
      "Mengmei Zhang",
      "Nian Liu",
      "Chuan Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d87a0c38431d0ec8d8b8ece95198c04-Abstract-Conference.html": {
    "title": "Generalized Weighted Path Consistency for Mastering Atari Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dengwei Zhao",
      "Shikui Tu",
      "Lei Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d89448b63ce1e2e8dc7af72c984c196-Abstract-Conference.html": {
    "title": "Scaling Data-Constrained Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niklas Muennighoff",
      "Alexander Rush",
      "Boaz Barak",
      "Teven Le Scao",
      "Nouamane Tazi",
      "Aleksandra Piktus",
      "Sampo Pyysalo",
      "Thomas Wolf",
      "Colin A. Raffel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d8cf1247786d6dfeefeeb53b8b5f6d7-Abstract-Conference.html": {
    "title": "A Definition of Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Abel",
      "Andre Barreto",
      "Benjamin Van Roy",
      "Doina Precup",
      "Hado P. van Hasselt",
      "Satinder Singh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d8ed3c9e27a9265ee60c8edba3dec1d-Abstract-Conference.html": {
    "title": "A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minkyu Choi",
      "Kuan Han",
      "Xiaokai Wang",
      "Yizhen Zhang",
      "Zhongming Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9dc5accb1e4f4a9798eae145f2e4869b-Abstract-Conference.html": {
    "title": "When Do Transformers Shine in RL? Decoupling Memory from Credit Assignment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianwei Ni",
      "Michel Ma",
      "Benjamin Eysenbach",
      "Pierre-Luc Bacon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9dd67d30e0edd53581363c1b49006e1d-Abstract-Conference.html": {
    "title": "Hypothesis Selection with Memory Constraints",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maryam Aliakbarpour",
      "Mark Bun",
      "Adam Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9dfcc83c01e94d02c751c47517855c9f-Abstract-Conference.html": {
    "title": "Optimization or Architecture: How to Hack Kalman Filtering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ido Greenberg",
      "Netanel Yannay",
      "Shie Mannor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9e15d892c63903ecc278e0dd05536951-Abstract-Conference.html": {
    "title": "Online robust non-stationary estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abishek Sankararaman",
      "Balakrishnan Narayanaswamy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9e30acdeff572463c1db9b7de59de64c-Abstract-Conference.html": {
    "title": "POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonin Vobecky",
      "Oriane Siméoni",
      "David Hurych",
      "Spyridon Gidaris",
      "Andrei Bursuc",
      "Patrick Pérez",
      "Josef Sivic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9e720fce64f91114c49cfd640d821da3-Abstract-Conference.html": {
    "title": "Faster Relative Entropy Coding with Greedy Rejection Coding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gergely Flamich",
      "Stratis Markou",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9e8889198d16fb79926e71adbe38cae4-Abstract-Conference.html": {
    "title": "Sparse Parameterization for Epitomic Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Wei",
      "Anjia Cao",
      "Funing Yang",
      "Zhiheng Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9ed1c94a6c87276f25ebb65231c86c3e-Abstract-Conference.html": {
    "title": "HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junkun Yuan",
      "Xinyu Zhang",
      "Hao Zhou",
      "Jian Wang",
      "Zhongwei Qiu",
      "Zhiyin Shao",
      "Shaofeng Zhang",
      "Sifan Long",
      "Kun Kuang",
      "Kun Yao",
      "Junyu Han",
      "Errui Ding",
      "Lanfen Lin",
      "Fei Wu",
      "Jingdong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9eda77f505efbb89462970d739143f73-Abstract-Conference.html": {
    "title": "Trust Your $\\nabla$: Gradient-based Intervention Targeting for Causal Discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mateusz Olko",
      "Michał Zając",
      "Aleksandra Nowak",
      "Nino Scherrer",
      "Yashas Annadani",
      "Stefan Bauer",
      "Łukasz Kuciński",
      "Piotr Miłoś"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9ee3a664ccfeabc0da16ac6f1f1cfe59-Abstract-Conference.html": {
    "title": "SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuseung Lee",
      "Kunho Kim",
      "Hyunjin Kim",
      "Minhyuk Sung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9ef237e007e26180ce4d16738efdf83f-Abstract-Conference.html": {
    "title": "Deep learning with kernels through RKHM and the Perron-Frobenius operator",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuka Hashimoto",
      "Masahiro Ikeda",
      "Hachem Kadri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9ef5e965720193681fc8d16372ac4717-Abstract-Conference.html": {
    "title": "SmoothHess: ReLU Network Feature Interactions via Stein's Lemma",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Torop",
      "Aria Masoomi",
      "Davin Hill",
      "Kivanc Kose",
      "Stratis Ioannidis",
      "Jennifer Dy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9f09f316a3eaf59d9ced5ffaefe97e0f-Abstract-Conference.html": {
    "title": "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephanie Fu",
      "Netanel Tamir",
      "Shobhita Sundaram",
      "Lucy Chai",
      "Richard Zhang",
      "Tali Dekel",
      "Phillip Isola"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9f0b1220028dfa2ee82ca0a0e0fc52d1-Abstract-Conference.html": {
    "title": "Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siu Lun Chau",
      "Krikamol Muandet",
      "Dino Sejdinovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9f4064d145bad5e361206c3303bda7b8-Abstract-Conference.html": {
    "title": "Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotao Wang",
      "Ziyu Jiang",
      "Yuning You",
      "Yan Han",
      "Gaowen Liu",
      "Jayanth Srinivasa",
      "Ramana Kompella",
      "Zhangyang \"Atlas\" Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9f42f06a54ce3b709ad78d34c73e4363-Abstract-Conference.html": {
    "title": "Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quentin Delfosse",
      "Hikaru Shindo",
      "Devendra Dhami",
      "Kristian Kersting"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9f6f790f28a31fba89644f09faf4e0cb-Abstract-Conference.html": {
    "title": "Personalized Dictionary Learning for Heterogeneous Datasets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geyu Liang",
      "Naichen Shi",
      "Raed AL Kontar",
      "Salar Fattahi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9f7f2f57d8eaf44b2f09020f64ff6d96-Abstract-Conference.html": {
    "title": "Graph-Structured Gaussian Processes for Transferable Graph Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Wu",
      "Lisa Ainsworth",
      "Andrew Leakey",
      "Haixun Wang",
      "Jingrui He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9f94298bac4668db4dc77ddb0a244301-Abstract-Conference.html": {
    "title": "Language Models are Weak Learners",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hariharan Manikandan",
      "Yiding Jiang",
      "J. Zico Kolter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9fa03b16dbd6cabc7601fe98c6ec291e-Abstract-Conference.html": {
    "title": "SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wu",
      "Jingyu Hu",
      "Wuyue Lu",
      "Igor Gilitschenski",
      "Animesh Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9fc291fef2f9607a46777d367f900a15-Abstract-Conference.html": {
    "title": "ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Kou",
      "Jin Gao",
      "Bing Li",
      "Gang Wang",
      "Weiming Hu",
      "Yizheng Wang",
      "Liang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9febda1c8344cc5f2d51713964864e93-Abstract-Conference.html": {
    "title": "Improving neural network representations using human similarity judgments",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Muttenthaler",
      "Lorenz Linhardt",
      "Jonas Dippel",
      "Robert A. Vandermeulen",
      "Katherine Hermann",
      "Andrew Lampinen",
      "Simon Kornblith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a00548031e4647b13042c97c922fadf1-Abstract-Conference.html": {
    "title": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Wen",
      "Neel Jain",
      "John Kirchenbauer",
      "Micah Goldblum",
      "Jonas Geiping",
      "Tom Goldstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a0251e494a7e75d59e06d37e646f46b7-Abstract-Conference.html": {
    "title": "Bilevel Coreset Selection in Continual Learning: A New Formulation and Algorithm",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Hao",
      "Kaiyi Ji",
      "Mingrui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a03037317560b8c5f2fb4b6466d4c439-Abstract-Conference.html": {
    "title": "HyP-NeRF: Learning Improved NeRF Priors using a HyperNetwork",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bipasha Sen",
      "Gaurav Singh",
      "Aditya Agarwal",
      "Rohith Agaram",
      "Madhava Krishna",
      "Srinath Sridhar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a0673542a242759ea637972f053b2e0b-Abstract-Conference.html": {
    "title": "Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengcheng Wang",
      "Wei He",
      "Ying Nie",
      "Jianyuan Guo",
      "Chuanjian Liu",
      "Yunhe Wang",
      "Kai Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a07e5160196058120105ad7cb3505d3c-Abstract-Conference.html": {
    "title": "Curriculum Learning for Graph Neural Networks: Which Edges Should We Learn First",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "Junxiang Wang",
      "Liang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a07e87ecfa8a651d62257571669b0150-Abstract-Conference.html": {
    "title": "Unified Lower Bounds for Interactive High-dimensional Estimation under Information Constraints",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jayadev Acharya",
      "Clément L Canonne",
      "Ziteng Sun",
      "Himanshu Tyagi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a0a53fefef4c2ad72d5ab79703ba70cb-Abstract-Conference.html": {
    "title": "Differentiable Registration of Images and LiDAR Point Clouds with VoxelPoint-to-Pixel Matching",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsheng Zhou",
      "Baorui Ma",
      "Wenyuan Zhang",
      "Yi Fang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a0b1082fc7823c4c68abcab4fa850e9c-Abstract-Conference.html": {
    "title": "Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Connor Toups",
      "Rishi Bommasani",
      "Kathleen Creel",
      "Sarah Bana",
      "Dan Jurafsky",
      "Percy S. Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a0da690a47b2f52faa63f6fe054057b5-Abstract-Conference.html": {
    "title": "MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitao Tang",
      "Fuyang Zhang",
      "Jiacheng Chen",
      "Peng Wang",
      "Yasutaka Furukawa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a0e66093d7168b40246af1cddc025daa-Abstract-Conference.html": {
    "title": "The geometry of hidden representations of large transformer models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucrezia Valeriani",
      "Diego Doimo",
      "Francesca Cuturello",
      "Alessandro Laio",
      "Alessio Ansuini",
      "Alberto Cazzaniga"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a102d6cb996be3482c059c1e18bbe523-Abstract-Conference.html": {
    "title": "Django: Detecting Trojans in Object Detection Models via Gaussian Focus Calibration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyu Shen",
      "Siyuan Cheng",
      "Guanhong Tao",
      "Kaiyuan Zhang",
      "Yingqi Liu",
      "Shengwei An",
      "Shiqing Ma",
      "Xiangyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a103529738706979331778377f2d5864-Abstract-Conference.html": {
    "title": "CORNN: Convex optimization of recurrent neural networks for rapid inference of neural dynamics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatih Dinc",
      "Adam Shai",
      "Mark Schnitzer",
      "Hidenori Tanaka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a10946e1f46e1ffc0daf37cb2abfdcad-Abstract-Conference.html": {
    "title": "A Unified Framework for Rank-based Loss Minimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rufeng Xiao",
      "Yuze Ge",
      "Rujun Jiang",
      "Yifan Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a10da26f47120217c1b7c2aeb2979048-Abstract-Conference.html": {
    "title": "LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kensen Shi",
      "Hanjun Dai",
      "Wen-Ding Li",
      "Kevin Ellis",
      "Charles Sutton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a124b5e7385d35e5c8ad05d192106e19-Abstract-Conference.html": {
    "title": "HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack on Text",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Liu",
      "Zhi Xu",
      "Xiaotong Zhang",
      "Feng Zhang",
      "Fenglong Ma",
      "Hongyang Chen",
      "Hong Yu",
      "Xianchao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a12779b5e802668df1cbc73fa00da62f-Abstract-Conference.html": {
    "title": "Augmentation-free Dense Contrastive Distillation for Efficient Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Fan",
      "Chao Li",
      "Xiaolong Liu",
      "Meina Song",
      "Anbang Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a17251f8d595179eef5e466b1f5f7a85-Abstract-Conference.html": {
    "title": "Coneheads: Hierarchy Aware Attention",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert Tseng",
      "Tao Yu",
      "Toni Liu",
      "Christopher M. De Sa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a1c716638d9b618a1a40a96f473c8250-Abstract-Conference.html": {
    "title": "Vulnerabilities in Video Quality Assessment Models: The Challenge of Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aoxiang Zhang",
      "Yu Ran",
      "Weixuan Tang",
      "Yuan-Gen Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a1c8a68e52499c9396854e3f967e37c0-Abstract-Conference.html": {
    "title": "Unsupervised Behavior Extraction via Random Intent Priors",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Hu",
      "Yiqin Yang",
      "Jianing Ye",
      "Ziqing Mai",
      "Chongjie Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a1d20cc72a21ef971d7e49a90d8fa56f-Abstract-Conference.html": {
    "title": "Deconstructing Data Reconstruction: Multiclass, Weight Decay and General Losses",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gon Buzaglo",
      "Niv Haim",
      "Gilad Yehudai",
      "Gal Vardi",
      "Yakir Oz",
      "Yaniv Nikankin",
      "Michal Irani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a1e6783e4d739196cad3336f12d402bf-Abstract-Conference.html": {
    "title": "Information Maximizing Curriculum: A Curriculum-Based Approach for Learning Versatile Skills",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Blessing",
      "Onur Celik",
      "Xiaogang Jia",
      "Moritz Reuss",
      "Maximilian Li",
      "Rudolf Lioutikov",
      "Gerhard Neumann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a1f0c0cd6caaa4863af5f12608edf63e-Abstract-Conference.html": {
    "title": "Unleash the Potential of Image Branch for Cross-modal 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhang",
      "Qijian Zhang",
      "Junhui Hou",
      "Yixuan Yuan",
      "Guoliang Xing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a204aa68ab4e970e1ceccfb5b5cdc5e4-Abstract-Conference.html": {
    "title": "Model Sparsity Can Simplify Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "jinghan jia",
      "Jiancheng Liu",
      "Parikshit Ram",
      "Yuguang Yao",
      "Gaowen Liu",
      "Yang Liu",
      "PRANAY SHARMA",
      "Sijia Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a216c27f2f3160b1785c057fa510fdf1-Abstract-Conference.html": {
    "title": "IDRNet: Intervention-Driven Relation Network for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenchao Jin",
      "Xiaowei Hu",
      "Lingting Zhu",
      "Luchuan Song",
      "Li Yuan",
      "Lequan Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a23598416361c7a9860164155e6ddd0b-Abstract-Conference.html": {
    "title": "Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayal Singh Kalra",
      "Maissam Barkeshli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a2370db7c99791ad5d9f3ef48ad6d464-Abstract-Conference.html": {
    "title": "Neural Algorithmic Reasoning Without Intermediate Supervision",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gleb Rodionov",
      "Liudmila Prokhorenkova"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a2374637af47ac9471b43c99b68acf27-Abstract-Conference.html": {
    "title": "On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangha Park",
      "Jisoo Mok",
      "Dahuin Jung",
      "Saehyung Lee",
      "Sungroh Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a237f11d6aad94f59a182d70405d3fdb-Abstract-Conference.html": {
    "title": "Estimating Propensity for Causality-based Recommendation without Exposure Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongzhou Liu",
      "Yuan Fang",
      "Min Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a24a75ef009ee73b160653c16b18f00e-Abstract-Conference.html": {
    "title": "A Robust Exact Algorithm for the Euclidean Bipartite Matching Problem",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshaykumar Gattani",
      "Sharath Raghvendra",
      "Pouyan Shirzadian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a24cd16bc361afa78e57d31d34f3d936-Abstract-Conference.html": {
    "title": "Content-based Unrestricted Adversarial Attack",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyu Chen",
      "Bo Li",
      "Shuang Wu",
      "Kaixun Jiang",
      "Shouhong Ding",
      "Wenqiang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a264726ebd222124514a32bf0143b83d-Abstract-Conference.html": {
    "title": "On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Lin Hau",
      "Erick Delage",
      "Mohammad Ghavamzadeh",
      "Marek Petrik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a2cf225ba392627529efef14dc857e22-Abstract-Conference.html": {
    "title": "Evaluating the Moral Beliefs Encoded in LLMs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nino Scherrer",
      "Claudia Shi",
      "Amir Feder",
      "David Blei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a2e707354da36956945dbb288efe82b3-Abstract-Conference.html": {
    "title": "Enhancing Adversarial Robustness via Score-Based Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boya Zhang",
      "Weijian Luo",
      "Zhihua Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a2fe4bb50fc6f3564cee1551d6309fea-Abstract-Conference.html": {
    "title": "Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giorgio Giannone",
      "Akash Srivastava",
      "Ole Winther",
      "Faez Ahmed"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a3017a8d202a433be56a3dfdcac6c8eb-Abstract-Conference.html": {
    "title": "Optimal cross-learning for contextual bandits with unknown context distributions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jon Schneider",
      "Julian Zimmert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a31253f4871694f09541122d6b6f5ad1-Abstract-Conference.html": {
    "title": "Conservative Offline Policy Adaptation in Multi-Agent Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengjie Wu",
      "Pingzhong Tang",
      "Jun Yang",
      "Yujing Hu",
      "Tangjie Lv",
      "Changjie Fan",
      "Chongjie Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a344f7f474958cc0775be7e46bc94309-Abstract-Conference.html": {
    "title": "Bounding the Invertibility of Privacy-preserving Instance Encoding using Fisher Information",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiwan Maeng",
      "Chuan Guo",
      "Sanjay Kariyappa",
      "G. Edward Suh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a345ed605675c7c484e740a8ceaa6b45-Abstract-Conference.html": {
    "title": "Adjustable Robust Reinforcement Learning for Online 3D Bin Packing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Pan",
      "Yize Chen",
      "Fangzhen Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a355051cc32d36e2a971de190701745a-Abstract-Conference.html": {
    "title": "Promises and Pitfalls of Threshold-based Auto-labeling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harit Vishwakarma",
      "Heguang Lin",
      "Frederic Sala",
      "Ramya Korlakai Vinayak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a3621ee907def47c1b952ade25c67698-Abstract-Conference.html": {
    "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guohao Li",
      "Hasan Hammoud",
      "Hani Itani",
      "Dmitrii Khizbullin",
      "Bernard Ghanem"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a36c3dbe676fa8445715a31a90c66ab3-Abstract-Conference.html": {
    "title": "SUBP: Soft Uniform Block Pruning for 1$\\times$N Sparse CNNs Multithreading Acceleration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JINGYANG XIANG",
      "Siqi Li",
      "Jun Chen",
      "Guang Dai",
      "Shipeng Bai",
      "Yukai Ma",
      "Yong Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a399456a191ca36c7c78dff367887f0a-Abstract-Conference.html": {
    "title": "Adaptive Linear Estimating Equations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mufang Ying",
      "Koulik Khamaru",
      "Cun-Hui Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a39ab46bf619ada0e90ceed846648a81-Abstract-Conference.html": {
    "title": "Robust Knowledge Transfer in Tiered Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Huang",
      "Niao He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a3a661eb3308d0bb686f6a4bac521032-Abstract-Conference.html": {
    "title": "Bypassing the Simulator: Near-Optimal Adversarial Linear Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolin Liu",
      "Chen-Yu Wei",
      "Julian Zimmert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a3c01875a052f81d27a5211df096cd91-Abstract-Conference.html": {
    "title": "Generalization in the Face of Adaptivity: A Bayesian Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moshe Shenfeld",
      "Katrina Ligett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a3cc50126338b175e56bb3cad134db0b-Abstract-Conference.html": {
    "title": "Convergence of Adam Under Relaxed Assumptions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochuan Li",
      "Alexander Rakhlin",
      "Ali Jadbabaie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a3cf318fbeec1126da21e9185ae9908c-Abstract-Conference.html": {
    "title": "On the Convergence of Encoder-only Shallow Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongtao Wu",
      "Fanghui Liu",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a42d8f43fae4d267e3084b10056153f7-Abstract-Conference.html": {
    "title": "Accelerated On-Device Forward Neural Network Training with Module-Wise Descending Asynchronism",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Zhao",
      "Hualin Zhang",
      "Zhouyuan Huo",
      "Bin Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4316bb210a59fb7aafeca5dd21c2703-Abstract-Conference.html": {
    "title": "Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Chen",
      "Zhihang Fu",
      "Kai Liu",
      "Ze Chen",
      "Mingyuan Tao",
      "Jieping Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a439259e78294c38d157a51a2c40486b-Abstract-Conference.html": {
    "title": "Unbalanced Low-rank Optimal Transport Solvers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meyer Scetbon",
      "Michal Klein",
      "Giovanni Palla",
      "Marco Cuturi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a45296e83b19f656392e0130d9e53cb1-Abstract-Conference.html": {
    "title": "Geodesic Multi-Modal Mixup for Robust Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changdae Oh",
      "Junhyuk So",
      "Hoyoon Byun",
      "YongTaek Lim",
      "Minchul Shin",
      "Jong-June Jeon",
      "Kyungwoo Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html": {
    "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichang Liu",
      "Aditya Desai",
      "Fangshuo Liao",
      "Weitao Wang",
      "Victor Xie",
      "Zhaozhuo Xu",
      "Anastasios Kyrillidis",
      "Anshumali Shrivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a45b205c10ef082515cacae80555bbef-Abstract-Conference.html": {
    "title": "Asymmetric Certified Robustness via Feature-Convex Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Pfrommer",
      "Brendon Anderson",
      "Julien Piet",
      "Somayeh Sojoudi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a45d344b28179c8da7646bc38ff50ad8-Abstract-Conference.html": {
    "title": "A Unified Fast Gradient Clipping Framework for DP-SGD",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Kong",
      "Andres Munoz Medina"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a46c84276e3a4249ab7dbf3e069baf7f-Abstract-Conference.html": {
    "title": "Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangsen Wang",
      "Haoran Xu",
      "Yinan Zheng",
      "Xianyuan Zhan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a48e5877c7bf86a513950ab23b360498-Abstract-Conference.html": {
    "title": "A Logic for Expressing Log-Precision Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Merrill",
      "Ashish Sabharwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4a1ee071ce0fe63b83bce507c9dc4d7-Abstract-Conference.html": {
    "title": "Universal Prompt Tuning for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taoran Fang",
      "Yunchao Zhang",
      "YANG YANG",
      "Chunping Wang",
      "Lei Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4b6ad6b48850c0c331d1259fc66a69c-Abstract-Conference.html": {
    "title": "Stochastic Approximation Approaches to Group Distributionally Robust Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Zhang",
      "Peng Zhao",
      "Zhen-Hua Zhuang",
      "Tianbao Yang",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4c3a66ed818455b8bbe591b6a5d0f56-Abstract-Conference.html": {
    "title": "Learning Efficient Surrogate Dynamic Models with Graph Spline Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanbo Hua",
      "Federico Berto",
      "Michael Poli",
      "Stefano Massaroli",
      "Jinkyoo Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4ca07aa108036f80cbb5b82285fd4b1-Abstract-Conference.html": {
    "title": "Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Dong",
      "Dawei Yan",
      "Zhijun Lin",
      "Peng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4d92f656cc99f60fe1bfc98386aee34-Abstract-Conference.html": {
    "title": "Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tamas Sarlos",
      "Xingyou Song",
      "David Woodruff",
      "Richard Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4ddb865e0a8ca3cca43fd7387b4b0da-Abstract-Conference.html": {
    "title": "Efficient Training of Energy-Based Models Using Jarzynski Equality",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Carbone",
      "Mengjian Hua",
      "Simon Coste",
      "Eric Vanden-Eijnden"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4dfcbcfa1f0425cd18aafa35a68019a-Abstract-Conference.html": {
    "title": "High Precision Causal Model Evaluation with Conditional Randomization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Ma",
      "Cheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4e720ce31ccd8ba747d8863e1580fa8-Abstract-Conference.html": {
    "title": "Reducing Blackwell and Average Optimality to Discounted MDPs via the Blackwell Discount Factor",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Grand-Clément",
      "Marek Petrik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a51f974947c42b40a40a882a7d9b2479-Abstract-Conference.html": {
    "title": "Marginal Density Ratio for Off-Policy Evaluation in Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Faaiz Taufiq",
      "Arnaud Doucet",
      "Rob Cornish",
      "Jean-Francois Ton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a526cc8f6ffb74bedb6ff313e3fdb450-Abstract-Conference.html": {
    "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Yu",
      "Yong Cheng",
      "Zhiruo Wang",
      "Vivek Kumar",
      "Wolfgang Macherey",
      "Yanping Huang",
      "David Ross",
      "Irfan Essa",
      "Yonatan Bisk",
      "Ming-Hsuan Yang",
      "Kevin P. Murphy",
      "Alexander Hauptmann",
      "Lu Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a52b0d191b619477cc798d544f4f0e4b-Abstract-Conference.html": {
    "title": "Energy-based learning algorithms for analog computing: a comparative study",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Scellier",
      "Maxence Ernoult",
      "Jack Kendall",
      "Suhas Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a5321f64005b0d4a94d0b18e84e19f48-Abstract-Conference.html": {
    "title": "Distribution Learnability and Robustness",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shai Ben-David",
      "Alex Bie",
      "Gautam Kamath",
      "Tosca Lechner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a5357781c204d4412e44ed9cbcdb08d5-Abstract-Conference.html": {
    "title": "Behavior Alignment via Reward Function Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhawal Gupta",
      "Yash Chandak",
      "Scott Jordan",
      "Philip S. Thomas",
      "Bruno C. da Silva"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a547d86953a4e36aa8a1390e6f4708e2-Abstract-Conference.html": {
    "title": "Tuning Multi-mode Token-level Prompt Alignment across Modalities",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongsheng Wang",
      "Miaoge Li",
      "Xinyang Liu",
      "MingSheng Xu",
      "Bo Chen",
      "Hanwang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a5755ccd0efeca8852ae0a1193f319f6-Abstract-Conference.html": {
    "title": "Censored Sampling of Diffusion Models Using 3 Minutes of Human Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "TaeHo Yoon",
      "Kibeom Myoung",
      "Keon Lee",
      "Jaewoong Cho",
      "Albert No",
      "Ernest Ryu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a598c367280f9054434fdcc227ce4d38-Abstract-Conference.html": {
    "title": "Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Klein",
      "Andrew Foong",
      "Tor Fjelde",
      "Bruno Mlodozeniec",
      "Marc Brockschmidt",
      "Sebastian Nowozin",
      "Frank Noe",
      "Ryota Tomioka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a5c47c1b7adf19e8dc633812a4acf6d2-Abstract-Conference.html": {
    "title": "Harnessing Hard Mixed Samples with Decoupled Regularizer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Liu",
      "Siyuan Li",
      "Ge Wang",
      "Lirong Wu",
      "Cheng Tan",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a5e146ca55a2b18be41942cfa677123d-Abstract-Conference.html": {
    "title": "The Utility of \"Even if\" Semifactual Explanation to Optimise Positive Outcomes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eoin Kenny",
      "Weipeng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a5e3cf29c269b041ccd644b6beaf5c42-Abstract-Conference.html": {
    "title": "VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Yin",
      "Muchao Ye",
      "Tianrong Zhang",
      "Tianyu Du",
      "Jinguo Zhu",
      "Han Liu",
      "Jinghui Chen",
      "Ting Wang",
      "Fenglong Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a5e4907a40c0dcb8433a35c714ba9d79-Abstract-Conference.html": {
    "title": "Mode Connectivity in Auction Design",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Hertrich",
      "Yixin Tao",
      "László A. Végh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a60c43ba078b723d3d517d28c50ded4c-Abstract-Conference.html": {
    "title": "Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Súkeník",
      "Marco Mondelli",
      "Christoph H. Lampert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a61023ce36d21010f1423304f8ec49af-Abstract-Conference.html": {
    "title": "IEBins: Iterative Elastic Bins for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuwei Shao",
      "Zhongcai Pei",
      "Xingming Wu",
      "Zhong Liu",
      "Weihai Chen",
      "Zhengguo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a627810151be4d13f907ac898ff7e948-Abstract-Conference.html": {
    "title": "Fine-Tuning Language Models with Just Forward Passes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadhika Malladi",
      "Tianyu Gao",
      "Eshaan Nichani",
      "Alex Damian",
      "Jason D. Lee",
      "Danqi Chen",
      "Sanjeev Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a64e641fa00a7eb9500cb7e1835d0495-Abstract-Conference.html": {
    "title": "HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Zhang",
      "Chen Junnan",
      "Guohuan Gao",
      "Jianmin Li",
      "Xiaolin Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a6678e2be4ce7aef9d2192e03cd586b7-Abstract-Conference.html": {
    "title": "FedGame: A Game-Theoretic Defense against Backdoor Attacks in Federated Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyuan Jia",
      "Zhuowen Yuan",
      "Dinuka Sahabandu",
      "Luyao Niu",
      "Arezoo Rajabi",
      "Bhaskar Ramasubramanian",
      "Bo Li",
      "Radha Poovendran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a68120d2eb2f53f7d9e71547591aef11-Abstract-Conference.html": {
    "title": "Ensemble-based Deep Reinforcement Learning for Vehicle Routing Problems under Distribution Shift",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "YUAN JIANG",
      "Zhiguang Cao",
      "Yaoxin Wu",
      "Wen Song",
      "Jie Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a6a1e4c756d700d9aedcc1896a7e6fb0-Abstract-Conference.html": {
    "title": "Module-wise Training of Neural Networks via the Minimizing Movement Scheme",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Skander Karkar",
      "Ibrahim Ayed",
      "Emmanuel de Bézenac",
      "Patrick Gallinari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a6d7226db2ff3643d8624624e3859c19-Abstract-Conference.html": {
    "title": "POMDP Planning for Object Search in Partially Unknown Environment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongbo Chen",
      "Hanna Kurniawati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a6df53f082619d02b9fad64a022e5de3-Abstract-Conference.html": {
    "title": "On the Statistical Consistency of Risk-Sensitive Bayesian Decision-Making",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Jaiswal",
      "Harsha Honnappa",
      "Vinayak Rao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a6efa49c54bedf4411f1bcd32f15937a-Abstract-Conference.html": {
    "title": "Does Graph Distillation See Like Vision Dataset Counterpart?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beining Yang",
      "Kai Wang",
      "Qingyun Sun",
      "Cheng Ji",
      "Xingcheng Fu",
      "Hao Tang",
      "Yang You",
      "Jianxin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a6f2763089c0bd8f56006c42f09ee24c-Abstract-Conference.html": {
    "title": "Online Learning under Adversarial Nonlinear Constraints",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Kolev",
      "Georg Martius",
      "Michael Muehlebach"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a6f6a5c517b2b92f3d309786af64086c-Abstract-Conference.html": {
    "title": "Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojun Kim",
      "Yongjae Shin",
      "Jongeui Park",
      "Youngchul Sung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a71c1931d3fb8ba564f7458d0657d0b1-Abstract-Conference.html": {
    "title": "Im-Promptu: In-Context Composition from Image Prompts",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhishma Dedhia",
      "Michael Chang",
      "Jake Snell",
      "Tom Griffiths",
      "Niraj Jha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a72b207734d6112f6b47447e46be40e9-Abstract-Conference.html": {
    "title": "Sequential Predictive Two-Sample and Independence Testing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandr Podkopaev",
      "Aaditya Ramdas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a73474c359ed523e6cd3174ed29a4d56-Abstract-Conference.html": {
    "title": "Towards Symmetry-Aware Generation of Periodic Materials",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youzhi Luo",
      "Chengkai Liu",
      "Shuiwang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a75db7d2ee1e4bee8fb819979b0a6cad-Abstract-Conference.html": {
    "title": "SALSA VERDE: a machine learning attack on LWE with sparse small secrets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cathy Li",
      "Emily Wenger",
      "Zeyuan Allen-Zhu",
      "Francois Charton",
      "Kristin E. Lauter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a766f56d2da42cae20b5652970ec04ef-Abstract-Conference.html": {
    "title": "Designing Robust Transformers using Robust Kernel Density Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Han",
      "Tongzheng Ren",
      "Tan Nguyen",
      "Khai Nguyen",
      "Joydeep Ghosh",
      "Nhat Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a76b693f36916a5ed84d6e5b39a0dc03-Abstract-Conference.html": {
    "title": "Weakly Supervised 3D Open-vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunhao Liu",
      "Fangneng Zhan",
      "Jiahui Zhang",
      "MUYU XU",
      "Yingchen Yu",
      "Abdulmotaleb El Saddik",
      "Christian Theobalt",
      "Eric Xing",
      "Shijian Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a79699db176ed0efc04a9da171e52112-Abstract-Conference.html": {
    "title": "Better Private Linear Regression Through Better Private Feature Selection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Travis Dick",
      "Jennifer Gillenwater",
      "Matthew Joseph"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a797c2d2e0c1fdabf4d1ab8cd0b465c6-Abstract-Conference.html": {
    "title": "Geometric Neural Diffusion Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emile Mathieu",
      "Vincent Dutordoir",
      "Michael Hutchinson",
      "Valentin De Bortoli",
      "Yee Whye Teh",
      "Richard Turner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a7a7180fe7f82ff98eee0827c5e9c141-Abstract-Conference.html": {
    "title": "Online Adaptive Policy Selection in Time-Varying Systems: No-Regret via Contractive Perturbations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Lin",
      "James A. Preiss",
      "Emile Anand",
      "Yingying Li",
      "Yisong Yue",
      "Adam Wierman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a7affe50ab177b9a7f0a05f07a9ca205-Abstract-Conference.html": {
    "title": "Learning Multi-agent Behaviors from Distributed and Streaming Demonstrations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicheng Liu",
      "Minghui Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a7e0d77325db843fd5baf1298163e89a-Abstract-Conference.html": {
    "title": "CAPro: Webly Supervised Learning with Cross-modality Aligned Prototypes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulei Qin",
      "Xingyu Chen",
      "Yunhang Shen",
      "Chaoyou Fu",
      "Yun Gu",
      "Ke Li",
      "Xing Sun",
      "Rongrong Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a815fe7cad6af20a6c118f2072a881d2-Abstract-Conference.html": {
    "title": "Diversify \\& Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daesol Cho",
      "Seungjae Lee",
      "H. Jin Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a81dc87f7b3b7ab8489d5bb48c4a8d92-Abstract-Conference.html": {
    "title": "Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shang Liu",
      "Zhongze Cai",
      "Xiaocheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a8223b0ad64007423ffb308b0dd92298-Abstract-Conference.html": {
    "title": "Synthetic-to-Real Pose Estimation with Geometric Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuxia Lin",
      "Kerui Gu",
      "Linlin Yang",
      "Angela Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a834ac3dfdb90da54292c2c932c997cc-Abstract-Conference.html": {
    "title": "Parallel Spiking Neurons with High Efficiency and Ability to Learn Long-term Dependencies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Fang",
      "Zhaofei Yu",
      "Zhaokun Zhou",
      "Ding Chen",
      "Yanqi Chen",
      "Zhengyu Ma",
      "Timothée Masquelier",
      "Yonghong Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a845fdc3f87751710218718adb634fe7-Abstract-Conference.html": {
    "title": "Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihui (Sherry) Xue",
      "Kristen Grauman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a8526465a91166fbb90aaa8452b21eda-Abstract-Conference.html": {
    "title": "Training Private Models That Know What They Don't Know",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephan Rabanser",
      "Anvith Thudi",
      "Abhradeep Guha Thakurta",
      "Krishnamurthy Dvijotham",
      "Nicolas Papernot"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html": {
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rafael Rafailov",
      "Archit Sharma",
      "Eric Mitchell",
      "Christopher D Manning",
      "Stefano Ermon",
      "Chelsea Finn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a86b7a9bf7647d6f9f9168d8167d9283-Abstract-Conference.html": {
    "title": "Diffusion Representation for Asymmetric Kernels via Magnetic Transform",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhen He",
      "FAN He",
      "Ruikai Yang",
      "Xiaolin Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a86d17b6cd70366d56ab48d2a05a4df1-Abstract-Conference.html": {
    "title": "Uncovering the Hidden Dynamics of Video Self-supervised Learning under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pritam Sarkar",
      "Ahmad Beirami",
      "Ali Etemad"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a899a801fab59f14777fcc08842b6fc5-Abstract-Conference.html": {
    "title": "Projection-Free Methods for Solving Nonconvex-Concave Saddle Point Problems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Morteza Boroun",
      "Erfan Yazdandoost Hamedani",
      "Afrooz Jalilzadeh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a8b148559549ce33261e79b4400e0d77-Abstract-Conference.html": {
    "title": "Better Correlation and Robustness: A Distribution-Balanced Self-Supervised Learning Framework for Automatic Dialogue Evaluation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiwen Yuan",
      "Xinglin Wang",
      "Jiayi Shi",
      "Bin Sun",
      "Yiwei Li",
      " Prof. Kan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a8c893712cb7858e49631fb03c941f8d-Abstract-Conference.html": {
    "title": "Learning Topology-Agnostic EEG Representations with Geometry-Aware Modeling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Yi",
      "Yansen Wang",
      "Kan Ren",
      "Dongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a8e21789027e92739f89df92cc172bcf-Abstract-Conference.html": {
    "title": "Correlation Aware Sparsified Mean Estimation Using Random Projection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuli Jiang",
      "PRANAY SHARMA",
      "Gauri Joshi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a8f2713b5c6bdcd3d264f1aa9b9c6f03-Abstract-Conference.html": {
    "title": "Accelerating Value Iteration with Anchoring",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongmin Lee",
      "Ernest Ryu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a8f7f12b29d9b8c227785f6b529f63b7-Abstract-Conference.html": {
    "title": "Echoes Beyond Points: Unleashing the Power of Raw Radar Data in Multi-modality Fusion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Feng Wang",
      "Naiyan Wang",
      "ZHAO-XIANG ZHANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a901d5540789a086ee0881a82211b63d-Abstract-Conference.html": {
    "title": "Effective Bayesian Heteroscedastic Regression with Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Immer",
      "Emanuele Palumbo",
      "Alexander Marx",
      "Julia Vogt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a924b7178e5975dfed1de235f0b72973-Abstract-Conference.html": {
    "title": "Multi-task learning with summary statistics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Parker Knight",
      "Rui Duan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a935ba2236c6ba0fb620f23354e789ff-Abstract-Conference.html": {
    "title": "Estimating Noise Correlations Across Continuous Conditions With Wishart Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Nejatbakhsh",
      "Isabel Garon",
      "Alex Williams"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a94a8800a4b0af45600bab91164849df-Abstract-Conference.html": {
    "title": "Toward Understanding Generative Data Augmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Zheng",
      "Guoqiang Wu",
      "Chongxuan LI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a95cc4f370bcc418e7b57d6512e28f52-Abstract-Conference.html": {
    "title": "TOA: Task-oriented Active VQA",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "xiaoying xing",
      "Mingfu Liang",
      "Ying Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a961dea42c23c3c0d01b79918701fb6e-Abstract-Conference.html": {
    "title": "Universal Gradient Descent Ascent Method for Nonconvex-Nonconcave Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taoli Zheng",
      "Linglingzhi Zhu",
      "Anthony Man-Cho So",
      "Jose Blanchet",
      "Jiajin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a97b58c4f7551053b0512f92244b0810-Abstract-Conference.html": {
    "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqing Zhao",
      "Tianyu Pang",
      "Chao Du",
      "Xiao Yang",
      "Chongxuan LI",
      "Ngai-Man (Man) Cheung",
      "Min Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a97f0218b49bc17ea3f121a0e724f028-Abstract-Conference.html": {
    "title": "Generator Born from Classifier",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runpeng Yu",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a97f8072e51a785434b2da3e9cbf5aae-Abstract-Conference.html": {
    "title": "Scattering Vision Transformer: Spectral Mixing Matters",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Badri Patro",
      "Vijay Agneeswaran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a995960dd0193654d6b18eca4ac5b936-Abstract-Conference.html": {
    "title": "Task-aware world model learning with meta weighting via bi-level optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huining Yuan",
      "Hongkun Dou",
      "Xingyu Jiang",
      "Yue Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a99f50fb024a56d15f057a1830ed0a00-Abstract-Conference.html": {
    "title": "LEPARD: Learning Explicit Part Discovery for 3D Articulated Shape Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Liu",
      "Anastasis Stathopoulos",
      "Qilong Zhangli",
      "Yunhe Gao",
      "Dimitris Metaxas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a9ad92a81748a31ef6f2ef68d775da46-Abstract-Conference.html": {
    "title": "Curriculum Learning With Infant Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saber Sheybani",
      "Himanshu Hansaria",
      "Justin Wood",
      "Linda Smith",
      "Zoran Tiganj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a9bbeb2858dfbdbd4c19814e5d80ec60-Abstract-Conference.html": {
    "title": "MarioGPT: Open-Ended Text2Level Generation through Large Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shyam Sudhakaran",
      "Miguel González-Duque",
      "Matthias Freiberger",
      "Claire Glanois",
      "Elias Najarro",
      "Sebastian Risi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a9ea92ef18aae17627d133534209e640-Abstract-Conference.html": {
    "title": "Is Learning in Games Good for the Learners?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Brown",
      "Jon Schneider",
      "Kiran Vodrahalli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa31dc84098add7dd2ffdd20646f2043-Abstract-Conference.html": {
    "title": "The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Noci",
      "Chuning Li",
      "Mufan Li",
      "Bobby He",
      "Thomas Hofmann",
      "Chris J. Maddison",
      "Dan Roy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa31eee8f2351176ddd4d14646d4a950-Abstract-Conference.html": {
    "title": "Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingyu Weng",
      "Jun Xiao",
      "Haiyong Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa32ebcdd2ce1bed4ef7f456fc8fa5c1-Abstract-Conference.html": {
    "title": "Long Sequence Hopfield Memory",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamza Chaudhry",
      "Jacob Zavatone-Veth",
      "Dmitry Krotov",
      "Cengiz Pehlevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa3e67220ca4cd50010165c950fc8056-Abstract-Conference.html": {
    "title": "Provably Safe Reinforcement Learning with Step-wise Violation Constraints",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuoya Xiong",
      "Yihan Du",
      "Longbo Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa5c083f9d387c49514eb5c4dc2dc16b-Abstract-Conference.html": {
    "title": "Human spatiotemporal pattern learning as probabilistic program synthesis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tracey Mills",
      "Josh Tenenbaum",
      "Samuel Cheyette"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa5d22c77b380e2261332bb641b3c2e3-Abstract-Conference.html": {
    "title": "Fair Allocation of Indivisible Chores: Beyond Additive Costs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Fangxiao Wang",
      "Yu Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa5f224975a67914067519faddeacba3-Abstract-Conference.html": {
    "title": "Robust Second-Order Nonconvex Optimization and Its Application to Low Rank Matrix Sensing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyao Li",
      "Yu Cheng",
      "Ilias Diakonikolas",
      "Jelena Diakonikolas",
      "Rong Ge",
      "Stephen Wright"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa61d142c0081a8259a6372a3bb0af2b-Abstract-Conference.html": {
    "title": "Incentivized Communication for Federated Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhepei Wei",
      "Chuanhao Li",
      "Haifeng Xu",
      "Hongning Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa6287ca31ae1474ea802342d0c8ba63-Abstract-Conference.html": {
    "title": "Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Guo",
      "Yiming Li",
      "Lixu Wang",
      "Shu-Tao Xia",
      "Heng Huang",
      "Cong Liu",
      "Bo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa933b5abc1be30baece1d230ec575a7-Abstract-Conference.html": {
    "title": "Guide Your Agent with Adaptive Multimodal Rewards",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changyeon Kim",
      "Younggyo Seo",
      "Hao Liu",
      "Lisa Lee",
      "Jinwoo Shin",
      "Honglak Lee",
      "Kimin Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aaa973f65b98c96e5f850d706464a3c4-Abstract-Conference.html": {
    "title": "Fine-Grained Theoretical Analysis of Federated Zeroth-Order Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Chen",
      "Hong Chen",
      "Bin Gu",
      "Hao Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aaa9c20f0a217a1aef6fa5d97f310292-Abstract-Conference.html": {
    "title": "Sparse Deep Learning for Time Series Data: Theory and Applications",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxuan Zhang",
      "Yan Sun",
      "Faming Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aad615d33ba5071045656ba24d800c7b-Abstract-Conference.html": {
    "title": "Imbalanced Mixed Linear Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pini Zilber",
      "Boaz Nadler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ab003a4f85ecb1b7b1514ff539dc7395-Abstract-Conference.html": {
    "title": "GAN You See Me? Enhanced Data Reconstruction Attacks against Split Inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziang Li",
      "Mengda Yang",
      "Yaxin Liu",
      "Juan Wang",
      "Hongxin Hu",
      "Wenzhe Yi",
      "Xiaoyang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ab05dc8bf36a9f66edbff6992ec86f56-Abstract-Conference.html": {
    "title": "Random-Access Infinite Context Length for Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirkeivan Mohtashami",
      "Martin Jaggi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ab0b1be09c317cb068aecfa7fa86a7e3-Abstract-Conference.html": {
    "title": "Egocentric Planning for Scalable Embodied Task Achievement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiatoian Liu",
      "Hector Palacios",
      "Christian Muise"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ab3f114401f0523ca1cc09de0621f400-Abstract-Conference.html": {
    "title": "Removing Hidden Confounding in Recommendation: A Unified Multi-Task Learning Approach",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxuan Li",
      "Kunhan Wu",
      "Chunyuan Zheng",
      "Yanghao Xiao",
      "Hao Wang",
      "Zhi Geng",
      "Fuli Feng",
      "Xiangnan He",
      "Peng Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ab59d149fc0c2c9039d3e3049f7914b1-Abstract-Conference.html": {
    "title": "Generative Category-level Object Pose Estimation via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyao Zhang",
      "Mingdong Wu",
      "Hao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ab5a2bf4385bee44f3919060b184605b-Abstract-Conference.html": {
    "title": "On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Serrurier",
      "Franck Mamalet",
      "Thomas FEL",
      "Louis Béthune",
      "Thibaut Boissin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ab6e7ad2354f350b451b5a8e14d04f51-Abstract-Conference.html": {
    "title": "DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Wu",
      "Yuzhong Zhao",
      "Hao Chen",
      "Yuchao Gu",
      "Rui Zhao",
      "Yefei He",
      "Hong Zhou",
      "Mike Zheng Shou",
      "Chunhua Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ab9f9cfe97da3665e08f50ade9f8c4d6-Abstract-Conference.html": {
    "title": "No-Regret Online Prediction with Strategic Experts",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omid Sadeghi",
      "Maryam Fazel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/abb4847bbd60f38b1b7649d26c7a0067-Abstract-Conference.html": {
    "title": "Learning Unseen Modality Interaction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhua Zhang",
      "Hazel Doughty",
      "Cees Snoek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/abbb7f20cdffdd3bb7d98447f60b0b0c-Abstract-Conference.html": {
    "title": "Autonomous Capability Assessment of Sequential Decision-Making Systems in Stochastic Settings",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pulkit Verma",
      "Rushang Karia",
      "Siddharth Srivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/abbbb25cddb2c2cd08714e6bfa2f0634-Abstract-Conference.html": {
    "title": "Model-Free Active Exploration in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessio Russo",
      "Alexandre Proutiere"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/abccb8a90b30d45b948360ba41f5a20f-Abstract-Conference.html": {
    "title": "Universality laws for Gaussian mixtures in generalized linear models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yatin Dandi",
      "Ludovic Stephan",
      "Florent Krzakala",
      "Bruno Loureiro",
      "Lenka Zdeborová"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/abe1eb21ceb046209c96a0f5e7544ccc-Abstract-Conference.html": {
    "title": "ALGO: Synthesizing Algorithmic Programs with Generated Oracle Verifiers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kexun Zhang",
      "Danqing Wang",
      "Jingtao Xia",
      "William Yang Wang",
      "Lei Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/abe31a12e83111fdf2cfd54deed5a2ce-Abstract-Conference.html": {
    "title": "Private Everlasting Prediction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moni Naor",
      "Kobbi Nissim",
      "Uri Stemmer",
      "Chao Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/abf3682c9cf9245a0294a4bebe4544ff-Abstract-Conference.html": {
    "title": "A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas FEL",
      "Victor Boutin",
      "Louis Béthune",
      "Remi Cadene",
      "Mazda Moayeri",
      "Léo Andéol",
      "Mathieu Chalvidal",
      "Thomas Serre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac0035c349f3fe8af6a93fe44697b5bd-Abstract-Conference.html": {
    "title": "An Iterative Self-Learning Framework for Medical Domain Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenbang Wu",
      "Huaxiu Yao",
      "David Liebovitz",
      "Jimeng Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac04e54e0a2d1927d60709019e4e7870-Abstract-Conference.html": {
    "title": "Structure of universal formulas",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitry Yarotsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac112e8ffc4e5b9ece32070440a8ca43-Abstract-Conference.html": {
    "title": "Model-enhanced Vector Index",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hailin Zhang",
      "Yujing Wang",
      "Qi Chen",
      "Ruiheng Chang",
      "Ting Zhang",
      "Ziming Miao",
      "Yingyan Hou",
      "Yang Ding",
      "Xupeng Miao",
      "Haonan Wang",
      "Bochen Pang",
      "Yuefeng Zhan",
      "Hao Sun",
      "Weiwei Deng",
      "Qi Zhang",
      "Fan Yang",
      "Xing Xie",
      "Mao Yang",
      "Bin CUI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac24656b0b5f543b202f748d62041637-Abstract-Conference.html": {
    "title": "Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxiang Gao",
      "Xiaokai Huo",
      "Hailiang Liu",
      "Hongyang Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac58b418745b3e5f10c80110c963969f-Abstract-Conference.html": {
    "title": "Tree Variational Autoencoders",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura Manduchi",
      "Moritz Vandenhirtz",
      "Alain Ryser",
      "Julia Vogt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac5c594dedf66affb098c39a3bcfdb3d-Abstract-Conference.html": {
    "title": "Dynamo-Depth: Fixing Unsupervised Depth Estimation for Dynamical Scenes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Sun",
      "Bharath Hariharan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac662d74829e4407ce1d126477f4a03a-Abstract-Conference.html": {
    "title": "LIMA: Less Is More for Alignment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunting Zhou",
      "Pengfei Liu",
      "Puxin Xu",
      "Srinivasan Iyer",
      "Jiao Sun",
      "Yuning Mao",
      "Xuezhe Ma",
      "Avia Efrat",
      "Ping Yu",
      "LILI YU",
      "Susan Zhang",
      "Gargi Ghosh",
      "Mike Lewis",
      "Luke Zettlemoyer",
      "Omer Levy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac6de776b8de8c9aed1d356997eb54b8-Abstract-Conference.html": {
    "title": "Bi-Level Offline Policy Optimization with Limited Exploration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhuo Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/acb3e20075b0a2dfa3565f06681578e5-Abstract-Conference.html": {
    "title": "Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chendi Wang",
      "Buxin Su",
      "Jiayuan Ye",
      "Reza Shokri",
      "Weijie Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/acb7ce5aab6e134300a2361dd90a501f-Abstract-Conference.html": {
    "title": "On the Role of Entanglement and Statistics in Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srinivasan Arunachalam",
      "Vojtech Havlicek",
      "Louis Schatzki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/acddda9cd6f310689f7657f947705a99-Abstract-Conference.html": {
    "title": "Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "weitao Du",
      "Jiujiu Chen",
      "Xuecang Zhang",
      "Zhi-Ming Ma",
      "Shengchao Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/acf2b98eeb09b21968c2de6b1c6952e9-Abstract-Conference.html": {
    "title": "Federated Learning with Manifold Regularization and Normalized Update Reaggregation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuming An",
      "Li Shen",
      "Han Hu",
      "Yong Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/acf4a08f67724e9d2de34099f57a9c25-Abstract-Conference.html": {
    "title": "Long-Term Fairness with Unknown Dynamics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongxin Yin",
      "Reilly Raab",
      "Mingyan Liu",
      "Yang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad02c6f3824f871395112ae71a28eff7-Abstract-Conference.html": {
    "title": "Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandra Nowak",
      "Bram Grooten",
      "Decebal Constantin Mocanu",
      "Jacek Tabor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad08767706825033b99122332293033d-Abstract-Conference.html": {
    "title": "Tree-Based Diffusion Schrödinger Bridge with Applications to Wasserstein Barycenters",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxence Noble",
      "Valentin De Bortoli",
      "Arnaud Doucet",
      "Alain Durmus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad1efab57a04d93f097e7fbb2d4fc054-Abstract-Conference.html": {
    "title": "Bucks for Buckets (B4B): Active Defenses Against Stealing Encoders",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Dubiński",
      "Stanisław Pawlak",
      "Franziska Boenisch",
      "Tomasz Trzcinski",
      "Adam Dziedzic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad1f2197941348b1c4373fd6c19ee0b4-Abstract-Conference.html": {
    "title": "A General Framework for Equivariant Neural Networks on Reductive Lie Groups",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilyes Batatia",
      "Mario Geiger",
      "Jose Munoz",
      "Tess Smidt",
      "Lior Silberman",
      "Christoph Ortner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad2fa437f7c23e4e9875599c6065d18a-Abstract-Conference.html": {
    "title": "Context-PIPs: Persistent Independent Particles Demands Context Features",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weikang Bian",
      "Zhaoyang Huang",
      "Xiaoyu Shi",
      "Yitong Dong",
      "Yijin Li",
      "Hongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad350eaaa93c8c3ab762cdc119d12889-Abstract-Conference.html": {
    "title": "GloptiNets: Scalable Non-Convex Optimization with Certificates",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaspard Beugnot",
      "Julien Mairal",
      "Alessandro Rudi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad48f017e6c3d474caf511208e600459-Abstract-Conference.html": {
    "title": "Meta-Adapter: An Online Few-shot Learner for Vision-Language Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "cheng cheng",
      "Lin Song",
      "Ruoyi Xue",
      "Hang Wang",
      "Hongbin Sun",
      "Yixiao Ge",
      "Ying Shan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad58c61c71efd5436134a3ecc87da6ea-Abstract-Conference.html": {
    "title": "Taming Local Effects in Graph-based Spatiotemporal Forecasting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Cini",
      "Ivan Marisca",
      "Daniele Zambon",
      "Cesare Alippi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad5fa03c906ca15905144ca3fbf2a768-Abstract-Conference.html": {
    "title": "Latent Space Translation via Semantic Alignment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentino Maiorca",
      "Luca Moschella",
      "Antonio Norelli",
      "Marco Fumero",
      "Francesco Locatello",
      "Emanuele Rodolà"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad6a3bd12095fdca71c306871bdec400-Abstract-Conference.html": {
    "title": "A case for reframing automated medical image classification as segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Hooper",
      "Mayee Chen",
      "Khaled Saab",
      "Kush Bhatia",
      "Curtis Langlotz",
      "Christopher Ré"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad72633e034990a97e878fc2fc100afb-Abstract-Conference.html": {
    "title": "Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "wonje choi",
      "Woo Kyung Kim",
      "SeungHyun Kim",
      "Honguk Woo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad84864002a72c344c2227d7eb8842b1-Abstract-Conference.html": {
    "title": "Improvements on Uncertainty Quantification for Node Classification via Distance Based Regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Russell Hart",
      "Linlin Yu",
      "Yifei Lou",
      "Feng Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ada418ae9b6677dcda32d9dca0f7441f-Abstract-Conference.html": {
    "title": "Efficient Learning of Linear Graph Neural Networks via Node Subsampling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seiyun Shin",
      "Ilan Shomorony",
      "Han Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ada8de994b46571bdcd7eeff2d3f9cff-Abstract-Conference.html": {
    "title": "DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Zheng",
      "Cheng Lu",
      "Jianfei Chen",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/adb77ecc8ba1c2d3135c86a46b8f2496-Abstract-Conference.html": {
    "title": "Active Bipartite Ranking",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Cheshire",
      "Vincent Laurent",
      "Stephan Clémençon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/adc98a266f45005c403b8311ca7e8bd7-Abstract-Conference.html": {
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rylan Schaeffer",
      "Brando Miranda",
      "Sanmi Koyejo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ade04fd4f26263f86b47ffb535c4cafb-Abstract-Conference.html": {
    "title": "Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gen Li",
      "Wenhao Zhan",
      "Jason D. Lee",
      "Yuejie Chi",
      "Yuxin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/adf5a38a2e2e7606fbfc3eff72998afa-Abstract-Conference.html": {
    "title": "The Exact Sample Complexity Gain from Invariances for Kernel Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Behrooz Tahmasebi",
      "Stefanie Jegelka"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ae0cba715b60c4052359b3d52a2cff7f-Abstract-Conference.html": {
    "title": "FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao ZHANG",
      "Tianyuan DAI",
      "Yanbo Xu",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ae2d574d2c309f3a45880e4460efd176-Abstract-Conference.html": {
    "title": "Chanakya: Learning Runtime Decisions for Adaptive Real-Time Perception",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anurag Ghosh",
      "Vaibhav Balloli",
      "Akshay Nambi",
      "Aditya Singh",
      "Tanuja Ganu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ae54ce310476218f26dd48c1626d5187-Abstract-Conference.html": {
    "title": "RoboCLIP: One Demonstration is Enough to Learn Robot Policies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumedh Sontakke",
      "Jesse Zhang",
      "Séb Arnold",
      "Karl Pertsch",
      "Erdem Bıyık",
      "Dorsa Sadigh",
      "Chelsea Finn",
      "Laurent Itti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ae7d9c77b5ff9e3b7833a68523b880f2-Abstract-Conference.html": {
    "title": "Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihe Wang",
      "Yu Han",
      "Haishuai Wang",
      "Xiang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ae8b0b5838ba510daff1198474e7b984-Abstract-Conference.html": {
    "title": "Importance-aware Co-teaching for Offline Model-based Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Yuan",
      "Can (Sam) Chen",
      "Zixuan Liu",
      "Willie Neiswanger",
      "Xue (Steve) Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ae9bbdcea94d808882f3535e8ca00542-Abstract-Conference.html": {
    "title": "GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxuan Ju",
      "Tong Zhao",
      "Wenhao Yu",
      "Neil Shah",
      "Yanfang Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aea831d6c7af37fd4230937225be3414-Abstract-Conference.html": {
    "title": "Optimal privacy guarantees for a relaxed threat model: Addressing sub-optimal adversaries in differentially private machine learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Kaissis",
      "Alexander Ziller",
      "Stefan Kolek",
      "Anneliese Riess",
      "Daniel Rueckert"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aebbbfa9680eafefd43a0edc85c101f9-Abstract-Conference.html": {
    "title": "Stochastic Approximation Algorithms for Systems of Interacting Particles",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Reza Karimi Jaghargh",
      "Ya-Ping Hsieh",
      "Andreas Krause"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aebec8058f23a445353c83ede0e1ec48-Abstract-Conference.html": {
    "title": "Provable Guarantees for Neural Networks via Gradient Feature Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenmei Shi",
      "Junyi Wei",
      "Yingyu Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aebf6284fe85a8f44b4785d41bc8249a-Abstract-Conference.html": {
    "title": "Binary Radiance Fields",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjoo Shin",
      "Jaesik Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aec5e2847c5ae90f939ab786774856cc-Abstract-Conference.html": {
    "title": "A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alicia Curth",
      "Alan Jeffares",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aee1de5f335558b546b7e58c380be087-Abstract-Conference.html": {
    "title": "FABind: Fast and Accurate Protein-Ligand Binding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhi Pei",
      "Kaiyuan Gao",
      "Lijun Wu",
      "Jinhua Zhu",
      "Yingce Xia",
      "Shufang Xie",
      "Tao Qin",
      "Kun He",
      "Tie-Yan Liu",
      "Rui Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aee2f03ecb2b2c1ea55a43946b651cfd-Abstract-Conference.html": {
    "title": "Geometric Transformer with Interatomic Positional Encoding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusong Wang",
      "Shaoning Li",
      "Tong Wang",
      "Bin Shao",
      "Nanning Zheng",
      "Tie-Yan Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aeeddfbab4e99763ebac9221732c80dd-Abstract-Conference.html": {
    "title": "A Diffusion-Model of Joint Interactive Navigation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Niedoba",
      "Jonathan Lavington",
      "Yunpeng Liu",
      "Vasileios Lioutas",
      "Justice Sefas",
      "Xiaoxuan Liang",
      "Dylan Green",
      "Setareh Dabiri",
      "Berend Zwartsenberg",
      "Adam Scibior",
      "Frank Wood"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aef34c770664d06eabdfebc5d3d58a9c-Abstract-Conference.html": {
    "title": "Diversifying Spatial-Temporal Perception for Video Domain Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun-Yu Lin",
      "Jia-Run Du",
      "Yipeng Gao",
      "Jiaming Zhou",
      "Wei-Shi Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aef75887979ae1287b5deb54a1e3cbda-Abstract-Conference.html": {
    "title": "Conformal Prediction for Time Series with Modern Hopfield Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Auer",
      "Martin Gauch",
      "Daniel Klotz",
      "Sepp Hochreiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/af01716e08073368a7c8a62be46dba17-Abstract-Conference.html": {
    "title": "Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyi Duan",
      "Yan Xia",
      "Zhou Mingze",
      "Li Tang",
      "Jieming Zhu",
      "Zhou Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/af076c3bdbf935b81d808e37c5ede463-Abstract-Conference.html": {
    "title": "Causes and Effects of Unanticipated Numerical Deviations in Neural Network Inference Frameworks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Schlögl",
      "Nora Hofer",
      "Rainer Böhme"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/af2bb2b2280d36f8842e440b4e275152-Abstract-Conference.html": {
    "title": "Learning via Wasserstein-Based High Probability Generalisation Bounds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Viallard",
      "Maxime Haddouche",
      "Umut Simsekli",
      "Benjamin Guedj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/af2d9fb5bcee19ef2dfa70d843520c97-Abstract-Conference.html": {
    "title": "Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Metod Jazbec",
      "James Allingham",
      "Dan Zhang",
      "Eric Nalisnick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/af31604708f3e44b4de9fdfa6dcaa9d1-Abstract-Conference.html": {
    "title": "A Scalable Neural Network for DSIC Affine Maximizer Auction Design",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Duan",
      "Haoran Sun",
      "Yurong Chen",
      "Xiaotie Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/af38fb8e90d586f209235c94119ba193-Abstract-Conference.html": {
    "title": "Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Wan",
      "Che Liu",
      "Mi Zhang",
      "Jie Fu",
      "Benyou Wang",
      "Sibo Cheng",
      "Lei Ma",
      "César Quilodrán-Casas",
      "Rossella Arcucci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/af9ac087ed9123957bb3a45dca56b9d4-Abstract-Conference.html": {
    "title": "CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "A. Feder Cooper",
      "Wentao Guo",
      "Duc Khiem Pham",
      "Tiancheng Yuan",
      "Charlie Ruan",
      "Yucheng Lu",
      "Christopher M. De Sa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/afc9f18089928eca34c347fee4757f72-Abstract-Conference.html": {
    "title": "Individualized Dosing Dynamics via Neural Eigen Decomposition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stav Belogolovsky",
      "Ido Greenberg",
      "Danny Eytan",
      "Shie Mannor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/afcac2e300bc243d15c25cd4f4040f0d-Abstract-Conference.html": {
    "title": "Unified Embedding: Battle-Tested Feature Representations for Web-Scale ML Systems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Coleman",
      "Wang-Cheng Kang",
      "Matthew Fahrbach",
      "Ruoxi Wang",
      "Lichan Hong",
      "Ed Chi",
      "Derek Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/afda6bf3fb086eabbaf161ba1cec5a9a-Abstract-Conference.html": {
    "title": "Counterfactual Generation with Identifiability Guarantees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanqi Yan",
      "Lingjing Kong",
      "Lin Gui",
      "Yuejie Chi",
      "Eric Xing",
      "Yulan He",
      "Kun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/afe99e55be23b3523818da1fefa33494-Abstract-Conference.html": {
    "title": "A Batch-to-Online Transformation under Random-Order Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Dong",
      "Yuichi Yoshida"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b00ef390dcd5f147fd7c5c2bb35f09be-Abstract-Conference.html": {
    "title": "The CLIP Model is Secretly an Image-to-Prompt Converter",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Ding",
      "Chunna Tian",
      "Haoxuan Ding",
      "Lingqiao Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b010241b9f1cdfc7d4c392db899cef86-Abstract-Conference.html": {
    "title": "Invariant Anomaly Detection under Distribution Shifts: A Causal Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "João Carvalho",
      "Mengtao Zhang",
      "Robin Geyer",
      "Carlos Cotrini",
      "Joachim M Buhmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b016cbec36ff7118db303229c9048733-Abstract-Conference.html": {
    "title": "Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Indradyumna Roy",
      "Rishi Agarwal",
      "Soumen Chakrabarti",
      "Anirban Dasgupta",
      "Abir De"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b0313c2f4501a81d0e0d4a1e8fbf4995-Abstract-Conference.html": {
    "title": "L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Linhart",
      "Alexandre Gramfort",
      "Pedro Rodrigues"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b048dd19ba6d85b9066aa93b4de9ad4a-Abstract-Conference.html": {
    "title": "Self-Supervised Reinforcement Learning that Transfers using Random Features",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyuan Chen",
      "Chuning Zhu",
      "Pulkit Agrawal",
      "Kaiqing Zhang",
      "Abhishek Gupta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b0506debbf49e31d25690fbd1e69cd2f-Abstract-Conference.html": {
    "title": "MGDD: A Meta Generator for Fast Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhua Liu",
      "Xinchao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b07091c16719ad3990e3d1ccee6641f1-Abstract-Conference.html": {
    "title": "New Complexity-Theoretic Frontiers of Tractability for Neural Network Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cornelius Brand",
      "Robert Ganian",
      "Mathis Rocton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b07d36fb3fae0630897700593c8cf49d-Abstract-Conference.html": {
    "title": "V-InFoR: A Robust Graph Neural Networks Explainer for Structurally Corrupted Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senzhang Wang",
      "Jun Yin",
      "Chaozhuo Li",
      "Xing Xie",
      "Jianxin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b0a34e3c64f7e842f20ec10479c32b35-Abstract-Conference.html": {
    "title": "Beyond Average Return in Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Marthe",
      "Aurélien Garivier",
      "Claire Vernade"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b0ca717599b7ba84d5e4f4c8b1ef6657-Abstract-Conference.html": {
    "title": "Latent exploration for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alberto Silvio Chiappa",
      "Alessandro Marin Vargas",
      "Ann Huang",
      "Alexander Mathis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b0cd0e8027309ea050951e758b70d60e-Abstract-Conference.html": {
    "title": "Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyler Kastner",
      "Murat A. Erdogdu",
      "Amir-massoud Farahmand"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b0d9ceb3d11d013e55da201d2a2c07b2-Abstract-Conference.html": {
    "title": "Group Robust Classification Without Any Group Information",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christos Tsirigotis",
      "Joao Monteiro",
      "Pau Rodriguez",
      "David Vazquez",
      "Aaron C. Courville"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b11393733b1ea5890100302ab8a0f74c-Abstract-Conference.html": {
    "title": "Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Huang",
      "Han Zhong",
      "Liwei Wang",
      "Lin Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b113e1441ad107b80c576b5028fd2c51-Abstract-Conference.html": {
    "title": "Learning Dictionary for Visual Attention",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingjie Liu",
      "Xuan Liu",
      "Hui Yu",
      "XUAN TANG",
      "Xian Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b146e7c87685fa208bd95ce4b08e330c-Abstract-Conference.html": {
    "title": "A Bayesian Take on Gaussian Process Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrico Giudice",
      "Jack Kuipers",
      "Giusi Moffa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b14cf0a01f7a8b9cd3e365e40f910272-Abstract-Conference.html": {
    "title": "Exploring Question Decomposition for Zero-Shot VQA",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaid Khan",
      "Vijay Kumar B G",
      "Samuel Schulter",
      "Manmohan Chandraker",
      "Yun Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b14d76c7266be21b338527cd25deac45-Abstract-Conference.html": {
    "title": "Sharp Recovery Thresholds of Tensor PCA Spectral Algorithms",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Feldman",
      "David Donoho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b157cfde6794e93b2353b9712bbd45a5-Abstract-Conference.html": {
    "title": "R-divergence for Estimating Model-oriented Distribution Discrepancy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilin Zhao",
      "Longbing Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b16e6de5fbbdcb2df237aa66b302bc17-Abstract-Conference.html": {
    "title": "On-the-Fly Adapting Code Summarization on Trainable Cost-Effective Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Cai",
      "Yun Lin",
      "Chenyan Liu",
      "Jinglian Wu",
      "Yifan Zhang",
      "Yiming Liu",
      "Yeyun Gong",
      "Jin Song Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b1719f44953c2e0754a016ab267fe4e7-Abstract-Conference.html": {
    "title": "Exploring and Interacting with the Set of Good Sparse Generalized Additive Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chudi Zhong",
      "Zhi Chen",
      "Jiachang Liu",
      "Margo Seltzer",
      "Cynthia Rudin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b18e5d6a10ba57d5273871f38189f062-Abstract-Conference.html": {
    "title": "Convergence Analysis of Sequential Federated Learning on Heterogeneous Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yipeng Li",
      "Xinchen Lyu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b1917a4bcfab403c3cdd6c6bbaf9fda0-Abstract-Conference.html": {
    "title": "Disambiguated Attention Embedding for Multi-Instance Partial-Label Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Tang",
      "Weijia Zhang",
      "Min-Ling Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b1bdb0f22c9748203c62f29aa297ac57-Abstract-Conference.html": {
    "title": "Efficient Batched Algorithm for Contextual Linear Bandits with Large Action Space via Soft Elimination",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Osama Hanna",
      "Lin Yang",
      "Christina Fragouli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b1d9c7e7bd265d81aae8d74a7a6bd7f1-Abstract-Conference.html": {
    "title": "Add and Thin: Diffusion for Temporal Point Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Lüdke",
      "Marin Biloš",
      "Oleksandr Shchur",
      "Marten Lienen",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b1eb88348ee19a33c81cf5bc3fb8e9d2-Abstract-Conference.html": {
    "title": "Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taehyun Cho",
      "Seungyub Han",
      "Heesoo Lee",
      "Kyungjae Lee",
      "Jungwoo Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html": {
    "title": "Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinbiao Chen",
      "Jiahai Wang",
      "Zizhen Zhang",
      "Zhiguang Cao",
      "Te Ye",
      "Siyuan Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2169d573d75ff90c7b12dc3a5fc2898-Abstract-Conference.html": {
    "title": "QuantSR: Accurate Low-bit Quantization for Efficient Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotong Qin",
      "Yulun Zhang",
      "Yifu Ding",
      "Yifan liu",
      "Xianglong Liu",
      "Martin Danelljan",
      "Fisher Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b231d91e700c465dfdd6116d091a4194-Abstract-Conference.html": {
    "title": "Streaming Factor Trajectory Learning for Temporal Tensor Decomposition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shikai Fang",
      "Xin Yu",
      "Shibo Li",
      "Zheng Wang",
      "Mike Kirby",
      "Shandian Zhe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2876deb92cbd098219a10da25671577-Abstract-Conference.html": {
    "title": "DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyangguang Zhang",
      "Yan Di",
      "Ruida Zhang",
      "Guangyao Zhai",
      "Fabian Manhardt",
      "Federico Tombari",
      "Xiangyang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b28ae1166e1035c26b89d20f0286c9eb-Abstract-Conference.html": {
    "title": "Effective Targeted Attacks for Adversarial Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minseon Kim",
      "Hyeonjeong Ha",
      "Sooel Son",
      "Sung Ju Hwang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b29500824d22ee9bbd25e4cd97c49b55-Abstract-Conference.html": {
    "title": "Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sokhna Diarra Mbacke",
      "Florence Clerc",
      "Pascal Germain"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b295b3a940706f431076c86b78907757-Abstract-Conference.html": {
    "title": "Multi-Head Adapter Routing for Cross-Task Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Page-Caccia",
      "Edoardo Maria Ponti",
      "Zhan Su",
      "Matheus Pereira",
      "Nicolas Le Roux",
      "Alessandro Sordoni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b29ab822442a1616f9bd390fddf6e425-Abstract-Conference.html": {
    "title": "GenS: Generalizable Neural Surface Reconstruction from Multi-View Images",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Peng",
      "Xiaodong Gu",
      "Luyang Tang",
      "Shihe Shen",
      "Fanqi Yu",
      "Ronggang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b29adb4bf2364acec8fb402ef731bb3b-Abstract-Conference.html": {
    "title": "Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarong Xu",
      "Renhong Huang",
      "XIN JIANG",
      "Yuxuan Cao",
      "Carl Yang",
      "Chunping Wang",
      "YANG YANG"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b29ec434e049fb96f3c4245a405ee976-Abstract-Conference.html": {
    "title": "Brain-like Flexible Visual Inference by Harnessing Feedback Feedforward Alignment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tahereh Toosi",
      "Elias Issa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2a2bd5d5051ff6af52e1ef60aefd255-Abstract-Conference.html": {
    "title": "Latent Diffusion for Language Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Lovelace",
      "Varsha Kishore",
      "Chao Wan",
      "Eliot Shekhtman",
      "Kilian Q. Weinberger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2b3e1d9840eba17ad9bbf073e009afe-Abstract-Conference.html": {
    "title": "The emergence of clusters in self-attention dynamics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Borjan Geshkovski",
      "Cyril Letrouit",
      "Yury Polyanskiy",
      "Philippe Rigollet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2b781badeeb49896c4b324c466ec442-Abstract-Conference.html": {
    "title": "Self-Consistent Velocity Matching of Probability Flows",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingxiao Li",
      "Samuel Hurault",
      "Justin M. Solomon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2c39fe6ce838440faf03a0f780e7a63-Abstract-Conference.html": {
    "title": "Deep Momentum Multi-Marginal Schrödinger Bridge",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianrong Chen",
      "Guan-Horng Liu",
      "Molei Tao",
      "Evangelos Theodorou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2d4051f03a7038a2771dfbbe5c7b54e-Abstract-Conference.html": {
    "title": "Semi-Supervised Contrastive Learning for Deep Regression with Ordinal Rankings from Spectral Seriation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihang Dai",
      "Yao DU",
      "Hanru Bai",
      "Kwang-Ting Cheng",
      "Xiaomeng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2e20d7402c9985eae4ba924c65370a8-Abstract-Conference.html": {
    "title": "Domain Re-Modulation for Few-Shot Generative Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Wu",
      "Ziqiang Li",
      "Chaoyue Wang",
      "Heliang Zheng",
      "Shanshan Zhao",
      "Bin Li",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2e63e36c57e153b9015fece2352a9f9-Abstract-Conference.html": {
    "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Bai",
      "Fan Chen",
      "Huan Wang",
      "Caiming Xiong",
      "Song Mei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2fbf1c9bc92e7ef2f6cab2e8a3e09af-Abstract-Conference.html": {
    "title": "Arbitrarily Scalable Environment Generators via Neural Cellular Automata",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulun Zhang",
      "Matthew Fontaine",
      "Varun Bhatt",
      "Stefanos Nikolaidis",
      "Jiaoyang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b2fe1ee8d936ac08dd26f2ff58986c8f-Abstract-Conference.html": {
    "title": "FAMO: Fast Adaptive Multitask Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Liu",
      "Yihao Feng",
      "Peter Stone",
      "Qiang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b316495425d076b4abffc065a64c2cca-Abstract-Conference.html": {
    "title": "A Theory of Multimodal Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b33ad9d46ab2a23b6783d954121d26e3-Abstract-Conference.html": {
    "title": "IDEA: An Invariant Perspective for Efficient Domain Adaptive Image Retrieval",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haixin Wang",
      "Hao Wu",
      "Jinan Sun",
      "Shikun Zhang",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Xiao Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3411e30afa6caeefa4d6d39a5ea84cd-Abstract-Conference.html": {
    "title": "Learning Provably Robust Estimators for Inverse Problems via Jittering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anselm Krainovic",
      "Mahdi Soltanolkotabi",
      "Reinhard Heckel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b36554b97da741b1c48c9de05c73993e-Abstract-Conference.html": {
    "title": "Black-box Backdoor Defense via Zero-shot Image Purification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Shi",
      "Mengnan Du",
      "Xuansheng Wu",
      "Zihan Guan",
      "Jin Sun",
      "Ninghao Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3748cdac932d91f0a51a37db90dec50-Abstract-Conference.html": {
    "title": "Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arvind Mahankali",
      "Haochen Zhang",
      "Kefan Dong",
      "Margalit Glasgow",
      "Tengyu Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b37c2e26b75ee02fcabd65a2a0367136-Abstract-Conference.html": {
    "title": "Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhejun Zhang",
      "Alexander Liniger",
      "Christos Sakaridis",
      "Fisher Yu",
      "Luc V Gool"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3847cda0c8cc0cfcdacf462dc122214-Abstract-Conference.html": {
    "title": "Customizable Image Synthesis with Multiple Subjects",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Liu",
      "Yifei Zhang",
      "Yujun Shen",
      "Kecheng Zheng",
      "Kai Zhu",
      "Ruili Feng",
      "Yu Liu",
      "Deli Zhao",
      "Jingren Zhou",
      "Yang Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b39cef2ef90591cffdc9c674cd55bebe-Abstract-Conference.html": {
    "title": "How do Minimum-Norm Shallow Denoisers Look in Function Space?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zeno",
      "Greg Ongie",
      "Yaniv Blumenfeld",
      "Nir Weinberger",
      "Daniel Soudry"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3a08d179347e33414badadf100e4e8d-Abstract-Conference.html": {
    "title": "Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction Network for Tone Mapping",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Zhang",
      "Ming Tian",
      "Zhiqiang Li",
      "Bin Xu",
      "Qingbo Lu",
      "Changxin Gao",
      "Nong Sang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3bac97f3227c52c0179a6d967480867-Abstract-Conference.html": {
    "title": "Masked Image Residual Learning for Scaling Deeper Vision Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxi Huang",
      "Hongtao Fu",
      "Adrian G. Bors"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3bec3f5ad96055b7f60c93edc3606c8-Abstract-Conference.html": {
    "title": "Revisiting Area Convexity: Faster Box-Simplex Games and Spectrahedral Generalizations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arun Jambulapati",
      "Kevin Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3cd64ddad0a28da0f28a0e03a73ea7d-Abstract-Conference.html": {
    "title": "Adversarial Learning for Feature Shift Detection and Correction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Míriam Barrabés",
      "Daniel Mas Montserrat",
      "Margarita Geleta",
      "Xavier Giró-i-Nieto",
      "Alexander Ioannidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3e866c228f8f4ea18021ae63aea5453-Abstract-Conference.html": {
    "title": "General Munchausen Reinforcement Learning with Tsallis Kullback-Leibler Divergence",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingwei Zhu",
      "Zheng Chen",
      "Matthew Schlegel",
      "Martha White"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3f48945f6fb402b4b5cdcf490e72847-Abstract-Conference.html": {
    "title": "Residual Alignment: Uncovering the Mechanisms of Residual Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Li",
      "Vardan Papyan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b40d5797756800c97f3d525c2e4c8357-Abstract-Conference.html": {
    "title": "Globally injective and bijective neural operators",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takashi Furuya",
      "Michael Puthawala",
      "Matti Lassas",
      "Maarten V. de Hoop"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b418964bafb4fdd9aef9017301323a8a-Abstract-Conference.html": {
    "title": "On the Convergence of CART under Sufficient Impurity Decrease Condition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Mazumder",
      "Haoyue Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b41907dd4df5c60f86216b73fe0c7465-Abstract-Conference.html": {
    "title": "PERFOGRAPH: A Numerical Aware Program Graph Representation for Performance Optimization and Program Analysis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali TehraniJamsaz",
      "Quazi Ishtiaque Mahmud",
      "Le Chen",
      "Nesreen K. Ahmed",
      "Ali Jannesari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b444ad72520a5f5c467343be88e352ed-Abstract-Conference.html": {
    "title": "Penalising the biases in norm regularisation enforces sparsity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Etienne Boursier",
      "Nicolas Flammarion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b456a00e145ad56f6f251f79f8c8a7de-Abstract-Conference.html": {
    "title": "Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seunghwan An",
      "Jong-June Jeon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b46bc1449205888e1883f692aff1a252-Abstract-Conference.html": {
    "title": "Optimistic Meta-Gradients",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Flennerhag",
      "Tom Zahavy",
      "Brendan O'Donoghue",
      "Hado P. van Hasselt",
      "András György",
      "Satinder Singh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b49213694c3e752252d62ca360b72a36-Abstract-Conference.html": {
    "title": "Norm-guided latent space exploration for text-to-image generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dvir Samuel",
      "Rami Ben-Ari",
      "Nir Darshan",
      "Haggai Maron",
      "Gal Chechik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b4aadf04d6fde46346db455402860708-Abstract-Conference.html": {
    "title": "Scale Alone Does not Improve Mechanistic Interpretability in Vision Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roland S. Zimmermann",
      "Thomas Klein",
      "Wieland Brendel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b4c898eb1fb556b8d871fbe9ead92256-Abstract-Conference.html": {
    "title": "MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junho Song",
      "Keonwoo Kim",
      "Jeonglyul Oh",
      "Sungzoon Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b4dde7f1bc45bf9c0fda8db8f272b758-Abstract-Conference.html": {
    "title": "Minimax Risks and Optimal Procedures for Estimation under Functional Local Differential Privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bonwoo Lee",
      "Jeongyoun Ahn",
      "Cheolwoo Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b4e3fea367538ea6b1b5ba6ebf5c39a8-Abstract-Conference.html": {
    "title": "Switching Autoregressive Low-rank Tensor Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyun Dong Lee",
      "Andrew Warrington",
      "Joshua Glaser",
      "Scott Linderman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b52e8c6c1a798fed53ac2e6a5e23ddc8-Abstract-Conference.html": {
    "title": "From Tempered to Benign Overfitting in ReLU Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Kornowski",
      "Gilad Yehudai",
      "Ohad Shamir"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b54d1757c190ba20dbc4f9e4a2f54149-Abstract-Conference.html": {
    "title": "Tree-Rings Watermarks: Invisible Fingerprints for Diffusion Images",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Wen",
      "John Kirchenbauer",
      "Jonas Geiping",
      "Tom Goldstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b589d92785e39486e978fa273d0dc343-Abstract-Conference.html": {
    "title": "Understanding and Improving Ensemble Adversarial Defense",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yian Deng",
      "Tingting Mu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b5a801e6bc4f4ffa3e6786518a324488-Abstract-Conference.html": {
    "title": "Adversarial Training for Graph Neural Networks: Pitfalls, Solutions, and New Directions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Gosch",
      "Simon Geisler",
      "Daniel Sturm",
      "Bertrand Charpentier",
      "Daniel Zügner",
      "Stephan Günnemann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b5afe13494c825089b1e3944fdaba212-Abstract-Conference.html": {
    "title": "Joint Prompt Optimization of Stacked LLMs using Variational Inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Sordoni",
      "Eric Yuan",
      "Marc-Alexandre Côté",
      "Matheus Pereira",
      "Adam Trischler",
      "Ziang Xiao",
      "Arian Hosseini",
      "Friederike Niedtner",
      "Nicolas Le Roux"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b5b4d92374323c53c24bbbc8ee0e715c-Abstract-Conference.html": {
    "title": "On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufeng Zhang",
      "Jialu Pan",
      "Li Ken Li",
      "Wanwei Liu",
      "Zhenbang Chen",
      "Xinwang Liu",
      "J Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b5b528767aa35f5b1a60fe0aaeca0563-Abstract-Conference.html": {
    "title": "Implicit Bias of (Stochastic) Gradient Descent for Rank-1 Linear Neural Network",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bochen Lyu",
      "Zhanxing Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b5c8c1c117618267944b2617add0a766-Abstract-Conference.html": {
    "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Sun",
      "Yuchen Zhuang",
      "Lingkai Kong",
      "Bo Dai",
      "Chao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b60161e93f3e0e4207081a3b4ef5e8d8-Abstract-Conference.html": {
    "title": "Fairness Aware Counterfactuals for Subgroups",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Loukas Kavouras",
      "Konstantinos Tsopelas",
      "Giorgos Giannopoulos",
      "Dimitris Sacharidis",
      "Eleni Psaroudaki",
      "Nikolaos Theologitis",
      "Dimitrios Rontogiannis",
      "Dimitris Fotakis",
      "Ioannis Emiris"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b61da4f02b271cb7b5e3d538e2b78fb9-Abstract-Conference.html": {
    "title": "Lovász Principle for Unsupervised Graph Representation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Sun",
      "Chris Ding",
      "Jicong Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6262f7a34e5d641cdb3d33dc9ad1a5a-Abstract-Conference.html": {
    "title": "ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyang Le",
      "Yao Qian",
      "Long Zhou",
      "Shujie LIU",
      "Yanmin Qian",
      "Michael Zeng",
      "Xuedong Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b63ad8c24354b0e5bcb7aea16490beab-Abstract-Conference.html": {
    "title": "Reverse Engineering Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ido Ben-Shaul",
      "Ravid Shwartz-Ziv",
      "Tomer Galanti",
      "Shai Dekel",
      "Yann LeCun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6404bf461c3c3186bdf5f55756af908-Abstract-Conference.html": {
    "title": "DinoSR: Self-Distillation and Online Clustering for Self-supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander H. Liu",
      "Heng-Jui Chang",
      "Michael Auli",
      "Wei-Ning Hsu",
      "Jim Glass"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6446566965fa38e183650728ab70318-Abstract-Conference.html": {
    "title": "4M: Massively Multimodal Masked Modeling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Mizrahi",
      "Roman Bachmann",
      "Oguzhan Kar",
      "Teresa Yeo",
      "Mingfei Gao",
      "Afshin Dehghan",
      "Amir Zamir"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b654d6150630a5ba5df7a55621390daf-Abstract-Conference.html": {
    "title": "Non-Rigid Shape Registration via Deep Functional Maps Prior",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puhua Jiang",
      "Mingze Sun",
      "Ruqi Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b663eb1512ce6c268e3e56f34c6d2959-Abstract-Conference.html": {
    "title": "Game Solving with Online Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ti-Rong Wu",
      "Hung Guei",
      "Ting Han Wei",
      "Chung-Chin Shih",
      "Jui-Te Chin",
      "I-Chen Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b693a240cf1009bff9fa4422141c9392-Abstract-Conference.html": {
    "title": "Beyond probability partitions: Calibrating neural networks with semantic aware grouping",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Qi Yang",
      "De-Chuan Zhan",
      "Le Gan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6a171867138c80de2a35a6125d6757c-Abstract-Conference.html": {
    "title": "Identifiable Contrastive Learning with Automatic Feature Importance Discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Yifei Wang",
      "Yisen  Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6c05f8254a00709e16fb0fdaae56cd8-Abstract-Conference.html": {
    "title": "Towards Consistent Video Editing with Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Zhang",
      "Bonan Li",
      "Xuecheng Nie",
      "Congying Han",
      "Tiande Guo",
      "Luoqi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6cd2650926d332c86a84c48529cc421-Abstract-Conference.html": {
    "title": "Federated Spectral Clustering via Secure Similarity Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Qiao",
      "Chris Ding",
      "Jicong Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6d67c380f8bde2adc4247d0036c0c73-Abstract-Conference.html": {
    "title": "CS-Isolate: Extracting Hard Confident Examples by Content and Style Isolation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexiong Lin",
      "Yu Yao",
      "Xiaolong Shi",
      "Mingming Gong",
      "Xu Shen",
      "Dong Xu",
      "Tongliang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6f2b16abf590e80c9df30bb5f8e2b7d-Abstract-Conference.html": {
    "title": "Conditional independence testing under misspecified inductive biases",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe Maia Polo",
      "Yuekai Sun",
      "Moulinath Banerjee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6fa3ed9624c184bd73e435123bd576a-Abstract-Conference.html": {
    "title": "Blurred-Dilated Method for Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Deng",
      "Weibin Wu",
      "Jianping Zhang",
      "Zibin Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b7216f4a324864e1f592c18de4d83d10-Abstract-Conference.html": {
    "title": "Towards Distribution-Agnostic Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhong Bai",
      "Zuozhu Liu",
      "Hualiang Wang",
      "Ruizhe Chen",
      "Lianrui Mu",
      "Xiaomeng Li",
      "Joey Tianyi Zhou",
      "YANG FENG",
      "Jian Wu",
      "Haoji Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b733cdd80ed2ae7e3156d8c33108c5d5-Abstract-Conference.html": {
    "title": "Stable Diffusion is Unstable",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengbin Du",
      "Yanxi Li",
      "Zhongwei Qiu",
      "Chang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b7385cb3fa76a0aeedb23d4163640db0-Abstract-Conference.html": {
    "title": "A Competitive Algorithm for Agnostic Active Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Zhou",
      "Eric Price"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b7500454af92cf3934eb1cc2d59abbdf-Abstract-Conference.html": {
    "title": "Efficient Hyper-parameter Optimization with Cubic Regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenqian Shen",
      "Hansi Yang",
      "Yong Li",
      "James Kwok",
      "Quanming Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b762632135b16f1225672f9fe2a9740b-Abstract-Conference.html": {
    "title": "Joint Attribute and Model Generalization Learning for Privacy-Preserving Action Recognition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Peng",
      "Li Xu",
      "Qiuhong Ke",
      "Ping Hu",
      "Jun Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b783c44ba9adbc30344473dc633b4869-Abstract-Conference.html": {
    "title": "3D-Aware Visual Question Answering about Parts, Poses and Occlusions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingrui Wang",
      "Wufei Ma",
      "Zhuowan Li",
      "Adam Kortylewski",
      "Alan L. Yuille"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b7870bd43b2d133a1ed95582ae5d82a4-Abstract-Conference.html": {
    "title": "MixFormerV2: Efficient Fully Transformer Tracking",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutao Cui",
      "Tianhui Song",
      "Gangshan Wu",
      "Limin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b7aa34d2d24f9bab3056993b7bfa0f1b-Abstract-Conference.html": {
    "title": "Generalized Information-theoretic Multi-view Clustering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitian Huang",
      "Sirui Yang",
      "Hongmin Cai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b7d795e655c1463d7299688d489e8ef4-Abstract-Conference.html": {
    "title": "Counterfactual Evaluation of Peer-Review Assignment Policies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Saveski",
      "Steven Jecmen",
      "Nihar Shah",
      "Johan Ugander"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b7d9b1d4a9464d5d1ece82198e351349-Abstract-Conference.html": {
    "title": "Temporal Causal Mediation through a Point Process: Direct and Indirect Effects of Healthcare Interventions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Çağlar Hızlı",
      "ST John",
      "Anne Juuti",
      "Tuure Saarinen",
      "Kirsi Pietiläinen",
      "Pekka Marttinen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b7ed46bd87cd51d4c031b96d9b1a8eb6-Abstract-Conference.html": {
    "title": "Randomized and Deterministic Maximin-share Approximations for Fractionally Subadditive Valuations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hannaneh Akrami",
      "Kurt Mehlhorn",
      "Masoud Seddighin",
      "Golnoosh Shahkarami"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b8402301e7f06bdc97a31bfaa653dc32-Abstract-Conference.html": {
    "title": "Causal normalizing flows: from theory to practice",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrián Javaloy",
      "Pablo Sanchez-Martin",
      "Isabel Valera"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b84adff45775e92a45f0cd87c37f5ce9-Abstract-Conference.html": {
    "title": "Maximum Average Randomly Sampled: A Scale Free and Non-parametric Algorithm for Stochastic Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masoud Moravej Khorasani",
      "Erik Weyer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b860c0c546f4a3a786f9c9468228c99f-Abstract-Conference.html": {
    "title": "Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Tan",
      "Yun Zhu",
      "Lijuan Liu",
      "Eric Xing",
      "Zhiting Hu",
      "Jindong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b8734840bf65c8facd619f5105c6acd0-Abstract-Conference.html": {
    "title": "Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Xu",
      "Yuyuan Gao",
      "Jiangrong Shen",
      "Yaxin Li",
      "Xuming Ran",
      "Huajin Tang",
      "Gang Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b87738474533cab76c7bee4e08443aca-Abstract-Conference.html": {
    "title": "Reducing Shape-Radiance Ambiguity in Radiance Fields with a Closed-Form Color Estimation Method",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Fang",
      "Yafei Song",
      "Keqiang Li",
      "Liefeng Bo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b87bdcf963cad3d0b265fcb78ae7d11e-Abstract-Conference.html": {
    "title": "Text-to-Image Diffusion Models are Zero Shot Classifiers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Clark",
      "Priyank Jaini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b87d9d19ecb5927f7e18c537908610ef-Abstract-Conference.html": {
    "title": "MADG: Margin-based Adversarial Learning for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aveen Dayal",
      "Vimal K B",
      "Linga Reddy Cenkeramaddi",
      "C Mohan",
      "Abhinav Kumar",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b8be628bf719550b560de8bec9456e0b-Abstract-Conference.html": {
    "title": "Bridging RL Theory and Practice with the Effective Horizon",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cassidy Laidlaw",
      "Stuart J Russell",
      "Anca Dragan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b8c90b65739ae8417e61eadb521f63d5-Abstract-Conference.html": {
    "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqiu Wu",
      "Yushi Hu",
      "Weijia Shi",
      "Nouha Dziri",
      "Alane Suhr",
      "Prithviraj Ammanabrolu",
      "Noah A. Smith",
      "Mari Ostendorf",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b8e2046160a568145af6d42eeef199f4-Abstract-Conference.html": {
    "title": "Towards Optimal Effective Resistance Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajat Vadiraj Dwaraknath",
      "Ishani Karmarkar",
      "Aaron Sidford"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b914a8fcea5c176cf1ed75c762ce27fd-Abstract-Conference.html": {
    "title": "Towards Optimal Caching and Model Selection for Large Model Inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Banghua Zhu",
      "Ying Sheng",
      "Lianmin Zheng",
      "Clark Barrett",
      "Michael Jordan",
      "Jiantao Jiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b91cc0a242e6518ee731f74e82b2eebd-Abstract-Conference.html": {
    "title": "Deep Non-line-of-sight Imaging from Under-scanning Measurements",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Li",
      "Yueyi Zhang",
      "Juntian Ye",
      "Feihu Xu",
      "Zhiwei Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b93fda2862db7a7ac4a5c412adfb1ac2-Abstract-Conference.html": {
    "title": "Learning Adversarial Low-rank Markov Decision Processes with Unknown Transition and Full-information Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Canzhe Zhao",
      "Ruofeng Yang",
      "Baoxiang Wang",
      "Xuezhou Zhang",
      "Shuai Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b94d8b035e2183e47afef9e2f299ba47-Abstract-Conference.html": {
    "title": "UE4-NeRF:Neural Radiance Field for Real-Time Rendering of Large-Scale Scene",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Gu",
      "Minchao Jiang",
      "Hongsheng Li",
      "Xiaoyuan Lu",
      "Guangming Zhu",
      "Syed Afaq Ali Shah",
      "Liang Zhang",
      "Mohammed Bennamoun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9603de9e49d0838e53b6c9cf9d06556-Abstract-Conference.html": {
    "title": "H2RBox-v2: Incorporating Symmetry for Boosting Horizontal Box Supervised Oriented Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yu",
      "Xue Yang",
      "Qingyun Li",
      "Yue Zhou",
      "Feipeng Da",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b973a107336177a274069cefb011244c-Abstract-Conference.html": {
    "title": "Online RL in Linearly $q^\\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gellert Weisz",
      "András György",
      "Csaba Szepesvari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9801626a6ffaf6664af1e983dbd0094-Abstract-Conference.html": {
    "title": "On Transfer of Adversarial Robustness from Pretraining to Downstream Tasks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura F. Nern",
      "Harsh Raj",
      "Maurice André Georgi",
      "Yash Sharma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9a17133e3943509243b5e197c1c23b2-Abstract-Conference.html": {
    "title": "Entropy-dissipation Informed Neural Network for McKean-Vlasov Type PDEs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zebang Shen",
      "Zhenfu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9a4d7b88a41652c63962ebcc21701b7-Abstract-Conference.html": {
    "title": "Revisiting Visual Model Robustness: A Frequency Long-Tailed Distribution View",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Lin",
      "Yifei Gao",
      "Yunfan Yang",
      "Jitao Sang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9b4f084b2e6709a2bfad0f601271aec-Abstract-Conference.html": {
    "title": "How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hai Zhang",
      "Hang Yu",
      "Junqiao Zhao",
      "Di Zhang",
      "xiao zhang",
      "Hongtu Zhou",
      "Chang Huang",
      "Chen Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9c2e8a0bbed5fcfaf62856a3a719ada-Abstract-Conference.html": {
    "title": "Dynamic Pricing and Learning with Bayesian Persuasion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shipra Agrawal",
      "Yiding Feng",
      "Wei Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9c353d02e565f0f7cba94c4f3584eaa-Abstract-Conference.html": {
    "title": "RH-BrainFS: Regional Heterogeneous Multimodal Brain Networks Fusion Strategy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongting Ye",
      "Yalu Zheng",
      "Yueying Li",
      "Ke Zhang",
      "Youyong Kong",
      "Yonggui Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9e472cd579c83e2f6aa3459f46aac28-Abstract-Conference.html": {
    "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuzhao Xue",
      "Yao Fu",
      "Wangchunshu Zhou",
      "Zangwei Zheng",
      "Yang You"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9e98316cb72fee82cc1160da5810abc-Abstract-Conference.html": {
    "title": "A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaocheng Zhu",
      "Xinyu Yuan",
      "Michael Galkin",
      "Louis-Pascal Xhonneux",
      "Ming Zhang",
      "Maxime Gazeau",
      "Jian Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9ecf4d84999a61783c360c3782e801e-Abstract-Conference.html": {
    "title": "HASSOD: Hierarchical Adaptive Self-Supervised Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengcao Cao",
      "Dhiraj Joshi",
      "Liangyan Gui",
      "Yu-Xiong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9f253c2758a323f9d2095f91de9a974-Abstract-Conference.html": {
    "title": "Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Taylor",
      "Andrew King",
      "Nicol S Harper"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b9fd027eb16434174b8bb3d3b18110af-Abstract-Conference.html": {
    "title": "Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingting Dan",
      "Jiaqi Ding",
      "Ziquan Wei",
      "Shahar Kovalsky",
      "Minjeong Kim",
      "Won Hwa Kim",
      "Guorong Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ba0ad9d1e0c737800b2340b9cd68c208-Abstract-Conference.html": {
    "title": "Adapting Fairness Interventions to Missing Values",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raymond Feng",
      "Flavio Calmon",
      "Hao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ba178fab60f9306a0b2d7ec8973715a6-Abstract-Conference.html": {
    "title": "Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantisation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Subia-Waud",
      "Srinandan Dasmahapatra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ba1c5356d9164bb64c446a4b690226b0-Abstract-Conference.html": {
    "title": "PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Char",
      "Jeff Schneider"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ba8aee784ffe0813890288b334444eda-Abstract-Conference.html": {
    "title": "Policy Gradient for Rectangular Robust Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navdeep Kumar",
      "Esther Derman",
      "Matthieu Geist",
      "Kfir Y. Levy",
      "Shie Mannor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ba8d1b46292c5e82cbfb3b3dc3b968af-Abstract-Conference.html": {
    "title": "MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Bellagente",
      "Manuel Brack",
      "Hannah Teufel",
      "Felix Friedrich",
      "Björn Deiseroth",
      "Constantin Eichenberg",
      "Andrew M. Dai",
      "Robert Baldock",
      "Souradeep Nanda",
      "Koen Oostermeijer",
      "Andres Felipe Cruz-Salinas",
      "Patrick Schramowski",
      "Kristian Kersting",
      "Samuel Weinbach"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ba90e56a74fd77d0ddec033dc199f0fa-Abstract-Conference.html": {
    "title": "What Planning Problems Can A Relational Neural Network Solve?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayuan Mao",
      "Tomás Lozano-Pérez",
      "Josh Tenenbaum",
      "Leslie Kaelbling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/baa7fc022f35b6ea7b8b2a2fe60babe0-Abstract-Conference.html": {
    "title": "Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias Gerstgrasser",
      "Tom Danino",
      "Sarah Keren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bad8ddaed5feb552f9e8f2e37c0531a1-Abstract-Conference.html": {
    "title": "Learning From Biased Soft Labels",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hua Yuan",
      "Yu Shi",
      "Ning Xu",
      "Xu Yang",
      "Xin Geng",
      "Yong Rui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bb203e938836544655996d1bb94a0fd7-Abstract-Conference.html": {
    "title": "Learning from Visual Observation via Offline Pretrained State-to-Go Transformer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Zhou",
      "Ke Li",
      "Jiechuan Jiang",
      "Zongqing Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bb36593e5e438aac5dd07907e757e087-Abstract-Conference.html": {
    "title": "Rotating Features for Object Discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sindy Löwe",
      "Phillip Lippe",
      "Francesco Locatello",
      "Max Welling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bb3cfcb0284642a973dd631ec9184f2f-Abstract-Conference.html": {
    "title": "Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlong Huang",
      "Fei Xia",
      "Dhruv Shah",
      "Danny Driess",
      "Andy Zeng",
      "Yao Lu",
      "Pete Florence",
      "Igor Mordatch",
      "Sergey Levine",
      "Karol Hausman",
      "brian ichter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bbb7506579431a85861a05fff048d3e1-Abstract-Conference.html": {
    "title": "Focus Your Attention when Few-Shot Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqing Wang",
      "Shibo Jie",
      "Zhihong Deng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bbc461518c59a2a8d64e70e2c38c4a0e-Abstract-Conference.html": {
    "title": "Unifying GANs and Score-Based Diffusion as Generative Particle Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean-Yves Franceschi",
      "Mike Gartrell",
      "Ludovic Dos Santos",
      "Thibaut Issenhuth",
      "Emmanuel de Bézenac",
      "Mickael Chen",
      "Alain Rakotomamonjy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bbf38332580c1bed99fa99bc9ee53229-Abstract-Conference.html": {
    "title": "Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tolga Ergen",
      "Mert Pilanci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bc222e8153a49c1b30a1b8ba96b35117-Abstract-Conference.html": {
    "title": "Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Pagliardini",
      "Daniele Paliotta",
      "Martin Jaggi",
      "François Fleuret"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bc26087d3f82e62044fc77752e86737e-Abstract-Conference.html": {
    "title": "Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guozheng Ma",
      "Linrui Zhang",
      "Haoyu Wang",
      "Lu Li",
      "Zilin Wang",
      "Zhen Wang",
      "Li Shen",
      "Xueqian Wang",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bc6a1f968f8b1dae3e880f3f723d7d46-Abstract-Conference.html": {
    "title": "Improved Communication Efficiency in Federated Natural Policy Gradient via ADMM-based Gradient Updates",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangchen Lan",
      "Han Wang",
      "James Anderson",
      "Christopher Brinton",
      "Vaneet Aggarwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bc827452450356f9f558f4e4568d553b-Abstract-Conference.html": {
    "title": "Equivariant flow matching",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leon Klein",
      "Andreas Krämer",
      "Frank Noe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bc943cd038a5531d5433b1431c822c01-Abstract-Conference.html": {
    "title": "InsActor: Instruction-driven Physics-based Characters",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Ren",
      "Mingyuan Zhang",
      "Cunjun Yu",
      "Xiao Ma",
      "Liang Pan",
      "Ziwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bccdd196d798a51a4961989984a9ed4a-Abstract-Conference.html": {
    "title": "An Efficient Doubly-Robust Test for the Kernel Treatment Effect",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diego Martinez Taboada",
      "Aaditya Ramdas",
      "Edward Kennedy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bcdaaa1aec3ae2aa39542acefdec4e4b-Abstract-Conference.html": {
    "title": "Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Zhang",
      "Andrea Zanette"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bcdcd565f83a8a6681a8269d325a5304-Abstract-Conference.html": {
    "title": "Zero-sum Polymatrix Markov Games: Equilibrium Collapse and Efficient Computation of Nash Equilibria",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fivos Kalogiannis",
      "Ioannis Panageas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bcdec1c2d60f94a93b6e36f937aa0530-Abstract-Conference.html": {
    "title": "Learning to Configure Separators in Branch-and-Cut",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Li",
      "Wenbin Ouyang",
      "Max Paulus",
      "Cathy Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bcef27c5825d1ed8757290f237b2d851-Abstract-Conference.html": {
    "title": "Multi-Object Representation Learning via Feature Connectivity and Object-Centric Regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Foo",
      "Wynne Hsu",
      "Mong Li Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bcf26768143c94bd36e363cd4bf5daf0-Abstract-Conference.html": {
    "title": "Ess-InfoGAIL: Semi-supervised Imitation Learning from Imbalanced Demonstrations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiqiao Fu",
      "Kaiqiang Tang",
      "Yuanyang Lu",
      "Yiming Qi",
      "Guizhou Deng",
      "Flood Sung",
      "Chunlin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bcfcf7232cb74e1ef82d751880ff835b-Abstract-Conference.html": {
    "title": "Revisiting Implicit Differentiation for Learning Problems in Optimal Control",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Xu",
      "Timothy L. Molloy",
      "Stephen Gould"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bd18189308a4c45c7d71ca83acf3deaa-Abstract-Conference.html": {
    "title": "$p$-Poisson surface reconstruction in curl-free flow from point clouds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yesom Park",
      "Taekyung Lee",
      "Jooyoung Hahn",
      "Myungjoo Kang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bd1fc5cbedfe4d90d0ac2d23966fa27e-Abstract-Conference.html": {
    "title": "Binarized Neural Machine Translation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Ankush Garg",
      "Yuan Cao",
      "Lukasz Lew",
      "Behrooz Ghorbani",
      "Zhiru Zhang",
      "Orhan Firat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bd2107343c9cc973635d90dbfc122223-Abstract-Conference.html": {
    "title": "Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nived Rajaraman",
      "Fnu Devvrit",
      "Aryan Mokhtari",
      "Kannan Ramchandran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bd2605c5d854837aaf095537e82f1883-Abstract-Conference.html": {
    "title": "EgoEnv: Human-centric environment representations from egocentric video",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tushar Nagarajan",
      "Santhosh Kumar Ramakrishnan",
      "Ruta Desai",
      "James Hillis",
      "Kristen Grauman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bd5d436621dd3ee728b11c067d32d488-Abstract-Conference.html": {
    "title": "Optimal Unbiased Randomizers for Regression with Label Differential Privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashwinkumar Badanidiyuru Varadaraja",
      "Badih Ghazi",
      "Pritish Kamath",
      "Ravi Kumar",
      "Ethan Leeman",
      "Pasin Manurangsi",
      "Avinash V Varadarajan",
      "Chiyuan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bd6bb13e78da078d8adcabbe6d9ca737-Abstract-Conference.html": {
    "title": "Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yue",
      "Rui Lu",
      "Bingyi Kang",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bd8284e53b6d177cbede82def77d4951-Abstract-Conference.html": {
    "title": "Online List Labeling with Predictions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel McCauley",
      "Ben Moseley",
      "Aidin Niaparast",
      "Shikha Singh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bd9ea5d671ee761a69dba811348d78ba-Abstract-Conference.html": {
    "title": "How a Student becomes a Teacher: learning and forgetting through Spectral methods",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Giambagli",
      "Lorenzo Buffoni",
      "Lorenzo Chicchi",
      "Duccio Fanelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bda5c35eded86adaf0231748e3ce071c-Abstract-Conference.html": {
    "title": "PAPR: Proximity Attention Point Rendering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanshu Zhang",
      "Shichong Peng",
      "Alireza Moazeni",
      "Ke Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bdabb5d4262bcfb6a1d529d690a6c82b-Abstract-Conference.html": {
    "title": "Enhancing Minority Classes by Mixing: An Adaptative Optimal Transport Approach for Long-tailed Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintong Gao",
      "He Zhao",
      "Zhuo Li",
      "Dandan Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bdb0596d13cfccf2db6f0cc5280d2a3f-Abstract-Conference.html": {
    "title": "Robust Data Valuation with Weighted Banzhaf Values",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weida Li",
      "Yaoliang Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bdc48324d6158a7edef88d673855a3f4-Abstract-Conference.html": {
    "title": "Multi-Modal Inverse Constrained Reinforcement Learning from a Mixture of Demonstrations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanren Qiao",
      "Guiliang Liu",
      "Pascal Poupart",
      "Zhiqiang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bdcdf38389d7fcefc73c4c3720217155-Abstract-Conference.html": {
    "title": "FedFed: Feature Distillation against Data Heterogeneity in Federated Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqin Yang",
      "Yonggang Zhang",
      "Yu Zheng",
      "Xinmei Tian",
      "Hao Peng",
      "Tongliang Liu",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bdd5522a32b3a959a6d81fb6ddc1cb38-Abstract-Conference.html": {
    "title": "A Privacy-Friendly Approach to Data Valuation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen (Tianhao) Wang",
      "Yuqing Zhu",
      "Yu-Xiang Wang",
      "Ruoxi Jia",
      "Prateek Mittal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bdeab378efe6eb289714e2a5abc6ed42-Abstract-Conference.html": {
    "title": "Learning Nonparametric Latent Causal Graphs with Unknown Interventions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibo Jiang",
      "Bryon Aragam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bdebb4549d5a79501bc151411abdb6d7-Abstract-Conference.html": {
    "title": "Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Qin",
      "Kwang-Sung Jun",
      "Chicheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/be38c74290c251820e396680a82ce12d-Abstract-Conference.html": {
    "title": "The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artyom Gadetsky",
      "Maria Brbic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/be7430d22a4dae8516894e32f2fcc6db-Abstract-Conference.html": {
    "title": "Improving Compositional Generalization using Iterated Learning and Simplicial Embeddings",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Ren",
      "Samuel Lavoie",
      "Michael Galkin",
      "Danica J. Sutherland",
      "Aaron C. Courville"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/be7b70477c8fca697f14b1dbb1c086d1-Abstract-Conference.html": {
    "title": "Geometric Analysis of Matrix Sensing over Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haixiang Zhang",
      "Ying Chen",
      "Javad Lavaei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/be82bb4bf8333107b0fe430e1017831a-Abstract-Conference.html": {
    "title": "Towards Combinatorial Generalization for Catalysts: A Kohn-Sham Charge-Density Approach",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phillip Pope",
      "David Jacobs"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/be93b16564e96859da8401b917f307c6-Abstract-Conference.html": {
    "title": "Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Yuan",
      "Kaixuan Huang",
      "Chengzhuo Ni",
      "Minshuo Chen",
      "Mengdi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bea78e2bb0abccc14404b24b90d9299f-Abstract-Conference.html": {
    "title": "Unifying Predictions of Deterministic and Stochastic Physics in Mesh-reduced Space with Sequential Flow Generative Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luning Sun",
      "Xu Han",
      "Han Gao",
      "Jian-Xun Wang",
      "Liping Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/beba7cfdac084a0f53f378d42cbe2824-Abstract-Conference.html": {
    "title": "Transitivity Recovering Decompositions: Interpretable and Robust Fine-Grained Relationships",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ABHRA CHAUDHURI",
      "Massimiliano Mancini",
      "Zeynep Akata",
      "Anjan Dutta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/becd02b89259774da2ede23116a80648-Abstract-Conference.html": {
    "title": "Dynamic Regret of Adversarial Linear Mixture MDPs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long-Fei Li",
      "Peng Zhao",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bede8c7d5ed2348494d2b0621d613592-Abstract-Conference.html": {
    "title": "Neural Harmonics: Bridging Spectral Embedding and Matrix Completion in Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marina Munkhoeva",
      "Ivan Oseledets"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bee3d6218d7414f8cadfff0eafd0d7be-Abstract-Conference.html": {
    "title": "Keypoint-Augmented Self-Supervised Learning for Medical Image Segmentation with Limited Annotation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangsihao Yang",
      "Mengwei Ren",
      "Kaize Ding",
      "Guido Gerig",
      "Yalin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bef2af7a1199ec7a134b15ac00bd5377-Abstract-Conference.html": {
    "title": "Aiming towards the minimizers: fast convergence of SGD for overparametrized problems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyue Liu",
      "Dmitriy Drusvyatskiy",
      "Misha Belkin",
      "Damek Davis",
      "Yian Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf0857cb9a41c73639f028a80301cdf0-Abstract-Conference.html": {
    "title": "ResMem: Learn what you can and memorize the rest",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitong Yang",
      "MICHAL LUKASIK",
      "Vaishnavh Nagarajan",
      "Zonglin Li",
      "Ankit Rawat",
      "Manzil Zaheer",
      "Aditya K. Menon",
      "Sanjiv Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf145010b30dc5f14fa87dc152074e4d-Abstract-Conference.html": {
    "title": "Generalized Semi-Supervised Learning via Self-Supervised Feature Adaptation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Liang",
      "RuiBing Hou",
      "Hong Chang",
      "Bingpeng MA",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf215fa7fe70a38c5e967e59c44a99d0-Abstract-Conference.html": {
    "title": "Soft-Unification in Deep Probabilistic Logic",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaron Maene",
      "Luc De Raedt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf2a5ce85aea9ff40d9bf8b2c2561cae-Abstract-Conference.html": {
    "title": "Scaling MLPs: A Tale of Inductive Bias",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gregor Bachmann",
      "Sotiris Anagnostidis",
      "Thomas Hofmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf331c87e29f473b610336f00fe1cb51-Abstract-Conference.html": {
    "title": "Local Convergence of Gradient Methods for Min-Max Games: Partial Curvature Generically Suffices",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Wang",
      "Lénaïc Chizat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf3ee5a5422b0e2a88b0c9c6ed3b6144-Abstract-Conference.html": {
    "title": "Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanpeng Zhou",
      "Yongyi Yang",
      "Xiaojiang Yang",
      "Junchi Yan",
      "Wei Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf5311df07f3efce97471921e6d2f159-Abstract-Conference.html": {
    "title": "Dream the Impossible: Outlier Imagination with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuefeng Du",
      "Yiyou Sun",
      "Jerry Zhu",
      "Yixuan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf64451da212313c5ef1a00f49232c47-Abstract-Conference.html": {
    "title": "RETVec: Resilient and Efficient Text Vectorizer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elie Bursztein",
      "Marina Zhang",
      "Owen Vallis",
      "XINYU JIA",
      "Alexey Kurakin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf665e1cf271faa5037374c884ba3808-Abstract-Conference.html": {
    "title": "An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yudong Luo",
      "Guiliang Liu",
      "Pascal Poupart",
      "Yangchen Pan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf73c283ed3108ee9f84da2e29bcc336-Abstract-Conference.html": {
    "title": "Do Not Marginalize Mechanisms, Rather Consolidate!",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Willig",
      "Matej Zečević",
      "Devendra Dhami",
      "Kristian Kersting"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf78fc727cf882df66e6dbc826161e86-Abstract-Conference.html": {
    "title": "Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihai Wu",
      "Kai Cheng",
      "Yan Zhao",
      "Chuanruo Ning",
      "Guanqi Zhan",
      "Hao Dong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf85879363044ca21f7868a3d1b4021c-Abstract-Conference.html": {
    "title": "Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristina Menghini",
      "Andrew Delworth",
      "Stephen Bach"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bf89c9fcd0ef605571a03666f6a6a44d-Abstract-Conference.html": {
    "title": "Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangqing Fu",
      "Ming Sun",
      "Buqing Nie",
      "Yue Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bfa815ac6f08f4ada34fe22be054f2b9-Abstract-Conference.html": {
    "title": "MeCo: Zero-Shot NAS with One Data and Single Forward Pass via Minimum Eigenvalue of Correlation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tangyu Jiang",
      "Haodi Wang",
      "Rongfang Bie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bfb6a69c0d9e2bc596e1cd31f16fcdde-Abstract-Conference.html": {
    "title": "On the Constrained Time-Series Generation Problem",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Coletta",
      "Sriram Gopalakrishnan",
      "Daniel Borrajo",
      "Svitlana Vyetrenko"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c01c0da4fe2ef2df9863f55261e2e924-Abstract-Conference.html": {
    "title": "InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junda Wu",
      "Tong Yu",
      "Rui Wang",
      "Zhao Song",
      "Ruiyi Zhang",
      "Handong Zhao",
      "Chaochao Lu",
      "Shuai Li",
      "Ricardo Henao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c03a9ccdb3e95f2c2dcfc3f4bc16bf42-Abstract-Conference.html": {
    "title": "On the Size and Approximation Error of Distilled Datasets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alaa Maalouf",
      "Murad Tukan",
      "Noel Loo",
      "Ramin Hasani",
      "Mathias Lechner",
      "Daniela Rus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c041d58d2250e67f70a5b004655315b5-Abstract-Conference.html": {
    "title": "A Unified Approach for Maximizing Continuous DR-submodular Functions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Pedramfar",
      "Christopher Quinn",
      "Vaneet Aggarwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c056d6cf7b7108418f2b8c307dfaab02-Abstract-Conference.html": {
    "title": "On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling and Beyond",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh Nguyen-Tang",
      "Raman Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c057cb81b8d3c67093427bf1c16a4e9f-Abstract-Conference.html": {
    "title": "GRAND-SLAMIN' Interpretable Additive Modeling with Structural Constraints",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibal Ibrahim",
      "Gabriel Afriat",
      "Kayhan Behdin",
      "Rahul Mazumder"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c06f788963f0ce069f5b2dbf83fe7822-Abstract-Conference.html": {
    "title": "S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwoo Mo",
      "Minkyu Kim",
      "Kyungmin Lee",
      "Jinwoo Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c07d71ff0bc042e4b9acd626a79597fa-Abstract-Conference.html": {
    "title": "A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangfan Zhang",
      "Jinyuan Jia",
      "Jinghui Chen",
      "Lu Lin",
      "Dinghao Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c0ae487420ebc8d0ed7c541b4e3f09d4-Abstract-Conference.html": {
    "title": "Towards Understanding the Dynamics of Gaussian-Stein Variational Gradient Descent",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianle Liu",
      "Promit Ghosal",
      "Krishnakumar Balasubramanian",
      "Natesh Pillai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c11f8d40c119867e30e3421f696f931d-Abstract-Conference.html": {
    "title": "Provable benefits of score matching",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chirag Pabbaraju",
      "Dhruv Rohatgi",
      "Anish Prasad Sevekari",
      "Holden Lee",
      "Ankur Moitra",
      "Andrej Risteski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c132c02176577c4319a878f6417a331a-Abstract-Conference.html": {
    "title": "Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yankun Huang",
      "Qihang Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c142c14699223f7417cad706fd6f652e-Abstract-Conference.html": {
    "title": "Performance Scaling via Optimal Transport: Enabling Data Selection from Partially Revealed Sources",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyang Kang",
      "Hoang Anh Just",
      "Anit Kumar Sahu",
      "Ruoxi Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c14d902be45c72833018b2ccfac071e4-Abstract-Conference.html": {
    "title": "The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Schmidt",
      "Philipp Hennig",
      "Jörg Nick",
      "Filip Tronarp"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c194ced51c857ec2c1928b02250e0ac8-Abstract-Conference.html": {
    "title": "Cognitive Model Discovery via Disentangled RNNs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Miller",
      "Maria Eckstein",
      "Matt Botvinick",
      "Zeb Kurth-Nelson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c1aaf7c3f306fe94f77236dc0756d771-Abstract-Conference.html": {
    "title": "Offline Reinforcement Learning with Differential Privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Qiao",
      "Yu-Xiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c1b3d1e2cf53bb28cabd801bd58b3521-Abstract-Conference.html": {
    "title": "Chatting Makes Perfect: Chat-based Image Retrieval",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matan Levy",
      "Rami Ben-Ari",
      "Nir Darshan",
      "Dani Lischinski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c1cdf3236050ad902c6581458e55f0c5-Abstract-Conference.html": {
    "title": "SHOT: Suppressing the Hessian along the Optimization Trajectory for Gradient-Based Meta-Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JunHoo Lee",
      "Jayeon Yoo",
      "Nojun Kwak"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c1e2faff6f588870935f114ebe04a3e5-Abstract-Conference.html": {
    "title": "ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahua Dong",
      "Yu-Xiong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c1f0b856a35986348ab3414177266f75-Abstract-Conference.html": {
    "title": "Are aligned neural networks adversarially aligned?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Carlini",
      "Milad Nasr",
      "Christopher A. Choquette-Choo",
      "Matthew Jagielski",
      "Irena Gao",
      "Pang Wei W. Koh",
      "Daphne Ippolito",
      "Florian Tramer",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c1f7b1ed763e9c75e4db74b49b76db5f-Abstract-Conference.html": {
    "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhai Wang",
      "Zhe Chen",
      "Xiaokang Chen",
      "Jiannan Wu",
      "Xizhou Zhu",
      "Gang Zeng",
      "Ping Luo",
      "Tong Lu",
      "Jie Zhou",
      "Yu Qiao",
      "Jifeng Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c1fdec0d7ea1affa15bd09dd0fd3af05-Abstract-Conference.html": {
    "title": "Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrii Zadaianchuk",
      "Maximilian Seitzer",
      "Georg Martius"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c209cd57e13f3344a4cad4ce84d0ee1b-Abstract-Conference.html": {
    "title": "Regret Matching+: (In)Stability and Fast Convergence in Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriele Farina",
      "Julien Grand-Clément",
      "Christian Kroer",
      "Chung-Wei Lee",
      "Haipeng Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c20ac0df6c213db6d3a930fe9c7296c8-Abstract-Conference.html": {
    "title": "For SALE: State-Action Representation Learning for Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Scott Fujimoto",
      "Wei-Di Chang",
      "Edward Smith",
      "Shixiang (Shane) Gu",
      "Doina Precup",
      "David Meger"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c2201e444d2b22a10ca50116a522b9a9-Abstract-Conference.html": {
    "title": "First- and Second-Order Bounds for Adversarial Linear Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Olkhovskaya",
      "Jack Mayo",
      "Tim van Erven",
      "Gergely Neu",
      "Chen-Yu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c23ccf9eedf87e4380e92b75b24955bb-Abstract-Conference.html": {
    "title": "Calibrated Stackelberg Games: Learning Optimal Commitments Against Calibrated Agents",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nika Haghtalab",
      "Chara Podimata",
      "Kunhe Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c23fdcb9f8e28af705a87de1375a705c-Abstract-Conference.html": {
    "title": "Density of States Prediction of Crystalline Materials via Prompt-guided Multi-Modal Transformer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Namkyeong Lee",
      "Heewoong Noh",
      "Sungwon Kim",
      "Dongmin Hyun",
      "Gyoung S. Na",
      "Chanyoung Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c242f2b7f60d8c685b6481939330e241-Abstract-Conference.html": {
    "title": "A Single-Loop Accelerated Extra-Gradient Difference Algorithm with Improved Complexity Bounds for Constrained Minimax Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Liu",
      "Fanhua Shang",
      "Weixin An",
      "Junhao Liu",
      "Hongying Liu",
      "Zhouchen Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c2469e35d469e3c0eca09dbe484eb474-Abstract-Conference.html": {
    "title": "Unleashing the Full Potential of Product Quantization for Large-Scale Image Retrieval",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Liang",
      "Shiliang Zhang",
      "Li Ken Li",
      "Xiaoyu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c2586b71fd150fb56952e253a9c551cc-Abstract-Conference.html": {
    "title": "The Bayesian Stability Zoo",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shay Moran",
      "Hilla Schefler",
      "Jonathan Shafer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c272409133942e2f4b7631c8cb7e507e-Abstract-Conference.html": {
    "title": "Improving the Knowledge Gradient Algorithm",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Yang",
      "Siyang Gao",
      "Chin Pang Ho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c276c3303c0723c83a43b95a44a1fcbf-Abstract-Conference.html": {
    "title": "Hierarchical Multi-Agent Skill Discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyu Yang",
      "Yaodong Yang",
      "Zhenbo Lu",
      "Wengang Zhou",
      "Houqiang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c281c5a17ad2e55e1ac1ca825071f991-Abstract-Conference.html": {
    "title": "Deep Optimal Transport: A Practical Algorithm for Photo-realistic Image Restoration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theo Adrai",
      "Guy Ohayon",
      "Michael Elad",
      "Tomer Michaeli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c28ef8449dc21c90696c80ce47b3b5cc-Abstract-Conference.html": {
    "title": "DAW: Exploring the Better Weighting Function for Semi-supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Sun",
      "Huayu Mai",
      "Tianzhu Zhang",
      "Feng Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c290d4373c495b2cad0625d6288260f0-Abstract-Conference.html": {
    "title": "Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Tamkin",
      "Margalit Glasgow",
      "Xiluo He",
      "Noah Goodman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c2a8060fd22744b38177d9e428a052e0-Abstract-Conference.html": {
    "title": "On the Exploitability of Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manli Shu",
      "Jiongxiao Wang",
      "Chen Zhu",
      "Jonas Geiping",
      "Chaowei Xiao",
      "Tom Goldstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c2e4cebba2fdb3dac7d2022421062765-Abstract-Conference.html": {
    "title": "Residual Q-Learning: Offline and Online Policy Customization without Value",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenran Li",
      "Chen Tang",
      "Haruki Nishimura",
      "Jean Mercat",
      "Masayoshi TOMIZUKA",
      "Wei Zhan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c2eac51b6353a4441e8b7426f8e8db78-Abstract-Conference.html": {
    "title": "LICO: Explainable Models with Language-Image COnsistency",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Lei",
      "Zilong Li",
      "Yangyang Li",
      "Junping Zhang",
      "Hongming Shan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c2f2230abc7ccf669f403be881d3ffb7-Abstract-Conference.html": {
    "title": "Solving Inverse Physics Problems with Score Matching",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Holzschuh",
      "Simona  Vegetti",
      "Nils Thuerey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c3532dd633e600e9f6db57aa7ae0c858-Abstract-Conference.html": {
    "title": "Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashanka Venkataramanan",
      "Ewa Kijak",
      "laurent amsaleg",
      "Yannis Avrithis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c35f8e2fc6d81f195009a1d2ae5f6ae9-Abstract-Conference.html": {
    "title": "Approximation-Generalization Trade-offs under (Approximate) Group Equivariance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mircea Petrache",
      "Shubhendu Trivedi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c362fbc0d182c6b4b8dadb90177239e4-Abstract-Conference.html": {
    "title": "Equivariant Neural Operator Learning with Graphon Convolution",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoran Cheng",
      "Jian Peng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c3909e3abe8ebdb20c42a42ce0bc907d-Abstract-Conference.html": {
    "title": "Reinforcement Learning with Simple Sequence Priors",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tankred Saanum",
      "Noémi Éltető",
      "Peter Dayan",
      "Marcel Binz",
      "Eric Schulz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c39578c86423df5f9e8834ce1cd456e4-Abstract-Conference.html": {
    "title": "Fed-FA: Theoretically Modeling Client Data Divergence for Federated Language Backdoor Defense",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhang",
      "Deli Chen",
      "Hao Zhou",
      "Fandong Meng",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c3aba4234afd1c8116d879ba183f4835-Abstract-Conference.html": {
    "title": "One Less Reason for Filter Pruning: Gaining Free Adversarial Robustness with Structured Grouped Kernel Pruning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaochen (Henry) Zhong",
      "Zaichuan You",
      "Jiamu Zhang",
      "Sebastian Zhao",
      "Zachary LeClaire",
      "Zirui Liu",
      "Daochen Zha",
      "Vipin Chaudhary",
      "Shuai Xu",
      "Xia Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c3e969ea20542a6a11e6caeac736a0b9-Abstract-Conference.html": {
    "title": "Survival Instinct in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Li",
      "Dipendra Misra",
      "Andrey Kolobov",
      "Ching-An Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c3fa3a7d50b34732c6d08f6f66380d75-Abstract-Conference.html": {
    "title": "Recurrent Hypernetworks are Surprisingly Strong in Meta-RL",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Beck",
      "Risto Vuorio",
      "Zheng Xiong",
      "Shimon Whiteson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c3fe2a07ec47b89c50e89706d2e23358-Abstract-Conference.html": {
    "title": "The Target-Charging Technique for Privacy Analysis across Interactive Computations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edith Cohen",
      "Xin Lyu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c428adf74782c2092d254329b6b02482-Abstract-Conference.html": {
    "title": "Diffusion Schrödinger Bridge Matching",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Shi",
      "Valentin De Bortoli",
      "Andrew Campbell",
      "Arnaud Doucet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c43b987f23fd5ea840df2b2be426315c-Abstract-Conference.html": {
    "title": "Interpreting Unsupervised Anomaly Detection in Security via Rule Extraction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyu Li",
      "Qing Li",
      "Yu Zhang",
      "Dan Zhao",
      "Yong Jiang",
      "Yong Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c44a04289beaf0a7d968a94066a1d696-Abstract-Conference.html": {
    "title": "Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mitsuhiko Nakamoto",
      "Simon Zhai",
      "Anikait Singh",
      "Max Sobol Mark",
      "Yi Ma",
      "Chelsea Finn",
      "Aviral Kumar",
      "Sergey Levine"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c464fc4516aca4e68f2a14e67c6f0402-Abstract-Conference.html": {
    "title": "PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hojoon Lee",
      "Hanseul Cho",
      "HYUNSEUNG KIM",
      "DAEHOON GWAK",
      "Joonkee Kim",
      "Jaegul Choo",
      "Se-Young Yun",
      "Chulhee Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c47bfcc8e2eccdc540fad1e25f13aa4d-Abstract-Conference.html": {
    "title": "Metropolis Sampling for Constrained Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nic Fishman",
      "Leo Klarner",
      "Emile Mathieu",
      "Michael Hutchinson",
      "Valentin De Bortoli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c47ec10bc135be5c3663ba344d29a6a5-Abstract-Conference.html": {
    "title": "ReTR: Modeling Rendering Via Transformer for Generalizable Neural Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixun Liang",
      "Hao He",
      "Yingcong Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c4889bd7f7ce643003746526da2c2fc4-Abstract-Conference.html": {
    "title": "Stability Guarantees for Feature Attributions with Multiplicative Smoothing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Xue",
      "Rajeev Alur",
      "Eric Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c48bc80aa5d3cbbdd712d1cc107b8319-Abstract-Conference.html": {
    "title": "Pruning vs Quantization: Which is Better?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrey Kuzmin",
      "Markus Nagel",
      "Mart van Baalen",
      "Arash Behboodi",
      "Tijmen Blankevoort"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c48fe446e651cd49fb58a6833e015103-Abstract-Conference.html": {
    "title": "EvoFed: Leveraging Evolutionary Strategies for Communication-Efficient Federated Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mahdi Rahimi",
      "Hasnain Irshad Bhatti",
      "Younghyun Park",
      "Humaira Kousar",
      "Do-Yeon Kim",
      "Jaekyun Moon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c4bf73386022473a652a18941e9ea6f8-Abstract-Conference.html": {
    "title": "StateMask: Explaining Deep Reinforcement Learning through State Mask",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelei Cheng",
      "Xian Wu",
      "Jiahao Yu",
      "Wenhai Sun",
      "Wenbo Guo",
      "Xinyu Xing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c4cfdc27b46659e70a142ac249485a49-Abstract-Conference.html": {
    "title": "Faster Margin Maximization Rates for Generic Optimization Methods",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanghui Wang",
      "Zihao Hu",
      "Vidya Muthukumar",
      "Jacob D. Abernethy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c4d66eae503694424123b93ac0fbaf17-Abstract-Conference.html": {
    "title": "Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen (Vincent) Zhang",
      "Johannes Kirschner",
      "Junxi Zhang",
      "Francesco Zanini",
      "Alex Ayoub",
      "Masood Dehghan",
      "Dale Schuurmans"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c4e380fb74dec9da9c7212e834657aa9-Abstract-Conference.html": {
    "title": "Federated Linear Bandits with Finite Adversarial Actions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Fan",
      "Ruida Zhou",
      "Chao Tian",
      "Cong Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c50a537060022ba5fc3d6a856625b664-Abstract-Conference.html": {
    "title": "BERT Lost Patience Won't Be Robust to Adversarial Slowdown",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Coalson",
      "Gabriel Ritter",
      "Rakesh Bobba",
      "Sanghyun Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c518f504ad5894ccb264a9890f0f5544-Abstract-Conference.html": {
    "title": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeming Chen",
      "Gail Weiss",
      "Eric Mitchell",
      "Asli Celikyilmaz",
      "Antoine Bosselut"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c529dba08a146ea8d6cf715ae8930cbe-Abstract-Conference.html": {
    "title": "Regularity as Intrinsic Reward for Free Play",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cansu Sancaktar",
      "Justus Piater",
      "Georg Martius"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c5601d99ed028448f29d1dae2e4a926d-Abstract-Conference.html": {
    "title": "Guiding Large Language Models via Directional Stimulus Prompting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekun Li",
      "Baolin Peng",
      "Pengcheng He",
      "Michel Galley",
      "Jianfeng Gao",
      "Xifeng Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c5cf13bfd3762821ef7607e63ee90075-Abstract-Conference.html": {
    "title": "Distributionally Robust Ensemble of Lottery Tickets Towards Calibrated Sparse Network Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hitesh Sapkota",
      "Dingrong Wang",
      "Zhiqiang Tao",
      "Qi Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c5e44243e16c9d61d3897ba1095f5f6c-Abstract-Conference.html": {
    "title": "A Recurrent Neural Circuit Mechanism of Temporal-scaling Equivariant Representation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Zuo",
      "Xiao Liu",
      "Ying Nian Wu",
      "Si Wu",
      "Wenhao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c5ed2c8acda8c3716b1b6f9c6c713aaa-Abstract-Conference.html": {
    "title": "Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnaud Robert",
      "Ciara Pike-Burke",
      "Aldo A. Faisal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c60bd92a01804b7df0540ed7ca2f7c05-Abstract-Conference.html": {
    "title": "NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Yi",
      "Haokui Zhang",
      "Rong Xiao",
      "Nannan Wang",
      "Xiaoyu Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c62fe1daeb10814d33e5a33ba466ecaf-Abstract-Conference.html": {
    "title": "Sensitivity in Translation Averaging",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lalit Manam",
      "Venu Madhav Govindu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c6483c8a68083af3383f91ee0dc6db95-Abstract-Conference.html": {
    "title": "Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chung-En Tsai",
      "Ying-Ting Lin",
      "Yen-Huan Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c67b138497305835e76fdedd48dd4e59-Abstract-Conference.html": {
    "title": "Outlier-Robust Wasserstein DRO",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sloan Nietert",
      "Ziv Goldfeld",
      "Soroosh Shafiee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c69465280855cfe25d566e359da140c1-Abstract-Conference.html": {
    "title": "Certified Minimax Unlearning with Generalization Rates and Deletion Capacity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Liu",
      "Jian Lou",
      "Zhan Qin",
      "Kui Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c6af791af7ef0f3e02bccef011211ca5-Abstract-Conference.html": {
    "title": "An Empirical Study Towards Prompt-Tuning for Graph Contrastive Pre-Training in Recommendations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Yang",
      "Xiangyu Zhao",
      "Yicong Li",
      "Hongxu Chen",
      "Guandong Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c6afe9a5d1e1068796d32613ddca1ab7-Abstract-Conference.html": {
    "title": "Can Language Models Teach? Teacher Explanations Improve Student Performance via Personalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Swarnadeep Saha",
      "Peter Hase",
      "Mohit Bansal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c6b84d35d783cf289bb0cd7c7b897ea6-Abstract-Conference.html": {
    "title": "Finding Local Minima Efficiently in Decentralized Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhan Xian",
      "Heng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c6e0125e14ea3d1a3de3c33fd2d49fc4-Abstract-Conference.html": {
    "title": "Clifford Group Equivariant Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Ruhe",
      "Johannes Brandstetter",
      "Patrick Forré"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c6f1e44be16e87887b7b894d59ba7f29-Abstract-Conference.html": {
    "title": "NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Lionar",
      "Xiangyu Xu",
      "Min Lin",
      "Gim Hee Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c70741145c2c4f1d0c2e91b98729a49a-Abstract-Conference.html": {
    "title": "Convergence analysis of ODE models for accelerated first-order methods via positive semidefinite kernels",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungbin Kim",
      "Insoon Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c710d6b4507e70c6332bee871b8d1ca5-Abstract-Conference.html": {
    "title": "Curvature Filtrations for Graph Generative Model Evaluation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Southern",
      "Jeremy Wayland",
      "Michael Bronstein",
      "Bastian Rieck"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c7138635035501eb71b0adf6ddc319d6-Abstract-Conference.html": {
    "title": "DiffUTE: Universal Text Editing Diffusion Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxing Chen",
      "Zhuoer Xu",
      "Zhangxuan Gu",
      "jun lan",
      "行 郑",
      "Yaohui Li",
      "Changhua Meng",
      "Huijia Zhu",
      "Weiqiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c7201deff8d507a8fe2e86d34094e154-Abstract-Conference.html": {
    "title": "Sampling weights of deep neural networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik L Bolager",
      "Iryna Burak",
      "Chinmay Datar",
      "Qing Sun",
      "Felix Dietrich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c72861451d6fa9dfa64831102b9bb71a-Abstract-Conference.html": {
    "title": "Fast Attention Requires Bounded Entries",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josh Alman",
      "Zhao Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c74a3a6f44a44b204e26b1a6d7fe4a66-Abstract-Conference.html": {
    "title": "Open Compound Domain Adaptation with Object Style Compensation for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingliang Feng",
      "Hao Shi",
      "Xueyang Liu",
      "Wei Feng",
      "Liang Wan",
      "Yanlin Zhou",
      "Di Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c78f81a878a72566422f37279bca0fd0-Abstract-Conference.html": {
    "title": "Going beyond persistent homology using persistent homology",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johanna Immonen",
      "Amauri Souza",
      "Vikas Garg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c793577b644268259b1416464a6cdb8c-Abstract-Conference.html": {
    "title": "Explore to Generalize in Zero-Shot RL",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ev Zisselman",
      "Itai Lavie",
      "Daniel Soudry",
      "Aviv Tamar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c7b5a35ea98b62512a869c19ea7b03cb-Abstract-Conference.html": {
    "title": "CoLLAT: On Adding Fine-grained Audio Understanding to Language Models using Token-Level Locked-Language Tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dadallage A R Silva",
      "Spencer Whitehead",
      "Christopher Lengerich",
      "Hugh Leather"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c7bee9b76be21146fd592fc2b46614d5-Abstract-Conference.html": {
    "title": "Abide by the law and follow the flow: conservation laws for gradient flows",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sibylle Marcotte",
      "Remi Gribonval",
      "Gabriel Peyré"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c7c7cf10082e454b9662a686ce6f1b6f-Abstract-Conference.html": {
    "title": "Breadcrumbs to the Goal: Supervised Goal Selection from Human-in-the-Loop Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcel Torne Villasevil",
      "Max Balsells I Pamies",
      "Zihan Wang",
      "Samedh Desai",
      "Tao Chen",
      "Pulkit Agrawal",
      "Abhishek Gupta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c7f35864fef057d6fa315afa0275b3ad-Abstract-Conference.html": {
    "title": "Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neel Guha",
      "Mayee Chen",
      "Kush Bhatia",
      "Azalia Mirhoseini",
      "Frederic Sala",
      "Christopher Ré"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c7f43ada17acc234f568dc66da527418-Abstract-Conference.html": {
    "title": "Perceptual Kalman Filters: Online State Estimation under a Perfect Perceptual-Quality Constraint",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dror Freirich",
      "Tomer Michaeli",
      "Ron Meir"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c801e68207da477bbc44182b9fac1129-Abstract-Conference.html": {
    "title": "SEENN: Towards Temporal Spiking Early Exit Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Li",
      "Tamar Geller",
      "Youngeun Kim",
      "Priyadarshini Panda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c80addda8bcd95339921cba7581ac7bd-Abstract-Conference.html": {
    "title": "Distributionally Robust Skeleton Learning of Discrete Bayesian Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeshu Li",
      "Brian Ziebart"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c81690e2cfe63aede8519ad448f56d71-Abstract-Conference.html": {
    "title": "Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanishk Jain",
      "Shyamgopal Karthik",
      "Vineet Gandhi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c836d71b4702d9046b14ce1228c4c11b-Abstract-Conference.html": {
    "title": "Robust Matrix Sensing in the Semi-Random Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xing Gao",
      "Yu Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c837ab3eebe77bffac634939f22ac458-Abstract-Conference.html": {
    "title": "Implicit variance regularization in non-contrastive SSL",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manu Srinath Halvagal",
      "Axel Laborieux",
      "Friedemann Zenke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c83bc020a020cdeb966ed10804619664-Abstract-Conference.html": {
    "title": "ATMAN: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Björn Deiseroth",
      "Mayukh Deb",
      "Samuel Weinbach",
      "Manuel Brack",
      "Patrick Schramowski",
      "Kristian Kersting"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c8450235f227f136242f774b2799581f-Abstract-Conference.html": {
    "title": "Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schrödinger Equation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kirill Neklyudov",
      "Jannes Nys",
      "Luca Thiede",
      "Juan Carrasquilla",
      "Qiang Liu",
      "Max Welling",
      "Alireza Makhzani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c859b99b5d717c9035e79d43dfd69435-Abstract-Conference.html": {
    "title": "Textually Pretrained Speech Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Hassid",
      "Tal Remez",
      "Tu Anh Nguyen",
      "Itai Gat",
      "Alexis CONNEAU",
      "Felix Kreuk",
      "Jade Copet",
      "Alexandre Defossez",
      "Gabriel Synnaeve",
      "Emmanuel Dupoux",
      "Roy Schwartz",
      "Yossi Adi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c868aa7437dc9b29e674cd2e25689021-Abstract-Conference.html": {
    "title": "Riemannian Residual Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isay Katsman",
      "Eric Chen",
      "Sidhanth Holalkere",
      "Anna Asch",
      "Aaron Lou",
      "Ser Nam Lim",
      "Christopher M. De Sa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c87bd5843849884e9430f1693b018d71-Abstract-Conference.html": {
    "title": "Aligning Gradient and Hessian for Neural Signed Distance Function",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruian Wang",
      "Zixiong Wang",
      "Yunxiao Zhang",
      "Shuangmin Chen",
      "Shiqing Xin",
      "Changhe Tu",
      "Wenping Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c89f09849eb5af489abb122394ff0f0b-Abstract-Conference.html": {
    "title": "Achieving Cross Modal Generalization with Multimodal Unified Representation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Xia",
      "Hai Huang",
      "Jieming Zhu",
      "Zhou Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c8a4dd7d9e13583d714ce8580da7bbc7-Abstract-Conference.html": {
    "title": "Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yefan Zhou",
      "TIANYU PANG",
      "Keqin Liu",
      "charles martin",
      "Michael W. Mahoney",
      "Yaoqing Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c8b100b376a7b338c84801b699935098-Abstract-Conference.html": {
    "title": "Gaussian Process Probes (GPP) for Uncertainty-Aware Probing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Wang",
      "Alexander Ku",
      "Jason Baldridge",
      "Tom Griffiths",
      "Been Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c8e1620b29d546c2999a9339ab29aa82-Abstract-Conference.html": {
    "title": "Inferring Hybrid Neural Fluid Fields from Videos",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong-Xing Yu",
      "Yang Zheng",
      "Yuan Gao",
      "Yitong Deng",
      "Bo Zhu",
      "Jiajun Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9034f4f90fbfad5b80f47fe3dd6cf51-Abstract-Conference.html": {
    "title": "MeGraph: Capturing Long-Range Interactions by Alternating Local and Hierarchical Aggregation on Multi-Scaled Graph Hierarchy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honghua Dong",
      "Jiawei Xu",
      "Yu Yang",
      "Rui Zhao",
      "Shiwen Wu",
      "Chun Yuan",
      "Xiu Li",
      "Chris J. Maddison",
      "Lei Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c904c5d43d8a01177063977bd67bf6fc-Abstract-Conference.html": {
    "title": "Double and Single Descent in Causal Inference with an Application to High-Dimensional Synthetic Control",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jann Spiess",
      "guido imbens",
      "Amar Venugopal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c917d8b9e01427f3184d80ade22f4d1f-Abstract-Conference.html": {
    "title": "IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenglin Huang",
      "Xiaoan Bao",
      "Na Zhang",
      "Qingqi Zhang",
      "Xiao Tu",
      "Biao Wu",
      "Xi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c919a2b5ec1de69f2629f9119676e336-Abstract-Conference.html": {
    "title": "Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungyong Moon",
      "Junyoung Yeom",
      "Bumsoo Park",
      "Hyun Oh Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9450295fd667740a39a68148fc17f6e-Abstract-Conference.html": {
    "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Avani Gupta",
      "Saurabh Saini",
      "P J Narayanan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9493f7cb0d1ec4ae5fc6e0c1a5aca63-Abstract-Conference.html": {
    "title": "Mitigating the Effect of Incidental Correlations on Part-based Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Bhatt",
      "Deepayan Das",
      "Leonid Sigal",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c94a632545000531f0b47000e9caa5b6-Abstract-Conference.html": {
    "title": "Towards In-context Scene Understanding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ivana Balazevic",
      "David Steiner",
      "Nikhil Parthasarathy",
      "Relja Arandjelović",
      "Olivier Henaff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c94bbbef466ab1b2cfa100e41413b3a8-Abstract-Conference.html": {
    "title": "Prediction and Control in Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishanth Anand",
      "Doina Precup"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c95c049637c5c549c2a08e8d6dcbca4b-Abstract-Conference.html": {
    "title": "EDGI: Equivariant Diffusion for Planning with Embodied Agents",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johann Brehmer",
      "Joey Bose",
      "Pim de Haan",
      "Taco S. Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c972859a984a21658432d7320c7df385-Abstract-Conference.html": {
    "title": "Topological RANSAC for instance verification and retrieval without fine-tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoyuan An",
      "Ju-hyeong Seon",
      "Inkyu An",
      "Yuchi Huo",
      "Sung-eui Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c981fd12b1d5703f19bd8289da9fc996-Abstract-Conference.html": {
    "title": "An Alternating Optimization Method for Bilevel Problems under the Polyak-Łojasiewicz Condition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Xiao",
      "Songtao Lu",
      "Tianyi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9a7214961b9bd0ee93755bfa0abcea7-Abstract-Conference.html": {
    "title": "Sub-optimality of the Naive Mean Field approximation for proportional high-dimensional Linear Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaze Qiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9b1fe9c41f1eeec3a659154d575a282-Abstract-Conference.html": {
    "title": "The Gain from Ordering in Online Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vasilis Kontonis",
      "Mingchen Ma",
      "Christos Tzamos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9c54ac0dd5e942b99b2b51c297544fd-Abstract-Conference.html": {
    "title": "Variational Annealing on Graphs for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Sanokowski",
      "Wilhelm Berghammer",
      "Sepp Hochreiter",
      "Sebastian Lehner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9cd2d12abe92f30b1442557bdbe8f5a-Abstract-Conference.html": {
    "title": "Chasing Fairness Under Distribution Shift: A Model Weight Perturbation Approach",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimeng (Stephen) Jiang",
      "Xiaotian Han",
      "Hongye Jin",
      "Guanchu Wang",
      "Rui Chen",
      "Na Zou",
      "Xia Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9cde817d04811ba28e44071bd9f76a5-Abstract-Conference.html": {
    "title": "Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow Shrink Trees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bryan Andrews",
      "Joseph Ramsey",
      "Ruben Sanchez Romero",
      "Jazmin Camchong",
      "Erich Kummerfeld"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9d9659d1d960b53e8121469ef1f2df5-Abstract-Conference.html": {
    "title": "Bayesian Active Causal Discovery with Multi-Fidelity Experiments",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Zhang",
      "Chaozhuo Li",
      "Xu Chen",
      "Xing Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9e6ac15e689e06139d7b39e1667b165-Abstract-Conference.html": {
    "title": "Meta-Learning with Neural Bandit Scheduler",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhe Qi",
      "Yikun Ban",
      "Tianxin Wei",
      "Jiaru Zou",
      "Huaxiu Yao",
      "Jingrui He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c9ef471a579197c4ed99df2aa542ce97-Abstract-Conference.html": {
    "title": "ClusterFomer: Clustering As A Universal Visual Learner",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Liang",
      "Yiming Cui",
      "Qifan Wang",
      "Tong Geng",
      "Wenguan Wang",
      "Dongfang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ca0f5358dbadda74b3049711887e9ead-Abstract-Conference.html": {
    "title": "Spike-driven Transformer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Man Yao",
      "JiaKui Hu",
      "Zhaokun Zhou",
      "Li Yuan",
      "Yonghong Tian",
      "Bo Xu",
      "Guoqi Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ca22641c182b3b9608634edb4d09bc33-Abstract-Conference.html": {
    "title": "Dis-inhibitory neuronal circuits can control the sign of synaptic plasticity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Rossbroich",
      "Friedemann Zenke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ca24eb48806df3af49e5ac59d8a46f67-Abstract-Conference.html": {
    "title": "Accelerated Zeroth-order Method for Non-Smooth Stochastic Convex Optimization Problem with Infinite Variance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Kornilov",
      "Ohad Shamir",
      "Aleksandr Lobanov",
      "Darina Dvinskikh",
      "Alexander Gasnikov",
      "Innokentiy Shibaev",
      "Eduard Gorbunov",
      "Samuel Horváth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ca642f8e1174012d67c05c1c9f969644-Abstract-Conference.html": {
    "title": "Generator Identification for Linear SDEs with Additive and Multiplicative Noise",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyuan Wang",
      "Xi Geng",
      "Wei Huang",
      "Biwei Huang",
      "Mingming Gong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ca6980a3dba7fb3e4e66925656dba68b-Abstract-Conference.html": {
    "title": "Post-processing Private Synthetic Data for Improving Utility on Selected Measures",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wang",
      "Shivchander Sudalairaj",
      "John Henning",
      "Kristjan Greenewald",
      "Akash Srivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ca774047bc3b46cc81e53ead34cd5d5a-Abstract-Conference.html": {
    "title": "A Bayesian Approach To Analysing Training Data Attribution In Deep Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elisa Nguyen",
      "Minjoon Seo",
      "Seong Joon Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ca8c6f28d8ba1e732e3f217ab05c4ec0-Abstract-Conference.html": {
    "title": "GPEX, A Framework For Interpreting Artificial Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Hossein Hosseini Akbarnejad",
      "Gilbert Bigras",
      "Nilanjan Ray"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ca9567d8ef6b2ea2da0d7eed57b933ee-Abstract-Conference.html": {
    "title": "On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieyu Zhang",
      "Bohan Wang",
      "Zhengyu Hu",
      "Pang Wei W. Koh",
      "Alexander J. Ratner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ca9eaef07eca2a50fc626cb929617b1c-Abstract-Conference.html": {
    "title": "An information-theoretic quantification of the content of communication between brain regions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Celotto",
      "Jan Bím",
      "Alejandro Tlaie",
      "Vito De Feo",
      "Alessandro Toso",
      "Stefan Lemke",
      "Daniel Chicharro",
      "Hamed Nili",
      "Malte Bieler",
      "Ileana Hanganu-Opatz",
      "Tobias Donner",
      "Andrea Brovelli",
      "Stefano Panzeri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cab5ae2704d3e01f06a92512a5376b87-Abstract-Conference.html": {
    "title": "Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anant Raj",
      "Umut Simsekli",
      "Alessandro Rudi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/caba69fbc9fa0b06241b98a44cab8b31-Abstract-Conference.html": {
    "title": "TopoSRL: Topology preserving self-supervised Simplicial Representation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiren Madhu",
      "Sundeep Prabhakar Chepuri"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cad2fd66cf88226d868f90a7cbaa4a53-Abstract-Conference.html": {
    "title": "On the spectral bias of two-layer linear networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya  Vardhan Varre",
      "Maria-Luiza Vladarean",
      "Loucas PILLAUD-VIVIEN",
      "Nicolas Flammarion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cb1c4782f159b55380b4584671c4fd88-Abstract-Conference.html": {
    "title": "GMSF: Global Matching Scene Flow",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushan Zhang",
      "Johan Edstedt",
      "Bastian Wandt",
      "Per-Erik Forssen",
      "Maria Magnusson",
      "Michael Felsberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cb2266111eadcfa2c02187ace64e2183-Abstract-Conference.html": {
    "title": "Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Huang",
      "Henry Lam",
      "Haofeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cb3658b9983f677670a246c46ece553d-Abstract-Conference.html": {
    "title": "LuminAIRe: Illumination-Aware Conditional Image Repainting for Lighting-Realistic Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Tang",
      "Haofeng Zhong",
      "Shuchen Weng",
      "Boxin Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cb7943be26bb34f036c7e4068c490903-Abstract-Conference.html": {
    "title": "A graphon-signal analysis of graph neural networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ron Levie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cb931eddd563f8d473c355518ce8601c-Abstract-Conference.html": {
    "title": "Class-Conditional Conformal Prediction with Many Classes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiffany Ding",
      "Anastasios Angelopoulos",
      "Stephen Bates",
      "Michael Jordan",
      "Ryan J. Tibshirani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cba76ef96c4cd625631ab4d33285b045-Abstract-Conference.html": {
    "title": "Reward Imputation with Sketching for Contextual Batched Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Zhang",
      "Ninglu Shao",
      "Zihua Si",
      "Jun Xu",
      "Wenhan Wang",
      "Hanjing Su",
      "Ji-Rong Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cbabc2f70de2dd09f491a8715ec3e80f-Abstract-Conference.html": {
    "title": "A Unified Model and Dimension for Interactive Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nataly Brukhim",
      "Miro Dudik",
      "Aldo Pacchiano",
      "Robert E. Schapire"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cbaffeeda13dbd8bf9489feb3f198ff4-Abstract-Conference.html": {
    "title": "Simple, Scalable and Effective Clustering via One-Dimensional Projections",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moses Charikar",
      "Monika Henzinger",
      "Lunjia Hu",
      "Maximilian Vötsch",
      "Erik Waingarten"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cbb1fa8e7f515e796cda6621a703492f-Abstract-Conference.html": {
    "title": "Streaming PCA for Markovian Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syamantak Kumar",
      "Purnamrita Sarkar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cbe1fd3136e0f049bb8bc104231ccb99-Abstract-Conference.html": {
    "title": "Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beier Zhu",
      "Kaihua Tang",
      "QIANRU SUN",
      "Hanwang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cc19e4ffde5540ac3fcda240e6d975cb-Abstract-Conference.html": {
    "title": "Learning to Parameterize Visual Attributes for Open-set Fine-grained Retrieval",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Wang",
      "Jianlong Chang",
      "Haojie  Li",
      "Zhihui Wang",
      "Wanli Ouyang",
      "Qi Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cc473bb3ec4176a5e640c3a6b5fb5239-Abstract-Conference.html": {
    "title": "Learning List-Level Domain-Invariant Representations for Ranking",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruicheng Xian",
      "Honglei Zhuang",
      "Zhen Qin",
      "Hamed Zamani",
      "Jing Lu",
      "Ji Ma",
      "Kai Hui",
      "Han Zhao",
      "Xuanhui Wang",
      "Michael Bendersky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cc56ae4929d792351a66c39aafb4a34d-Abstract-Conference.html": {
    "title": "Homotopy-based training of NeuralODEs for accurate dynamics discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joon-Hyuk Ko",
      "Hankyul Koh",
      "Nojun Park",
      "Wonho Jhe"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cc57fac10eacadb3b72a907ac48f9a98-Abstract-Conference.html": {
    "title": "Simplifying and Empowering Transformers for Large-Graph Representations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitian Wu",
      "Wentao Zhao",
      "Chenxiao Yang",
      "Hengrui Zhang",
      "Fan Nie",
      "Haitian Jiang",
      "Yatao Bian",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cc83e97320000f4e08cb9e293b12cf7e-Abstract-Conference.html": {
    "title": "Understanding the Limitations of Deep Models for Molecular property prediction: Insights and Solutions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Xia",
      "Lecheng Zhang",
      "Xiao Zhu",
      "Yue Liu",
      "Zhangyang Gao",
      "Bozhen Hu",
      "Cheng Tan",
      "Jiangbin Zheng",
      "Siyuan Li",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cc8638553a347b1834d98be7613fa3f0-Abstract-Conference.html": {
    "title": "Neural Circuits for Fast Poisson Compressed Sensing in the Olfactory Bulb",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Zavatone-Veth",
      "Paul Masset",
      "William Tong",
      "Joseph D. Zak",
      "Venkatesh Murthy",
      "Cengiz Pehlevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ccba10dd4e80e7276054222bb95d467c-Abstract-Conference.html": {
    "title": "Group Fairness in Peer Review",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haris Aziz",
      "Evi Micha",
      "Nisarg Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ccda3c632cc8590ee60ca5ba226a4c30-Abstract-Conference.html": {
    "title": "Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran He",
      "Chenjia Bai",
      "Kang Xu",
      "Zhuoran Yang",
      "Weinan Zhang",
      "Dong Wang",
      "Bin Zhao",
      "Xuelong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ccf02786d28730e8311676ffa842e216-Abstract-Conference.html": {
    "title": "Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayantan Choudhury",
      "Eduard Gorbunov",
      "Nicolas Loizou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ccf4a7323b9ee3e54bf77f0e876b3f8b-Abstract-Conference.html": {
    "title": "Assessor360: Multi-sequence Network for Blind Omnidirectional Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhe Wu",
      "Shuwei Shi",
      "Haoming Cai",
      "Mingdeng Cao",
      "Jing Xiao",
      "Yinqiang Zheng",
      "Yujiu Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ccf6d8b4a1fe9d9c8192f00c713872ea-Abstract-Conference.html": {
    "title": "Lossy Image Compression with Conditional Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihan Yang",
      "Stephan Mandt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cd062f8003e38f55dcb93df55b2683d6-Abstract-Conference.html": {
    "title": "Leveraging the two-timescale regime to demonstrate convergence of neural networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Marion",
      "Raphaël Berthier"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cd40d0d65bfebb894ccc9ea822b47fa8-Abstract-Conference.html": {
    "title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bailin Wang",
      "Zi Wang",
      "Xuezhi Wang",
      "Yuan Cao",
      "Rif A. Saurous",
      "Yoon Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cd5404354496e39d37b7947d8a0d7b72-Abstract-Conference.html": {
    "title": "Don't just prune by magnitude! Your mask topology is a secret weapon",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duc Hoang",
      "Souvik Kundu",
      "Shiwei Liu",
      "Zhangyang \"Atlas\" Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cd687a58a13b673eea3fc1b2e4944cf7-Abstract-Conference.html": {
    "title": "Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingyi Chen",
      "Qinghua Tao",
      "Francesco Tonin",
      "Johan Suykens"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cd830afc6208a346e4ec5caf1b08b4b4-Abstract-Conference.html": {
    "title": "End-To-End Latent Variational Diffusion Models for Inverse Problems in High Energy Physics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Shmakov",
      "Kevin Greif",
      "Michael Fenton",
      "Aishik Ghosh",
      "Pierre Baldi",
      "Daniel Whiteson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cd9b4a28fb9eebe0430c3312a4898a41-Abstract-Conference.html": {
    "title": "DiffTraj: Generating GPS Trajectory with Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanshao Zhu",
      "Yongchao Ye",
      "Shiyao Zhang",
      "Xiangyu Zhao",
      "James Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cda04d7ea67ea1376bf8c6962d8541e0-Abstract-Conference.html": {
    "title": "Meta-in-context learning in large language models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Coda-Forno",
      "Marcel Binz",
      "Zeynep Akata",
      "Matt Botvinick",
      "Jane Wang",
      "Eric Schulz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cdaac2a02c4fdcae77ba083b110efcc3-Abstract-Conference.html": {
    "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sotiris Anagnostidis",
      "Dario Pavllo",
      "Luca Biggio",
      "Lorenzo Noci",
      "Aurelien Lucchi",
      "Thomas Hofmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cdcaf772b4f8eda0385d0930517de64a-Abstract-Conference.html": {
    "title": "SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohyeok Lee",
      "Seungyub Han",
      "Taehyun Cho",
      "Jungwoo Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cdd0640218a27e9e2c0e52e324e25db0-Abstract-Conference.html": {
    "title": "SwapPrompt: Test-Time Prompt Adaptation for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XIAOSONG MA",
      "Jie ZHANG",
      "Song Guo",
      "Wenchao Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cdda0657a9f32bc7ddd4343686e7371e-Abstract-Conference.html": {
    "title": "Does a sparse ReLU network training problem always admit an optimum ?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "QUOC-TUNG LE",
      "Remi Gribonval",
      "Elisa Riccietti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cdddf13f06182063c4dbde8cbd5a5c21-Abstract-Conference.html": {
    "title": "Knowledge Diffusion for Distillation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Huang",
      "Yuan Zhang",
      "Mingkai Zheng",
      "Shan You",
      "Fei Wang",
      "Chen Qian",
      "Chang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cde2dc73e0ad650176cdfa9b779eefc7-Abstract-Conference.html": {
    "title": "BayesTune: Bayesian Sparse Deep Model Fine-tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minyoung Kim",
      "Timothy Hospedales"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cde874a797a8300da693d5e412b7fdc0-Abstract-Conference.html": {
    "title": "Exploring Loss Functions for Time-based Training Strategy in Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaoyu Zhu",
      "Wei Fang",
      "Xiaodong Xie",
      "Tiejun Huang",
      "Zhaofei Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cdee6c3eaa2adc285f11da7711a75c12-Abstract-Conference.html": {
    "title": "Learning Rate Free Bayesian Inference in Constrained Domains",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Sharrock",
      "Lester Mackey",
      "Christopher Nemeth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ce182e31662883d4decc84a0255335b6-Abstract-Conference.html": {
    "title": "Volume Feature Rendering for Fast Neural Radiance Field Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Han",
      "Wei Xiang",
      "Lu Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ce1c1ff5d94079dea348a2317a889281-Abstract-Conference.html": {
    "title": "Offline RL with Discrete Proxy Representations for Generalizability in POMDPs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengjie Gu",
      "Xinyu Cai",
      "Dong Xing",
      "Xinrun Wang",
      "Mengchen Zhao",
      "Bo An"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ce26d21662c979d515164b416d4571fe-Abstract-Conference.html": {
    "title": "Meta-AdaM: An Meta-Learned Adaptive Optimizer with Momentum for Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Sun",
      "Hongyang Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ce3cf998b7f59271e80ce03fb74a7115-Abstract-Conference.html": {
    "title": "Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thanh-Dat Truong",
      "Hoang-Quan Nguyen",
      "Bhiksha Raj",
      "Khoa Luu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ce65173b994cf7c925c71b482ee14a8d-Abstract-Conference.html": {
    "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satyapriya Krishna",
      "Jiaqi Ma",
      "Dylan Slack",
      "Asma Ghandeharioun",
      "Sameer  Singh",
      "Himabindu Lakkaraju"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ce79fbf9baef726645bc2337abb0ade2-Abstract-Conference.html": {
    "title": "Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diederik Kingma",
      "Ruiqi Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ce7ff3405c782f761fac7f849b41ae9a-Abstract-Conference.html": {
    "title": "Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zangwei Zheng",
      "Xiaozhe Ren",
      "Fuzhao Xue",
      "Yang Luo",
      "Xin Jiang",
      "Yang You"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ce9d3c592712d23f2ec3671941d67fa1-Abstract-Conference.html": {
    "title": "When Demonstrations meet Generative World Models: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siliang Zeng",
      "Chenliang Li",
      "Alfredo Garcia",
      "Mingyi Hong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cea5bc68b890bffb10f18aaaab2becb1-Abstract-Conference.html": {
    "title": "Neural Priming for Sample-Efficient Adaptation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Wallingford",
      "Vivek Ramanujan",
      "Alex Fang",
      "Aditya Kusupati",
      "Roozbeh Mottaghi",
      "Aniruddha Kembhavi",
      "Ludwig Schmidt",
      "Ali Farhadi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cec8ad7715d0d13899d5d7d31970f527-Abstract-Conference.html": {
    "title": "Derandomized novelty detection with FDR control via conformal e-values",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meshi Bashari",
      "Amir Epstein",
      "Yaniv Romano",
      "Matteo Sesia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ced46a50befedcb884ccf0cbe8c3ad23-Abstract-Conference.html": {
    "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eldar Kurtić",
      "Elias Frantar",
      "Dan Alistarh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cede701f00079e43d053ac57b1e75c3e-Abstract-Conference.html": {
    "title": "Private (Stochastic) Non-Convex Optimization Revisited: Second-Order Stationary Points and Excess Risks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daogao Liu",
      "Arun Ganesh",
      "Sewoong Oh",
      "Abhradeep Guha Thakurta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf04d01a0e76f8b13095349d9caca033-Abstract-Conference.html": {
    "title": "TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Xue",
      "Mengxin Zheng",
      "Ting Hua",
      "Yilin Shen",
      "Yepeng Liu",
      "Ladislau Bölöni",
      "Qian Lou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf4114c34a2b93019aa6e70f99680fae-Abstract-Conference.html": {
    "title": "Minimax Forward and Backward Learning of Evolving Tasks with Performance Guarantees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Veronica Alvarez",
      "Santiago Mazuelas",
      "Jose A. Lozano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf42f133f355e0e07a8957b508b26a1b-Abstract-Conference.html": {
    "title": "Online Label Shift: Optimal Dynamic Regret meets Practical Algorithms",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dheeraj Baby",
      "Saurabh Garg",
      "Tzu-Ching Yen",
      "Sivaraman Balakrishnan",
      "Zachary Lipton",
      "Yu-Xiang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf57022dff0929796f85ac99d7cefa86-Abstract-Conference.html": {
    "title": "Self-supervised video pretraining yields robust and more human-aligned visual representations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Parthasarathy",
      "S. M. Ali Eslami",
      "Joao Carreira",
      "Olivier Henaff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf66f995883298c4db2f0dcba28fb211-Abstract-Conference.html": {
    "title": "Slot-guided Volumetric Object Radiance Fields",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "DI QI",
      "Tong Yang",
      "Xiangyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf701db0e3b4d0b8681ca6915ac3e87e-Abstract-Conference.html": {
    "title": "Riemannian SAM: Sharpness-Aware Minimization on Riemannian Manifolds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihun Yun",
      "Eunho Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf70320e93c08b39b1b29a348097a376-Abstract-Conference.html": {
    "title": "ODE-based Recurrent Model-free Reinforcement Learning for POMDPs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanle Zhao",
      "Duzhen Zhang",
      "Han Liyuan",
      "Tielin Zhang",
      "Bo Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf7700139af1fa346d2f57f1f5c26c18-Abstract-Conference.html": {
    "title": "Deep Contract Design via Discontinuous Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tonghan Wang",
      "Paul Duetting",
      "Dmitry Ivanov",
      "Inbal Talgam-Cohen",
      "David C. Parkes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf7a83a5342befd11d3d65beba1be5b0-Abstract-Conference.html": {
    "title": "Temporal Continual Learning with Prior Compensation for Human Motion Prediction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Tang",
      "Jiangxin Sun",
      "Xiaotong Lin",
      "lifang zhang",
      "Wei-Shi Zheng",
      "Jian-Fang Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cf7ba4b2d14e0f6a0e8247af77745094-Abstract-Conference.html": {
    "title": "Kernel Quadrature with Randomly Pivoted Cholesky",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Epperly",
      "Elvira Moreno"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cfaea3a519edf73c3a0480ae8f00bc4e-Abstract-Conference.html": {
    "title": "Analyzing the Sample Complexity of Self-Supervised Image Reconstruction Methods",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobit Klug",
      "Dogukan Atik",
      "Reinhard Heckel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cfcadfe84ee49908cde1fc2992c38d20-Abstract-Conference.html": {
    "title": "FIRAL: An Active Learning Algorithm for Multinomial Logistic Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youguang Chen",
      "George Biros"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cfce727868dcaf5295c0125f9d6fbc0b-Abstract-Conference.html": {
    "title": "AMDP: An Adaptive Detection Procedure for False Discovery Rate Control in High-Dimensional Mediation Analysis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarong Ding",
      "Xuehu ZHU"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d00904cebc0d5b69fada8ad33d0f1422-Abstract-Conference.html": {
    "title": "Strong and Precise Modulation of Human Percepts via Robustified ANNs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Gaziv",
      "Michael Lee",
      "James J DiCarlo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d01bda31bbcd780774ff15b534e03c40-Abstract-Conference.html": {
    "title": "MomentDiff: Generative Video Moment Retrieval from Random to Real",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pandeng Li",
      "Chen-Wei Xie",
      "Hongtao Xie",
      "Liming Zhao",
      "Lei Zhang",
      "Yun Zheng",
      "Deli Zhao",
      "Yongdong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d01db5cd2555ba11f75da0454d57b903-Abstract-Conference.html": {
    "title": "Experimental Designs for Heteroskedastic Variance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Weltz",
      "Tanner Fiez",
      "Alexander Volfovsky",
      "Eric Laber",
      "Blake Mason",
      "houssam nassif",
      "Lalit Jain"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d027a5c93d484a4312cc486d399c62c1-Abstract-Conference.html": {
    "title": "NeuralGF: Unsupervised Point Normal Estimation by Learning Neural Gradient Function",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Li",
      "Huifang Feng",
      "Kanle Shi",
      "Yue Gao",
      "Yi Fang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d04f08ccf582011f43af91ee1c1956d2-Abstract-Conference.html": {
    "title": "Gacs-Korner Common Information Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Kleinman",
      "Alessandro Achille",
      "Stefano Soatto",
      "Jonathan Kao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d066d21c619d0a78c5b557fa3291a8f4-Abstract-Conference.html": {
    "title": "LEACE: Perfect linear concept erasure in closed form",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nora Belrose",
      "David Schneider-Joseph",
      "Shauli Ravfogel",
      "Ryan Cotterell",
      "Edward Raff",
      "Stella Biderman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d073692637b4fb8c4eb4b81f0fa2df7b-Abstract-Conference.html": {
    "title": "Robust low-rank training via approximate orthonormal constraints",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayana Savostianova",
      "Emanuele Zangrando",
      "Gianluca Ceruti",
      "Francesco Tudisco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d083980ec9f874025550136b776a96a9-Abstract-Conference.html": {
    "title": "Feature Selection in the Contrastive Analysis Setting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Weinberger",
      "Ian Covert",
      "Su-In Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d0949cbcec31c09431610553a284f94a-Abstract-Conference.html": {
    "title": "Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongsheng Ding",
      "Chen-Yu Wei",
      "Kaiqing Zhang",
      "Alejandro Ribeiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d09ef5264966e17adffd3157265c9946-Abstract-Conference.html": {
    "title": "Unleashing the Power of Randomization in Auditing Differentially Private ML",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Pillutla",
      "Galen Andrew",
      "Peter Kairouz",
      "H. Brendan McMahan",
      "Alina Oprea",
      "Sewoong Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d0ac28b79816b51124fcc804b2496a36-Abstract-Conference.html": {
    "title": "Worst-case Performance of Popular Approximate Nearest Neighbor Search Implementations: Guarantees and Limitations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piotr Indyk",
      "Haike Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d0b2eda0386f477ab14d7e181e16c899-Abstract-Conference.html": {
    "title": "On the Overlooked Structure of Stochastic Gradients",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeke Xie",
      "Qian-Yuan Tang",
      "Mingming Sun",
      "Ping Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d0bcff6425bbf850ec87d5327a965db9-Abstract-Conference.html": {
    "title": "Provable convergence guarantees for black-box variational inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Domke",
      "Robert Gower",
      "Guillaume Garrigos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d0c3841867db2c516454845a450ca885-Abstract-Conference.html": {
    "title": "Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Ding",
      "Laixi Shi",
      "Yuejie Chi",
      "DING ZHAO"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d0c6bc641a56bebee9d985b937307367-Abstract-Conference.html": {
    "title": "IBA: Towards Irreversible Backdoor Attacks in Federated Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thuy Dung Nguyen",
      "Tuan A. Nguyen",
      "Anh Tran",
      "Khoa D Doan",
      "Kok-Seng Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d0da30e312b75a3fffd9e9191f8bc1b0-Abstract-Conference.html": {
    "title": "Spontaneous symmetry breaking in generative diffusion models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Raya",
      "Luca Ambrogioni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d1422213c9f2bdd5178b77d166fba86a-Abstract-Conference.html": {
    "title": "RL-based Stateful Neural Adaptive Sampling and Denoising for Real-Time Path Tracing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Scardigli",
      "Lukas Cavigelli",
      "Lorenz K. Müller"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d160ea01902c33e30660851dfbac5980-Abstract-Conference.html": {
    "title": "A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Babakniya",
      "Zalan Fabian",
      "Chaoyang He",
      "Mahdi Soltanolkotabi",
      "Salman Avestimehr"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d1786f5246c67eefde011599d31b2006-Abstract-Conference.html": {
    "title": "On the Connection between Pre-training Data Diversity and Fine-tuning Robustness",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Ramanujan",
      "Thao Nguyen",
      "Sewoong Oh",
      "Ali Farhadi",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d1881b5125b4e9cf42f6d6d0b6575934-Abstract-Conference.html": {
    "title": "Structured Neural Networks for Density Estimation and Causal Inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asic Chen",
      "Ruian (Ian) Shi",
      "Xiang Gao",
      "Ricardo Baptista",
      "Rahul G. Krishnan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d18d208fa9c333483e5724ade7beff0f-Abstract-Conference.html": {
    "title": "Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharan Vaswani",
      "Amirreza Kazemi",
      "Reza Babanezhad Harikandeh",
      "Nicolas Le Roux"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d191ba4c8923ed8fd8935b7c98658b5f-Abstract-Conference.html": {
    "title": "Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Chen",
      "Ruiyi Zhang",
      "Tong Yu",
      "Rohan Sharma",
      "Zhiqiang Xu",
      "Tong Sun",
      "Changyou Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d1a14493e5f84d6c6129414f0cd1a7c6-Abstract-Conference.html": {
    "title": "Cheaply Estimating Inference Efficiency Metrics for Autoregressive Transformer Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepak Narayanan",
      "Keshav Santhanam",
      "Peter Henderson",
      "Rishi Bommasani",
      "Tony Lee",
      "Percy S. Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d1a25d7e93f06cb422b3a74a0aa3bf3f-Abstract-Conference.html": {
    "title": "Differentiable sorting for censored time-to-event data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andre Vauvelle",
      "Benjamin Wild",
      "Roland Eils",
      "Spiros Denaxas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d1b1a091088904cbc7f7faa2b45c8f36-Abstract-Conference.html": {
    "title": "Multi-Agent Meta-Reinforcement Learning: Sharper Convergence Rates with Task Similarity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichao Mao",
      "Haoran Qiu",
      "Chen Wang",
      "Hubertus Franke",
      "Zbigniew Kalbarczyk",
      "Ravishankar Iyer",
      "Tamer Basar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d1b4076ae067dd23bad5ac2693547a01-Abstract-Conference.html": {
    "title": "Feature Learning for Interpretable, Performant Decision Trees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Good",
      "Torin Kovach",
      "Kyle Miller",
      "Artur Dubrawski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d1c1f61023dca672117b58f813a12d99-Abstract-Conference.html": {
    "title": "Particle-based Variational Inference with Generalized Wasserstein Gradient Flow",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Cheng",
      "Shiyue Zhang",
      "Longlin Yu",
      "Cheng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d1d3cdc9e28b0c67b9df90fca4d1c1b3-Abstract-Conference.html": {
    "title": "PAC Learning Linear Thresholds from Label Proportions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anand Brahmbhatt",
      "Rishi Saket",
      "Aravindan Raghuveer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d212c6c26603c0eb3c9a6b6432386a1f-Abstract-Conference.html": {
    "title": "A new perspective on building efficient and expressive 3D equivariant graph neural networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "weitao Du",
      "Yuanqi Du",
      "Limei Wang",
      "Dieqiao Feng",
      "Guifeng Wang",
      "Shuiwang Ji",
      "Carla P. Gomes",
      "Zhi-Ming Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d242dafdb2c5407ae420bc54c9325fdf-Abstract-Conference.html": {
    "title": "Scalable Fair Influence Maximization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobin Rui",
      "Zhixiao Wang",
      "Jiayu Zhao",
      "Lichao Sun",
      "Wei Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d2706f9149856b6f7016ebf270dd9f25-Abstract-Conference.html": {
    "title": "Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational and Temporal Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeyuan Chen",
      "Dingmin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d28077e5ff52034cd35b4aa15320caea-Abstract-Conference.html": {
    "title": "Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillermo Ortiz-Jimenez",
      "Alessandro Favero",
      "Pascal Frossard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d2b752ed4726286a4b488ae16e091d64-Abstract-Conference.html": {
    "title": "ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Yan",
      "Zhuo Zhang",
      "Guanhong Tao",
      "Kaiyuan Zhang",
      "Xuan Chen",
      "Guangyu Shen",
      "Xiangyu Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d2bdcd4f51eea138365af22b50f3bf0a-Abstract-Conference.html": {
    "title": "DiViNeT: 3D Reconstruction from Disparate Views using Neural Template Regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Vora",
      "Akshay Gadi Patil",
      "Hao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d2dc4d6c7b102d05f111c02a32e7c6bc-Abstract-Conference.html": {
    "title": "Efficient Model-Free Exploration in Low-Rank MDPs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zak Mhammedi",
      "Adam Block",
      "Dylan J Foster",
      "Alexander Rakhlin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d2f6f1dfbf9cd89a78c5a58ef0dec245-Abstract-Conference.html": {
    "title": "Convex-Concave Zero-Sum Stochastic Stackelberg Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denizalp Goktas",
      "Arjun Prakash",
      "Amy Greenwald"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d2fe3a5711a6d488da9e9a78b84ee24c-Abstract-Conference.html": {
    "title": "Near-Linear Time Algorithm for the Chamfer Distance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ainesh Bakshi",
      "Piotr Indyk",
      "Rajesh Jayaram",
      "Sandeep Silwal",
      "Erik Waingarten"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d31b005d817e9c635ec8ffb0fb90190e-Abstract-Conference.html": {
    "title": "Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jose Blanchet",
      "Miao Lu",
      "Tong Zhang",
      "Han Zhong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d33b177b69425e7685b0b1c05bd2a5e4-Abstract-Conference.html": {
    "title": "StyleDrop: Text-to-Image Synthesis of Any Style",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kihyuk Sohn",
      "Lu Jiang",
      "Jarred Barber",
      "Kimin Lee",
      "Nataniel Ruiz",
      "Dilip Krishnan",
      "Huiwen Chang",
      "Yuanzhen Li",
      "Irfan Essa",
      "Michael Rubinstein",
      "Yuan Hao",
      "Glenn Entis",
      "Irina Blok",
      "Daniel Castro Chin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d3408794e41dd23e34634344d662f5e9-Abstract-Conference.html": {
    "title": "Random Cuts are Optimal for Explainable k-Medians",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Makarychev",
      "Liren Shan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d346609ec2fefd3938c898a0dda4a480-Abstract-Conference.html": {
    "title": "Order Matters in the Presence of Dataset Imbalance for Multilingual Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dami Choi",
      "Derrick Xin",
      "Hamid Dadkhahi",
      "Justin Gilmer",
      "Ankush Garg",
      "Orhan Firat",
      "Chih-Kuan Yeh",
      "Andrew M. Dai",
      "Behrooz Ghorbani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d346d91999074dd8d6073d4c3b13733b-Abstract-Conference.html": {
    "title": "Optimizing Prompts for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaru Hao",
      "Zewen Chi",
      "Li Dong",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d3602fc92fb8b9e0d55356c9e8815e2b-Abstract-Conference.html": {
    "title": "Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meena Jagadeesan",
      "Michael Jordan",
      "Jacob Steinhardt",
      "Nika Haghtalab"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d36dfcdb14473a8526111c221660f2ab-Abstract-Conference.html": {
    "title": "Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Brandfonbrener",
      "Ofir Nachum",
      "Joan Bruna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d37c9ad425fe5b65304d500c6edcba00-Abstract-Conference.html": {
    "title": "Exploiting hidden structures in non-convex games for convergence to Nash equilibrium",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iosif Sakos",
      "Emmanouil-Vasileios Vlatakis-Gkaragkounis",
      "Panayotis Mertikopoulos",
      "Georgios Piliouras"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d39e3ae9a11b79691709a7a6e06a63d9-Abstract-Conference.html": {
    "title": "Secure Out-of-Distribution Task Generalization with Energy-Based Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengzhuang Chen",
      "Long-Kai Huang",
      "Jonathan Richard Schwarz",
      "Yilun Du",
      "Ying Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d3b93537b521f15613524415dfe43f37-Abstract-Conference.html": {
    "title": "Autodecoding Latent 3D Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evangelos Ntavelis",
      "Aliaksandr Siarohin",
      "Kyle Olszewski",
      "Chaoyang Wang",
      "Luc V Gool",
      "Sergey Tulyakov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d42523d621194ba54dda098669645f91-Abstract-Conference.html": {
    "title": "A General Framework for Robust G-Invariance in G-Equivariant Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sophia Sanborn",
      "Nina Miolane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d4387c37b3b06e55f86eccdb8cd1f829-Abstract-Conference.html": {
    "title": "State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devleena Das",
      "Sonia Chernova",
      "Been Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d453490ada2b1991852f053fbd213a6a-Abstract-Conference.html": {
    "title": "Machine learning detects terminal singularities",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom Coates",
      "Alexander Kasprzyk",
      "Sara Veneziale"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d45e0bfb5a39477d56b55c0824200008-Abstract-Conference.html": {
    "title": "Efficient Diffusion Policies For Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingyi Kang",
      "Xiao Ma",
      "Chao Du",
      "Tianyu Pang",
      "Shuicheng Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d470d6e007a19ff1666386562c77517c-Abstract-Conference.html": {
    "title": "Selective Sampling and Imitation Learning via Online Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Sekhari",
      "Karthik Sridharan",
      "Wen Sun",
      "Runzhe Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d482f1362bd6a8448d7c35e717c7063a-Abstract-Conference.html": {
    "title": "CamoPatch: An Evolutionary Strategy for Generating Camoflauged Adversarial Patches",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phoenix Williams",
      "Ke Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d4c2f25bf0c33065b7d4fb9be2a9add1-Abstract-Conference.html": {
    "title": "Learning Regularized Monotone Graphon Mean-Field Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengzhuo Zhang",
      "Vincent Tan",
      "Zhaoran Wang",
      "Zhuoran Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d4e1c24ac41ff0b82ca1b171731f0b23-Abstract-Conference.html": {
    "title": "From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwen Xu",
      "Xin Li",
      "Wenxuan Zhang",
      "Meng Zhou",
      "Wai Lam",
      "Luo Si",
      "Lidong Bing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d4eed238cf5807c6b75face996302892-Abstract-Conference.html": {
    "title": "DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuye Gu",
      "Yin Cui",
      "Jonathan Huang",
      "Abdullah Rashwan",
      "Xuan Yang",
      "Xingyi Zhou",
      "Golnaz Ghiasi",
      "Weicheng Kuo",
      "Huizhong Chen",
      "Liang-Chieh Chen",
      "David Ross"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d51e2a4628b15518f58bd1056b2d9124-Abstract-Conference.html": {
    "title": "Multi-scale Diffusion Denoised Smoothing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongheon Jeong",
      "Jinwoo Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d529b943af3dba734f8a7d49efcb6d09-Abstract-Conference.html": {
    "title": "PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phillip Lippe",
      "Bas Veeling",
      "Paris Perdikaris",
      "Richard Turner",
      "Johannes Brandstetter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d53d51e88d92d3723755f6d425bc513b-Abstract-Conference.html": {
    "title": "Accelerating Exploration with Unlabeled Prior Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyang Li",
      "Jason Zhang",
      "Dibya Ghosh",
      "Amy Zhang",
      "Sergey Levine"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d5470483dd38f71f7bd9e68ce1b94145-Abstract-Conference.html": {
    "title": "Towards a Unified Framework of Contrastive Learning for Disentangled Representations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Matthes",
      "Zhiwei Han",
      "Hao Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d54e440c92affd396117e161bbab5e78-Abstract-Conference.html": {
    "title": "An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiarash Banihashem",
      "MohammadTaghi Hajiaghayi",
      "Suho Shin",
      "Max Springer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d553f0e0abb80e2a60328d634583bd2e-Abstract-Conference.html": {
    "title": "Sequential Subset Matching for Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JIAWEI DU",
      "Qin Shi",
      "Joey Tianyi Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d56b84c063265da949fe0feb815dcce8-Abstract-Conference.html": {
    "title": "SLaM: Student-Label Mixing for Distillation with Unlabeled Examples",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vasilis Kontonis",
      "Fotis Iliopoulos",
      "Khoa Trinh",
      "Cenk Baykal",
      "Gaurav Menghani",
      "Erik Vee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d5753be6f71fbfefaf47aa27ec41279c-Abstract-Conference.html": {
    "title": "Mitigating the Popularity Bias of Graph Collaborative Filtering: A Dimensional Collapse Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhang",
      "Hao Zhu",
      "yankai Chen",
      "Zixing Song",
      "Piotr Koniusz",
      "Irwin King"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d599b81036fd1a3b3949b7d444f31082-Abstract-Conference.html": {
    "title": "The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linhao Qu",
      "xiaoyuan luo",
      "Kexue Fu",
      "Manning Wang",
      "Zhijian Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d5b94ca503b33d07f9bef8ed8ee4678b-Abstract-Conference.html": {
    "title": "State Sequences Prediction via Fourier Transform for Representation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingxuan Ye",
      "Yufei Kuang",
      "Jie Wang",
      "Yang Rui",
      "Wengang Zhou",
      "Houqiang Li",
      "Feng Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d5c0f9585592bad5251133813893a6c0-Abstract-Conference.html": {
    "title": "Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Xinrui",
      "Wenhai Wan",
      "Chuanxing Geng",
      "Shao-Yuan Li",
      "Songcan Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d5c56ec4f69c9a473089b16000d3f8cd-Abstract-Conference.html": {
    "title": "Fast Approximation of Similarity Graphs with Kernel Density Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Macgregor",
      "He Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d5f34e7e70d80f5037ab16a48e2d186e-Abstract-Conference.html": {
    "title": "An Efficient Dataset Condensation Plugin and Its Application to Continual Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enneng Yang",
      "Li Shen",
      "Zhenyi Wang",
      "Tongliang Liu",
      "Guibing Guo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d601a9b708cacfad167f6c6c45647a18-Abstract-Conference.html": {
    "title": "Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsu Kim",
      "Federico Berto",
      "Sungsoo Ahn",
      "Jinkyoo Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d60e14c19cd6e0fc38556ad29ac8fbc9-Abstract-Conference.html": {
    "title": "Statistically Valid Variable Importance Assessment through Conditional Permutations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad CHAMMA",
      "Denis A. Engemann",
      "Bertrand Thirion"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d611019afba70d547bd595e8a4158f55-Abstract-Conference.html": {
    "title": "Towards Better Dynamic Graph Learning: New Architecture and Unified Library",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Yu",
      "Leilei Sun",
      "Bowen Du",
      "Weifeng Lv"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d611d06e3207330555fbc10810e70163-Abstract-Conference.html": {
    "title": "Implicit Manifold Gaussian Process Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernardo Fichera",
      "Slava Borovitskiy",
      "Andreas Krause",
      "Aude G Billard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d616a353c711f11c722e3f28d2d9e956-Abstract-Conference.html": {
    "title": "The Crucial Role of Normalization in Sharpness-Aware Minimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Dai",
      "Kwangjun Ahn",
      "Suvrit Sra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d61819e9b4a607b8448de762235148c4-Abstract-Conference.html": {
    "title": "Policy Space Diversity for Non-Transitive Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Yao",
      "Weiming Liu",
      "Haobo Fu",
      "Yaodong Yang",
      "Stephen McAleer",
      "Qiang Fu",
      "Wei Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d62e65cfdba247e0cd7cac5964f9fbd9-Abstract-Conference.html": {
    "title": "Video-Mined Task Graphs for Keystep Recognition in Instructional Videos",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumar Ashutosh",
      "Santhosh Kumar Ramakrishnan",
      "Triantafyllos Afouras",
      "Kristen Grauman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d642b0633afad94f660554e05b40608e-Abstract-Conference.html": {
    "title": "Affinity-Aware Graph Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ameya Velingker",
      "Ali Sinop",
      "Ira Ktena",
      "Petar Veličković",
      "Sreenivas Gollapudi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d65befe6b80ecf7f180b4def503d7776-Abstract-Conference.html": {
    "title": "Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples Regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runqi Lin",
      "Chaojian Yu",
      "Tongliang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d664de48bf5ad8e8e48c77e175eb0e80-Abstract-Conference.html": {
    "title": "Hardware Resilience Properties of Text-Guided Image Classifiers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syed Talal Wasim",
      "Kabila Haile Soboka",
      "Abdulrahman Mahmoud",
      "Salman H. Khan",
      "David Brooks",
      "Gu-Yeon Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d69103d7895f4e2083f24b664003d386-Abstract-Conference.html": {
    "title": "Reliable Off-Policy Learning for Dosage Combinations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Schweisthal",
      "Dennis Frauen",
      "Valentyn Melnychuk",
      "Stefan Feuerriegel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d6938c8e88ef62394d2f4f3fd428e036-Abstract-Conference.html": {
    "title": "A Unified Algorithm Framework for Unsupervised Discovery of Skills based on Determinantal Point Process",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Chen",
      "Vaneet Aggarwal",
      "Tian Lan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d69fdbe4d13080bb7fa33249ca136976-Abstract-Conference.html": {
    "title": "Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengzhi Cao",
      "Chao Yang",
      "Ruimao Zhang",
      "Shuang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d6c01b025cad37d5c8bab4ba18846c02-Abstract-Conference.html": {
    "title": "DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo",
      "Enze Xie",
      "Ruihang Chu",
      "Lanqing Hong",
      "Matthias Niessner",
      "Zhenguo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d6cc45de2e2dea14b96c1eba88fd8ef7-Abstract-Conference.html": {
    "title": "DESSERT: An Efficient Algorithm for Vector Set Search with Vector Set Queries",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Engels",
      "Benjamin Coleman",
      "Vihan Lakshman",
      "Anshumali Shrivastava"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d6d0e41e0b1ed38c76d13c9e417a8f1f-Abstract-Conference.html": {
    "title": "Dynamic Sparsity Is Channel-Level Sparsity Learner",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Yin",
      "Gen Li",
      "Meng Fang",
      "Li Shen",
      "Tianjin Huang",
      "Zhangyang \"Atlas\" Wang",
      "Vlado Menkovski",
      "Xiaolong Ma",
      "Mykola Pechenizkiy",
      "Shiwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d6db7eb6245ec0c6e45f445956994143-Abstract-Conference.html": {
    "title": "Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Liu",
      "Mate Lengyel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d6f764aae383d9ff28a0f89f71defbd9-Abstract-Conference.html": {
    "title": "SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Gonzalez",
      "Nelson Fernandez Pinto",
      "Thuy Tran",
      "elies Gherbi",
      "Hatem Hajri",
      "Nader Masmoudi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d6f8517fceeca1e2cd61721dff786c14-Abstract-Conference.html": {
    "title": "Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Bukharin",
      "Yan Li",
      "Yue Yu",
      "Qingru Zhang",
      "Zhehui Chen",
      "Simiao Zuo",
      "Chao Zhang",
      "Songan Zhang",
      "Tuo Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d702d78b2468d2bc80b22a2fc3e59faf-Abstract-Conference.html": {
    "title": "Private estimation algorithms for stochastic block models and mixture models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjie Chen",
      "Vincent Cohen-Addad",
      "Tommaso d’Orsi",
      "Alessandro Epasto",
      "Jacob Imola",
      "David Steurer",
      "Stefan Tiegel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d705dd6e77decdc399162d6d5b92f6e8-Abstract-Conference.html": {
    "title": "UP-NeRF: Unconstrained Pose Prior-Free Neural Radiance Field",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Injae Kim",
      "Minhyuk Choi",
      "Hyunwoo J. Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d72ae75abaa70a3b19c5d4f436c680d1-Abstract-Conference.html": {
    "title": "Nearly Optimal Bounds for Cyclic Forgetting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Swartworth",
      "Deanna Needell",
      "Rachel Ward",
      "Mark Kong",
      "Halyun Jeong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d73d5645ddbb9ada6c862116435574f6-Abstract-Conference.html": {
    "title": "Understanding and Improving Feature Learning for Out-of-Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqiang Chen",
      "Wei Huang",
      "Kaiwen Zhou",
      "Yatao Bian",
      "Bo Han",
      "James Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d74e6bfe9ce029526e69db14d2c281ec-Abstract-Conference.html": {
    "title": "Train Hard, Fight Easy: Robust Meta Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ido Greenberg",
      "Shie Mannor",
      "Gal Chechik",
      "Eli Meirom"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d74f9efa1d8ca30b31d65cef8de7c2bf-Abstract-Conference.html": {
    "title": "Towards Semi-Structured Automatic ICD Coding via Tree-based Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Lu",
      "Chandan Reddy",
      "Ping Wang",
      "Yue Ning"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d75c474bc01735929a1fab5d0de3b189-Abstract-Conference.html": {
    "title": "Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Loiseaux",
      "Luis Scoccola",
      "Mathieu Carrière",
      "Magnus Bakke Botnan",
      "Steve OUDOT"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d763b4a2dde0ae7b77498516ce9f439e-Abstract-Conference.html": {
    "title": "Subclass-Dominant Label Noise: A Counterexample for the Success of Early Stopping",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingbin Bai",
      "Zhongyi Han",
      "Erkun Yang",
      "Jun Yu",
      "Bo Han",
      "Dadong Wang",
      "Tongliang Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d77b5482e38339a8068791d939126be2-Abstract-Conference.html": {
    "title": "OpenMask3D: Open-Vocabulary 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayca Takmaz",
      "Elisabetta Fedele",
      "Robert Sumner",
      "Marc Pollefeys",
      "Federico Tombari",
      "Francis Engelmann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d78e9e4316e1714fbb0f20be66f8044c-Abstract-Conference.html": {
    "title": "Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenao Zhang",
      "Boyi Liu",
      "Zhaoran Wang",
      "Tuo Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d79c1390baa2e4835586b094d82e5ffb-Abstract-Conference.html": {
    "title": "Structured Neural-PI Control with End-to-End Stability and Output Tracking Guarantees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqi Cui",
      "Yan Jiang",
      "Baosen Zhang",
      "Yuanyuan Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d7a6f4830a18b6974326310478bfa489-Abstract-Conference.html": {
    "title": "DäRF: Boosting Radiance Fields from Sparse Input Views with Monocular Depth Adaptation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuhn Song",
      "Seonghoon Park",
      "Honggyu An",
      "Seokju Cho",
      "Min-Seop Kwak",
      "Sungjin Cho",
      "Seungryong Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d7b3cef7c31b94a4a533db83d01a8882-Abstract-Conference.html": {
    "title": "Enhancing Knowledge Transfer for Task Incremental Learning with Data-free Subnetwork",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Gao",
      "Xiaojun Shan",
      "Yuchen Zhang",
      "Fan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d826f5aadb26db488b8686097ceea2d1-Abstract-Conference.html": {
    "title": "Proximity-Informed Calibration for Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miao Xiong",
      "Ailin Deng",
      "Pang Wei W. Koh",
      "Jiaying Wu",
      "Shen Li",
      "Jianqing Xu",
      "Bryan Hooi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html": {
    "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timo Schick",
      "Jane Dwivedi-Yu",
      "Roberto Dessi",
      "Roberta Raileanu",
      "Maria Lomeli",
      "Eric Hambro",
      "Luke Zettlemoyer",
      "Nicola Cancedda",
      "Thomas Scialom"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d84a27ff694345aacc21c72097a69ea2-Abstract-Conference.html": {
    "title": "The probability flow ODE is provably fast",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sitan Chen",
      "Sinho Chewi",
      "Holden Lee",
      "Yuanzhi Li",
      "Jianfeng Lu",
      "Adil Salim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d84c0dd9b1bfeee361f3268dcaebf849-Abstract-Conference.html": {
    "title": "Faster Discrete Convex Function Minimization with Predictions: The M-Convex Case",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taihei Oki",
      "Shinsaku Sakaue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d862f7f5445255090de13b825b880d59-Abstract-Conference.html": {
    "title": "Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naoki Egami",
      "Musashi Hinck",
      "Brandon Stewart",
      "Hanying Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d891d240b5784656a0356bf4b00f5cdd-Abstract-Conference.html": {
    "title": "Individual Arbitrariness and Group Fairness",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carol Long",
      "Hsiang Hsu",
      "Wael Alghamdi",
      "Flavio Calmon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d899a31938c7838965b589d9b14a5ca6-Abstract-Conference.html": {
    "title": "ASPEN: Breaking Operator Barriers for Efficient Parallelization of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongseok Park",
      "Kyungmin Bin",
      "Gibum Park",
      "Sangtae Ha",
      "Kyunghan Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d8a7f2f7e346410e8ac7b39d9ff28c4a-Abstract-Conference.html": {
    "title": "Parallel Submodular Function Minimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deeparnab Chakrabarty",
      "Andrei Graur",
      "Haotian Jiang",
      "Aaron Sidford"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d8ace30c68b085556ccce04ed4ae4ebb-Abstract-Conference.html": {
    "title": "Emergent Communication for Rules Reasoning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Guo",
      "Yifan Hao",
      "Rui Zhang",
      "Enshuai Zhou",
      "Zidong Du",
      "xishan zhang",
      "Xinkai Song",
      "Yuanbo Wen",
      "Yongwei Zhao",
      "Xuehai Zhou",
      "Jiaming Guo",
      "Qi Yi",
      "Shaohui Peng",
      "Di Huang",
      "Ruizhi Chen",
      "Qi Guo",
      "Yunji Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d8b29f07599fecdba93d87ed27a65524-Abstract-Conference.html": {
    "title": "A Regularized Conditional GAN for Posterior Sampling in Image Recovery Problems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Bendel",
      "Rizwan Ahmad",
      "Philip Schniter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d8bd445c2abe1343cce0e14b361b2fb3-Abstract-Conference.html": {
    "title": "Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Meulemans",
      "Simon Schug",
      "Seijin Kobayashi",
      "nathaniel daw",
      "Gregory Wayne"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d8ca28a32c05cd3b9b0940e43720f31b-Abstract-Conference.html": {
    "title": "Tight Risk Bounds for Gradient Descent on Separable Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matan Schliserman",
      "Tomer Koren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d9042abf40782fbce28901c1c9c0e8d8-Abstract-Conference.html": {
    "title": "Video Prediction Models as Rewards for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Escontrela",
      "Ademi Adeniji",
      "Wilson Yan",
      "Ajay Jain",
      "Xue Bin Peng",
      "Ken Goldberg",
      "Youngwoon Lee",
      "Danijar Hafner",
      "Pieter Abbeel"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d91b532a76ea98ac1ef5226b862bfc49-Abstract-Conference.html": {
    "title": "Provably (More) Sample-Efficient Offline RL with Options",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyan Hu",
      "Ho-fung Leung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d937cb3fe2851ed0ab9af5e38f885077-Abstract-Conference.html": {
    "title": "Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Xing",
      "Jian Kang",
      "Aoran Xiao",
      "Jiahao Nie",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d938b739ac250e22729cc26e6176f65e-Abstract-Conference.html": {
    "title": "Approximate Allocation Matching for Structural Causal Bandits with Unobserved Confounders",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lai Wei",
      "Muhammad Qasim Elahi",
      "Mahsa Ghasemi",
      "Murat Kocaoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d94b46ec30adee2bbb134f813fc9dde0-Abstract-Conference.html": {
    "title": "Human-Guided Complexity-Controlled Abstractions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Peng",
      "Mycal Tucker",
      "Eoin Kenny",
      "Noga Zaslavsky",
      "Pulkit Agrawal",
      "Julie A Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d95cb79a3421e6d9b6c9a9008c4d07c5-Abstract-Conference.html": {
    "title": "Scenario Diffusion: Controllable Driving Scenario Generation With Diffusion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Pronovost",
      "Meghana Reddy Ganesina",
      "Noureldin Hendy",
      "Zeyu Wang",
      "Andres Morales",
      "Kai Wang",
      "Nick Roy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d9827e811c5a205c1313fb950c072c7d-Abstract-Conference.html": {
    "title": "Label-Only Model Inversion Attacks via Knowledge Transfer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bao-Ngoc Nguyen",
      "Keshigeyan Chandrasegaran",
      "Milad Abdollahzadeh",
      "Ngai-Man (Man) Cheung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d9888cc7baa04c2e44e8115588133515-Abstract-Conference.html": {
    "title": "On the Adversarial Robustness of Out-of-distribution Generalization Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zou",
      "Weiwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d98d9cef0c189f1db95f1d94652f7051-Abstract-Conference.html": {
    "title": "Utilitarian Algorithm Configuration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Devon Graham",
      "Kevin Leyton-Brown",
      "Tim Roughgarden"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d9af4d6ac714626b652da5616ca71f99-Abstract-Conference.html": {
    "title": "Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanshi Liu",
      "Cong Fang",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d9b564716709357b4bccec9fc9ad04d2-Abstract-Conference.html": {
    "title": "Non-Asymptotic Analysis of a UCB-based Top Two Algorithm",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Jourdan",
      "Rémy Degenne"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d9c7c8bd6ad4cebb7d006e5109e0b682-Abstract-Conference.html": {
    "title": "Statistical and Computational Trade-off in Multi-Agent Multi-Armed Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippo Vannella",
      "Alexandre Proutiere",
      "Jaeseong Jeong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d9dc5573f7368201d6409e07e882aa77-Abstract-Conference.html": {
    "title": "On permutation symmetries in Bayesian neural network posteriors: a variational perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Rossi",
      "Ankit Singh",
      "Thomas Hannagan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d9f8b5abc8e0926539ecbb492af7b2f1-Abstract-Conference.html": {
    "title": "Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyuan Wang",
      "Jingyi Xie",
      "Xingxing Zhang",
      "Mingyi Huang",
      "Hang Su",
      "Jun Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/da1131a86ac3c70e0b7cae89c3d4df22-Abstract-Conference.html": {
    "title": "3D molecule generation by denoising voxel grids",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro O. O. Pinheiro",
      "Joshua Rackers",
      "Joseph Kleinhenz",
      "Michael Maser",
      "Omar Mahmood",
      "Andrew Watkins",
      "Stephen Ra",
      "Vishnu Sresht",
      "Saeed Saremi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/da31f4275972a58406b95c277ce7bc8d-Abstract-Conference.html": {
    "title": "Accessing Higher Dimensions for Unsupervised Word Translation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sida Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/da409884a933ecbc4af03338111bf6aa-Abstract-Conference.html": {
    "title": "Inverse Reinforcement Learning with the Average Reward Criterion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyang Wu",
      "Jingyang Ke",
      "Anqi Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/da47bfaf3f3a8d5bbab0d60c5195dc18-Abstract-Conference.html": {
    "title": "DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Yang",
      "Yuwang Wang",
      "Yan Lu",
      "Nanning Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/da5498f88193ff61f0daea1940b819da-Abstract-Conference.html": {
    "title": "Information-guided Planning: An Online Approach for Partially Observable Problems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matheus Aparecido Do Carmo Alves",
      "Amokh Varma",
      "Yehia Elkhatib",
      "Leandro Soriano Marcolino"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/da7ce04b3683b173691ecbb801f2690f-Abstract-Conference.html": {
    "title": "Bayesian Metric Learning for Uncertainty Quantification in Image Retrieval",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederik Warburg",
      "Marco Miani",
      "Silas Brack",
      "Søren Hauberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/da7e0d7210b99ebc91c4a5f911962d6c-Abstract-Conference.html": {
    "title": "Neural Modulation for Flash Memory: An Unsupervised Learning Framework for Improved Reliability",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Zedaka",
      "Elisha Halperin",
      "Evgeny Blaichman",
      "Amit Berman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/da8860a2fe8ddb7589136853bcc313fc-Abstract-Conference.html": {
    "title": "Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Ning Chen",
      "Dan Song",
      "Ayfer Ozgur",
      "Peter Kairouz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/da909fc3893d272f26fd9db82e09d954-Abstract-Conference.html": {
    "title": "Normalization Layers Are All That Sharpness-Aware Minimization Needs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Mueller",
      "Tiffany Vlaar",
      "David Rolnick",
      "Matthias Hein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/daa098aa8e1fc718943ff1ab7b5b30c9-Abstract-Conference.html": {
    "title": "Robust Bayesian Satisficing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artun Saday",
      "Y. Cahit Yıldırım",
      "Cem Tekin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dabaded617b3be96c3ed161498a7d71c-Abstract-Conference.html": {
    "title": "Neural Ideal Large Eddy Simulation: Modeling Turbulence with Neural Stochastic Differential Equations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anudhyan Boral",
      "Zhong Yi Wan",
      "Leonardo Zepeda-Núñez",
      "James Lottes",
      "Qing Wang",
      "Yi-Fan Chen",
      "John Anderson",
      "Fei Sha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/daca83eba0a30a5ff2a3b9c53ff5a976-Abstract-Conference.html": {
    "title": "On the Generalization Error of Stochastic Mirror Descent for Quadratically-Bounded Losses: an Improved Analysis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ta Duy Nguyen",
      "Alina Ene",
      "Huy Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dae8bdacd265399b193e6b43d44a80f0-Abstract-Conference.html": {
    "title": "StableFDG: Style and Attention Based Learning for Federated Domain Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungwuk Park",
      "Dong-Jun Han",
      "Jinho Kim",
      "Shiqiang Wang",
      "Christopher Brinton",
      "Jaekyun Moon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/daeef96627a461ec43b7567b2930cfde-Abstract-Conference.html": {
    "title": "MG-ViT: A Multi-Granularity Method for Compact and Efficient Vision Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhang",
      "Yepeng Liu",
      "Duoqian Miao",
      "Qi Zhang",
      "Yiwei Shi",
      "Liang Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dafd116ac8c735f149558b79fd48e090-Abstract-Conference.html": {
    "title": "Recurrent Temporal Revision Graph Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhou Chen",
      "Anxiang Zeng",
      "Qingtao Yu",
      "Kerui Zhang",
      "Cao Yuanpeng",
      "Kangle Wu",
      "Guangda Huzhang",
      "Han Yu",
      "Zhiming Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/db178cd03313e23cffb8937e93f0d464-Abstract-Conference.html": {
    "title": "Recursion in Recursion: Two-Level Nested Recursion for Length Generalization with Scalability",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jishnu Ray Chowdhury",
      "Cornelia Caragea"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/db68f1c25678f72561ab7c97ce15d912-Abstract-Conference.html": {
    "title": "xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Gong",
      "Minsheng Hao",
      "Xingyi Cheng",
      "Xin Zeng",
      "Chiming Liu",
      "Jianzhu Ma",
      "Xuegong Zhang",
      "Taifeng Wang",
      "Le Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dba8fa689ede9e56cbcd4f719def38fb-Abstract-Conference.html": {
    "title": "ANPL: Towards Natural Programming with Interactive Decomposition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Huang",
      "Ziyuan Nan",
      "Xing Hu",
      "Pengwei Jin",
      "Shaohui Peng",
      "Yuanbo Wen",
      "Rui Zhang",
      "Zidong Du",
      "Qi Guo",
      "Yewen Pu",
      "Yunji Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dbb5180957513805ebeea787b8c66ac9-Abstract-Conference.html": {
    "title": "Anonymous and Copy-Robust Delegations for Liquid Democracy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Utke",
      "Ulrike Schmidt-Kraepelin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dbc8ce0fdfcd55172d73fb05dbae07fc-Abstract-Conference.html": {
    "title": "KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujun Li",
      "Peijie Dong",
      "Anggeng Li",
      "Zimian Wei",
      "Ya Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dbd6b295535e44f2b8ec0c3f1da7c509-Abstract-Conference.html": {
    "title": "Minimum-Risk Recalibration of Classifiers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Sun",
      "Dogyoon Song",
      "Alfred Hero"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dbdc7a9779ce0278c6e43b62c7e97759-Abstract-Conference.html": {
    "title": "DynPoint: Dynamic Neural Point For View Synthesis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaichen Zhou",
      "Jia-Xing Zhong",
      "Sangyun Shin",
      "Kai Lu",
      "Yiyuan Yang",
      "Andrew Markham",
      "Niki Trigoni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dbe8185809cb7032ec7ec6e365e3ed3b-Abstract-Conference.html": {
    "title": "Data-driven Optimal Filtering for Linear Systems with Unknown Noise Covariances",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahriar Talebi",
      "Amirhossein Taghvaei",
      "Mehran Mesbahi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dbeb7e621d4a554069a6a775da0f7273-Abstract-Conference.html": {
    "title": "PPi: Pretraining Brain Signal Model for Patient-independent Seizure Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhizhang Yuan",
      "Daoze Zhang",
      "YANG YANG",
      "Junru Chen",
      "Yafeng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dbf02b21d77409a2db30e56866a8ab3a-Abstract-Conference.html": {
    "title": "Unsupervised Polychromatic Neural Representation for CT Metal Artifact Reduction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Wu",
      "Lixuan Chen",
      "Ce Wang",
      "Hongjiang Wei",
      "S. Kevin Zhou",
      "Jingyi Yu",
      "Yuyao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dc192b3eeffebba21bd1d82f6752b84b-Abstract-Conference.html": {
    "title": "DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Jain",
      "Harkirat Behl",
      "Zsolt Kira",
      "Vibhav Vineet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dc1e32dd3eb381dbc71482f6a96cbf86-Abstract-Conference.html": {
    "title": "FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Yi",
      "Qi Zhang",
      "Wei Fan",
      "Hui He",
      "Liang Hu",
      "Pengyang Wang",
      "Ning An",
      "Longbing Cao",
      "Zhendong Niu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dc35c593e61f6df62db541b976d09dcf-Abstract-Conference.html": {
    "title": "Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesca Bartolucci",
      "Emmanuel de Bézenac",
      "Bogdan Raonic",
      "Roberto Molinaro",
      "Siddhartha Mishra",
      "Rima Alaifari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dc48c738d3ef8c81b6e968453a84a819-Abstract-Conference.html": {
    "title": "Unsupervised Anomaly Detection with Rejection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Perini",
      "Jesse Davis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dc6319dde4fb182b22fb902da9418566-Abstract-Conference.html": {
    "title": "4D Panoptic Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingkang Yang",
      "Jun CEN",
      "WENXUAN PENG",
      "Shuai Liu",
      "Fangzhou Hong",
      "Xiangtai Li",
      "Kaiyang Zhou",
      "Qifeng Chen",
      "Ziwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dc81297c791bb989deade65c6bd8c1d8-Abstract-Conference.html": {
    "title": "ChatGPT-Powered Hierarchical Comparisons for Image Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Ren",
      "Yiyang Su",
      "Xiaoming Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dc9544b26ad3579477e567588db18cfc-Abstract-Conference.html": {
    "title": "Module-wise Adaptive Distillation for Multimodality Foundation Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Liang",
      "Jiahui Yu",
      "Ming-Hsuan Yang",
      "Matthew Brown",
      "Yin Cui",
      "Tuo Zhao",
      "Boqing Gong",
      "Tianyi Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dc9d5dcf3e86b83e137bad367227c8ca-Abstract-Conference.html": {
    "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ida Momennejad",
      "Hosein Hasanbeig",
      "Felipe Vieira Frujeri",
      "Hiteshi Sharma",
      "Nebojsa Jojic",
      "Hamid Palangi",
      "Robert Ness",
      "Jonathan Larson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dc9e095f668044e7a0909a4ea3926beb-Abstract-Conference.html": {
    "title": "Unsupervised Image Denoising with Score Function",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Xie",
      "Mingze Yuan",
      "Bin Dong",
      "Quanzheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dca63f2650fe9e88956c1b68440b8ee9-Abstract-Conference.html": {
    "title": "Iterative Reachability Estimation for Safe Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milan Ganai",
      "Zheng Gong",
      "Chenning Yu",
      "Sylvia Herbert",
      "Sicun Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dcba6be91359358c2355cd920da3fcbd-Abstract-Conference.html": {
    "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sang Michael Xie",
      "Hieu Pham",
      "Xuanyi Dong",
      "Nan Du",
      "Hanxiao Liu",
      "Yifeng Lu",
      "Percy S. Liang",
      "Quoc V Le",
      "Tengyu Ma",
      "Adams Wei Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dcc0ac74ac8b95dc1939804acce0317d-Abstract-Conference.html": {
    "title": "Guiding The Last Layer in Federated Learning with Pre-Trained Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gwen Legate",
      "Nicolas Bernier",
      "Lucas Page-Caccia",
      "Edouard Oyallon",
      "Eugene Belilovsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dcc337bb2a4d25afefd9ab800721debb-Abstract-Conference.html": {
    "title": "Unbiased learning of deep generative models with structured discrete representations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henry C Bendekgey",
      "Gabe Hope",
      "Erik Sudderth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dd03f856fc7f2efeec8b1c796284561d-Abstract-Conference.html": {
    "title": "STEVE-1: A Generative Model for Text-to-Behavior in Minecraft",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shalev Lifshitz",
      "Keiran Paster",
      "Harris Chan",
      "Jimmy Ba",
      "Sheila McIlraith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dd058e9ec9dc012a273594d717c46ef3-Abstract-Conference.html": {
    "title": "Thrust: Adaptively Propels Large Language Models with External Knowledge",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Zhao",
      "Hongming Zhang",
      "Xiaoman Pan",
      "Wenlin Yao",
      "Dong Yu",
      "Jianshu Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dd6a47bc0aad6f34aa5e77706d90cdc4-Abstract-Conference.html": {
    "title": "OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "yifan zhang",
      "Qingsong Wen",
      "xue wang",
      "Weiqi Chen",
      "Liang Sun",
      "Zhang Zhang",
      "Liang Wang",
      "Rong Jin",
      "Tieniu Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dd9b76f050a86a3ded6135ad3556e786-Abstract-Conference.html": {
    "title": "Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Souhaib Attaiki",
      "Maks Ovsjanikov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ddbbcd937d63d5c6b935c07b1a8222ec-Abstract-Conference.html": {
    "title": "Frequency Domain-Based Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghyeok Shin",
      "Seungjae Shin",
      "Il-chul Moon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ddcf34623ca2d63823b6d40e4d980580-Abstract-Conference.html": {
    "title": "Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lisha Chen",
      "Heshan Fernando",
      "Yiming Ying",
      "Tianyi Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ddfe6bae7b869e819f842753009b94ad-Abstract-Conference.html": {
    "title": "Large Language Models are Visual Reasoning Coordinators",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangyu Chen",
      "Bo Li",
      "Sheng Shen",
      "Jingkang Yang",
      "Chunyuan Li",
      "Kurt Keutzer",
      "Trevor Darrell",
      "Ziwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/de1739eba209c682a90ec3669229ab2d-Abstract-Conference.html": {
    "title": "Boosting Adversarial Transferability by Achieving Flat Local Maxima",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijin Ge",
      "Wang Xiaosen",
      "Hongying Liu",
      "Fanhua Shang",
      "Yuanyuan Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/de2d52c5cf2bea853ef39bb2e1535dde-Abstract-Conference.html": {
    "title": "From Trainable Negative Depth to Edge Heterophily in Graphs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Yan",
      "Yuzhong Chen",
      "Huiyuan Chen",
      "Minghua Xu",
      "Mahashweta Das",
      "Hao Yang",
      "Hanghang Tong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/de7858e3e7f9f0f7b2c7bfdc86f6d928-Abstract-Conference.html": {
    "title": "GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghang Li",
      "Lianghao Xia",
      "Yong Xu",
      "Chao Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/de8bd6b2b01cfa788e63f62e5b9a99b9-Abstract-Conference.html": {
    "title": "Direct Preference-based Policy Optimization without Reward Modeling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaon An",
      "Junhyeok Lee",
      "Xingdong Zuo",
      "Norio Kosaka",
      "Kyung-Min Kim",
      "Hyun Oh Song"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dea2b4f9012686bcc1f59a62bcd28158-Abstract-Conference.html": {
    "title": "On the Identifiability and Interpretability of Gaussian Process Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawen Chen",
      "Wancen Mu",
      "Yun Li",
      "Didong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dead3d8ff3f9198e38a36a950ebbcafd-Abstract-Conference.html": {
    "title": "Enhancing Motion Deblurring in High-Speed Scenes with Spike Streams",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyan Chen",
      "Jiyuan Zhang",
      "Yajing Zheng",
      "Tiejun Huang",
      "Zhaofei Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/deb3c28192f979302c157cb653c15e90-Abstract-Conference.html": {
    "title": "Faith and Fate: Limits of Transformers on Compositionality",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nouha Dziri",
      "Ximing Lu",
      "Melanie Sclar",
      "Xiang (Lorraine) Li",
      "Liwei Jiang",
      "Bill Yuchen Lin",
      "Sean Welleck",
      "Peter West",
      "Chandra Bhagavatula",
      "Ronan Le Bras",
      "Jena Hwang",
      "Soumya Sanyal",
      "Xiang Ren",
      "Allyson Ettinger",
      "Zaid Harchaoui",
      "Yejin Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/debd0ae2083160397a22a4a8831c7230-Abstract-Conference.html": {
    "title": "Towards a fuller understanding of neurons with Clustered Compositional Explanations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biagio La Rosa",
      "Leilani Gilpin",
      "Roberto Capobianco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ded98d28f82342a39f371c013dfb3058-Abstract-Conference.html": {
    "title": "ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongzhan Huang",
      "Pan Zhou",
      "Shuicheng Yan",
      "Liang Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/deddcfbf08f57489b0088b71a00db640-Abstract-Conference.html": {
    "title": "Faster approximate subgraph counts with privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dung Nguyen",
      "Mahantesh Halappanavar",
      "Venkatesh Srinivasan",
      "Anil Vullikanti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dee254cdacbab59f17dc6a8fbdffa59f-Abstract-Conference.html": {
    "title": "Recasting Continual Learning as Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soochan Lee",
      "Jaehyeon Son",
      "Gunhee Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/def4492b32f0248a0e4d92cc46bbdaad-Abstract-Conference.html": {
    "title": "Multiply Robust Federated Estimation of Targeted Average Treatment Effects",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Larry Han",
      "Zhu Shen",
      "Jose Zubizarreta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df2df463f98abc4de7734dbd0b0dc49d-Abstract-Conference.html": {
    "title": "Learning Motion Refinement for Unsupervised Face Animation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiale Tao",
      "Shuhang Gu",
      "Wen Li",
      "Lixin Duan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df31126302921ca9351fab73923a172f-Abstract-Conference.html": {
    "title": "Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Wang",
      "Zilong Chen",
      "Guokang Wang",
      "Yafei Song",
      "Huaping Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df334022279996b07e0870a629c18857-Abstract-Conference.html": {
    "title": "Bifurcations and loss jumps in RNN training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Eisenmann",
      "Zahra Monfared",
      "Niclas Göring",
      "Daniel Durstewitz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df438caa36714f69277daa92d608dd63-Abstract-Conference.html": {
    "title": "Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aran Nayebi",
      "Rishi Rajalingham",
      "Mehrdad Jazayeri",
      "Guangyu Robert Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df4f6e43446b1ee29c5a33d32c279f83-Abstract-Conference.html": {
    "title": "Learning Visual Prior via Generative Pre-Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinheng Xie",
      "Kai Ye",
      "Yudong Li",
      "Yuexiang Li",
      "Kevin Qinghong Lin",
      "Yefeng Zheng",
      "Linlin Shen",
      "Mike Zheng Shou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df54302388bbc145aacaa1a54a4a5933-Abstract-Conference.html": {
    "title": "Operator Learning with Neural Fields: Tackling PDEs on General Geometries",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Serrano",
      "Lise Le Boudec",
      "Armand Kassaï Koupaï",
      "Thomas X Wang",
      "Yuan Yin",
      "Jean-Noël Vittaut",
      "Patrick Gallinari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df5f94d6ac6e13d830d70536cde9f0d2-Abstract-Conference.html": {
    "title": "Learning Dense Flow Field for Highly-accurate Cross-view Camera Localization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenbo Song",
      "ze xianghui",
      "Jianfeng Lu",
      "Yujiao Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df656d6ed77b565e8dcdfbf568aead0a-Abstract-Conference.html": {
    "title": "Spatially Resolved Gene Expression Prediction from Histology Images via Bi-modal Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronald Xie",
      "Kuan Pang",
      "Sai Chung",
      "Catia Perciani",
      "Sonya MacParland",
      "Bo Wang",
      "Gary Bader"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df88b275bef31ac96c85f0c4013734fc-Abstract-Conference.html": {
    "title": "Causal-structure Driven Augmentations for Text OOD Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Feder",
      "Yoav Wald",
      "Claudia Shi",
      "Suchi Saria",
      "David Blei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/df927a06a0d9f5f06d9cd4a91ce58e56-Abstract-Conference.html": {
    "title": "Adversarial Counterfactual Environment Model Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiong-Hui Chen",
      "Yang Yu",
      "Zhengmao Zhu",
      "ZhiHua Yu",
      "Chen Zhenjun",
      "Chenghe Wang",
      "Yinan Wu",
      "Rong-Jun Qin",
      "Hongqiu Wu",
      "Ruijin Ding",
      "Huang Fangsheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dfaa29ed28dfa175bcc5e2a54aa199f8-Abstract-Conference.html": {
    "title": "Finding Safe Zones of Markov Decision Processes Policies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lee Cohen",
      "Yishay Mansour",
      "Michal Moshkovitz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dfba85bc32a3cb63a96d1412062b4d8e-Abstract-Conference.html": {
    "title": "Zero-One Laws of Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sam Adam-Day",
      " Iliant",
      "Ismail Ceylan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dfc310e81992d2e4cedc09ac47eff13e-Abstract-Conference.html": {
    "title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guhao Feng",
      "Bohang Zhang",
      "Yuntian Gu",
      "Haotian Ye",
      "Di He",
      "Liwei Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dfd0bd56e8a6f82d1619f5d093d5f9ca-Abstract-Conference.html": {
    "title": "Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaskirat Singh",
      "Liang Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dfee09496a5a8b0b01d9d4c589758832-Abstract-Conference.html": {
    "title": "Distributed Personalized Empirical Risk Minimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Deng",
      "Mohammad Mahdi Kamani",
      "Pouria Mahdavinia",
      "Mehrdad Mahdavi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e0378e0c642b1d292fcb224e8d5a39b3-Abstract-Conference.html": {
    "title": "Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Jin",
      "Xuli Shen",
      "Bin Li",
      "Xiangyang Xue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e095c0a3717629aa5497601985bfcf0e-Abstract-Conference.html": {
    "title": "Enhancing Sharpness-Aware Optimization Through Variance Suppression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingcong Li",
      "Georgios Giannakis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e0982cbc81401df3430ee1ff780dc7a2-Abstract-Conference.html": {
    "title": "Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Xue",
      "Yimu Wang",
      "Yuanyu Wan",
      "Jinfeng Yi",
      "Lijun Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e0ac27bf3327c9cb99cc5f548db4f73a-Abstract-Conference.html": {
    "title": "Multinomial Logistic Regression: Asymptotic Normality on Null Covariates in High-Dimensions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Tan",
      "Pierre C Bellec"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e0af79ad53a336b4c4b4f7e2a68eb609-Abstract-Conference.html": {
    "title": "Why think step by step? Reasoning emerges from the locality of experience",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Prystawski",
      "Michael Li",
      "Noah Goodman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e0b6f389739496e363a89155c9448a8a-Abstract-Conference.html": {
    "title": "Analyzing Generalization of Neural Networks through Loss Path Kernels",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilan Chen",
      "Wei Huang",
      "Hao Wang",
      "Charlotte Loh",
      "Akash Srivastava",
      "Lam Nguyen",
      "Lily Weng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e0bc6dbcbcc957b2aeadb20c39ba7f05-Abstract-Conference.html": {
    "title": "Operation-Level Early Stopping for Robustifying Differentiable NAS",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shen Jiang",
      "Zipeng Ji",
      "Guanghui Zhu",
      "Chunfeng Yuan",
      "Yihua Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e0c256700465c158de71081b4cf5e8c3-Abstract-Conference.html": {
    "title": "Training on Foveated Images Improves Robustness to Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Shah",
      "Aqsa Kashaf",
      "Bhiksha Raj"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e0c9b65fb3e41aaa86576df3ec33ad2e-Abstract-Conference.html": {
    "title": "Label Poisoning is All You Need",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Jha",
      "Jonathan Hayase",
      "Sewoong Oh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e0da54d3dbc0107692da952358965f5f-Abstract-Conference.html": {
    "title": "Learning Trajectories are Generalization Indicators",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwen Fu",
      "Zhizheng Zhang",
      "Dacheng Yin",
      "Yan Lu",
      "Nanning Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e10a6a906ef323efaf708f76cf3c1d1e-Abstract-Conference.html": {
    "title": "CoDet: Co-occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuofan Ma",
      "Yi Jiang",
      "Xin Wen",
      "Zehuan Yuan",
      "Xiaojuan Qi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e12a3b98b67e8395f639fde4c2b03168-Abstract-Conference.html": {
    "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Rame",
      "Guillaume Couairon",
      "Corentin Dancette",
      "Jean-Baptiste Gaya",
      "Mustafa Shukor",
      "Laure Soulier",
      "Matthieu Cord"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e142fd2b70f10db2543c64bca1417de8-Abstract-Conference.html": {
    "title": "Optimal Block-wise Asymmetric Graph Construction for Graph-based Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixing Song",
      "Yifei Zhang",
      "Irwin King"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e14de1a0ebc31d9b989f5f5528c125bb-Abstract-Conference.html": {
    "title": "On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Achraf Azize",
      "Marc Jourdan",
      "Aymen Al Marjani",
      "Debabrota Basu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e150e6d0a1e5214740c39c6e4503ba7a-Abstract-Conference.html": {
    "title": "BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Ni",
      "Hang Yu",
      "Shizhan Liu",
      "Jianguo Li",
      "Weiyao Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e15790966a4a9d85d688635c88ee6d8a-Abstract-Conference.html": {
    "title": "Towards Foundation Models for Scientific Machine Learning: Characterizing Scaling and Transfer Behavior",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Subramanian",
      "Peter Harrington",
      "Kurt Keutzer",
      "Wahid Bhimji",
      "Dmitriy Morozov",
      "Michael W. Mahoney",
      "Amir Gholami"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e17e11960843febbc2dd22d3c7d79144-Abstract-Conference.html": {
    "title": "Hyperbolic Space with Hierarchical Margin Boosts Fine-Grained Learning from Coarse Labels",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shu-Lin Xu",
      "Yifan Sun",
      "Faen Zhang",
      "Anqi Xu",
      "Xiu-Shen Wei",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e187897ed7780a579a0d76fd4a35d107-Abstract-Conference.html": {
    "title": "PromptIR: Prompting for All-in-One Image Restoration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaishnav Potlapalli",
      "Syed Waqas Zamir",
      "Salman H. Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e19560e93418dd0d6498bd3b2de856cd-Abstract-Conference.html": {
    "title": "Creating a Public Repository for Joining Private Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Cook",
      "Milind Shyani",
      "Nina Mishra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e197fe307eb3467035f892dc100d570a-Abstract-Conference.html": {
    "title": "What Truly Matters in Trajectory Prediction for Autonomous Driving?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tran Phong",
      "Haoran Wu",
      "Cunjun Yu",
      "Panpan Cai",
      "Sifa Zheng",
      "David Hsu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e1b619a9e241606a23eb21767f16cf81-Abstract-Conference.html": {
    "title": "AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuancheng Wang",
      "Zeqian Ju",
      "Xu Tan",
      "Lei He",
      "Zhizheng Wu",
      "Jiang Bian",
      "sheng zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e1c73e9595126794186536cfbbed012f-Abstract-Conference.html": {
    "title": "An Optimization-based Approach To Node Role Discovery in Networks: Approximating Equitable Partitions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Scholkemper",
      "Michael T Schaub"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e1de63ec74f40d3234c4e053f3528e18-Abstract-Conference.html": {
    "title": "Robust Model Reasoning and Fitting via Dual Sparsity Pursuit",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Jiang",
      "Jiayi Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e1f418450107c4a0ddc16d008d131573-Abstract-Conference.html": {
    "title": "Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Crabbé",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e20a65c7308b7b94ed1178eebc45bf76-Abstract-Conference.html": {
    "title": "Back-Modality: Leveraging Modal Transformation for Data Augmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Li",
      "Yifan Liu",
      "Yin Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e21955c93dede886af1d0d362c756757-Abstract-Conference.html": {
    "title": "Gradient-Based Feature Learning under Structured Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Mousavi-Hosseini",
      "Denny Wu",
      "Taiji Suzuki",
      "Murat A. Erdogdu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e21a7b668ce3ea2c9c964c52d1c9f161-Abstract-Conference.html": {
    "title": "Does Invariant Graph Learning via Environment Augmentation Learn Invariance?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqiang Chen",
      "Yatao Bian",
      "Kaiwen Zhou",
      "Binghui Xie",
      "Bo Han",
      "James Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e24570da4fa1c005b189104250993aee-Abstract-Conference.html": {
    "title": "Mitigating Test-Time Bias for Fair Image Retrieval",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanjie Kong",
      "Shuai Yuan",
      "Weituo Hao",
      "Ricardo Henao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e258bb98cc032ab6ae9053db453431f7-Abstract-Conference.html": {
    "title": "Lower Bounds on Adaptive Sensing for Matrix Recovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Praneeth Kacham",
      "David Woodruff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e261e92e1cfb820da930ad8c38d0aead-Abstract-Conference.html": {
    "title": "Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anagh Malik",
      "Parsa Mirdehghan",
      "Sotiris Nousias",
      "Kyros Kutulakos",
      "David Lindell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e262fc23ec7275230ee77c55d0cc9555-Abstract-Conference.html": {
    "title": "An Exploration-by-Optimization Approach to Best of Both Worlds in Linear Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shinji Ito",
      "Kei Takemura"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e26f31de8b13ec569bf507e6ae2cd952-Abstract-Conference.html": {
    "title": "The expressive power of pooling in Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Filippo Maria Bianchi",
      "Veronica Lachi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e271e30de7a2e462ca1f85cefa816380-Abstract-Conference.html": {
    "title": "Cal-DETR: Calibrated Detection Transformer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Akhtar Munir",
      "Salman H. Khan",
      "Muhammad Haris Khan",
      "Mohsen Ali",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e2a9256bd816ab9e082dfaa22f1f62a2-Abstract-Conference.html": {
    "title": "Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minhak Song",
      "Chulhee Yun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e304d374c85e385eb217ed4a025b6b63-Abstract-Conference.html": {
    "title": "ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Teney",
      "Yong Lin",
      "Seong Joon Oh",
      "Ehsan Abbasnejad"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e30bf4765ae6b16a87fb4d7b0b3b3dec-Abstract-Conference.html": {
    "title": "On Generalization Bounds for Projective Clustering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Sofia Bucarelli",
      "Matilde Larsen",
      "Chris Schwiegelshohn",
      "Mads Toftrup"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e31c16c7b3e0ccee5159ae5443154fac-Abstract-Conference.html": {
    "title": "Emergence of Shape Bias in Convolutional Neural Networks through Activation Sparsity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianqin Li",
      "Ziqi Wen",
      "Yangfan Li",
      "Tai Sing Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e32349fe7e3cd4f9ef598c2b7b7a31f4-Abstract-Conference.html": {
    "title": "Resilient Constrained Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ignacio Hounie",
      "Alejandro Ribeiro",
      "Luiz F. O. Chamon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e33a4d41305fb34316df6f3fa8a0e58c-Abstract-Conference.html": {
    "title": "Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Kümmerle",
      "Johannes Maly"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e34d908241aef40440e61d2a27715424-Abstract-Conference.html": {
    "title": "Error Bounds for Learning with Vector-Valued Random Features",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Lanthaler",
      "Nicholas H. Nelsen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e352b765e625934ce86919995e2371aa-Abstract-Conference.html": {
    "title": "CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Cao",
      "Zeng Yihan",
      "Hang Xu",
      "Dan Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e35460304fdf6df523f068a59aaf8829-Abstract-Conference.html": {
    "title": "Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aahlad Manas Puli",
      "Lily Zhang",
      "Yoav Wald",
      "Rajesh Ranganath"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e359ebe56ba306b674e8952349c6049e-Abstract-Conference.html": {
    "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuandong Tian",
      "Yiping Wang",
      "Beidi Chen",
      "Simon S. Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e389b15166cf98966ba058965a8c17e3-Abstract-Conference.html": {
    "title": "Stein $\\Pi$-Importance Sampling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Congye Wang",
      "Ye Chen",
      "Heishiro Kanagawa",
      "Chris J. Oates"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e393677793767624f2821cec8bdd02f1-Abstract-Conference.html": {
    "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yang",
      "Lin Song",
      "Yanwei Li",
      "Sijie Zhao",
      "Yixiao Ge",
      "Xiu Li",
      "Ying Shan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e3bf2f0f10774c474de22a12cb060e2c-Abstract-Conference.html": {
    "title": "Reinforcement Learning with Fast and Forgetful Memory",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Morad",
      "Ryan Kortvelesy",
      "Stephan Liwicki",
      "Amanda Prorok"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e3cdc587873dd1d00ac78f0c1f9aa60c-Abstract-Conference.html": {
    "title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taylor Webb",
      "Shanka Subhra Mondal",
      "Jonathan D Cohen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e3fe7b34ba4f378df39cb12a97193f41-Abstract-Conference.html": {
    "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonard Salewski",
      "Stephan Alaniz",
      "Isabel Rio-Torto",
      "Eric Schulz",
      "Zeynep Akata"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e3fea99df80195b316cefa7aa6099cd5-Abstract-Conference.html": {
    "title": "The s-value: evaluating stability with respect to distributional shifts",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyash Gupta",
      "Dominik Rothenhäusler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e4165c96702bac5f4962b70f3cf2f136-Abstract-Conference.html": {
    "title": "When Does Optimizing a Proper Loss Yield Calibration?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaroslaw Blasiok",
      "Parikshit Gopalan",
      "Lunjia Hu",
      "Preetum Nakkiran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e425b75bac5742a008d643826428787c-Abstract-Conference.html": {
    "title": "Language Is Not All You Need: Aligning Perception with Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohan Huang",
      "Li Dong",
      "Wenhui Wang",
      "Yaru Hao",
      "Saksham Singhal",
      "Shuming Ma",
      "Tengchao Lv",
      "Lei Cui",
      "Owais Khan Mohammed",
      "Barun Patra",
      "Qiang Liu",
      "Kriti Aggarwal",
      "Zewen Chi",
      "Nils Bjorck",
      "Vishrav Chaudhary",
      "Subhojit  Som",
      "XIA SONG",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e43f900f571de6c96a70d5724a0fb565-Abstract-Conference.html": {
    "title": "Out-of-distribution Detection Learning with Unreliable Out-of-distribution Sources",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Zheng",
      "Qizhou Wang",
      "Zhen Fang",
      "Xiaobo Xia",
      "Feng Liu",
      "Tongliang Liu",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e444859b2a22df6b56af9381ad1e9480-Abstract-Conference.html": {
    "title": "Robust covariance estimation with missing values and cell-wise contamination",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grégoire Pacreau",
      "Karim Lounici"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e4667dd0a5a54b74019b72b677ed8ec1-Abstract-Conference.html": {
    "title": "Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhendong Wang",
      "Yifan Jiang",
      "Huangjie Zheng",
      "Peihao Wang",
      "Pengcheng He",
      "Zhangyang \"Atlas\" Wang",
      "Weizhu Chen",
      "Mingyuan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e468a76212a58c1af94a3d235151944a-Abstract-Conference.html": {
    "title": "Clustering the Sketch: Dynamic Compression for Embedding Tables",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henry Tsang",
      "Thomas Ahle"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e4724af0e2a0d52ce5a0a4e084b87f59-Abstract-Conference.html": {
    "title": "Dynamic Personalized Federated Learning with Adaptive Differential Privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyuan Yang",
      "Wenke Huang",
      "Mang Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e4748b6b6ca49f04b6a8cfce1d5f9a70-Abstract-Conference.html": {
    "title": "Bias in Evaluation Processes: An Optimization-Based Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "L. Elisa Celis",
      "Amit Kumar",
      "Anay Mehrotra",
      "Nisheeth K. Vishnoi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e48880ea81caa7836e6a0694049093ae-Abstract-Conference.html": {
    "title": "Data Minimization at Inference Time",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cuong Tran",
      "Nando Fioretto"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e4be7e9867ef163563f4a5e90cec478f-Abstract-Conference.html": {
    "title": "Learning Adaptive Tensorial Density Fields for Clean Cryo-ET Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Wang",
      "Ramzi Idoughi",
      "Wolfgang Heidrich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e4bf5c3245fd92a4554a16af9803b757-Abstract-Conference.html": {
    "title": "Resetting the Optimizer in Deep RL: An Empirical Study",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kavosh Asadi",
      "Rasool Fakoor",
      "Shoham Sabach"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e4d3fe32495088805bbbb4f1de63e947-Abstract-Conference.html": {
    "title": "Why Does Sharpness-Aware Minimization Generalize Better Than SGD?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixiang Chen",
      "Junkai Zhang",
      "Yiwen Kou",
      "Xiangning Chen",
      "Cho-Jui Hsieh",
      "Quanquan Gu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e50e253e21cbcdcd200394f61d73acc8-Abstract-Conference.html": {
    "title": "Grassmann Manifold Flows for Stable Shape Generation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryoma Yataka",
      "Kazuki Hirashima",
      "Masashi Shiraishi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e5440ffceaf4831b5f98652b8a27ffde-Abstract-Conference.html": {
    "title": "Marich: A Query-efficient Distributionally Equivalent Model Extraction Attack",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratik Karmakar",
      "Debabrota Basu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e55c2f3fdde519014c879aa3554414c0-Abstract-Conference.html": {
    "title": "Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Fang",
      "Wei Liu",
      "Yuan Gao",
      "Zemin Liu",
      "An Zhang",
      "Xiang Wang",
      "Xiangnan He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e55edcdb01ac45c839a602f96e09fbcb-Abstract-Conference.html": {
    "title": "A Unifying Perspective on Multi-Calibration: Game Dynamics for Multi-Objective Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nika Haghtalab",
      "Michael Jordan",
      "Eric Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e560202b6e779a82478edb46c6f8f4dd-Abstract-Conference.html": {
    "title": "Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Marconato",
      "Stefano Teso",
      "Antonio Vergari",
      "Andrea Passerini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e5a71ba556c84fef542aaace56b6cfe9-Abstract-Conference.html": {
    "title": "Contrastive Moments: Unsupervised Halfspace Learning in Polynomial Time",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyuan Cao",
      "Santosh Vempala"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e5aa7171449b83f8b4eec1623eac9906-Abstract-Conference.html": {
    "title": "Boosting Spectral Clustering on Incomplete Data via Kernel Correction and Affinity Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangchen Yu",
      "Runze Zhao",
      "Zhan Shi",
      "Yiwen Lu",
      "Jicong Fan",
      "Yicheng Zeng",
      "Jianfeng Mao",
      "Wenye Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e5b1c0d4866f72393c522c8a00eed4eb-Abstract-Conference.html": {
    "title": "DASpeech: Directed Acyclic Transformer for Fast and High-quality Speech-to-Speech Translation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingkai Fang",
      "Yan Zhou",
      "Yang Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e5b5c402bb7bd5e60bede6961d6fe39e-Abstract-Conference.html": {
    "title": "Learning Large-scale Neural Fields via Context Pruned Meta-Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihoon Tack",
      "Subin Kim",
      "Sihyun Yu",
      "Jaeho Lee",
      "Jinwoo Shin",
      "Jonathan Richard Schwarz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e5b6eb1dbabff82838d5e99f62de37c8-Abstract-Conference.html": {
    "title": "AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiki E. Matsunaga",
      "Jongmin Lee",
      "Jaeseok Yoon",
      "Stefanos Leonardos",
      "Pieter Abbeel",
      "Kee-Eung Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e5beb17e56bbb8fd562efeefab79425f-Abstract-Conference.html": {
    "title": "Approximate inference of marginals using the IBIA framework",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivani Bathla",
      "Vinita Vasudevan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e5dc475c370ff42f2f96dddf8191a40c-Abstract-Conference.html": {
    "title": "HiNeRV: Video Compression with Hierarchical Encoding-based Neural Representation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ho Man Kwan",
      "Ge Gao",
      "Fan Zhang",
      "Andrew Gower",
      "David Bull"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e5eaf67f3405be58cd12848a89cd8ace-Abstract-Conference.html": {
    "title": "Bicriteria Approximation Algorithms for the Submodular Cover Problem",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjing Chen",
      "Victoria Crawford"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6057bf047bcc5f86ebf4e8db6e24a1f-Abstract-Conference.html": {
    "title": "Responsible AI (RAI) Games and Ensembles",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Gupta",
      "Runtian Zhai",
      "Arun Suggala",
      "Pradeep Ravikumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e637029c42aa593850eeebf46616444d-Abstract-Conference.html": {
    "title": "May the Force be with You: Unified Force-Centric Pre-Training for 3D Molecular Conformations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Feng",
      "Qi Zhu",
      "Huan Tran",
      "Binghong Chen",
      "Aubrey Toland",
      "Rampi Ramprasad",
      "Chao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e66309ead63bc1410d2df261a28f602d-Abstract-Conference.html": {
    "title": "Deep Fractional Fourier Transform",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hu Yu",
      "Jie Huang",
      "Lingzhi LI",
      "man zhou",
      "Feng Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e664650506f1cf2b4696df892147c06e-Abstract-Conference.html": {
    "title": "Survival Permanental Processes for Survival Analysis with Time-Varying Covariates",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hideaki Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6789e468c65a7816760a00a487d3c4e-Abstract-Conference.html": {
    "title": "Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Rastegar",
      "Hazel Doughty",
      "Cees Snoek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e69a9560c450ca76584d9eb37e7f5ae8-Abstract-Conference.html": {
    "title": "Training Chain-of-Thought via Latent-Variable Inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Douglas Hoffman",
      "Du Phan",
      "David Dohan",
      "Sholto Douglas",
      "Tuan Anh Le",
      "Aaron Parisi",
      "Pavel Sountsov",
      "Charles Sutton",
      "Sharad Vikram",
      "Rif A. Saurous"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6b2b48b5ed90d07c305932729927781-Abstract-Conference.html": {
    "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihan Chen",
      "Handong Li",
      "Qunbo Wang",
      "Zijia Zhao",
      "Mingzhen Sun",
      "Xinxin Zhu",
      "Jing Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6bfdd58f1326ff821a1b92743963bdf-Abstract-Conference.html": {
    "title": "Bayesian Learning via Q-Exponential Process",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyi Li",
      "Michael O'Connor",
      "Shiwei Lan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6c2e85db1f1039177c4495ccd399ac4-Abstract-Conference.html": {
    "title": "Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huayang Li",
      "Tian Lan",
      "Zihao Fu",
      "Deng Cai",
      "Lemao Liu",
      "Nigel Collier",
      "Taro Watanabe",
      "Yixuan Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6c65eb9b56719c1aa45ff73874de317-Abstract-Conference.html": {
    "title": "Facing Off World Model Backbones: RNNs, Transformers, and S4",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Deng",
      "Junyeong Park",
      "Sungjin Ahn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6d37cc5723e810b793c834bcb6647cf-Abstract-Conference.html": {
    "title": "Inserting Anybody in Diffusion Models via Celeb Basis",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Yuan",
      "Xiaodong Cun",
      "Yong Zhang",
      "Maomao Li",
      "Chenyang Qi",
      "Xintao Wang",
      "Ying Shan",
      "Huicheng Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6d58fc68c0f3c36ae6e0e64478a69c0-Abstract-Conference.html": {
    "title": "Scaling Open-Vocabulary Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias Minderer",
      "Alexey Gritsenko",
      "Neil Houlsby"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6e706454d72c18582b9c1ff70b11f7d-Abstract-Conference.html": {
    "title": "Formulating Discrete Probability Flow Through Optimal Transport",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengze Zhang",
      "Hubery Yin",
      "Chen Li",
      "Xiaohua Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6f2b968c4ee8ba260cd7077e39590dd-Abstract-Conference.html": {
    "title": "Successor-Predecessor Intrinsic Exploration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changmin Yu",
      "Neil Burgess",
      "Maneesh Sahani",
      "Samuel J Gershman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e71a42c64851834013e2658b69d7fe93-Abstract-Conference.html": {
    "title": "TFLEX: Temporal Feature-Logic Embedding Framework for Complex Reasoning over Temporal Knowledge Graph",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyuan Lin",
      "Haihong E",
      "Chengjin Xu",
      "Gengxian Zhou",
      "Haoran Luo",
      "Tianyi Hu",
      "Fenglong Su",
      "Ningyuan Li",
      "Mingzhi Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e7407ab5e89c405d28ff6807ffec594a-Abstract-Conference.html": {
    "title": "StyleGAN knows Normal, Depth, Albedo, and More",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anand Bhattad",
      "Daniel McKee",
      "Derek Hoiem",
      "David Forsyth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e75dce944052276caf89c17aca8963d3-Abstract-Conference.html": {
    "title": "Are Vision Transformers More Data Hungry Than Newborn Visual Systems?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lalit Pandey",
      "Samantha Wood",
      "Justin Wood"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e7681dd6fe16052433ab68cd1555bdc9-Abstract-Conference.html": {
    "title": "How to Scale Your EMA",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Busbridge",
      "Jason Ramapuram",
      "Pierre Ablin",
      "Tatiana Likhomanenko",
      "Eeshan Gunesh Dhekane",
      "Xavier Suau Cuadros",
      "Russell Webb"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e78399fc43dbb2d87b7e1e6906ce5baf-Abstract-Conference.html": {
    "title": "Unsupervised Graph Neural Architecture Search with Disentangled Self-Supervision",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyang Zhang",
      "Xin Wang",
      "Ziwei Zhang",
      "Guangyao Shen",
      "Shiqi Shen",
      "Wenwu Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e7938ede51225b490bb69f7b361a9259-Abstract-Conference.html": {
    "title": "Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruixiang (Ryan) Tang",
      "Jiayi Yuan",
      "Yiming Li",
      "Zirui Liu",
      "Rui Chen",
      "Xia Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e7e506bc5a94768243083216fe51d98b-Abstract-Conference.html": {
    "title": "Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiwen WANG",
      "Jiaxi Ying",
      "Daniel Palomar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e7fd2c0a1a6f956c94024e955b34cc43-Abstract-Conference.html": {
    "title": "Collaborative Score Distillation for Consistent Visual Editing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subin Kim",
      "Kyungmin Lee",
      "June Suk Choi",
      "Jongheon Jeong",
      "Kihyuk Sohn",
      "Jinwoo Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e7feb9dbd9a94b6c552fc403fcebf2ef-Abstract-Conference.html": {
    "title": "FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Irene Wang",
      "Prashant Nair",
      "Divya Mahajan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e812af67a942c21dd0104bd929f99da1-Abstract-Conference.html": {
    "title": "Learning to Augment Distributions for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhou Wang",
      "Zhen Fang",
      "Yonggang Zhang",
      "Feng Liu",
      "Yixuan Li",
      "Bo Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e82ef7865f29b40640f486bbbe7959a7-Abstract-Conference.html": {
    "title": "Covariance-adaptive best arm identification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "El Mehdi Saad",
      "Gilles Blanchard",
      "Nicolas Verzelen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e85454a113e8b41e017c81875ae68d47-Abstract-Conference.html": {
    "title": "Swarm Reinforcement Learning for Adaptive Mesh Refinement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niklas Freymuth",
      "Philipp Dahlinger",
      "Tobias Würth",
      "Simon Reisch",
      "Luise Kärger",
      "Gerhard Neumann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e878c8f38381d0964677fb9536c494ee-Abstract-Conference.html": {
    "title": "Fast Projected Newton-like Method for Precision Matrix Estimation under Total Positivity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian-Feng CAI",
      "José Vinícius de Miranda Cardoso ",
      "Daniel Palomar",
      "Jiaxi Ying"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e885e5bc6e13b9dd8f80bc5482b1fa2f-Abstract-Conference.html": {
    "title": "BanditPAM++: Faster $k$-medoids Clustering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mo Tiwari",
      "Ryan Kang",
      "Donghyun Lee",
      "Sebastian Thrun",
      "Ilan Shomorony",
      "Martin J. Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e8ad87f1076fb0f75d89a45828f186b0-Abstract-Conference.html": {
    "title": "Cross-Domain Policy Adaptation via Value-Guided Data Filtering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Xu",
      "Chenjia Bai",
      "Xiaoteng Ma",
      "Dong Wang",
      "Bin Zhao",
      "Zhen Wang",
      "Xuelong Li",
      "Wei Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e8b0c97b34fdaf58b2f48f8cca85e76a-Abstract-Conference.html": {
    "title": "Connecting Certified and Adversarial Training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Mao",
      "Mark Müller",
      "Marc Fischer",
      "Martin Vechev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e8da56eb93676e8f60ed2b696e44e7dc-Abstract-Conference.html": {
    "title": "Effectively Learning Initiation Sets in Hierarchical Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akhil Bagaria",
      "Ben Abbatematteo",
      "Omer Gottesman",
      "Matt Corsaro",
      "Sreehari Rammohan",
      "George Konidaris"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e8ddc03b001d4c4b44b29bc1167e7fdd-Abstract-Conference.html": {
    "title": "Alignment with human representations supports robust few-shot learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilia Sucholutsky",
      "Tom Griffiths"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e8e30fda5ab87ea93360a36288ac0145-Abstract-Conference.html": {
    "title": "ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyang Sun",
      "WEIJUN WANG",
      "Andrew Howard",
      "Qihang Yu",
      "Philip Torr",
      "Liang-Chieh Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e8f4eae0a41cab67fdead3aa6b77f083-Abstract-Conference.html": {
    "title": "The Behavior and Convergence of Local Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Wu",
      "Kyurae Kim",
      "Roman Garnett",
      "Jacob Gardner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e8ff788779f2e9e74ccd0d6b84607437-Abstract-Conference.html": {
    "title": "Contrastive Sampling Chains in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Zhang",
      "Daochang Liu",
      "Shichao Zhang",
      "Chang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e9000ecb86d45c442a1d38fae68dd8fb-Abstract-Conference.html": {
    "title": "Effective Robustness against Natural Distribution Shifts for Models with Different Training Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhouxing Shi",
      "Nicholas Carlini",
      "Ananth Balashankar",
      "Ludwig Schmidt",
      "Cho-Jui Hsieh",
      "Alex Beutel",
      "Yao Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e90ba1fc564a69809d7391bf76a5f087-Abstract-Conference.html": {
    "title": "Tailoring Self-Attention for Graph via Rooted Subtrees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Huang",
      "Yunchong Song",
      "Jiayue Zhou",
      "Zhouhan Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e91fb65c6324a984ea9ef39a5b84af04-Abstract-Conference.html": {
    "title": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyuan Yin",
      "Eric Xing",
      "Zhiqiang Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e95da8078ec8389533c802e368da5298-Abstract-Conference.html": {
    "title": "Disentangled Wasserstein Autoencoder for T-Cell Receptor Engineering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxiao Li",
      "Hongyu Guo",
      "Filippo Grazioli",
      "Mark Gerstein",
      "Martin Renqiang Min"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e95e9f0c127aa1cfa2628adb2f3cb107-Abstract-Conference.html": {
    "title": "Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event Parser",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yung-Hsuan Lai",
      "Yen-Chun Chen",
      "Frank Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e95eb5206c867be843fbc14bbfe8c10e-Abstract-Conference.html": {
    "title": "Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Zhang",
      "Tianfei Zhou",
      "Boyang Li",
      "Hao He",
      "Chaofan Ma",
      "Tianjiao Zhang",
      "Jiangchao Yao",
      "Ya Zhang",
      "Yanfeng Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e9721921b799b6ea98d37f9e77f1a7fe-Abstract-Conference.html": {
    "title": "A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhong",
      "Tong Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e97ac22927560eb2de6b658498cbc575-Abstract-Conference.html": {
    "title": "Uncertainty-Aware Instance Reweighting for Off-Policy Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoying Zhang",
      "Junpu Chen",
      "Hongning Wang",
      "Hong Xie",
      "Yang Liu",
      "John C.S. Lui",
      "Hang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e985dfca10e1167c0836a70880ef0858-Abstract-Conference.html": {
    "title": "Model-free Posterior Sampling via Learning Rate Randomization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Tiapkin",
      "Denis Belomestny",
      "Daniele Calandriello",
      "Eric Moulines",
      "Remi Munos",
      "Alexey Naumov",
      "Pierre Perrault",
      "Michal Valko",
      "Pierre Ménard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e991e5587c1daa49bbf9a818b3f02f9a-Abstract-Conference.html": {
    "title": "TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Preetha Vijayan",
      "Prashant Bhat",
      "Bahram Zonooz",
      "Elahe Arani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e99be8b1f637996eaf1154f2f4cb6f49-Abstract-Conference.html": {
    "title": "Implicit Variational Inference for High-Dimensional Posteriors",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anshuk Uppal",
      "Kristoffer Stensbo-Smidt",
      "Wouter Boomsma",
      "Jes Frellsen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e9a612969b4df241ff0d8273656bd5a4-Abstract-Conference.html": {
    "title": "k-Median Clustering via Metric Embedding: Towards Better Initialization with Differential Privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenglin Fan",
      "Ping Li",
      "Xiaoyun Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e9b0ae84d6879b30c78cb8537466a4e0-Abstract-Conference.html": {
    "title": "Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingdong Feng",
      "Xin HE",
      "Caixing Wang",
      "Chao Wang",
      "Jingnan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e9b8a3362a6d9a7f9f842bd2d919e1a0-Abstract-Conference.html": {
    "title": "Learning Functional Transduction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathieu Chalvidal",
      "Thomas Serre",
      "Rufin VanRullen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e9df36b21ff4ee211a8b71ee8b7e9f57-Abstract-Conference.html": {
    "title": "Gaussian Membership Inference Privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Leemann",
      "Martin Pawelczyk",
      "Gjergji Kasneci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e9df55bf67e499635908395931ed6ea9-Abstract-Conference.html": {
    "title": "Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiwon Jang",
      "Jihoon Tack",
      "Daewon Choi",
      "Jongheon Jeong",
      "Jinwoo Shin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e9e140df6de01afb672cb859d203c307-Abstract-Conference.html": {
    "title": "Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative MARL",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Xu",
      "Bin Zhang",
      "dapeng li",
      "Guangchong Zhou",
      "Zeren Zhang",
      "Guoliang Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ea0b28cbbd0cbc45ec4ac38e92da9cb2-Abstract-Conference.html": {
    "title": "DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mintong Kang",
      "Dawn Song",
      "Bo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ea13534ee239bb3977795b8cc855bacc-Abstract-Conference.html": {
    "title": "Squared Neural Families: A New Class of Tractable Density Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Russell Tsuchida",
      "Cheng Soon Ong",
      "Dino Sejdinovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ea1a7f7bc0fc14142106a84c94c826d0-Abstract-Conference.html": {
    "title": "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zibo Zhao",
      "Wen Liu",
      "Xin Chen",
      "Xianfang Zeng",
      "Rui Wang",
      "Pei Cheng",
      "BIN FU",
      "Tao Chen",
      "Gang Yu",
      "Shenghua Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ea5cb7d9fd2deb0554def3552962d276-Abstract-Conference.html": {
    "title": "Estimating Riemannian Metric with Noise-Contaminated Intrinsic Distance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Qiu",
      "Xiongtao Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ea7623ff02edffe68866f88da2667592-Abstract-Conference.html": {
    "title": "Inner Product-based Neural Network Similarity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Zichen Miao",
      "Qiang Qiu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ea8608c6258450e75b3443ec8022fb2e-Abstract-Conference.html": {
    "title": "State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shida Wang",
      "Beichen Xue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ea94957d81b1c1caf87ef5319fa6b467-Abstract-Conference.html": {
    "title": "Many-body Approximation for Non-negative Tensors",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "KAZU GHALAMKARI",
      "Mahito Sugiyama",
      "Yoshinobu Kawahara"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ead13878cd158f013becb6a559a60364-Abstract-Conference.html": {
    "title": "Breaking the Communication-Privacy-Accuracy Tradeoff with $f$-Differential Privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richeng Jin",
      "Zhonggen Su",
      "caijun zhong",
      "Zhaoyang Zhang",
      "Tony Quek",
      "Huaiyu Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eadeef7c51ad86989cc3b311cb49ec89-Abstract-Conference.html": {
    "title": "Multi-Prompt Alignment for Multi-Source Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Chen",
      "Xintong Han",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eaef3b49866b942041a34bb8da397eb7-Abstract-Conference.html": {
    "title": "Characterization and Learning of Causal Graphs with Small Conditioning Sets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Murat Kocaoglu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eaf5d2cdb582c058a078d4fdf52a20f9-Abstract-Conference.html": {
    "title": "Finite Population Regression Adjustment and Non-asymptotic Guarantees for Treatment Effect Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehrdad Ghadiri",
      "David Arbour",
      "Tung Mai",
      "Cameron Musco",
      "Anup B. Rao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb0965da1d2cb3fbbbb8dbbad5fa0bfc-Abstract-Conference.html": {
    "title": "P-Flow: A Fast and Data-Efficient Zero-Shot TTS through Speech Prompting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungwon Kim",
      "Kevin Shih",
      "rohan badlani",
      "Joao Felipe Santos",
      "Evelina Bakhturina",
      "Mikyas Desta",
      "Rafael Valle",
      "Sungroh Yoon",
      "Bryan Catanzaro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb189151ced0ff808abafd16a51fec92-Abstract-Conference.html": {
    "title": "Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfeng Wu",
      "Vladimir Braverman",
      "Jason D. Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb1a323fa10d4102ff13422476a744ff-Abstract-Conference.html": {
    "title": "Two Sides of One Coin: the Limits of Untuned SGD and the Power of Adaptive Methods",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junchi YANG",
      "Xiang Li",
      "Ilyas Fatkhullin",
      "Niao He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb1bad7a84ef68a64f1afd6577725d45-Abstract-Conference.html": {
    "title": "Theoretical Analysis of the Inductive Biases in Deep Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Wang",
      "Lei Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb3c42ddfa16d8421fdba13528107cc1-Abstract-Conference.html": {
    "title": "Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Dooley",
      "Rhea Sukthanker",
      "John Dickerson",
      "Colin White",
      "Frank Hutter",
      "Micah Goldblum"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb4b1f7feadcd124a59de6ff7b9196f3-Abstract-Conference.html": {
    "title": "H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanjie Ze",
      "Yuyao Liu",
      "Ruizhe Shi",
      "Jiaxin Qin",
      "Zhecheng Yuan",
      "Jiashun Wang",
      "Huazhe Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb5254c4ee813d05af9c098f2d9c5708-Abstract-Conference.html": {
    "title": "DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lazar Atanackovic",
      "Alexander Tong",
      "Bo Wang",
      "Leo J Lee",
      "Yoshua Bengio",
      "Jason S. Hartford"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb5d9195b201ec7ba66c8e20b396d349-Abstract-Conference.html": {
    "title": "AutoGO: Automated Computation Graph Optimization for Neural Network Evolution",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Salameh",
      "Keith Mills",
      "Negar Hassanpour",
      "Fred Han",
      "Shuting Zhang",
      "Wei Lu",
      "Shangling Jui",
      "CHUNHUA ZHOU",
      "Fengyu Sun",
      "Di Niu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ebb4c188fafe7da089b41a9f615ad84d-Abstract-Conference.html": {
    "title": "Where Did I Come From? Origin Attribution of AI-Generated Images",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenting Wang",
      "Chen Chen",
      "Yi Zeng",
      "Lingjuan Lyu",
      "Shiqing Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ebb6bee50913ba7e1efeb91a1d47a002-Abstract-Conference.html": {
    "title": "Robust Data Pruning under Label Noise via Maximizing Re-labeling Accuracy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongmin Park",
      "Seola Choi",
      "Doyoung Kim",
      "Hwanjun Song",
      "Jae-Gil Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ebd82705f44793b6f9ade5a669d0f0bf-Abstract-Conference.html": {
    "title": "Augmenting Language Models with Long-Term Memory",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi Wang",
      "Li Dong",
      "Hao Cheng",
      "Xiaodong Liu",
      "Xifeng Yan",
      "Jianfeng Gao",
      "Furu Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ebf95a6f3c575322da15d4fd0fc2b3c8-Abstract-Conference.html": {
    "title": "Expressivity-Preserving GNN Simulation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabian Jogl",
      "Maximilian Thiessen",
      "Thomas Gärtner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ebfabf372037aaa4a8d92c9b457ece3e-Abstract-Conference.html": {
    "title": "Rethinking Incentives in Recommender Systems: Are Monotone Rewards Always Beneficial?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Yao",
      "Chuanhao Li",
      "Karthik Abinav Sankararaman",
      "Yiming Liao",
      "Yan Zhu",
      "Qifan Wang",
      "Hongning Wang",
      "Haifeng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ec0bff8bf4b11e36f874790046dfdb65-Abstract-Conference.html": {
    "title": "Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Praveen Venkatesh",
      "Corbett Bennett",
      "Sam Gale",
      "Tamina Ramirez",
      "Greggory Heller",
      "Severine Durand",
      "Shawn Olsen",
      "Stefan Mihalas"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ec2833cda146c277cdaa39066764f25c-Abstract-Conference.html": {
    "title": "Unconstrained Dynamic Regret via Sparse Coding",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Zhang",
      "Ashok Cutkosky",
      "Yannis Paschalidis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ec3d49763c653ad7c8d587f52220c129-Abstract-Conference.html": {
    "title": "Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeshuai Deng",
      "Zhuokun Chen",
      "Shuaicheng Niu",
      "Thomas Li",
      "Bohan Zhuang",
      "Mingkui Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ec4d2e436794d1bf55ca83f5ebb31887-Abstract-Conference.html": {
    "title": "Replicability in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Karbasi",
      "Grigoris Velegkas",
      "Lin Yang",
      "Felix Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ec4f0b0a7557d6a51c42308800f2c23a-Abstract-Conference.html": {
    "title": "Probabilistic Invariant Learning with Randomized Linear Classifiers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonardo Cotta",
      "Gal Yehuda",
      "Assaf Schuster",
      "Chris J. Maddison"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ec52572b9e16b91edff5dc70e2642240-Abstract-Conference.html": {
    "title": "FedNAR: Federated Optimization with Normalized Annealing Regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junbo Li",
      "Ang Li",
      "Chong Tian",
      "Qirong Ho",
      "Eric Xing",
      "Hongyi Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ec702dd6e83b2113a43614685a7e2ac6-Abstract-Conference.html": {
    "title": "Trial matching: capturing variability with data-constrained spiking neural networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christos Sourmpis",
      "Carl Petersen",
      "Wulfram Gerstner",
      "Guillaume Bellec"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html": {
    "title": "Decentralized Randomly Distributed Multi-agent Multi-armed Bandit with Heterogeneous Rewards",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengfan Xu",
      "Diego Klabjan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ecc28b4ce9b39f5f23c3efb03e25b7bf-Abstract-Conference.html": {
    "title": "(Amplified) Banded Matrix Factorization: A unified approach to private training",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher A. Choquette-Choo",
      "Arun Ganesh",
      "Ryan McKenna",
      "H. Brendan McMahan",
      "John Rush",
      "Abhradeep Guha Thakurta",
      "Zheng Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ecc9b6dfdbe374c0a3364ff81cd28642-Abstract-Conference.html": {
    "title": "Anchor Data Augmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nora Schneider",
      "Shirin Goshtasbpour",
      "Fernando Perez-Cruz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ecffd829f90b0a4b6aa017b6df15904f-Abstract-Conference.html": {
    "title": "The noise level in linear regression with dependent data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ingvar Ziemann",
      "Stephen Tu",
      "George J. Pappas",
      "Nikolai Matni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ed2fb79f2664c3d9ba878be7e575b2af-Abstract-Conference.html": {
    "title": "SafeDICE: Offline Safe Imitation Learning with Non-Preferred Demonstrations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngsoo Jang",
      "Geon-Hyeong Kim",
      "Jongmin Lee",
      "Sungryull Sohn",
      "Byoungjip Kim",
      "Honglak Lee",
      "Moontae Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ed3fea9033a80fea1376299fa7863f4a-Abstract-Conference.html": {
    "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miles Turpin",
      "Julian Michael",
      "Ethan Perez",
      "Samuel Bowman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ed4bacc8c7ca1ee0e1d4e0ef376b7ac7-Abstract-Conference.html": {
    "title": "Static and Sequential Malicious Attacks in the Context of Selective Forgetting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxu Zhao",
      "Wei Qian",
      "Rex Ying",
      "Mengdi Huai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ed67dff7cb96e7e86c4d91c0d5db49bb-Abstract-Conference.html": {
    "title": "Language-based Action Concept Spaces Improve Video Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanchana Ranasinghe",
      "Michael S Ryoo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ed687a5f52b651b19e7c18f702907b8b-Abstract-Conference.html": {
    "title": "TRIAGE: Characterizing and auditing training data for improved regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nabeel Seedat",
      "Jonathan Crabbé",
      "Zhaozhi Qian",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eda830e16044587b5082a853c4f25a90-Abstract-Conference.html": {
    "title": "Advice Querying under Budget Constraint for Online Algorithms",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyad Benomar",
      "Vianney Perchet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/edac78c3e300629acfe6cbe9ca88fb84-Abstract-Conference.html": {
    "title": "DIFFER:Decomposing Individual Reward for Fair Experience Replay in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunhan Hu",
      "Jian Zhao",
      "Wengang Zhou",
      "Ruili Feng",
      "Houqiang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/edbcb7583fd8921dad78adecfe06a99b-Abstract-Conference.html": {
    "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yelysei Bondarenko",
      "Markus Nagel",
      "Tijmen Blankevoort"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/edcd1aa172dceda2ea9d45a48f25d3e3-Abstract-Conference.html": {
    "title": "Adversarial Training from Mean Field Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soichiro Kumano",
      "Hiroshi Kera",
      "Toshihiko Yamasaki"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/edd00cead3425393baf13004de993017-Abstract-Conference.html": {
    "title": "MMD-Fuse: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Biggs",
      "Antonin Schrab",
      "Arthur Gretton"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/edd0d433f8a1a51aa11237a6543fc280-Abstract-Conference.html": {
    "title": "DAC-DETR: Divide the Attention Layers and Conquer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengdong Hu",
      "Yifan Sun",
      "Jingdong Wang",
      "Yi Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee1a1ecc92f35702b5c29dad3dc909ea-Abstract-Conference.html": {
    "title": "Streaming Algorithms and Lower Bounds for Estimating Correlation Clustering Cost",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sepehr Assadi",
      "Vihan Shah",
      "Chen Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee1e549d6fb7c58ed06557bfc264335c-Abstract-Conference.html": {
    "title": "Episodic Multi-Task Learning with Heterogeneous Neural Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Shen",
      "Xiantong Zhen",
      "Qi Wang",
      "Marcel Worring"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee1f0da706829d7f198eac0edaacc338-Abstract-Conference.html": {
    "title": "Knowledge Distillation Performs Partial Variance Reduction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mher Safaryan",
      "Alexandra Peste",
      "Dan Alistarh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee208bfc04b1bf6125a6a34baa1c28d3-Abstract-Conference.html": {
    "title": "Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifu Wang",
      "Xuefei Ning",
      "Matthew Blaschko"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee37d51b3c003d89acba2363dde256af-Abstract-Conference.html": {
    "title": "Towards Stable Backdoor Purification through Feature Shift Tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Min",
      "Zeyu Qin",
      "Li Shen",
      "Minhao Cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee56aa4fe26a189782f507d843fd5272-Abstract-Conference.html": {
    "title": "Langevin Quasi-Monte Carlo",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sifan Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee5bb72130c332c3d4bf8d231e617506-Abstract-Conference.html": {
    "title": "What Can We Learn from Unlearnable Datasets?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Sandoval-Segura",
      "Vasu Singla",
      "Jonas Geiping",
      "Micah Goldblum",
      "Tom Goldstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee6630dcbcff857026e474fc857aa9f0-Abstract-Conference.html": {
    "title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiannan Xiang",
      "Tianhua Tao",
      "Yi Gu",
      "Tianmin Shu",
      "Zirui Wang",
      "Zichao Yang ",
      "Zhiting Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee6c4b99b4c0d3d60efd22c1ecdd9891-Abstract-Conference.html": {
    "title": "Activity Grammars for Temporal Action Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dayoung Gong",
      "Joonseok Lee",
      "Deunsol Jung",
      "Suha Kwak",
      "Minsu Cho"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee74a6ade401e200985e2421b20bbae4-Abstract-Conference.html": {
    "title": "SANFlow: Semantic-Aware Normalizing Flow for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daehyun Kim",
      "Sungyong Baik",
      "Tae Hyun Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee860a9fa65a55a335754c557a5211de-Abstract-Conference.html": {
    "title": "On Convergence of Polynomial Approximations to the Gaussian Mixture Entropy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caleb Dahlke",
      "Jason Pacheco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee90fb9511b263f2ff971be9b374f9ee-Abstract-Conference.html": {
    "title": "CEIL: Generalized Contextual Imitation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxin Liu",
      "Li He",
      "Yachen Kang",
      "Zifeng Zhuang",
      "Donglin Wang",
      "Huazhe Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eeac51414a11484d048432f614d5bb1b-Abstract-Conference.html": {
    "title": "Entropic Neural Optimal Transport via Diffusion Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Gushchin",
      "Alexander Kolesov",
      "Alexander Korotin",
      "Dmitry P. Vetrov",
      "Evgeny Burnaev"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eeb57fdf745eb31a3c7ef22c59a4661d-Abstract-Conference.html": {
    "title": "On the Convergence to a Global Solution of Shuffling-Type Gradient Algorithms",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lam Nguyen",
      "Trang H. Tran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eec7fee9a8595ca964b9a11562767345-Abstract-Conference.html": {
    "title": "Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gleb Bazhenov",
      "Denis Kuznedelev",
      "Andrey Malinin",
      "Artem Babenko",
      "Liudmila Prokhorenkova"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eee6efe709623f36483e3fbb0bb513dd-Abstract-Conference.html": {
    "title": "Learning Causal Models under Independent Changes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Mameche",
      "David Kaltenpoth",
      "Jilles Vreeken"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eef6aecfe050b556c6a48d9c16b15558-Abstract-Conference.html": {
    "title": "Universality and Limitations of Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Wang",
      "Jatin Chauhan",
      "Wei Wang",
      "Cho-Jui Hsieh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eef6cb60fd59b32d35718e176b4b08d6-Abstract-Conference.html": {
    "title": "Evaluating Neuron Interpretation Methods of NLP Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimin Fan",
      "Fahim Dalvi",
      "Nadir Durrani",
      "Hassan Sajjad"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eeffa70bcbbd43f6bd067edebc6595e8-Abstract-Conference.html": {
    "title": "How Re-sampling Helps for Long-Tail Learning?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang-Xin Shi",
      "Tong Wei",
      "Yuke Xiang",
      "Yu-Feng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef0c0a23a1a8219c4fc381614664df3e-Abstract-Conference.html": {
    "title": "Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Luo",
      "Maggie Henderson",
      "Leila Wehbe",
      "Michael Tarr"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef0dcb44a47185f5bacac62571f6e920-Abstract-Conference.html": {
    "title": "Query-based Temporal Fusion with Explicit Motion for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghua Hou",
      "Zhe Liu",
      "dingkang liang",
      "Zhikang Zou",
      "Xiaoqing Ye",
      "Xiang Bai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef4f2a0232a246b8a502135175e08953-Abstract-Conference.html": {
    "title": "Efficient Adversarial Contrastive Learning via Robustness-Aware Coreset Selection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xilie Xu",
      "Jingfeng ZHANG",
      "Feng Liu",
      "Masashi Sugiyama",
      "Mohan S. Kankanhalli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef62614753535977071395fb1f1435be-Abstract-Conference.html": {
    "title": "A Finite-Sample Analysis of Payoff-Based Independent Learning in Zero-Sum Stochastic Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaiwei Chen",
      "Kaiqing Zhang",
      "Eric Mazumdar",
      "Asuman Ozdaglar",
      "Adam Wierman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef63b00ad8475605b2eaf520747f61d4-Abstract-Conference.html": {
    "title": "Compact Neural Volumetric Video Representations with Dynamic Codebooks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Guo",
      "Sida Peng",
      "Yunzhi Yan",
      "Linzhan Mou",
      "Yujun Shen",
      "Hujun Bao",
      "Xiaowei Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef6c94e9cf4d169298479ee2e230ee13-Abstract-Conference.html": {
    "title": "Towards Label-free Scene Understanding by Vision Foundation Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runnan Chen",
      "Youquan Liu",
      "Lingdong Kong",
      "Nenglun Chen",
      "Xinge ZHU",
      "Yuexin Ma",
      "Tongliang Liu",
      "Wenping Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef72fa6579401ffff9da246a5014f055-Abstract-Conference.html": {
    "title": "Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Feinberg",
      "Xinyi Chen",
      "Y. Jennifer Sun",
      "Rohan Anil",
      "Elad Hazan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef7bd1f9cbf8a5ab7ddcaccd50699c90-Abstract-Conference.html": {
    "title": "DiffComplete: Diffusion-based Generative 3D Shape Completion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihang Chu",
      "Enze Xie",
      "Shentong Mo",
      "Zhenguo Li",
      "Matthias Niessner",
      "Chi-Wing Fu",
      "Jiaya Jia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/efaf1c9726648c8ba363a5c927440529-Abstract-Conference.html": {
    "title": "Bayesian Risk-Averse Q-Learning with Streaming Observations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Wang",
      "Enlu Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/efb2072a358cefb75886a315a6fcf880-Abstract-Conference.html": {
    "title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Valmeekam",
      "Matthew Marquez",
      "Sarath Sreedharan",
      "Subbarao Kambhampati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/efb9629755e598c4f261c44aeb6fde5e-Abstract-Conference.html": {
    "title": "Is RLHF More Difficult than Standard RL? A Theoretical Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Wang",
      "Qinghua Liu",
      "Chi Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/efbba7719cc5172d175240f24be11280-Abstract-Conference.html": {
    "title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Hanna",
      "Ollie Liu",
      "Alexandre Variengien"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/efca456a4e861f3b47455c44bb134424-Abstract-Conference.html": {
    "title": "Multi-body SE(3) Equivariance for Unsupervised Rigid Segmentation and Motion Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia-Xing Zhong",
      "Ta-Ying Cheng",
      "Yuhang He",
      "Kai Lu",
      "Kaichen Zhou",
      "Andrew Markham",
      "Niki Trigoni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/efcb5b06ce8bb672ffa26b9dc5cdd0f9-Abstract-Conference.html": {
    "title": "Suggesting Variable Order for Cylindrical Algebraic Decomposition via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuqi Jia",
      "Yuhang Dong",
      "Minghao Liu",
      "Pei Huang",
      "Feifei Ma",
      "Jian Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/efe36e55d80a94d1726f660b8d237a0f-Abstract-Conference.html": {
    "title": "GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhen Sun",
      "Weining Wang",
      "Zihan Qin",
      "Jiahui Sun",
      "Sihan Chen",
      "Jing Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/efe406d6d2674d176cdcd958ce605d17-Abstract-Conference.html": {
    "title": "Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sivan Doveh",
      "Assaf Arbelle",
      "Sivan Harary",
      "Roei Herzig",
      "Donghyun Kim",
      "Paola Cascante-Bonilla",
      "Amit Alfassy",
      "Rameswar Panda",
      "Raja Giryes",
      "Rogerio Feris",
      "Shimon Ullman",
      "Leonid Karlinsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0172a5da5a2611e3dc0fe9c6e9a7480-Abstract-Conference.html": {
    "title": "Latent SDEs on Homogeneous Spaces",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Zeng",
      "Florian Graf",
      "Roland Kwitt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f02a7dd6bd3d038b51d092d99e74c638-Abstract-Conference.html": {
    "title": "Balancing Risk and Reward: A Batched-Bandit Strategy for Automated Phased Release",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Li",
      "Jialiang Mao",
      "Iavor Bojinov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f02f1185b97518ab5bd7ebde466992d3-Abstract-Conference.html": {
    "title": "Preconditioning Matters: Fast Global Convergence of Non-convex Matrix Factorization via Scaled Gradient Descent",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Jia",
      "Hailin Wang",
      "Jiangjun Peng",
      "Xiangchu Feng",
      "Deyu Meng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0318ba897cee71ce200e408dea6062e-Abstract-Conference.html": {
    "title": "Deep Insights into Noisy Pseudo Labeling on Graph Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Botao WANG",
      "Jia Li",
      "Yang Liu",
      "Jiashun Cheng",
      "Yu Rong",
      "Wenjia Wang",
      "Fugee Tsung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f03cb785864596fa5901f1359d23fd81-Abstract-Conference.html": {
    "title": "Predicting a Protein's Stability under a Million Mutations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeffrey Ouyang-Zhang",
      "Daniel Diaz",
      "Adam Klivans",
      "Philipp Kraehenbuehl"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f04957cc30544d62386f402e1da0b001-Abstract-Conference.html": {
    "title": "Deep Gaussian Markov Random Fields for Graph-Structured Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fiona Lippert",
      "Bart Kranstauber",
      "Emiel van Loon",
      "Patrick Forré"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0552f14388d95b19740dee809f5cad1-Abstract-Conference.html": {
    "title": "Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amit Daniely",
      "Nati Srebro",
      "Gal Vardi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0606b882692637835e8ac981089eccd-Abstract-Conference.html": {
    "title": "LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsuyuki Miyai",
      "Qing Yu",
      "Go Irie",
      "Kiyoharu Aizawa"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f062da1973ac9ac61fc6d44dd7fa309f-Abstract-Conference.html": {
    "title": "AdANNS: A Framework for Adaptive Semantic Search",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniket Rege",
      "Aditya Kusupati",
      "Sharan Ranjit S",
      "Alan Fan",
      "Qingqing Cao",
      "Sham Kakade",
      "Prateek Jain",
      "Ali Farhadi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0722b58f02d7793acf7d328928f933a-Abstract-Conference.html": {
    "title": "Adversarially Robust Distributed Count Tracking via Partial Differential Privacy",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongzheng Xiong",
      "Xiaoyi Zhu",
      "zengfeng Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0878b7efa656b3bbd407c9248d13751-Abstract-Conference.html": {
    "title": "Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geon Yeong Park",
      "Jeongsol Kim",
      "Beomsu Kim",
      "Sang Wan Lee",
      "Jong Chul Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0a6b46b0183a62a2db973014e3429f4-Abstract-Conference.html": {
    "title": "Optimal Algorithms for the Inhomogeneous Spiked Wigner Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandr Pak",
      "Justin Ko",
      "Florent Krzakala"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0aa7e9e67515fa0c607c2959ccda6a0-Abstract-Conference.html": {
    "title": "Learning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Zhang",
      "Suren Jayasuriya",
      "Visar Berisha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0b1515be276f6ba82b4f2b25e50bef0-Abstract-Conference.html": {
    "title": "Momentum Provably Improves Error Feedback!",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilyas Fatkhullin",
      "Alexander Tyurin",
      "Peter Richtarik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f0d7b528c31bc3f9a0d5bab515ed6ed5-Abstract-Conference.html": {
    "title": "Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmeran Johnson",
      "Ciara Pike-Burke",
      "Patrick Rebeschini"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f115f619b62833aadc5acb058975b0e6-Abstract-Conference.html": {
    "title": "Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijian Luo",
      "Tianyang Hu",
      "Shifeng Zhang",
      "Jiacheng Sun",
      "Zhenguo Li",
      "Zhihua Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f144ab9985c739a5091ec188a2688644-Abstract-Conference.html": {
    "title": "Characterization of Overfitting in Robust Multiclass Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyuan Xu",
      "Weiwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f188a55392d3a7509b0b27f8d24364bb-Abstract-Conference.html": {
    "title": "Expanding Small-Scale Datasets with Guided Imagination",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhang",
      "Daquan Zhou",
      "Bryan Hooi",
      "Kai Wang",
      "Jiashi Feng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f189e7580acad0fc7fd45405817ddee3-Abstract-Conference.html": {
    "title": "Parallel-mentoring for Offline Model-based Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can (Sam) Chen",
      "Christopher Beckham",
      "Zixuan Liu",
      "Xue (Steve) Liu",
      "Chris Pal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f1cf02ce09757f57c3b93c0db83181e0-Abstract-Conference.html": {
    "title": "Nominality Score Conditioned Time Series Anomaly Detection by Point/Sequential Reconstruction",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chih-Yu (Andrew) Lai",
      "Fan-Keng Sun",
      "Zhengqi Gao",
      "Jeffrey H Lang",
      "Duane Boning"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f1d16af76939f476b5f040fd1398c0a3-Abstract-Conference.html": {
    "title": "Frequency-domain MLPs are More Effective Learners in Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Yi",
      "Qi Zhang",
      "Wei Fan",
      "Shoujin Wang",
      "Pengyang Wang",
      "Hui He",
      "Ning An",
      "Defu Lian",
      "Longbing Cao",
      "Zhendong Niu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f1ee1cca0721de55bb35cf28ab95e1b4-Abstract-Conference.html": {
    "title": "Q-DM: An Efficient Low-bit Quantized Diffusion Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanjing Li",
      "Sheng Xu",
      "Xianbin Cao",
      "Xiao Sun",
      "Baochang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f201b3f3d0f08c6ab46c36b9052c1b64-Abstract-Conference.html": {
    "title": "Beyond Exponential Graph: Communication-Efficient Topologies for Decentralized Learning via Finite-time Convergence",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Takezawa",
      "Ryoma Sato",
      "Han Bao",
      "Kenta Niwa",
      "Makoto Yamada"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f2059277ac6ce66e7e5543001afa8bb5-Abstract-Conference.html": {
    "title": "Alternating Updates for Efficient Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cenk Baykal",
      "Dylan Cutler",
      "Nishanth Dikkala",
      "Nikhil Ghosh",
      "Rina Panigrahy",
      "Xin Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f224f056694bcfe465c5d84579785761-Abstract-Conference.html": {
    "title": "Interpretable Prototype-based Graph Information Bottleneck",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwoo Seo",
      "Sungwon Kim",
      "Chanyoung Park"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f22a9af8dbb348952b08bd58d4734b50-Abstract-Conference.html": {
    "title": "Self-Chained Image-Language Model for Video Localization and Question Answering",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoubin Yu",
      "Jaemin Cho",
      "Prateek Yadav",
      "Mohit Bansal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f249db9ab5975586f36df46f8958c008-Abstract-Conference.html": {
    "title": "The Tunnel Effect: Building Data Representations in Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wojciech Masarczyk",
      "Mateusz Ostaszewski",
      "Ehsan Imani",
      "Razvan Pascanu",
      "Piotr Miłoś",
      "Tomasz Trzcinski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f2543511e5f4d4764857f9ad833a977d-Abstract-Conference.html": {
    "title": "Restart Sampling for Improving Generative Processes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Xu",
      "Mingyang Deng",
      "Xiang Cheng",
      "Yonglong Tian",
      "Ziming Liu",
      "Tommi Jaakkola"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f25602918e8a0d0c86e3c752ecfbbaa1-Abstract-Conference.html": {
    "title": "Constructing Non-isotropic Gaussian Diffusion Model Using Isotropic Gaussian Diffusion Model for Image Editing",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Yu",
      "Xiang Gu",
      "Haozhi Liu",
      "Jian Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f26119b4ffe38c24d97e4c49d334b99e-Abstract-Conference.html": {
    "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Duan",
      "Adam Dziedzic",
      "Nicolas Papernot",
      "Franziska Boenisch"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f2957e48240c1d90e62b303574871b47-Abstract-Conference.html": {
    "title": "Dataset Diffusion: Diffusion-based Synthetic Data Generation for Pixel-Level Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quang Nguyen",
      "Truong Vu",
      "Anh Tran",
      "Khoi Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f2a11632520f4b7473d7838f074a7d25-Abstract-Conference.html": {
    "title": "Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Wang",
      "Peipei Li",
      "Huaibo Huang",
      "Chunshui Cao",
      "Ran He",
      "Zhaofeng He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f2b1b2e974fa5ea622dd87f22815f423-Abstract-Conference.html": {
    "title": "GAUCHE: A Library for Gaussian Processes in Chemistry",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan-Rhys Griffiths",
      "Leo Klarner",
      "Henry Moss",
      "Aditya Ravuri",
      "Sang Truong",
      "Yuanqi Du",
      "Samuel Stanton",
      "Gary Tom",
      "Bojana Rankovic",
      "Arian Jamasb",
      "Aryan Deshwal",
      "Julius Schwartz",
      "Austin Tripp",
      "Gregory Kell",
      "Simon Frieder",
      "Anthony Bourached",
      "Alex Chan",
      "Jacob Moss",
      "Chengzhi Guo",
      "Johannes Peter Dürholt",
      "Saudamini Chaurasia",
      "Ji Won Park",
      "Felix Strieth-Kalthoff",
      "Alpha Lee",
      "Bingqing Cheng",
      "Alan Aspuru-Guzik",
      "Philippe Schwaller",
      "Jian Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f2c80b3c9cf8102d38c4b21af25d9740-Abstract-Conference.html": {
    "title": "Exponentially Convergent Algorithms for Supervised Matrix Factorization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joowon Lee",
      "Hanbaek Lyu",
      "Weixin Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f2ea1943896474b7cd9796b93e526f6f-Abstract-Conference.html": {
    "title": "InfoCD: A Contrastive Chamfer Distance Loss for Point Cloud Completion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangzhou Lin",
      "Yun Yue",
      "Ziming Zhang",
      "Songlin Hou",
      "Kazunori Yamada",
      "Vijaya Kolachalama",
      "Venkatesh Saligrama"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f3024ea88cec9f45a411cf4d51ab649c-Abstract-Conference.html": {
    "title": "Differentially Private Statistical Inference through $\\beta$-Divergence One Posterior Sampling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack E. Jewson",
      "Sahra Ghalebikesabi",
      "Chris C Holmes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f31bf160569618084ba9bdc2a8de29d0-Abstract-Conference.html": {
    "title": "Doubly Robust Augmented Transfer for Meta-Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuankun Jiang",
      "Nuowen Kan",
      "Chenglin Li",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f32b13bfc384b3b1d52d675b05f2bece-Abstract-Conference.html": {
    "title": "Efficiently incorporating quintuple interactions into geometric deep learning force fields",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zun Wang",
      "Guoqing Liu",
      "Yichi Zhou",
      "Tong Wang",
      "Bin Shao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f334c3375bd3744e98a0ca8eaa2403b0-Abstract-Conference.html": {
    "title": "Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Stojanovic",
      "Yassir Jedra",
      "Alexandre Proutiere"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f36a180277bd3d5781dc02245f9d5f52-Abstract-Conference.html": {
    "title": "Function Space Bayesian Pseudocoreset for Bayesian Neural Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Balhae Kim",
      "Hyungi Lee",
      "Juho Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f3716db40060004d0629d4051b2c57ab-Abstract-Conference.html": {
    "title": "One-step differentiation of iterative algorithms",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jerome Bolte",
      "Edouard Pauwels",
      "Samuel Vaiter"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f37265d7493377170a3b4ba91823119a-Abstract-Conference.html": {
    "title": "Adaptive Principal Component Regression with Applications to Panel Data",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anish Agarwal",
      "Keegan Harris",
      "Justin Whitehouse",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f39931608cdc52d7d9f8ba7003af9136-Abstract-Conference.html": {
    "title": "The Best of Both Worlds in Network Population Games: Reaching Consensus and Convergence to Equilibrium",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyue Hu",
      "Harold Soh",
      "Georgios Piliouras"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f3bfbd65743e60c685a3845bd61ce15f-Abstract-Conference.html": {
    "title": "L-CAD: Language-based Colorization with Any-level Descriptions using Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zheng chang",
      "Shuchen Weng",
      "Peixuan Zhang",
      "Yu Li",
      "Si Li",
      "Boxin Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f3c1951b34f7f55ffaecada7fde6bd5a-Abstract-Conference.html": {
    "title": "Convolutional Neural Operators for robust and accurate learning of PDEs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bogdan Raonic",
      "Roberto Molinaro",
      "Tim De Ryck",
      "Tobias Rohner",
      "Francesca Bartolucci",
      "Rima Alaifari",
      "Siddhartha Mishra",
      "Emmanuel de Bézenac"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f3c5e56274140e0420baa3916c529210-Abstract-Conference.html": {
    "title": "Neural Image Compression: Generalization, Robustness, and Spectral Biases",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelsey Lieberman",
      "James Diffenderfer",
      "Charles Godfrey",
      "Bhavya Kailkhura"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f3d1e34a15c0af0954ae36a7f811c754-Abstract-Conference.html": {
    "title": "Estimating Koopman operators with sketching to provably learn large scale dynamical systems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giacomo Meanti",
      "Antoine Chatalic",
      "Vladimir Kostic",
      "Pietro Novelli",
      "Massimiliano Pontil",
      "Lorenzo Rosasco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f3da4165893c2465fd7e8df453c41ffa-Abstract-Conference.html": {
    "title": "Self-Adaptive Motion Tracking against On-body Displacement of Flexible Sensors",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengxu Zuo",
      "Fang Jiawei",
      "Shihui Guo",
      "Yipeng Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f3f2ff9579ba6deeb89caa2fe1f0b99c-Abstract-Conference.html": {
    "title": "Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianzhun Shao",
      "Yun Qu",
      "Chen Chen",
      "Hongchang Zhang",
      "Xiangyang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f418594e90047a10f4c158f70d6701cc-Abstract-Conference.html": {
    "title": "Black-Box Differential Privacy for Interactive ML",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haim Kaplan",
      "Yishay Mansour",
      "Shay Moran",
      "Kobbi Nissim",
      "Uri Stemmer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f41b6e5af73421e46ceed9cb036e72e7-Abstract-Conference.html": {
    "title": "Mnemosyne: Learning to Train Transformers with Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepali Jain",
      "Krzysztof M Choromanski",
      "Kumar Avinava Dubey",
      "Sumeet Singh",
      "Vikas Sindhwani",
      "Tingnan Zhang",
      "Jie Tan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4366126eba252699b280e8f93c0ab2f-Abstract-Conference.html": {
    "title": "PoET: A generative model of protein families as sequences-of-sequences",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothy Truong Jr",
      "Tristan Bepler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f445ba15f0f05c26e1d24f908ea78d60-Abstract-Conference.html": {
    "title": "BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darko Drakulic",
      "Sofia Michel",
      "Florian Mai",
      "Arnaud Sors",
      "Jean-Marc Andreoli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4757db82a02eea015670ecca605d5cc-Abstract-Conference.html": {
    "title": "Neural Functional Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Allan Zhou",
      "Kaien Yang",
      "Yiding Jiang",
      "Kaylee Burns",
      "Winnie Xu",
      "Samuel Sokota",
      "J. Zico Kolter",
      "Chelsea Finn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4821075019a058700f6e6738eea1365-Abstract-Conference.html": {
    "title": "LinkerNet: Fragment Poses and Linker Co-Design with 3D Equivariant Diffusion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Guan",
      "Xingang Peng",
      "PeiQi Jiang",
      "Yunan Luo",
      "Jian Peng",
      "Jianzhu Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f49287371916715b9209fa41a275851e-Abstract-Conference.html": {
    "title": "One Risk to Rule Them All: A Risk-Sensitive Perspective on Model-Based Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Rigter",
      "Bruno Lacerda",
      "Nick Hawes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f498c1ce6bff52eb04febf87438dd84b-Abstract-Conference.html": {
    "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Fu",
      "Simran Arora",
      "Jessica Grogan",
      "Isys Johnson",
      "Evan Sabri Eyuboglu",
      "Armin Thomas",
      "Benjamin Spector",
      "Michael Poli",
      "Atri Rudra",
      "Christopher Ré"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f499387f191d6be56e68966181095878-Abstract-Conference.html": {
    "title": "Bypassing spike sorting: Density-based decoding using spike localization from dense multielectrode probes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizi Zhang",
      "Tianxiao He",
      "Julien Boussard",
      "Charles Windolf",
      "Olivier Winter",
      "Eric Trautmann",
      "Noam Roth",
      "Hailey Barrell",
      "Mark Churchland",
      "Nicholas A Steinmetz",
      "Erdem Varol",
      "Cole Hurwitz",
      "Liam Paninski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4a6806490d31216a3ba667eb240c897-Abstract-Conference.html": {
    "title": "SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchen Xue",
      "Mingyang Yi",
      "Weijian Luo",
      "Shifeng Zhang",
      "Jiacheng Sun",
      "Zhenguo Li",
      "Zhi-Ming Ma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4b52b45a677d855dee0ca9ba1ddf638-Abstract-Conference.html": {
    "title": "Social Motion Prediction with Cognitive Hierarchies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Zhu",
      "Jason Qin",
      "Yuke Lou",
      "Hang Ye",
      "Xiaoxuan Ma",
      "Hai Ci",
      "Yizhou Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4b6ef2a78684dca2fb3f1c09372e041-Abstract-Conference.html": {
    "title": "Unbounded Differentially Private Quantile and Maximum Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Durfee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4b768188be63b8d2680a46934fd295a-Abstract-Conference.html": {
    "title": "How to Turn Your Knowledge Graph Embeddings into Generative Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Loconte",
      "Nicola Di Mauro",
      "Robert Peharz",
      "Antonio Vergari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4b8ddb9b1aa3cb11462d64a70b84db2-Abstract-Conference.html": {
    "title": "Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikai Xiao",
      "Zihan Chen",
      "Songshang Liu",
      "Hualiang Wang",
      "YANG FENG",
      "Jin Hao",
      "Joey Tianyi Zhou",
      "Jian Wu",
      "Howard Yang",
      "Zuozhu Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4eaa4b8f2d08edb3f0af990d56134ea-Abstract-Conference.html": {
    "title": "On Differentially Private Sampling from Gaussian and Product Distributions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Badih Ghazi",
      "Xiao Hu",
      "Ravi Kumar",
      "Pasin Manurangsi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f53437debdd397c42929d929614bc705-Abstract-Conference.html": {
    "title": "Anytime-Competitive Reinforcement Learning with Policy Prior",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyi Yang",
      "Pengfei Li",
      "Tongxin Li",
      "Adam Wierman",
      "Shaolei Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f54bd48aba0dff7acdac86123188f1b6-Abstract-Conference.html": {
    "title": "Metis: Understanding and Enhancing In-Network Regular Expressions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxin Zhang",
      "Yucheng Huang",
      "Guanglin Duan",
      "Qing Li",
      "Dan Zhao",
      "Yong Jiang",
      "Lianbo Ma",
      "Xi Xiao",
      "Hengyang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f555b62384279b98732204cb1a670a23-Abstract-Conference.html": {
    "title": "Adaptive Test-Time Personalization for Federated Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Bao",
      "Tianxin Wei",
      "Haohan Wang",
      "Jingrui He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f564a952c1b86684baf7d7241ae27ac8-Abstract-Conference.html": {
    "title": "Context-lumpable stochastic bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chung-Wei Lee",
      "Qinghua Liu",
      "Yasin Abbasi Yadkori",
      "Chi Jin",
      "Tor Lattimore",
      "Csaba Szepesvari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f5708199bdc013c5b56406db305b991e-Abstract-Conference.html": {
    "title": "Text Alignment Is An Efficient Unified Model for Massive NLP Tasks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Zha",
      "Yichi Yang",
      "Ruichen Li",
      "Zhiting Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f57ffe47d0b528fbb97901d16bd4eba2-Abstract-Conference.html": {
    "title": "Learning from Active Human Involvement through Proxy Value Propagation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao (Mark) Peng",
      "Wenjie Mo",
      "Chenda Duan",
      "Quanyi Li",
      "Bolei Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f5846131aa6a72d1df3bd6d43a4a960b-Abstract-Conference.html": {
    "title": "PrObeD: Proactive Object Detection Wrapper",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishal Asnani",
      "Abhinav Kumar",
      "Suya You",
      "Xiaoming Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f58c24798220ba724fe05c0fa786227d-Abstract-Conference.html": {
    "title": "Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anirudhan Badrinath",
      "Yannis Flet-Berliac",
      "Allen Nie",
      "Emma Brunskill"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f5ccb3ab757131a93586ef61ec701533-Abstract-Conference.html": {
    "title": "Should Under-parameterized Student Networks Copy or Average Teacher Weights?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Berfin Simsek",
      "Amire Bendjeddou",
      "Wulfram Gerstner",
      "Johanni Brea"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f5fcd88d3deb97bb62559208cfa0ab62-Abstract-Conference.html": {
    "title": "Learning from Rich Semantics and Coarse Locations for Long-tailed Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingchen Meng",
      "Xiyang Dai",
      "Jianwei Yang",
      "Dongdong Chen",
      "Yinpeng Chen",
      "Mengchen Liu",
      "Yi-Ling Chen",
      "Zuxuan Wu",
      "Lu Yuan",
      "Yu-Gang Jiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f66340d6f28dae6aab0176892c9065e7-Abstract-Conference.html": {
    "title": "Granger Components Analysis: Unsupervised learning of latent temporal dependencies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacek Dmochowski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f670ef96387d9a5a8a51e2ed80cb148d-Abstract-Conference.html": {
    "title": "Monte Carlo Tree Search with Boltzmann Exploration",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Painter",
      "Mohamed Baioumy",
      "Nick Hawes",
      "Bruno Lacerda"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f6712d5191d2501dfc7024389f7bfcdd-Abstract-Conference.html": {
    "title": "False Discovery Proportion control for aggregated Knockoffs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Blain",
      "Bertrand Thirion",
      "Olivier Grisel",
      "Pierre Neuvial"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f6a8b109d4d4fd64c75e94aaf85d9697-Abstract-Conference.html": {
    "title": "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxuan Wu",
      "Atticus Geiger",
      "Thomas Icard",
      "Christopher Potts",
      "Noah Goodman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f6b22ac37beb5da61efd4882082c9ecd-Abstract-Conference.html": {
    "title": "Large Language Models Are Semi-Parametric Reinforcement Learning Agents",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danyang Zhang",
      "Lu Chen",
      "Situo Zhang",
      "Hongshen Xu",
      "Zihan Zhao",
      "Kai Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f6b30f3e2dd9cb53bbf2024402d02295-Abstract-Conference.html": {
    "title": "BIOT: Biosignal Transformer for Cross-data Learning in the Wild",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoqi Yang",
      "M Westover",
      "Jimeng Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f6c1843f11d34312b11ec5ff9a10c5a6-Abstract-Conference.html": {
    "title": "Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Zhang",
      "Zhuohang Li",
      "Kamalika Das",
      "Sricharan Kumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f72d4fdfd5eb425cd81df9fe6272a533-Abstract-Conference.html": {
    "title": "Amortized Reparametrization: Efficient and Scalable Variational Inference for Latent SDEs",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Course",
      "Prasanth Nair"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f737da5ea0e122870fad209509f87d5b-Abstract-Conference.html": {
    "title": "Boundary Guided Learning-Free Semantic Control with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Zhu",
      "Yu Wu",
      "Zhiwei Deng",
      "Olga Russakovsky",
      "Yan Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f74054328beeb0c21a9b8e99da557f5a-Abstract-Conference.html": {
    "title": "Kiki or Bouba? Sound Symbolism in Vision-and-Language Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Morris Alper",
      "Hadar Averbuch-Elor"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f751c6f8bfb52c60f43942896fe65904-Abstract-Conference.html": {
    "title": "MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Allen Nie",
      "Yuhui Zhang",
      "Atharva Shailesh Amdekar",
      "Chris Piech",
      "Tatsunori B. Hashimoto",
      "Tobias Gerstenberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f763f7c9a6599e14b07add5937d8189c-Abstract-Conference.html": {
    "title": "Collapsed Inference for Bayesian Deep Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Zeng",
      "Guy Van den Broeck"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f77d9409647c096789067c09455858a2-Abstract-Conference.html": {
    "title": "Contextual Stochastic Bilevel Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Hu",
      "Jie Wang",
      "Yao Xie",
      "Andreas Krause",
      "Daniel Kuhn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f780a86b7145988ac219d49d8e37a58f-Abstract-Conference.html": {
    "title": "Learning Invariant Molecular Representation in Latent Discrete Space",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Zhuang",
      "Qiang Zhang",
      "Keyan Ding",
      "Yatao Bian",
      "Xiao Wang",
      "Jingsong Lv",
      "Hongyang Chen",
      "Huajun Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f7a94134f1c726796c6f81fb946e489d-Abstract-Conference.html": {
    "title": "Accelerating Motion Planning via Optimal Transport",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An T. Le",
      "Georgia Chalvatzaki",
      "Armin Biess",
      "Jan R. Peters"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f7b77476d89d5fb58aeb77691d2f40f5-Abstract-Conference.html": {
    "title": "CWCL: Cross-Modal Transfer with Continuously Weighted Contrastive Loss",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rakshith Sharma Srinivasa",
      "Jaejin Cho",
      "Chouchang Yang",
      "Yashas Malur Saidutta",
      "Ching-Hua Lee",
      "Yilin Shen",
      "Hongxia Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f7d3cef7ff579f2f903c8f458e730cae-Abstract-Conference.html": {
    "title": "Decompose a Task into Generalizable Subtasks in Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikang Tian",
      "Ruizhi Chen",
      "Xing Hu",
      "Ling Li",
      "Rui Zhang",
      "Fan Wu",
      "Shaohui Peng",
      "Jiaming Guo",
      "Zidong Du",
      "Qi Guo",
      "Yunji Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f7e8bc4c853e3e58bc487e213c79c587-Abstract-Conference.html": {
    "title": "The Equivalence of Dynamic and Strategic Stability under Regularized Learning in Games",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Boone",
      "Panayotis Mertikopoulos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f7f98663c516fceb582354ee2d9d274d-Abstract-Conference.html": {
    "title": "HubRouter: Learning Global Routing via Hub Generation and Pin-hub Connection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingbo Du",
      "Chonghua Wang",
      "Ruizhe Zhong",
      "Junchi Yan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f7fc38fdd95fd146a471791b93ff9f12-Abstract-Conference.html": {
    "title": "$L_2$-Uniform Stability of Randomized Learning Algorithms: Sharper Generalization Bounds and Confidence Boosting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaotong Yuan",
      "Ping Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f7fdebf712db182eddaee2eb02af91e0-Abstract-Conference.html": {
    "title": "Neural Sampling in Hierarchical Exponential-family Energy-based Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingsi Dong",
      "Si Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f810c2ba07bae78dfe9d25c5d40c5536-Abstract-Conference.html": {
    "title": "Block Coordinate Plug-and-Play Methods for Blind Inverse Problems",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Gan",
      "shirin shoushtari",
      "Yuyang Hu",
      "Jiaming Liu",
      "Hongyu An",
      "Ulugbek Kamilov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f82ba6a6b981fbbecf5f2ee5de7db39c-Abstract-Conference.html": {
    "title": "PreDiff: Precipitation Nowcasting with Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihan Gao",
      "Xingjian Shi",
      "Boran Han",
      "Hao Wang",
      "Xiaoyong Jin",
      "Danielle Maddix",
      "Yi Zhu",
      "Mu Li",
      "Yuyang (Bernie) Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f86c5c4d4dca70d30b1c12a33a2bc1a4-Abstract-Conference.html": {
    "title": "All Points Matter: Entropy-Regularized Distribution Alignment for Weakly-supervised 3D Segmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyao Tang",
      "Zhe Chen",
      "Shanshan Zhao",
      "Chaoyue Wang",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f88bec15cc4cb56b432ee040bb63f94f-Abstract-Conference.html": {
    "title": "SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Dong",
      "Ismail Nejjar",
      "Han Sun",
      "Eleni Chatzi",
      "Olga Fink"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f8928b073ccbec15d35f2a9d39430bfd-Abstract-Conference.html": {
    "title": "Bounding training data reconstruction in DP-SGD",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jamie Hayes",
      "Borja Balle",
      "Saeed Mahloujifar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f8cea6a15db693dc525cde5e688410a9-Abstract-Conference.html": {
    "title": "Neural Processes with Stability",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huafeng Liu",
      "Liping Jing",
      "Jian Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f8d39584f87944e5dbe46ec76f19e20a-Abstract-Conference.html": {
    "title": "Multi-Agent Learning with Heterogeneous Linear Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Do",
      "Thanh Nguyen-Tang",
      "Raman Arora"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f8e55d98b0c2569bd0aa25b076e6b3f8-Abstract-Conference.html": {
    "title": "A polar prediction model for learning to represent visual transformations",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre-Étienne Fiquet",
      "Eero Simoncelli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f8f78f8043f35890181a824e53a57134-Abstract-Conference.html": {
    "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "LILI YU",
      "Daniel Simig",
      "Colin Flaherty",
      "Armen Aghajanyan",
      "Luke Zettlemoyer",
      "Mike Lewis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f93df618c6907bc0a03222040d70d004-Abstract-Conference.html": {
    "title": "CQM: Curriculum Reinforcement Learning with a Quantized World Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjae Lee",
      "Daesol Cho",
      "Jonghae Park",
      "H. Jin Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f944a7bcfe9e76b34490ebe4e29196d9-Abstract-Conference.html": {
    "title": "Debiasing Conditional Stochastic Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lie He",
      "Shiva Kasiviswanathan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f95606d8e870020085990d9650b4f2a1-Abstract-Conference.html": {
    "title": "Cascading Bandits: Optimizing Recommendation Frequency in Delayed Feedback Environments",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dairui Wang",
      "Junyu Cao",
      "Yan Zhang",
      "Wei Qi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f96839fc751b67492e17e70f5c9730e4-Abstract-Conference.html": {
    "title": "CoPriv: Network/Protocol Co-Optimization for Communication-Efficient Private Inference",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Zeng",
      "Meng Li",
      "Haichuan Yang",
      "Wen-jie Lu",
      "Runsheng Wang",
      "Ru Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f976982cd1c1b9e076c096787ef6652e-Abstract-Conference.html": {
    "title": "Generalized equivalences between subsampling and ridge regularization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratik Patil",
      "Jin-Hong Du"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f978c8f3b5f399cae464e85f72e28503-Abstract-Conference.html": {
    "title": "D4Explainer: In-distribution Explanations of Graph Neural Network via Discrete Denoising Diffusion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialin Chen",
      "Shirley Wu",
      "Abhijit Gupta",
      "Rex Ying"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f980ba94f513168f2b292f58aef929ec-Abstract-Conference.html": {
    "title": "Core-sets for Fair and Diverse Data Summarization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sepideh Mahabadi",
      "Stojan Trajanovski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f99bb39502f09c4825e89760b4e1ad04-Abstract-Conference.html": {
    "title": "Energy-Efficient Scheduling with Predictions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Balkanski",
      "Noemie Perivier",
      "Clifford Stein",
      "Hao-Ting Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f99f7b22ad47fa6ce151730cf8d17911-Abstract-Conference.html": {
    "title": "Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lisa Dunlap",
      "Alyssa Umino",
      "Han Zhang",
      "Jiezhi Yang",
      "Joseph E. Gonzalez",
      "Trevor Darrell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f9d7d6c695bc983fcfb5b70a5fbdfd2f-Abstract-Conference.html": {
    "title": "Improving Robustness with Adaptive Weight Decay",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Amin Ghiasi",
      "Ali Shafahi",
      "Reza Ardekani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f9f54762cbb4fe4dbffdd4f792c31221-Abstract-Conference.html": {
    "title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Guan",
      "Karthik Valmeekam",
      "Sarath Sreedharan",
      "Subbarao Kambhampati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f9fd24fd32eccc14cd3ecd3716a1cbf8-Abstract-Conference.html": {
    "title": "Described Object Detection: Liberating Object Detection with Flexible Expressions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Xie",
      "Zhao Zhang",
      "Yixuan Wu",
      "Feng Zhu",
      "Rui Zhao",
      "Shuang Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa0126bb7ebad258bf4ffdbbac2dd787-Abstract-Conference.html": {
    "title": "Learning Cuts via Enumeration Oracles",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Thuerck",
      "Boro Sofranac",
      "Marc E Pfetsch",
      "Sebastian Pokutta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa1cfe4e956d85e016b1f8f49b189a0b-Abstract-Conference.html": {
    "title": "Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Yue",
      "Anwen Hu",
      "Liang Zhang",
      "Qin Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa384d5f9e85380833d523766af5941c-Abstract-Conference.html": {
    "title": "Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hassan Akbari",
      "Dan Kondratyuk",
      "Yin Cui",
      "Rachel Hornung",
      "Huisheng Wang",
      "Hartwig Adam"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa55bf1947530fc9567059ff42a806c2-Abstract-Conference.html": {
    "title": "Self-Correcting Bayesian Optimization through Bayesian Active Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carl Hvarfner",
      "Erik Hellsten",
      "Frank Hutter",
      "Luigi Nardi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa55eb802a531c8087e225ecf2dcfbca-Abstract-Conference.html": {
    "title": "SE(3) Equivariant Augmented Coupling Flows",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurence Midgley",
      "Vincent Stimper",
      "Javier Antorán",
      "Emile Mathieu",
      "Bernhard Schölkopf",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa5b423e24b442180bcd4e13ae75a27f-Abstract-Conference.html": {
    "title": "Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimin Chen",
      "Longlong Jing",
      "Yingwei Li",
      "Bing Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa64505ebdc94531087bc81251ce2376-Abstract-Conference.html": {
    "title": "Imagine That! Abstract-to-Intricate Text-to-Image Synthesis with Scene Graph Hallucination Diffusion",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Hanwang Zhang",
      "Tat-Seng Chua"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa67d13ba6c73637593bbcc92f6400ff-Abstract-Conference.html": {
    "title": "A unified framework for information-theoretic generalization bounds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng Chu",
      "Maxim Raginsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa68ea2ff794ce792a688dec82c04f49-Abstract-Conference.html": {
    "title": "Diverse Shape Completion via Style Modulated Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wesley Khademi",
      "Fuxin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa9755043814e7f08d859a286bb83c35-Abstract-Conference.html": {
    "title": "On the Role of Randomization in Adversarially Robust Classification",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Gnecco Heredia",
      "Muni Sreenivas Pydi",
      "Laurent Meunier",
      "Benjamin Negrevergne",
      "Yann Chevaleyre"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/faacb7a4827b4d51e201666b93ab5fa7-Abstract-Conference.html": {
    "title": "Controlling Text-to-Image Diffusion by Orthogonal Finetuning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeju Qiu",
      "Weiyang Liu",
      "Haiwen Feng",
      "Yuxuan Xue",
      "Yao Feng",
      "Zhen Liu",
      "Dan Zhang",
      "Adrian Weller",
      "Bernhard Schölkopf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fab489de1a3224f0394d8f1d3c3213a8-Abstract-Conference.html": {
    "title": "NCDL: A Framework for Deep Learning on non-Cartesian Lattices",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Horacsek",
      "Usman Alim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb06bc3abcece7b8725a8b83b8fa3632-Abstract-Conference.html": {
    "title": "Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Granley",
      "Tristan Fauvel",
      "Matthew Chalk",
      "Michael Beyeler"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb1b83b35e96998ddfc0ce1dab635445-Abstract-Conference.html": {
    "title": "CAST: Cross-Attention in Space and Time for Video Action Recognition",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongho Lee",
      "Jongseo Lee",
      "Jinwoo Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb1d9c3fc2161e12aa71cdcab74b9d2c-Abstract-Conference.html": {
    "title": "Faster Differentially Private Convex Optimization via Second-Order Methods",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arun Ganesh",
      "Mahdi Haghifam",
      "Thomas Steinke",
      "Abhradeep Guha Thakurta"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb44a668c2d4bc984e9d6ca261262cbb-Abstract-Conference.html": {
    "title": "Auditing for Human Expertise",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Alur",
      "Loren Laine",
      "Darrick Li",
      "Manish Raghavan",
      "Devavrat Shah",
      "Dennis Shung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb4a7e3522363907b26a86cc5be627ac-Abstract-Conference.html": {
    "title": "Smooth, exact rotational symmetrization for deep learning on point clouds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergey Pozdnyakov",
      "Michele Ceriotti"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb64a43508e0cfe53ee6179ff31ea900-Abstract-Conference.html": {
    "title": "Functional Equivalence and Path Connectivity of Reducible Hyperbolic Tangent Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Farrugia-Roberts"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb71332951af4ae27fbd457daadc5341-Abstract-Conference.html": {
    "title": "On Robust Streaming for Learning with Experts: Algorithms and Lower Bounds",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Woodruff",
      "Fred Zhang",
      "Samson Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb7f55f36c53247a704792a721272706-Abstract-Conference.html": {
    "title": "Stochastic Optimal Control for Collective Variable Free Sampling of Molecular Transition Paths",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lars Holdijk",
      "Yuanqi Du",
      "Ferry Hooft",
      "Priyank Jaini",
      "Berend Ensing",
      "Max Welling"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb8e52adcd9b59bad73f109c53afc43a-Abstract-Conference.html": {
    "title": "RangePerception: Taming LiDAR Range View for Efficient and Accurate 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeqi BAI",
      "Ben Fei",
      "Youquan Liu",
      "Tao MA",
      "Yuenan Hou",
      "Botian Shi",
      "Yikang LI"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb8e5f198c7a5dcd48860354e38c0edc-Abstract-Conference.html": {
    "title": "One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Hao",
      "Jianyuan Guo",
      "Kai Han",
      "Yehui Tang",
      "Han Hu",
      "Yunhe Wang",
      "Chang Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fb9f53edbfd80b3a543f7963b63363ff-Abstract-Conference.html": {
    "title": "The Graph Pencil Method: Mapping Subgraph Densities to Stochastic Block Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lee Gunderson",
      "Gecia Bravo-Hermsdorff",
      "Peter Orbanz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fba4a59c7a569fce120eea9aa9227052-Abstract-Conference.html": {
    "title": "Cross-links Matter for Link Prediction: Rethinking the Debiased GNN from a Data Perspective",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Luo",
      "Hong Huang",
      "Jianxun Lian",
      "Xiran Song",
      "Xing Xie",
      "Hai Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fbbda4e85a6641bf425be3a6cfd84d20-Abstract-Conference.html": {
    "title": "On the Robustness of Removal-Based Feature Attributions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Lin",
      "Ian Covert",
      "Su-In Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fbc9981dd6316378aee7fd5975250f21-Abstract-Conference.html": {
    "title": "Sample-efficient Multi-objective Molecular Optimization with GFlowNets",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Zhu",
      "Jialu Wu",
      "Chaowen Hu",
      "Jiahuan Yan",
      "kim hsieh",
      "Tingjun Hou",
      "Jian Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fbdaea4878318e214c0577dae4b8bc43-Abstract-Conference.html": {
    "title": "DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Wang",
      "Wei Mao",
      "Hongdong Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fbe30aab28ad7148bc73804689ac0bd7-Abstract-Conference.html": {
    "title": "Towards Data-Algorithm Dependent Generalization: a Case Study on Overparameterized Linear Regression",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Xu",
      "Jiaye Teng",
      "Yang Yuan",
      "Andrew Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc034d186280f55370b6aca7a3285a65-Abstract-Conference.html": {
    "title": "Global Structure-Aware Diffusion Process for Low-light Image Enhancement",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhui HOU",
      "Zhiyu Zhu",
      "Junhui Hou",
      "Hui LIU",
      "Huanqiang Zeng",
      "Hui Yuan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc07feae9af49dd3f1a1e049b77f4e17-Abstract-Conference.html": {
    "title": "FedGCN: Convergence-Communication Tradeoffs in Federated Training of Graph Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Yao",
      "Weizhao Jin",
      "Srivatsan Ravi",
      "Carlee Joe-Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc0cc55dca3d791c4a0bb2d8ddeefe4f-Abstract-Conference.html": {
    "title": "SegRefiner: Towards Model-Agnostic Segmentation Refinement with Discrete Diffusion Process",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyu Wang",
      "Henghui Ding",
      "Jun Hao Liew",
      "Jiajun Liu",
      "Yao Zhao",
      "Yunchao Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc0e3f908a2116ba529ad0a1530a3675-Abstract-Conference.html": {
    "title": "On Masked Pre-training and the Marginal Likelihood",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Moreno-Muñoz",
      "Pol Garcia Recasens",
      "Søren Hauberg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc30caeb45721bab13507c50199e6403-Abstract-Conference.html": {
    "title": "Smoothed Analysis of Sequential Probability Assignment",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alankrita Bhatt",
      "Nika Haghtalab",
      "Abhishek Shetty"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc657b7fd7b9aaa462f2ef9f0362b273-Abstract-Conference.html": {
    "title": "Invariant Learning via Probability of Sufficient and Necessary Causes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyue Yang",
      "Yonggang Zhang",
      "Zhen Fang",
      "Yali Du",
      "Furui Liu",
      "Jean-Francois Ton",
      "Jianhong Wang",
      "Jun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc65fab891d83433bd3c8d966edde311-Abstract-Conference.html": {
    "title": "Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Fan",
      "Olivia Watkins",
      "Yuqing Du",
      "Hao Liu",
      "Moonkyung Ryu",
      "Craig Boutilier",
      "Pieter Abbeel",
      "Mohammad Ghavamzadeh",
      "Kangwook Lee",
      "Kimin Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc6bd0eef19459655d5b097af783661d-Abstract-Conference.html": {
    "title": "Online POMDP Planning with Anytime Deterministic Guarantees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moran Barenboim",
      "Vadim Indelman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc8ee7c7ab5b5f6b1615045dfb617ed6-Abstract-Conference.html": {
    "title": "The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laixi Shi",
      "Gen Li",
      "Yuting Wei",
      "Yuxin Chen",
      "Matthieu Geist",
      "Yuejie Chi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fcd3909db30887ce1da519c4468db668-Abstract-Conference.html": {
    "title": "Strategic Apple Tasting",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keegan Harris",
      "Chara Podimata",
      "Steven Z. Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fcdccd419c4dc471fa3b73ec97b53789-Abstract-Conference.html": {
    "title": "GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinggang Chen",
      "Junjie Li",
      "Xiaoyang Qu",
      "Jianzong Wang",
      "Jiguang Wan",
      "Jing Xiao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fce176458ff542940fa3ed16e6f9c852-Abstract-Conference.html": {
    "title": "Context-guided Embedding Adaptation for Effective Topic Modeling in Low-Resource Regimes",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yishi Xu",
      "Jianqiao Sun",
      "Yudi Su",
      "Xinyang Liu",
      "Zhibin Duan",
      "Bo Chen",
      "Mingyuan Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fce2d8a485746f76aac7b5650db2679d-Abstract-Conference.html": {
    "title": "Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew T Jackson",
      "Minqi Jiang",
      "Jack Parker-Holder",
      "Risto Vuorio",
      "Chris Lu",
      "Greg Farquhar",
      "Shimon Whiteson",
      "Jakob Foerster"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fd02779b6c8885efc69bab6dd9571cee-Abstract-Conference.html": {
    "title": "A Riemannian Exponential Augmented Lagrangian Method for Computing the Projection Robust Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Jiang",
      "Ya-Feng Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fd489a44f3bcb9f122e4931ef21d0c43-Abstract-Conference.html": {
    "title": "Context Shift Reduction for Offline Meta-Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunkai Gao",
      "Rui Zhang",
      "Jiaming Guo",
      "Fan Wu",
      "Qi Yi",
      "Shaohui Peng",
      "Siming Lan",
      "Ruizhi Chen",
      "Zidong Du",
      "Xing Hu",
      "Qi Guo",
      "Ling Li",
      "Yunji Chen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fd5013ea0c3f96931dec77174eaf9d80-Abstract-Conference.html": {
    "title": "Towards Data-Agnostic Pruning At Initialization: What Makes a Good Sparse Mask?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Pham",
      "The Anh Ta",
      "Shiwei Liu",
      "Lichuan Xiang",
      "Dung Le",
      "Hongkai Wen",
      "Long Tran-Thanh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fd62b65606f0f0d2af2c01623a224258-Abstract-Conference.html": {
    "title": "New Bounds for Hyperparameter Tuning of Regression Problems Across Instances",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria-Florina F. Balcan",
      "Anh Nguyen",
      "Dravyansh Sharma"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html": {
    "title": "Jailbroken: How Does LLM Safety Training Fail?",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Wei",
      "Nika Haghtalab",
      "Jacob Steinhardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fd750154df5f199f94df897975621306-Abstract-Conference.html": {
    "title": "Conditional Mutual Information for Disentangled Representations in Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mhairi Dunion",
      "Trevor McInroe",
      "Kevin Sebastian Luck",
      "Josiah Hanna",
      "Stefano Albrecht"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fd83f4e0dcaf1c64ea15bbb1695bb40f-Abstract-Conference.html": {
    "title": "Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duligur Ibeling",
      "Thomas Icard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fd8872fcba4ba87312cdfe5ebba91ca9-Abstract-Conference.html": {
    "title": "LogSpecT: Feasible Graph Learning Model from Stationary Signals with Recovery Guarantees",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangyuan LIU",
      "Linglingzhi Zhu",
      "Anthony Man-Cho So"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fda257e65f46e21dbc117b20fd0aba3c-Abstract-Conference.html": {
    "title": "Depth-discriminative Metric Learning for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonhyeok Choi",
      "Mingyu Shin",
      "Sunghoon Im"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fdba5e0a9b57fce03e89cc0cad0a24e9-Abstract-Conference.html": {
    "title": "PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhe Zhang",
      "Jiatao Gu",
      "Zhuofeng Wu",
      "Shuangfei Zhai",
      "Joshua Susskind",
      "Navdeep Jaitly"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fdd565f63f49776bef620e0ce368a492-Abstract-Conference.html": {
    "title": "Generalized Bayesian Inference for Scientific Simulators via Amortized Cost Estimation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Richard Gao",
      "Michael Deistler",
      "Jakob H Macke"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fddad60891bdf85aac8041f80ed022df-Abstract-Conference.html": {
    "title": "Harnessing the power of choices in decision tree learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Blanc",
      "Jane Lange",
      "Chirag Pabbaraju",
      "Colin Sullivan",
      "Li-Yang Tan",
      "Mo Tiwari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fde1a69a5b6e554b2f1f727197d2651d-Abstract-Conference.html": {
    "title": "Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tam Nguyen",
      "Tan Nguyen",
      "Richard Baraniuk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe03053bd2cf5b5c56de1e463bc53e1a-Abstract-Conference.html": {
    "title": "On the Role of Noise in the Sample Complexity of Learning Recurrent Neural Networks: Exponential Gaps for Long Sequences",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza F. Pour",
      "Hassan Ashtiani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe061ec0ae03c5cf5b5323a2b9121bfd-Abstract-Conference.html": {
    "title": "Provably Bounding Neural Network Preimages",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhas Kotha",
      "Christopher Brix",
      "J. Zico Kolter",
      "Krishnamurthy Dvijotham",
      "Huan Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe1ab2f77a9a0f224839cc9f1034a908-Abstract-Conference.html": {
    "title": "Scaling Riemannian Diffusion Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron Lou",
      "Minkai Xu",
      "Adam Farris",
      "Stefano Ermon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe1c4991d57f37dfef62d01b3901ca54-Abstract-Conference.html": {
    "title": "Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyan Zhao",
      "Aditya Grover"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe318a2b6c699808019a456b706cd845-Abstract-Conference.html": {
    "title": "Conformal Prediction for Uncertainty-Aware Planning with Diffusion Dynamics Model",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiankai Sun",
      "Yiqi Jiang",
      "Jianing Qiu",
      "Parth Nobel",
      "Mykel J Kochenderfer",
      "Mac Schwager"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe4da14f07561a232782820d30ea22f3-Abstract-Conference.html": {
    "title": "Max-Sliced Mutual Information",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dor Tsur",
      "Ziv Goldfeld",
      "Kristjan Greenewald"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe51de4e7baf52e743b679e3bdba7905-Abstract-Conference.html": {
    "title": "Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Ye",
      "Jennifer Collinger",
      "Leila Wehbe",
      "Robert Gaunt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe692980c5d9732cf153ce27947653a7-Abstract-Conference.html": {
    "title": "Data Quality in Imitation Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suneel Belkhale",
      "Yuchen Cui",
      "Dorsa Sadigh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe8debfd5a36ada52e038c8b2078b2ce-Abstract-Conference.html": {
    "title": "Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jameel Abdul Samadh",
      "Mohammad Hanan Gani",
      "Noor Hussein",
      "Muhammad Uzair Khattak",
      "Muhammad Muzammal Naseer",
      "Fahad Shahbaz Khan",
      "Salman H. Khan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fe90657b12193c7b52a3418bdc351807-Abstract-Conference.html": {
    "title": "SLM: A Smoothed First-Order Lagrangian Method for Structured Constrained Nonconvex Optimization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songtao Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/febe5c5c6973f713cc43bf0f7c90edbe-Abstract-Conference.html": {
    "title": "Red Teaming Deep Neural Networks with Feature Synthesis Tools",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Casper",
      "Tong Bu",
      "Yuxiao Li",
      "Jiawei Li",
      "Kevin Zhang",
      "Kaivalya Hariharan",
      "Dylan Hadfield-Menell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fed1ea8dcc2a13f3835cc854e8c8294c-Abstract-Conference.html": {
    "title": "Rehearsal Learning for Avoiding Undesired Future",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Qin",
      "Tian-Zuo Wang",
      "Zhi-Hua Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fef126561bbf9d4467dbb8d27334b8fe-Abstract-Conference.html": {
    "title": "Understanding How Consistency Works in Federated Learning via Stage-wise Relaxed Initialization",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Sun",
      "Li Shen",
      "Dacheng Tao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ff06c57ef80625386884906c2d2d2429-Abstract-Conference.html": {
    "title": "Errors-in-variables Fr\\'echet Regression with Low-rank Covariate Approximation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dogyoon Song",
      "Kyunghee Han"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ff0da832a110c6537e885cdfbac80a94-Abstract-Conference.html": {
    "title": "Coupled Reconstruction of Cortical Surfaces by Diffeomorphic Mesh Deformation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zheng",
      "Hongming Li",
      "Yong Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ff4039889b7f89635e9cbd5cefffa0d4-Abstract-Conference.html": {
    "title": "Active representation learning for general task space with applications in robotics",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifang Chen",
      "Yingbing Huang",
      "Simon S. Du",
      "Kevin G. Jamieson",
      "Guanya Shi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ff521f7570d6ed23217ba5780753a1f7-Abstract-Conference.html": {
    "title": "Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van Cuong Pham",
      "Cuong C Nguyen",
      "Trung Le",
      "Dinh Phung",
      "Gustavo Carneiro",
      "Thanh-Toan Do"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ff6540c54a847ef9114a332c101f5edc-Abstract-Conference.html": {
    "title": "Fair Graph Distillation",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhang Feng",
      "Zhimeng (Stephen) Jiang",
      "Ruiquan Li",
      "Yicheng Wang",
      "Na Zou",
      "Jiang Bian",
      "Xia Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ff703bfaf652f00ae7b609ce0da3fde2-Abstract-Conference.html": {
    "title": "Optimal testing using combined test statistics across independent studies",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lasse Vuursteen",
      "Botond Szabo",
      "Aad van der Vaart",
      "Harry van Zanten"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ff887781480973bd3cb6026feb378d1e-Abstract-Conference.html": {
    "title": "Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Ji",
      "Gen Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ff9783ec29688387d44779d67d06ef66-Abstract-Conference.html": {
    "title": "Convolutional State Space Models for Long-Range Spatiotemporal Modeling",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jimmy Smith",
      "Shalini De Mello",
      "Jan Kautz",
      "Scott Linderman",
      "Wonmin Byeon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ff99390b6e942fb1dd7023f787fb0a27-Abstract-Conference.html": {
    "title": "CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography",
    "volume": "main",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiwen Yu",
      "Xuanyu Zhang",
      "Youmin Xu",
      "Jian Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/00ba06ba5c324efdfb068865ca44cf0b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Gigastep - One Billion Steps per Second Multi-agent Reinforcement Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mathias Lechner",
      "lianhao yin",
      "Tim Seyde",
      "Tsun-Hsuan Johnson Wang",
      "Wei Xiao",
      "Ramin Hasani",
      "Joshua Rountree",
      "Daniela Rus"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/00dada608b8db212ea7d9d92b24c68de-Abstract-Datasets_and_Benchmarks.html": {
    "title": "PopSign ASL v1.0: An Isolated American Sign Language Dataset Collected via Smartphones",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thad Starner",
      "Sean Forbes",
      "Matthew So",
      "David Martin",
      "Rohit Sridhar",
      "Gururaj Deshpande",
      "Sam Sepah",
      "Sahir Shahryar",
      "Khushi Bhardwaj",
      "Tyler Kwok",
      "Daksh Sehgal",
      "Saad Hassan",
      "Bill Neubauer",
      "Sofia Vempala",
      "Alec Tan",
      "Jocelyn Heath",
      "Unnathi Kumar",
      "Priyanka Mosur",
      "Tavenner Hall",
      "Rajandeep Singh",
      "Christopher Cui",
      "Glenn Cameron",
      "Sohier Dane",
      "Garrett Tanzer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/01726ae05d72ddba3ac784a5944fa1ef-Abstract-Datasets_and_Benchmarks.html": {
    "title": "BubbleML: A Multiphase Multiphysics Dataset and Benchmarks for Machine Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheikh Md Shakeel Hassan",
      "Arthur Feeney",
      "Akash Dhruv",
      "Jihoon Kim",
      "Youngjoon Suh",
      "Jaiyoung Ryu",
      "Yoonjin Won",
      "Aparna Chandramowlishwaran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0296e17ec30fc36007edaaa2f96b5f17-Abstract-Datasets_and_Benchmarks.html": {
    "title": "AllSim: Simulating and Benchmarking Resource Allocation Policies in Multi-User Systems",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeroen Berrevoets",
      "Daniel Jarrett",
      "Alex Chan",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/02b9d1e6d1b5295a6f883969ddc1bbbd-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Wyze Rule: Federated Rule Dataset for Rule Recommendation Benchmarking",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mahdi Kamani",
      "Yuhang Yao",
      "Hanjia Lyu",
      "Zhongwei Cheng",
      "Lin Chen",
      "Liangju Li",
      "Carlee Joe-Wong",
      "Jiebo Luo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/052e22cfdd344c79634f7ec76fa03e22-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Uncovering Neural Scaling Laws in Molecular Representation Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingshuo Chen",
      "Yanqiao Zhu",
      "Jieyu Zhang",
      "Yuanqi Du",
      "Zhixun Li",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05957c194f4c77ac9d91e1374d2def6b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "$\\mathcal{M}^4$: A Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities and Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuhong Li",
      "Mengnan Du",
      "Jiamin Chen",
      "Yekun Chai",
      "Himabindu Lakkaraju",
      "Haoyi Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/05ffe69463062b7f9fb506c8351ffdd7-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiting Dong",
      "Yang Li",
      "Dongcheng Zhao",
      "Guobin Shen",
      "Yi Zeng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/066b98e63313162f6562b35962671288-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Temporal Graph Benchmark for Machine Learning on Temporal Graphs",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenyang Huang",
      "Farimah Poursafaei",
      "Jacob Danovitch",
      "Matthias Fey",
      "Weihua Hu",
      "Emanuele Rossi",
      "Jure Leskovec",
      "Michael Bronstein",
      "Guillaume Rabusseau",
      "Reihaneh Rabbany"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/09723c9f291f6056fd1885081859c186-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Synthcity: a benchmark framework for diverse use cases of tabular synthetic data",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaozhi Qian",
      "Rob Davis",
      "Mihaela van der Schaar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/09f8b2469a3d1089a7c60d9ef1983271-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Tartarus: A Benchmarking Platform for Realistic And Practical Inverse Molecular Design",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "AkshatKumar Nigam",
      "Robert Pollice",
      "Gary Tom",
      "Kjell Jorner",
      "John Willes",
      "Luca Thiede",
      "Anshul Kundaje",
      "Alan Aspuru-Guzik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0be50b4590f1c5fdf4c8feddd63c4f67-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juanhui Li",
      "Harry Shomer",
      "Haitao Mao",
      "Shenglai Zeng",
      "Yao Ma",
      "Neil Shah",
      "Jiliang Tang",
      "Dawei Yin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0c007ebef1d11fd48da6ce4f54687db6-Abstract-Datasets_and_Benchmarks.html": {
    "title": "EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongsu Bae",
      "Daeun Kyung",
      "Jaehee Ryu",
      "Eunbyeol Cho",
      "Gyubok Lee",
      "Sunjun Kweon",
      "Jungwoo Oh",
      "Lei Ji",
      "Eric Chang",
      "Tackeun Kim",
      "Edward Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0c26a501df8fb919a0350e2df06b5d39-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ScenarioNet: Open-Source Platform for Large-Scale Traffic Scenario Simulation and Modeling",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanyi Li",
      "Zhenghao (Mark) Peng",
      "Lan Feng",
      "Zhizheng Liu",
      "Chenda Duan",
      "Wenjie Mo",
      "Bolei Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0dc91de822b71c66a7f54fa121d8cbb9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Building Socio-culturally Inclusive Stereotype Resources with Community Engagement",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunipa Dev",
      "Jaya Goyal",
      "Dinesh Tewari",
      "Shachi Dave",
      "Vinodkumar Prabhakaran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0e735e4b4f07de483cbe250130992726-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Yan",
      "Yong Zhang",
      "Xinhang Yuan",
      "Siwei Lyu",
      "Baoyuan Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0eb82171240776fe19da498bef3b1abe-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OpenProteinSet: Training data for structural biology at scale",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gustaf Ahdritz",
      "Nazim Bouatta",
      "Sachin Kadyan",
      "Lukas Jarosch",
      "Dan Berenberg",
      "Ian Fisk",
      "Andrew Watkins",
      "Stephen Ra",
      "Richard Bonneau",
      "Mohammed AlQuraishi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0f12c9975ff4f2e44a5a26ef01b0b249-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Datasets and Benchmarks for Nanophotonic Structure and Parametric Design Simulations",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungtaek Kim",
      "Mingxuan Li",
      "Oliver Hinder",
      "Paul Leu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/0f2cd3d09a132757555b602e2dd43784-Abstract-Datasets_and_Benchmarks.html": {
    "title": "NIS3D: A Completely Annotated Benchmark for Dense 3D Nuclei Image Segmentation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zheng",
      "Cheng Peng",
      "Zeyuan Hou",
      "Boyu Lyu",
      "Mengfan Wang",
      "Xuelong Mi",
      "Shuoxuan Qiao",
      "Yinan Wan",
      "Guoqiang Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/112db88215e25b3ae2750e9eefcded94-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DataPerf: Benchmarks for Data-Centric AI Development",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Mazumder",
      "Colby Banbury",
      "Xiaozhe Yao",
      "Bojan Karlaš",
      "William Gaviria Rojas",
      "Sudnya Diamos",
      "Greg Diamos",
      "Lynn He",
      "Alicia Parrish",
      "Hannah Rose Kirk",
      "Jessica Quaye",
      "Charvi Rastogi",
      "Douwe Kiela",
      "David Jurado",
      "David Kanter",
      "Rafael Mosquera",
      "Will Cukierski",
      "Juan Ciro",
      "Lora Aroyo",
      "Bilge Acun",
      "Lingjiao Chen",
      "Mehul Raje",
      "Max Bartolo",
      "Evan Sabri Eyuboglu",
      "Amirata Ghorbani",
      "Emmett Goodman",
      "Addison Howard",
      "Oana Inel",
      "Tariq Kane",
      "Christine R. Kirkpatrick",
      "D. Sculley",
      "Tzu-Sheng Kuo",
      "Jonas W. Mueller",
      "Tristan Thrush",
      "Joaquin Vanschoren",
      "Margaret Warren",
      "Adina Williams",
      "Serena Yeung",
      "Newsha Ardalani",
      "Praveen Paritosh",
      "Ce Zhang",
      "James Y. Zou",
      "Carole-Jean Wu",
      "Cody Coleman",
      "Andrew Ng",
      "Peter Mattson",
      "Vijay Janapa Reddi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/117c5c8622b0d539f74f6d1fb082a2e9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Zhang",
      "Mahani Aljunied",
      "Chang Gao",
      "Yew Ken Chia",
      "Lidong Bing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1190733f217404edc8a7f4e15a57f301-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OpenAGI: When LLM Meets Domain Experts",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingqiang Ge",
      "Wenyue Hua",
      "Kai Mei",
      "jianchao ji",
      "Juntao Tan",
      "Shuyuan Xu",
      "Zelong Li",
      "Yongfeng Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/11e3e0f1b29dcd31bd0952bfc1357f68-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeid Alavi Naeini",
      "Raeid Saqur",
      "Mozhgan Saeidi",
      "John Giorgi",
      "Babak Taati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/14f656f21d09a4114666f60a45aab1aa-Abstract-Datasets_and_Benchmarks.html": {
    "title": "NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anwar Said",
      "Roza Bayrak",
      "Tyler Derr",
      "Mudassir Shabbir",
      "Daniel Moyer",
      "Catie Chang",
      "Xenofon Koutsoukos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1585da86b5a3c4fb15520a2b3682051f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized Codebase",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiuyu Wang",
      "Zifan Shi",
      "Kecheng Zheng",
      "Yinghao Xu",
      "Sida Peng",
      "Yujun Shen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/15c9f64ec172b046470d2a4d2b7669fc-Abstract-Datasets_and_Benchmarks.html": {
    "title": "RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhecheng Yuan",
      "Sizhe Yang",
      "Pu Hua",
      "Can Chang",
      "Kaizhe Hu",
      "Huazhe Xu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/16b14e3f288f076e0ca73bdad6405f77-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ChessGPT: Bridging Policy Learning and Language Modeling",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xidong Feng",
      "Yicheng Luo",
      "Ziyan Wang",
      "Hongrui Tang",
      "Mengyue Yang",
      "Kun Shao",
      "David Mguni",
      "Yali Du",
      "Jun Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/170035f97007fdfa665880107b56f384-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Learning to Taste: A Multimodal Wine Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thoranna Bender",
      "Simon Sørensen",
      "Alireza Kashani",
      "Kristjan Eldjarn Hjorleifsson",
      "Grethe Hyldig",
      "Søren Hauberg",
      "Serge Belongie",
      "Frederik Warburg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1838feeb71c4b4ea524d0df2f7074245-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cole Gulino",
      "Justin Fu",
      "Wenjie Luo",
      "George Tucker",
      "Eli Bronstein",
      "Yiren Lu",
      "Jean Harb",
      "Xinlei Pan",
      "Yan Wang",
      "Xiangyu Chen",
      "John Co-Reyes",
      "Rishabh Agarwal",
      "Rebecca Roelofs",
      "Yao Lu",
      "Nico Montali",
      "Paul Mougin",
      "Zoey Yang",
      "Brandyn White",
      "Aleksandra Faust",
      "Rowan McAllister",
      "Dragomir Anguelov",
      "Benjamin Sapp"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/18ef499ee57c4822e1e3ea9b9948af18-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soukayna Mouatadid",
      "Paulo Orenstein",
      "Genevieve Flaspohler",
      "Miruna Oprescu",
      "Judah Cohen",
      "Franklyn Wang",
      "Sean Knight",
      "Maria Geogdzhayeva",
      "Sam Levang",
      "Ernest Fraenkel",
      "Lester Mackey"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1909ac72220bf5016b6c93f08b66cf36-Abstract-Datasets_and_Benchmarks.html": {
    "title": "RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongwei Pan",
      "Long Zhuo",
      "Jingtan Piao",
      "Huiwen Luo",
      "Wei Cheng",
      "Yuxin WANG",
      "Siming Fan",
      "Shengqi Liu",
      "Lei Yang",
      "Bo Dai",
      "Ziwei Liu",
      "Chen Change Loy",
      "Chen Qian",
      "Wayne Wu",
      "Dahua Lin",
      "Kwan-Yee Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/193df57a2366d032fb18dcac0698d09a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Jin",
      "Haitao Mao",
      "Zheng Li",
      "Haoming Jiang",
      "Chen Luo",
      "Hongzhi Wen",
      "Haoyu Han",
      "Hanqing Lu",
      "Zhengyang Wang",
      "Ruirui Li",
      "Zhen Li",
      "Monica Cheng",
      "Rahul Goutam",
      "Haiyang Zhang",
      "Karthik Subbian",
      "Suhang Wang",
      "Yizhou Sun",
      "Jiliang Tang",
      "Bing Yin",
      "Xianfeng Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/19a260641ebaf68d412f427e591bb74a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elysia Smyers",
      "Sydney Katz",
      "Anthony Corso",
      "Mykel J Kochenderfer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1b88e65f737256d437e56764d39ba06d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DVSOD: RGB-D Video Salient Object Detection",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingjing Li",
      "Wei Ji",
      "Size Wang",
      "Wenbo Li",
      "Li cheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1be3843e534ee06d3a70c7f62b983b31-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Wang",
      "Jing Zhang",
      "Bo Du",
      "Minqiang Xu",
      "Lin Liu",
      "Dacheng Tao",
      "Liangpei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1c6bed78d3813886d3d72595dbecb80b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanrong Zhu",
      "Jack Hessel",
      "Anas Awadalla",
      "Samir Yitzhak Gadre",
      "Jesse Dodge",
      "Alex Fang",
      "Youngjae Yu",
      "Ludwig Schmidt",
      "William Yang Wang",
      "Yejin Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1e5fa672b2c35744cbcfacb2e77f1cb0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Revisiting the Evaluation of Image Synthesis with GANs",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "mengping yang",
      "Ceyuan Yang",
      "Yichi Zhang",
      "Qingyan Bai",
      "Yujun Shen",
      "Bo Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/1ec408df112bc9b186d7b8fe0ada902a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Degraded Polygons Raise Fundamental Questions of Neural Network Perception",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonard Tang",
      "Dan Ley"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2052b3e0617ecb2ce9474a6feaf422b3-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Humans in Kitchens: A Dataset for Multi-Person Human Motion Forecasting with Scene Context",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Tanke",
      "Oh-Hun Kwon",
      "Felix B Mueller",
      "Andreas Doering",
      "Jürgen Gall"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/215a55741fbe4baad173468f93336a7d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "How to Data in Datathons",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Mougan",
      "Richard Plant",
      "Clare Teng",
      "Marya Bazzi",
      "Alvaro Cabrejas Egea",
      "Ryan Chan",
      "David Salvador Jasin",
      "Martin Stoffel",
      "Kirstie Whitaker",
      "JULES MANSER"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/232eee8ef411a0a316efa298d7be3c2b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyuan Ma",
      "Hongshu Guo",
      "Jiacheng Chen",
      "Zhenrui Li",
      "Guojun Peng",
      "Yue-Jiao Gong",
      "Yining Ma",
      "Zhiguang Cao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/23e3d86c9a19d0caf2ec997e73dfcfbd-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruth Dannenfelser",
      "Jeffrey Zhong",
      "Ran Zhang",
      "Vicky Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2432e9646556f3a98dc78c1f4a10481b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "How hard are computer vision datasets? Calibrating dataset difficulty to viewing time",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Mayo",
      "Jesse Cummings",
      "Xinyu Lin",
      "Dan Gutfreund",
      "Boris Katz",
      "Andrei Barbu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2614947a25d7c435bcd56c51958ddcb1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongang Cai",
      "Wanqi Yin",
      "Ailing Zeng",
      "CHEN WEI",
      "Qingping SUN",
      "Wang Yanjun",
      "Hui En Pang",
      "Haiyi Mei",
      "Mingyuan Zhang",
      "Lei Zhang",
      "Chen Change Loy",
      "Lei Yang",
      "Ziwei Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/29c80c549ed67ddd7259559c1bb07c1b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "WCLD: Curated Large Dataset of Criminal Cases from Wisconsin Circuit Courts",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elliott Ash",
      "Naman Goel",
      "Nianyun Li",
      "Claudia Marangon",
      "Peiyao Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2b9efb085d3829a2aadffab63ba206de-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Understanding Social Reasoning in Language Models with Language Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kanishk Gandhi",
      "Jan-Philipp Fraenken",
      "Tobias Gerstenberg",
      "Noah Goodman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2d421cd0e763f9f01958a30bace955bf-Abstract-Datasets_and_Benchmarks.html": {
    "title": "URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Kirchhof",
      "Bálint Mucsányi",
      "Seong Joon Oh",
      "Dr. Enkelejda Kasneci"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/2ebf43d20e5933ab6d98225bbb908ade-Abstract-Datasets_and_Benchmarks.html": {
    "title": "REASONER: An Explainable Recommendation Dataset with Comprehensive Labeling Ground Truths",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Chen",
      "Jingsen Zhang",
      "Lei Wang",
      "Quanyu Dai",
      "Zhenhua Dong",
      "Ruiming Tang",
      "Rui Zhang",
      "Li Chen",
      "Xin Zhao",
      "Ji-Rong Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3112ee706d21d734c15532c1239773e1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Data Portraits: Recording Foundation Model Training Data",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Marone",
      "Benjamin Van Durme"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/32093649cbbcff773d9a991d8c30a7fe-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DynaDojo: An Extensible Platform for Benchmarking Scaling in Dynamical System Identification",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Logan M Bhamidipaty",
      "Tommy Bruzzese",
      "Caryn Tran",
      "Rami Ratl Mrad",
      "Maxinder S. Kanwal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/353ca88f722cdd0c481b999428ae113a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "FLAIR : a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anatol Garioud",
      "Nicolas Gonthier",
      "Loic Landrieu",
      "Apolline De Wit",
      "Marion Valette",
      "Marc Poupée",
      "Sebastien Giordano",
      "boris Wattrelos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/37d00f567a18b478065f1a91b95622a0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A Comprehensive Study on Text-attributed Graphs: Benchmarking and Rethinking",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yan",
      "Chaozhuo Li",
      "Ruosong Long",
      "Chao Yan",
      "Jianan Zhao",
      "Wenwen Zhuang",
      "Jun Yin",
      "Peiyan Zhang",
      "Weihao Han",
      "Hao Sun",
      "Weiwei Deng",
      "Qi Zhang",
      "Lichao Sun",
      "Xing Xie",
      "Senzhang Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/396beafa6feba781a7114780e6837253-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A Dataset of Relighted 3D Interacting Hands",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongsik Moon",
      "Shunsuke Saito",
      "Weipeng Xu",
      "Rohan Joshi",
      "Julia Buffalini",
      "Harley Bellan",
      "Nicholas Rosen",
      "Jesse Richardson",
      "Mallorie Mize",
      "Philippe De Bree",
      "Tomas Simon",
      "Bo Peng",
      "Shubham Garg",
      "Kevyn McPhail",
      "Takaaki Shiratori"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/39736af1b9d87a1fddad9f84a6bcf64c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "INSPECT: A Multimodal Dataset for Patient Outcome Prediction of Pulmonary Embolisms",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shih-Cheng Huang",
      "Zepeng Huo",
      "Ethan Steinberg",
      "Chia-Chun Chiang",
      "Curtis Langlotz",
      "Matthew Lungren",
      "Serena Yeung",
      "Nigam Shah",
      "Jason Fries"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/39d02e8e23bafadd7cd405f2281bc05c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SG×P : A Sorghum Genotype × Phenotype Prediction Dataset and Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Zhang",
      "Robert Pless",
      "Nadia Shakoor",
      "Austin Carnahan",
      "Abby Stylianou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/39f8ef62e061042cca8c8f46d7e0e31b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OpenGSL: A Comprehensive Benchmark for Graph Structure Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Zhiyao",
      "Sheng Zhou",
      "Bochao Mao",
      "Xuanyi Zhou",
      "Jiawei Chen",
      "Qiaoyu Tan",
      "Daochen Zha",
      "Yan Feng",
      "Chun Chen",
      "Can Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a48b0eaba26ba862220a307a9edb0bb-Abstract-Datasets_and_Benchmarks.html": {
    "title": "NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Hu",
      "Lin Wang",
      "Siyuan Yan",
      "Don Ma",
      "Qingli Ren",
      "Peng Xia",
      "Wei Feng",
      "Peibo Duan",
      "Lie Ju",
      "Zongyuan Ge"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3a71ee306d6991f2f87dd414e0bdf851-Abstract-Datasets_and_Benchmarks.html": {
    "title": "RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MD WAHIDUZZAMAN KHAN",
      "Hongwei Sheng",
      "Hu Zhang",
      "Heming Du",
      "Sen Wang",
      "Minas Coroneo",
      "Farshid Hajati",
      "Sahar Shariflou",
      "Michael Kalloniatis",
      "Jack Phu",
      "Ashish Agar",
      "Zi Huang",
      "S.Mojtaba Golzan",
      "Xin Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3baf7a39d07e9f4f1e258a412df94521-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianfei Yang",
      "He Huang",
      "Yunjiao Zhou",
      "Xinyan Chen",
      "Yuecong Xu",
      "Shenghai Yuan",
      "Han Zou",
      "Chris Xiaoxuan Lu",
      "Lihua Xie"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3be31c1a2fdcb7b748c53c3f4cb0e9d2-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Zhang",
      "Bingqiao Luo",
      "Shengliang Lu",
      "Bingsheng He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3be60b4a739b95a07a944a1a2c41e05e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "CMMA: Benchmarking Multi-Affection Detection in Chinese Multi-Modal Conversations",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yazhou Zhang",
      "Yang Yu",
      "Qing Guo",
      "Benyou Wang",
      "Dongming Zhao",
      "Sagar Uprety",
      "Dawei Song",
      "Qiuchi Li",
      "Jing Qin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3c0a4c8c236144f1b99b7e1531debe9c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huijie Wang",
      "Tianyu Li",
      "Yang Li",
      "Li Chen",
      "Chonghao Sima",
      "Zhenbo Liu",
      "Bangjun Wang",
      "Peijin Jia",
      "Yuting Wang",
      "Shengyin Jiang",
      "Feng Wen",
      "Hang Xu",
      "Ping Luo",
      "Junchi Yan",
      "Wei Zhang",
      "Hongyang Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3c4688b6a76f25f2311daa0d75a58f1a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Building the Bridge of Schrödinger: A Continuous Entropic Optimal Transport Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Gushchin",
      "Alexander Kolesov",
      "Petr Mokrov",
      "Polina Karpikova",
      "Andrei Spiridonov",
      "Evgeny Burnaev",
      "Alexander Korotin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3c557a3d6a48cc99444f85e924c66753-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Safety Gymnasium: A Unified Safe Reinforcement Learning Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Ji",
      "Borong Zhang",
      "Jiayi Zhou",
      "Xuehai Pan",
      "Weidong Huang",
      "Ruiyang Sun",
      "Yiran Geng",
      "Yifan Zhong",
      "Josef Dai",
      "Yaodong Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3d0758f0b95e19abc68c1c8070d36510-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Alexa Arena: A User-Centric Interactive Platform for Embodied AI",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiaozi Gao",
      "Govind Thattai",
      "Suhaila Shakiah",
      "Xiaofeng Gao",
      "Shreyas Pansare",
      "Vasu Sharma",
      "Gaurav Sukhatme",
      "Hangjie Shi",
      "Bofei Yang",
      "Desheng Zhang",
      "Lucy Hu",
      "Karthika Arumugam",
      "Shui Hu",
      "Matthew Wen",
      "Dinakar Guthy",
      "Shunan Chung",
      "Rohan Khanna",
      "Osman Ipek",
      "Leslie Ball",
      "Kate Bland",
      "Heather Rocker",
      "Michael Johnston",
      "Reza Ghanadan",
      "Dilek Hakkani-Tur",
      "Prem Natarajan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3ecca655ac67685fdc2155da0eceda6b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "CHAMMI: A benchmark for channel-adaptive models in microscopy imaging",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitong Sam Chen",
      "Chau Pham",
      "Siqi Wang",
      "Michael Doron",
      "Nikita Moshkov",
      "Bryan Plummer",
      "Juan C. Caicedo"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/3f17bf868966df01ca125e5bbc9ee24e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Emami",
      "Abhijeet Sahu",
      "Peter Graf"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/40daf2a00278c4bea1b26cd4c8a654f8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "BioMassters: A Benchmark Dataset for Forest Biomass Estimation using Multi-modal Satellite Time-series",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Nascetti",
      "Ritu Yadav",
      "Kirill Brodt",
      "Qixun Qu",
      "Hongwei Fan",
      "Yuri Shendryk",
      "Isha Shah",
      "Christine Chung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/412732f172bdd5ad0efde2fafa110700-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Bone Shape Reconstruction",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahesh Shakya",
      "Bishesh Khanal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/42f225509e8263e2043c9d834ccd9a2b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Into the LAION's Den: Investigating Hate in Multimodal Datasets",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abeba Birhane",
      "vinay prabhu",
      "Sanghyun Han",
      "Vishnu Boddeti",
      "Sasha Luccioni"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/43119db5d59f07cc08fca7ba6820179a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingdong Kong",
      "Shaoyuan Xie",
      "Hanjiang Hu",
      "Lai Xing Ng",
      "Benoit Cottereau",
      "Wei Tsang Ooi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4441469427094f8873d0fecb0c4e1cee-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyu Yang",
      "Aidan Swope",
      "Alex Gu",
      "Rahul Chalamala",
      "Peiyang Song",
      "Shixing Yu",
      "Saad Godil",
      "Ryan J Prenger",
      "Animashree Anandkumar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/44a6769fe6c695f8dfb347c649f7c9f0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Kaltenborn",
      "Charlotte Lange",
      "Venkatesh Ramesh",
      "Philippe Brouillard",
      "Yaniv Gurwicz",
      "Chandni Nagda",
      "Jakob Runge",
      "Peer Nowack",
      "David Rolnick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/44e3a3115ca26e5127851acd0cedd0d9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Data-Driven Network Neuroscience: On Data Collection and Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxing Xu",
      "Yunhan Yang",
      "David Huang",
      "Sophi Shilpa Gururajapathy",
      "Yiping Ke",
      "Miao Qiao",
      "Alan Wang",
      "Haribalan Kumar",
      "Josh McGeown",
      "Eryn Kwon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/45e604a3e33d10fba508e755faa72345-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Improving multimodal datasets with image captioning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thao Nguyen",
      "Samir Yitzhak Gadre",
      "Gabriel Ilharco",
      "Sewoong Oh",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/45fbcc01349292f5e059a0b8b02c8c3f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ClimSim: A large multi-scale dataset for hybrid physics-ML climate emulation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungduk Yu",
      "Walter Hannah",
      "Liran Peng",
      "Jerry Lin",
      "Mohamed Aziz Bhouri",
      "Ritwik Gupta",
      "Björn Lütjens",
      "Justus C. Will",
      "Gunnar Behrens",
      "Julius Busecke",
      "Nora Loose",
      "Charles Stern",
      "Tom Beucler",
      "Bryce Harrop",
      "Benjamin Hillman",
      "Andrea Jenney",
      "Savannah L. Ferretti",
      "Nana Liu",
      "Animashree Anandkumar",
      "Noah Brenowitz",
      "Veronika Eyring",
      "Nicholas Geneva",
      "Pierre Gentine",
      "Stephan Mandt",
      "Jaideep Pathak",
      "Akshay Subramaniam",
      "Carl Vondrick",
      "Rose Yu",
      "Laure Zanna",
      "Tian Zheng",
      "Ryan Abernathey",
      "Fiaz Ahmed",
      "David Bader",
      "Pierre Baldi",
      "Elizabeth Barnes",
      "Christopher Bretherton",
      "Peter Caldwell",
      "Wayne Chuang",
      "Yilun Han",
      "YU HUANG",
      "Fernando Iglesias-Suarez",
      "Sanket Jantre",
      "Karthik Kashinath",
      "Marat Khairoutdinov",
      "Thorsten Kurth",
      "Nicholas Lutsko",
      "Po-Lun Ma",
      "Griffin Mooers",
      "J. David Neelin",
      "David Randall",
      "Sara Shamekh",
      "Mark Taylor",
      "Nathan Urban",
      "Janni Yuval",
      "Guang Zhang",
      "Mike Pritchard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/464fefa022aaefc85d901317bbf13f85-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Qu",
      "Boyuan Wang",
      "Jianzhun Shao",
      "Yuhang Jiang",
      "Chen Chen",
      "Zhenbin Ye",
      "Liu Linc",
      "Yang Feng",
      "Lin Lai",
      "Hongyang Qin",
      "Minwen Deng",
      "Juchao Zhuo",
      "Deheng Ye",
      "Qiang Fu",
      "YANG GUANG",
      "Wei Yang",
      "Lanxiao Huang",
      "Xiangyang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4786c0d1b9687a841bc579b0b8b01b8e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SynMob: Creating High-Fidelity Synthetic GPS Trajectory Dataset for Urban Mobility Analysis",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanshao Zhu",
      "Yongchao Ye",
      "Ying Wu",
      "Xiangyu Zhao",
      "James Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4962a23916103301b27bde29a27642e8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wojciech Kusa",
      "Oscar E. Mendoza",
      "Matthias Samwald",
      "Petr Knoth",
      "Allan Hanbury"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4a47dd69242d5af908cdd5d51c971cbf-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beichen Zhang",
      "Kun Zhou",
      "Xilin Wei",
      "Xin Zhao",
      "Jing Sha",
      "Shijin Wang",
      "Ji-Rong Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4aa8891583f07ae200ba07843954caeb-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A Toolkit for Reliable Benchmarking and Research in Multi-Objective Reinforcement Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Felten",
      "Lucas N. Alegre",
      "Ann Nowe",
      "Ana Bazzan",
      "El Ghazali Talbi",
      "Grégoire Danoy",
      "Bruno C. da Silva"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4aa8d18aad014fb3d0076e0afd2e3b2e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "$\\mathbf{\\mathbb{E}^{FWI}}$: Multiparameter Benchmark Datasets for Elastic Full Waveform Inversion of Geophysical Properties",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihang Feng",
      "Hanchen Wang",
      "Chengyuan Deng",
      "Yinan Feng",
      "Yanhua Liu",
      "Min Zhu",
      "Peng Jin",
      "Yinpeng Chen",
      "Youzuo Lin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4b175d846fb008d540d233c188379ff9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Yang",
      "Akshara Prabhakar",
      "Karthik Narasimhan",
      "Shunyu Yao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4c0986bd04d747745beba3752bdf4d9d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Hyper-Skin: A Hyperspectral Dataset for Reconstructing Facial Skin-Spectra from RGB Images",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pai Chet Ng",
      "Zhixiang Chi",
      "Yannick Verdie",
      "Juwei Lu",
      "Konstantinos N  Plataniotis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4c8f197b24e9b05d22028c2de16a45d2-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew McDermott",
      "Bret Nestor",
      "Peniel Argaw",
      "Isaac S Kohane"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4d6a000c216974f59e597bc878cd6325-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SiT Dataset: Socially Interactive Pedestrian Trajectory Dataset for Social Navigation Robots",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jong Wook Bae",
      "Jungho Kim",
      "Junyong Yun",
      "Changwon Kang",
      "Jeongseon Choi",
      "Chanhyeok Kim",
      "Junho Lee",
      "Jungwook Choi",
      "Jun Won Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4dbb61cb68671edc4ca3712d70083b9f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Ji",
      "Mickel Liu",
      "Josef Dai",
      "Xuehai Pan",
      "Chi Zhang",
      "Ce Bian",
      "Boyuan Chen",
      "Ruiyang Sun",
      "Yizhou Wang",
      "Yaodong Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4e3378a8e80af4ffc456c4fa13d46550-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Exploring Why Object Recognition Performance Degrades Across Income Levels and Geographies with Factor Annotations",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laura Gustafson",
      "Megan Richards",
      "Melissa Hall",
      "Caner Hazirbas",
      "Diane Bouchacourt",
      "Mark Ibrahim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4ea14e6090343523ddcd5d3ca449695f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Hu",
      "Dailing Zhang",
      "wu meiqi",
      "Xiaokun Feng",
      "Xuchen Li",
      "Xin Zhao",
      "Kaiqi Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4eb33c53ed5b14ce9028309431f565cc-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Does progress on ImageNet transfer to real-world datasets?",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Fang",
      "Simon Kornblith",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/4f8e27f6036c1d8b4a66b5b3a947dd7b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Lin",
      "Ailing Zeng",
      "Shunlin Lu",
      "Yuanhao Cai",
      "Ruimao Zhang",
      "Haoqian Wang",
      "Lei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/505df5ea30f630661074145149274af0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Lu",
      "Di Huang",
      "LEI BAI",
      "Jingjing Qu",
      "Chengyue Wu",
      "Xihui Liu",
      "Wanli Ouyang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/506630e4a43bb9d64a49f98b9ba934e9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "FORB: A Flat Object Retrieval Benchmark for Universal Image Embedding",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengxiang Wu",
      "Siman Wang",
      "Kevin Dela Rosa",
      "Derek Hu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/52c1ce1a0eaf61e8b6e3a899c1b9c61f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "WordScape: a Pipeline to extract multilingual, visually rich Documents with Layout Annotations from Web Crawl Data",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maurice Weber",
      "Carlo Siebenschuh",
      "Rory Butler",
      "Anton Alexandrov",
      "Valdemar Thanner",
      "Georgios Tsolakis",
      "Haris Jabbar",
      "Ian Foster",
      "Bo Li",
      "Rick Stevens",
      "Ce Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/52e78a95d8baa6d082fb2d0e9499b661-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SARAMIS: Simulation Assets for Robotic Assisted and Minimally Invasive Surgery",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Montana-Brown",
      "Shaheer U. Saeed",
      "Ahmed Abdulaal",
      "Thomas Dowrick",
      "Yakup Kilic",
      "Sophie Wilkinson",
      "Jack Gao",
      "Meghavi Mashar",
      "Chloe He",
      "Alkisti Stavropoulou",
      "Emma Thomson",
      "Zachary MC Baum",
      "Simone Foti",
      "Brian Davidson",
      "Yipeng Hu",
      "Matthew Clarkson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/543d4e171150cb931f1d401cacc3d7af-Abstract-Datasets_and_Benchmarks.html": {
    "title": "EPIC Fields: Marrying 3D Geometry and Video Understanding",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vadim Tschernezki",
      "Ahmad Darkhalil",
      "Zhifan Zhu",
      "David Fouhey",
      "Iro Laina",
      "Diane Larlus",
      "Dima Damen",
      "Andrea Vedaldi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/548a41b9cac6f50dccf7e63e9e1b1b9b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenfei Yin",
      "Jiong Wang",
      "Jianjian Cao",
      "Zhelun Shi",
      "Dingning Liu",
      "Mukai Li",
      "Xiaoshui Huang",
      "Zhiyong Wang",
      "Lu Sheng",
      "LEI BAI",
      "Jing Shao",
      "Wanli Ouyang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/54bf430f5d3090502ea021941e9cb18e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dragos Georgian Corlatescu",
      "Alexandru Dinu",
      "Mihaela Petruta Gaman",
      "Paul Sumedrea"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5503389dbe070cdae9b48086c4996a59-Abstract-Datasets_and_Benchmarks.html": {
    "title": "VisIT-Bench: A Dynamic Benchmark for Evaluating Instruction-Following Vision-and-Language Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yonatan Bitton",
      "Hritik Bansal",
      "Jack Hessel",
      "Rulin Shao",
      "Wanrong Zhu",
      "Anas Awadalla",
      "Josh Gardner",
      "Rohan Taori",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/56332d41d55ad7ad8024aac625881be7-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DataComp: In search of the next generation of multimodal datasets",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samir Yitzhak Gadre",
      "Gabriel Ilharco",
      "Alex Fang",
      "Jonathan Hayase",
      "Georgios Smyrnis",
      "Thao Nguyen",
      "Ryan Marten",
      "Mitchell Wortsman",
      "Dhruba Ghosh",
      "Jieyu Zhang",
      "Eyal Orgad",
      "Rahim Entezari",
      "Giannis Daras",
      "Sarah Pratt",
      "Vivek Ramanujan",
      "Yonatan Bitton",
      "Kalyani Marathe",
      "Stephen Mussmann",
      "Richard Vencu",
      "Mehdi Cherti",
      "Ranjay Krishna",
      "Pang Wei W. Koh",
      "Olga Saukh",
      "Alexander J. Ratner",
      "Shuran Song",
      "Hannaneh Hajishirzi",
      "Ali Farhadi",
      "Romain Beaumont",
      "Sewoong Oh",
      "Alex Dimakis",
      "Jenia Jitsev",
      "Yair Carmon",
      "Vaishaal Shankar",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57a95cd3898bf4912269848a01f53620-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ChimpACT: A Longitudinal Dataset for Understanding Chimpanzee Behaviors",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxuan Ma",
      "Stephan Kaufhold",
      "Jiajun Su",
      "Wentao Zhu",
      "Jack Terwilliger",
      "Andres Meza",
      "Yixin Zhu",
      "Federico Rossano",
      "Yizhou Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/57bb67dbe17bfb660c8c63d089ea05b9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "trajdata: A Unified Interface to Multiple Human Trajectory Datasets",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boris Ivanovic",
      "Guanyu Song",
      "Igor Gilitschenski",
      "Marco Pavone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/58168e8a92994655d6da3939e7cc0918-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Mathematical Capabilities of ChatGPT",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Frieder",
      "Luca Pinchetti",
      " Chevalier",
      "Ryan-Rhys Griffiths",
      "Tommaso Salvatori",
      "Thomas Lukasiewicz",
      "Philipp Petersen",
      "Julius Berner"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/58af908d6293810f1a29e69bf723dc48-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeongbin Kim",
      "Gautam Singh",
      "Junyeong Park",
      "Caglar Gulcehre",
      "Sungjin Ahn"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5950bf290a1570ea401bf98882128160-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Mind2Web: Towards a Generalist Agent for the Web",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Deng",
      "Yu Gu",
      "Boyuan Zheng",
      "Shijie Chen",
      "Sam Stevens",
      "Boshi Wang",
      "Huan Sun",
      "Yu Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5a0f92efaa8f0cd67992caf6b2fa2bac-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Video Timeline Modeling For News Story Understanding",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Liu",
      "Mingda Zhang",
      "Jialu Liu",
      "Hanjun Dai",
      "Ming-Hsuan Yang",
      "Shuiwang Ji",
      "Zheyun Feng",
      "Boqing Gong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5abcdf8ecdcacba028c6662789194572-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunyuan Li",
      "Cliff Wong",
      "Sheng Zhang",
      "Naoto Usuyama",
      "Haotian Liu",
      "Jianwei Yang",
      "Tristan Naumann",
      "Hoifung Poon",
      "Jianfeng Gao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5b047c7d862059a5df623c1ce2982fca-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OpenDataVal: a Unified Benchmark for Data Valuation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Jiang",
      "Weixin Liang",
      "James Y. Zou",
      "Yongchan Kwon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5c61452daca5f0c260e683b317d13a3f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dave Uthus",
      "Garrett Tanzer",
      "Manfred Georg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5d9571470bb750f0e2325a030016f63f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Micah Goldblum",
      "Hossein Souri",
      "Renkun Ni",
      "Manli Shu",
      "Viraj Prabhu",
      "Gowthami Somepalli",
      "Prithvijit Chattopadhyay",
      "Mark Ibrahim",
      "Adrien Bardes",
      "Judy Hoffman",
      "Rama Chellappa",
      "Andrew G. Wilson",
      "Tom Goldstein"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5eaafd67434a4cfb1cf829722c65f184-Abstract-Datasets_and_Benchmarks.html": {
    "title": "GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianheng Tang",
      "Fengrui Hua",
      "Ziqi Gao",
      "Peilin Zhao",
      "Jia Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f09bfe6730e9627a9f800d01a8ad5cd-Abstract-Datasets_and_Benchmarks.html": {
    "title": "StressID: a Multimodal Dataset for Stress Identification",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hava Chaptoukaev",
      "Valeriya Strizhkova",
      "Michele Panariello",
      "Bianca Dalpaos",
      "Aglind Reka",
      "Valeria Manera",
      "Susanne Thümmler",
      "Esma ISMAILOVA",
      "Nicholas W.",
      "francois bremond",
      "Massimiliano Todisco",
      "Maria A Zuluaga",
      "Laura M. Ferrari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/5f38404edff6f3f642d6fa5892479c42-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Realistic Synthetic Financial Transactions for Anti-Money Laundering Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Altman",
      "Jovan Blanuša",
      "Luc von Niederhäusern",
      "Beni Egressy",
      "Andreea Anghel",
      "Kubilay Atasu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/604b9fa9e1c16284e6517d923cf9ff20-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LithoBench: Benchmarking AI Computational Lithography for Semiconductor Manufacturing",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Su Zheng",
      "Haoyu Yang",
      "Binwu Zhu",
      "Bei Yu",
      "Martin Wong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/60bc87f3cf5257579435d92ec12c761b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "GSLB: The Graph Structure Learning Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixun Li",
      "Xin Sun",
      "Yifan Luo",
      "Yanqiao Zhu",
      "Dingshuo Chen",
      "Yingtao Luo",
      "Xiangxin Zhou",
      "Qiang Liu",
      "Shu Wu",
      "Liang Wang",
      "Jeffrey Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/60d25b3210c92f5ba2002a8e1f1adf1c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Consensus and Subjectivity of Skin Tone Annotation for ML Fairness",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Candice Schumann",
      "Femi Olanubi",
      "Auriel Wright",
      "Ellis Monk",
      "Courtney Heldreth",
      "Susanna Ricco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/611b896d447df43c898062358df4c114-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Real3D-AD: A Dataset of Point Cloud Anomaly Detection",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Liu",
      "Guoyang Xie",
      "Ruitao Chen",
      "Xinpeng Li",
      "Jinbao Wang",
      "Yong Liu",
      "Chengjie Wang",
      "Feng Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/617ff5271b2b41dfb217a3b0f1b3d1be-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LoRA: A Logical Reasoning Augmented Dataset for Visual Question Answering",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingying Gao",
      "Qi Wu",
      "Alan Blair",
      "Maurice Pagnucco"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/62a24b69b820d30e9e5ad4f15ff7bf72-Abstract-Datasets_and_Benchmarks.html": {
    "title": "QATCH: Benchmarking SQL-centric tasks with Table Representation Learning Models on Your Data",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Papicchio",
      "Paolo Papotti",
      "Luca Cagliero"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/62d2cec62b7fd46dd35fa8f2d4aeb52d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "CORL: Research-oriented Deep Offline Reinforcement Learning Library",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Tarasov",
      "Alexander Nikulin",
      "Dmitry Akimov",
      "Vladislav Kurenkov",
      "Sergey Kolesnikov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/63461de0b4cb760fc498e85b18a7fe81-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-Yu Hsieh",
      "Jieyu Zhang",
      "Zixian Ma",
      "Aniruddha Kembhavi",
      "Ranjay Krishna"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/63cb9921eecf51bfad27a99b2c53dd6d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boxin Wang",
      "Weixin Chen",
      "Hengzhi Pei",
      "Chulin Xie",
      "Mintong Kang",
      "Chenhui Zhang",
      "Chejian Xu",
      "Zidi Xiong",
      "Ritik Dutta",
      "Rylan Schaeffer",
      "Sang Truong",
      "Simran Arora",
      "Mantas Mazeika",
      "Dan Hendrycks",
      "Zinan Lin",
      "Yu Cheng",
      "Sanmi Koyejo",
      "Dawn Song",
      "Bo Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/64008fa30cba9b4d1ab1bd3bd3d57d61-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhang",
      "Lingbo Mo",
      "Wenhu Chen",
      "Huan Sun",
      "Yu Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6492267465a7ac507be1f9fd1174e78d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "CHEN CHEN",
      "Yuchen Hu",
      "Chao-Han Huck Yang",
      "Sabato Marco Siniscalchi",
      "Pin-Yu Chen",
      "Eng-Siong Chng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/660ba7851661638c559df47743c69e40-Abstract-Datasets_and_Benchmarks.html": {
    "title": "NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robert Lange",
      "Yujin Tang",
      "Yingtao Tian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/662bb9c4dcc96aeaac8e7cd3fc6a0add-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Charles",
      "Nicole Mitchell",
      "Krishna Pillutla",
      "Michael Reneer",
      "Zachary Garrett"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67a6726dcd555b982cabb3446ffac01d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephanie Milani",
      "Anssi Kanervisto",
      "Karolis Ramanauskas",
      "Sander Schulhoff",
      "Brandon Houghton",
      "Rohin Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/67fc628f17c2ad53621fb961c6bafcaf-Abstract-Datasets_and_Benchmarks.html": {
    "title": "XES3G5M: A Knowledge Tracing Benchmark Dataset with Auxiliary Information",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zitao Liu",
      "Qiongqiong Liu",
      "Teng Guo",
      "Jiahao Chen",
      "Shuyan Huang",
      "Xiangyu Zhao",
      "Jiliang Tang",
      "Weiqi Luo",
      "Jian Weng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/68c33c4e6fc97f7b31c964dc83303a28-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LOVM: Language-Only Vision Model Selection",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orr Zohar",
      "Shih-Cheng Huang",
      "Kuan-Chieh Wang",
      "Serena Yeung"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a386d703b50f1cf1f61ab02a15967bb-Abstract-Datasets_and_Benchmarks.html": {
    "title": "PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianqian Xie",
      "Weiguang Han",
      "Xiao Zhang",
      "Yanzhao Lai",
      "Min Peng",
      "Alejandro Lopez-Lira",
      "Jimin Huang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6a42b45af2b72e6e5b5e3a6fe695809f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Does Continual Learning Meet Compositionality? New Benchmarks and An Evaluation Framework",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiduo Liao",
      "Ying Wei",
      "Mingchen Jiang",
      "Qingfu Zhang",
      "Hisao Ishibuchi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6aa9a05b929fb08ff46a58cab6cf860d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lasse Hansen",
      "Nabeel Seedat",
      "Mihaela van der Schaar",
      "Andrija Petrovic"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6d0cfc5db3feeabf6762129ba91bd3a1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OFCOURSE: A Multi-Agent Reinforcement Learning Environment for Order Fulfillment",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Zhu",
      "Yang Zhan",
      "Xuankun Huang",
      "Yuwei Chen",
      "yujie Chen",
      "Jiangwen Wei",
      "Wei Feng",
      "Yinzhi Zhou",
      "Haoyuan Hu",
      "Jieping Ye"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/6e566c91d381bd7a45647d9a90838817-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A Comprehensive Benchmark for Neural Human Radiance Fields",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenkun Liu",
      "Derong Jin",
      "Ailing Zeng",
      "Xiaoguang Han",
      "Lei Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/701eba0f98c6f28ffee0de5969d8d034-Abstract-Datasets_and_Benchmarks.html": {
    "title": "RD-Suite: A Benchmark for Ranking Distillation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qin",
      "Rolf Jagerman",
      "Rama Kumar Pasumarthi",
      "Honglei Zhuang",
      "He Zhang",
      "Aijun Bai",
      "Kai Hui",
      "Le Yan",
      "Xuanhui Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/70364304877b5e767de4e9a2a511be0c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Objaverse-XL: A Universe of 10M+ 3D Objects",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matt Deitke",
      "Ruoshi Liu",
      "Matthew Wallingford",
      "Huong Ngo",
      "Oscar Michel",
      "Aditya Kusupati",
      "Alan Fan",
      "Christian Laforte",
      "Vikram Voleti",
      "Samir Yitzhak Gadre",
      "Eli VanderBilt",
      "Aniruddha Kembhavi",
      "Carl Vondrick",
      "Georgia Gkioxari",
      "Kiana Ehsani",
      "Ludwig Schmidt",
      "Ali Farhadi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/706390d6f9208b03bc54f97ac3cfe99e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Reza Taesiri",
      "Giang Nguyen",
      "Sarra Habchi",
      "Cor-Paul Bezemer",
      "Anh Nguyen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7331077e0449e94a91370c46b4f80f57-Abstract-Datasets_and_Benchmarks.html": {
    "title": "AbdomenAtlas-8K: Annotating 8,000 CT Volumes for Multi-Organ Segmentation in Three Weeks",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongyu Qu",
      "Tiezheng Zhang",
      "Hualin Qiao",
      "jie liu",
      "Yucheng Tang",
      "Alan L. Yuille",
      "Zongwei Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/74a67268c5cc5910f64938cac4526a90-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isabella Liu",
      "Linghao Chen",
      "Ziyang Fu",
      "Liwen Wu",
      "Haian Jin",
      "Zhong Li",
      "Chin Ming Ryan Wong",
      "Yi Xu",
      "Ravi Ramamoorthi",
      "Zexiang Xu",
      "Hao Su"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/75d0956c9594f47bfb86a07bef58d4b0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Knowledge-based in silico models and dataset for the comparative evaluation of mammography AI for a range of breast characteristics, lesion conspicuities and doses",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Sizikova",
      "Niloufar Saharkhiz",
      "Diksha Sharma",
      "Miguel Lago",
      "Berkman Sahiner",
      "Jana Delfino",
      "Aldo Badano"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/764c18ad230f9e7bf6a77ffc2312c55e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Ellis",
      "Jonathan Cook",
      "Skander Moalla",
      "Mikayel Samvelyan",
      "Mingfei Sun",
      "Anuj Mahajan",
      "Jakob Foerster",
      "Shimon Whiteson"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/765043fe026f7d704c96cec027f13843-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yazhe Niu",
      "YUAN PU",
      "Zhenjie Yang",
      "Xueyan Li",
      "Tong Zhou",
      "Jiyuan Ren",
      "Shuai Hu",
      "Hongsheng Li",
      "Yu Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/769b70d1a9a6b21af53c00d0b322c763-Abstract-Datasets_and_Benchmarks.html": {
    "title": "TWIGMA: A dataset of AI-Generated Images with Metadata From Twitter",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqun Chen",
      "James Y. Zou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/76bf913ad349686b2aa552a1c6ee0a2e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Estimating Generic 3D Room Structures from 2D Annotations",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denys Rozumnyi",
      "Stefan Popov",
      "Kevis-kokitsi Maninis",
      "Matthias Niessner",
      "Vittorio Ferrari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/775ec578876fa6812c062644964b9870-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Quilt-1M: One Million Image-Text Pairs for Histopathology",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wisdom Ikezogwo",
      "Saygin Seyfioglu",
      "Fatemeh Ghezloo",
      "Dylan Geva",
      "Fatwir Sheikh Mohammed",
      "Pavan Kumar Anand",
      "Ranjay Krishna",
      "Linda Shapiro"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/78f9c04bdcb06f1ada3902912d8b64ba-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Scientific Document Retrieval using Multi-level Aspect-based Queries",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyou (Andre) Wang",
      "Kaicheng Wang",
      "Xiaoyue Wang",
      "Prudhviraj Naidu",
      "Leon Bergen",
      "Ramamohan Paturi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7945ab41f2aada1247a7c95e75cdf6c8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukasz Augustyniak",
      "Szymon Woźniak",
      "Marcin Gruza",
      "Piotr Gramacki",
      "Krzysztof Rajda",
      "Mikołaj Morzy",
      "Tomasz Kajdanowicz"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7a53bf4e02022aad32a4019d41b3b476-Abstract-Datasets_and_Benchmarks.html": {
    "title": "VTaC: A Benchmark Dataset of Ventricular Tachycardia Alarms from ICU Monitors",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li-wei Lehman",
      "Benjamin Moody",
      "Harsh Deep",
      "Feng Wu",
      "Hasan Saeed",
      "Lucas McCullum",
      "Diane Perry",
      "Tristan Struja",
      "Qiao Li",
      "Gari Clifford",
      "Roger Mark"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7a65606fa1a6849450550325832036e5-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yale Song",
      "Eugene Byrne",
      "Tushar Nagarajan",
      "Huiyu Wang",
      "Miguel Martin",
      "Lorenzo Torresani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7a92bcdede88c7afd108072faf5485c8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Valmeekam",
      "Matthew Marquez",
      "Alberto Olmo",
      "Sarath Sreedharan",
      "Subbarao Kambhampati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7adce80e86aa841490e6307109094de5-Abstract-Datasets_and_Benchmarks.html": {
    "title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehran Kazemi",
      "Quan Yuan",
      "Deepti Bhatia",
      "Najoung Kim",
      "Xin Xu",
      "Vaiva Imbrasaite",
      "Deepak Ramachandran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7b16688a2b053a1b01474ab5c78ce662-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuzheng Si",
      "Wentao Ma",
      "Haoyu Gao",
      "Yuchuan Wu",
      "Ting-En Lin",
      "Yinpei Dai",
      "Hangyu Li",
      "Rui Yan",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7cbeec46f979618beafb4f46d8f39f36-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MARBLE: Music Audio Representation Benchmark for Universal Evaluation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruibin Yuan",
      "Yinghao Ma",
      "Yizhi Li",
      "Ge Zhang",
      "Xingran Chen",
      "Hanzhi Yin",
      "zhuo le",
      "Yiqi Liu",
      "Jiawen Huang",
      "Zeyue Tian",
      "Binyue Deng",
      "Ningzhi Wang",
      "Chenghua Lin",
      "Emmanouil Benetos",
      "Anton Ragni",
      "Norbert Gyenge",
      "Roger Dannenberg",
      "Wenhu Chen",
      "Gus Xia",
      "Wei Xue",
      "Si Liu",
      "Shi Wang",
      "Ruibo Liu",
      "Yike Guo",
      "Jie Fu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7ccaa4f9a89cce6619093226f26b84e6-Abstract-Datasets_and_Benchmarks.html": {
    "title": "An NLP Benchmark Dataset for Assessing Corporate Climate Policy Engagement",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaku Morio",
      "Christopher D Manning"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f755e271717450020fda40f020922dd-Abstract-Datasets_and_Benchmarks.html": {
    "title": "QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Yu",
      "Meng Liu",
      "Youzhi Luo",
      "Alex Strasser",
      "Xiaofeng Qian",
      "Xiaoning Qian",
      "Shuiwang Ji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7f880e3a325b06e3601af1384a653038-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Mr. HiSum: A Large-scale Dataset for Video Highlight Detection and Summarization",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhwan Sul",
      "Jihoon Han",
      "Joonseok Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/7fc36bce5de315751001981baaf4751a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Digital Typhoon: Long-term Satellite Image Dataset for the Spatio-Temporal Modeling of Tropical Cyclones",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asanobu Kitamoto",
      "Jared Hwang",
      "Bastien Vuillod",
      "Lucas Gautier",
      "Yingtao Tian",
      "Tarin Clanuwat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/803c6ab3d62346e004ef70211d2d15b8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Are These the Same Apple? Comparing Images Based on Object Intrinsics",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Klemen Kotar",
      "Stephen Tian",
      "Hong-Xing Yu",
      "Dan Yamins",
      "Jiajun Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/803d8d4b4a549d0d062fc704f8659ce3-Abstract-Datasets_and_Benchmarks.html": {
    "title": "The ToMCAT Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adarsh Pyarelal",
      "Eric Duong",
      "Caleb Shibu",
      "Paulo Soares",
      "Savannah Boyd",
      "Payal Khosla",
      "Valeria A. Pfeifer",
      "Diheng Zhang",
      "Eric Andrews",
      "Rick Champlin",
      "Vincent Raymond",
      "Meghavarshini Krishnaswamy",
      "Clayton Morrison",
      "Emily Butler",
      "Kobus Barnard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/819b8452be7d6af1351d4c4f9cbdbd9b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "The Cambridge Law Corpus: A Corpus for Legal AI Research",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Östling",
      "Holli Sargeant",
      "Huiyuan Xie",
      "Ludwig Bull",
      "Alexander Terenin",
      "Leif Jonsson",
      "Måns Magnusson",
      "Felix Steffek"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/81c7202dbd3cd3006b35a58a076195c0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieming Cui",
      "Ziren Gong",
      "Baoxiong Jia",
      "Siyuan Huang",
      "Zilong Zheng",
      "Jianzhu Ma",
      "Yixin Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/82f39c7409155b74d15d73b048f06771-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valeriia Cherepanova",
      "Roman Levin",
      "Gowthami Somepalli",
      "Jonas Geiping",
      "C. Bayan Bruss",
      "Andrew G. Wilson",
      "Tom Goldstein",
      "Micah Goldblum"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/832ea0ff01bd512aab28bf416db9489c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A High-Resolution Dataset for Instance Detection with Multi-View Object Capture",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "QIANQIAN SHEN",
      "Yunhan Zhao",
      "Nahyun Kwon",
      "Jeeeun Kim",
      "Yanan Li",
      "Shu Kong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8339dacd9df7ffe9623760f74169dd1e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca for Predicting Antigen-Antibody Interactions",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hirofumi Tsuruta",
      "Hiroyuki Yamazaki",
      "Ryota Maeda",
      "Ryotaro Tamura",
      "Jennifer Wei",
      "Zelda E. Mariet",
      "Poomarin Phloyphisut",
      "Hidetoshi Shimokawa",
      "Joseph R. Ledsam",
      "Lucy Colwell",
      "Akihiro Imura"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/838694e9ab6b0a193b84daaafcac0eed-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Renku: a platform for sustainable data science",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rok Roškar",
      "Chandrasekhar Ramakrishnan",
      "Michele Volpi",
      "Fernando Perez-Cruz",
      "Lilian Gasser",
      "Firat Ozdemir",
      "Patrick Paitz",
      "Mohammad Alisafaee",
      "Philipp Fischer",
      "Ralf Grubenmann",
      "Eliza Harris",
      "Tasko Olevski",
      "Carl Remlinger",
      "Luis Salamanca",
      "Elisabet Capon Garcia",
      "Lorenzo Cavazzi",
      "Jakub Chrobasik",
      "Darlin Cordoba Osnas",
      "Alessandro Degano",
      "Jimena Dupre",
      "Wesley Johnson",
      "Eike Kettner",
      "Laura Kinkead",
      "Sean D. Murphy",
      "Flora Thiebaut",
      "Olivier Verscheure"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/83fc8fab1710363050bbd1d4b8cc0021-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyang Li",
      "Binyuan Hui",
      "Ge Qu",
      "Jiaxi Yang",
      "Binhua Li",
      "Bowen Li",
      "Bailin Wang",
      "Bowen Qin",
      "Ruiying Geng",
      "Nan Huo",
      "Xuanhe Zhou",
      "Ma Chenhao",
      "Guoliang Li",
      "Kevin Chang",
      "Fei Huang",
      "Reynold Cheng",
      "Yongbin Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/84062fe53d23e0791c6dbb456783e4a9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Xiang",
      "Liangzhi Li",
      "Wangyue Li",
      "Mingbai Bai",
      "Lu Wei",
      "Bowen Wang",
      "Noa Garcia"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/844f722dbbcb27933ff5baf58a1f00c8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Interactive Visual Reasoning under Uncertainty",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manjie Xu",
      "Guangyuan Jiang",
      "Wei Liang",
      "Chi Zhang",
      "Yixin Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/84948f178cfd3f6a0ffecda8fdcb3488-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenwen Zhang",
      "Arvin Tashakori",
      "Zenan Jiang",
      "Amir Servati",
      "Harishkumar Narayana",
      "Saeid Soltanian",
      "Rou Yi Yeap",
      "Menghan Ma",
      "Lauren Toy",
      "Peyman Servati"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8540fba4abdc7f9f7a7b1cc6cd60e409-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Perception Test: A Diagnostic Benchmark for Multimodal Video Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viorica Patraucean",
      "Lucas Smaira",
      "Ankush Gupta",
      "Adria Recasens",
      "Larisa Markeeva",
      "Dylan Banarse",
      "Skanda Koppula",
      "joseph heyward",
      "Mateusz Malinowski",
      "Yi Yang",
      "Carl Doersch",
      "Tatiana Matejovicova",
      "Yury Sulsky",
      "Antoine Miech",
      "Alexandre Fréchette",
      "Hanna Klimczak",
      "Raphael Koster",
      "Junlin Zhang",
      "Stephanie Winkler",
      "Yusuf Aytar",
      "Simon Osindero",
      "Dima Damen",
      "Andrew Zisserman",
      "Joao Carreira"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/85c123f6da0fa159eb249e6a2e171903-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Benchmarking Robustness to Adversarial Image Obfuscations",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Stimberg",
      "Ayan Chakrabarti",
      "Chun-Ta Lu",
      "Hussein Hazimeh",
      "Otilia Stretcu",
      "Wei Qiao",
      "Yintao Liu",
      "Merve Kaya",
      "Cyrus Rashtchian",
      "Ariel Fuxman",
      "Mehmet Tek",
      "Sven Gowal"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8644353f7d307baaf29bc1e56fe8e0ec-Abstract-Datasets_and_Benchmarks.html": {
    "title": "RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open Environments",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxue Qu",
      "Yu Wu",
      "Wu Liu",
      "Xiaodan Liang",
      "Jingkuan Song",
      "Yao Zhao",
      "Yunchao Wei"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/87dbbdc3a685a97ad28489a1d57c45c1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zahra Gharaee",
      "ZeMing Gong",
      "Nicholas Pellegrino",
      "Iuliia Zarubiieva",
      "Joakim Bruslund Haurum",
      "Scott Lowe",
      "Jaclyn McKeown",
      "Chris Ho",
      "Joschka McLeod",
      "Yi-Yun Wei",
      "Jireh Agda",
      "Sujeevan Ratnasingham",
      "Dirk Steinke",
      "Angel Chang",
      "Graham W. Taylor",
      "Paul Fieguth"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/89e44582fd28ddfea1ea4dcb0ebbf4b0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neel Guha",
      "Julian Nyarko",
      "Daniel Ho",
      "Christopher Ré",
      "Adam Chilton",
      "Aditya K",
      "Alex Chohlas-Wood",
      "Austin Peters",
      "Brandon Waldon",
      "Daniel Rockmore",
      "Diego Zambrano",
      "Dmitry Talisman",
      "Enam Hoque",
      "Faiz Surani",
      "Frank Fagan",
      "Galit Sarfaty",
      "Gregory Dickinson",
      "Haggai Porat",
      "Jason Hegland",
      "Jessica Wu",
      "Joe Nudell",
      "Joel Niklaus",
      "John Nay",
      "Jonathan Choi",
      "Kevin Tobia",
      "Margaret Hagan",
      "Megan Ma",
      "Michael Livermore",
      "Nikon Rasumov-Rahe",
      "Nils Holzenberger",
      "Noam Kolt",
      "Peter Henderson",
      "Sean Rehaag",
      "Sharad Goel",
      "Shang Gao",
      "Spencer Williams",
      "Sunny Gandhi",
      "Tom Zur",
      "Varun Iyer",
      "Zehua Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8a84a4341c375b8441b36836bb343d4e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "RoboHive: A Unified Framework for Robot Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vikash Kumar",
      "Rutav Shah",
      "Gaoyue Zhou",
      "Vincent Moens",
      "Vittorio Caggiano",
      "Abhishek Gupta",
      "Aravind Rajeswaran"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8b8a7960d343e023a6a0afe37eee6022-Abstract-Datasets_and_Benchmarks.html": {
    "title": "FELM: Benchmarking Factuality Evaluation of Large Language Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "shiqi chen",
      "Yiran Zhao",
      "Jinghan Zhang",
      "I-Chun Chern",
      "Siyang Gao",
      "Pengfei Liu",
      "Junxian He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8b94879b177d9780c17f5a78f62a6a8a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "AircraftVerse: A Large-Scale Multimodal Dataset of Aerial Vehicle Designs",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Cobb",
      "Anirban Roy",
      "Daniel Elenius",
      "Frederick Heim",
      "Brian Swenson",
      "Sydney Whittington",
      "James Walker",
      "Theodore Bapty",
      "Joseph Hite",
      "Karthik Ramani",
      "Christopher McComb",
      "Susmit Jha"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8bc5aef775aacc1650a9790f1428bcea-Abstract-Datasets_and_Benchmarks.html": {
    "title": "PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Zhou",
      "Weize Li",
      "Lihan Jiang",
      "Guoliang Wang",
      "Guyue Zhou",
      "Shanghang Zhang",
      "Hao Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c3c666820ea055a77726d66fc7d447f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Liu",
      "Yifeng Zhu",
      "Chongkai Gao",
      "Yihao Feng",
      "Qiang Liu",
      "Yuke Zhu",
      "Peter Stone"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8c7f8f98f9a8f5650922dd4545254f28-Abstract-Datasets_and_Benchmarks.html": {
    "title": "KuaiSim: A Comprehensive Simulator for Recommender Systems",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kesen Zhao",
      "Shuchang Liu",
      "Qingpeng Cai",
      "Xiangyu Zhao",
      "Ziru Liu",
      "Dong Zheng",
      "Peng Jiang",
      "Kun Gai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8cb92f326d01fd7f4371283ee2fa6386-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei He",
      "Kai Han",
      "Ying Nie",
      "Chengcheng Wang",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8cf04c64d1734e5f7e63418a2a4d49de-Abstract-Datasets_and_Benchmarks.html": {
    "title": "PTADisc: A Cross-Course Dataset Supporting Personalized Learning in Cold-Start Scenarios",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liya Hu",
      "Zhiang Dong",
      "Jingyuan Chen",
      "Guifeng Wang",
      "Zhihua Wang",
      "Zhou Zhao",
      "Fei Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8d352fd0f07fde4a74f9476603b3773b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Bordes",
      "Shashank Shekhar",
      "Mark Ibrahim",
      "Diane Bouchacourt",
      "Pascal Vincent",
      "Ari Morcos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8dfc3a2720a4112243a285b98e0d4415-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsheng Bai",
      "Atefeh Sohrabizadeh",
      "Zongyue Qin",
      "Ziniu Hu",
      "Yizhou Sun",
      "Jason Cong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8f153093758af93861a74a1305dfdc18-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sotetsu Koyamada",
      "Shinri Okano",
      "Soichiro Nishimori",
      "Yu Murata",
      "Keigo Habara",
      "Haruka Kita",
      "Shin Ishii"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/8f61049e8fe5b9ed714860b951066f1e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emma Chen",
      "Aman Kansal",
      "Julie Chen",
      "Boyang Tom Jin",
      "Julia Reisler",
      "David E. Kim",
      "Pranav Rajpurkar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/90ce332aff156b910b002ce4e6880dec-Abstract-Datasets_and_Benchmarks.html": {
    "title": "EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karttikeya Mangalam",
      "Raiymbek Akshulakov",
      "Jitendra Malik"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/917cd410aa55b61594fa2a6f6e5a9e94-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Cola: A Benchmark for Compositional Text-to-image Retrieval",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arijit Ray",
      "Filip Radenovic",
      "Abhimanyu Dubey",
      "Bryan Plummer",
      "Ranjay Krishna",
      "Kate Saenko"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianmin Zheng",
      "Wei-Lin Chiang",
      "Ying Sheng",
      "Siyuan Zhuang",
      "Zhanghao Wu",
      "Yonghao Zhuang",
      "Zi Lin",
      "Zhuohan Li",
      "Dacheng Li",
      "Eric Xing",
      "Hao Zhang",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/920f2dced7d32ab2ba2f1970bc306af6-Abstract-Datasets_and_Benchmarks.html": {
    "title": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangruibo Ding",
      "Zijian Wang",
      "Wasi Ahmad",
      "Hantian Ding",
      "Ming Tan",
      "Nihal Jain",
      "Murali Krishna Ramanathan",
      "Ramesh Nallapati",
      "Parminder Bhatia",
      "Dan Roth",
      "Bing Xiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/92225ec7e87b97a9e007ca6ab7944b14-Abstract-Datasets_and_Benchmarks.html": {
    "title": "CAPP-130: A Corpus of Chinese Application Privacy Policy Summarization and Interpretation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "pengyun zhu",
      "Long Wen",
      "Jinfei Liu",
      "Feng Xue",
      "Jian Lou",
      "Zhibo Wang",
      "Kui Ren"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/924303c6a45685510877ee018cdc8f80-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Diplomat: A Dialogue Dataset for Situated PragMATic Reasoning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengli Li",
      "Song-Chun Zhu",
      "Zilong Zheng"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/92a821f6c25b29241df6985ceb673a85-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfei Kuang",
      "Yunzhi Zhang",
      "Hong-Xing Yu",
      "Samir Agarwala",
      "Elliott / Shangzhe Wu",
      "Jiajun Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/949f0f8f32267d297c2d4e3ee10a2e7e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Köpf",
      "Yannic Kilcher",
      "Dimitri von Rütte",
      "Sotiris Anagnostidis",
      "Zhi Rui Tam",
      "Keith Stevens",
      "Abdullah Barhoum",
      "Duc Nguyen",
      "Oliver Stanley",
      "Richárd Nagyfi",
      "Shahul ES",
      "Sameer Suri",
      "David Glushkov",
      "Arnav Dantuluri",
      "Andrew Maguire",
      "Christoph Schuhmann",
      "Huu Nguyen",
      "Alexander Mattick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/98c9b79e9c686aadd4d81e34a7773dd1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Recasens Lafuente",
      "Martin R. Oswald",
      "Marc Pollefeys",
      "Javier Civera"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9941624ef7f867a502732b5154d30cb7-Abstract-Datasets_and_Benchmarks.html": {
    "title": "RealTime QA: What's the Answer Right Now?",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungo Kasai",
      "Keisuke Sakaguchi",
      "yoichi takahashi",
      "Ronan Le Bras",
      "Akari Asai",
      "Xinyan Yu",
      "Dragomir Radev",
      "Noah A. Smith",
      "Yejin Choi",
      "Kentaro Inui"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9b5c3e00d6ed30aad7adac9e7a664de1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "VidChapters-7M: Video Chapters at Scale",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Yang",
      "Arsha Nagrani",
      "Ivan Laptev",
      "Josef Sivic",
      "Cordelia Schmid"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9bc59aff4685e39e1a8175d5303248a1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "JourneyDB: A Benchmark for Generative Image Understanding",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keqiang Sun",
      "Junting Pan",
      "Yuying Ge",
      "Hao Li",
      "Haodong Duan",
      "Xiaoshi Wu",
      "Renrui Zhang",
      "Aojun Zhou",
      "Zipeng Qin",
      "Yi Wang",
      "Jifeng Dai",
      "Yu Qiao",
      "Limin Wang",
      "Hongsheng Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9c1535a02f0ce079433344e14d910597-Abstract-Datasets_and_Benchmarks.html": {
    "title": "BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhro Roy",
      "Samuel Thomson",
      "Tongfei Chen",
      "Richard Shin",
      "Adam Pauls",
      "Jason Eisner",
      "Benjamin Van Durme"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9ca22870ae0ba55ee50ce3e2d269e5de-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Suarez",
      "David Bloomin",
      "Kyoung Whan Choe",
      "Hao Xiang Li",
      "Ryan Sullivan",
      "Nishaanth Kanna",
      "Daniel Scott",
      "Rose Shuman",
      "Herbie Bradley",
      "Louis Castricato",
      "Phillip Isola",
      "Chenghui Yu",
      "Yuhao Jiang",
      "Qimai Li",
      "Jiaxin Chen",
      "Xiaolong Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9cb2a7495900f8b602cb10159246a016-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Zhuang",
      "Yue Yu",
      "Kuan Wang",
      "Haotian Sun",
      "Chao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9d58d85bfc041b4f901c62ba37a3f322-Abstract-Datasets_and_Benchmarks.html": {
    "title": "HT-Step: Aligning Instructional Articles with How-To Videos",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Triantafyllos Afouras",
      "Effrosyni Mavroudi",
      "Tushar Nagarajan",
      "Huiyu Wang",
      "Lorenzo Torresani"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9ee3ed2dd656402f954ef9dc37e39f48-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spyridon Kondylatos",
      "Ioannis Prapas",
      "Gustau Camps-Valls",
      "Ioannis Papoutsis"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9efe8db7fab57e19eed25718abedbbd2-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MLFMF: Data Sets for Machine Learning for Mathematical Formalization",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrej Bauer",
      "Matej Petković",
      "Ljupco Todorovski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/9f34484e5b8d87f09cc58c292a1c9f5d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "WBCAtt: A White Blood Cell Dataset Annotated with Detailed Morphological Attributes",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satoshi Tsutsui",
      "Winnie Pang",
      "Bihan Wen"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a054ff49751dbc991ec30ae479397c3d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MultiVENT: Multilingual Videos of Events and Aligned Natural Text",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kate Sanders",
      "David Etter",
      "Reno Kriz",
      "Benjamin Van Durme"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a0644215d9cff6646fa334dfa5d29c5a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "GEO-Bench: Toward Foundation Models for Earth Monitoring",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandre Lacoste",
      "Nils Lehmann",
      "Pau Rodriguez",
      "Evan Sherwin",
      "Hannah Kerner",
      "Björn Lütjens",
      "Jeremy Irvin",
      "David Dao",
      "Hamed Alemohammad",
      "Alexandre Drouin",
      "Mehmet Gunturkun",
      "Gabriel Huang",
      "David Vazquez",
      "Dava Newman",
      "Yoshua Bengio",
      "Stefano Ermon",
      "Xiaoxiang Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a134eaebd55b7406ff29cd75d5f1a622-Abstract-Datasets_and_Benchmarks.html": {
    "title": "On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo Liu",
      "Tianyu Wang",
      "Peng Cui",
      "Hongseok Namkoong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a15032f8199511ced4d7a8e2bbb487a5-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Diverse Community Data for Benchmarking Data Privacy Algorithms",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniruddha Sen",
      "Christine Task",
      "Dhruv Kapur",
      "Gary Howarth",
      "Karan Bhagat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Benchmark of Machine Learning Force Fields for Semiconductor Simulations: Datasets, Metrics, and Comparative Analysis",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geonu Kim",
      "Byunggook Na",
      "Gunhee Kim",
      "Hyuntae Cho",
      "Seungjin Kang",
      "Hee Sun Lee",
      "Saerom Choi",
      "Heejae Kim",
      "Seungwon Lee",
      "Yongdeok Kim"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a2a544e43acb8b954dc5846ff0d77ad5-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Chen",
      "Jindong Gu",
      "Zhen Han",
      "Yunpu Ma",
      "Philip Torr",
      "Volker Tresp"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a365be0950259c9624edfb4d26eabd46-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhinav Nippani",
      "Dongyue Li",
      "Haotian Ju",
      "Haris Koutsopoulos",
      "Hongyang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a3bf71c7c63f0c3bcb7ff67c67b1e7b1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "GenEval: An object-focused framework for evaluating text-to-image alignment",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhruba Ghosh",
      "Hannaneh Hajishirzi",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a4289154c9209b679ac761a50d5fec3a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SoundCam: A Dataset for Finding Humans Using Room Acoustics",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mason Wang",
      "Samuel Clarke",
      "Jui-Hsien Wang",
      "Ruohan Gao",
      "Jiajun Wu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a48ad12d588c597f4725a8b84af647b5-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Benchmarking Large Language Models on CMExam - A comprehensive Chinese Medical Exam Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junling Liu",
      "Peilin Zhou",
      "Yining Hua",
      "Dading Chong",
      "Zhongyu Tian",
      "Andrew Liu",
      "Helin Wang",
      "Chenyu You",
      "Zhenhua Guo",
      "LEI ZHU",
      "Michael Lingzhi Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a5f596699d8d4637532f955c7f2860f4-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Katakomba: Tools and Benchmarks for Data-Driven NetHack",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladislav Kurenkov",
      "Alexander Nikulin",
      "Denis Tarasov",
      "Sergey Kolesnikov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a74b697bce4cac6c91896372abaa8863-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DICES Dataset: Diversity in Conversational AI Evaluation for Safety",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lora Aroyo",
      "Alex Taylor",
      "Mark Díaz",
      "Christopher Homan",
      "Alicia Parrish",
      "Gregory Serapio-García",
      "Vinodkumar Prabhakaran",
      "Ding Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a76a757ed479a1e6a5f8134bea492f83-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Benchmarking Distribution Shift in Tabular Data with TableShift",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josh Gardner",
      "Zoran Popovic",
      "Ludwig Schmidt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a7a7c0c92f195cce85f99768621ac6c0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "IMP-MARL: a Suite of Environments for Large-scale Infrastructure Management Planning via MARL",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pascal Leroy",
      "Pablo G. Morato",
      "Jonathan Pisane",
      "Athanasios Kolios",
      "Damien Ernst"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a8757b889350a3782b384a3ec0dfbae9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "M$^{2}$SODAI: Multi-Modal Maritime Object Detection Dataset With RGB and Hyperspectral Image Sensors",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonggyu Jang",
      "Sangwoo Oh",
      "Youjin Kim",
      "Dongmin Seo",
      "Youngchol Choi",
      "Hyun Jong Yang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/a8f8cbd7f7a5fb2c837e578c75e5b615-Abstract-Datasets_and_Benchmarks.html": {
    "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kushal Tirumala",
      "Daniel Simig",
      "Armen Aghajanyan",
      "Ari Morcos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/aa7ef4c0f4aaabf376088a1a74e09d4c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DISCO-10M: A Large-Scale Music Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Lanzendörfer",
      "Florian Grötschla",
      "Emil Funke",
      "Roger Wattenhofer"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/abf37695a4562ac4c05194d717d47eec-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Lung250M-4B: A Combined 3D Dataset for CT- and Point Cloud-Based Intra-Patient Lung Registration",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fenja Falta",
      "Christoph Großbröhmer",
      "Alessa Hering",
      "Alexander Bigalke",
      "Mattias Heinrich"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac01e21bb14609416760f790dd8966ae-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A benchmark of categorical encoders for binary classification",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Matteucci",
      "Vadim Arzamasov",
      "Klemens Böhm"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ac7f98dd0b342edaf3be79844a180a6b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Generating QM1B with PySCF$_{\\text{IPU}}$",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Mathiasen",
      "Hatem Helal",
      "Kerstin Klaser",
      "Paul Balanca",
      "Josef Dean",
      "Carlo Luschi",
      "Dominique Beaini",
      "Andrew Fitzgibbon",
      "Dominic Masters"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/acffd5024f52c3a9ecc8ccb4b75b4e5c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "YouTubePD: A Multimodal Benchmark for Parkinson's Disease Analysis",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andy Zhou",
      "Samuel Li",
      "Pranav Sriram",
      "Xiang Li",
      "Jiahua Dong",
      "Ansh Sharma",
      "Yuanyi Zhong",
      "Shirui Luo",
      "Volodymyr Kindratenko",
      "George Heintz",
      "Christopher Zallek",
      "Yu-Xiong Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ad3ebc951f43d1e9ed20187a7b5bc4ee-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Ethical Considerations for Responsible Data Curation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jerone Andrews",
      "Dora Zhao",
      "William Thong",
      "Apostolos Modas",
      "Orestis Papakyriakopoulos",
      "Alice Xiang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ae9500c4f5607caf2eff033c67daa9d7-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Yu",
      "Yuchen Zhuang",
      "Jieyu Zhang",
      "Yu Meng",
      "Alexander J. Ratner",
      "Ranjay Krishna",
      "Jiaming Shen",
      "Chao Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b01153e7112b347d8ed54f317840d8af-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Stable Bias: Evaluating Societal Representations in Diffusion Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sasha Luccioni",
      "Christopher Akiki",
      "Margaret Mitchell",
      "Yacine Jernite"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b3640c2d3e58f716c67066046318db0f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "On Occlusions in Video Action Detection: Benchmark Datasets And Training Recipes",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rajat Modi",
      "Vibhav Vineet",
      "Yogesh Rawat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b4b02a09f2e6ad29fdbeb1386d68f4c4-Abstract-Datasets_and_Benchmarks.html": {
    "title": "The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mirac Suzgun",
      "Luke Melas-Kyriazi",
      "Suproteem Sarkar",
      "Scott D Kominers",
      "Stuart Shieber"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b5727c1bab903e0ff21cec84a9a7f5a6-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MVDoppler: Unleashing the Power of Multi-View Doppler for MicroMotion-based Gait Classification",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soheil Hor",
      "Shubo Yang",
      "Jaeho Choi",
      "Amin Arbabian"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b5ae304ecd18c5d4ac4a011ab086ba23-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A Massive Scale Semantic Similarity Dataset of Historical English",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emily Silcock",
      "Abhishek Arora",
      "Melissa Dell"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6167294ed3d6fc61e11e1592ce5cb77-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ProteinShake: Building datasets and benchmarks for deep learning on protein structures",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Kucera",
      "Carlos Oliver",
      "Dexiong Chen",
      "Karsten Borgwardt"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b6b5f50a2001ad1cbccca96e693c4ab4-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Revisiting Out-of-distribution Robustness in NLP: Benchmarks, Analysis, and LLMs Evaluations",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lifan Yuan",
      "Yangyi Chen",
      "Ganqu Cui",
      "Hongcheng Gao",
      "FangYuan Zou",
      "Xingyi Cheng",
      "Heng Ji",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b8f6f7f2ba4137124ac976286eacb611-Abstract-Datasets_and_Benchmarks.html": {
    "title": "TradeMaster: A Holistic Quantitative Trading Platform Empowered by Reinforcement Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Sun",
      "Molei Qin",
      "Wentao Zhang",
      "Haochong Xia",
      "Chuqiao Zong",
      "Jie Ying",
      "Yonggang Xie",
      "Lingxuan Zhao",
      "Xinrun Wang",
      "Bo An"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/b96ce67b2f2d45e4ab315e13a6b5b9c5-Abstract-Datasets_and_Benchmarks.html": {
    "title": "The Waymo Open Sim Agents Challenge",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nico Montali",
      "John Lambert",
      "Paul Mougin",
      "Alex Kuefler",
      "Nicholas Rhinehart",
      "Michelle Li",
      "Cole Gulino",
      "Tristan Emrich",
      "Zoey Yang",
      "Shimon Whiteson",
      "Brandyn White",
      "Dragomir Anguelov"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ba74855789913e5ed36f87288af79e5b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SustainGym: Reinforcement Learning Environments for Sustainable Energy Systems",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Yeh",
      "Victor Li",
      "Rajeev Datta",
      "Julio Arroyo",
      "Nicolas Christianson",
      "Chi Zhang",
      "Yize Chen",
      "Mohammad Mehdi Hosseini",
      "Azarang Golmohammadi",
      "Yuanyuan Shi",
      "Yisong Yue",
      "Adam Wierman"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bbb330189ce02be00cf7346167028ab1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taicheng Guo",
      "kehan Guo",
      "Bozhao Nan",
      "Zhenwen Liang",
      "Zhichun Guo",
      "Nitesh Chawla",
      "Olaf Wiest",
      "Xiangliang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bbbb6308b402fe909c39dd29950c32e0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "AndroidInTheWild: A Large-Scale Dataset For Android Device Control",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Rawles",
      "Alice Li",
      "Daniel Rodriguez",
      "Oriana Riva",
      "Timothy Lillicrap"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bbf7ee04e2aefec136ecf60e346c2e61-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SSL4EO-L: Datasets and Foundation Models for Landsat Imagery",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Stewart",
      "Nils Lehmann",
      "Isaac Corley",
      "Yi Wang",
      "Yi-Chia Chang",
      "Nassim Ait Ait Ali Braham",
      "Shradha Sehgal",
      "Caleb Robinson",
      "Arindam Banerjee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bc4cff0b37ccab13e98b6128d89ca172-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Pairwise GUI Dataset Construction Between Android Phones and Tablets",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "han hu",
      "Haolan Zhan",
      "Yujin Huang",
      "Di Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/bd3611971089d466ab4ca96a20f7ab13-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifu Wang",
      "Maxim Berman",
      "Amal Rannen-Triki",
      "Philip Torr",
      "Devis Tuia",
      "Tinne Tuytelaars",
      "Luc V Gool",
      "Jiaqian Yu",
      "Matthew Blaschko"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c0b91f9a3587bf35287f41dba5d20233-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Validated Image Caption Rating Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lothar D Narins",
      "Andrew Scott",
      "Aakash Gautam",
      "Anagha Kulkarni",
      "Mar Castanon",
      "Benjamin Kao",
      "Shasta Ihorn",
      "Yue-Ting Siu",
      "James M. Mason",
      "Alexander Blum",
      "Ilmi Yoon"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c41b5d8c1ba15b2aa83e4fa1541f02c8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "EV-Eye: Rethinking High-frequency Eye Tracking through the Lenses of Event Cameras",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangrong Zhao",
      "Yurun Yang",
      "Jingwei Liu",
      "Ning Chen",
      "Yiran Shen",
      "Hongkai Wen",
      "Guohao Lan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c481049f7410f38e788f67c171c64ad5-Abstract-Datasets_and_Benchmarks.html": {
    "title": "FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanxin Liu",
      "Lei Li",
      "Shuhuai Ren",
      "Rundong Gao",
      "Shicheng Li",
      "Sishuo Chen",
      "Xu Sun",
      "Lu Hou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c4a30a4dd840cfeff30ba4d2661ff097-Abstract-Datasets_and_Benchmarks.html": {
    "title": "UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yansong Ning",
      "Hao Liu",
      "Hao Wang",
      "Zhenyu Zeng",
      "Hui Xiong"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c60468eca9cd0b0083f0ff9d0aeb171a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MiliPoint: A Point Cloud Dataset for mmWave Radar",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Cui",
      "Shu Zhong",
      "Jiacheng Wu",
      "Zichao Shen",
      "Naim Dahnoun",
      "Yiren Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c6ec1844bec96d6d32ae95ae694e23d8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhen Huang",
      "Yuzhuo Bai",
      "Zhihao Zhu",
      "Junlei Zhang",
      "Jinghan Zhang",
      "Tangjun Su",
      "Junteng Liu",
      "Chuancheng Lv",
      "Yikai Zhang",
      "jiayi lei",
      "Yao Fu",
      "Maosong Sun",
      "Junxian He"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/c93f26b1381b17693055a611a513f1e9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siobhan Mackenzie Hall",
      "Fernanda Gonçalves Abrantes",
      "Hanwen Zhu",
      "Grace Sodunke",
      "Aleksandar Shtedritski",
      "Hannah Rose Kirk"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cabfaeecaae7d6540ee797a66f0130b0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Tian",
      "Tao Jiang",
      "Longfei Yun",
      "Yucheng Mao",
      "Huitong Yang",
      "Yue Wang",
      "Yilun Wang",
      "Hang Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pascal Notin",
      "Aaron Kollasch",
      "Daniel Ritter",
      "Lood van Niekerk",
      "Steffanie Paul",
      "Han Spinner",
      "Nathan Rollins",
      "Ada Shaw",
      "Rose Orenbuch",
      "Ruben Weitzman",
      "Jonathan Frazer",
      "Mafalda Dias",
      "Dinko Franceschi",
      "Yarin Gal",
      "Debora Marks"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cb82f1f97ad0ca1d92df852a44a3bd73-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Lo-Hi: Practical ML Drug Discovery Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Steshin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cca79c22037280d066fbd8bc35ac2e72-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SUPA: A Lightweight Diagnostic Simulator for Machine Learning in Particle Physics",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atul Kumar Sinha",
      "Daniele Paliotta",
      "Bálint Máté",
      "John Raine",
      "Tobias Golling",
      "François Fleuret"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ccac3b120c7dc86d45f56830732b62be-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artur Toshev",
      "Gianluca Galletti",
      "Fabian Fritz",
      "Stefan Adami",
      "Nikolaus Adams"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cd556f38dba3a6c367c42fa85fc0801c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Learning Human Action Recognition Representations Without Real Humans",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Howard Zhong",
      "Samarth Mishra",
      "Donghyun Kim",
      "SouYoung Jin",
      "Rameswar Panda",
      "Hilde Kuehne",
      "Leonid Karlinsky",
      "Venkatesh Saligrama",
      "Aude Oliva",
      "Rogerio Feris"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cd86a30526cd1aff61d6f89f107634e4-Abstract-Datasets_and_Benchmarks.html": {
    "title": "AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Schlichtkrull",
      "Zhijiang Guo",
      "Andreas Vlachos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/cef53466b62aebbcf8aa2210a89b33a1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Revealing the unseen: Benchmarking video action recognition under occlusion",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shresth Grover",
      "Vibhav Vineet",
      "Yogesh Rawat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d07379f3acf3af51dfc8598862cadfa0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengchao Liu",
      "weitao Du",
      "Yanjing Li",
      "Zhuoxinran Li",
      "Zhiling Zheng",
      "Chenru Duan",
      "Zhi-Ming Ma",
      "Omar Yaghi",
      "Animashree Anandkumar",
      "Christian Borgs",
      "Jennifer Chayes",
      "Hongyu Guo",
      "Jian Tang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d08b6801f24dda81199079a3371d77f9-Abstract-Datasets_and_Benchmarks.html": {
    "title": "GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vikram V. Ramaswamy",
      "Sing Yu Lin",
      "Dora Zhao",
      "Aaron Adcock",
      "Laurens van der Maaten",
      "Deepti Ghadiyaram",
      "Olga Russakovsky"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d0b67349dd16b83b2cf6167fb4e2be50-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungwoo Oh",
      "Gyubok Lee",
      "Seongsu Bae",
      "Joon-myoung Kwon",
      "Edward Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d3e8011c912e651ab2a76e7935a1e464-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsiao-Yu Tung",
      "Mingyu Ding",
      "Zhenfang Chen",
      "Daniel Bear",
      "Chuang Gan",
      "Josh Tenenbaum",
      "Dan Yamins",
      "Judith Fan",
      "Kevin Smith"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d40e6e4b3ee6c24f2bf2cb72c2412f4b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zheng",
      "Regina Lee",
      "Yuqian Lu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d41b70011dd21ec3de5e019302279551-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Classical Simulation of Quantum Circuits: Parallel Environments and Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao-Yang Liu",
      "Zeliang Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d42db1f74df54cb992b3956eb7f15a6f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Wornow",
      "Rahul Thapa",
      "Ethan Steinberg",
      "Jason Fries",
      "Nigam Shah"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d43621ff2dfe39d298dcd4a41937c912-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kushin Mukherjee",
      "Holly Huey",
      "Xuanchen Lu",
      "Yael Vinker",
      "Rio Aguina-Kang",
      "Ariel Shamir",
      "Judith Fan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d49042a5d49818711c401d34172f9900-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sneha Kudugunta",
      "Isaac Caswell",
      "Biao Zhang",
      "Xavier Garcia",
      "Derrick Xin",
      "Aditya Kusupati",
      "Romi Stella",
      "Ankur Bapna",
      "Orhan Firat"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d4f2bc9885ecbe30f65031819ef8699f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxiang Zhang",
      "Xiaojun Wan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d612971396f825dbf8e0e736f99a1955-Abstract-Datasets_and_Benchmarks.html": {
    "title": "UDC-SIT: A Real-World Dataset for Under-Display Cameras",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyusu Ahn",
      "Byeonghyun Ko",
      "HyunGyu Lee",
      "Chanwoo Park",
      "Jaejin Lee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d61d9f4fe4357296cb658795fd7999f0-Abstract-Datasets_and_Benchmarks.html": {
    "title": "COOM: A Game Benchmark for Continual Reinforcement Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tristan Tomilin",
      "Meng Fang",
      "Yudi Zhang",
      "Mykola Pechenizkiy"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d6dc15cc2442a40904e704d624d1fbe8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Evaluating Self-Supervised Learning for Molecular Graph Embeddings",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanchen Wang",
      "Jean Kaddour",
      "Shengchao Liu",
      "Jian Tang",
      "Joan Lasenby",
      "Qi Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d73078d49799693792fb0f3f32c57fc8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ProteinInvBench: Benchmarking Protein Inverse Folding on Diverse Tasks, Models, and Metrics",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangyang Gao",
      "Cheng Tan",
      "Yijie Zhang",
      "Xingran Chen",
      "Lirong Wu",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d7928f6dfb0c30d6a6917587dacbe4bc-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Bitstream-Corrupted Video Recovery: A Novel Benchmark Dataset and Method",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Liu",
      "Kejun Wu",
      "Yi Wang",
      "Wenyang Liu",
      "Kim-Hui Yap",
      "Lap-Pui Chau"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d7d0d548a6317407e02230f15ce75817-Abstract-Datasets_and_Benchmarks.html": {
    "title": "rPPG-Toolbox: Deep Remote PPG Toolbox",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Liu",
      "Girish Narayanswamy",
      "Akshay Paruchuri",
      "Xiaoyu Zhang",
      "Jiankai Tang",
      "Yuzhe Zhang",
      "Roni Sengupta",
      "Shwetak Patel",
      "Yuntao Wang",
      "Daniel McDuff"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/d8c6a37c4c94e9a63e53d296f1f668ae-Abstract-Datasets_and_Benchmarks.html": {
    "title": "HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Wiederhold",
      "Ava Megyeri",
      "DiMaggio Paris",
      "Sean Banerjee",
      "Natasha Banerjee"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dbc4b67c6430c22460623186c3d3fdc2-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamil Dreczkowski",
      "Antoine Grosnit",
      "Haitham Bou Ammar"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dcbff44d11130e75d09d3930411c23e1-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Tan",
      "Siyuan Li",
      "Zhangyang Gao",
      "Wenfei Guan",
      "Zedong Wang",
      "Zicheng Liu",
      "Lirong Wu",
      "Stan Z. Li"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dcd18e50ebca0af89187c6e35dabb584-Abstract-Datasets_and_Benchmarks.html": {
    "title": "GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Namyong Park",
      "Ryan Rossi",
      "Xing Wang",
      "Antoine Simoulin",
      "Nesreen K. Ahmed",
      "Christos Faloutsos"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dd83eada2c3c74db3c7fe1c087513756-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Holistic Evaluation of Text-to-Image Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tony Lee",
      "Michihiro Yasunaga",
      "Chenlin Meng",
      "Yifan Mai",
      "Joon Sung Park",
      "Agrim Gupta",
      "Yunzhi Zhang",
      "Deepak Narayanan",
      "Hannah Teufel",
      "Marco Bellagente",
      "Minguk Kang",
      "Taesung Park",
      "Jure Leskovec",
      "Jun-Yan Zhu",
      "Fei-Fei Li",
      "Jiajun Wu",
      "Stefano Ermon",
      "Percy S. Liang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/dde53059fdb0f45e1e9ad9c66997d662-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OV-PARTS: Towards Open-Vocabulary Part Segmentation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Wei",
      "Xiaoyu Yue",
      "Wenwei Zhang",
      "Shu Kong",
      "Xihui Liu",
      "Jiangmiao Pang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/de670b9d118229d09d9a9bd9dec2598b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ADGym: Design Choices for Deep Anomaly Detection",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minqi Jiang",
      "Chaochuan Hou",
      "Ao Zheng",
      "Songqiao Han",
      "Hailiang Huang",
      "Qingsong Wen",
      "Xiyang Hu",
      "Yue Zhao"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/de6ff07cbd222c10d694c2b2f732aceb-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Low-shot Object Learning with Mutual Exclusivity Bias",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Thai",
      "Ahmad Humayun",
      "Stefan Stojanov",
      "Zixuan Huang",
      "Bikram Boote",
      "James M. Rehg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ded1a89e2b3b925444ada973af66336e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mangpo Phothilimthana",
      "Sami Abu-El-Haija",
      "Kaidi Cao",
      "Bahare Fatemi",
      "Michael Burrows",
      "Charith Mendis",
      "Bryan Perozzi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e14e4cb8266184ceb234973dfe07faed-Abstract-Datasets_and_Benchmarks.html": {
    "title": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiep Le",
      "VASUDEV LAL",
      "Phillip Howard"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e2cfb719f58585f779d0a4f9f07bd618-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugo Laurençon",
      "Lucile Saulnier",
      "Leo Tronchon",
      "Stas Bekman",
      "Amanpreet Singh",
      "Anton Lozhkov",
      "Thomas Wang",
      "Siddharth Karamcheti",
      "Alexander Rush",
      "Douwe Kiela",
      "Matthieu Cord",
      "Victor Sanh"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e6c9671ed3b3106b71cafda3ba225c1a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuki Shimada",
      "Archontis Politis",
      "Parthasaarathy Sudarsanam",
      "Daniel A. Krause",
      "Kengo Uchida",
      "Sharath Adavanne",
      "Aapo Hakala",
      "Yuichiro Koyama",
      "Naoya Takahashi",
      "Shusuke Takahashi",
      "Tuomas Virtanen",
      "Yuki Mitsufuji"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e82f45e480f5f44d696ba15dad88f9a3-Abstract-Datasets_and_Benchmarks.html": {
    "title": "What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benedikt Blumenstiel",
      "Johannes Jakubik",
      "Hilde Kuehne",
      "Michael Vössing"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/e8916198466e8ef218a2185a491b49fa-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Chevalier-Boisvert",
      "Bolun Dai",
      "Mark Towers",
      "Rodrigo Perez-Vicente",
      "Lucas Willems",
      "Salem Lahlou",
      "Suman Pal",
      "Pablo Samuel Castro",
      "J Terry"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ea8758dbe6cc5e6e1764c009acb4c31e-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankur Sikarwar",
      "Mengmi Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb206443c93d07da8b1974b768d8a0d4-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Object Reprojection Error (ORE): Camera pose benchmarks from lightweight tracking annotations",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Chen",
      "Weiyao Wang",
      "Hao Tang",
      "Matt Feiszli"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/eb5683d06bdef51ed4dff644908eef4b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "RaLEs: a Benchmark for Radiology Language Evaluations",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juanma Zambrano Chaves",
      "Nandita Bhaskhar",
      "Maayane Attias",
      "Jean-Benoit Delbrouck",
      "Daniel Rubin",
      "Andreas Loening",
      "Curtis Langlotz",
      "Akshay Chaudhari"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ebd545176bdaa9cd5d45954947bd74b7-Abstract-Datasets_and_Benchmarks.html": {
    "title": "WildfireSpreadTS: A dataset of multi-modal time series for wildfire spread prediction",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Gerard",
      "Yu Zhao",
      "Josephine Sullivan"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ec6413875e4ab08d7bc4d8e225263398-Abstract-Datasets_and_Benchmarks.html": {
    "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhong Wang",
      "Hamish Ivison",
      "Pradeep Dasigi",
      "Jack Hessel",
      "Tushar Khot",
      "Khyathi Chandu",
      "David Wadden",
      "Kelsey MacMillan",
      "Noah A. Smith",
      "Iz Beltagy",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ed73c36e771881b232ef35fa3a1dec14-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tung Nguyen",
      "Jason Jewik",
      "Hritik Bansal",
      "Prakhar Sharma",
      "Aditya Grover"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee4814f9bce0cae7991d3341bb081b55-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Scalable 3D Captioning with Pretrained Models",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiange Luo",
      "Chris Rockwell",
      "Honglak Lee",
      "Justin Johnson "
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee57cd73a76bd927ffca3dda1dc3b9d4-Abstract-Datasets_and_Benchmarks.html": {
    "title": "LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Liu",
      "Yutong Xia",
      "Yuxuan Liang",
      "Junfeng Hu",
      "Yiwei Wang",
      "LEI BAI",
      "Chao Huang",
      "Zhenguang Liu",
      "Bryan Hooi",
      "Roger Zimmermann"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ee799aff607fcf39c01df6391e96f92c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "AirDelhi: Fine-Grained Spatio-Temporal Particulate Matter Dataset From Delhi For ML based Modeling",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sachin Chauhan",
      "Zeel Bharatkumar Patel",
      "Sayan Ranu",
      "Rijurekha Sen",
      "Nipun Batra"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef0164c1112f56246224af540857348f-Abstract-Datasets_and_Benchmarks.html": {
    "title": "FIND: A Function Description Benchmark for Evaluating Interpretability Methods",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Schwettmann",
      "Tamar Shaham",
      "Joanna Materzynska",
      "Neil Chowdhury",
      "Shuang Li",
      "Jacob Andreas",
      "David Bau",
      "Antonio Torralba"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef01d91aa87e7701aa9c8dc66a2d5bdb-Abstract-Datasets_and_Benchmarks.html": {
    "title": "EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Tang",
      "Kevin J Liang",
      "Kristen Grauman",
      "Matt Feiszli",
      "Weiyao Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ef7653bbc4655305efb89a32362e332a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "SatBird: a Dataset for Bird Species Distribution Modeling using Remote Sensing and Citizen Science Data",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mélisande Teng",
      "Amna Elmustafa",
      "Benjamin Akera",
      "Yoshua Bengio",
      "Hager Radi",
      "Hugo Larochelle",
      "David Rolnick"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/efc90033e6e1b05485312dd09fe302b8-Abstract-Datasets_and_Benchmarks.html": {
    "title": "NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Varun Jampani",
      "Kevis-kokitsi Maninis",
      "Andreas Engelhardt",
      "Arjun Karpur",
      "Karen Truong",
      "Kyle Sargent",
      "Stefan Popov",
      "Andre Araujo",
      "Ricardo Martin Brualla",
      "Kaushal Patel",
      "Daniel Vlasic",
      "Vittorio Ferrari",
      "Ameesh Makadia",
      "Ce Liu",
      "Yuanzhen Li",
      "Howard Zhou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f06d5ebd4ff40b40dd97e30cee632123-Abstract-Datasets_and_Benchmarks.html": {
    "title": "When Do Neural Nets Outperform Boosted Trees on Tabular Data?",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duncan McElfresh",
      "Sujay Khandagale",
      "Jonathan Valverde",
      "Vishak Prasad C",
      "Ganesh Ramakrishnan",
      "Micah Goldblum",
      "Colin White"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f29cf8f8b4996a4a453ef366cf496354-Abstract-Datasets_and_Benchmarks.html": {
    "title": "ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aashaka Desai",
      "Lauren Berger",
      "Fyodor Minakov",
      "Nessa Milano",
      "Chinmay Singh",
      "Kriston Pumphrey",
      "Richard Ladner",
      "Hal Daumé III",
      "Alex X Lu",
      "Naomi Caselli",
      "Danielle Bragg"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f323d594aa5d2c68154433a131c07959-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Evaluating Open-QA Evaluation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cunxiang Wang",
      "Sirui Cheng",
      "Qipeng Guo",
      "Yuanhao Yue",
      "Bowen Ding",
      "Zhikun Xu",
      "Yidong Wang",
      "Xiangkun Hu",
      "Zheng Zhang",
      "Yue Zhang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f37aba0f53fdb59f53254fe9098b2177-Abstract-Datasets_and_Benchmarks.html": {
    "title": "VisAlign: Dataset for Measuring the Alignment between AI and Humans in Visual Perception",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyoung Lee",
      "Seungho Kim",
      "Seunghyun Won",
      "Joonseok Lee",
      "Marzyeh Ghassemi",
      "James Thorne",
      "Jaeseok Choi",
      "O-Kil Kwon",
      "Edward Choi"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f43380ca3f86cd989f3269583c3c8b55-Abstract-Datasets_and_Benchmarks.html": {
    "title": "M$^2$Hub: Unlocking the Potential of Machine Learning for Materials Discovery",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanqi Du",
      "Yingheng Wang",
      "Yining Huang",
      "Jianan Canal Li",
      "Yanqiao Zhu",
      "Tian Xie",
      "Chenru Duan",
      "John Gregoire",
      "Carla P. Gomes"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f458af2455b1e12608c2a16c308d663d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wai Tong Chung",
      "Bassem Akoush",
      "Pushan Sharma",
      "Alex Tamkin",
      "Ki Sung Jung",
      "Jacqueline Chen",
      "Jack Guo",
      "Davy Brouzet",
      "Mohsen Talei",
      "Bruno Savard",
      "Alexei Poludnenko",
      "Matthias Ihme"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4cef76305dcad4efd3537da087ff520-Abstract-Datasets_and_Benchmarks.html": {
    "title": "CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taiki Miyanishi",
      "Fumiya Kitamori",
      "Shuhei Kurita",
      "Jungdae Lee",
      "Motoaki Kawanabe",
      "Nakamasa Inoue"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4d4a021f9051a6c18183b059117e8b5-Abstract-Datasets_and_Benchmarks.html": {
    "title": "GenImage: A Million-Scale Benchmark for Detecting AI-Generated Image",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjian Zhu",
      "Hanting Chen",
      "Qiangyu YAN",
      "Xudong Huang",
      "Guanyu Lin",
      "Wei Li",
      "Zhijun Tu",
      "Hailin Hu",
      "Jie Hu",
      "Yunhe Wang"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f4fdf676c3b21f20f8c391d929188386-Abstract-Datasets_and_Benchmarks.html": {
    "title": "MedSat: A Public Health Dataset for England Featuring Medical Prescriptions and Satellite Imagery",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanja Scepanovic",
      "Ivica Obadic",
      "Sagar Joglekar",
      "Laura GIUSTARINI",
      "Cristiano Nattero",
      "Daniele Quercia",
      "Xiaoxiang Zhu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f5da8ac52cf8857157c63c4803b6690b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "A Dataset for Analyzing Streaming Media Performance over HTTP/3 Browsers",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sapna Chaudhary",
      "Mukulika Maity",
      "Sandip Chakraborty",
      "Naval Shukla"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f63f5fbed1a4ef08c857c5f377b5d33a-Abstract-Datasets_and_Benchmarks.html": {
    "title": "StoryBench: A Multifaceted Benchmark for Continuous Story Visualization",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Bugliarello",
      "H. Hernan Moraldo",
      "Ruben Villegas",
      "Mohammad Babaeizadeh",
      "Mohammad Taghi Saffar",
      "Han Zhang",
      "Dumitru Erhan",
      "Vittorio Ferrari",
      "Pieter-Jan Kindermans",
      "Paul Voigtlaender"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f64927f5de00c47899e6e58c731966b6-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Aversa",
      "Gabriel Nobis",
      "Miriam Hägele",
      "Kai Standvoss",
      "Mihaela Chirica",
      "Roderick Murray-Smith",
      "Ahmed M. Alaa",
      "Lukas Ruff",
      "Daniela Ivanova",
      "Wojciech Samek",
      "Frederick Klauschen",
      "Bruno Sanguinetti",
      "Luis Oala"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f64e55d03e2fe61aa4114e49cb654acb-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushi Bai",
      "Jiahao Ying",
      "Yixin Cao",
      "Xin Lv",
      "Yuze He",
      "Xiaozhi Wang",
      "Jifan Yu",
      "Kaisheng Zeng",
      "Yijia Xiao",
      "Haozhe Lyu",
      "Jiayin Zhang",
      "Juanzi Li",
      "Lei Hou"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f6ccbf94fa57c2ae372ece91b537574d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "OceanBench: The Sea Surface Height Edition",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "J. Emmanuel Johnson",
      "Quentin Febvre",
      "Anastasiia Gorbunova",
      "Sam Metref",
      "Maxime Ballarotta",
      "Julien Le Sommer",
      "ronan fablet"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f7b424d242cc6bb7708cff241367334d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "M5HisDoc: A Large-scale Multi-style Chinese Historical Document Analysis Benchmark",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongxin Shi",
      "Chongyu Liu",
      "Dezhi Peng",
      "Cheng Jian",
      "Jiarong Huang",
      "Lianwen Jin"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f8ad010cdd9143dbb0e9308c093aff24-Abstract-Datasets_and_Benchmarks.html": {
    "title": "T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyi Huang",
      "Kaiyue Sun",
      "Enze Xie",
      "Zhenguo Li",
      "Xihui Liu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/f9ad87c1ebbae8a3555adb31dbcacf44-Abstract-Datasets_and_Benchmarks.html": {
    "title": "DISCS: A Benchmark for Discrete Sampling",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katayoon Goshvadi",
      "Haoran Sun",
      "Xingchao Liu",
      "Azade Nova",
      "Ruqi Zhang",
      "Will Grathwohl",
      "Dale Schuurmans",
      "Hanjun Dai"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html": {
    "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guilherme Penedo",
      "Quentin Malartic",
      "Daniel Hesslow",
      "Ruxandra Cojocaru",
      "Hamza Alobeidli",
      "Alessandro Cappelli",
      "Baptiste Pannier",
      "Ebtesam Almazrouei",
      "Julien Launay"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/fc20ea8d104cab737a5561096f9bde9b-Abstract-Datasets_and_Benchmarks.html": {
    "title": "AQuA: A Benchmarking Tool for Label Quality Assessment",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mononito Goswami",
      "Vedant Sanil",
      "Arjun Choudhry",
      "Arvind Srinivasan",
      "Chalisa Udompanyawit",
      "Artur Dubrawski"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/feb34ce77fc8b94c85d12e608b23ce67-Abstract-Datasets_and_Benchmarks.html": {
    "title": "Auslan-Daily: Australian Sign Language Translation for Daily Communication and News",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Shen",
      "Shaozu Yuan",
      "Hongwei Sheng",
      "Heming Du",
      "Xin Yu"
    ]
  },
  "https://papers.nips.cc/paper_files/paper/2023/hash/ffeb860479ccae44d84c0de32acd693d-Abstract-Datasets_and_Benchmarks.html": {
    "title": "American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers",
    "volume": "dataset",
    "abstract": "",
    "keywords": [],
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Melissa Dell",
      "Jacob Carlson",
      "Tom Bryan",
      "Emily Silcock",
      "Abhishek Arora",
      "Zejiang Shen",
      "Luca D'Amico-Wong",
      "Quan Le",
      "Pablo Querubin",
      "Leander Heldring"
    ]
  }
}