{
  "https://www.isca-speech.org/archive/interspeech_2023/narayanan23_interspeech.html": {
    "title": "Bridging Speech Science and Technology — Now and Into the Future",
    "volume": "main",
    "abstract": "Speech research is remarkable in so many ways – in its essential human-centeredness, the rich interconnections between the science and technology, and its wide-ranging impact that is both fundamental and applied. Crucial advances in speech science research catalyze and leverage technological advances across the machine intelligence ecosystem, from sensing and imaging to signal processing and machine learning. Likewise, creation of speech-centric societal applications benefits from an understanding of how humans produce, process and use speech in communication. In these complementary endeavors, two intertwined lines of inquiry endure: illuminating the rich information tapestry and inherent variability in speech and creating trustworthy speech technologies This talk will highlight some advances and possibilities in this multifaceted speech research realm. The first is capturing and modeling the human vocal instrument during speaking and how related technological and clinical applications leverage this technology. The second focuses on speech-based informatics tools to support research and clinical translation related to human health and wellbeing. Finally, the talk will highlight the critical goal of designing trustworthy speech and spoken language machine intelligence tools that are inclusive, equitable, robust, safe, and secure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23m_interspeech.html": {
    "title": "Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks",
    "volume": "main",
    "abstract": "Given an audio clip and a reference face image, the goal of the talking head generation is to generate a high-fidelity talking head video. Although some audio-driven methods of generating talking head videos have made some achievements in the past, most of them only focused on lip and audio synchronization and lack the ability to reproduce the facial expressions of the target person. To this end, we propose a talking head generation model consisting of a Memory-Sharing Emotion Feature extractor (MSEF) and an Attention-Augmented Translator based on U-net (AATU). Firstly, MSEF can extract implicit emotional auxiliary features from audio to estimate more accurate emotional face landmarks. Secondly, AATU acts as a translator between the estimated landmarks and the photo-realistic video frames. Extensive qualitative and quantitative experiments have shown the superiority of the proposed method to the previous works. Codes will be made publicly available",
    "checked": true,
    "id": "450c82af807da7c70dd8d43096cb1a0c5c8c929e",
    "semantic_title": "emotional talking head generation based on memory-sharing and attention-augmented networks",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23n_interspeech.html": {
    "title": "Speech Synthesis with Self-Supervisedly Learnt Prosodic Representations",
    "volume": "main",
    "abstract": "This paper presents S4LPR, a Speech Synthesis model conditioned on Self-Supervisedly Learnt Prosodic Representations. Instead of using raw acoustic features, such as F0 and energy, as intermediate prosodic variables, three self-supervised speech models are designed for comparison and are pre-trained on large-scale unlabeled data to extract frame-level prosodic representations. In addition to vanilla wav2vec 2.0, the other two pre-trained models learn representations from LPC residuals or adopt a multi-task learning strategy to focus on the prosodic information in speech. Based on FastSpeech2 and PnGBERT, our acoustic model is built with the learned prosodic representations as intermediate variables. Experimental results demonstrate that the naturalness of speech synthesized using S4LPR is significantly better than the FastSpeech2 baseline",
    "checked": true,
    "id": "c0a927f459171b0a15114a7a812996563964e91a",
    "semantic_title": "speech synthesis with self-supervisedly learnt prosodic representations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tang23_interspeech.html": {
    "title": "EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis",
    "volume": "main",
    "abstract": "There has been significant progress in emotional Text-To-Speech (TTS) synthesis technology in recent years. However, existing methods primarily focus on the synthesis of a limited number of emotion types and have achieved unsatisfactory performance in intensity control. To address these limitations, we propose EmoMix, which can generate emotional speech with specified intensity or a mixture of emotions. Specifically, EmoMix is a controllable emotional TTS model based on a diffusion probabilistic model and a pre-trained speech emotion recognition (SER) model used to extract emotion embedding. Mixed emotion synthesis is achieved by combining the noises predicted by diffusion model conditioned on different emotions during only one sampling process at the run-time. We further apply the Neutral and specific primary emotion mixed in varying degrees to control intensity. Experimental results validate the effectiveness of EmoMix for synthesizing mixed emotion and intensity control",
    "checked": true,
    "id": "5e635e749a90022f5b3704a8fb1c6b48645519cc",
    "semantic_title": "emomix: emotion mixing via diffusion models for emotional speech synthesis",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23b_interspeech.html": {
    "title": "Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus",
    "volume": "main",
    "abstract": "We present a large-scale in-the-wild Japanese laughter corpus and a laughter synthesis method. Previous work on laughter synthesis lacks not only data but also proper ways to represent laughter. To solve these problems, we first propose an in-the-wild corpus comprising 3.5 hours of laughter, which is to our best knowledge the largest laughter corpus designed for laughter synthesis. We then propose pseudo phonetic tokens (PPTs) to represent laughter by a sequence of discrete tokens, which are obtained by training a clustering model on features extracted from laughter by a pretrained self-supervised model. Laughter can then be synthesized by feeding PPTs into a text-to-speech system. We further show PPTs can be used to train a language model for unconditional laughter generation. Results of comprehensive subjective and objective evaluations demonstrate that the proposed method significantly outperforms a baseline method, and can generate natural laughter unconditionally",
    "checked": true,
    "id": "20f7cc755d832c664ee5a84b46a30c27b92a6ac5",
    "semantic_title": "laughter synthesis using pseudo phonetic tokens with a large-scale in-the-wild laughter corpus",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23u_interspeech.html": {
    "title": "Explicit Intensity Control for Accented Text-to-speech",
    "volume": "main",
    "abstract": "Accented text-to-speech (TTS) synthesis seeks to generate speech with an accent (L2) as a variant of the standard version (L1). How to control the intensity of accent is a very interesting research direction. Recent works design a speaker-adversarial loss to disentangle the speaker and accent information, and then adjust the loss weight to control the accent intensity. However, there is no direct correlation between the disentanglement factor and natural accent intensity. To this end, this paper proposes a new intuitive and explicit accent intensity control scheme for accented TTS. Specifically, we first extract the posterior probability from the L1 speech recognition model to quantify the phoneme accent intensity for accented speech, then design a FastSpeech2 based TTS model, named Ai-TTS, to take the accent intensity expression into account during speech generation. Experiments show that our method outperforms the baseline model in terms of accent rendering and intensity control",
    "checked": true,
    "id": "d354de4a4c182d294ffba7256e08bf6c4767d642",
    "semantic_title": "explicit intensity control for accented text-to-speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23o_interspeech.html": {
    "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "volume": "main",
    "abstract": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models",
    "checked": true,
    "id": "f3cff86b41ccbb4dd533e6d115b82f34d2d8a04c",
    "semantic_title": "comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/duquenne23_interspeech.html": {
    "title": "Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer",
    "volume": "main",
    "abstract": "Recent research has shown that independently trained encoders and decoders, combined through a shared fixed-size representation, can achieve competitive performance in speech-to-text translation. In this work, we show that this type of approach can be further improved with multilingual training. We observe significant improvements in zero-shot cross-modal speech translation, even outperforming a supervised approach based on XLSR for several languages",
    "checked": true,
    "id": "39b4255a439d2aa85c683935cd47314d098fecf3",
    "semantic_title": "modular speech-to-text translation for zero-shot cross-modal transfer",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pal23_interspeech.html": {
    "title": "Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters",
    "volume": "main",
    "abstract": "To translate speech for automatic dubbing, machine translation needs to be isochronous, i.e. translated speech needs to be aligned with the source in terms of speech durations. We introduce target factors in a transformer model to predict durations jointly with target language phoneme sequences. We also introduce auxiliary counters to help the decoder to keep track of the timing information while generating target phonemes. We show that our model improves translation quality and isochrony compared to previous work where the translation model is instead trained to predict interleaved sequences of phonemes and durations",
    "checked": true,
    "id": "38872b9e3f937287012bac3c66ab5df0c084a726",
    "semantic_title": "improving isochronous machine translation with target factors and auxiliary counters",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23_interspeech.html": {
    "title": "StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation",
    "volume": "main",
    "abstract": "Direct speech-to-speech translation (S2ST) has gradually become popular as it has many advantages compared with cascade S2ST. However, current research mainly focuses on the accuracy of semantic translation and ignores the speech style transfer from a source language to a target language. The lack of high-fidelity expressive parallel data makes such style transfer challenging, especially in more practical zero-shot scenarios. To solve this problem, we first build a parallel corpus using a multi-lingual multi-speaker text-to-speech synthesis (TTS) system and then propose the StyleS2ST model with cross-lingual speech style transfer ability based on a style adaptor on a direct S2ST system framework. Enabling continuous style space modeling of an acoustic model through parallel corpus training and non-parallel TTS data augmentation, StyleS2ST captures cross-lingual acoustic feature mapping from the source to the target language. Experiments show that StyleS2ST achieves good style similarity and naturalness in both in-set and out-of-set zero-shot scenarios",
    "checked": true,
    "id": "d7ad1b04a805e970edcd8972236f4ea6ea2452c6",
    "semantic_title": "styles2st: zero-shot style transfer for direct speech-to-speech translation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gaido23_interspeech.html": {
    "title": "Joint Speech Translation and Named Entity Recognition",
    "volume": "main",
    "abstract": "Modern automatic translation systems aim at supporting the users by providing contextual knowledge. In this framework, a critical task is the output enrichment with information regarding the mentioned entities. This is currently achieved by processing the generated translations with named entity recognition (NER) tools and retrieving their description from knowledge bases. In light of the recent promising results shown by direct speech translation (ST) models and the known weaknesses of cascades (error propagation and additional latency), in this paper we propose multitask models that jointly perform ST and NER, and compare them with a cascade baseline. Experimental results on three language pairs (en-es/fr/it) show that our models significantly outperform the cascade on the NER task (by 0.4-1.0 F1), without degradation in terms of translation quality, and with the same computational efficiency of a plain direct ST model",
    "checked": true,
    "id": "af742fb1413b8c87c997f8720d68c67688f4ff33",
    "semantic_title": "joint speech translation and named entity recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sant23_interspeech.html": {
    "title": "Analysis of Acoustic information in End-to-End Spoken Language Translation",
    "volume": "main",
    "abstract": "End-to-End Transformer-based models are the most popular approach for Spoken Language Translation (SLT). While obtaining state-of-the-art results, we are still far from understanding how these models extract acoustic information from the data and how they are transformed into semantic representations. In this paper, we seek to provide a better understanding of the flow of acoustic information along speech-to-text translation models. By means of the Speaker Classification and Spectrogram Reconstruction tasks, this study (i) interprets the main role of the encoder with respect to the acoustic features, (ii) highlights the importance of the acoustic information throughout the model and its transfer between encoder and decoder, and (iii) reveals the significant effect of downsampling convolutional layers for learning acoustic features. (iv) Finally, we also observe the existence of a strong correlation between the semantic domain and the speakers' labels in MuST-C",
    "checked": true,
    "id": "89dc1dd817b0af8184fb55e2b819d0fbcbdfad17",
    "semantic_title": "analysis of acoustic information in end-to-end spoken language translation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23oa_interspeech.html": {
    "title": "LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) and speech translation (ST) can both use neural transducers as the model structure. It is thus possible to use a single transducer model to perform both tasks. In real-world applications, such joint ASR and ST models may need to be streaming and do not require source language identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a streaming language-agnostic multilingual speech recognition and translation model using neural transducers. Based on the transducer model structure, we propose four methods, a unified joint and prediction network for multilingual output, a clustered multilingual encoder, target language identification for encoder, and connectionist temporal classification regularization. Experimental results show that LAMASSU not only drastically reduces the model size but also reaches the performances of monolingual ASR and bilingual ST models",
    "checked": true,
    "id": "b08070b6b55bc81bec7d59fbca454fdba9b5937f",
    "semantic_title": "lamassu: a streaming language-agnostic multilingual speech recognition and translation model using neural transducers",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23c_interspeech.html": {
    "title": "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available",
    "checked": true,
    "id": "e4f2d75856ce149b994f079ae50fd33ca47245d3",
    "semantic_title": "dphubert: joint distillation and pruning of self-supervised speech models",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zaiem23_interspeech.html": {
    "title": "Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases",
    "checked": true,
    "id": "6801e61b2fb45b660fab6d287cae5e23cd6b76dd",
    "semantic_title": "automatic data augmentation for domain adapted fine-tuning of self-supervised speech representations",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23e_interspeech.html": {
    "title": "Dual Acoustic Linguistic Self-supervised Representation Learning for Cross-Domain Speech Recognition",
    "volume": "main",
    "abstract": "The integration of well-pre-trained acoustic and linguistic representations boosts the performance of speech-to-text cross-modality tasks. However, the potential of fine-tuning cross-modality integrated model on accented and noisy corpus is still under-explored. To address this gap, we propose an end-to-end acoustic and linguistic integrated representation learning model, namely Dual-w2v-BART. Our model incorporates acoustic representations from wav2vec2.0 and linguistic information from BART model by utilizing the cross-attention mechanism in the decoder, with paired speech-text dual inputs. To enhance model robustness on accent and noise, we propose a text-centric representation consistency component that helps to gain the similarity between different modality inputs while representing the same content. The results on accented and noisy speech recognition tasks demonstrate the effectiveness of the proposed model for reducing error rates compared to baseline and other competitive models",
    "checked": true,
    "id": "d5111d4769b60fa9940dda15146af3c7959c2cee",
    "semantic_title": "dual acoustic linguistic self-supervised representation learning for cross-domain speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baskar23_interspeech.html": {
    "title": "O-1: Self-training with Oracle and 1-best Hypothesis",
    "volume": "main",
    "abstract": "We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80% relative compared to EMBR which bridges the gap by 43% relative. O-1 achieves 13% to 25% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets",
    "checked": true,
    "id": "c01ac0cdc1268c089a8073404a439630ccd876bb",
    "semantic_title": "o-1: self-training with oracle and 1-best hypothesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23d_interspeech.html": {
    "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets",
    "volume": "main",
    "abstract": "In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend",
    "checked": true,
    "id": "d2451c2cce0d44e3d390aa09059a8a0e5369c223",
    "semantic_title": "mt4ssl: boosting self-supervised speech representation learning by integrating multiple targets",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lamyeemui23_interspeech.html": {
    "title": "Comparing Self-Supervised Pre-Training and Semi-Supervised Training for Speech Recognition in Languages with Weak Language Models",
    "volume": "main",
    "abstract": "This paper investigates the potential of improving a hybrid automatic speech recognition model trained on 10 hours of transcribed data with 200 hours of untranscribed data in low-resource languages. First, we compare baseline methods of cross-lingual transfer with MFCC features and features extracted with the multilingual self-supervised model XLSR-53. Subsequently, we compare two approaches that can leverage the untranscribed data: semi-supervised training with LF-MMI and continued self-supervised pre-training of XLSR-53. Our results on well-resourced English broadcast data derived from MGB show that both methods achieve 18% and 27% relative improvements compared to the baseline, respectively. On the low-resource South African Soap Opera dataset, the relative improvement with semi-supervised training is only 3% due to the inherently weak language model. However, continued pre-training achieves 8.6% relative improvement because it does not rely on any external information",
    "checked": true,
    "id": "0e8e5d938af4fa0d114ae155421681e0755a649e",
    "semantic_title": "comparing self-supervised pre-training and semi-supervised training for speech recognition in languages with weak language models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23da_interspeech.html": {
    "title": "Chinese EFL Learners' Perception of English Prosodic Focus",
    "volume": "main",
    "abstract": "Focus in a sentence can be realized prosodically in speech communication. It has been found not easy for L2 learners to acquire. The present study examines Chinese learners' perception of English prosodic focus, specifically the effects of learners' English proficiency, intonation type, sentence length, and focus location on the perceptual accuracy of English prosodic focus by Chinese EFL learners. Results of two trials in the perception experiment reveal that focus location, intonation type, and English proficiency significantly impacted Chinese learners' perceptual accuracy of both single focus and dual focus in English. Focus in statements was perceived more accurately than that in questions for both single focus and dual focus. Focus located on sentence-final words in questions was perceived more accurately than that on non-final words in questions. Learners' English proficiency positively correlated to the accuracy of focus perception, especially for dual focus",
    "checked": true,
    "id": "e85507287ca0daa9bd73a0a82182c8212e6d0249",
    "semantic_title": "chinese efl learners' perception of english prosodic focus",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sostarics23_interspeech.html": {
    "title": "Pitch Accent Variation and the Interpretation of Rising and Falling Intonation in American English",
    "volume": "main",
    "abstract": "This study tests the division of labor in the meaning conveyed by pitch accents and edge tones in English intonation. In three perception studies, we investigate where the locus of the contrast between an assertive vs inquisitive interpretation resides. By doing so, we also gain insight into the role of potentially meaningful within- and between-category variation in the phonetic implementation of discrete intonational tunes. We find that the pitch accent does not contribute to assertive interpretation. Rather, the distinction between assertive and inquisitive interpretation is cued primarily by the final F0 of the pitch contour regardless of the pitch accent, but that increased overall pitch prominence may trigger a salient focus interpretation that interferes with judging assertiveness",
    "checked": true,
    "id": "d47244483903120338e389ac8db57617477d73d5",
    "semantic_title": "pitch accent variation and the interpretation of rising and falling intonation in american english",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kuang23_interspeech.html": {
    "title": "Tonal coarticulation as a cue for upcoming prosodic boundary",
    "volume": "main",
    "abstract": "It has been established that the lack of tonal coarticulation or pitch reset is a salient cue for the beginning of a large prosodic domain, however, it is yet unclear whether tonal coarticulation can be an informative cue for the end of a prosodic domain. We examined this question with two continuous speech corpora of Mandarin, and both expert and crowd-sourced perceptual annotations were used. The FPCA model of the holistic tonal contours shows that the carry-over effect of the preceding tone is significantly affected by the strength of the following boundaries. Stronger carry-over effects are associated with the end of larger prosodic boundaries. Moreover, machine learning classification shows that the fine-grained tonal coarticulation patterns are salient cues for predicting larger prosodic boundaries. This result is further validated by crowd-sourced boundary perceptual ratings from human listeners. This study has important implications for the understanding of prosodic phrasing",
    "checked": true,
    "id": "84c4d2832913419041c3d8d6b63c6e7cd8abc176",
    "semantic_title": "tonal coarticulation as a cue for upcoming prosodic boundary",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/repp23_interspeech.html": {
    "title": "Alignment of Beat Gestures and Prosodic Prominence in German",
    "volume": "main",
    "abstract": "We present evidence on the alignment of beat gestures and prosodic prominence from a video corpus consisting of six German educational videos for students from six presenters. Our analysis of 120 beat gestures (with a substantial variety of hand shapes) shows that beat gestures almost always align with prosodically prominent syllables, i.e., syllables carrying a pitch accent. Specifically, the stroke always starts before, or - more often - on, a pitch-accented syllable; the apex mostly falls on the accented syllable (74%) but may also occur in subsequent syllables. The degree of prosodic prominence of the accented syllable (in terms of DIMA-prominence levels) is predictive for the position of the apex, which occurs within rather than after the accented syllable more often for higher degrees of prominence. These findings provide new insights into the alignment of prominence-lending features of prosody and gesture, thereby broadening the empirical landscape for beat gestures",
    "checked": true,
    "id": "5293c0b253dc03cbff04cd19fb28c865a0fcba14",
    "semantic_title": "alignment of beat gestures and prosodic prominence in german",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/white23_interspeech.html": {
    "title": "Creak Prevalence and Prosodic Context in Australian English",
    "volume": "main",
    "abstract": "Creaky voice has been found to mark phrase-finality in many varieties of English, as well as in other languages. The present study aims to investigate whether this is also true for Australian English (AusE), a variety that is understudied in creaky voice research. Using automatic creak detection methods, the need for manual annotation of creak is reduced, and we are able to analyse a large dataset of Australian teenagers' speech. As in other varieties, creak is found to be a marker of finality in AusE. Additionally, we find that males use higher rates of creaky voice than females, challenging the widely held assumption that creak is a feature of female speech",
    "checked": true,
    "id": "6ba65b1ad194c04f21ea2b05088ccb1d92aa432d",
    "semantic_title": "creak prevalence and prosodic context in australian english",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bodur23_interspeech.html": {
    "title": "Speech reduction: position within French prosodic structure",
    "volume": "main",
    "abstract": "Variation in the speech signal is a characteristic of spoken language, emerging partially as a result of interactions between various linguistic levels. One example of variation is phonetic reduction, where words are produced with missing or underspecified phonetic forms. Using a French conversational corpus, this paper focuses on the relationship between reduction and prosodic structure to see whether certain positions favor the occurrence of reduction. We annotated and observed the distribution of reduced sequences within specific prosodic domains (Intonational and Accentual Phrases). Preliminary analyses revealed that the detected reductions occur mostly mid- IP and very rarely at IP-final. However, this pattern may vary among speakers, as speakers have different patterns in terms of the number of reductions produced and their positions. It is also usually the case that the reduced sequences occurring mid-IP, coincide with the AP level boundaries, extending from one AP to another",
    "checked": true,
    "id": "a21240a74af5b918c91e05a0ebae2292b35ff2f5",
    "semantic_title": "speech reduction: position within french prosodic structure",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23d_interspeech.html": {
    "title": "Transvelar Nasal Coupling Contributing to Speaker Characteristics in Non-nasal Vowels",
    "volume": "main",
    "abstract": "Nasal-cavity structure is stable in speech and varied across speakers, which potentially gives rise to speaker characteristics. Many studies have reported the acoustic contribution of the nasal cavity for nasal and nasalized sounds with velopharyngeal port opening. However, nasal-cavity resonance does emerge in non-nasal vowels through transvelar nasal coupling, which results in non-negligible modifications to non-nasal vowel spectra. In this study, nasal and oral output sounds were separately recorded during non-nasal utterances, and spectral analysis was conducted. The results indicate clear inter-speaker variability in two spectral measures below 2 kHz: frequency location of double-peaked first nasal-cavity resonance and inconsistent distribution of minor dips above the first resonance. It was also observed that nostril outputs modulate oral output signals to lower the first formant frequency of naturally produced non-low vowels, which also exhibited varied degrees across speakers",
    "checked": true,
    "id": "d6c08360cc77b37e108194d8af38d40d7b98de26",
    "semantic_title": "transvelar nasal coupling contributing to speaker characteristics in non-nasal vowels",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/otani23_interspeech.html": {
    "title": "Speech Synthesis from Articulatory Movements Recorded by Real-time MRI",
    "volume": "main",
    "abstract": "Previous speech synthesis models from articulatory movements recorded using real-time MRI (rtMRI) only predicted vocal tract shape parameters and required additional pitch information to generate a speech waveform. This study proposes a two-stage deep learning model composed of CNN-BiLSTM that predicts a mel-spectrogram from a rtMRI video and a HiFi-GAN vocoder that synthesizes a speech waveform. We evaluated our model on two databases: the ATR 503 sentences rtMRI database and the USC-TIMIT database. The experimental results on the ATR 503 sentences rtMRI database show that the PESQ score and the RMSE of F0 are 1.64 and 26.7 Hz. This demonstrates that all acoustic parameters, including fundamental frequency, can be estimated from the rtMRI videos. In the experiment on the USC-TIMIT database, we obtained a good PESQ score and RMSE for F0. However, the synthesized speech is unclear, indicating that the quality of the datasets affects the intelligibility of the synthesized speech",
    "checked": true,
    "id": "977f2f760d0eb99d73182fd8334c1986989c5b07",
    "semantic_title": "speech synthesis from articulatory movements recorded by real-time mri",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23b_interspeech.html": {
    "title": "The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN",
    "volume": "main",
    "abstract": "Phonetic convergence describes the automatic and unconscious speech adaptation of two interlocutors in a conversation. This paper proposes a Siamese recurrent neural network (RNN) architecture to measure the convergence of the holistic spectral characteristics of speech sounds in an L2-L2 interaction. We extend an alternating reading task (the ART) dataset by adding 20 native Slovak L2 English speakers. We train and test the Siamese RNN model to measure phonetic convergence of L2 English speech from three different native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10 dyads). Our results indicate that the Siamese RNN model effectively captures the dynamics of phonetic convergence and the speaker's imitation ability. Moreover, this text-independent model is scalable and capable of handling L1-induced speaker variability",
    "checked": true,
    "id": "2b9b85a451ee43149db92918bda991886295374b",
    "semantic_title": "the art of conversation: measuring phonetic convergence and deliberate imitation in l2-speech with a siamese rnn",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mahshie23_interspeech.html": {
    "title": "Did you see that? Exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants",
    "volume": "main",
    "abstract": "This project aimed to explore the potential role of vision in speech contrast production and auditory perception development in children with cochlear implants (CWCI). Ten CWCI between 43 and 61 months of age, with at least 2 years of CI experience, served as participants. Employing an auditory imitation task, children's ability to auditorily perceive contrasts that are more or less visible was examined both at baseline and one year after the initial assessment. The children's ability to produce these contrasts was also examined through a picture-naming task. The CWCI tended to produce features in both visibility conditions with greater accuracy than they perceived, both at baseline and at 1 year. Production and perception accuracy increased after one year of CI usage, with the mean perceptual gain for the more visible contrasts exceeding that of the less visible contrasts. The implications of the role of vision in contrast development are discussed",
    "checked": true,
    "id": "81ec5fa89978dde59b20c0c580c79f9a37cd3d3e",
    "semantic_title": "did you see that? exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vanbemmel23_interspeech.html": {
    "title": "Automatic assessments of dysarthric speech: the usability of acoustic-phonetic features",
    "volume": "main",
    "abstract": "Individuals with dysarthria suffer from difficulties in speech production and consequent reductions in speech intelligibility, which is an important concept for diagnosing and assessing effectiveness of speech therapy. In the current study, we investigate which acoustic-phonetic features are most relevant and important in automatically assessing intelligibility and in classifying speech as healthy or dysarthric. After feature selection, we applied a stepwise linear regression to predict intelligibility ratings and a Linear Discriminant Analysis to classify healthy and dysarthric speech. We observed a very strong correlation between actual and predicted intelligibility ratings in the regression analysis. We also observed a high classification accuracy of 98.06% by using 17 features and a comparable, high accuracy of 96.11% with only two features. These results indicate the usefulness of the acoustic-phonetic features in automatic assessments of dysarthric speech",
    "checked": true,
    "id": "8174cc3c98bb4fe0af86fd75c3dc000a54a39489",
    "semantic_title": "automatic assessments of dysarthric speech: the usability of acoustic-phonetic features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/venkatathirumalakumar23_interspeech.html": {
    "title": "Classification of Multi-class Vowels and Fricatives From Patients Having Amyotrophic Lateral Sclerosis with Varied Levels of Dysarthria Severity",
    "volume": "main",
    "abstract": "Dysarthria due to Amyotrophic Lateral Sclerosis (ALS) progressively distorts the acoustic space affecting the discriminability of different vowels and fricatives. However, the extent to which this happens with increasing severity is not thoroughly investigated. In this work, we perform automatic 4-class vowel (/a/, /i/, /o/, /u/) and 3-class fricative (/s/, /sh/, /f/) classification at varied severity levels and compare the performances with those from manual classification (through listening tests). Experiments with speech data from 119 ALS and 40 healthy subjects suggest that the manual and automatic classification accuracies reduce with an increase in dysarthria severity reaching 59.22% and 61.67% for vowels and 41.78% and 38.00% for fricatives, respectively, at the most severe cases. While manual classification is better than automatic one for all severity levels except the highest severity case for vowels, the difference between the two gradually reduces with an increase in severity",
    "checked": true,
    "id": "5eb48172191d12d829d3d4646b9d600cfb3751c5",
    "semantic_title": "classification of multi-class vowels and fricatives from patients having amyotrophic lateral sclerosis with varied levels of dysarthria severity",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qi23b_interspeech.html": {
    "title": "Parameter-efficient Dysarthric Speech Recognition Using Adapter Fusion and Householder Transformation",
    "volume": "main",
    "abstract": "In dysarthric speech recognition, data scarcity and the vast diversity between dysarthric speakers pose significant challenges. While finetuning has been a popular solution, it can lead to overfitting and low parameter efficiency. Adapter modules offer a better solution, with their small size and easy applicability. Additionally, Adapter Fusion can facilitate knowledge transfer from multiple learned adapters, but may employ more parameters. In this work, we apply Adapter Fusion for target speaker adaptation and speech recognition, achieving acceptable accuracy with significantly fewer speaker-specific trainable parameters than classical finetuning methods. We further improve the parameter efficiency of the fusion layer by reducing the size of query and key layers and using Householder transformation to reparameterize the value linear layer. Our proposed fusion layer achieves comparable recognition results to the original method with only one third of the parameters",
    "checked": true,
    "id": "c8c29732ed33b019854bbeb56e7ad8c707c6f270",
    "semantic_title": "parameter-efficient dysarthric speech recognition using adapter fusion and householder transformation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hermann23_interspeech.html": {
    "title": "Few-shot Dysarthric Speech Recognition with Text-to-Speech Data Augmentation",
    "volume": "main",
    "abstract": "Speakers with dysarthria could particularly benefit from assistive speech technology, but are underserved by current automatic speech recognition (ASR) systems. The differences of dysarthric speech pose challenges, while recording large amounts of training data can be exhausting for patients. In this paper, we synthesise dysarthric speech with a FastSpeech 2-based multi-speaker text-to-speech (TTS) system for ASR data augmentation. We evaluate its few-shot capability by generating dysarthric speech with as few as 5 words from an unseen target speaker and then using it to train speaker-dependent ASR systems. The results indicated that, while the TTS output is not yet of sufficient quality, this could allow easy development of personalised acoustic models for new dysarthric speakers and domains in the future",
    "checked": true,
    "id": "4692b33dca3e8dd24bef1ce0e2ba1a8f2acc2e44",
    "semantic_title": "few-shot dysarthric speech recognition with text-to-speech data augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yee23_interspeech.html": {
    "title": "Latent Phrase Matching for Dysarthric Speech",
    "volume": "main",
    "abstract": "Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies has emphasized interest in designing and improving personalized speech models for atypical speech. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases",
    "checked": true,
    "id": "d757a58200254625c3326a32a1da6fa8eaa2eff3",
    "semantic_title": "latent phrase matching for dysarthric speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeo23_interspeech.html": {
    "title": "Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification",
    "volume": "main",
    "abstract": "This paper proposes an improved Goodness of Pronunciation (GoP) that utilizes Uncertainty Quantification (UQ) for automatic speech intelligibility assessment for dysarthric speech. Current GoP methods rely heavily on neural network-driven overconfident predictions, which is unsuitable for assessing dysarthric speech due to its significant acoustic differences from healthy speech. To alleviate the problem, UQ techniques were used on GoP by 1) normalizing the phoneme prediction (entropy, margin, maxlogit, logit-margin) and 2) modifying the scoring function (scaling, prior normalization). As a result, prior-normalized maxlogit GoP achieves the best performance, with a relative increase of 5.66%, 3.91%, and 23.65% compared to the baseline GoP for English, Korean, and Tamil, respectively. Furthermore, phoneme analysis is conducted to identify which phoneme scores significantly correlate with intelligibility scores in each language",
    "checked": true,
    "id": "4879abd5687f59bc4123ebe5c0de84f94f6d8583",
    "semantic_title": "speech intelligibility assessment of dysarthric speech by using goodness of pronunciation with uncertainty quantification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23c_interspeech.html": {
    "title": "CQNV: A Combination of Coarsely Quantized Bitstream and Neural Vocoder for Low Rate Speech Coding",
    "volume": "main",
    "abstract": "Recently, speech codecs based on neural networks have proven to perform better than traditional methods. However, redundancy in traditional parameter quantization is visible within the codec architecture of combining the traditional codec with the neural vocoder. In this paper, we propose a novel framework named CQNV, which combines the coarsely quantized parameters of a traditional parametric codec to reduce the bitrate with a neural vocoder to improve the quality of the decoded speech. Furthermore, we introduce a parameters processing module into the neural vocoder to enhance the application of the bitstream of traditional speech coding parameters to the neural vocoder, further improving the reconstructed speech's quality. In the experiments, both subjective and objective evaluations demonstrate the effectiveness of the proposed CQNV framework. Specifically, our proposed method can achieve higher quality reconstructed speech at 1.1 kbps than Lyra and Encodec at 3 kbps",
    "checked": true,
    "id": "79ccc49646b09821403ae4f0d76dace82b19db22",
    "semantic_title": "cqnv: a combination of coarsely quantized bitstream and neural vocoder for low rate speech coding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kamo23_interspeech.html": {
    "title": "Target Speech Extraction with Conditional Diffusion Model",
    "volume": "main",
    "abstract": "Diffusion model-based speech enhancement has received increased attention since it can generate very natural enhanced signals and generalizes well to unseen conditions. Diffusion models have been explored for several sub-tasks of speech enhancement, such as speech denoising, dereverberation, and source separation. In this paper, we investigate their use for target speech extraction (TSE), which consists of estimating the clean speech signal of a target speaker in a mixture of multi-talkers. TSE is realized by conditioning the extraction process on a clue identifying the target speaker. We show we can realize TSE using a conditional diffusion model conditioned on the clue. Besides, we introduce ensemble inference to reduce potential extraction errors caused by the diffusion process. In experiments on Libri2mix corpus, we show that the proposed diffusion model-based TSE combined with ensemble inference outperforms a comparable TSE system trained discriminatively",
    "checked": true,
    "id": "cb8a0a610ddfd79794603b4729c501a662d461f0",
    "semantic_title": "target speech extraction with conditional diffusion model",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cohen23_interspeech.html": {
    "title": "Towards Fully Quantized Neural Networks For Speech Enhancement",
    "volume": "main",
    "abstract": "Deep learning models have shown state-of-the-art results in speech enhancement. However, deploying such models on an eight-bit integer-only device is challenging. In this work, we analyze the gaps in deploying a vanilla quantization-aware training method for speech enhancement, revealing two significant observations. First, quantization mainly affects signals with a high input Signal-to-Noise Ratio (SNR). Second, quantizing the model's input and output shows major performance degradation. Based on our analysis, we propose Fully Quantized Speech Enhancement (FQSE), a new quantization-aware training method that closes these gaps and enables eight-bit integer-only quantization. FQSE introduces data augmentation to mitigate the quantization effect on high SNR. Additionally, we add an input splitter and a residual quantization block to the model to overcome the error of the input-output quantization. We show that FQSE closes the performance gaps induced by eight-bit quantization",
    "checked": true,
    "id": "e85c275be3f166e3208ed6a981c9e90ce4498f93",
    "semantic_title": "towards fully quantized neural networks for speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23p_interspeech.html": {
    "title": "Complex Image Generation SwinTransformer Network for Audio Denoising",
    "volume": "main",
    "abstract": "Achieving high-performance audio denoising is still a challenging task in real-world applications. Existing time-frequency methods often ignore the quality of generated frequency domain images. This paper converts the audio denoising problem into an image generation task. We first develop a complex image generation SwinTransformer network to capture more information from the complex Fourier domain. We then impose structure similarity and detailed loss functions to generate high-quality images and develop an SDR loss to minimize the difference between denoised and clean audios. Extensive experiments on two benchmark datasets demonstrate that our proposed model is better than state-of-the-art methods",
    "checked": true,
    "id": "34e95df66dc9419ea92343fce9b10f0ccf24b687",
    "semantic_title": "complex image generation swintransformer network for audio denoising",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/blau23_interspeech.html": {
    "title": "Using Text Injection to Improve Recognition of Personal Identifiers in Speech",
    "volume": "main",
    "abstract": "Accurate recognition of specific categories, such as persons' names, dates or other identifiers is critical in many Automatic Speech Recognition (ASR) applications. As these categories represent personal information, ethical use of this data including collection, transcription, training and evaluation demands special care. One way of ensuring the security and privacy of individuals is to redact or eliminate Personally Identifiable Information (PII) from collection altogether. However, this results in ASR models that tend to have lower recognition accuracy of these categories. We use text-injection to improve the recognition of PII categories by including fake textual substitutes of PII categories in the training data using a text injection method. We demonstrate substantial improvement to Recall of Names and Dates in medical notes while improving overall WER. For alphanumeric digit sequences we show improvements to Character Error Rate and Sentence Accuracy",
    "checked": true,
    "id": "645e9909180d729a069b71f7c52750b534e28a83",
    "semantic_title": "using text injection to improve recognition of personal identifiers in speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grosz23_interspeech.html": {
    "title": "Investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a Finnish model",
    "volume": "main",
    "abstract": "Self-supervised speech models, such as the wav2vec2, have become extremely popular in the past few years. Their main appeal is that after their pre-training on a large amount of audio, they require only a small amount of supervised, finetuning data to achieve outstanding results. Despite their immense success, very little is understood about the pre-trained models and how finetuning changes them. In this work, we take the first steps towards a better understanding of wav2vec2 systems using model interpretation tools such as visualization and latent embedding clustering. Through our analysis, we gain new insights into the abilities of the pre-trained networks and the effect that finetuning has on them. We demonstrate that the clusters learned by the pre-trained model are just as important a factor as the supervised training data distribution in determining the accuracy of the finetuned system, which could aid us in selecting the most suitable pre-trained model for the supervised data",
    "checked": true,
    "id": "2ae665744fc5662c89168f14f3153144936cdd37",
    "semantic_title": "investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a finnish model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lehecka23_interspeech.html": {
    "title": "Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech",
    "volume": "main",
    "abstract": "This paper is a step forward in our effort to make vast oral history archives more accessible to the public and researchers by breaking down the decoding barriers between the knowledge encoded in the spoken testimonies and users who want to search for the information of their interest. We present new Transformer-based monolingual models suitable for speech recognition of oral history archives in English, German, and Czech. Our experiments show that although the all-purpose speech recognition systems have recently made tremendous progress, the transcription of oral history archives is still a challenging task for them; our tailored models significantly outperformed larger public multilingual models and scored new state-of-the-art results on all tested datasets. Due to the 2-phase fine-tuning process, our models are robust and can be used for oral history archives of various domains. We publicly release our models within a public speech recognition service",
    "checked": true,
    "id": "d26d8f4c79120558aa4f8131df408af091aff001",
    "semantic_title": "transformer-based speech recognition models for oral history archives in english, german, and czech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23_interspeech.html": {
    "title": "Iteratively Improving Speech Recognition and Voice Conversion",
    "volume": "main",
    "abstract": "Many existing works on voice conversion (VC) tasks use automatic speech recognition (ASR) models for ensuring linguistic consistency between source and converted samples. However, for the low-data resource domains, training a high-quality ASR remains to be a challenging task. In this work, we propose a novel iterative way of improving both the ASR and VC models. We first train an ASR model which is used to ensure content preservation while training a VC model. In the next iteration, the VC model is used as a data augmentation method to further fine-tune the ASR model and generalize it to diverse speakers. By iteratively leveraging the improved ASR model to train VC model and vice-versa, we experimentally show improvement in both the models. Our proposed framework outperforms the ASR and one-shot VC baseline models on English singing and Hindi speech domains in subjective and objective evaluations in low-data resource settings",
    "checked": true,
    "id": "e9f0e92d7a18cc78b71146d67358fcfd1a6bdc4d",
    "semantic_title": "iteratively improving speech recognition and voice conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fatehi23_interspeech.html": {
    "title": "LABERT: A Combination of Local Aggregation and Self-Supervised Speech Representation Learning for Detecting Informative Hidden Units in Low-Resource ASR Systems",
    "volume": "main",
    "abstract": "With advances in deep learning methodologies, Automatic Speech Recognition (ASR) systems have seen impressive results. However, ASR in Low-Resource Environments (LREs) are challenged by a lack of training data for the specific target domain. We propose that data sampling criteria for choosing more informative speech samples can be critical to addressing the problem of training data bottleneck. Our proposed Local Aggregation BERT (LABERT) method for self-supervised speech representation learning fuses an active learning model with an adapted local aggregation metric. Active learning is used to pick informative speech units, whereas the aggregation metric forces the model to move similar data together in the latent space while separating dissimilar instances to detect hidden units in LRE tasks. We evaluate LABERT with two LRE datasets: I-CUBE and UASpeech to explore the performance of our model in the LRE ASR problems",
    "checked": true,
    "id": "046609639dcd941f5438bde0eb73481f60411735",
    "semantic_title": "labert: a combination of local aggregation and self-supervised speech representation learning for detecting informative hidden units in low-resource asr systems",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xue23_interspeech.html": {
    "title": "TranUSR: Phoneme-to-word Transcoder Based Unified Speech Representation Learning for Cross-lingual Speech Recognition",
    "volume": "main",
    "abstract": "UniSpeech has achieved superior performance in cross-lingual automatic speech recognition (ASR) by explicitly aligning latent representations to phoneme units using multi-task self-supervised learning. While the learned representations transfer well from high-resource to low-resource languages, predicting words directly from these phonetic representations in downstream ASR is challenging. In this paper, we propose TranUSR, a two-stage model comprising a pre-trained UniData2vec and a phoneme-to-word Transcoder. Different from UniSpeech, UniData2vec replaces the quantized discrete representations with continuous and contextual representations from a teacher model for phonetically-aware pre-training. Then, Transcoder learns to translate phonemes to words with the aid of extra texts, enabling direct word generation. Experiments on Common Voice show that UniData2vec reduces PER by 5.3% compared to UniSpeech, while Transcoder yields a 14.4% WER reduction compared to grapheme fine-tuning",
    "checked": true,
    "id": "df2906c38eb605b537e376b93e937481e6600a93",
    "semantic_title": "tranusr: phoneme-to-word transcoder based unified speech representation learning for cross-lingual speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23e_interspeech.html": {
    "title": "Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR",
    "volume": "main",
    "abstract": "ASR systems in real applications must be adapted on the fly to correctly recognize task-specific contextual terms, such as contacts, application names and media entities. However, it is challenging to achieve scalability, large in-domain quality gains, and minimal out-of-domain quality regressions simultaneously. In this work, we introduce an effective neural biasing architecture called Dual-Mode NAM. Dual-Mode NAM embeds a top-k search process in its attention mechanism in a trainable fashion to perform an accurate top-k phrase selection before injecting the corresponding word-piece context into the acoustic encoder. We further propose a controllable mechanism to enable the ASR system to be able to trade off its in-domain and out-of-domain quality at inference time. When evaluated on a large-scale biasing benchmark, the combined techniques improve a previously proposed method with an average in-domain and out-of-domain WER reduction by up to 53.3% and 12.0% relative respectively",
    "checked": true,
    "id": "7e39237918eb9b5c0b3baa66b174a268c684a975",
    "semantic_title": "dual-mode nam: effective top-k context injection for end-to-end asr",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23g_interspeech.html": {
    "title": "GhostRNN: Reducing State Redundancy in RNN with Cheap Operations",
    "volume": "main",
    "abstract": "Recurrent neural network (RNNs) that are capable of modeling long-distance dependencies are widely used in various speech tasks, eg., keyword spotting (KWS) and speech enhancement (SE). Due to the limitation of power and memory in low-resource devices, efficient RNN models are urgently required for real-world applications. In this paper, we propose an efficient RNN architecture, GhostRNN, which reduces hidden state redundancy with cheap operations. In particular, we observe that partial dimensions of hidden states are similar to the others in trained RNN models, suggesting that redundancy exists in specific RNNs. To reduce the redundancy and hence computational cost, we propose to first generate a few intrinsic states, and then apply cheap operations to produce ghost states based on the intrinsic states. Experiments on KWS and SE tasks demonstrate that the proposed GhostRNN significantly reduces the memory usage (~40%) and computation cost while keeping performance similar. Codes will be available at https://gitee.com/mindspore/models/tree/master/research/audio/ghostrnn",
    "checked": true,
    "id": "2ca7637abf82a22f345829ff6a6e065c67a3925f",
    "semantic_title": "ghostrnn: reducing state redundancy in rnn with cheap operations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23da_interspeech.html": {
    "title": "Task-Agnostic Structured Pruning of Speech Representation Models",
    "volume": "main",
    "abstract": "Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have been shown to significantly improve many speech tasks. However, their large memory and strong computational requirements hinder their industrial applicability. Structured pruning is a hardware-friendly model compression technique but usually results in a larger loss of accuracy. In this paper, we propose a fine-grained attention head pruning method to compensate for the performance degradation. In addition, we also introduce the straight through estimator into the L0 regularization to further accelerate the pruned model. Experiments on the SUPERB benchmark show that our model can achieve comparable performance to the dense model in multiple tasks and outperforms the Wav2vec 2.0 base model on average, with 72% fewer parameters and 2 times faster inference speed",
    "checked": true,
    "id": "99aa56c8136a83756c4b8d901941c5bb2b2dcdac",
    "semantic_title": "task-agnostic structured pruning of speech representation models",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kanda23_interspeech.html": {
    "title": "Factual Consistency Oriented Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents a novel optimization framework for automatic speech recognition (ASR) with the aim of reducing hallucinations produced by an ASR model. The proposed framework optimizes the ASR model to maximize an expected factual consistency score between ASR hypotheses and ground-truth transcriptions, where the factual consistency score is computed by a separately trained estimator. Experimental results using the AMI meeting corpus and the VoxPopuli corpus show that the ASR model trained with the proposed framework generates ASR hypotheses that have significantly higher consistency scores with ground-truth transcriptions while maintaining the word error rates close to those of cross entropy-trained ASR models. Furthermore, it is shown that training the ASR models with the proposed framework improves the speech summarization quality as measured by the factual consistency of meeting conversation summaries generated by a large language model",
    "checked": true,
    "id": "979a68d069fe7b41105085e9c6182da5058665b6",
    "semantic_title": "factual consistency oriented speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fathullah23_interspeech.html": {
    "title": "Multi-Head State Space Model for Speech Recognition",
    "volume": "main",
    "abstract": "State space models (SSMs) have recently shown promising results on small-scale sequence and language modelling tasks, rivalling and outperforming many attention-based approaches. In this paper, we propose a multi-head state space (MH-SSM) architecture equipped with special gating mechanisms, where parallel heads are taught to learn local and global temporal dynamics on sequence data. As a drop-in replacement for multi-head attention in transformer encoders, this new model significantly outperforms the transformer transducer on the LibriSpeech speech recognition corpus. Furthermore, we augment the transformer block with MH-SSMs layers, referred to as the Stateformer, achieving state-of-the-art performance on the LibriSpeech task, with word error rates of 1.76%/4.37% on the development and 1.91%/4.36% on the test sets without using an external language model",
    "checked": true,
    "id": "067aaf0d1cde4ee21063be137559f2fe50125570",
    "semantic_title": "multi-head state space model for speech recognition",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23_interspeech.html": {
    "title": "Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search",
    "volume": "main",
    "abstract": "Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance",
    "checked": true,
    "id": "9c607471820fe74572e142fc6e9ce432716048c8",
    "semantic_title": "cascaded multi-task adaptive learning based on neural architecture search",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martin23_interspeech.html": {
    "title": "Probing Self-supervised Speech Models for Phonetic and Phonemic Information: A Case Study in Aspiration",
    "volume": "main",
    "abstract": "Textless self-supervised speech models have grown in capabilities in recent years, but the nature of the linguistic information they encode has not yet been thoroughly examined. We evaluate the extent to which these models' learned representations align with basic representational distinctions made by humans, focusing on a set of phonetic (low-level) and phonemic (more abstract) contrasts instantiated in word-initial stops. We find that robust representations of both phonetic and phonemic distinctions emerge in early layers of these models' architectures, and are preserved in the principal components of deeper layer representations. Our analyses suggest two sources for this success: some can only be explained by the optimization of the models on speech data, while some can be attributed to these models' high-dimensional architectures. Our findings show that speech-trained HuBERT derives a low-noise and low-dimensional subspace corresponding to abstract phonological distinctions",
    "checked": true,
    "id": "7373477778b12d453191b09d46e2f77f4295ca52",
    "semantic_title": "probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/harding23_interspeech.html": {
    "title": "Selective Biasing with Trie-based Contextual Adapters for Personalised Speech Recognition using Neural Transducers",
    "volume": "main",
    "abstract": "Neural transducer ASR models achieve state of the art accuracy on many tasks, however rare word recognition poses a particular challenge as models often fail to recognise words that occur rarely, or not at all, in the training data. Methods of contextual biasing, where models are dynamically adapted to bias their outputs towards a given list of relevant words and phrases, have been shown to be effective at alleviating this issue. While such methods are effective at improving rare word recognition, over-biasing can lead to degradation on common words. In this work we propose several extensions to a recently proposed trie-based method of contextual biasing. We show how performance of the method can be improved in terms of rare word recognition, especially in the case of very large catalogues, by introducing a simple normalisation term, how the method can be trained as an adapter module, and how selective biasing can be applied to practically eliminate over-biasing on common words",
    "checked": true,
    "id": "c261ebb5eec148522963b9c6bdd958e463ebcc2c",
    "semantic_title": "selective biasing with trie-based contextual adapters for personalised speech recognition using neural transducers",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23b_interspeech.html": {
    "title": "Robust Prototype Learning for Anomalous Sound Detection",
    "volume": "main",
    "abstract": "In this paper, we present a robust prototype learning framework for anomalous sound detection (ASD), where prototypical loss is exploited to measure the similarity between samples and prototypes. We show that existing generative and discriminative based ASD methods can be unified into this framework from the perspective of prototypical learning. For ASD in recent DCASE challenges, extensions related to imbalanced learning are proposed to improve the robustness of prototypes learned from source and target domains. Specifically, balanced sampling and multiple-prototype expansion (MPE) strategies are proposed to address imbalances across attributes of source and target domains. Furthermore, a novel negative-prototype expansion (NPE) method is used to construct pseudo-anomalies to learn a more compact and effective embedding space for normal sounds. Evaluation on the DCASE2022 Task2 development dataset demonstrates the validity of the proposed prototype learning framework",
    "checked": true,
    "id": "d6a1b187a270900853a4a89c0dcc6ae3c0a2830e",
    "semantic_title": "robust prototype learning for anomalous sound detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kushwaha23_interspeech.html": {
    "title": "A multimodal prototypical approach for unsupervised sound classification",
    "volume": "main",
    "abstract": "In the context of environmental sound classification, the adaptability of systems is key: which sound classes are interesting depends on the context and the user's needs. Recent advances in text-to-audio retrieval allow for zero-shot audio classification, but performance compared to supervised models remains limited. This work proposes a multimodal prototypical approach that exploits local audio-text embeddings to provide more relevant answers to audio queries, augmenting the adaptability of sound detection in the wild. We do this by first using text to query a nearby community of audio embeddings that best characterize each query sound, and select the group's centroids as our prototypes. Second, we compare unseen audio to these prototypes for classification. We perform multiple ablation studies to understand the impact of the embedding models and prompts. Our unsupervised approach improves upon the zero-shot state-of-the-art in three sound recognition benchmarks by an average of 12%",
    "checked": true,
    "id": "fb082d89b1f8906509991f738961e1cbe21d7435",
    "semantic_title": "a multimodal prototypical approach for unsupervised sound classification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wen23_interspeech.html": {
    "title": "Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms",
    "volume": "main",
    "abstract": "Robust audio anti-spoofing has been increasingly challeng- ing due to the recent advancements on deepfake techniques. While spectrograms have demonstrated their capability for anti- spoofing, complementary information presented in multi-order spectral patterns have not been well explored, which limits their effectiveness for varying spoofing attacks. Therefore, we propose a novel deep learning method with a spectral fusion- reconstruction strategy, namely S2pecNet, to utilise multi-order spectral patterns for robust audio anti-spoofing representations. Specifically, spectral patterns up to second-order are fused in a coarse-to-fine manner and two branches are designed for the fine-level fusion from the spectral and temporal contexts. A reconstruction from the fused representation to the input spec- trograms further reduces the potential fused information loss. Our method achieved the state-of-the-art performance with an EER of 0.77% on a widely used dataset - ASVspoof2019 LA Challenge",
    "checked": true,
    "id": "9f8e5fa471adab4938b3145fcb41b79c22ec2f0b",
    "semantic_title": "robust audio anti-spoofing with fusion-reconstruction learning on multi-order spectrograms",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23c_interspeech.html": {
    "title": "Adapting Language-Audio Models as Few-Shot Audio Learners",
    "volume": "main",
    "abstract": "Contrastive language-audio pretraining (CLAP) has become a new paradigm to learn audio concepts with audio-text pairs. CLAP models have shown unprecedented performance as zero-shot classifiers on downstream tasks. To further adapt CLAP with domain-specific knowledge, a popular method is to finetune its audio encoder with available labelled examples. However, this is challenging in low-shot scenarios, as the amount of annotations is limited compared to the model size. In this work, we introduce a Training-efficient (Treff) adapter to rapidly learn with a small set of examples while maintaining the capacity for zero-shot classification. First, we propose a cross-attention linear model (CALM) to map a set of labelled examples and test audio to test labels. Second, we find initialising CALM as a cosine measurement improves our Treff adapter even without training. The Treff adapter beats metric-based methods in few-shot settings and yields competitive results to fully-supervised methods",
    "checked": true,
    "id": "237032b6256087766e6d366a47227aef980fd2b7",
    "semantic_title": "adapting language-audio models as few-shot audio learners",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23l_interspeech.html": {
    "title": "TFECN: Time-Frequency Enhanced ConvNet for Audio Classification",
    "volume": "main",
    "abstract": "Recently, transformer-based models have shown leading performance in audio classification, gradually replacing the dominant ConvNet in the past. However, some research has shown that certain characteristics and designs in transformers can be applied to other architectures and make them achieve similar performance as transformers. In this paper, we introduce TFECN, a pure ConvNet that combines the design in transformers and has time-frequency enhanced convolution with large kernels. It can provide a global receptive field on the frequency dimension as well as avoid the influence of the convolution's shift-equivariance on the recognition of not shift-invariant patterns along the frequency axis. Furthermore, to use ImageNet-pretrained weights, we propose a method for transferring weights between kernels of different sizes. On the commonly used datasets AudioSet, FSD50K, and ESC50, our TFECN outperforms the models trained in the same way",
    "checked": true,
    "id": "8cb1a3348c7004f0d3c4aa666edd09532da0b4db",
    "semantic_title": "tfecn: time-frequency enhanced convnet for audio classification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23b_interspeech.html": {
    "title": "Resolution Consistency Training on Time-Frequency Domain for Semi-Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "The fact that unlabeled data can be used for supervised learning is of considerable relevance concerning polyphonic sound event detection (PSED) because of the high costs of frame-wise labeling. While semi-supervised learning (SSL) for image tasks has been extensively developed, SSL for PSED has not been substantially explored due to data augmentation limitations. In this paper, we propose a novel SSL strategy for PSED called resolution consistency training (ResCT), combining unsupervised terms with the mean teacher using different resolutions of a spectrogram for data augmentation. The proposed method regularizes the consistency between the model predictions for different resolutions by controlling the sampling rate and window size. Experimental results show that ResCT outperforms other SSL methods on various evaluation metrics: event-f1 score, intersection-f1 score, and PSDSs. Finally, we report on some ablation studies for the weak and strong augmentation policies",
    "checked": true,
    "id": "24624c92335e516840fe5f362d34ab272c66f78b",
    "semantic_title": "resolution consistency training on time-frequency domain for semi-supervised sound event detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23n_interspeech.html": {
    "title": "Fine-tuning Audio Spectrogram Transformer with Task-aware Adapters for Sound Event Detection",
    "volume": "main",
    "abstract": "In this paper, we present a task-aware fine-tuning method to transfer Patchout faSt Spectrogram Transformer (PaSST) model to sound event detection (SED) task. Pretrained PaSST has shown significant performance on audio tagging (AT) and SED tasks, but it is not optimal to fine-tune the model from a single layer as the local and semantic information have not been well exploited. To address this, we first introduce task-aware adapters including SED-adapter and AT-adapter to fine-tune PaSST for SED and AT task respectively, and then propose task-aware fine-tuning to combine local information from shallower layer with semantic information from deeper layer, based on task-aware adapters. Besides, we propose the self-distillated mean teacher (SdMT) to train a robust student model with soft pseudo labels from teacher. Experiments are conducted on DCASE2022 task4 development set, the EB-F1 of 64.85% and PSDS1 of 0.5548 are achieved which outperform previous state-of-the-art systems",
    "checked": true,
    "id": "5035bf01ae576ab415d822284e629eb3734c6708",
    "semantic_title": "fine-tuning audio spectrogram transformer with task-aware adapters for sound event detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23b_interspeech.html": {
    "title": "Small Footprint Multi-channel Network for Keyword Spotting with Centroid Based Awareness",
    "volume": "main",
    "abstract": "Spoken Keyword Spotting (KWS) in noisy far-field environments is challenging for small-footprint models, given the restrictions on computational resources (e.g., model size, running memory). This is even more intricate when handling noises from multiple microphones. To address this, we present a new multi-channel model that uses a CNN-based network with a linear mixing unit to achieve local-global dependency representations. Our method enhances noise-robustness while ensuring more efficient computation. Besides, we propose an end-to-end centroid-based awareness module that provides class similarity awareness at the bottleneck level to correct ambiguous cases during prediction. We conducted experiments using real noisy far-field data from the MISP challenge 2021 and achieved SOTA results compared to existing small-footprint KWS models. Our best score of 0.126 is highly competitive against larger models like 3D-ResNet, which is 0.122, but ours is much smaller at 473K compared to 13M",
    "checked": true,
    "id": "ca0781a2185f29b95cb0d7b627911a3e4162c975",
    "semantic_title": "small footprint multi-channel network for keyword spotting with centroid based awareness",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23b_interspeech.html": {
    "title": "Few-shot Class-incremental Audio Classification Using Adaptively-refined Prototypes",
    "volume": "main",
    "abstract": "New classes of sounds constantly emerge with a few samples, making it challenging for models to adapt to dynamic acoustic environments. This challenge motivates us to address the new problem of few-shot class-incremental audio classification. This study aims to enable a model to continuously recognize new classes of sounds with a few training samples of new classes while remembering the learned ones. To this end, we propose a method to generate discriminative prototypes and use them to expand the model's classifier for recognizing sounds of new and learned classes. The model is first trained with a random episodic training strategy, and then its backbone is used to generate the prototypes. A dynamic relation projection module refines the prototypes to enhance their discriminability. Results on two datasets (derived from the corpora of Nsynth and FSD-MIX-CLIPS) show that the proposed method exceeds three state-of-the-art methods in average accuracy and performance dropping rate",
    "checked": true,
    "id": "178677fe7e568509249e657635287bf6285ba00e",
    "semantic_title": "few-shot class-incremental audio classification using adaptively-refined prototypes",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vali23_interspeech.html": {
    "title": "Interpretable Latent Space Using Space-Filling Curves for Phonetic Analysis in Voice Conversion",
    "volume": "main",
    "abstract": "Vector quantized variational autoencoders (VQ-VAE) are well-known deep generative models, which map input data to a latent space that is used for data generation. Such latent spaces are unstructured and can thus be difficult to interpret. Some earlier approaches have introduced a structure to the latent space through supervised learning by defining data labels as latent variables. In contrast, we propose an unsupervised technique incorporating space-filling curves into vector quantization (VQ), which yields an arranged form of latent vectors such that adjacent elements in the VQ codebook refer to similar content. We applied this technique to the latent codebook vectors of a VQ-VAE, which encode the phonetic information of a speech signal in a voice conversion task. Our experiments show there is a clear arrangement in latent vectors representing speech phones, which clarifies what phone each latent vector corresponds to and facilitates other detailed interpretations of latent vectors",
    "checked": true,
    "id": "4f3e14e1d39ca360c76239dde618ea44500ed98a",
    "semantic_title": "interpretable latent space using space-filling curves for phonetic analysis in voice conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tulchinskii23_interspeech.html": {
    "title": "Topological Data Analysis for Speech Processing",
    "volume": "main",
    "abstract": "We apply topological data analysis (TDA) to speech classification problems and to the introspection of a pretrained speech model, HuBERT. To this end, we introduce a number of topological and algebraic features derived from Transformer attention maps and embeddings. We show that a simple linear classifier built on top of such features outperforms a fine-tuned classification head. We achieve an improvement of about 9% accuracy and 5% ERR on two common datasets; on CREMA-D, the proposed feature set reaches a new state of the art performance with accuracy 80.155. We also show that topological features are able to reveal functional roles of speech Transformer heads; e.g., we find the heads capable to distinguish between pairs of sample sources (natural/synthetic) or voices without any downstream fine-tuning. Our results demonstrate that TDA is a promising new approach for speech analysis, especially for tasks that require structural prediction",
    "checked": true,
    "id": "12c37eaa9ab37d66e1b014fd79b62bf544522065",
    "semantic_title": "topological data analysis for speech processing",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jang23_interspeech.html": {
    "title": "Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation",
    "volume": "main",
    "abstract": "Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark",
    "checked": true,
    "id": "e798e240d58cb8f38257a2bbdc85f1392166d644",
    "semantic_title": "recycle-and-distill: universal compression strategy for transformer-based speech ssl models with attention map reusing and masking distillation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koppelmann23_interspeech.html": {
    "title": "Personalized Acoustic Scene Classification in Ultra-low Power Embedded Devices Using Privacy-preserving Data Augmentation",
    "volume": "main",
    "abstract": "In this work we present an adaptation method for personalized acoustic scene classification in ultra-low power embedded devices (EDs). The computational limitation of EDs and a large variety of acoustic scenes may lead to poor performance of the embedded classifier in specific real-world user environments. We propose a semi-supervised scheme that estimates the audio feature distribution at ED level and then samples this statistical model to generate artificial data points which emulate user-specific audio features. Then, a second, cloud-based classifier assigns pseudo labels to samples, which are merged with existing labeled data for retraining the embedded classifier. The proposed method leads to significant performance improvements on user-specific data sets and does neither require a persistent connection to a cloud service nor the transmission of raw audio or audio features. It thus results in low data rates, high utility, and privacy-preservation",
    "checked": true,
    "id": "0e85062bba41b628de2d96cac8d77d03191e6a5c",
    "semantic_title": "personalized acoustic scene classification in ultra-low power embedded devices using privacy-preserving data augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23_interspeech.html": {
    "title": "Background Domain Switch: A Novel Data Augmentation Technique for Robust Sound Event Detection",
    "volume": "main",
    "abstract": "Data augmentation is a key component to achieve robust and generalizable performance in sound event detection (SED). A well trained SED model should be able to resist the interference of non-target audio events and maintain a robust recognition rate under unknown and possibly mismatched testing conditions. In this study, we propose a novel background domain switch (BDS) data augmentation technique for SED. BDS utilizes a trained SED model on-the-fly to detect backgrounds in audio clips, and switches them among the data points to increase sample variability. This approach can be easily combined with other types of data augmentation techniques. We evaluate the effectiveness of BDS by applying it to several state-of-the-art SED frameworks, and used both publicly available datasets as well as a synthesized mismatched test set. Experiment results systematically show that BDS obtains significant performance improvements from all evaluation aspects. The code is available at: https://github.com/boschresearch/soundseebackgrounddomainswitch",
    "checked": true,
    "id": "cfcf644b1805c24fdbc9e6cddfac41383a2f0d8f",
    "semantic_title": "background domain switch: a novel data augmentation technique for robust sound event detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hou23_interspeech.html": {
    "title": "Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning",
    "volume": "main",
    "abstract": "Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans' listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR",
    "checked": true,
    "id": "e74522543b25c3eca86f7dd62793dbf8d527e8be",
    "semantic_title": "joint prediction of audio event and annoyance rating in an urban soundscape by hierarchical graph representation learning",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23fa_interspeech.html": {
    "title": "Anomalous Sound Detection Using Self-Attention-Based Frequency Pattern Analysis of Machine Sounds",
    "volume": "main",
    "abstract": "Different machines can exhibit diverse frequency patterns in their emitted sound. This feature has been recently explored in anomaly sound detection and reached state-of-the-art performance. However, existing methods rely on the manual or empirical determination of the frequency filter by observing the effective frequency range in the training data, which may be impractical for general application. This paper proposes an anomalous sound detection method using self-attention-based frequency pattern analysis and spectral-temporal information fusion. Our experiments demonstrate that the self-attention module automatically and adaptively analyses the effective frequencies of a machine sound and enhances that information in the spectral feature representation. With spectral-temporal information fusion, the obtained audio feature eventually improves the anomaly detection performance on the DCASE 2020 Challenge Task 2 dataset",
    "checked": true,
    "id": "5656e4b77179dbc012460460bf92334f45b9235a",
    "semantic_title": "anomalous sound detection using self-attention-based frequency pattern analysis of machine sounds",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23c_interspeech.html": {
    "title": "Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions",
    "volume": "main",
    "abstract": "Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained crossmodal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical crossmodal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets",
    "checked": true,
    "id": "29dc27e49654fc7cd0a9001bbf44a57d78bb74ca",
    "semantic_title": "improving audio-text retrieval via hierarchical cross-modal interaction and auxiliary captions",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bn23_interspeech.html": {
    "title": "Differential Privacy enabled Dementia Classification: An Exploration of the Privacy-Accuracy Trade-off in Speech Signal Data",
    "volume": "main",
    "abstract": "Early detection of dementia is critical for effective symptom management. Recent studies have aimed to develop machine learning (ML) models to identify dementia onset and severity using language and speech features. However, existing methods can lead to serious privacy concerns due to sensitive data collected from a vulnerable population. In this work, we aim to establish the privacy-accuracy tradeoff benchmark for dementia classification models using audio and speech features. Specifically, we explore the effects of differential privacy (DP) on the training phase of the audio model. We then compare the classification accuracy of DP and non-DP models using a publicly available dataset. The resultant comparison provides useful insights to make informed decisions about the need for balancing privacy and accuracy tradeoff for dementia classification tasks. Our findings have implications for real-world deployment of ML models to support early detection and effective management of dementia",
    "checked": true,
    "id": "7304c74495f8c4630a852f614e8a613a3b0e410b",
    "semantic_title": "differential privacy enabled dementia classification: an exploration of the privacy-accuracy trade-off in speech signal data",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ka_interspeech.html": {
    "title": "Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech",
    "volume": "main",
    "abstract": "Effective speech emotional representations play a key role in Speech Emotion Recognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional speech samples are more difficult and expensive to acquire compared with Neutral style speech, which causes one issue that most related works unfortunately neglect: imbalanced datasets. Models might overfit to the majority Neutral class and fail to produce robust and effective emotional representations. In this paper, we propose an Emotion Extractor to address this issue. We use augmentation approaches to train the model and enable it to extract effective and generalizable emotional representations from imbalanced datasets. Our empirical results show that (1) for the SER task, the proposed Emotion Extractor surpasses the state-of-the-art baseline on three imbalanced datasets; (2) the produced representations from our Emotion Extractor benefit the TTS model, and enable it to synthesize more expressive speech",
    "checked": true,
    "id": "044e47bc8995d603122963d92b84aa6dabea1a53",
    "semantic_title": "learning emotional representations from imbalanced speech data for speech emotion recognition and emotional text-to-speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/behera23_interspeech.html": {
    "title": "Towards Multi-Lingual Audio Question Answering",
    "volume": "main",
    "abstract": "Audio Question Answering (AQA) is a multi-modal translation task where a system analyzes an audio signal and a natural language question to generate a desirable natural language answer. AQA has been primarily studied through the lens of the English language. However, addressing AQA in other languages, in the same manner, would require a considerable amount of resources. This paper proposes scalable solutions to multi-lingual audio question answering on both data and modeling fronts. We propose mClothoAQA, a translation-based multi-lingual AQA dataset in eight languages. The dataset consists of 1991 audio files and nearly 0.3 million question-answer pairs. Finally, we introduce a multi-lingual AQA model and demonstrate its strong performance in eight languages. The dataset and code can be accessed at https://github.com/swarupbehera/mAQA",
    "checked": true,
    "id": "431ec63f76d5d64c18e3ab92008cca98975a1678",
    "semantic_title": "towards multi-lingual audio question answering",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/aldarmaki23_interspeech.html": {
    "title": "Diacritic Recognition Performance in Arabic ASR",
    "volume": "main",
    "abstract": "In Arabic text, most vowels are encoded in the form of diacritics that are often omitted, so most speech corpora and ASR models are undiacritized. Text-based diacritization has previously been used to preprocess the input or post-processs ASR hypotheses. It is generally believed that input diacritization degrades ASR quality, but no systematic evaluation of ASR diacritization performance has been conducted to date. We experimentally clarify whether input diacritiztation indeed degrades ASR quality and compare ASR diacritization with text-based diacritization. We fine-tune pre-trained ASR models on transcribed speech with different diacritization conditions: manual, automatic, and no diacritization. We isolate diacritic recognition performance from the overall ASR performance using coverage and precision metrics. We find that ASR diacritization significantly outperforms text-based diacritization, particularly when the ASR model is fine-tuned with manually diacritized transcripts",
    "checked": true,
    "id": "cd31445ba366d10bd11e2a7b2da5c8281c8f1564",
    "semantic_title": "diacritic recognition performance in arabic asr",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kolehmainen23_interspeech.html": {
    "title": "Personalization for BERT-based Discriminative Speech Recognition Rescoring",
    "volume": "main",
    "abstract": "Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%",
    "checked": true,
    "id": "f520b19cbd33bc9b35602436b1879ea50cbaf0ed",
    "semantic_title": "personalization for bert-based discriminative speech recognition rescoring",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/krishnan23_interspeech.html": {
    "title": "On the N-gram Approximation of Pre-trained Language Models",
    "volume": "main",
    "abstract": "Large pre-trained language models (PLMs) have shown remarkable performance across various natural language understanding (NLU) tasks, particularly in low-resource settings. Nevertheless, their potential in Automatic Speech Recognition (ASR) remains largely unexplored. This study investigates the potential usage of PLMs for language modelling in ASR. We compare the application of large-scale text sampling and probability conversion for approximating GPT-2 into an n-gram model. Furthermore, we introduce a vocabulary-restricted decoding method for random sampling, and evaluate the effects of domain difficulty and data size on the usability of generated text. Our findings across eight domain-specific corpora support the use of sampling-based approximation and show that interpolating with a large sampled corpus improves test perplexity over a baseline trigram by 15%. Our vocabulary-restricted decoding method pushes this improvement further by 5% in domain-specific settings",
    "checked": true,
    "id": "821d918fd7c316a1d48579979fa7bde2f7a63c50",
    "semantic_title": "on the n-gram approximation of pre-trained language models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23g_interspeech.html": {
    "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
    "volume": "main",
    "abstract": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers' true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction",
    "checked": true,
    "id": "b3c7d778e5bdafa8b2cd62b996d0e6dfc670effc",
    "semantic_title": "record deduplication for entity distribution modeling in asr transcripts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/agrawal23_interspeech.html": {
    "title": "Learning When to Trust Which Teacher for Weakly Supervised ASR",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) training can utilize multiple experts as teacher models, each trained on a specific domain or accent. Teacher models may be opaque in nature since their architecture may be not be known or their training cadence is different from that of the student ASR model. Still, the student models are updated incrementally using the pseudo-labels generated independently by the expert teachers. In this paper, we exploit supervision from multiple domain experts in training student ASR models. This training strategy is especially useful in scenarios where few or no human transcriptions are available. To that end, we propose a Smart-Weighter mechanism that selects an appropriate expert based on the input audio, and then trains the student model in an unsupervised setting. We show the efficacy of our approach using LibriSpeech and LibriLight benchmarks and find an improvement of 4 to 25% over baselines that uniformly weight all the experts, use a single expert model, or combine experts using ROVER",
    "checked": true,
    "id": "124c16b1243ef8f727df9db70d3c2f1b47ec31c1",
    "semantic_title": "learning when to trust which teacher for weakly supervised asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23f_interspeech.html": {
    "title": "Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer",
    "volume": "main",
    "abstract": "Domain adaptation using text-only corpus is challenging in end-to-end(E2E) speech recognition. Adaptation by synthesizing audio from text through TTS is resource-consuming. We present a method to learn Unified Speech-Text Representation in Conformer Transducer(USTR-CT) to enable fast domain adaptation using the text-only corpus. Different from the previous textogram method, an extra text encoder is introduced in our work to learn text representation and is removed during inference, so there is no modification for online deployment. To improve the efficiency of adaptation, single-step and multistep adaptations are also explored. The experiments on adapting LibriSpeech to SPGISpeech show the proposed method reduces the word error rate(WER) by relatively 44% on the target domain, which is better than those of TTS method and textogram method. Also, it is shown the proposed method can be combined with internal language model estimation(ILME) to further improve the performance",
    "checked": true,
    "id": "f035b38c98d79dd43dda10b919604b1d46cb63df",
    "semantic_title": "text-only domain adaptation using unified speech-text representation in transducer",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23e_interspeech.html": {
    "title": "Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model",
    "volume": "main",
    "abstract": "In this paper, we show that representations capturing syllabic units emerge when training a self-supervised speech model with a visually-grounded training objective. We demonstrate that a nearly identical model architecture (HuBErT) trained with a masked language modeling loss does not exhibit this same ability, suggesting that the visual grounding objective is responsible for the emergence of this phenomenon. We propose the use of a minimum cut algorithm to automatically predict syllable boundaries in speech, followed by a 2-stage clustering method to group identical syllables together. We show that our model not only outperforms a state-of-the-art syllabic segmentation method on the language it was trained on (English), but also generalizes in a zero-shot fashion to Estonian. Finally, we show that the same model is capable of zero-shot generalization for a word segmentation task on 4 other languages from the Zerospeech Challenge, in some cases beating the previous state-of-the-art",
    "checked": true,
    "id": "47ba6504f14a3d16f25b1b9afaf5531e41671faf",
    "semantic_title": "syllable discovery and cross-lingual generalization in a visually grounded, self-supervised speech model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23d_interspeech.html": {
    "title": "Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization",
    "volume": "main",
    "abstract": "We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. we design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available here",
    "checked": true,
    "id": "10e8dc07ea256c6a88d7043cf135417402ed38f4",
    "semantic_title": "prompting the hidden talent of web-scale speech models for zero-shot task generalization",
    "citation_count": 16,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moore23_interspeech.html": {
    "title": "Progress and Prospects for Spoken Language Technology: Results from Five Sexennial Surveys",
    "volume": "main",
    "abstract": "Every six years (since 1997), a survey has been conducted at the IEEE workshop on Automatic Speech Recognition and Understanding (ASRU). The aim has been to gain an insight into the research community's perspective on the 'progress and prospects' for spoken language technology. These surveys have been based on a set of 'statements' describing possible scenarios, and respondents are asked to estimate the year (in the future or in the past) when each given scenario might be realised. A number of the statements have appeared in multiple surveys, hence it has been possible to track changes in opinion over time. This paper presents the combined results from five such surveys, the most recent of which was conducted at ASRU-2021. The latest results reveal a dramatic increase in optimism",
    "checked": true,
    "id": "9098d7a04eb5d8a1d64e57e808d240a426a16be1",
    "semantic_title": "progress and prospects for spoken language technology: results from five sexennial surveys",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sanabria23_interspeech.html": {
    "title": "Acoustic Word Embeddings for Untranscribed Target Languages with Continued Pretraining and Learned Pooling",
    "volume": "main",
    "abstract": "Acoustic word embeddings are typically created by training a pooling function using pairs of word-like units. For unsupervised systems, these are mined using k-nearest neighbor (KNN) search, which is slow. Recently, mean-pooled representations from a pre-trained self-supervised English model were suggested as a promising alternative, but their performance on target languages was not fully competitive. Here, we explore improvements to both approaches: we use continued pre-training to adapt the self-supervised model to the target language, and we use a multilingual phone recognizer (MPR) to mine phone n-gram pairs for training the pooling function. Evaluating on four languages, we show that both methods outperform a recent approach on word discrimination. Moreover, the MPR method is orders of magnitude faster than KNN, and is highly data efficient. We also show a small improvement from performing learned pooling on top of the continued pre-trained representations",
    "checked": true,
    "id": "2c82c551b151835cec78df7a2ab86f2a58d0a682",
    "semantic_title": "acoustic word embeddings for untranscribed target languages with continued pretraining and learned pooling",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23d_interspeech.html": {
    "title": "CASA-ASR: Context-Aware Speaker-Attributed ASR",
    "volume": "main",
    "abstract": "Recently, speaker-attributed automatic speech recognition (SAASR) has attracted a wide attention, which aims at answering the question \"who spoke what\". Different from modular systems, end-to-end (E2E) SA-ASR minimizes the speaker-dependent recognition errors directly and shows a promising applicability. In this paper, we propose a context-aware SAASR (CASA-ASR) model by enhancing the contextual modeling ability of E2E SA-ASR. Specifically, in CASA-ASR, a contextual text encoder is involved to aggregate the semantic information of the whole utterance, and a context-dependent scorer is employed to model the speaker discriminability by contrasting with speakers in the context. In addition, a two-pass decoding strategy is further proposed to fully leverage the contextual modeling ability resulting in a better recognition performance. Experimental results on AliMeeting corpus show that the proposed CASA-ASR model outperforms the original E2E SAASR system with a relative improvement of 11.76% in terms of speaker-dependent character error rate",
    "checked": true,
    "id": "eaf765c07764a802c5200a9abd739a921349caab",
    "semantic_title": "casa-asr: context-aware speaker-attributed asr",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/takahashi23_interspeech.html": {
    "title": "Unsupervised Learning of Discrete Latent Representations with Data-Adaptive Dimensionality from Continuous Speech Streams",
    "volume": "main",
    "abstract": "This work presents a novel deep generative model for unsupervised learning of sparse binary feature representations with data-adaptive dimensionality directly from continuous speech streams. Sharing the critical assumption of unbounded latent dimensionality with previously proposed Bayesian non-parametric approaches, our proposed model can capture the much richer, non-Markovian dependencies between its latent representations. The present work focuses on an investigation of our proposed model's performance in learning linguistically meaningful representations under challenging, realistic scenarios. We train our model with highly speaker-imbalanced datasets and evaluate it on the ABX phone discriminability test. Our model achieves a promising, competitive performance to the state-of-the-art model, despite its huge disadvantage: limited or no access to speaker information during training",
    "checked": true,
    "id": "1355151151f18fb3e98e65b57549ea2e87d5cf80",
    "semantic_title": "unsupervised learning of discrete latent representations with data-adaptive dimensionality from continuous speech streams",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23n_interspeech.html": {
    "title": "AD-TUNING: An Adaptive CHILD-TUNING Approach to Efficient Hyperparameter Optimization of Child Networks for Speech Processing Tasks in the SUPERB Benchmark",
    "volume": "main",
    "abstract": "In this paper, we propose AD-TUNING, an adaptive CHILD-TUNING approach for hyperparameter tuning of child networks. To address the issue of selecting an optimal hyperparameter set P, which often varies for different tasks in CHILD-TUNING, we first analyze the distribution of parameter importance to ascertain the range of P. Next, we propose a simple yet efficient early-stop algorithm to select the appropriate child network from different sizes for various speech tasks. When evaluated on seven speech processing tasks in the SUPERB benchmark, our proposed framework only requires fine-tuning less than 0.1%~10% of pre-trained model parameters for each task to achieve state-of-the-art results in most of the tasks. For instance, the DER of the speaker diarization task is 9.22% relatively lower than the previously reported best results. Other benchmark results are also very competitive. Our code is available at https://github.com/liyunlongaaa/AD-TUNING",
    "checked": true,
    "id": "8d3bbe7ded99577cf07844e63a58c51aaebe25e5",
    "semantic_title": "ad-tuning: an adaptive child-tuning approach to efficient hyperparameter optimization of child networks for speech processing tasks in the superb benchmark",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wong23_interspeech.html": {
    "title": "Distilling knowledge from Gaussian process teacher to neural network student",
    "volume": "main",
    "abstract": "Neural Networks (NN) and Gaussian Processes (GP) are different modelling approaches. The former stores characteristics of the training data in its many parameters, and then performs inference by parsing inputs through these parameters. The latter instead performs inference by computing a similarity between the test and training inputs, and then predicts test outputs that are correlated with the reference training outputs of similar inputs. These models may be combined to leverage upon their diversity. However, both combination and the matrix computations for GP inference are expensive. This paper investigates whether a NN student is able to effectively learn from the information distilled from a GP or ensemble teacher. It is computationally cheaper to infer using this student. Experiments on the speechocean762 spoken language assessment dataset suggest that learning is effective",
    "checked": true,
    "id": "480c5f9471a8822eacd04f2545b01f23539313f7",
    "semantic_title": "distilling knowledge from gaussian process teacher to neural network student",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhati23_interspeech.html": {
    "title": "Segmental SpeechCLIP: Utilizing Pretrained Image-text Models for Audio-Visual Learning",
    "volume": "main",
    "abstract": "Visually grounded models learn from paired images and their spoken captions. Recently, there have been attempts to utilize the visually grounded models trained from images and their corresponding text captions, such as CLIP, to improve speech-based visually grounded models' performance. However, the majority of these models only utilize the pretrained image encoder. Cascaded SpeechCLIP attempted to generate localized word-level information and utilize both the pretrained image and text encoders. Despite using both, they noticed a substantial drop in retrieval performance. Here, we propose to use a hierarchical segmental audio encoder that can generate a sequence of word-like units from audio. We use the pretrained CLIP text encoder on top of these word-like units representations and show significant improvements over the cascaded variant of SpeechCLIP",
    "checked": true,
    "id": "1617d389b7947161f2943e2d30afeb1856052b14",
    "semantic_title": "segmental speechclip: utilizing pretrained image-text models for audio-visual learning",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jacobs23_interspeech.html": {
    "title": "Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili",
    "volume": "main",
    "abstract": "We consider hate speech detection through keyword spotting on radio broadcasts. One approach is to build an automatic speech recognition (ASR) system for the target low-resource language. We compare this to using acoustic word embedding (AWE) models that map speech segments to a space where matching words have similar vectors. We specifically use a multilingual AWE model trained on labelled data from well-resourced languages to spot keywords in data in the unseen target language. In contrast to ASR, the AWE approach only requires a few keyword exemplars. In controlled experiments on Wolof and Swahili where training and test data are from the same domain, an ASR model trained on just five minutes of data outperforms the AWE approach. But in an in-the-wild test on Swahili radio broadcasts with actual hate speech keywords, the AWE model (using one minute of template data) is more robust, giving similar performance to an ASR system trained on 30 hours of labelled data",
    "checked": true,
    "id": "ce1c8b4655157435f87f9b4c5eb3589e64d3f0da",
    "semantic_title": "towards hate speech detection in low-resource languages: comparing asr to acoustic word embeddings on wolof and swahili",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vandermerwe23_interspeech.html": {
    "title": "Mitigating Catastrophic Forgetting for Few-Shot Spoken Word Classification Through Meta-Learning",
    "volume": "main",
    "abstract": "We consider the problem of few-shot spoken word classification in a setting where a model is incrementally introduced to new word classes. This would occur in a user-defined keyword system where new words can be added as the system is used. In such a continual learning scenario, a model might start to misclassify earlier words as newer classes are added, i.e. catastrophic forgetting. To address this, we propose an extension to model-agnostic meta-learning (MAML). In our new approach, each inner learning loop—where a model \"learns how to learn\" new classes—ends with a single gradient update using stored templates from all the classes that the model has already seen (one template per class). We compare this method to OML (another extension of MAML) in few-shot isolated-word classification experiments on Google Commands and FACC. Our method consistently outperforms OML in experiments where the number of shots and the final number of classes are varied",
    "checked": true,
    "id": "af849704754c3cb8ea621cca465c4c13372c9148",
    "semantic_title": "mitigating catastrophic forgetting for few-shot spoken word classification through meta-learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/polacek23_interspeech.html": {
    "title": "Online Punctuation Restoration using ELECTRA Model for streaming ASR Systems",
    "volume": "main",
    "abstract": "In this work, we propose a lightweight online approach to automatic punctuation restoration (APR), which can be utilized in speech transcription systems for, e.g., live captioning TV or radio streams. It uses only text input without prosodic features and a fine-tuned ELECTRA-Small model with a two-layer classification head. It allows for restoring question marks, commas, and periods with a very short inference time and a low latency of just three words. Our APR scheme is first tuned and compared to other architectures on a set of manual TV news transcripts. The resulting system is then compared to another real-time APR module utilizing a recurrent network and a combination of text and acoustic features. The test data we use contains automatic transcripts of radio talks and TV debates; we are also publishing this data. The results show that our APR module performs better than the above-mentioned system and yields on the two test sets an average F1 of 71.2% and 69.4%, respectively",
    "checked": true,
    "id": "01372fd6f9c099ea351050a99e547ebe2a36f746",
    "semantic_title": "online punctuation restoration using electra model for streaming asr systems",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23s_interspeech.html": {
    "title": "Language Agnostic Data-Driven Inverse Text Normalization",
    "volume": "main",
    "abstract": "The rise of automatic speech recognition (ASR) models has created an urgent need for converting spoken language into written text to provide better user experiences. This has drawn the attention of researchers, particularly for real-time on-device ASR deployment, towards the inverse text normalization (ITN) problem. While data-driven ITN methods have shown great promise in recent studies, the lack of labeled spoken-written datasets is hindering the development for non-English data-driven ITN. To bridge this gap, we propose a language-agnostic data-driven ITN framework that leverages data augmentation and neural machine translation specifically designed for real-time miniature models and low-resource languages. Additionally, we have developed an evaluation method for language-agnostic ITN models when only English data is available. Our empirical evaluation attests to the efficacy of this language-agnostic ITN modeling with data augmentation approach for multiple non-English languages",
    "checked": true,
    "id": "68f9e81413ea8bfbc10e88191983a7f4dc7b1b30",
    "semantic_title": "language agnostic data-driven inverse text normalization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23j_interspeech.html": {
    "title": "How to Estimate Model Transferability of Pre-Trained Speech Models?",
    "volume": "main",
    "abstract": "In this work, we introduce a ''score-based assessment'' framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks. We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations. Our framework efficiently computes transferability scores without actual fine-tuning of candidate models or layers by making a temporal independent hypothesis. We evaluate some popular supervised speech models (and self-supervised speech models in cross-layer and cross-model settings using public data. Experimental results show a high Spearman's rank correlation and low p-value between our estimation framework and fine-tuning ground truth. Our proposed transferability framework requires less computational time and resources, making it a resource-saving and time-efficient approach for tuning speech foundation models",
    "checked": true,
    "id": "81a6b9cba431287c46fc29148ccbf6e01bf52d30",
    "semantic_title": "how to estimate model transferability of pre-trained speech models?",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ihori23_interspeech.html": {
    "title": "Transcribing Speech as Spoken and Written Dual Text Using an Autoregressive Model",
    "volume": "main",
    "abstract": "This paper proposes a novel method to jointly generate spoken and written text from input speech for expanding use cases of speech-based applications. The spoken text generated using speech-to-spoken text systems, i.e., speech recognition systems, has disfluencies and no punctuation marks. Thus, spoken text is often converted into written text using a spoken text-to-written text system. However, this cascading is unsuitable for overall optimization and computationally expensive. Although a speech-to-written-text system that directly outputs the written text from the speech is also developed, it cannot output the spoken text. To efficiently produce both spoken and written text from speech, our key advance is to handle a joint text of spoken and written texts in an autoregressive model. This enables us to correctly generate both spoken and written texts by considering their dependencies via a single decoding process. Our experiments demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "1ddf41cf13aeda48ce9a79df0eb08e8255bc233f",
    "semantic_title": "transcribing speech as spoken and written dual text using an autoregressive model",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuksel23_interspeech.html": {
    "title": "NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning",
    "volume": "main",
    "abstract": "This paper introduces NoRefER, a novel referenceless quality metric for automatic speech recognition (ASR) systems. Traditional reference-based metrics for evaluating ASR systems require costly ground-truth transcripts. NoRefER overcomes this limitation by fine-tuning a multilingual language model for pair-wise ranking ASR hypotheses using contrastive learning with Siamese network architecture. The self-supervised NoRefER exploits the known quality relationships between hypotheses from multiple compression levels of an ASR for learning to rank intra-sample hypotheses by quality, which is essential for model comparisons. The semi-supervised version also uses a referenced dataset to improve its inter-sample quality ranking, which is crucial for selecting potentially erroneous samples. The results indicate that NoRefER correlates highly with reference-based metrics and their intra-sample ranks, indicating a high potential for referenceless ASR evaluation or a/b testing",
    "checked": true,
    "id": "b99733670ef7c2553944208a16a68968cbb85946",
    "semantic_title": "norefer: a referenceless quality metric for automatic speech recognition via semi-supervised language model fine-tuning with contrastive learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23c_interspeech.html": {
    "title": "Scaling Laws for Discriminative Speech Recognition Rescoring Models",
    "volume": "main",
    "abstract": "Recent studies have found that model performance has a smooth power-law relationship, or scaling laws, with training data and model size, for a wide range of problems. These scaling laws allow one to choose nearly optimal data and model sizes. We study whether this scaling property is also applicable to second-pass rescoring, which is an important component of speech recognition systems. We focus on RescoreBERT as the rescoring model, which uses a pre-trained Transformer-based architecture fined tuned with an ASR discriminative loss. Using such a rescoring model, we show that the word error rate (WER) follows a scaling law for over two orders of magnitude as training data and model size increase. In addition, it is found that a pre-trained model would require less data than a randomly initialized model of the same size, representing effective data transferred from pre-training step. This effective data transferred is found to also follow a scaling law with the data and model size",
    "checked": true,
    "id": "a20c6ce80872dbc6a8e6403b2a366973061a9f89",
    "semantic_title": "scaling laws for discriminative speech recognition rescoring models",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23w_interspeech.html": {
    "title": "Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition",
    "volume": "main",
    "abstract": "Energy-based language models (ELMs) parameterize an unnormalized distribution for natural sentences and are radically different from popular autoregressive language models (ALMs). As an important application, ELMs have been successfully used as a means for calculating sentence scores in speech recognition, but they all use less-modern CNN or LSTM networks. The recent progress in Transformer networks and large pretrained models such as BERT and GPT2 opens new possibility to further advancing ELMs. In this paper, we explore different architectures of energy functions and different training methods to investigate the capabilities of ELMs in rescoring for speech recognition, all using large pretrained models as backbones. Extensive experiments are conducted on two datasets, AISHELL-1 and WenetSpeech. The results show that the best ELM achieves competitive results with the finetuned GPT2 and performs significantly better than the finetuned BERT. Further analysis show that the ELM obtains better confidence estimate performance than the finetuned GPT2",
    "checked": true,
    "id": "1f99d057382445a7c83f15693e287f56d6305185",
    "semantic_title": "exploring energy-based language models with different architectures and training methods for speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23d_interspeech.html": {
    "title": "Memory Augmented Lookup Dictionary Based Language Modeling for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Recent studies have shown that using an external Language Model (LM) benefits the end-to-end Automatic Speech Recognition (ASR). However, predicting tokens that appear less frequently in the training set is still quite challenging. The long-tail prediction problems have been widely studied in many applications, but only been addressed by a few studies for ASR and LMs. In this paper, we propose a new memory augmented lookup dictionary based Transformer architecture for LM. The newly introduced lookup dictionary incorporates rich contextual information in training set, which is vital to correctly predict long-tail tokens. With intensive experiments on Chinese and English data sets, our proposed method is proved to outperform the baseline Transformer LM by a great margin on both word/character error rate and tail tokens error rate. This is achieved without impact on the decoding efficiency",
    "checked": true,
    "id": "b0c037b4037597f6e023cf300ea864bb9b7aa6dc",
    "semantic_title": "memory augmented lookup dictionary based language modeling for automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/iwamoto23_interspeech.html": {
    "title": "Memory Network-Based End-To-End Neural ES-KMeans for Improved Word Segmentation",
    "volume": "main",
    "abstract": "Unsupervised word learning from unlabeled speech is a fundamental problem in zero-resource speech processing, which enables dialogue agents to learn new words directly from spoken utterances. The embedded segmental K-means (ES-KMeans) is a representative unsupervised word segmentation method. However, it has a heterogeneous structure consisting of word boundary search based on Dynamic Programming, segment embedding, and K-Means clustering, which prevents unified optimization. This paper proposes an end-to-end neural network version of the ES-KMeans model. We apply the memory network to hold a dictionary of word embeddings and realize the word boundary search and the clustering respectively as forward and backward propagations. Moreover, we replace the fixed embedding function of the original method with a learnable neural network. Experimental results using the ZeroSpeech Challenge 2020 package show the proposed approach provides superior performance to the state-of-the-art methods",
    "checked": true,
    "id": "88768f2934aa59d668a3e18a4e9c77694702230c",
    "semantic_title": "memory network-based end-to-end neural es-kmeans for improved word segmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23b_interspeech.html": {
    "title": "Retraining-free Customized ASR for Enharmonic Words Based on a Named-Entity-Aware Model and Phoneme Similarity Estimation",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (E2E-ASR) has the potential to improve performance, but a specific issue that needs to be addressed is the difficulty it has in handling enharmonic words: named entities (NEs) with the same pronunciation and part of speech that are spelled differently. This often occurs with Japanese personal names that have the same pronunciation but different Kanji characters. Since such NE words tend to be important keywords, ASR easily loses user trust if it misrecognizes them. To solve these problems, this paper proposes a novel retraining-free customized method for E2E-ASRs based on a named-entity-aware E2E-ASR model and phoneme similarity estimation. Experimental results show that the proposed method improves the target NE character error rate by 35.7% on average relative to the conventional E2E-ASR model when selecting personal names as a target NE",
    "checked": true,
    "id": "2c9beae437c30cd16bea013c10568e0a428feb88",
    "semantic_title": "retraining-free customized asr for enharmonic words based on a named-entity-aware model and phoneme similarity estimation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23c_interspeech.html": {
    "title": "Lightweight and Efficient Spoken Language Identification of Long-form Audio",
    "volume": "main",
    "abstract": "State-of-the-art Spoken Language Identification (SLI) systems usually focus on tackling short audio clips, and thus their performance degrade drastically when applied to long-form audio, such as podcast, which poses peculiar challenges to existing SLI approaches due to its long duration and diverse content that frequently involves multiple speakers as well as various languages, topics, and speech styles. In this paper, we propose the first system to tackle SLI for long-form audio using podcast data by training a lightweight, multi-class feedforward neural classifier using speaker embeddings as input. We demonstrate that our approach can make inference on long audio input efficiently; furthermore, our system can handle long audio files with multiple speakers and can be further extended into utterance-level inference and code-switching detection, which is currently not covered by any existing SLI system",
    "checked": true,
    "id": "1ff7218b81c3d54eb4c17e70def2d0675de3bd91",
    "semantic_title": "lightweight and efficient spoken language identification of long-form audio",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mishra23_interspeech.html": {
    "title": "End to End Spoken Language Diarization with Wav2vec Embeddings",
    "volume": "main",
    "abstract": "The performance of the available end-to-end (E2E) spoken language diarization (LD) systems is biased toward primary language. This is due to the unavailability of sufficient secondary language data. Because in code-switched (CS) utterances, the duration of the primary language is significant over the secondary language. Hence, to resolve the issue, this work initially uses wav2vec (W2V) pre-trained embedding in place of x-vector and can reduce the primary language bias and provides a relative improvement of 30.7% in terms of Jaccard error rate (JER) over the baseline x-vector based E2E (X-E2E) framework. Further, the performance of LD is improved by fine-tuning the W2V embedding extractor and modifying the temporal aggregation strategy from statistical pooling to attention pooling. The Final performance achieved in terms of JER is 22.5, which provides a relative improvement of 38.8% and 62.6% over the standalone W2V fine-tuned and the baseline X-E2E framework, respectively",
    "checked": true,
    "id": "fc2236674339b0c280b16d452c811164f8fd09bb",
    "semantic_title": "end to end spoken language diarization with wav2vec embeddings",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nieto23_interspeech.html": {
    "title": "Efficient Spoken Language Recognition via Multilabel Classification",
    "volume": "main",
    "abstract": "Spoken language recognition (SLR) is the task of automatically identifying the language present in a speech signal. Existing SLR models are either too computationally expensive or too large to run effectively on devices with limited resources. For real-world deployment, a model should also gracefully handle unseen languages outside of the target language set, yet prior work has focused on closed-set classification where all input languages are known a-priori. In this paper we address these two limitations: we explore efficient model architectures for SLR based on convolutional networks, and propose a multilabel training strategy to handle non-target languages at inference time. Using the VoxLingua107 dataset, we show that our models obtain competitive results while being orders of magnitude smaller and faster than current state-of-the-art methods, and that our multilabel strategy is more robust to unseen non-target languages compared to multiclass classification",
    "checked": true,
    "id": "bdddcef337decd84c51d34091e52cc0e6f51204d",
    "semantic_title": "efficient spoken language recognition via multilabel classification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matejka23_interspeech.html": {
    "title": "Description and Analysis of ABC Submission to NIST LRE 2022",
    "volume": "main",
    "abstract": "This paper summarizes our efforts in the NIST Language Recognition Evaluations 2022 resulting in systems providing competitive performance. We provide both the description and analysis of the systems. We describe what data we have used to train our models, and we follow with embedding extractors and backend classifiers. After covering the architecture, we concentrate on post-evaluation analysis. We compare different topologies of DNN, different backend classifiers, and the impact of the data used to train them. We also report results with XLS-R pre-trained models. We present the performance of the systems in the Fixed condition, where participants are required to use only predefined data sets, and also in the Open condition allowing to use any data to train the systems",
    "checked": true,
    "id": "a570f70209140ef4d62fb3e8c68bf7465e549710",
    "semantic_title": "description and analysis of abc submission to nist lre 2022",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alumae23_interspeech.html": {
    "title": "Exploring the Impact of Pretrained Models and Web-Scraped Data for the 2022 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "This paper describes Vocapia-TalTech team systems developed for the 2022 NIST Language Recognition Evaluation (LRE22) which focused on spoken language identication of African languages, including low-resource languages. In both fixed and open conditions, our primary systems were fused from multiple individual systems using logistic regression. In the fixed condition, we largely relied on wav2vec2.0 conformer models pretrained on the provided training data. In the open condition, we used external pretrained wav2vec2.0 models, phonotactic models and features derived from a multilingual speech recognition system, and also augmented the provided target language development data with additional data scraped from the web. On the LRE22 evaluation data, our final fixed and open condition systems obtained excellent results, with primary metric Cact values of 0.111 and 0.067, respectively. A post-evaluation study shows that both pretrained models as well as additional data are important for accurate models",
    "checked": true,
    "id": "dfb3b676ea6cda7b184911db7fc60fd9a466ed57",
    "semantic_title": "exploring the impact of pretrained models and web-scraped data for the 2022 nist language recognition evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/villalba23_interspeech.html": {
    "title": "Advances in Language Recognition in Low Resource African Languages: The JHU-MIT Submission for NIST LRE22",
    "volume": "main",
    "abstract": "We present the effort of JHU-CLSP/HLTCOE and MIT Lincoln labs for NIST Language Recognition Evaluation (LRE) 2022. LRE22 consisted of a language detection task, i.e., determining whether a given target language was spoken in a speech segment. LRE22 focused on telephone and broadcast narrowband speech in African languages. Since LRE17, there has been large progress in neural embeddings, combined or not, with self-supervised models like Wav2Vec2. Therefore, one of our goals was to investigate these new models, i.e., ECAPA-TDNN, Res2Net or Wav2Vec2+ECAPA-TDNN, in the LRE scenario. In the fixed training condition, LRE22 target languages were only included in a small development set. Hence, we focused on tuning our models to exploit the limited data. For the open condition, we built a massive training set including African data, which improved Cprimary by 50% w.r.t. fixed. Wav2Vec2 embeddings were the best, outperforming ECAPA and Res2Net by 11 and 3%, respectively",
    "checked": true,
    "id": "51aca07d500c44ebde896b8df3b0388dd3ade489",
    "semantic_title": "advances in language recognition in low resource african languages: the jhu-mit submission for nist lre22",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23d_interspeech.html": {
    "title": "DeePMOS: Deep Posterior Mean-Opinion-Score of Speech",
    "volume": "main",
    "abstract": "We propose a deep neural network (DNN) based method that provides a posterior distribution of mean-opinion-score (MOS) for an input speech signal. The DNN outputs parameters of the posterior, mainly the posterior's mean and variance. The proposed method is referred to as deep posterior MOS (DeePMOS). The relevant training data is inherently limited in size (limited number of labeled samples) and noisy due to the subjective nature of human listeners. For robust training of DeePMOS, we use a combination of maximum-likelihood learning, stochastic gradient noise, and a student-teacher learning setup. Using the mean of the posterior as a point estimate, we evaluate standard performance measures of the proposed DeePMOS. The results show comparable performance with existing DNN-based methods that only provide point estimates of the MOS. Then we provide an ablation study showing the importance of various components in DeePMOS",
    "checked": true,
    "id": "8a69d5a2b10234ac6cf64e2a3dafe3b6a7a22e84",
    "semantic_title": "deepmos: deep posterior mean-opinion-score of speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dasare23_interspeech.html": {
    "title": "The Role of Formant and Excitation Source Features in Perceived Naturalness of Low Resource Tribal Language TTS: An Empirical Study",
    "volume": "main",
    "abstract": "Text-to-speech synthesis is a prominent area in the speechprocessing domain that has significant use in reading digital content in a given language. In the proposed work, we worked on two tribal languages of India viz., Lambani and Soliga, which are zero-resource languages. The study began with a dataset collection for both tribal languages. Secondly, a Text-To-Speech (TTS) system was built separately based on the transfer learning approach. To validate the voice quality of TTS-generated speech, subjective as well as objective evaluations were performed. As a part of objective analysis, the voice source and vocal tract filter properties of the synthetic speech have been explored. The extensive study on various aspects of speech, such as LP residual, F0 contour, and formants (F1 & sF2) has shown interesting results that can correlate to the subjective listening test results. The link to the original and synthetic speech can be found online",
    "checked": true,
    "id": "7828e3abbedaecbd890ccf7475783cfa3f397c71",
    "semantic_title": "the role of formant and excitation source features in perceived naturalness of low resource tribal language tts: an empirical study",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23_interspeech.html": {
    "title": "A no-reference speech quality assessment method based on neural network with densely connected convolutional architecture",
    "volume": "main",
    "abstract": "Most speech quality assessment methods require a perfect reference signal to evaluate the damaged speech's quality. However, it is challenging to obtain clean reference signals due to various types and levels of noise in reality. Meanwhile, no-reference speech quality assessment is less accurate than full-reference method. To address these issues, we propose a novel no-reference speech quality assessment model that improves evaluation accuracy with lower complexity. The model is primarily composed of three densely connected convolutional (DCC) modules and a bidirectional long short-term memory (BLSTM) module. Experiment results demonstrate that our method outperforms the baselines, achieving state-of-the-art on the no-reference speech quality assessment task. When using PESQ as optimization targets, the MSE, PLCC and SRCC reach 0.0389, 0.9695 and 0.9715, whereas when using STOI, these metrics reach 0.0019, 0.9608, and 0.9630, respectively",
    "checked": true,
    "id": "8e9a4eb13c54dbadee82df8389a683ef27f9c994",
    "semantic_title": "a no-reference speech quality assessment method based on neural network with densely connected convolutional architecture",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ta23_interspeech.html": {
    "title": "Probing Speech Quality Information in ASR Systems",
    "volume": "main",
    "abstract": "This paper investigates how intermediate speech representations in a state-of-the-art automatic speech recognition (ASR) system encode multi-dimensional speech quality, including MOS, Noisiness, Coloration, Discontinuity, and Loudness. We found that speech quality information is encoded in the ASR encoder layers at various levels but is still much richer than the Mel-spectrogram, an input widely used in previous works. This discovery inspires us to develop the Attentive Conformer with ASR pretraining, a novel deep learning model that enables the utilization of rich information from pretrained ASR models and the ability to focus on specific layers. Experiments on the NISQA speech quality assessment dataset demonstrate that the proposed model achieves state-of-the-art performance with significantly less training data",
    "checked": true,
    "id": "ffa5ed4bd8625dc803fe3de4e0c0f72752ff6df2",
    "semantic_title": "probing speech quality information in asr systems",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23d_interspeech.html": {
    "title": "Preference-based training framework for automatic speech quality assessment using deep neural network",
    "volume": "main",
    "abstract": "One objective of Speech Quality Assessment (SQA) is to estimate the ranks of synthetic speech systems. However, recent SQA models are typically trained using low-precision direct scores such as mean opinion scores (MOS) as the training objective, which is not straightforward to estimate ranking. Although it is effective for predicting quality scores of individual sentences, this approach does not account for speech and system preferences when ranking multiple systems. We propose a training framework of SQA models that can be trained with only preference scores derived from pairs of MOS to improve ranking prediction. Our experiment reveals conditions where our framework works the best in terms of pair generation, aggregation functions to derive system score from utterance preferences, and threshold functions to determine preference from a pair of MOS. Our results demonstrate that our proposed method significantly outperforms the baseline model in Spearman's Rank Correlation Coefficient",
    "checked": true,
    "id": "3c54bd226841a3994ab5239ffdc4b9031081d961",
    "semantic_title": "preference-based training framework for automatic speech quality assessment using deep neural network",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/phatthiyaphaibun23_interspeech.html": {
    "title": "Crowdsourced Data Validation for ASR Training",
    "volume": "main",
    "abstract": "Many ASR engines are based on crowdsourced speech corpora, such as Common Voice. Although crowdsourced data is inexpensive, the utterances obtained from crowdsourcing can be noisy because of uncontrollable factors such as accents, environments, etc. Another issue with the Common Voice corpus is the lack of validators to cover a vast collection of crowdsourced utterances. This issue presents a significant challenge to speech data validation. To mitigate this bottleneck, we propose a machine-learning classifier that predicts the correctness of the data, which can act as either the validator itself or a prescreen for the validator. Our system achieves more than 95% F1-score in the three Common Voice languages, including Thai, Japanese, and Turkish, and performs even better when we have only one human judge involved in the decision. Furthermore, we also found that the data obtained from our method outperformed the current crowdsourcing validation method when used to train the ASR model",
    "checked": true,
    "id": "7dae6964be1b4dd1e254c7f2c2af88945b26c643",
    "semantic_title": "crowdsourced data validation for asr training",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huo23b_interspeech.html": {
    "title": "Re-investigating the Efficient Transfer Learning of Speech Foundation Model using Feature Fusion Methods",
    "volume": "main",
    "abstract": "Speech foundation models, pre-trained on large amounts of unsupervised or supervised audio data, have demonstrated an impressive ability to transfer their learning to specific domains for speech recognition. Parameter-efficient fine-tuning methods offer an efficient paradigm where a small set of parameters are updated to adapt the foundation model to new tasks. However, it is unclear how the intermediate features of the foundation model behave, and how to utilize them in a more efficient way. In this paper, we compare the performance of three speech foundation models for speech recognition. We re-investigate how features from different layers behave and propose a simple and effective feature fusion method for efficient transfer learning. Experimental results demonstrate that the proposed method uses 31.7% fewer trainable encoder parameters, 13.4% less computational memory cost than compared method, and does not compromise quality on the target task",
    "checked": true,
    "id": "d3ccf04f65bfa037227ebf1637e0b1bc9654ff59",
    "semantic_title": "re-investigating the efficient transfer learning of speech foundation model using feature fusion methods",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qi23_interspeech.html": {
    "title": "Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training",
    "volume": "main",
    "abstract": "Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (WAPAT). WAPAT use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, WAPAT utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of WAPAT on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-WAPAT outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art",
    "checked": true,
    "id": "1628c06f7b63c10c0aa78317aa9ca6c8da68198f",
    "semantic_title": "robust automatic speech recognition via wavaugment guided phoneme adversarial training",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23b_interspeech.html": {
    "title": "InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the baseline models",
    "checked": true,
    "id": "a86370b202b4856f32ba9db9ed10cb2ba4aca8e6",
    "semantic_title": "interformer: interactive local and global features fusion for automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23_interspeech.html": {
    "title": "Transductive Feature Space Regularization for Few-shot Bioacoustic Event Detection",
    "volume": "main",
    "abstract": "In few-shot bioacoustic event detection, besides interested target events, background noises and various uninterested sound events lead to complex decision boundaries, which require regularized feature distributions in feature space. Due to the low label availability of uncertain noise events, existing few-shot learning methods with entropy-based regularizers suffer from overfitting during optimization. In this paper, we propose a transductive inference model with a prior knowledge based regularizer (PKR) to overcome the overfitting problem. We use a task-adaptive feature extractor to reconstruct a regularized feature space. A PKR is proposed to minimize the divergence between the original and reconstructed feature space. The development set of DCASE 2022 Task 5 is adopted as the experimental dataset. With the increasing iterations, the proposed model performs with long-lasting results around 55.43 F-measure, and well solves the overfitting problem in transductive inference",
    "checked": true,
    "id": "2a4b8260e63debd06db059f02b14fa36b603e7e7",
    "semantic_title": "transductive feature space regularization for few-shot bioacoustic event detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23e_interspeech.html": {
    "title": "Incorporating L2 Phonemes Using Articulatory Features for Robust Speech Recognition",
    "volume": "main",
    "abstract": "The limited availability of non-native speech datasets presents a major challenge in automatic speech recognition (ASR) to narrow the performance gap between native and non-native speakers. To address this, the focus of this study is on the efficient incorporation of the L2 phonemes, which in this work refer to Korean phonemes, through articulatory feature analysis. This not only enables accurate modeling of pronunciation variants but also allows for the utilization of both native Korean and English speech datasets. We employ the lattice-free maximum mutual information (LF-MMI) objective in an end-to-end manner, to train the acoustic model to align and predict one of multiple pronunciation candidates. Experimental results show that the proposed method improves ASR accuracy for Korean L2 speech by training solely on L1 speech data. Furthermore, fine-tuning on L2 speech improves recognition accuracy for both L1 and L2 speech without performance trade-offs",
    "checked": true,
    "id": "64abb0112d24dc62105e302cf98af445651b86ab",
    "semantic_title": "incorporating l2 phonemes using articulatory features for robust speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/parcollet23_interspeech.html": {
    "title": "On the (In)Efficiency of Acoustic Feature Extractors for Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Speech representations learned with self-supervised learning (SSL) have the potential to significantly improve the performance of a number of audio applications, especially when availability of labeled data from the deployment domain is limited. Despite their successes, SSL training methods are compute- and memory-heavy, and require large investments in computing infrastructure, thus putting it out of the reach of most institutions. Therefore, building efficient model architectures is essential for the wide-scale adoption of SSL in speech technologies. CNN-based Acoustic Feature Extractors (AFE), which are widely used as encoders of acoustic waveforms, remain one of the main efficiency bottlenecks. This work proposes replacing CNN-based AFEs with more efficient ones and demonstrates that SSL pre-training time and memory consumption can be reduced by a factor of two to three over existing methods while preserving performances in speech-, command-, and speaker-recognition tasks",
    "checked": true,
    "id": "67377c8759a5a21ed27a8f6eee585dde1c3bd6a7",
    "semantic_title": "on the (in)efficiency of acoustic feature extractors for self-supervised speech representation learning",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tenbosch23_interspeech.html": {
    "title": "Phonemic competition in end-to-end ASR models",
    "volume": "main",
    "abstract": "Advanced end-to-end ASR systems encode speech signals by means of a multi-layer network architecture. In Wav2vec2.0, for example, a CNN is used as feature encoder on top of which transformer layers are used to map the high-dimensional CNN representations to the elements of some lexicon. Compared to the previous generation of 'modular' ASR systems it is much more difficult to interpret the processing and representations in an end-to-end system from a phonetic point of view. We built a Wav2vec2.0-based end-to-end system for producing broad phonetic transcriptions of Dutch. In this paper we investigate to what extent the CNN features and the representations on several transformer layers of a pre-trained and fine-tuned model reflect widely-shared phonetic knowledge. For that purpose we analyze distances between phones and the phonetic features of the most-activated phones in the output of an MLP classifier operating on the representations in several layers",
    "checked": true,
    "id": "294e96df613afb8a84c37ffabd183ba049cedd99",
    "semantic_title": "phonemic competition in end-to-end asr models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hughes23_interspeech.html": {
    "title": "Automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics",
    "volume": "main",
    "abstract": "Automatic Speaker Recognition (ASR) involves a complex range of processes to extract, model, and compare speaker-specific information from a pair of voice samples. Using heavily controlled recordings, this paper explores the impact of specific vocal conditions (i.e. vocal setting, disguise, accent guises) on ASR performance. When vocal conditions are matched, ASR performance is generally excellent (whisper is an exception). When conditions are mismatched, as in most forensic cases, we see an increase in discrimination and calibration error in some cases. The most problematic mismatches are those involving whisper and supralaryngeal vocal settings; these produce the greatest phonetic changes to speech. Mismatches involving high pitch also produce poor performance, although this appears to be driven by speaker-specific differences in articulatory implementation. We discuss the implications of the findings for the use of ASR in forensic casework and the interpretability of system output",
    "checked": true,
    "id": "199229d3865f23548de42544004502ffee6f843c",
    "semantic_title": "automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geiger23_interspeech.html": {
    "title": "Exploring Graph Theory Methods For the Analysis of Pronunciation Variation in Spontaneous Speech",
    "volume": "main",
    "abstract": "Given the development of automatic speech recognition based techniques for creating phonetic annotations of large speech corpora, there has been a growing interest in investigating the frequencies of occurrence of phonological and reduction processes. Given that most studies have analyzed these processes separately, they did not provide insights about their co-occurrences. This paper contributes with introducing graph theory methods for the analysis of pronunciation variation in a large corpus of Austrian German conversational speech. More specifically, we investigate how reduction processes that are typical for spontaneous German in general co-occur with phonological processes typical for the Austrian German variety. Whereas our concrete findings are of special interest to scientists investigating variation in German, the approach presented opens new possibilities to analyze pronunciation variation in large corpora of different speaking styles in any language",
    "checked": true,
    "id": "338f9ef7acb8d1b25b8eda135ec9e8fb710f8085",
    "semantic_title": "exploring graph theory methods for the analysis of pronunciation variation in spontaneous speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nuttall23_interspeech.html": {
    "title": "Automatic Speaker Recognition performance with matched and mismatched female bilingual speech data",
    "volume": "main",
    "abstract": "Validation of forensic voice comparison methods requires testing using speech samples that are representative of forensic casework conditions. Increasingly, around the world, forensic voice comparison casework is being undertaken using automatic speaker recognition (ASR) systems. However, multilingualism remains a key issue in applying automatic systems to forensic casework. This research aims to consider the effect of language on ASR performance, testing developers' claims of 'language independency'. Specifically, we examine the extent to which language mismatch either between the known and questioned samples, or between the evidential samples and the calibration data, affects overall system performance and the resulting strength of evidence (i.e., likelihood ratios for individual comparisons). Results indicate that mixed language trials produce more errors than single language trials which makes drawing evidential conclusions based on bilingual data challenging",
    "checked": true,
    "id": "1e58d3ff954b2690435fcb91d712ced6875d06a4",
    "semantic_title": "automatic speaker recognition performance with matched and mismatched female bilingual speech data",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23x_interspeech.html": {
    "title": "FACTSpeech: Speaking a Foreign Language Pronunciation Using Only Your Native Characters",
    "volume": "main",
    "abstract": "Recent text-to-speech models have been requested to synthesize natural speech from language-mixed sentences because they are commonly used in real-world applications. However, most models do not consider transliterated words as input. When generating speech from transliterated text, it is not always natural to pronounce transliterated words as they are written, such as in the case of song titles. To address this issue, we introduce FACTSpeech, a system that can synthesize natural speech from transliterated text while allowing users to control the pronunciation between native and literal languages. Specifically, we propose a new language shift embedding to control the pronunciation of input text between native or literal pronunciation. Moreover, we leverage conditional instance normalization to improve pronunciation while preserving the speaker identity. The experimental results show that FACTSpeech generates native speech even from the sentences of transliterated form",
    "checked": true,
    "id": "8b0feedfec68ce2e39a5d9604b3b16fdfd3fcf07",
    "semantic_title": "factspeech: speaking a foreign language pronunciation using only your native characters",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23e_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model",
    "volume": "main",
    "abstract": "Phrase break prediction is a crucial task for improving the prosody naturalness of a text-to-speech (TTS) system. However, most proposed phrase break prediction models are monolingual, trained exclusively on a large amount of labeled data. In this paper, we address this issue for low-resource languages with limited labeled data using cross-lingual transfer. We investigate the effectiveness of zero-shot and few-shot cross-lingual transfer for phrase break prediction using a pre-trained multilingual language model. We use manually collected datasets in four Indo-European languages: one high-resource language and three with limited resources. Our findings demonstrate that cross-lingual transfer learning can be a particularly effective approach, especially in the few-shot setting, for improving performance in low-resource languages. This suggests that cross-lingual transfer can be inexpensive and effective for developing TTS front-end in resource-poor languages",
    "checked": true,
    "id": "07ecfeb52ff0768c3067cac5309d8150701d5906",
    "semantic_title": "cross-lingual transfer learning for phrase break prediction with multilingual language model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23d_interspeech.html": {
    "title": "DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "Although high-fidelity speech can be obtained for intralingual speech synthesis, cross-lingual text-to-speech (CTTS) is still far from satisfactory as it is difficult to accurately retain the speaker timbres (i.e. speaker similarity) and eliminate the accents from their first language (i.e. nativeness). In this paper, we demonstrated that vector-quantized (VQ) acoustic feature contains less speaker information than mel-spectrogram. Based on this finding, we propose a novel dual speaker embedding TTS (DSE-TTS) framework for CTTS with authentic speaking style. Here, one embedding is fed to the acoustic model to learn the linguistic speaking style, while the other one is integrated into the vocoder to mimic the target speaker's timbre. Experiments show that by combining both embeddings, DSE-TTS significantly outperforms the state-of-the-art SANE-TTS in cross-lingual synthesis, especially in terms of nativeness",
    "checked": true,
    "id": "45a04efc208faa3eb97c60b96dd07bbbb853f69c",
    "semantic_title": "dse-tts: dual speaker embedding for cross-lingual text-to-speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/markopoulos23_interspeech.html": {
    "title": "Generating Multilingual Gender-Ambiguous Text-to-Speech Voices",
    "volume": "main",
    "abstract": "The gender of any voice user interface is a key element of its perceived identity. Recently there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices",
    "checked": true,
    "id": "be0afc3192c002797d767930ce271fccab713471",
    "semantic_title": "generating multilingual gender-ambiguous text-to-speech voices",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/badlani23_interspeech.html": {
    "title": "RAD-MMM: Multilingual Multiaccented Multispeaker Text To Speech",
    "volume": "main",
    "abstract": "We create a multilingual speech synthesis system that can generate speech with a native accent in any seen language while retaining the characteristics of an individual's voice. It is expensive to obtain bilingual training data for a speaker and the lack of such data results in strong correlations that entangle speaker, language, and accent, resulting in poor transfer capabilities. To overcome this, we present RADMMM, a speech synthesis model based on RADTTS with explicit control over accent, language, speaker, and fine-grained F0 and energy features. Our proposed model does not rely on bilingual training data. We demonstrate an ability to control synthesized accent for any speaker in an open-source dataset comprising of 7 languages, with one native speaker per language. Human subjective evaluation demonstrates that, when compared to controlled baselines, our model better retains a speaker's voice and target accent, while synthesizing fluent speech in all target languages and accents in our dataset",
    "checked": true,
    "id": "3beba85b45c7ee6e567a8e44fbaa31a6867cd17b",
    "semantic_title": "rad-mmm: multilingual multiaccented multispeaker text to speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/comini23_interspeech.html": {
    "title": "Multilingual context-based pronunciation learning for Text-to-Speech",
    "volume": "main",
    "abstract": "Phonetic information and linguistic knowledge are an essential component of a Text-to-speech (TTS) front-end. Given a language, a lexicon can be collected offline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in order to predict the pronunciation for out-of-vocabulary (OOV) words. Additionally, post-lexical phonology, often defined in the form of rule-based systems, is used to correct pronunciation within or between words. In this work we showcase a multilingual unified front-end system that addresses any pronunciation related task, typically handled by separate modules. We evaluate the proposed model on G2P conversion and other language-specific challenges, such as homograph and polyphones disambiguation, post-lexical rules and implicit diacritization. We find that the multilingual model is competitive across languages and tasks, however, some trade-offs exists when compared to equivalent monolingual solutions",
    "checked": true,
    "id": "0d2776b80f93b96be8f4542ffdeb3a6a31f9b770",
    "semantic_title": "multilingual context-based pronunciation learning for text-to-speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23c_interspeech.html": {
    "title": "Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition",
    "volume": "main",
    "abstract": "There are individual differences in expressive behaviors driven by cultural norms and personality. This between-person variation can result in reduced emotion recognition performance. Therefore, personalization is an important step in improving the generalization and robustness of speech emotion recognition. In this paper, to achieve unsupervised personalized emotion recognition, we first pre-train an encoder with learnable speaker embeddings in a self-supervised manner to learn robust speech representations conditioned on speakers. Second, we propose an unsupervised method to compensate for the label distribution shifts by finding similar speakers and leveraging their label distributions from the training set. Extensive experimental results on the MSP-Podcast corpus indicate that our method consistently outperforms strong personalization baselines and achieves state-of-the-art performance for valence estimation",
    "checked": true,
    "id": "a8e465a05d423f6201b4cd5631f04d00261e41b8",
    "semantic_title": "personalized adaptation with pre-trained speech encoders for continuous emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chou23_interspeech.html": {
    "title": "The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers",
    "volume": "main",
    "abstract": "The uncertainty in modeling emotions makes speech emotion recognition (SER) systems less reliable. An intuitive way to increase trust in SER is to reject predictions with low confidence. This approach assumes that an SER system is well calibrated, where highly confident predictions are often right and low confident predictions are often wrong. Hence, it is desirable to calibrate the confidence of SER classifiers. We evaluate the reliability of SER systems by exploring the relationship between confidence and accuracy, using the expected calibration error (ECE) metric. We develop a multi-label variant of the post-hoc temperature scaling (TS) method to calibrate SER systems, while preserving their accuracy. The best method combines an emotion co-occurrence weight penalty function, a class-balanced objective function, and the proposed multi-label TS calibration method. The experiments show the effectiveness of our developed multi-label calibration method in terms of accuracy and ECE",
    "checked": true,
    "id": "dfb438c044685e25a970c622d5c282c79b4831c1",
    "semantic_title": "the importance of calibration: rethinking confidence and performance of speech multi-label emotion classifiers",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/malik23_interspeech.html": {
    "title": "A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model",
    "volume": "main",
    "abstract": "In this paper, we propose to utilise diffusion models for data augmentation in speech emotion recognition (SER). In particular, we present an effective approach to utilise improved denoising diffusion probabilistic models (IDDPM) to generate synthetic emotional data. We condition the IDDPM with the textual embedding from bidirectional encoder representations from transformers (BERT) to generate high-quality synthetic emotional samples in different speakers' voiceswe uploaded the synthetic samples for reviewers to listen.. We implement a series of experiments and show that better quality synthetic data helps improve SER performance. We compare results with generative adversarial networks (GANs) and show that the proposed model generates better-quality synthetic samples that can considerably improve the performance of SER when augmented with synthetic data",
    "checked": true,
    "id": "21da715d3a9bee5e91206347560a898587303ce0",
    "semantic_title": "a preliminary study on augmenting speech emotion recognition using a diffusion model",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alsenani23_interspeech.html": {
    "title": "Privacy Risks in Speech Emotion Recognition: A Systematic Study on Gender Inference Attack",
    "volume": "main",
    "abstract": "Increasingly more applications now use deep networks to analyse speaker's affective states. An undesirable side effect is that models trained to perform one task (e.g, emotion from speech) can be attacked to infer other, possibly privacy-sensitive attributes (e.g., gender) of the speaker. The amount of information an attacker can infer through such attacks is called leakage, and this article presents the first systematic study of the interplay between gender leakage and the main characteristics of the attacker model (family, architecture and training condition). To this end, we define various attack scenarios, and perform extensive experiments to analyse privacy risks in Speech Emotion Recognition (SER). Results show that SER models can leak a speaker's gender with an accuracy of 51% to 95% (upper bound) depending on the attack condition. Furthermore, our results provide fresh insights on how to limit the effectiveness of possible attacks and, thereby, to ensure privacy preservation",
    "checked": true,
    "id": "9982df071491764bee55ec7ea6c2f896934785f7",
    "semantic_title": "privacy risks in speech emotion recognition: a systematic study on gender inference attack",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tavernor23_interspeech.html": {
    "title": "Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion conveys abundant information that can improve the user experience of various automated systems, in addition to communicating information important for managing well-being. Human speech conveys emotion, but speech emotion recognition models do not perform well in unseen environments. This limits the ubiquitous use of speech emotion recognition models. In this paper, we investigate how a model can be adapted to unseen environments without forgetting previously learned environments. We show that memory-based methods maintain performance on previously seen environments while still being able to adapt to new environments. These methods enable continual training of speech emotion recognition models following deployment while retaining previous knowledge, working towards a more general, adaptable, acoustic model",
    "checked": true,
    "id": "f734a8f825d3c0b2982f343d95d622a4e3eecaa1",
    "semantic_title": "episodic memory for domain-adaptable, robust speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ding23_interspeech.html": {
    "title": "Stable Speech Emotion Recognition with Head-k-Pooling Loss",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) aims to detect the emotion of the speaker involved in a given utterance. Most existing SER methods focus on local speech features by stacking convolutions and training all segments of an utterance with an utterance-level label. Two deficiencies exist in these methods: i) learning only local speech features may be insufficient for SER due to the ambiguity of emotions; ii) consistent supervision of each segment may lead to label error propagation, as the true emotions of some segments may not match the utterance label. To solve the two issues, we first devise a global-local fusion network to model both long- and short-range relations in speech. Second, we tailor a novel head-k-pooling loss for SER tasks, which dynamically assigns labels for each segment and selectively performs loss calculation across segments. We test our method on the IEMOCAP and the newly collected ST-EMO dataset, and the results show its superiority and stability",
    "checked": true,
    "id": "b8f958dcae03dcb7a3f27277a1a10ab784a49e0f",
    "semantic_title": "stable speech emotion recognition with head-k-pooling loss",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gibson23b_interspeech.html": {
    "title": "A Personalised Speech Communication Application for Dysarthric Speakers",
    "volume": "main",
    "abstract": "Individuals with impaired speech are often understood only by those familiar with their speech e.g. a care-giver or close family member. These impaired speakers are therefore highly dependent upon those familiar listeners for their spoken communication needs. These needs vary from basic expressions of hunger or thirst to much more advanced requirements like being understood at a work meeting. A significant subset of individuals with impaired speech also have reduced motor function which limits their mobility or dexterity. For this subset of individuals, the ability to communicate via the medium of speech is crucial. This paper describes a personalised speech communication application targeted towards English language speakers with impaired speech. This application enables the user to hold conversations with other humans, dictate text to a machine and participate in meetings via closed captioning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23l_interspeech.html": {
    "title": "Video Multimodal Emotion Recognition System for Real World Applications",
    "volume": "main",
    "abstract": "This paper proposes a system capable of recognizing a speaker's utterance-level emotion through multimodal cues in a video. The system seamlessly integrates multiple AI models to first extract and pre-process multimodal information from the raw video input. Next, an end-to-end MER model sequentially predicts the speaker's emotions at the utterance level. Additionally, users can interactively demonstrate the system through the implemented interface",
    "checked": true,
    "id": "74d9bea6a26e928b042f009bc277bb3fe04190bb",
    "semantic_title": "video multimodal emotion recognition system for real world applications",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rohmatillah23_interspeech.html": {
    "title": "Promoting Mental Self-Disclosure in a Spoken Dialogue System",
    "volume": "main",
    "abstract": "This paper proposes a mental health spoken dialogue to relax mental illness for university students by acting as an active listener to promote self-disclosure. The proposed system is designed for Mandarin with the specific accent and lexicon in Taiwan which is known as one of the underrepresented spoken languages. To achieve the objective, this work considers three key factors which are high quality speech components including automatic speech recognition and text-to-speech models, and the personalized responses while keeping the trustworthiness and seamless integration among dialogue system components",
    "checked": true,
    "id": "0e44ebb3494e7481b2b9ace4dbf647cb60a902ef",
    "semantic_title": "promoting mental self-disclosure in a spoken dialogue system",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bujnowski23_interspeech.html": {
    "title": "Select language, modality or put on a mask!\" Experiments with Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "We propose a system designed for multimodal emotion recognition. Our research focuses on showing the impact of various signals in the emotion recognition process. Apart from reporting the average results of our models, we would like to encourage individual engagement of conference participants and explore how a unique emotional scene recorded on the spot can be interpreted by the models - for individual modalities as well as their combinations. Our models work for English, German and Korean. We show the comparison of emotion recognition accuracy for these 3 languages, including the influence of each modality. Our second experiment explores emotion recognition for people wearing face masks. We show that the use of face masks affects not only the video signal but also audio and text. To our knowledge, no other study shows the effects of wearing a mask for three modalities. Unlike other studies where masks are added artificially, we use real recordings with actors in masks",
    "checked": true,
    "id": "7d21b4ce8bb4c5f7f4343e13b6dd7645875995d2",
    "semantic_title": "select language, modality or put on a mask!\" experiments with multimodal emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/valentine23_interspeech.html": {
    "title": "My Vowels Matter: Formant Automation Tools for Diverse Child Speech",
    "volume": "main",
    "abstract": "Tools to automate formant measurement in vowels have been developed recently, but they have not been tested on pediatric speech samples. Critically, child speech includes unique acoustic challenges including high fundamental frequencies, wide formant bandwidths, more variable formant values, and increased subglottal coupling relative to adult speech. More importantly, these tools have not been tested on the diverse linguistic variations spoken by children. This study compares three tools for automatic formant estimation: Voweltine, Fast Track, and SpeechMark. The tools are tested on vowel productions from a young child with a speech sound disorder from a Black-identifying family. Benefits and tradeoffs of each automation tool are discussed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chongwhite23_interspeech.html": {
    "title": "NEMA: An Ecologically Valid Tool for Assessing Hearing Devices, Advanced Algorithms, and Communication in Diverse Listening Environments",
    "volume": "main",
    "abstract": "Ecological Momentary Assessment (EMA) is valuable research method for evaluating the real-world performance of novel computational algorithms and device technologies, addressing the shortcomings of objective metrics and laboratory assessments. Our customisable, cloud-connected smartphone app, NEMA, gathers repeated self-reports and related acoustic features in users' natural environments, providing personalised insights on how specific technologies impact daily activities. NEMA has proven effective in assessing the real-world performance of novel hearing aid algorithms and features, while also improving our understanding of the challenges faced by those with hearing loss which drive new developments. This paper outlines NEMA's innovative features designed to facilitate efficient data collection and presents findings from a recent clinical trial where NEMA played a key role in providing real-world evidence of user benefits for a medical device seeking FDA approval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ramanarayanan23_interspeech.html": {
    "title": "When Words Speak Just as Loudly as Actions: Virtual Agent Based Remote Health Assessment Integrating What Patients Say with What They Do",
    "volume": "main",
    "abstract": "We present a unified multimodal dialog platform for the remote assessment and monitoring of patients' neurological and mental health. Tina, a virtual agent, guides participants through an immersive interaction wherein objective speech, facial, linguistic and cognitive biomarkers can be automatically computed from participant speech and video in near real time. Furthermore, Tina encourages participants to describe, in their own words, their most bothersome problems and what makes them better or worse, through the Patient Report of Problems (PROP) instrument. The PROP captures unfiltered verbatim replies of patients, in contrast with traditional patient reported outcomes that typically rely on categorical assessments. We argue that combining these patient reports (i.e., what they say) with objective biomarkers (i.e., how they say it and what they do) can greatly enhance the quality of telemedicine and improve the efficacy of siteless trials and digital therapeutic interventions",
    "checked": false,
    "id": "ad93f5b0696a1da863ed3768243fb587fa508076",
    "semantic_title": "when words speak just as loudly as actions : virtual agent based remote health assessment integrating what patients say with what they do",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/motepalli23_interspeech.html": {
    "title": "Stuttering Detection Application",
    "volume": "main",
    "abstract": "Stuttering is a prevalent speech disorder that affects millions of people worldwide. In this Show and Tell presentation, we demonstrate a novel platform that takes speech samples in English and Kannada to detect and analyze stuttering in patients. The user-friendly interface includes demographic details and speech samples, generating comprehensive reports for different stuttering disfluencies. The platform has four different user types, providing full read-only access for admins and full write access for super admins. Our platform provides valuable assistance for speech-language pathologists to evaluate speech samples. The proposed platform supports both live and recorded speech samples and presents a flexible approach to stuttering detection and analysis. Our research demonstrates the potential of technology to improve speech-language pathology for stuttering. Used F-score as a metric for evaluating the models for the stutter detection task",
    "checked": false,
    "id": "7da56460260e74076d524527eb89a981747864e7",
    "semantic_title": "drowsiness detection application",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zusag23b_interspeech.html": {
    "title": "Providing Interpretable Insights for Neurological Speech and Cognitive Disorders from Interactive Serious Games",
    "volume": "main",
    "abstract": "We propose an automated pipeline for robustly identifying neurological disorders from interactive therapeutic exercises, which are gathered via the mobile therapy app myReha. The app captures speech and cognitive parameters from over 30.000 tasks in various scenarios. Users get immediate and highly accurate feedback for pronunciation and coherency for language tasks, while voice recordings are fed to a feature extraction pipeline in the backend. These features are then used to construct speech characteristics, which are highly indicative of different neurological disorders, such as acquired aphasia after stroke. The data is visually presented in a web application nyra.insights, which allows medical professionals to quickly derive recommendations for treatment and closely monitor outcomes. During the Show and Tell session, users can experiment with the interactive myReha app and experience the real-time speech analysis capabilities via the nyra.insights web platform",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/solinsky23_interspeech.html": {
    "title": "Automated Neural Nursing Assistant (ANNA): An Over-The-Phone System for Cognitive Monitoring",
    "volume": "main",
    "abstract": "ANNA is a telephony-based cognitive assessment tool designed to aid nurses in caring for patients who require close monitoring for the development of confusion or neurological impairment. Of particular concern is the treatment of Immune Effector Cell-Associated Neurotoxicity Syndrome (ICANS), a condition which occurs quite frequently as an adverse outcome of Chimeric Antigen Receptor-T (CAR-T) cancer immunotherapy. ANNA employs both traditional verbal tests for cognitive impairment and novel linguistic methods which identify abnormalities in the patient's speech during ordinary conversation. To collect ordinary speech it uses a lightweight instance of the Facebook's Large Language Model BlenderBot to engage the patient in a partially unscripted conversation. ANNA is designed with easy employment by healthcare providers in mind, being sufficiently lightweight to run on consumer-grade hardware and needing access only to a patient's phone number to interact with them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gupta23b_interspeech.html": {
    "title": "5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids",
    "volume": "main",
    "abstract": "Over twenty percent of the world's population suffers from some form of hearing loss, making it one of the most significant public health challenges. Current hearing aids commonly amplify noises while failing to improve speech comprehension in crowded social settings. In this demonstration, we showcase a proof-of-concept implementation of the world's first 5G and Internet of Things (IoT) enabled multi-modal hearing aid (MM HA) prototype. This integrates an innovative 5G cloud-radio access network (C-RAN) and IoT based transceiver model for real-time audio-visual speech enhancement (AVSE). Specifically, we demonstrate a transceiver model for Cloud-based AVSE which satisfies high data rate and low latency requirements for future MM HAs. The innovative 5G-IoT transceiver application is shown to satisfy HA latency limitations while transmitting raw noisy AV data from an MM HA prototype device to the cloud for deep learning-based real-time AVSE processing and obtaining a clean audio signal",
    "checked": false,
    "id": "672c2b5508b040435a90193b8a2e127b42bf6bea",
    "semantic_title": "live demonstration: cloud-based audio-visual speech enhancement in multimodal hearing-aids",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raza23_interspeech.html": {
    "title": "Towards Two-point Neuron-inspired Energy-efficient Multimodal Open Master Hearing Aid",
    "volume": "main",
    "abstract": "Here we demonstrate a two-point neuron-inspired audio-visual (AV) open Master Hearing Aid (OpenMHA) framework for on-chip energy-efficient speech enhancement (SE). The developed system is compared against state-of-the-art cepstrum-based audio-only (A-only) SE and conventional point-neuron inspired deep neural net (DNN) driven multimodal (MM) SE. Pilot experiments demonstrate that the proposed system outperforms audio-only SE in terms of speech quality and intelligibility and performs comparably to point neuron-inspired DNN with significantly reduced energy consumption at any time --- both during training and inferencing",
    "checked": false,
    "id": "e997b77fad7a7418c45dbd18742904cca5a212bb",
    "semantic_title": "towards two-point neuron-inspired energy-efficient multi-modal open master hearing aid",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23b_interspeech.html": {
    "title": "FC-MTLF: A Fine- and Coarse-grained Multi-Task Learning Framework for Cross-Lingual Spoken Language Understanding",
    "volume": "main",
    "abstract": "Currently, zero-shot cross-lingual spoken language understanding (SLU) attracts increasing attention. Most of existing methods construct a mixed-language context via the code-switching approach. However, due to the different syntactic structures of each language, code-switching might fail to perform well and result in the loss of semantics. To address this issue, we propose a novel framework termed FC-MTLF, which applies a multi-task learning by introducing an auxiliary multilingual neural machine translation (NMT) task to compensate for the shortcomings of code-switching. In addition, we also adopt the curriculum learning strategy to further improve the performance. Experimental results show that our framework achieves the new state-of-the-art performance on the MultiATIS++ dataset. Further analysis verifies that our FC-MTLF can effectively transfer knowledge from source languages to target languages",
    "checked": true,
    "id": "118a703723c066895be4477f8d7bfb62e370ac2f",
    "semantic_title": "fc-mtlf: a fine- and coarse-grained multi-task learning framework for cross-lingual spoken language understanding",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23c_interspeech.html": {
    "title": "C²A-SLU: Cross and Contrastive Attention for Improving ASR Robustness in Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) is a critical task in task-oriented dialogue systems. However, automatic speech recognition (ASR) errors often impair the understanding performance. Despite many previous models have obtained promising results for improving ASR robustness in SLU, most of them treat clean manual transcripts and ASR transcripts equally during the fine-tuning stage. To tackle this issue, in this paper, we propose a novel method termed C²A-SLU. Specifically speaking, we add calculated cross attention to the original hidden states and apply contrastive attention to compare the input transcript with clean manual transcripts to distill the contrastive information, which can better capture distinctive features of ASR transcripts. Experiments on three datasets show that C²A-SLU surpasses existing models and achieves a new state-of-the-art performance, with a relative improvement of 3.4% in terms of accuracy over the previous best model on SLURP dataset",
    "checked": true,
    "id": "55b95df9297c2eb3dbf32b2cbc65c402ad67a856",
    "semantic_title": "c²a-slu: cross and contrastive attention for improving asr robustness in spoken language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/weld23_interspeech.html": {
    "title": "Tri-level Joint Natural Language Understanding for Multi-turn Conversational Datasets",
    "volume": "main",
    "abstract": "Natural language understanding typically maps single utterances to a dual level semantic frame, sentence level intent and slot labels at the word level. The best performing models force explicit interaction between intent detection and slot filling. We present a novel tri-level joint natural language understanding approach, adding domain, and explicitly exchange semantic information between all levels. This approach enables the use of multi-turn datasets which are a more natural conversational environment than single utterance. We evaluate our model on two multi-turn datasets for which we are the first to conduct joint slot-filling and intent detection. Our model outperforms state-of-the-art joint models in slot filling and intent detection on multi-turn data sets. We provide an analysis of explicit interaction locations between the layers. We conclude that including domain information improves model performance",
    "checked": true,
    "id": "bbebba31b33f265c2e9b475080051b1a8246f089",
    "semantic_title": "tri-level joint natural language understanding for multi-turn conversational datasets",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/laperriere23_interspeech.html": {
    "title": "Semantic Enrichment Towards Efficient Speech Representations",
    "volume": "main",
    "abstract": "Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR",
    "checked": true,
    "id": "6b0aff42f33d169a0ac5668e23f5f99f695da170",
    "semantic_title": "semantic enrichment towards efficient speech representations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kashiwagi23b_interspeech.html": {
    "title": "Tensor decomposition for minimization of E2E SLU model toward on-device processing",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters",
    "checked": true,
    "id": "0534fd0ed04acaa60f820b730bf3c4816767fa43",
    "semantic_title": "tensor decomposition for minimization of e2e slu model toward on-device processing",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mao23_interspeech.html": {
    "title": "DiffSLU: Knowledge Distillation Based Diffusion Model for Cross-Lingual Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) has achieved great success in high-resource languages, but it still remains challenging in the low-resource languages due to the scarcity of labeled training data. Hence, there is an increasing interest in zero-shot cross-lingual SLU. SLU typically has two subtasks, including intent detection and slot filling. Slots and intent in the same utterance are correlated, thus it is beneficial to achieve mutual guidance between them. In this paper, we propose a novel cross-lingual SLU framework termed DiffSLU, which leverages powerful diffusion model to enhance the mutual guidance. In addition, we also utilize knowledge distillation to facilitate knowledge transfer. Experimental results demonstrate that our DiffSLU can improve the performance compared with the strong baselines and achieves the new state-of-the-art performance on MultiATIS++ dataset, obtaining a relative improvement of 3.1% over the previous best model in overall accuracy",
    "checked": true,
    "id": "9e46a2f7d0193fee8b3d702bfeb9692ed6660e14",
    "semantic_title": "diffslu: knowledge distillation based diffusion model for cross-lingual spoken language understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arora23_interspeech.html": {
    "title": "Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding",
    "volume": "main",
    "abstract": "There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances",
    "checked": true,
    "id": "0c7018db4a00df1792a7b3de3cb0b48aa19ca041",
    "semantic_title": "integrating pretrained asr and lm to perform sequence generation for spoken language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23e_interspeech.html": {
    "title": "Contrastive Learning Based ASR Robust Knowledge Selection For Spoken Dialogue System",
    "volume": "main",
    "abstract": "The construction of knowledge-based, task-oriented systems for spoken conversations is a challenging task. Given the spoken dialogue history information, a knowledge selection model selects the appropriate knowledge snippet from an unstructured knowledge base. However, the performance of this model is sensitive to automatic speech recognizer (ASR) recognition errors. To address this problem, we propose a method called CLKS, which develops a knowledge selection model that is robust to ASR recognition errors. This approach involves: 1) To leverage a wide range of information from various ASR outputs, we employ the self-attention mechanism to aggregate the representation of the N-best hypotheses of the dialogue history. 2) We use the written dialogue representation to guide the aggregated spoken dialogue representation to select the correct knowledge candidate through contrastive learning. Experimental results on the DSTC10 dataset demonstrate the effectiveness of our method",
    "checked": true,
    "id": "519f73c93cf31cce7d760f146235e8f0fd2c179c",
    "semantic_title": "contrastive learning based asr robust knowledge selection for spoken dialogue system",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23f_interspeech.html": {
    "title": "Unsupervised Dialogue Topic Segmentation in Hyperdimensional Space",
    "volume": "main",
    "abstract": "We present HyperSeg, a hyperdimensional computing (HDC) approach to unsupervised dialogue topic segmentation. HDC is a class of vector symbolic architectures that leverages the probabilistic orthogonality of randomly drawn vectors at extremely high dimensions (typically over 10,000). HDC generates rich token representations through its low-cost initialization of many unrelated vectors. This is especially beneficial in topic segmentation, which often operates as a resource-constrained pre-processing step for downstream transcript understanding tasks. HyperSeg outperforms the current state-of-the-art in 4 out of 5 segmentation benchmarks -- even when baselines are given partial access to the ground truth -- and is 10 times faster on average. We show that HyperSeg also improves downstream summarization accuracy. With HyperSeg, we demonstrate the viability of HDC in a major language task. We open-source HyperSeg to provide a strong baseline for unsupervised topic segmentation",
    "checked": true,
    "id": "073c91fc6082111154d4525b48eca8e54477f71f",
    "semantic_title": "unsupervised dialogue topic segmentation in hyperdimensional space",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cappellazzo23_interspeech.html": {
    "title": "An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Continual learning refers to a dynamical framework in which a model receives a stream of non-stationary data over time and must adapt to new data while preserving previously acquired knowledge. Unluckily, neural networks fail to meet these two desiderata, incurring the so-called catastrophic forgetting phenomenon. Whereas a vast array of strategies have been proposed to attenuate forgetting in the computer vision domain, for speech-related tasks, on the other hand, there is a dearth of works. In this paper, we consider the joint use of rehearsal and knowledge distillation (KD) approaches for spoken language understanding under a class-incremental learning scenario. We report on multiple KD combinations at different levels in the network, showing that combining feature-level and predictions-level KDs leads to the best results. Finally, we provide an ablation study on the effect of the size of the rehearsal memory that corroborates the efficacy of our approach for low-resource devices",
    "checked": true,
    "id": "2802090f23e01cc1341a767daf825c6256341a0f",
    "semantic_title": "an investigation of the combination of rehearsal and knowledge distillation in continual learning for spoken language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23h_interspeech.html": {
    "title": "Enhancing New Intent Discovery via Robust Neighbor-based Contrastive Learning",
    "volume": "main",
    "abstract": "New intent discovery (NID) has become a hot topic for dialogue system, which aims to discover the Out-Of-Domain intents from conversation corpus and classify these utterances correctly. Existing methods usually focus on learning compact representations of utterances, and leverage the clustering algorithm to generate new intents. Inspired by the recent progress of contrastive learning, in this work, we propose a novel neighbor-based contrastive learning (NCL) to obtain discriminative representations for utterances. Specifically, to enhance the robustness of NCL, on the one hand, we pick out diverse samples as positive pairs by considering both the anchor neighborhood and nearby neighborhood. On the other hand, we also devise a boundary distance constraint to avoid introducing noisy samples when extending the positives via neighbors. Extensive experiments are conducted on three public NID datasets and the results demonstrate the competitiveness and effectiveness of our proposed approach",
    "checked": true,
    "id": "b789782866e0ba2078353dc5a3331a589263b9f5",
    "semantic_title": "enhancing new intent discovery via robust neighbor-based contrastive learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schwarz23_interspeech.html": {
    "title": "Personalized Predictive ASR for Latency Reduction in Voice Assistants",
    "volume": "main",
    "abstract": "Streaming Automatic Speech Recognition (ASR) in voice assistants can utilize prefetching to partially hide the latency of response generation. Prefetching involves passing a preliminary ASR hypothesis to downstream systems in order to prefetch and cache a response. If the final ASR hypothesis after endpoint detection matches the preliminary one, the cached response can be delivered to the user, thus saving latency. In this paper, we extend this idea by introducing predictive automatic speech recognition, where we predict the full utterance from a partially observed utterance, and prefetch the response based on the predicted utterance. We introduce two personalization approaches and investigate the tradeoff between potential latency gains from successful predictions and the cost increase from failed predictions. We evaluate our methods on an internal voice assistant dataset as well as the public SLURP dataset",
    "checked": true,
    "id": "0fed61ee16a5fe55a74a7a1791f869c1ac2b67c8",
    "semantic_title": "personalized predictive asr for latency reduction in voice assistants",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ray23_interspeech.html": {
    "title": "Compositional Generalization in Spoken Language Understanding",
    "volume": "main",
    "abstract": "State-of-the-art spoken language understanding (SLU) models have shown tremendous success in benchmark SLU datasets, yet they still fail in many practical scenario due to the lack of model compositionality when trained on limited training data. In this paper, we study two types of compositionality: novel slot combination, and length generalization. We first conduct in-depth analysis, and find that state-of-the-art SLU models often learn spurious slot correlations during training, which leads to poor performance in both compositional cases. To mitigate these limitations, we create the first compositional splits of benchmark SLU datasets and we propose the first compositional SLU model, including compositional loss and paired training that tackle each compositional case respectively. On both benchmark and compositional splits in ATIS and SNIPS, we show that our compositional SLU model significantly outperforms (up to 5% F1 score) state-of-the-art BERT SLU model",
    "checked": true,
    "id": "0543cd2268bb70b7557a620019022f37350b92b7",
    "semantic_title": "compositional generalization in spoken language understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ha_interspeech.html": {
    "title": "Sampling bias in NLU models: Impact and Mitigation",
    "volume": "main",
    "abstract": "Natural Language Understanding (NLU) systems such as chatbots or virtual assistants have seen a significant rise in popularity in recent times, thanks to availability of large volumes of user data. However, typical user data collected for training such models may suffer from sampling biases due to a variety of factors. In this paper, we study the impact of bias in the training data for intent classification task, a core component of NLU systems. We experiment with three kinds of data bias settings: (i) random down-sampling, (ii) class-dependent bias, and (iii) class-independent bias injection. For each setting, we report the loss in model performance and survey strategies to mitigate the loss from two families of methods: (i) semi-supervised learning (SSL), and (ii) synthetic data generation. Overall, we find that while both methods perform well with random down-sampling, synthetic data generation out-performs SSL when only biased training data is available",
    "checked": true,
    "id": "fd60853c71938007a8564d53b25f4a15e746a4d5",
    "semantic_title": "sampling bias in nlu models: impact and mitigation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23d_interspeech.html": {
    "title": "5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair",
    "volume": "main",
    "abstract": "Providing voice assistants the ability to navigate multi-turn conversations is a challenging problem. Handling multi-turn interactions requires the system to understand various conversational use-cases, such as steering, intent carryover, disfluencies, entity carryover, and repair. The complexity of this problem is compounded by the fact that these use-cases mix with each other, often appearing simultaneously in natural language. This work proposes a non-autoregressive query rewriting architecture that can handle not only the five aforementioned tasks, but also complex compositions of these use-cases. We show that our proposed model has competitive single task performance compared to the baseline approach, and even outperforms a fine-tuned T5 model in use-case compositions, despite being 15 times smaller in parameters and 25 times faster in latency",
    "checked": true,
    "id": "f7b21c1a591129d926d8ddb6b1babe2b0e140b50",
    "semantic_title": "5ider: unified query rewriting for steering, intent carryover, disfluencies, entity carryover and repair",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23e_interspeech.html": {
    "title": "Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation",
    "volume": "main",
    "abstract": "The aim of emotion prediction in conversation (EPC) is to predict the future emotional state of a speaker based on context information, which is essential for conducting a friendly human-computer conversation. Most EPC works only investigated context information by merging a speaker's multiple utterances into a single utterance per turn and focused on conversations in a dual-speaker scenario, which ignored the information in multi-utterance turn and a more complex and natural scenario of multi-speaker conversations. This paper introduces a context information modeling approach that considers potential emotional interactive information within a speaker's multi-utterance turn, which dominates his/her future emotions. Moreover, our approach advances emotion prediction in both dual- and multi-speaker conversations. Experimental results show that such an approach significantly enhances context information modeling and renders a higher accuracy in EPC than reported in the literature",
    "checked": true,
    "id": "e45ee1978b572b07ffb578c0dcd855510015d56c",
    "semantic_title": "emotion awareness in multi-utterance turn for improving emotion prediction in multi-speaker conversation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ga_interspeech.html": {
    "title": "WhiSLU: End-to-End Spoken Language Understanding with Whisper",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) systems commonly use cascading structures. However, these systems are prone to error propagation, information loss, high costs, and latency, leading researchers to explore end-to-end (E2E) SLU as a hot topic. However, E2E SLU faces the challenge of insufficient data, resulting in most previous work relying on pretrained acoustic models. Nevertheless, pre-training task and SLU task solution spaces are often substantially different, making it difficult for E2E SLU models to surpass cascading models. To address this, we propose using OpenAI's Whisper model for SLU tasks. We employ the Sequence-level Multitask Learning (SML) paradigm, which encodes multiple ASR-related tasks into a sequence for learning. Our method significantly outperforms the E2E baseline by a large margin (with a 10% improvement in EM score) and even outperforms cascading models, achieving a 77% EM score on the STOP dataset, demonstrating its effectiveness",
    "checked": true,
    "id": "005d3c8d253c940f5b709258df5975dde7f17259",
    "semantic_title": "whislu: end-to-end spoken language understanding with whisper",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wen23b_interspeech.html": {
    "title": "Biophysically-inspired single-channel speech enhancement in the time domain",
    "volume": "main",
    "abstract": "Most state-of-the-art speech enhancement (SE) methods utilize time-frequency (T-F) features or waveforms as input features and have poor generalizability at negative signal-to-noise ratios (SNR). To overcome these issues, we propose a novel network that integrates biophysical properties of the human auditory system known to perform even at negative SNRs. We generated biophysical features using CoNNear, a neural network auditory model, which were fed into a SOTA speech enhancement model AECNN. The model was trained on the INTERSPEECH 2021 DNS Challenge dataset and evaluated on mismatched noise conditions at various SNRs. The experimental results revealed that the bio-inspired approaches outperformed T-F and waveform features under positive SNRs and demonstrated stronger robustness to unseen noise at negative SNRs. We conclude that incorporating human-like features can extend the operating range of SE systems to more negative SNRs",
    "checked": true,
    "id": "b1a4342950e0081fb86220105d660e36eb59a6fa",
    "semantic_title": "biophysically-inspired single-channel speech enhancement in the time domain",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jalal23_interspeech.html": {
    "title": "On-Device Speaker Anonymization of Acoustic Embeddings for ASR based on Flexible Location Gradient Reversal Layer",
    "volume": "main",
    "abstract": "Smart devices serviced by large-scale AI models necessitates user data transfer to the cloud for inference. For speech applications, this means transferring private user information, e.g., speaker identity. Our paper proposes a privacy-enhancing framework that targets speaker identity anonymization while preserving speech recognition accuracy for our downstream task Automatic Speech Recognition (ASR). The proposed framework attaches flexible gradient reversal based speaker adversarial layers to target layers within an ASR model, where speaker adversarial training anonymizes acoustic embeddings generated by the targeted layers to remove speaker identityy. We propose on-device deployment by execution of initial layers of the ASR model, and transmitting anonymized embeddings to the cloud, where the rest of the model is executed while preserving privacy. The results show that our method efficiently reduces speaker recognition relative accuracy by 33%, and improves ASR performance by achieving 6.2% relative Word Error Rate (WER) reduction",
    "checked": false,
    "id": "b26478ebf3744bcfc99ad2c9d6dad1e240127b80",
    "semantic_title": "on-device speaker anonymization of acoustic embeddings for asr based onflexible location gradient reversal layer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23b_interspeech.html": {
    "title": "How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning",
    "volume": "main",
    "abstract": "Shortcut learning, or 'Clever Hans effect' refers to situations where a learning agent (e.g., deep neural networks) learns spurious correlations present in data, resulting in biased models. We focus on finding shortcuts in deep learning based spoofing countermeasures (CMs) that predict whether a given utterance is spoofed or not. While prior work has addressed specific data artifacts, such as silence, no general normative framework has been explored for analyzing shortcut learning in CMs. In this study, we propose a generic approach to identifying shortcuts by introducing systematic interventions on the training and test sides, including the boundary cases of 'near-perfect' and 'worse than coin flip' (label flip). By using three different models, ranging from classic to state-of-the-art, we demonstrate the presence of shortcut learning in five simulated conditions. We also analyze the results using a regression model to understand how biases affect the class-conditional score statistics",
    "checked": true,
    "id": "d499af874ef3bd4c7de351b42d479efad8c79a7c",
    "semantic_title": "how to construct perfect and worse-than-coin-flip spoofing countermeasures: a word of warning on shortcut learning",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23c_interspeech.html": {
    "title": "CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram",
    "volume": "main",
    "abstract": "In this work, we present CleanUNet 2, a speech denoising model that combines the advantages of waveform denoiser and spectrogram denoiser and achieves the best of both worlds. CleanUNet 2 uses a two-stage framework inspired by popular speech synthesis methods that consist of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further boosts its performance by taking predicted spectrograms from a spectrogram denoiser as the input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations",
    "checked": true,
    "id": "8926261bf0181a30d92efb0990e924ce2cb5f552",
    "semantic_title": "cleanunet 2: a hybrid speech denoising model on waveform and spectrogram",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23e_interspeech.html": {
    "title": "A Two-stage Progressive Neural Network for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Recent studies in deep learning based acoustic echo cancellation proves the benefits of introducing a linear echo cancellation module. However, the convergence problem and potential target speech distortion impose an additional learning burden for the neural network. In this paper, we propose a two-stage progressive neural network consisting of a coarse-stage and a fine-stage module. For the coarse-stage, a light-weighted network module is designed to suppress partial echo and potential noise, where a voice activity detection path is used to enhance the learned features. For the fine-stage, a larger network is employed to deal with the more complex echo path and restore the near-end speech. We have conducted extensive experiments to verify the proposed method, and the results show that the proposed two-stage method provides a superior performance to other state-of-the-art methods",
    "checked": true,
    "id": "ff45fad1ca0be6c76264aa11a4c4e11eed8c718b",
    "semantic_title": "a two-stage progressive neural network for acoustic echo cancellation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23_interspeech.html": {
    "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
    "volume": "main",
    "abstract": "Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, underutilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beamsearch Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps",
    "checked": true,
    "id": "223569e02810238817ea7fbb12386410a04bcb7f",
    "semantic_title": "an intra-brnn and gb-rvq based end-to-end neural audio codec",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23r_interspeech.html": {
    "title": "Real-Time Personalised Speech Enhancement Transformers with Dynamic Cross-attended Speaker Representations",
    "volume": "main",
    "abstract": "Personalised speech enhancement (PSE) extracts only the speech of a target user and removes everything else from corrupted input audio. This can greatly improve on-device streaming audio processing, such as voice calls and speech recognition, which has strict requirements on model size and latency. To focus the PSE system on the target speaker, it is conditioned on a recording of the user's voice. This recording is usually summarised as a single static vector. However, a static vector cannot reflect all the target user's voice characteristics. Thus, we propose using the full recording. To condition on such a variable-length sequence, we propose fully Transformer-based PSE models with a cross-attention mechanism which generates target speaker representations dynamically. To better reflect the on-device scenario, we carefully design and publish a new PSE dataset. On the dataset, our proposed model significantly surpasses strong baselines while halving the model size and reducing latency",
    "checked": true,
    "id": "f0e33ad6855ba4f977b73f897484d8d46faee9c1",
    "semantic_title": "real-time personalised speech enhancement transformers with dynamic cross-attended speaker representations",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mamun23_interspeech.html": {
    "title": "CFTNet: Complex-valued Frequency Transformation Network for Speech Enhancement",
    "volume": "main",
    "abstract": "It is widely known that the presence of multi-speaker babble noise greatly degrades speech intelligibility. However, suppressing noise without creating artifacts in human speech is challenging in environments with a low signal-to-noise ratio (SNR), and even more so if noise is speechlike such as babble noise. Deep learning-based systems either enhance the magnitude response and reuse distorted phases or enhance the complex spectrogram. Frequency transformation block (FTB) has emerged as a useful architecture to implicitly capture harmonic correlation which is especially important for people with hearing loss (hearing aid/ cochlear implant users). This study proposes a complex-valued frequency transformation network (CFTNet) for speech enhancement, which leverages both a complex-valued U-Net and FTB to capture sufficient low-level contextual information. The proposed system learns a complex transformation matrix to accurately recover speech in the time-frequency domain from a noisy spectrogram. Experimental results demonstrate that the proposed system can achieve significant improvements in both seen and unseen noise over state-of-art networks. Furthermore, the proposed CFTNet can suppress highly nonstationary noise without creating musical artifacts commonly observed in conventional enhancement methods",
    "checked": true,
    "id": "202b65cb3c783aea57daecbd350b643bbc9acca9",
    "semantic_title": "cftnet: complex-valued frequency transformation network for speech enhancement",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23h_interspeech.html": {
    "title": "Feature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement",
    "volume": "main",
    "abstract": "Large, pre-trained representation models trained using self-supervised learning have gained popularity in various fields of machine learning because they are able to extract high-quality salient features from input data. As such, they have been frequently used as base networks for various pattern classification tasks such as speech recognition. However, not much research has been conducted on applying these types of models to the field of speech signal generation. In this paper, we investigate the feasibility of using pre-trained speech representation models for a downstream speech enhancement task. To alleviate mismatches between the input features of the pre-trained model and the target enhancement model, we adopt a novel feature normalization technique to smoothly link these modules together. Our proposed method enables significant improvements in speech quality compared to baselines when combined with various types of pre-trained speech models",
    "checked": true,
    "id": "a323e0e959f8d8d3df94b2f05c2be3e9d1cfae68",
    "semantic_title": "feature normalization for fine-tuning self-supervised models in speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23c_interspeech.html": {
    "title": "Multi-mode Neural Speech Coding Based on Deep Generative Networks",
    "volume": "main",
    "abstract": "The wideband or super wideband speech is one of the most prominent features in real-time communication services, with higher resolution spectrum. However, it requires higher computing expenses. In this paper, we introduce the Penguins codec, based on a multi-mode neural speech coding structure that combines sub-band speech processing and applies different strategies from the low band to the high band. Especially, it refers to deep generative networks with perceptual constraint loss functions and knowledge distillations to reconstruct wideband components and bandwidth extension to generate artificial super wideband components. The method results in high-quality speech at very low bitrates. Several subjective and objective experiments, including ablation studies, were organized, and the results proved the merit of the proposed scheme when compared with traditional coding schemes and state-of-the-art neural coding methods",
    "checked": true,
    "id": "dbc492a0c1f2981be9d7a447730e52c72a5444a5",
    "semantic_title": "multi-mode neural speech coding based on deep generative networks",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bae23_interspeech.html": {
    "title": "Streaming Dual-Path Transformer for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement employing a dual-path transformer (DPT) with a dilated DenseNet-based encoder and decoder has shown state-of-the-art performance. By applying attention in both time and frequency paths, the DPT learns the long-term dependency of speech and the relationship between frequency components. However, the batch processing of the DPT, which performs attention on all past and future frames, makes it impractical for real-time applications. To satisfy the real-time requirement, we propose a streaming dual-path transformer (stDPT) with zero look-ahead structure. In the training phase, we apply masking techniques to control the context length, and in the inference phase, caching methods are utilized to preserve sequential information. Extensive experiments have been conducted to show the performance based on different context lengths, and the results verify that the proposed method outperforms the current state-of-the-art speech enhancement models based on real-time processing",
    "checked": true,
    "id": "4e2f2070a4ac85aec9ba32c6864868b3d4632560",
    "semantic_title": "streaming dual-path transformer for speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kadkhodaeielyaderani23_interspeech.html": {
    "title": "Sequence-to-Sequence Multi-Modal Speech In-Painting",
    "volume": "main",
    "abstract": "Speech in-painting is the task of regenerating missing audio contents using reliable context information. Despite various recent studies in multi-modal perception of audio in-painting, there is still a need for an effective infusion of visual and auditory information in speech in-painting. In this paper, we introduce a novel sequence-to-sequence model that leverages the visual information to in-paint audio signals via an encoder-decoder architecture. The encoder plays the role of a lip-reader for facial recordings and the decoder takes both encoder outputs as well as the distorted audio spectrograms to restore the original speech. Our model outperforms an audio-only speech in-painting model and has comparable results with a recent multi-modal speech in-painter in terms of speech quality and intelligibility metrics for distortions of 300 ms to 1500 ms duration, which proves the effectiveness of the introduced multi-modality in speech in-painting",
    "checked": true,
    "id": "0ef005eaa93b572ff419bfe37f55b9d4358c9f63",
    "semantic_title": "sequence-to-sequence multi-modal speech in-painting",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23q_interspeech.html": {
    "title": "Hybrid AHS: A Hybrid of Kalman Filter and Deep Learning for Acoustic Howling Suppression",
    "volume": "main",
    "abstract": "Deep learning has been recently introduced for efficient acoustic howling suppression (AHS). However, the recurrent nature of howling creates a mismatch between offline training and streaming inference, limiting the quality of enhanced speech. To address this limitation, we propose a hybrid method that combines a Kalman filter with a self-attentive recurrent neural network (SARNN) to leverage their respective advantages for robust AHS. During offline training, a pre-processed signal obtained from the Kalman filter and an ideal microphone signal generated via teacher-forced training strategy are used to train the deep neural network (DNN). During streaming inference, the DNN's parameters are fixed while its output serves as a reference signal for updating the Kalman filter. Evaluation in both offline and streaming inference scenarios using simulated and real-recorded data shows that the proposed method efficiently suppresses howling and consistently outperforms baselines",
    "checked": true,
    "id": "ef989cfd7f93b3a8d61bfd546ee467f7548ac47e",
    "semantic_title": "hybrid ahs: a hybrid of kalman filter and deep learning for acoustic howling suppression",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ho23_interspeech.html": {
    "title": "Differentially Private Adapters for Parameter Efficient Acoustic Modeling",
    "volume": "main",
    "abstract": "In this work, we devise a parameter-efficient solution to bring differential privacy (DP) guarantees into adaptation of a cross-lingual speech classifier. We investigate a new frozen pretrained adaptation framework for DP-preserving speech modeling without full model fine-tuning. First, we introduce a noisy teacher-student ensemble into a conventional adaptation scheme leveraging a frozen pre-trained acoustic model and attain superior performance than DP-based stochastic gradient descent (DPSGD). Next, we insert residual adapters (RA) between layers of the frozen pre-trained acoustic model. The RAs reduce training cost and time significantly with a negligible performance drop. Evaluated on the open-access Multilingual Spoken Words (MLSW) dataset, our solution reduces the number of trainable parameters by 97.5% using the RAs with only a 4% performance drop with respect to fine-tuning the cross-lingual speech classifier while preserving DP guarantees",
    "checked": true,
    "id": "fad5ed6c02d2f6a2be6c474d14c70d9608777c5d",
    "semantic_title": "differentially private adapters for parameter efficient acoustic modeling",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23b_interspeech.html": {
    "title": "Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation",
    "volume": "main",
    "abstract": "Audio-visual speech enhancement (AV-SE) aims to enhance degraded speech along with extra visual information such as lip videos, and has been shown to be more effective than audio-only speech enhancement. This paper proposes further incorporating ultrasound tongue images to improve lip-based AV-SE systems' performance. Knowledge distillation is employed at the training stage to address the challenge of acquiring ultrasound tongue images during inference, enabling an audio-lip speech enhancement student model to learn from a pre-trained audio-lip-tongue speech enhancement teacher model. Experimental results demonstrate significant improvements in the quality and intelligibility of the speech enhanced by the proposed method compared to the traditional audio-lip speech enhancement baselines. Further analysis using phone error rates (PER) of automatic speech recognition (ASR) shows that palatal and velar consonants benefit most from the introduction of ultrasound tongue images",
    "checked": true,
    "id": "e9fe7cbfd3a4dca8d0ab9d37e03741c89e98fc7c",
    "semantic_title": "incorporating ultrasound tongue images for audio-visual speech enhancement through knowledge distillation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/uezu23_interspeech.html": {
    "title": "Consonant-emphasis Method Incorporating Robust Consonant-section Detection to Improve Intelligibility of Bone-conducted speech",
    "volume": "main",
    "abstract": "A consonant-emphasis (CE) method was proposed to improve the word intelligibility of presented speech by using bone-conducted (BC) headphones. However, the consonant-section detection (CSD) performance of this method is not robust against certain consonants. Therefore, a CE method with robust CSD is necessary for presented BC speech. We focused on improving the word intelligibility of presented BC speech in noisy environments and propose a CE method with robust CSD that combines the detection processes of voiced and unvoiced consonant sections. The evaluation of CSD procedures showed that more robust CSD procedure outperformed those of the conventional CE method as well as voiced CSD only and unvoiced CSD only. Word-intelligibility tests were also conducted on presented BC speech in noisy environments to compare the proposed and conventional methods, and the proposed method significantly improved word intelligibility over these conventional methods at a noise level of 75 dB",
    "checked": true,
    "id": "4589bb8446d21caa7d3e06e1e4f4a8778b1a3266",
    "semantic_title": "consonant-emphasis method incorporating robust consonant-section detection to improve intelligibility of bone-conducted speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sato23_interspeech.html": {
    "title": "Downstream Task Agnostic Speech Enhancement with Self-Supervised Representation Loss",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) is the latest breakthrough in speech processing, especially for label-scarce downstream tasks by leveraging massive unlabeled audio data. The noise robustness of the SSL is one of the important challenges to expanding its application. We can use speech enhancement (SE) to tackle this issue. However, the mismatch between the SE model and SSL models potentially limits its effect. In this work, we propose a new SE training criterion that minimizes the distance between clean and enhanced signals in the feature representation of the SSL model to alleviate the mismatch. We expect that the loss in the SSL domain could guide SE training to preserve or enhance various levels of characteristics of the speech signals that may be required for high-level downstream tasks. Experiments show that our proposal improves the performance of an SE and SSL pipeline on five downstream tasks with noisy input while maintaining the SE performance",
    "checked": true,
    "id": "649fd45481be158627e602252ce2d6c6c6a8713e",
    "semantic_title": "downstream task agnostic speech enhancement with self-supervised representation loss",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/byun23_interspeech.html": {
    "title": "Perceptual Improvement of Deep Neural Network (DNN) Speech Coder Using Parametric and Non-parametric Density Models",
    "volume": "main",
    "abstract": "This paper proposes a method to improve the perceptual quality of an end-to-end neural speech coder using density models for bottleneck samples. Two parametric and non-parametric approaches are explored for modeling the bottleneck sample density. The first approach utilizes a sub-network to generate mean-scale hyperpriors for bottleneck samples, while the second approach models the bottleneck samples using a separate sub-network without any side information. The whole network, including the sub-network, is trained using PAM-based perceptual losses in different timescales to shape quantization noise below the masking threshold. The proposed method achieves a frame-dependent entropy model that enhances arithmetic coding efficiency while emphasizing perceptually relevant audio cues. Experimental results show that the proposed density model combined with PAM-based losses improves perceptual quality compared to conventional speech coders in both objective and subjective tests",
    "checked": true,
    "id": "91c932d16623a7c8904af0aec728064a059843b6",
    "semantic_title": "perceptual improvement of deep neural network (dnn) speech coder using parametric and non-parametric density models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23j_interspeech.html": {
    "title": "DeFT-AN RT: Real-time Multichannel Speech Enhancement using Dense Frequency-Time Attentive Network and Non-overlapping Synthesis Window",
    "volume": "main",
    "abstract": "In real-time speech enhancement models based on the short-time Fourier transform (STFT), algorithmic latency induced by the STFT window size can induce perceptible delays, leading to reduced immersion in real-time applications. This study proposes an efficient real-time enhancement model based on dense frequency-time attentive network (DeFT-AN). The vanilla DeFT-AN consists of cascaded dense blocks and time-frequency transformers, which allow for a smooth transition between time frames through a temporal attention mechanism. To inherit this advantage and reduce algorithmic latency, we develop the lightweight and causal version of DeFT-AN with dual-window size processing that utilizes synthesis windows shorter than analysis windows. The benefit of DeFT-AN in identifying temporal context enables the use of non-overlapping synthesis windows, and experimental results show that the model can achieve the highest performance with the lowest algorithmic latency among STFT-based models",
    "checked": true,
    "id": "fc3e2cb1b0b3a850621bd8d293d9ab62abb5abaf",
    "semantic_title": "deft-an rt: real-time multichannel speech enhancement using dense frequency-time attentive network and non-overlapping synthesis window",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23b_interspeech.html": {
    "title": "A More Accurate Internal Language Model Score Estimation for the Hybrid Autoregressive Transducer",
    "volume": "main",
    "abstract": "We present a novel constrained learning method for hybrid autoregressive transducer (HAT) models that results in more validated language model (LM) adaptation. LM adaptation in HAT is justified only when the transducer logits and the sum of speech and text logits in the label estimation sub-networks are approximately the same. The mean squared error (MSE) between the two logits was added to the HAT loss to encourage the HAT models to satisfy the required condition. The proposed method exhibited significantly lower and more stable internal language model perplexities than those of HAT. Consequently, it attained lower word error rates (WERs) compared to HAT in various model architecture settings and in both cases with and without LM adaptation. In the television content task, the proposed method achieved a relative reduction in WERs of up to 28.60% compared to HAT. In most cases, the accuracy of pre-trained HAT models also improved upon training with the additional MSE loss",
    "checked": true,
    "id": "c1f4fd83d318525db351c7fac8e4dc7863662c87",
    "semantic_title": "a more accurate internal language model score estimation for the hybrid autoregressive transducer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23_interspeech.html": {
    "title": "Attention Gate Between Capsules in Fully Capsule-Network Speech Recognition",
    "volume": "main",
    "abstract": "We present a novel capsule network-based speech recognition model that effectively utilizes the full context of past time capsules. The input capsule sequences are recurrently used by filtering unnecessary contextual information using multi-head attention, which uses previous time output vectors as keys and values, and current time output vectors as queries. We applied the attention gate to the sequential dynamic routing (SDR), an all-capsule speech recognition model. The proposed method attained higher accuracy than the existing SDR with two attention heads on all test sets of the TIMIT and Wall Street Journal (WSJ) corpora while maintaining the same algorithmic delay. For the WSJ corpus, 10.75% of a relative word error rate (WER) reduction was achieved when the required delay was set to 525 ms. In addition, the model showed a 1.76x reduction in delay while maintaining the WERs. The proposed method results in an increase of approximately 0.1% in the number of parameters",
    "checked": true,
    "id": "97bb673f61e3b5370f22ff5527680e3e48f91201",
    "semantic_title": "attention gate between capsules in fully capsule-network speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23g_interspeech.html": {
    "title": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark",
    "volume": "main",
    "abstract": "Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research",
    "checked": true,
    "id": "090b284b2f8fc93ac3e7a92fc9f91bf4965ba75c",
    "semantic_title": "ml-superb: multilingual speech universal performance benchmark",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23l_interspeech.html": {
    "title": "General-purpose Adversarial Training for Enhanced Automatic Speech Recognition Model Generalization",
    "volume": "main",
    "abstract": "We present a new adversarial training method called General-purpose adversarial training (GPAT) that enhances the performance of automatic speech recognition models. In GPAT, we propose the followings: (1) a plausible adversarial examples converter (PAC); (2) a distribution matching regularization term (DM reg.). Compared to previous studies that directly compute gradients with respect to the input, PAC incorporates non-linearity to achieve performance improvement while eliminating the need for extra forward passes. Furthermore, unlike previous studies that use fixed norms, GPAT can generate similar yet diverse samples through DM reg. We demonstrate that the GPAT elevates the performance of various models on the LibriSpeech dataset. Specifically, by applying GPAT to the conformer model, we achieved 5.3% average relative improvements. With respect to the wav2vec 2.0 experiments, our method yielded a 2.0%/4.4% word error rate on the LibriSpeech test sets without a language model",
    "checked": true,
    "id": "818da1cd8dae525aec31b499e468003d0741a964",
    "semantic_title": "general-purpose adversarial training for enhanced automatic speech recognition model generalization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23_interspeech.html": {
    "title": "Joint Instance Reconstruction and Feature Subspace Alignment for Cross-Domain Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition is a popular research branch of speech signal processing. Many previous studies have proven that the generalization ability of the emotion recognition model across domains can be improved by using transfer learning methods. To solve the cross-domain speech emotion recognition problem, this paper proposes a novel transfer learning method, which simultaneously performs the instance reconstruction and subspace alignment. Firstly, we conduct the instance transferring based on coupled projection, which utilizes a weighting reconstruction strategy to exploit the intrinsic information of cross-domain samples and improve the contribution of essential features through an adaptive weighting matrix. Then, we conduct the feature transferring through a novel co-regularized term, which can make the source and target subspace be well aligned. Finally, extensive experiments indicate that our method is superior to several state-of-the-art methods",
    "checked": true,
    "id": "0e22929ae837d48dad4a47352754dc0b51c6e313",
    "semantic_title": "joint instance reconstruction and feature subspace alignment for cross-domain speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moriya23_interspeech.html": {
    "title": "Knowledge Distillation for Neural Transducer-based Target-Speaker ASR: Exploiting Parallel Mixture/Single-Talker Speech Data",
    "volume": "main",
    "abstract": "Neural transducer (RNNT)-based target-speaker speech recognition (TS-RNNT) directly transcribes a target speaker's voice from a multi-talker mixture. It is a promising approach for streaming applications because it does not incur the extra computation costs of a target speech extraction frontend, which is a critical barrier to quick response. TS-RNNT is trained end-to-end given the input speech (i.e., mixtures and enrollment speech) and reference transcriptions. The training mixtures are generally simulated by mixing single-talker signals, but conventional TS-RNNT training does not utilize single-speaker signals. This paper proposes using knowledge distillation (KD) to exploit the parallel mixture/single-talker speech data. Our proposed KD scheme uses an RNNT system pretrained with the target single-talker speech input to generate pseudo labels for the TS-RNNT training. Experimental results show that TS-RNNT systems trained with the proposed KD scheme outperform a baseline TS-RNNT",
    "checked": true,
    "id": "000ca94ed121225aa997088def719c747c4fb797",
    "semantic_title": "knowledge distillation for neural transducer-based target-speaker asr: exploiting parallel mixture/single-talker speech data",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23i_interspeech.html": {
    "title": "Random Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition",
    "volume": "main",
    "abstract": "One of limitations in end-to-end automatic speech recognition (ASR) framework is its performance would be compromised if train-test utterance lengths are mismatched. In this paper, we propose an on-the-fly random utterance concatenation (RUC) based data augmentation method to alleviate train-test utterance length mismatch issue for short-video ASR task. Specifically, we are motivated by observations that our human-transcribed training utterances tend to be much shorter for short-video spontaneous speech (∼3 seconds on average), while our test utterance generated from voice activity detection front-end is much longer (∼10 seconds on average). Such a mismatch can lead to suboptimal performance. Empirically, it's observed the proposed RUC method significantly improves long utterance recognition without performance drop on short one. Overall, it achieves 5.72% word error rate reduction on average for 15 languages and improved robustness to various utterance length",
    "checked": true,
    "id": "92f621eec89837c3b3f353690881a9d409b07e67",
    "semantic_title": "random utterance concatenation based data augmentation for improving short-video speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muthuchamyselvaraj23_interspeech.html": {
    "title": "Adapter Incremental Continual Learning of Efficient Audio Spectrogram Transformers",
    "volume": "main",
    "abstract": "Efficient tuning of neural networks for continual learning with minimal computational resources remains a challenge. In this paper, we propose continual learning of audio classifiers with parameter and compute efficient Audio Spectrogram Transformers (AST). To reduce the trainable parameters without performance degradation we propose AST with Convolutional Adapter, which has less than 5% of trainable parameters of full fine-tuning. To reduce the computational complexity of self-attention, we introduce a novel Frequency-Time factorized Attention (FTA) method that achieves competitive performance with only a factor of the computations. Finally, we formulate our method called Adapter Incremental Continual Learning (AI-CL), as a combination of the parameter-efficient Convolutional Adapter and the compute-efficient FTA. Experiments on ESC-50, SpeechCommandsV2, and Audio-Visual Event benchmarks show that our proposed method efficiently learns new tasks and prevents catastrophic forgetting",
    "checked": true,
    "id": "81018236b57da31875a453866b884e3b3dda71e7",
    "semantic_title": "adapter incremental continual learning of efficient audio spectrogram transformers",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23_interspeech.html": {
    "title": "Rethinking Speech Recognition with A Multimodal Perspective via Acoustic and Semantic Cooperative Decoding",
    "volume": "main",
    "abstract": "Attention-based encoder-decoder (AED) models have shown impressive performance in ASR. However, most existing AED methods neglect to simultaneously leverage both acoustic and semantic features in decoder, which is crucial for generating more accurate and informative semantic states. In this paper, we propose an Acoustic and Semantic Cooperative Decoder (ASCD) for ASR. In particular, unlike vanilla decoders that process acoustic and semantic features in two separate stages, ASCD integrates them cooperatively. To prevent information leakage during training, we design a Causal Multimodal Mask. Moreover, a variant Semi-ASCD is proposed to balance accuracy and computational cost. Our proposal is evaluated on the publicly available AISHELL-1 and aidatatang_200zh datasets using Transformer, Conformer, and Branchformer as encoders, respectively. The experimental results show that ASCD significantly improves the performance by leveraging both the acoustic and semantic information cooperatively",
    "checked": true,
    "id": "0be940a9578fe944175394ec8e0042cec258f7d2",
    "semantic_title": "rethinking speech recognition with a multimodal perspective via acoustic and semantic cooperative decoding",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23b_interspeech.html": {
    "title": "Improving Code-Switching and Name Entity Recognition in ASR with Speech Editing based Data Augmentation",
    "volume": "main",
    "abstract": "Recently, end-to-end (E2E) automatic speech recognition (ASR) models have made great strides and exhibit excellent performance in general speech recognition. However, there remain several challenging scenarios that E2E models are not competent in, such as code-switching and named entity recognition (NER). Data augmentation is a common and effective practice for these two scenarios. However, the current data augmentation methods mainly rely on audio splicing and text-to-speech (TTS) models, which might result in discontinuous, unrealistic, and less diversified speech. To mitigate these potential issues, we propose a novel data augmentation method by applying the text-based speech editing model. The augmented speech from speech editing systems is more coherent and diversified, also more akin to real speech. The experimental results on code-switching and NER tasks show that our proposed method can significantly outperform the audio splicing and neural TTS based data augmentation systems",
    "checked": true,
    "id": "85fe4894afe95e326814d8b7fa41b308a0054255",
    "semantic_title": "improving code-switching and name entity recognition in asr with speech editing based data augmentation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23h_interspeech.html": {
    "title": "Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts",
    "volume": "main",
    "abstract": "This paper presents a novel algorithm for building an automatic speech recognition (ASR) model with imperfect training data. Imperfectly transcribed speech is a prevalent issue in human-annotated speech corpora, which degrades the performance of ASR models. To address this problem, we propose Bypass Temporal Classification (BTC) as an expansion of the Connectionist Temporal Classification (CTC) criterion. BTC explicitly encodes the uncertainties associated with transcripts during training. This is accomplished by enhancing the flexibility of the training graph, which is implemented as a weighted finite-state transducer (WFST) composition. The proposed algorithm improves the robustness and accuracy of ASR systems, particularly when working with imprecisely transcribed speech corpora. Our implementation will be open-sourced",
    "checked": true,
    "id": "b620d46668d62930e41393168434118bb9a2bfcb",
    "semantic_title": "bypass temporal classification: weakly supervised automatic speech recognition with imperfect transcripts",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lv23_interspeech.html": {
    "title": "DCCRN-KWS: An Audio Bias Based Model for Noise Robust Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "Real-world complex acoustic environments especially the ones with a low signal-to-noise ratio (SNR) will bring tremendous challenges to a keyword spotting (KWS) system. Inspired by the recent advances of neural speech enhancement and context bias in speech recognition, we propose a robust audio context bias based DCCRN-KWS model to address this challenge. We form the whole architecture as a multi-task learning framework for both denoising and keyword spotting, where the DCCRN encoder is connected with the KWS model. Helped with the denoising task, we further introduce an audio context bias module to leverage the real keyword samples and bias the network to better discriminate keywords in noisy conditions. Feature merge and complex context linear modules are also introduced to strengthen such discrimination and to effectively leverage contextual information respectively. Experiments on an internal challenging dataset and the HIMIYA public dataset show that DCCRN-KWS is superior in performance, while the ablation study demonstrates the good design of the whole model",
    "checked": true,
    "id": "4f9f170be7b22dc9ad5f220e8f1ea414a9399bbe",
    "semantic_title": "dccrn-kws: an audio bias based model for noise robust small-footprint keyword spotting",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23b_interspeech.html": {
    "title": "OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models have shown great promise over Supervised Learning (SL) ones in low-resource settings. However, the advantages of SSL are gradually weakened when the amount of labeled data increases in many industrial applications. To further improve the ASR performance when abundant labels are available, we first explore the potential of combining SL and SSL ASR models via analyzing their complementarity in recognition accuracy and optimization property. Then, we propose a novel Optimal Transport based Fusion (OTF) method for SL and SSL models without incurring extra computation cost in inference. Specifically, optimal transport is adopted to softly align the layer-wise weights to unify the two different networks into a single one. Experimental results on the public 1k-hour English LibriSpeech dataset and our in-house 2.6k-hour Chinese dataset show that OTF largely outperforms the individual models with lower error rates",
    "checked": true,
    "id": "3820ec258664fab9e279db8ca7d31d375e6e530b",
    "semantic_title": "otf: optimal transport based fusion of supervised and self-supervised learning models for automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bleeker23_interspeech.html": {
    "title": "Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents an extension to train end-to-end Context-Aware Transformer Transducer (CATT) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases in the context list during training, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications",
    "checked": true,
    "id": "96a2c0877721ab1915dd9baa38286d2d950691d6",
    "semantic_title": "approximate nearest neighbour phrase mining for contextual speech recognition",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vandereeckt23_interspeech.html": {
    "title": "Rehearsal-Free Online Continual Learning for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Fine-tuning an Automatic Speech Recognition (ASR) model to new domains results in degradation on original domains, referred to as Catastrophic Forgetting (CF). Continual Learning (CL) attempts to train ASR models without suffering from CF. While in ASR, offline CL is usually considered, online CL is a more realistic but also more challenging scenario where the model, unlike in offline CL, does not know when a task boundary occurs. Rehearsal-based methods, which store previously seen utterances in a memory, are often considered for online CL, in ASR and other research domains. However, recent research has shown that weight averaging is an effective method for offline CL in ASR. Based on this result, we propose, in this paper, a rehearsal-free method applicable for online CL. Our method outperforms all baselines, including rehearsal-based methods, in two experiments. Our method is a next step towards general CL for ASR, which should enable CL in all scenarios with few if any constraints",
    "checked": true,
    "id": "2febc2e47c944378865796121466056156d195ae",
    "semantic_title": "rehearsal-free online continual learning for automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23_interspeech.html": {
    "title": "Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring",
    "volume": "main",
    "abstract": "Speech fluency/disfluency can be evaluated by analyzing a range of phonetic and prosodic features. Deep neural networks are commonly trained to map fluency-related features into the human scores. However, the effectiveness of deep learning-based models is constrained by the limited amount of labeled training samples. To address this, we introduce a self-supervised learning (SSL) approach that takes into account phonetic and prosody awareness for fluency scoring. Specifically, we first pre-train the model using a reconstruction loss function, by masking phones and their durations jointly on a large amount of unlabeled speech and text prompts. We then fine-tune the pre-trained model using human-annotated scoring data. Our experimental results, conducted on datasets such as Speechocean762 and our non-native datasets, show that our proposed method outperforms the baseline systems in terms of Pearson correlation coefficients (PCC). Moreover, we also conduct an ablation study to better understand the contribution of phonetic and prosody factors during the pre-training stage",
    "checked": true,
    "id": "cf8e7ecad1de2403baff4e0b035fc7daa623bfda",
    "semantic_title": "phonetic and prosody-aware self-supervised learning approach for non-native fluency scoring",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23_interspeech.html": {
    "title": "Disentangling the Contribution of Non-native Speech in Automated Pronunciation Assessment",
    "volume": "main",
    "abstract": "This study explores the impact of using non-native speech data in acoustic model training for pronunciation assessment systems. The goal is to determine how introducing non-native data in acoustic model training can influence alignment accuracy and assessment performance. Acoustic models are trained using different combinations of native and non-native speech data, and the Goodness of Pronunciation (GOP) metric is used to evaluate performance. Results show that models trained with manually labeled non-native data yield the highest assessment performance and alignment accuracy. Models trained with mixed non-native and native data perform best when considering the GOP distribution on both non-native and native speech. Additionally, models trained with native data are more robust to alignment variations. These findings highlight the importance of carefully selecting and incorporating non-native data in acoustic model training for pronunciation assessment systems",
    "checked": true,
    "id": "d107c6611d5c705d0c2f1924b8b03166fadf4f5b",
    "semantic_title": "disentangling the contribution of non-native speech in automated pronunciation assessment",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ryu23_interspeech.html": {
    "title": "A Joint Model for Pronunciation Assessment and Mispronunciation Detection and Diagnosis with Multi-task Learning",
    "volume": "main",
    "abstract": "Empirical studies report a strong correlation between pronunciation proficiency scores and phonetic errors in non-native speech assessments of human evaluators. However, the existing system of computer-assisted pronunciation training (CAPT) regards automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD) as independent and focuses on individual performance improvement. Motivated by the correlation between two tasks, we propose a novel architecture that jointly tackles APA and MDD using CTC and cross-entropy criteria with a multi-task learning scheme to benefit both tasks. To leverage additional knowledge transfer, Wav2Vec2-robust finetuned on TIMIT is used for the joint optimization. The integrated model significantly outperforms single-task learning, with a mean of 0.057 PCC increase for APA and 0.004 F1 increase for MDD on Speechocean762, which reveals that proficiency scores and phonetic errors are correlated for both human and model assessments",
    "checked": true,
    "id": "64a86c00541526821c4f8cf650691d1ae36879dd",
    "semantic_title": "a joint model for pronunciation assessment and mispronunciation detection and diagnosis with multi-task learning",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23d_interspeech.html": {
    "title": "Assessing Intelligibility in Non-native Speech: Comparing Measures Obtained at Different Levels",
    "volume": "main",
    "abstract": "Speech intelligibility (SI) is essential in communication and second language learning. In this study, non-native SI was measured through Visual Analogue Scale (VAS) scores and Orthographic Transcriptions (OTs) of read aloud sentences. Seven measures automatically derived from the OTs at word and subword levels were studied. The reliability of the intelligibility measures and the correlations between VAS scores and OT-based measures were also explored. Despite the different speaker language backgrounds, the recruited raters exhibited high scoring reliability. The correlations between VAS scores and OT-based measures were weak, corroborating previous assumptions that they refer to two related but distinct notions, comprehensibility (VAS) and intelligibility (OT). OT-based measures are reliable and valid indicators of SI. The results are discussed in relation to previous studies and avenues for future research are proposed",
    "checked": true,
    "id": "8de6875bdf6b1090d4e3f74ca1aee1f5ba47fd07",
    "semantic_title": "assessing intelligibility in non-native speech: comparing measures obtained at different levels",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23_interspeech.html": {
    "title": "End-to-End Word-Level Pronunciation Assessment with MASK Pre-training",
    "volume": "main",
    "abstract": "Pronunciation assessment is a major challenge in the computer-aided pronunciation training system, especially at the word (phoneme)-level. To obtain word (phoneme)-level scores, current methods usually rely on aligning components to obtain acoustic features of each word (phoneme), which limits the performance of assessment to the accuracy of alignments. Therefore, to address this problem, we propose a simple yet effective method, namely Masked pre-training for Pronunciation Assessment (MPA). Specifically, by incorporating a mask-predict strategy, our MPA supports end-to-end training without leveraging any aligning components and can solve misalignment issues to a large extent during prediction. Furthermore, we design two evaluation strategies to enable our model to conduct assessments in both unsupervised and supervised settings. Experimental results on SpeechOcean762 dataset demonstrate that MPA could achieve better performance than previous methods, without any explicit alignment. In spite of this, MPA still has some limitations, such as requiring more inference time and reference text. They expect to be addressed in future work",
    "checked": true,
    "id": "9c90d47bc4e4cdc53e409d810ed96882865f9c15",
    "semantic_title": "end-to-end word-level pronunciation assessment with mask pre-training",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chao23_interspeech.html": {
    "title": "A Hierarchical Context-aware Modeling Approach for Multi-aspect and Multi-granular Pronunciation Assessment",
    "volume": "main",
    "abstract": "Automatic Pronunciation Assessment (APA) plays a vital role in Computer-assisted Pronunciation Training (CAPT) when evaluating a second language (L2) learner's speaking proficiency. However, an apparent downside of most de facto methods is that they parallelize the modeling process throughout different speech granularities without accounting for the hierarchical and local contextual relationships among them. In light of this, a novel hierarchical approach is proposed in this paper for multi-aspect and multi-granular APA. Specifically, we first introduce the notion of sup-phonemes to explore more subtle semantic traits of L2 speakers. Second, a depth-wise separable convolution layer is exploited to better encapsulate the local context cues at the sub-word level. Finally, we use a score-restraint attention pooling mechanism to predict the sentence-level scores and optimize the component models with a multitask learning (MTL) framework. Extensive experiments carried out on a publicly-available benchmark dataset, viz. speechocean762, demonstrate the efficacy of our approach in relation to some cutting-edge baselines",
    "checked": true,
    "id": "6fc89eb8f2f8db50aff10ccdd2f2f5480e98e75e",
    "semantic_title": "a hierarchical context-aware modeling approach for multi-aspect and multi-granular pronunciation assessment",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23j_interspeech.html": {
    "title": "Automatic Prediction of Language Learners' Listenability Using Speech and Text Features Extracted from Listening Drills",
    "volume": "main",
    "abstract": "When language learners are listening to L2 speech, they experience listening disfluencies or breakdowns not rarely. Although listening disfluencies are mental phenomena, previous studies showed that they can be measured acoustically by asking the learners to shadow the L2 speech, where inarticulate productions in shadowing are reasonably attributed to listening disfluencies. In this paper, we model the measured listening disfluencies by BLSTM and attempt to predict which words in new listening drills are difficult to perceive correctly. Taking some studies in psycholinguistics and applied linguistics into account, which revealed what kind of factors influence human perception of spoken words, speech and text features are extracted from listening drills and used for prediction. Experiments show that our model shows a better performance than other models previously proposed and that learners' factors are very effective for prediction because learners are developing through training",
    "checked": true,
    "id": "921014073a118aa8383b3101e2c4bde5fa0911c1",
    "semantic_title": "automatic prediction of language learners' listenability using speech and text features extracted from listening drills",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shekar23b_interspeech.html": {
    "title": "Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-level Goodness of Pronunciation Transformer",
    "volume": "main",
    "abstract": "Automatic pronunciation assessment (APA) plays an important role in providing feedback for self-directed language learners in computer-assisted pronunciation training (CAPT). Several mispronunciation detection and diagnosis (MDD) systems have achieved promising performance based on end-to-end phoneme recognition. However, assessing the intelligibility of second language (L2) remains a challenging problem. One issue is the lack of large-scale labeled speech data from non-native speakers. Additionally, relying only on one aspect (e.g., accuracy) at a phonetic level may not provide a sufficient assessment of pronunciation quality and L2 intelligibility. It is possible to leverage segmental/phonetic-level features such as goodness of pronunciation (GOP), however, feature granularity may cause a discrepancy in prosodic-level (suprasegmental) pronunciation assessment. In this study, Wav2vec 2.0-based MDD and Goodness Of Pronunciation feature-based Transformer are employed to characterize L2 intelligibility. Here, an L2 speech dataset, with human-annotated prosodic (suprasegmental) labels, is used for multi-granular and multi-aspect pronunciation assessment and identification of factors important for intelligibility in L2 English speech. The study provides a transformative comparative assessment of automated pronunciation scores versus the relationship between suprasegmental features and listener perceptions, which taken collectively can help support the development of instantaneous assessment tools and solutions for L2 learners",
    "checked": true,
    "id": "5028fa83522d5df0e7080f537b6b740bfc8f3718",
    "semantic_title": "assessment of non-native speech intelligibility using wav2vec2-based mispronunciation detection and multi-level goodness of pronunciation transformer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23f_interspeech.html": {
    "title": "Adapting an Unadaptable ASR System",
    "volume": "main",
    "abstract": "As speech recognition model sizes and training data requirements grow, it is increasingly common for systems to only be available via APIs from online service providers rather than having direct access to models themselves. In this scenario it is challenging to adapt systems to a specific target domain. To address this problem we consider the recently released OpenAI Whisper ASR as an example of a large-scale ASR system to assess adaptation methods. An error correction based approach is adopted, as this does not require access to the model, but can be trained from either 1-best or N-best outputs that are normally available via the ASR API. LibriSpeech is used as the primary target domain for adaptation. The generalization ability of the system in two distinct dimensions are then evaluated. First, whether the form of correction model is portable to other speech recognition domains, and secondly whether it can be used for ASR models having a different architecture",
    "checked": true,
    "id": "6137cff058ee8d34b45839ddd228b042c9c2a072",
    "semantic_title": "adapting an unadaptable asr system",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23c_interspeech.html": {
    "title": "Addressing Cold Start Problem for End-to-end Automatic Speech Scoring",
    "volume": "main",
    "abstract": "Integrating automatic speech scoring/assessment systems has become a critical aspect of second-language speaking education. With self-supervised learning advancements, end-to-end speech scoring approaches have exhibited promising results. However, this study highlights the significant decrease in the performance of speech scoring systems in new question contexts, thereby identifying this as a cold start problem in terms of items. With the finding of cold-start phenomena, this paper seeks to alleviate the problem by following methods: 1) prompt embeddings, 2) question context embeddings using BERT or CLIP models, and 3) choice of the pretrained acoustic model. Experiments are conducted on TOEIC speaking test datasets collected from English-as-a-second-language (ESL) learners rated by professional TOEIC speaking evaluators. The results demonstrate that the proposed framework not only exhibits robustness in a cold-start environment but also outperforms the baselines for known content",
    "checked": true,
    "id": "ff81ca33d6fb8c7ef38c4e0f74ce4eeaacc8a341",
    "semantic_title": "addressing cold start problem for end-to-end automatic speech scoring",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ribeiro23b_interspeech.html": {
    "title": "Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings",
    "volume": "main",
    "abstract": "The Grapheme-to-Phoneme (G2P) task aims to convert orthographic input into a discrete phonetic representation. G2P conversion is beneficial to various speech processing applications, such as text-to-speech and speech recognition. However, these tend to rely on manually-annotated pronunciation dictionaries, which are often time-consuming and costly to acquire. In this paper, we propose a method to improve the G2P conversion task by learning pronunciation examples from audio recordings. Our approach bootstraps a G2P with a small set of annotated examples. The G2P model is used to train a multilingual phone recognition system, which then decodes speech recordings with a phonetic representation. Given hypothesized phoneme labels, we learn pronunciation dictionaries for out-of-vocabulary words, and we use those to re-train the G2P system. Results indicate that our approach consistently improves the phone error rate of G2P systems across languages and amount of available data",
    "checked": true,
    "id": "a67dc585d180ec3a646d01799293dc34e3e05c18",
    "semantic_title": "improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/richter23b_interspeech.html": {
    "title": "Orthography-based Pronunciation Scoring for Better CAPT Feedback",
    "volume": "main",
    "abstract": "We establish the viability of a streamlined architecture for pedagogically appropriate computer assisted pronunciation training (CAPT), to give second language learners automatic feedback about their mispronunciations. This takes advantage of end-to-end speech recognition models to detect mispronunciation in audio segments that correspond directly to orthographic letters, in contrast to standard mispronunciation detection using phone representations. Results in a classification task show the potential for similar sensitivity to non-nativelike phonetic errors in grapheme-aligned segments as in phone-aligned segments. Advantages of this approach over phone-based pronunciation scoring can include providing naturally comprehensible (orthographic, not phonemic) feedback to learners, being inherently open-vocabulary in the target language, and evaluating pronunciations with reference to a full range of target-language acoustic variants rather than a prespecified canonical phone sequence",
    "checked": true,
    "id": "156d0b46faf275d83cb82e802611d7cde4dc0007",
    "semantic_title": "orthography-based pronunciation scoring for better capt feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23r_interspeech.html": {
    "title": "Zero-Shot Automatic Pronunciation Assessment",
    "volume": "main",
    "abstract": "Automatic Pronunciation Assessment (APA) is vital for computer-assisted language learning. Prior methods rely on annotated speech-text data to train Automatic Speech Recognition (ASR) models or speech-score data to train regression models. In this work, we propose a novel zero-shot APA method based on the pre-trained acoustic model, HuBERT. Our method involves encoding speech input and corrupting them via a masking module. We then employ the Transformer encoder and apply k-means clustering to obtain token sequences. Finally, a scoring module is designed to measure the number of wrongly recovered tokens. Experimental results on speechocean762 demonstrate that the proposed method achieves comparable performance to supervised regression baselines and outperforms non-regression baselines in terms of Pearson Correlation Coefficient (PCC). Additionally, we analyze how masking strategies affect the performance of APA",
    "checked": true,
    "id": "c663cfa15080e564a4a361caf43b623695ba779d",
    "semantic_title": "zero-shot automatic pronunciation assessment",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huu23_interspeech.html": {
    "title": "Mispronunciation detection and diagnosis model for tonal language, applied to Vietnamese",
    "volume": "main",
    "abstract": "A tonal language is a language in which the meaning of words is not only determined by the sounds of the consonants and vowels, but also by the pitch or tone used to pronounce them. Mispronunciation Detection and Diagnosis (MD&D) of tonal languages is challenging since tone presentation is difficult to be detected correctly. There has been relatively little research conducted on tonal languages, with most focusing on Mandarin. Furthermore, there are no publicly available datasets and source codes for the task. This work constructs and publishes a Vietnamese dataset for experimenting with MD&D, as well as proposes an end-to-end model that utilizes pitch analysis to detect and diagnose mispronunciations for tonal languages, especially focusing on Vietnamese. Experiments show that the proposed model achieved a relative improvement in phone error rate of 7.1% and detection accuracy of 7.4% compared to a state-of-the-art baseline",
    "checked": true,
    "id": "64cde42cf0e4b3dcda9ab7a0a4ecb552aa96ee00",
    "semantic_title": "mispronunciation detection and diagnosis model for tonal language, applied to vietnamese",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dignum23_interspeech.html": {
    "title": "Beyond the AI hype: Balancing Innovation and Social Responsibility",
    "volume": "main",
    "abstract": "AI can extend human capabilities but requires addressing challenges in education, jobs, and biases. Taking a responsible approach involves understanding AI's nature, design choices, societal role, and ethical considerations. Recent AI developments, including foundational models, transformer models, generative models, and large language models (LLMs), raise questions about whether they are changing the paradigm of AI, and about the responsibility of those that are developing and deploying AI systems. In all these developments, is vital to understand that AI is not an autonomous entity but rather dependent on human responsibility and decision-making In this talk, I will further discuss the need for a responsible approach to AI that emphasize trust, cooperation, and the common good. Taking responsibility involves regulation, governance, and awareness. Ethics and dilemmas are ongoing considerations, but require understanding that trade-offs must be made and that decision processes are always contextual. Taking responsibility requires designing AI systems with values in mind, implementing regulations, governance, monitoring, agreements, and norms. Rather than viewing regulation as a constraint, it should be seen as a stepping stone for innovation, ensuring public acceptance, driving transformation, and promoting business differentiation. Responsible Artificial Intelligence (AI) is not an option but the only possible way to go forward in AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/stemmer23_interspeech.html": {
    "title": "Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach",
    "volume": "main",
    "abstract": "Speech emotion recognition for natural human-to-human conversations has many useful applications, including generating comprehensive meeting transcripts or detecting communication problems. We investigate the detection of emotional hotspots, i.e., regions of increased speaker involvement in technical meetings. As there is a scarcity of annotated, not-acted corpora, and to avoid introducing unwanted biases to our models, we follow a cross-corpus approach where models are trained on data from domains unrelated to the test data. In this work we propose a model ensemble trained on spontaneous phone conversations, political discussions and acted emotions. Evaluation is performed on the natural ICSI and AMI meeting corpora, where we used existing hotspot annotations for ICSI and created labels for the AMI corpus. A semi-supervised fine-tuning procedure is introduced to adapt the model. We show that an equal error rate of below 21% can be achieved using the proposed cross-corpus approach",
    "checked": true,
    "id": "c69cd5ba401a20261bbb9dd301c12e58aef7b3d7",
    "semantic_title": "detection of emotional hotspots in meetings using a cross-corpus approach",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matsuda23_interspeech.html": {
    "title": "Detection of Laughter and Screaming Using the Attention and CTC Models",
    "volume": "main",
    "abstract": "This study aimed to detect social signals, such as laughter and screams, in real environments. Social signals influence human-to-human communication. To effectively apply these signals in various systems, computer systems must appropriately detect social signals. In this study, social signal detection (SSD) experiments were conducted to demonstrate which of three feature sets, i.e., a spectral feature set, prosodic feature set, and spectral and prosodic feature set, was best for detecting laughter and screaming. The results showed that using both the spectral and prosodic feature sets yielded the best performance, with 81.83% accuracy for laughter and 81.68% accuracy for screams. Moreover, the detection model comparison results revealed that the bidirectional long short-term memory (BiLSTM)-connectionist temporal classification (CTC) yielded the best laughter detection performance, while attention-CTC was best for scream detection. These results suggest that CTC is effective for SSD",
    "checked": true,
    "id": "42671c0d5013b123fd5dc43e9d7e45d8fb8b6a7a",
    "semantic_title": "detection of laughter and screaming using the attention and ctc models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhattacharya23_interspeech.html": {
    "title": "Capturing Formality in Speech Across Domains and Languages",
    "volume": "main",
    "abstract": "The linguistic notion of formality is one dimension of stylistic variation in human communication. A universal characteristic of language production, formality has surface-level realizations in written and spoken language. In this work, we explore ways of measuring the formality of such realizations in multilingual speech corpora across a wide range of domains. We compare measures of formality, contrasting textual and acoustic-prosodic metrics. We believe that a combination of these should correlate well with downstream applications. Our findings include: an indication that certain prosodic variables might play a stronger role than others; no correlation between prosodic and textual measures; limited evidence for anticipated inter-domain trends, but some evidence of consistency of measures between languages. We conclude that non-lexical indicators of formality in speech may be more subtle than our initial expectations, motivating further work on reliably encoding spoken formality",
    "checked": true,
    "id": "36ca450788efeee276b1be7c1ababe05a32bc8e2",
    "semantic_title": "capturing formality in speech across domains and languages",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23e_interspeech.html": {
    "title": "Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio",
    "volume": "main",
    "abstract": "To perform automatic family audio analysis, past studies have collected recordings using phone, video, or audio-only recording device like LENA, investigated supervised learning methods, and used or fine-tuned general-purpose embeddings learned from large pretrained models. In this study, we advance the audio component of a new infant wearable multi-modal device called LittleBeats (LB) by learning family audio representation via wav2vec 2.0 (W2V2) pretraining. We show given a limited number of labeled LB home recordings, W2V2 pretrained using 1k-hour of unlabeled home recordings outperforms oracle W2V2 pretrained on 52k-hour unlabeled audio in terms of parent/infant speaker diarization (SD) and vocalization classifications (VC) at home. Extra relevant external unlabeled and labeled data further benefit W2V2 pretraining and fine-tuning. With SpecAug and environmental speech corruptions, we obtain 12% relative gain on SD and moderate boost on VC. Code and model weights are available",
    "checked": true,
    "id": "a6912b85944b1ff5055d5a914c7f0cab2d53dd3d",
    "semantic_title": "towards robust family-infant audio analysis based on unsupervised pretraining of wav2vec 2.0 on large-scale unlabeled family audio",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feindt23_interspeech.html": {
    "title": "Cues to next-speaker projection in conversational Swedish: Evidence from reaction times",
    "volume": "main",
    "abstract": "We present first results of a study investigating the salience and typicality of prosodic markers in Swedish at turn ends for turn-yielding and turn-keeping purposes. We performed an experiment where participants (N=32) were presented with conversational chunks and, after the audio ended, were asked to determine which of two speakers would speak next by clicking a picture on a screen. Audio stimuli were manipulated by (i) raising and (ii) lowering fo over the last 500 ms of a turn, (iii) speeding up or (iv) slowing down duration over the last 500 ms, and (v) raising and (vi) lowering the last pitch peak. Out of all manipulations, increasing the speech rate was found to be the most disruptive p<.005). Higher speech rate led to longer reaction times in turn-keeping, which were shorter in turn-yielding. Other manipulations did not significantly alter reaction times. Results may be complemented with eye movement data, to elucidate cognitive mechanisms underlying turn-taking behavior",
    "checked": true,
    "id": "810fc956a506312d3690678525013e9be3bb7875",
    "semantic_title": "cues to next-speaker projection in conversational swedish: evidence from reaction times",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/buker23_interspeech.html": {
    "title": "Multiple Instance Learning for Inference of Child Attachment From Paralinguistic Aspects of Speech",
    "volume": "main",
    "abstract": "Attachment is a psychological construct that accounts for the way children perceive their relationship with their caregivers. Depending on the attachment condition, a child can either be secure or insecure. Identifying as many insecure children as possible is important to mitigate the negative consequences of insecure attachment in adult life. For this reason, this article proposes an attachment recognition approach that, compared to other approaches, increases the Recall, the percentage of insecure children identified as such. The approach is based on Multiple Instance Learning, a body of methodologies dealing with data represented as \"bags\" of feature vectors. This is suitable for speech recordings because these are typically represented as vector sequences. The experiments involved 104 participants of age 5 to 9. The results show that insecure children can be identified with Recall up to 63.3% (accuracy up to 75%), an improvement with respect to most existing models",
    "checked": true,
    "id": "20b135d33966d60a5fb042cee8cf34b7087b9822",
    "semantic_title": "multiple instance learning for inference of child attachment from paralinguistic aspects of speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eskimez23_interspeech.html": {
    "title": "Real-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Personalized speech enhancement (PSE) is a real-time SE approach utilizing a speaker embedding of a target person to remove background noise, reverberation, and interfering voices. To deploy a PSE model for full duplex communications, the model must be combined with acoustic echo cancellation (AEC), although such a combination has been less explored. This paper proposes a series of methods that are applicable to various model architectures to develop efficient causal models that can handle the tasks of PSE, AEC, and joint PSE-AEC. We present extensive evaluation results using both simulated data and real recordings, covering various acoustic conditions and evaluation metrics. The results show the effectiveness of the proposed methods for two different model architectures. Our best joint PSE-AEC model comes close to the expert models optimized for individual tasks of PSE and AEC in their respective scenarios and significantly outperforms the expert models for the combined PSE-AEC task",
    "checked": true,
    "id": "fbfb5091ebbad691125d22ab76d2785bb0385b85",
    "semantic_title": "real-time joint personalized speech enhancement and acoustic echo cancellation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23g_interspeech.html": {
    "title": "TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective",
    "volume": "main",
    "abstract": "Despite the promising performance of existing frame-wise all-neural beamformers in the speech enhancement field, it remains unclear what the underlying mechanism exists. In this paper, we revisit the beamforming behavior from the beam-space dictionary perspective and formulate it into the learning and mixing of different beam-space components. Based on that, we propose an all-neural beamformer called TaylorBM to simulate Taylor's series expansion operation in which the 0th-order term serves as a spatial filter to conduct the beam mixing, and several high-order terms are tasked with residual noise cancellation for post-processing. The whole system is devised to work in an end-to-end manner. Experiments are conducted on the spatialized LibriSpeech corpus and results show that the proposed approach outperforms existing advanced baselines in terms of evaluation metrics",
    "checked": true,
    "id": "74cf3364ccbfb60dbceffe5ba2e61429c6ceda45",
    "semantic_title": "taylorbeamixer: learning taylor-inspired all-neural multi-channel speech enhancement from beam-space dictionary perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23s_interspeech.html": {
    "title": "MFT-CRN:Multi-scale Fourier Transform for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Convolutional recurrent networks (CRN) that combine a convolutional encoder-decoder (CED) structure with a recurrent structure have shown promising results in monaural speech enhancement. However, the commonly used short-time Fourier transform fails to balance the needs of frequency and time resolution effectively, which is crucial for accurate speech estimation. To address this issue, we propose MFT-CRN, a multi-scale short-time Fourier transform fusion model. We process the input speech signal through short-time Fourier transforms with different window functions, and add them layer by layer in the encoder and decoder of the network to achieve feature fusion with different window functions, effectively balancing frequency and temporal resolution. Comprehensive experiments on the WSJ0 dataset show that MFT-CRN significantly outperforms the method using only a single window function in terms of short-time intelligibility and perceptual evaluation of speech quality",
    "checked": true,
    "id": "d96f6fa8cb4fbbc5d5bd80d7f6ac70a2750743af",
    "semantic_title": "mft-crn:multi-scale fourier transform for monaural speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/guo23_interspeech.html": {
    "title": "Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement",
    "volume": "main",
    "abstract": "The goal of this study is to implement diffusion models for speech enhancement (SE). The first step is to emphasize the theoretical foundation of variance-preserving (VP)-based interpolation diffusion under continuous conditions. Subsequently, we present a more concise framework that encapsulates both the VP- and variance-exploding (VE)-based interpolation diffusion methods. We demonstrate that these two methods are special cases of the proposed framework. Additionally, we provide a practical example of VP-based interpolation diffusion for the SE task. To improve performance and ease model training, we analyze the common difficulties encountered in diffusion models and suggest amenable hyper-parameters. Finally, we evaluate our model against several methods using a public benchmark to showcase the effectiveness of our approach",
    "checked": true,
    "id": "e502ad15677ee41ef454c14acb280abc3b0ed70a",
    "semantic_title": "variance-preserving-based interpolation diffusion models for speech enhancement",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/taherian23_interspeech.html": {
    "title": "Multi-input Multi-output Complex Spectral Mapping for Speaker Separation",
    "volume": "main",
    "abstract": "Current deep learning based multi-channel speaker separation methods produce a monaural estimate of speaker signals captured by a reference microphone. This work presents a new multi-channel complex spectral mapping approach that simultaneously estimates the real and imaginary spectrograms of all speakers at all microphones. The proposed multi-input multi-output (MIMO) separation model uses a location-based training (LBT) criterion to resolve the permutation ambiguity in talker-independent speaker separation across microphones. Experimental results show that the proposed MIMO separation model outperforms a multi-input single-output (MISO) speaker separation model with monaural estimates. We also combine the MIMO separation model with a beamformer and a MISO speech enhancement model to further improve separation performance. The proposed approach achieves the state-of-the-art speaker separation on the open LibriCSS dataset",
    "checked": true,
    "id": "fce2ec66047942842e5e92ac767f17360929c1c4",
    "semantic_title": "multi-input multi-output complex spectral mapping for speaker separation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oberhag23_interspeech.html": {
    "title": "Short-term Extrapolation of Speech Signals Using Recursive Neural Networks in the STFT Domain",
    "volume": "main",
    "abstract": "This paper investigates several approaches for the short-term extrapolation of speech signals. The signal extrapolation methods are embedded into a nested two-stage spectral analysis-synthesis system for single-channel noise reduction in hearing aids. They predict additional signal samples in the low-frequency sub-bands of the first analysis stage and may compensate the additional algorithmic latency of the second, higher-resolution analysis stage in these bands. We thus achieve a higher spectral resolution in frequency bands below 3 kHz without increasing the algorithmic latency of the overall system. In the context of noise reduction, especially female voices benefit from the increased spectral resolution in the lower sub-bands of the first stage. We show that among the investigated approaches, both recursive neural-network-based extrapolation methods provide benefits in conjunction with a noise reduction algorithm and outperform our baseline linear extrapolation method",
    "checked": true,
    "id": "e33774540f41f85d22584c5e401ebddb1a419622",
    "semantic_title": "short-term extrapolation of speech signals using recursive neural networks in the stft domain",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pandey23_interspeech.html": {
    "title": "Listener sensitivity to deviating obstruents in WaveNet",
    "volume": "main",
    "abstract": "This paper investigates the perceptual significance of the deviation in obstruents previously observed in WaveNet vocoders. The study involved presenting stimuli of varying lengths to 128 participants, who were asked to identify whether each stimulus was produced by a human or a machine. The participants' responses were captured using a 2-alternative forced choice task. The study found that while the length of the stimuli did not reliably affect participants' accuracy in the task, the concentration of obstruents did have a significant effect. Participants were consistently more accurate in identifying WaveNet stimuli as machine when the phrases were obstruent-rich. These findings show that the deviation in obstruents reported in WaveNet voices is perceivable by human listeners. The test protocol may be of wider utility in TTS",
    "checked": true,
    "id": "4a1bc694b2f7eb7b15bb885e7d6b8b124cfee36a",
    "semantic_title": "listener sensitivity to deviating obstruents in wavenet",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23d_interspeech.html": {
    "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
    "volume": "main",
    "abstract": "We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech",
    "checked": true,
    "id": "f70d0bb5685b8a119fcaa91c7e9b29c10cf403b6",
    "semantic_title": "how generative spoken language modeling encodes noisy speech: investigation from phonetics to syntactics",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/camp23_interspeech.html": {
    "title": "MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors",
    "volume": "main",
    "abstract": "The quality of synthetic speech is typically evaluated using subjective listening tests. An underlying assumption is that these tests are reliable, i.e., running the test multiple times gives consistent results. A common approach to study reliability is a replication study. Existing studies focus primarily on Mean Opinion Score (MOS), and few consider the error bounds from the original test. In contrast, we present a replication study of both MOS and AB preference tests to answer two questions: (1) which of the two test types is more reliable for system comparison, and (2) for both test types, how reliable are the results with respect to their estimated standard error? We find that while AB tests are more reliable for system comparison, standard errors are underestimated for both test types. We show that these underestimates are partially due to broken independence assumptions, and suggest alternate methods of standard error estimation that account for dependencies among ratings",
    "checked": true,
    "id": "504bdd978507d86c3f27989479c7efe24b272e59",
    "semantic_title": "mos vs. ab: evaluating text-to-speech systems reliably using clustered standard errors",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23r_interspeech.html": {
    "title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting",
    "volume": "main",
    "abstract": "Automatic Mean Opinion Score (MOS) prediction is crucial to evaluate the perceptual quality of the synthetic speech. While recent approaches using pre-trained self-supervised learning (SSL) models have shown promising results, they only partly address the data scarcity issue for the feature extractor. This leaves the data scarcity issue for the decoder unresolved and leading to suboptimal performance. To address this challenge, we propose a retrieval-augmented MOS prediction method, dubbed RAMP, to enhance the decoder's ability against the data scarcity issue. A fusing network is also proposed to dynamically adjust the retrieval scope for each instance and the fusion weights based on the predictive confidence. Experimental results show that our proposed method outperforms the existing methods in multiple scenarios",
    "checked": true,
    "id": "6f6393908a5dd3059182da815876e1d5a7a7acfe",
    "semantic_title": "ramp: retrieval-augmented mos prediction via confidence-based dynamic weighting",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/melnikleroy23_interspeech.html": {
    "title": "Can Better Perception Become a Disadvantage? Synthetic Speech Perception in Congenitally Blind Users",
    "volume": "main",
    "abstract": "Modern Text-To-Speech systems are rarely tested on non-standard user groups, such as people with impairments. Nevertheless, evidence suggests that some of these groups might perceive synthetic speech differently (better or worse) than regular users. The current study investigated for the first time how synthetic speech is perceived by blind vs. sighted users. For this purpose, we used a speeded AX discrimination task and tested how sighted and blind listeners perceive synthetic speech of different qualities. Results show that blind participants had significantly better discrimination on this task, and both groups performed worse when the perceptual differences in the synthetic speech were smaller. This suggests that blind participants were indeed more sensitive to the acoustic characteristics of synthetic speech compared to their sighted peers. We discuss implications for speech perception and the development of modern speech technologies",
    "checked": true,
    "id": "0ea5ab42fe6c3d3cfc2409436bf8ea935b3a288e",
    "semantic_title": "can better perception become a disadvantage? synthetic speech perception in congenitally blind users",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cooper23_interspeech.html": {
    "title": "Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech",
    "volume": "main",
    "abstract": "Mean Opinion Score (MOS) is a popular measure for evaluating synthesized speech. However, the scores obtained in MOS tests are heavily dependent upon many contextual factors. One such factor is the overall range of quality of the samples presented in the test -- listeners tend to try to use the entire range of scoring options available to them regardless of this, a phenomenon which is known as range-equalizing bias. In this paper, we systematically investigate the effects of range-equalizing bias on MOS tests for synthesized speech by conducting a series of listening tests in which we progressively \"zoom in\" on a smaller number of systems in the higher-quality range. This allows us to better understand and quantify the effects of range-equalizing bias in MOS tests",
    "checked": true,
    "id": "0a41d9d29e7fa75313729ef3323497d4369479cd",
    "semantic_title": "investigating range-equalizing bias in mean opinion score ratings of synthesized speech",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/he23_interspeech.html": {
    "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU",
    "checked": true,
    "id": "daf9e24adbba3d1aead91cbac26502d3043db069",
    "semantic_title": "can chatgpt detect intent? evaluating large language models for spoken language understanding",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rajaa23_interspeech.html": {
    "title": "Improving End-to-End SLU performance with Prosodic Attention and Distillation",
    "volume": "main",
    "abstract": "Most End-to-End SLU methods depend on the pretrained ASR or language model features for intent prediction. However, other essential information in speech, such as prosody, is often ignored. Recent research has shown improved results in classifying dialogue acts by incorporating prosodic information. The margins of improvement in these methods are minimal as the neural models ignore prosodic features. In this work, we propose prosody-attention, which uses the prosodic features differently to generate attention maps across time frames of the utterance. Then we propose prosody-distillation to explicitly learn the prosodic information in the acoustic encoder rather than concatenating the implicit prosodic features. Both the proposed methods improve the baseline results, and the prosody-distillation method gives an intent classification accuracy improvement of 8% and 2% on SLURP and STOP datasets over the prosody baseline",
    "checked": true,
    "id": "2735ddc5618b54d5ce717adf3752921a4de511ce",
    "semantic_title": "improving end-to-end slu performance with prosodic attention and distillation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23n_interspeech.html": {
    "title": "Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "3c5470149173a6343346694d8da09a09c33c7d49",
    "semantic_title": "modality confidence aware training for robust end-to-end spoken language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23c_interspeech.html": {
    "title": "Cross-Modal Semantic Alignment before Fusion for Two-Pass End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "The deliberation-based two-pass model that combines both semantic and acoustic information can effectively improve the performance of end-to-end (E2E) spoken language understanding (SLU). However, existing two-pass models usually simply fuse speech embedding and text embedding without taking into account the inherent distinctions between these two modalities. We propose a novel approach named Cross-modal Semantic Alignment before Fusion (CSAF), which adopt contrastive loss aligning speech and text embeddings before fusing them. We introduce a shared semantic memory transformer to project the embeddings from two modalities into a common semantic space, and a multi-modal gated network to generate the fused embeddings. We conduct experiments on the FSC Challenge test set and SLURP dataset. The results demonstrate that our method can significantly promote intent classification accuracy, achieving an absolute improvement of 3.1% over previous works in the FSC Challenge Utterance Set",
    "checked": true,
    "id": "426965f0e3b3dc24a55821915523b6032b2ad01a",
    "semantic_title": "cross-modal semantic alignment before fusion for two-pass end-to-end spoken language understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sunder23_interspeech.html": {
    "title": "ConvKT: Conversation-Level Knowledge Transfer for Context Aware End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "Dialog history enhances downstream classification performance in both speech and text based dialog systems. However, there still exists a gap in dialog history integration in a fully end-to-end (E2E) spoken dialog system (SDS) versus a textual dialog system. Text-based dialog systems use large language models (LLMs) to encode long-range dependencies by attending to the entire conversation as a contiguous token sequence. This is not possible in an E2E SDS, as speech sequences can be intractably long. We propose a convolution subsampling approach to make the speech sequence of a conversation tractable and use a conformer to attend to the speech-based conversation in a fine-grained manner. This model is further enhanced via a conversation-level knowledge transfer from a LLM using a token-level alignment strategy. Finetuning the E2E model pretrained this way gives significant gains, of up to 8%, over strong non-contextual baselines in the E2E dialog act classification task on two datasets",
    "checked": true,
    "id": "0d388b5c4a617792d62655af6b8ee1f71dab09f4",
    "semantic_title": "convkt: conversation-level knowledge transfer for context aware end-to-end spoken language understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23_interspeech.html": {
    "title": "GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering",
    "volume": "main",
    "abstract": "Spoken question answering (SQA) aims to identify the correct answer to the given the question from a spoken passage. Most conventional SQA frameworks combine an automatic speech recognition (ASR) module and a text question answering (TQA) module in a cascaded manner, which might suffer from error propagation and high latency. To tackle these issues, several end-to-end SQA frameworks based on Textless NLP are proposed. However, existing end-to-end models still fail to outperform the cascade models with the similar number of parameters. In this paper, to improve textless SQA, we propose GhostT5, which generates more features from the remaining features with very cheap operations for stronger performance. Experiment results and further analysis show that our GhostT5 achieves the new state-of-the-art performance on NMSQA dataset and surpasses cascaded SQA models. More encouragingly, GhostT5 surpasses the previous best end-to-end SQA model with less than half of the parameters",
    "checked": true,
    "id": "dc0436318a08f4df8f9653f164a830f245caca8b",
    "semantic_title": "ghostt5: generate more features with cheap operations to improve textless spoken question answering",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23b_interspeech.html": {
    "title": "Obstructive Sleep Apnea Detection using Pre-trained Speech Representations",
    "volume": "main",
    "abstract": "Obstructive sleep apnea (OSA) is a condition commonly affecting middle-aged men that can disturb sleep, cause daytime tiredness, and increase the risk of heart disease. Speech can serve as a valuable biomarker for identifying and predicting the severity of OSA due to its connection with changes in throat structure. This study proposes a new deep-learning-based method for detecting OSA by analyzing speech recordings of participants in sitting and lying positions. The method utilizes a Siamese structure that employs a pre-trained XLSR model to encode ten utterances for each position, reducing the amount of necessary training data and enabling comparison of throat structure changes between the two positions through voice analysis. The study also explores the use of patient characteristic features. Results show this approach achieves an F1 value of 0.725 on our in-house dataset, proving the feasibility of end-to-end speech OSA detection with foundation models",
    "checked": true,
    "id": "0dc21b1086d042d696930e17b984b4327e4f12d0",
    "semantic_title": "obstructive sleep apnea detection using pre-trained speech representations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23f_interspeech.html": {
    "title": "EEG-based Auditory Attention Detection with Spatiotemporal Graph and Graph Convolutional Network",
    "volume": "main",
    "abstract": "The ability to detect auditory attention from electroencephalography (EEG) offers many possibilities for brain-computer interface (BCI) applications, such as hearing assistive devices. However, effective feature representation for EEG signals remains a challenge due to the complex spatial and temporal dynamics of EEG signals. To overcome this challenge, we introduce a Spatiotemporal Graph Convolutional Network (ST-GCN), which combines a temporal attention mechanism and a graph convolutional module. The temporal attention mechanism captures the temporal dynamics of EEG segments, while the graph convolutional module learns the spatial pattern of multi-channel EEG signals. We evaluate the performance of our proposed ST-GCN on two publicly available datasets and demonstrate significant improvements over existing state-of-the-art models. These findings suggest that the ST-GCN model has the potential to advance auditory attention detection in real-life BCI applications",
    "checked": true,
    "id": "7d5bcf3e0a7274326628c5f406942e7ae859a671",
    "semantic_title": "eeg-based auditory attention detection with spatiotemporal graph and graph convolutional network",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/beeson23_interspeech.html": {
    "title": "Silent Speech Recognition with Articulator Positions Estimated from Tongue Ultrasound and Lip Video",
    "volume": "main",
    "abstract": "We present a multi-speaker silent speech recognition system trained on articulator features derived from the Tongue and Lips corpus, a multi-speaker corpus of ultrasound tongue imaging and lip video data. We extracted articulator features using the pose estimation software DeepLabCut, then trained recognition models with these point-tracking features using Kaldi. We trained with voiced utterances, then tested performance on both voiced and silent utterances. Our multi-speaker SSR improved WER by 23.06% when compared to a previous similar multi-speaker SSR system which used image-based instead of point-tracking features. We also found great improvements (up to 15.45% decrease in WER) in recognition of silent speech using fMLLR adaptation compared to raw features. Finally, we investigated differences in articulator trajectories between voiced and silent speech and found that speakers tend to miss articulatory targets that are present in voiced speech when speaking silently",
    "checked": true,
    "id": "e1c80da2f7cf1fa7d3db4f8220bc7df0a8ff7041",
    "semantic_title": "silent speech recognition with articulator positions estimated from tongue ultrasound and lip video",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23r_interspeech.html": {
    "title": "Auditory Attention Detection in Real-Life Scenarios Using Common Spatial Patterns from EEG",
    "volume": "main",
    "abstract": "Auditory attention detection (AAD) methods based on electroencephalography (EEG) could be used in neuro-steered hearing devices to help hearing-loss people improve their hearing ability. However, previous studies have mostly obtained EEG data in laboratory settings which limits the practical application of neuro-steered hearing devices. In this study, we employ a common spatial pattern (CSP) algorithm to perform AAD using EEG signals collected by a wireless mobile EEG system, from real-life scenarios when people are walking and sitting. The results show that the CSP method can achieve AAD accuracy between 81.3% and 87.5% when using different decision windows (1 s- 30 s), which is better than previous methods based on linear mapping methods and convolutional neural networks (CNN). This proves that the CSP algorithm can decode people's attention efficiently even outside the laboratory. Analysis of EEG frequency bands shows that the δ and β bands have high activity in attention tasks",
    "checked": true,
    "id": "cbf24317cd7e287298ff2c4acce166adc2b4e35a",
    "semantic_title": "auditory attention detection in real-life scenarios using common spatial patterns from eeg",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23g_interspeech.html": {
    "title": "Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG",
    "volume": "main",
    "abstract": "Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech",
    "checked": true,
    "id": "465e3528f56edc662cbdde43fe9a02758411e5f1",
    "semantic_title": "diff-e: diffusion-based learning for decoding imagined speech eeg",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/csapo23_interspeech.html": {
    "title": "Towards Ultrasound Tongue Image prediction from EEG during speech production",
    "volume": "main",
    "abstract": "Previous initial research has already been carried out to propose speech-based BCI using brain signals (e.g. non-invasive EEG and invasive sEEG / ECoG), but there is a lack of combined methods that investigate non-invasive brain, articulation, and speech signals together and analyze the cognitive processes in the brain, the kinematics of the articulatory movement and the resulting speech signal. In this paper, we describe our multimodal (electroencephalography, ultrasound tongue imaging, and speech) analysis and synthesis experiments, as a feasibility study. We extend the analysis of brain signals recorded during speech production with ultrasound-based articulation data. From the brain signal measured with EEG, we predict ultrasound images of the tongue with a fully connected deep neural network. The results show that there is a weak but noticeable relationship between EEG and ultrasound tongue images, i.e. the network can differentiate articulated speech and neutral tongue position",
    "checked": true,
    "id": "5bc4ad449789de7ff72b4e47f7c7081fdff95a76",
    "semantic_title": "towards ultrasound tongue image prediction from eeg during speech production",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/toth23_interspeech.html": {
    "title": "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks",
    "volume": "main",
    "abstract": "Thanks to the latest deep learning algorithms, silent speech interfaces (SSI) are now able to synthesize intelligible speech from articulatory movement data under certain conditions. However, the resulting models are rather speaker-specific, making a quick switch between users troublesome. Even for the same speaker, these models perform poorly cross-session, i.e. after dismounting and re-mounting the recording equipment. To aid quick speaker and session adaptation of ultrasound tongue imaging-based SSI models, we extend our deep networks with a spatial transformer network (STN) module, capable of performing an affine transformation on the input images. Although the STN part takes up only about 10% of the network, our experiments show that adapting just the STN module might allow to reduce MSE by 88% on the average, compared to retraining the whole network. The improvement is even larger (around 92%) when adapting the network to different recording sessions from the same speaker",
    "checked": true,
    "id": "0735e74a9bc5aaf70979f2c626e8a8b9688ba7e7",
    "semantic_title": "adaptation of tongue ultrasound-based silent speech interfaces using spatial transformer networks",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/scheck23_interspeech.html": {
    "title": "STE-GAN: Speech-to-Electromyography Signal Conversion using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "With Speech-to-Electromyography Generative Adversarial Network (STE-GAN), we propose a model which can synthesize Electromyography (EMG) signals from acoustic speech. We condition the generator network on representations of the spoken content obtained from a voice conversion model. Given these representations, the generator outputs an EMG signal corresponding to the articulated content of the acoustic speech in the setting of a specific EMG recording session. In comparison to previous work, STE-GAN directly generates EMG signals from acoustic speech. As it uses more speaker-independent content representations as input, it can synthesize EMG signals from speech of speakers who were unseen during training",
    "checked": true,
    "id": "432875ec218e05e1ab77c1b4266d1754b5239945",
    "semantic_title": "ste-gan: speech-to-electromyography signal conversion using generative adversarial networks",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/salomons23_interspeech.html": {
    "title": "Spanish Phone Confusion Analysis for EMG-Based Silent Speech Interfaces",
    "volume": "main",
    "abstract": "This paper describes a set of phone classification experiments based on electromyography (EMG) signals and a subsequent phone confusion analysis, as part of a project that aims to restore speech for Spanish laryngectomees by developing a Silent Speech Interface (SSI). Understanding the relationship between speech and the muscles used for speaking is essential to learn the possibilities and limitations of such EMG-based SSIs, before advancing to a complex task such as direct EMG-to-speech conversion. When considering only information from the muscles of the face and neck, important information from the tongue and vocal cords is missing. This is reflected in the results, which show confusion between pairs of phones that only differ in the position of the tongue or the voicing feature",
    "checked": true,
    "id": "bac5c07481ad7b769b72370b64f692b7c5ca046c",
    "semantic_title": "spanish phone confusion analysis for emg-based silent speech interfaces",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23l_interspeech.html": {
    "title": "Hybrid Silent Speech Interface Through Fusion of Electroencephalography and Electromyography",
    "volume": "main",
    "abstract": "Silent Speech Interface (SSI) can enable interaction in a new and natural way based on no-audible biosignals generated by the human body. Electroencephalography (EEG) or surface electromyography (sEMG) generated during speech production can be utilized to decode silent speech. However, obtaining complementary information from EEG and sEMG is still challenging. This paper presents a hybrid SSI based on the converter between bimodal electrophysiological signals and audio signals. EEG and sEMG are fused through two sequence-to-sequence models, and multi-task losses are applied to achieve complementarity between speech intention and muscle activity in silent speech. The feasibility of the proposed fusion method is validated in the silent speech dataset, and an average objective character error rate (CER) of 7.22% among eight speakers is obtained. The experimental results show that our bimodal-based hybrid SSI facilitates the conversion of electrophysiological signals to audio",
    "checked": true,
    "id": "474caa51703da1b8d2bb0d89833efbe54d205714",
    "semantic_title": "hybrid silent speech interface through fusion of electroencephalography and electromyography",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarkar23_interspeech.html": {
    "title": "Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) models use only the intrinsic structure of a given signal, independent of its acoustic domain, to extract essential information from the input to an embedding space. This implies that the utility of such representations is not limited to modeling human speech alone. Building on this understanding, this paper explores the cross-transferability of SSL neural representations learned from human speech to analyze bio-acoustic signals. We conduct a caller discrimination analysis and a caller detection study on Marmoset vocalizations using eleven SSL models pre-trained with various pretext tasks. The results show that the embedding spaces carry meaningful caller information and can successfully distinguish the individual identities of Marmoset callers without fine-tuning. This demonstrates that representations pre-trained on human speech can be effectively applied to the bio-acoustics domain, providing valuable insights for future investigations in this field",
    "checked": true,
    "id": "f904b9ca82972a983b661a7d7855f52352d84dd3",
    "semantic_title": "can self-supervised neural representations pre-trained on human speech distinguish animal callers?",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cai23b_interspeech.html": {
    "title": "Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data Using Contrastive Learning with Varying Pre-Training Domains",
    "volume": "main",
    "abstract": "Rapid discovery of new diseases, such as COVID-19 can enable a timely epidemic response, preventing the large-scale spread and protecting public health. However, limited research efforts have been taken on this problem. In this paper, we propose a contrastive learning-based modeling approach for COVID-19 coughing and breathing pattern discovery from non-COVID coughs. To validate our models, extensive experiments have been conducted using four large audio datasets and one image dataset. We further explore the effects of different factors, such as domain relevance and augmentation order on the pre-trained models. Our results show that the proposed model can effectively distinguish COVID-19 coughing and breathing from unlabeled data and labeled non-COVID coughs with an accuracy of up to 0.81 and 0.86, respectively. Findings from this work will guide future research to detect an outbreak of a new disease early",
    "checked": true,
    "id": "0f28b2ef2d098bf8c194811afb0bb1280d48394d",
    "semantic_title": "discovering covid-19 coughing and breathing patterns from unlabeled data using contrastive learning with varying pre-training domains",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23_interspeech.html": {
    "title": "Background-aware Modeling for Weakly Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "Nowadays, a common framework for weakly supervised sound event detection (WSSED) is multiple instance learning (MIL). However, MIL directly optimizes the clip-level classification results, so it tends to localize the most distinct part rather than the entire sound event, making the indiscriminating parts of sound events mistakenly identified as background sounds. In this paper, we focus on adding background awareness for WSSED by proposing a learning structure called BA-WSSED. Our BA-WSSED first introduces a pseudo separator with softmax activation and two aggregators to purify and aggregate the event feature and the background feature, respectively. Then, with the help of the proposed background-aware staggered (BAS) loss, both the event classifier and the background classifier are learned to generate staggered classification scores for discerning and suppressing background sounds. Experiments show that our BA-WSSED significantly improves the performance of the general MIL-based WSSED method on multiple datasets and can be employed on various baseline models",
    "checked": true,
    "id": "65ae26f921b0abebaaba29af2bcba82591a068e9",
    "semantic_title": "background-aware modeling for weakly supervised sound event detection",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/srivastava23_interspeech.html": {
    "title": "How to (Virtually) Train Your Speaker Localizer",
    "volume": "main",
    "abstract": "Learning-based methods have become ubiquitous in speaker localization. Existing systems rely on simulated training sets for the lack of sufficiently large, diverse and annotated real datasets. Most room acoustics simulators used for this purpose rely on the image source method (ISM) because of its computational efficiency. This paper argues that carefully extending the ISM to incorporate more realistic surface, source and microphone responses into training sets can significantly boost the real-world performance of speaker localization systems. It is shown that increasing the training-set realism of a state-of-the-art direction-of-arrival estimator yields consistent improvements across three different real test sets featuring human speakers in a variety of rooms and various microphone arrays. An ablation study further reveals that every added layer of realism contributes positively to these improvements",
    "checked": true,
    "id": "50ec9df4395264e474850f4ed9ffd9573c4ad23b",
    "semantic_title": "how to (virtually) train your speaker localizer",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghosh23b_interspeech.html": {
    "title": "MMER: Multimodal Multi-task Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose MMER, a novel Multimodal Multi-task learning approach for Speech Emotion Recognition. MMER leverages a novel multimodal network based on early-fusion and cross-modal self-attention between text and acoustic modalities and solves three novel auxiliary tasks for learning emotion recognition from spoken utterances. In practice, MMER outperforms all our baselines and achieves state-of-the-art performance on the IEMOCAP benchmark. Additionally, we conduct extensive ablation studies and results analysis to prove the effectiveness of our proposed approach",
    "checked": true,
    "id": "10ab7df49d5a61cf656d97092590af3ed4defd4f",
    "semantic_title": "mmer: multimodal multi-task learning for speech emotion recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/khandelwal23_interspeech.html": {
    "title": "A Multi-Task Learning Framework for Sound Event Detection using High-level Acoustic Characteristics of Sounds",
    "volume": "main",
    "abstract": "Sound event detection (SED) entails identifying the type of sound and estimating its temporal boundaries from acoustic signals. These events are uniquely characterized by their spatio-temporal features, which are determined by the way they are produced. In this study, we leverage some distinctive high-level acoustic characteristics of various sound events to assist the SED model training, without requiring additional labeled data. Specifically, we use the DCASE Task 4 2022 dataset and categorize the 10 classes into four subcategories based on their high-level acoustic characteristics. We then introduce a novel multi-task learning framework that jointly trains the SED and high-level acoustic characteristics classification tasks, using shared layers and weighted loss. Our method significantly improves the performance of the SED system, achieving a 36.3% improvement in terms of the polyphonic sound event detection score compared to the baseline on the DCASE 2022 Task 4 validation set",
    "checked": true,
    "id": "3296c89f3338e5ff53097366dacfe44768450c4e",
    "semantic_title": "a multi-task learning framework for sound event detection using high-level acoustic characteristics of sounds",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/neumann23b_interspeech.html": {
    "title": "A Multimodal Investigation of Speech, Text, Cognitive and Facial Video Features for Characterizing Depression With and Without Medication",
    "volume": "main",
    "abstract": "Clinical depression is one of the most common mental disorders and technology for remote assessment of depression, including monitoring of treatment responses, is gaining more and more importance. Using a cloud-based multimodal dialog platform, we conducted a crowdsourced study to investigate the effect of depression severity and antidepressant use on various acoustic, linguistic, cognitive, and orofacial features. Our findings show that multiple features from all tested modalities show statistically significant differences between subjects with no or minimal depression and subjects with more severe depression symptoms. Moreover, certain acoustic and visual features show significant differences between subjects with moderately severe or severe symptoms who take antidepressants and those who do not take any. Machine learning experiments show that subjects with and without medication can be better discriminated from each other at higher severity levels",
    "checked": true,
    "id": "649703d09c09fd1d06a9c1f802302ba4bac533ce",
    "semantic_title": "a multimodal investigation of speech, text, cognitive and facial video features for characterizing depression with and without medication",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/addlesee23_interspeech.html": {
    "title": "Understanding Disrupted Sentences Using Underspecified Abstract Meaning Representation",
    "volume": "main",
    "abstract": "Voice assistant accessibility is generally overlooked as today's spoken dialogue systems are trained on huge corpora to help them understand the 'average' user. This raises frustrating barriers for certain user groups as their speech shifts from the average. People with dementia pause more frequently mid-sentence for example, and people with hearing impairments may mispronounce words learned post-diagnosis. We explore whether semantic parsing can improve accessibility for people with non-standard speech, and consequently become more robust to external disruptions like dogs barking, sirens passing, or doors slamming mid-utterance. We generate corpora of disrupted sentences paired with their underspecified Abstract Meaning Representation (AMR) graphs, and use these to train pipelines to understand and repair disruptions. Our best disruption recovery pipeline lost only 1.6% graph similarity f-score when compared to a model given the full original sentence",
    "checked": true,
    "id": "269a77ef89c0a88e6aac7a1bb0455bcbc39a7efa",
    "semantic_title": "understanding disrupted sentences using underspecified abstract meaning representation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/field23_interspeech.html": {
    "title": "Developing Speech Processing Pipelines for Police Accountability",
    "volume": "main",
    "abstract": "Police body-worn cameras have the potential to improve accountability and transparency in policing. Yet in practice, they result in millions of hours of footage that is never reviewed. We investigate the potential of large pre-trained speech models for facilitating reviews, focusing on ASR and officer speech detection in footage from traffic stops. Our proposed pipeline includes training data alignment and filtering, fine-tuning with resource constraints, and combining officer speech detection with ASR for a fully automated approach. We find that (1) fine-tuning strongly improves ASR performance on officer speech (WER=12-13%), (2) ASR on officer speech is much more accurate than on community member speech (WER=43.55-49.07%), (3) domain-specific tasks like officer speech detection and diarization remain challenging. Our work offers practical applications for reviewing body camera footage and general guidance for adapting pre-trained speech models to noisy multi-speaker domains",
    "checked": true,
    "id": "38c2e4e54a50ea5027c7a06ab325c22ece7b6c40",
    "semantic_title": "developing speech processing pipelines for police accountability",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szekely23_interspeech.html": {
    "title": "Prosody-controllable Gender-ambiguous Speech Synthesis: A Tool for Investigating Implicit Bias in Speech Perception",
    "volume": "main",
    "abstract": "This paper proposes a novel method to develop gender-ambiguous TTS, which can be used to investigate hidden gender bias in speech perception. Our aim is to provide a tool for researchers to conduct experiments on language use associated with specific genders. Ambiguous voices can also be beneficial for virtual assistants, to help reduce stereotypes and increase acceptance. Our approach uses a multi-speaker embedding in a neural TTS engine, combining two corpora recorded by a male and a female speaker to achieve a gender-ambiguous timbre. We also propose speaker-disentangled prosody control to ensure that the timbre is robust across a range of prosodies and enable more expressive speech. We optimised the output using an SSL-based network trained on hundreds of speakers. We conducted perceptual evaluations on the settings that were judged most ambiguous by the network, which showed that listeners perceived the speech samples as gender-ambiguous, also in prosody-controlled conditions",
    "checked": true,
    "id": "9428c0298159678aec0bc96e5ed1c4f79d7700c5",
    "semantic_title": "prosody-controllable gender-ambiguous speech synthesis: a tool for investigating implicit bias in speech perception",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rouas23_interspeech.html": {
    "title": "Affective attributes of French caregivers' professional speech",
    "volume": "main",
    "abstract": "In this paper, we detail our approach to studying the vocal characteristics of caregivers in retirement homes. To achieve this goal, we conducted recordings of 20 professional caregivers across two retirement homes. Using headset microphones connected to smartphones, we were able to capture the caregivers' speech while allowing them complete freedom of movement without compromising sound quality. The recordings consisted of three tasks: reading text, informal interviews, and professional role-play scenarios with a fictitious patient. We processed the recordings using an automatic speech recognition system, which provided word or phone sequences and their corresponding timestamps. Our analysis focused on identifying differences in emotional tone, lexical content, speech rate, fundamental frequency, and intensity between spontaneous speech conditions. Ultimately, our aim is to develop automated training tools that capture the unique vocal characteristics of professional caregivers",
    "checked": true,
    "id": "4fd98e75381201ca8eb87ff2e25ccd21e67dcc3c",
    "semantic_title": "affective attributes of french caregivers' professional speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/casanova23_interspeech.html": {
    "title": "ASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion",
    "volume": "main",
    "abstract": "We explore cross-lingual multi-speaker speech synthesis and cross-lingual voice conversion applied to data augmentation for automatic speech recognition (ASR) systems in low/medium-resource scenarios. Through extensive experiments, we show that our approach permits the application of speech synthesis and voice conversion to improve ASR systems using only one target-language speaker during model training. We also managed to close the gap between ASR models trained with synthesized versus human speech compared to other works that use many speakers. Finally, we show that it is possible to obtain promising ASR training results with our data augmentation method using only a single real speaker in a target language",
    "checked": true,
    "id": "32e3b5de1dca765c6a50f0797bb30e1dd012ae0e",
    "semantic_title": "asr data augmentation in low-resource settings using cross-lingual multi-speaker tts and cross-lingual voice conversion",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23_interspeech.html": {
    "title": "Personality-aware Training based Speaker Adaptation for End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "Speaker adaptation has been widely studied to solve the mismatch between training and test conditions for end-to-end automatic speech recognition (ASR). A key challenge of speaker adaptation is lack of sufficient annotated target-speaker data. Considering the training set is always a large-scale one and contains various speakers, it is likely that utterances in the training set can have similar voice characters with the target speaker, and naturally those similar utterances can be treated as a supplement for target speaker data in the adaptation process. Therefore, we propose personality-aware training (PAT) framework to adapt a pre-trained ASR to the target speaker. In PAT, the small-scale target speaker data is viewed as anchors, and the losses of training samples are re-weighted according to the voice character similarity between the anchors and training samples, where the voice character similarity is derived from the speaker or prosody embedding extractor. Experiments on KeSpeech and MagicData corpora show that, compared with the unadapted system, the proposed method achieves 6.35% and 11.86% relative reduction on character error rate with only 10-minute pseudo-label and true-label adaptation data, respectively",
    "checked": true,
    "id": "fef7c232ad0a9c39e1f7887045dc7a27aca428e8",
    "semantic_title": "personality-aware training based speaker adaptation for end-to-end speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ito23b_interspeech.html": {
    "title": "Target Vocabulary Recognition Based on Multi-Task Learning with Decomposed Teacher Sequences",
    "volume": "main",
    "abstract": "This paper proposes a method for target vocabulary recognition based on multi-task learning with decomposed teacher sequences. The proposed method first decomposes teacher sequences into the target vocabulary and the non-target vocabulary sequences. Then, multi-task learning is performed by calculating losses for both the target vocabulary sequence and the non-target vocabulary sequence. By utilizing information from both target and non-target vocabulary, our proposed method provides more stable training and more accurate recognition of target vocabulary than single-task learning using only the target vocabulary. Experiments conducted on the Corpus of Spontaneous Japanese (CSJ) dataset, using numerals and katakana as target vocabulary, demonstrate the effectiveness of our proposed method. The results show a maximum CER improvement rate of 27% for katakana and 34% for numerals in target vocabulary recognition, as well as an 84% reduction in insertion errors in non-target vocabulary utterances",
    "checked": true,
    "id": "8923410a16dfd8aefb7d4d91ad7859f7019519b3",
    "semantic_title": "target vocabulary recognition based on multi-task learning with decomposed teacher sequences",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shen23_interspeech.html": {
    "title": "Wave to Syntax: Probing spoken language models for syntax",
    "volume": "main",
    "abstract": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters",
    "checked": true,
    "id": "61190f218433126828863d20425202503ee4eaaf",
    "semantic_title": "wave to syntax: probing spoken language models for syntax",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naowarat23_interspeech.html": {
    "title": "Effective Training of Attention-based Contextual Biasing Adapters with Synthetic Audio for Personalised ASR",
    "volume": "main",
    "abstract": "Contextual biasing (CB) is an effective approach for contextualising hidden features of neural transducer ASR models to improve rare word recognition. CB relies on relatively large quantities of relevant human annotated natural speech during training, limiting its effectiveness in low-resource scenarios. In this work, we propose a novel approach that reduces the reliance on real speech by using synthesised audios for training CB adapters. We introduce a projection module (PM) that transforms encoder features of synthesised audios prior to CB training to better match real speech. We penalise PM with consistency regularisation to encourage higher similarity between features of real and synthesised speech. The proposed method maintains the same performance on both named-entity and general datasets while using half of the real speech data for CB training. Furthermore, we show a 16% word error rate reduction when the full real-speech training dataset is extended with synthetic utterances",
    "checked": true,
    "id": "4b873a002e365a593c5841ad02f7cad4e0d7835d",
    "semantic_title": "effective training of attention-based contextual biasing adapters with synthetic audio for personalised asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23b_interspeech.html": {
    "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
    "volume": "main",
    "abstract": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments",
    "checked": true,
    "id": "3e4397cf03ffb19d7bd6b7ce7b9dcb70da48d2f6",
    "semantic_title": "pushing the limits of unsupervised unit discovery for ssl speech representation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/haque23_interspeech.html": {
    "title": "SlothSpeech: Denial-of-service Attack Against Speech Recognition Models",
    "volume": "main",
    "abstract": "Deep Learning (DL) models have been popular nowadays to execute different speech-related tasks, including automatic speech recognition (ASR). As ASR is being used in different real-time scenarios, it is important that the ASR model remains efficient against minor perturbations to the input. Hence, evaluating efficiency robustness of the ASR model is the need of the hour. We show that popular ASR models like Speech2Text model and Whisper model have dynamic computation based on different inputs, causing dynamic efficiency. In this work, we propose SlothSpeech, a denial-of-service attack against ASR models, which exploits the dynamic behaviour of the model. SlothSpeech uses the probability distribution of the output text tokens to generate perturbations to the audio such that efficiency of the ASR model is decreased. We find that SlothSpeech generated inputs can increase the latency up to 40X times the latency induced by benign input",
    "checked": true,
    "id": "83be6de4499357fe78daeb2548c48cedc1bd8196",
    "semantic_title": "slothspeech: denial-of-service attack against speech recognition models",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23d_interspeech.html": {
    "title": "CLRL-Tuning: A Novel Continual Learning Approach for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a novel Continual Learning approach, which is Randomly Layer-wise Tuning (CLRL-Tuning) of a pre-trained Automatic Speech Recognition (ASR) model. CLRL-Tuning tackles the randomness of subsequent datasets by updating the parameters of randomly selected encoder layers of the pre-trained model (such as wav2vec 2.0) for every training epoch. CLRL-Tuning is different from the previous approaches in that it neither uses previous datasets, nor expands/runs previous models. Furthermore, we perform experiments to evaluate our approach compared with four strong baselines, including Knowledge Distillation and Gradient Episodic Memory. Our approach achieves significant improvements over the baselines in average word error rate (WER) for the wav2vec 2.0 model. Additionally, we implement ablation studies for our approach by tuning one, three, six and full encoder layers of the model, and experimental results show only tuning one encoder layer of the model at each training epoch is the most effective way to mitigate catastrophic forgetting",
    "checked": true,
    "id": "b6e2b95f5026a079a22d663592376aa9b8d4ab71",
    "semantic_title": "clrl-tuning: a novel continual learning approach for automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23_interspeech.html": {
    "title": "Exploring Sources of Racial Bias in Automatic Speech Recognition through the Lens of Rhythmic Variation",
    "volume": "main",
    "abstract": "Although studies have shown that one issue of bias in modern automatic speech recognition (ASR) technologies is degraded performance for African American English (AAE) speakers, the mechanism by which systems fail for AAE speakers is still not well-understood. The present study aims to offer insight into this issue by examining whether errors are driven by rhythmic variation in ethnolects. We computed seven quantitative measures of speech rhythm in a reading task as produced by AAE and General American English (GAE) speakers and related these metrics to word error rates. The results confirmed racial bias against AAE speakers with higher error rates when AAE speakers produced more variable durations in vowel sounds. Rhythmic variation, on the other hand, is not a contributing factor for the errors in GAE. The result calls for interdisciplinary collaboration between linguists and ASR builders to add timing components of speech to the system to ensure fairness in artificial intelligence for currently underserved groups",
    "checked": true,
    "id": "36c54e4d686f7aca920bcf297258590ad158b638",
    "semantic_title": "exploring sources of racial bias in automatic speech recognition through the lens of rhythmic variation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23e_interspeech.html": {
    "title": "Can Contextual Biasing Remain Effective with Whisper and GPT-2?",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (ASR) and large language models, such as Whisper and GPT-2, have recently been scaled to use vast amounts of training data. Despite the large amount of training data, infrequent content words that occur in a particular task may still exhibit poor ASR performance, with contextual biasing a possible remedy. This paper investigates the effectiveness of neural contextual biasing for Whisper combined with GPT-2. Specifically, this paper proposes integrating an adapted tree-constrained pointer generator (TCPGen) component for Whisper and a dedicated training scheme to dynamically adjust the final output without modifying any Whisper model parameters. Experiments across three datasets show a considerable reduction in errors on biasing words with a biasing list of 1000 words. Contextual biasing was more effective when applied to domain-specific data and can boost the performance of Whisper and GPT-2 without losing their generality",
    "checked": true,
    "id": "44c05783a548cd91110d901eb51b2b529c4b4fbc",
    "semantic_title": "can contextual biasing remain effective with whisper and gpt-2?",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niizumi23_interspeech.html": {
    "title": "Masked Modeling Duo for Speech: Specializing General-Purpose Audio Representation to Speech using Denoising Distillation",
    "volume": "main",
    "abstract": "Self-supervised learning general-purpose audio representations have demonstrated high performance in a variety of tasks. Although they can be optimized for application by fine-tuning, even higher performance can be expected if they can be specialized to pre-train for an application. This paper explores the challenges and solutions in specializing general-purpose audio representations for a specific application using speech, a highly demanding field, as an example. We enhance Masked Modeling Duo (M2D), a general-purpose model, to close the performance gap with state-of-the-art (SOTA) speech models. To do so, we propose a new task, denoising distillation, to learn from fine-grained clustered features, and M2D for Speech (M2D-S), which jointly learns the denoising distillation task and M2D masked prediction task. Experimental results show that M2D-S performs comparably to or outperforms SOTA speech models on the SUPERB benchmark, demonstrating that M2D can specialize in a demanding field",
    "checked": true,
    "id": "032d17249f59232152b3b3ff8477213f786a8a93",
    "semantic_title": "masked modeling duo for speech: specializing general-purpose audio representation to speech using denoising distillation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23c_interspeech.html": {
    "title": "Improving RNN Transducer Acoustic Models for English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "In this paper we investigate several techniques for improving the performance of RNN transducer (RNNT) acoustic models for conversational speech recognition and report state-of-the-art word error rates (WERs) on the 2000-hour Switchboard dataset. We show that n-best label smoothing and length perturbation which show improved performance on the smaller 300-hour dataset are also very effective on large datasets. We further give a rigorous theoretical interpretation of the n-best label smoothing based on stochastic approximation for training RNNT under the maximum likelihood criterion. Random quantization is also introduced to improve the generalization of RNNT models. On the 2000-hour Switchboard dataset, we report a single model performance of 4.9% and 7.7% WERs on the Switchboard and CallHome portions of NIST Hub5 2000, 7.1% on NIST Hub5 2001 and 6.8% on NIST RT03, without using external LMs",
    "checked": true,
    "id": "196c307f1ba2eecd7f890d51ab8e6916c826536d",
    "semantic_title": "improving rnn transducer acoustic models for english conversational speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23_interspeech.html": {
    "title": "MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we present MixRep, a simple and effective data augmentation strategy based on mixup for low-resource ASR. MixRep interpolates the feature dimensions of hidden representations in the neural network that can be applied to both the acoustic feature input and the output of each layer, which generalizes the previous MixSpeech method. Further, we propose to combine the mixup with a regularization along the time axis of the input, which is shown as complementary. We apply MixRep to a Conformer encoder of an E2E LAS architecture trained with a joint CTC loss. We experiment on the WSJ dataset and subsets of the SWB dataset, covering reading and telephony conversational speech. Experimental results show that MixRep consistently outperforms other regularization methods for low-resource ASR. Compared to a strong SpecAugment baseline, MixRep achieves a +6.5% and a +6.7% relative WER reduction on the eval92 set and the Callhome part of the eval'2000 set",
    "checked": true,
    "id": "e5780eae634fde7c84e0a97badd49f93afc681d6",
    "semantic_title": "mixrep: hidden representation mixup for low-resource speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23o_interspeech.html": {
    "title": "Adapting Multi-Lingual ASR Models for Handling Multiple Talkers",
    "volume": "main",
    "abstract": "State-of-the-art large-scale universal speech models (USMs) show a decent automatic speech recognition (ASR) performance across multiple domains and languages. However, it remains a challenge for these models to recognize overlapped speech, which is often seen in meeting conversations. We propose an approach to adapt USMs for multi-talker ASR. We first develop an enhanced version of serialized output training to jointly perform multi-talker ASR and utterance timestamp prediction. That is, we predict the ASR hypotheses for all speakers, count the speakers, and estimate the utterance timestamps at the same time. We further introduce a lightweight adapter module to maintain the multilingual property of the USMs even when we perform the adaptation with only a single language. Experimental results obtained using the AMI and AliMeeting corpora show that our proposed approach effectively transfers the USMs to a strong multilingual multi-talker ASR model with timestamp prediction capability",
    "checked": true,
    "id": "1a8186e075325e7e475fee8d4bb9589afb6a4bfc",
    "semantic_title": "adapting multi-lingual asr models for handling multiple talkers",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23c_interspeech.html": {
    "title": "Adapter-tuning with Effective Token-dependent Representation Shift for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "The use of self-supervised pre-trained speech models has greatly improved speech tasks in low-resource settings. However, fine-tuning the entire model can be computationally expensive and not scalable for multiple tasks (e.g., personalized ASR). While recent approaches have tried to solve this issue by training adapters, they fail to match the performance of full fine-tuning models, possibly due to the challenge of task domain transferability. Our proposed method enhances the performance of vanilla adapter tuning for ASR by using a simple yet effective token-dependent bias. This approach adds a token-specific representation shift (bias) to the intermediate representations of a pre-trained model, which better maps the latent features of the frozen network to the task domain. Our approach yields better recognition results with the adapter tuning strategy and achieves the performance of a full fine-tuning model on clean LibriSpeech while maintaining its lightweight nature",
    "checked": true,
    "id": "0984970b6edb8961441c3e07217acccfc58ce1ad",
    "semantic_title": "adapter-tuning with effective token-dependent representation shift for automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23c_interspeech.html": {
    "title": "Model-Internal Slot-triggered Biasing for Domain Expansion in Neural Transducer ASR Models",
    "volume": "main",
    "abstract": "Personal rare word recognition is an important yet challenging task for end-to-end speech recognition. Contextual biasing has demonstrated success in tackling this problem. Though effective in improving rare word recognition, these mechanisms can lead to errors due to false-biasing while facing further challenges when attempting to expand them to many domains. To address these limitations, in this work we propose a neural biasing design with a streaming model-internal slot classifier, trained to categorise the domain of each word piece before it is emitted. The neural biasing module can therefore be triggered in a controlled way, permitting natural scaling to many domains while reducing false-biasing and computational cost. Experiments on diverse domain slot types of application names, communications and playlist names demonstrate the proposed architecture results in 26% to 58% relative improvements on personal rare word recognition with minimal impact (0.6% rel.) on general data",
    "checked": true,
    "id": "ed61d5f9f030623bafceeaea91ce53fd2dd13fac",
    "semantic_title": "model-internal slot-triggered biasing for domain expansion in neural transducer asr models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yao23b_interspeech.html": {
    "title": "Delay-penalized CTC Implemented Based on Finite State Transducer",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification (CTC) suffers from the latency problem when applied to streaming models. We argue that in CTC lattice, the alignments that can access more future context are preferred during training, thereby leading to higher symbol delay. In this work we propose the delay-penalized CTC which is augmented with latency penalty regularization. We devise a flexible and efficient implementation based on the differentiable Finite State Transducer (FST). Specifically, by attaching a binary attribute to CTC topology, we can locate the frames that firstly emit non-blank tokens on the resulting CTC lattice, and add the frame offsets to the log-probabilities. Experimental results demonstrate the effectiveness of our proposed delay-penalized CTC, which is able to balance the delay-accuracy trade-off. Furthermore, combining the delay-penalized transducer enables the CTC model to achieve better performance and lower latency. Our work is open-sourced and publicly available",
    "checked": true,
    "id": "6554dead720c0289dbbd04aaf81929c218034c1a",
    "semantic_title": "delay-penalized ctc implemented based on finite state transducer",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23f_interspeech.html": {
    "title": "Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation",
    "volume": "main",
    "abstract": "Mapping two modalities, speech and text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed a novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "afae99c77ad03db427e72d5f6498fa0920944b84",
    "semantic_title": "text-only domain adaptation for end-to-end speech recognition through down-sampling acoustic representation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23u_interspeech.html": {
    "title": "Knowledge Distillation Approach for Efficient Internal Language Model Estimation",
    "volume": "main",
    "abstract": "Internal language model estimation (ILME) has demonstrated its efficacy in domain adaptation for end-to-end (E2E) ASR. However, the performance improvement is achieved at the expense of computational cost, compared with conventional shallow fusion. To estimate the internal language model prior, one should run an extra forward operation on either ASR decoder or a separate density ratio (DR) language model (LM) for each decoding utterance. In this paper, we propose to employ knowledge distillation (KD) approach to realize efficient ILME for the Listen-Attend-Spell (LAS) E2E ASR model. First, we extensively explore diverse ILME and DR methods. We find that the ILM can be approximated with a DR-LM much smaller than the original ASR decoder. Furthermore, to reach the performance of ILME, we propose to employ the estimated ILM as teacher to teach a small DR-LM by KD. In this way, we achieve the best of both worlds: comparable performance to ILME and high efficiency of DR with a small DR-LM",
    "checked": true,
    "id": "63a5c7f60191dfb6e451855bdedd63424a1bd2c0",
    "semantic_title": "knowledge distillation approach for efficient internal language model estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/adhikary23_interspeech.html": {
    "title": "Language Model Personalization for Improved Touchscreen Typing",
    "volume": "main",
    "abstract": "Touchscreen keyboards rely on language modeling to auto-correct noisy typing and to offer word predictions. While language models can be pre-trained on huge amounts of text, they may fail to capture a user's unique writing style. Using a recently released email personalization dataset, we show improved performance compared to a unigram cache by adapting to a user's text via language models based on prediction by partial match (PPM) and recurrent neural networks. On simulated noisy touchscreen typing of 44 users, our best model increased keystroke savings by 9.9% relative and reduced word error rate by 36% relative compared to a static background language model",
    "checked": true,
    "id": "51f4dc7d6b82632c4226a8f0c47eddfd9cf4b78e",
    "semantic_title": "language model personalization for improved touchscreen typing",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23b_interspeech.html": {
    "title": "Blank Collapse: Compressing CTC Emission for the Faster Decoding",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use the CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper, we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher",
    "checked": true,
    "id": "6498d95d3f988e684bc6a70004decbefec655222",
    "semantic_title": "blank collapse: compressing ctc emission for the faster decoding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peyser23_interspeech.html": {
    "title": "Improving Joint Speech-Text Representations Without Alignment",
    "volume": "main",
    "abstract": "The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system",
    "checked": true,
    "id": "12a3d43d9bc97b7d0e93893b98b5ed1c54c1ced2",
    "semantic_title": "improving joint speech-text representations without alignment",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/flynn23_interspeech.html": {
    "title": "Leveraging Cross-Utterance Context For ASR Decoding",
    "volume": "main",
    "abstract": "While external language models (LMs) are often incorporated into the decoding stage of automated speech recognition systems, these models usually operate with limited context. Cross utterance information has been shown to be beneficial during second pass re-scoring, however this limits the hypothesis space based on the local information available to the first pass LM. In this work, we investigate the incorporation of long-context transformer LMs for cross-utterance decoding of acoustic models via beam search, and compare against results from n-best rescoring. Results demonstrate that beam search allows for an improved use of cross-utterance context. When evaluating on the long-format dataset AMI, results show a 0.7% and 0.3% absolute reduction on dev and test sets compared to the single-utterance setting, with improvements when including up to 500 tokens of prior context. Evaluations are also provided for Tedlium-1 with less significant improvements of around 0.1% absolute",
    "checked": true,
    "id": "98c523dc3c971b96327232056ae7e9ec003310f9",
    "semantic_title": "leveraging cross-utterance context for asr decoding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23_interspeech.html": {
    "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation",
    "volume": "main",
    "abstract": "Large-scale pre-trained language models (PLMs) have shown great potential in natural language processing tasks. Leveraging the capabilities of PLMs to enhance automatic speech recognition (ASR) systems has also emerged as a promising research direction. However, previous works may be limited by the inflexible structures of PLMs and the insufficient utilization of PLMs. To alleviate these problems, we propose the hierarchical knowledge distillation (HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer knowledge from PLMs to the ASR models, HKD employs cross-modal knowledge distillation with contrastive loss at the acoustic level and knowledge distillation with regression loss at the linguistic level. Compared with the original CIF-based model, our method achieves 15% and 9% relative error rate reduction on the AISHELL-1 and LibriSpeech datasets, respectively",
    "checked": true,
    "id": "57eb41c7b5cbffb134cdcf67e455c9c852024cbd",
    "semantic_title": "knowledge transfer from pre-trained language models to cif-based speech recognizers via hierarchical distillation",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tsunoo23_interspeech.html": {
    "title": "Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition",
    "volume": "main",
    "abstract": "Although frame-based models, such as CTC and transducers, have an affinity for streaming automatic speech recognition, their decoding uses no future knowledge, which could lead to incorrect pruning. Conversely, label-based attention encoder--decoder mitigates this issue using soft attention to the input, while it tends to overestimate labels biased towards its training domain, unlike CTC. We exploit these complementary attributes and propose to integrate the frame- and label-synchronous (F-/L-Sync) decoding alternately performed within a single beam-search scheme. F-Sync decoding leads the decoding for block-wise processing, while L-Sync decoding provides the prioritized hypotheses using look-ahead future frames within a block. We maintain the hypotheses from both decoding methods to perform effective pruning. Experiments demonstrate that the proposed search algorithm achieves lower error rates compared to the other search methods, while being robust against out-of-domain situations",
    "checked": true,
    "id": "6c2b800cd03ad064922c8596a18d784ce25d47ac",
    "semantic_title": "integration of frame- and label-synchronous beam search for streaming encoder-decoder speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23e_interspeech.html": {
    "title": "A Neural Time Alignment Module for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end trainable (E2E) automatic speech recognition (ASR) systems have low word error rates, but they do not model timings or silence by default unlike hidden Markov model (HMM)-based systems. In this paper, an extra neural aligner module is proposed for E2E ASR models, which labels the word timings in a post-processing stage. Pre-trained neural transducer and attention-based encoder-decoder models are adopted as the ASR backbones for experiments. The aligner module uses self-attention and cross-attention and takes the hidden representations from the backbone to predict the durations of each word and the possible silences. A novel loss is proposed for aligner training with the backbone frozen. Experimental results showed that when trained using the references from an existing HMM-based forced aligner, the proposed methods can make time predictions at accuracy about 95% for matched recognised words, and about 99% for utterances up to 10 s with reference text, with 200 ms tolerance",
    "checked": true,
    "id": "28810a2d8a0943541ab4e9f60642b8a977a883dc",
    "semantic_title": "a neural time alignment module for end-to-end automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23i_interspeech.html": {
    "title": "Accelerating Transducers through Adjacent Token Merging",
    "volume": "main",
    "abstract": "Recent end-to-end automatic speech recognition (ASR) systems often utilize a Transformer-based acoustic encoder that generates embedding at a high frame rate. However, this design is inefficient, particularly for long speech signals due to the quadratic computation of self-attention. To address this, we propose a new method, Adjacent Token Merging (A-ToMe), which gradually combines adjacent tokens with high similarity scores between their key values. In this way, the total time step could be reduced, and the inference of both the encoder and joint network is accelerated. Experiments on LibriSpeech show that our method can reduce 57% of tokens and improve the inference speed on GPU by 70% without any notable loss of accuracy. Additionally, we demonstrate that A-ToMe is also an effective solution to reduce tokens in long-form ASR, where the input speech consists of multiple utterances",
    "checked": true,
    "id": "66378a4a0b32a076c69dcb8586ac3639a0e951d8",
    "semantic_title": "accelerating transducers through adjacent token merging",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23_interspeech.html": {
    "title": "Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "We improve low-resource ASR by integrating the ideas of multilingual training and self-supervised learning. Concretely, we leverage an International Phonetic Alphabet (IPA) multilingual model to create frame-level pseudo labels for unlabeled speech, and use these pseudo labels to guide hidden-unit BERT (HuBERT) based speech pretraining in a phonetically-informed manner. The experiments on the Multilingual Speech (MLS) Corpus show that the proposed approach consistently outperforms the standard HuBERT on all the target languages. Moreover, on 3 of the 4 languages, comparing to the standard HuBERT, the approach performs better, meanwhile is able to save supervised training data by 1.5k hours (75%) at most. Our approach outperforms most of the state of the arts, with much less pretraining data in terms of hours and language diversity. Compared to XLSR-53 and a retraining based multilingual method, our approach performs better with full and limited finetuning data scenarios",
    "checked": true,
    "id": "b7bc0e232456aefc029bb661ef3310bbff279fda",
    "semantic_title": "language-universal phonetic representation in multilingual speech pretraining for low-resource speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23sa_interspeech.html": {
    "title": "Language-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "Multilingual speech recognition for both monolingual and code-switching speech is a challenging task. Recently, based on the Mixture of Experts (MoE), many works have made good progress in multilingual and code-switching ASR, but present huge computational complexity with the increase of supported languages. In this work, we propose a computation-efficient network named Language-Routing Mixture of Experts (LR-MoE) for multilingual and code-switching ASR. LR-MoE extracts language-specific representations through the Mixture of Language Experts (MLE), which is guided to learn by a frame-wise language routing mechanism. The weight-shared frame-level language identification (LID) network is jointly trained as the shared pre-router of each MoE layer. Experiments show that the proposed method significantly improves multilingual and code-switching speech recognition performances over baseline with comparable computational efficiency",
    "checked": true,
    "id": "0557a70130c6d10b0f93d83a33627b883d2936c5",
    "semantic_title": "language-routing mixture of experts for multilingual and code-switching speech recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23h_interspeech.html": {
    "title": "Embedding Articulatory Constraints for Low-resource Speech Recognition Based on Large Pre-trained Model",
    "volume": "main",
    "abstract": "Knowledge about phonemes and their articulatory attributes can help improve automatic speech recognition (ASR) of low-resource languages. In this study, we propose a simple and effective approach to embed prior knowledge about phonemes into end-to-end ASR based on a large pre-trained model. An articulatory attribute prediction layer is constructed by embedding articulatory constraints in layer initialization, which allows for predicting articulatory attributes without the need for explicit training. The final ASR transcript is inferred by combining the output of this layer with encoded speech features. We apply our method to finetune a pre-trained XLS-R model using Ainu and Mboshi corpora, and achieve a 12% relative improvement when target data of only 1 hour is available. This demonstrates that the approach of incorporating phonetic prior knowledge is useful when combined with a large pre-trained model",
    "checked": true,
    "id": "89fb7e5bc354e76ed55057b188a699683056716a",
    "semantic_title": "embedding articulatory constraints for low-resource speech recognition based on large pre-trained model",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23b_interspeech.html": {
    "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs",
    "checked": true,
    "id": "47ba7df38e24da9bad9266d2b58abbb2b70db6e5",
    "semantic_title": "exploration of efficient end-to-end asr using discretized input from self-supervised learning",
    "citation_count": 14,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/antonova23_interspeech.html": {
    "title": "SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings",
    "volume": "main",
    "abstract": "Contextual spelling correction models are an alternative to shallow fusion to improve automatic speech recognition (ASR) quality given user vocabulary. To deal with large user vocabularies, most of these models include candidate retrieval mechanisms, usually based on minimum edit distance between fragments of ASR hypothesis and user phrases. However, the edit-distance approach is slow, non-trainable, and may have low recall as it relies only on common letters. We propose: 1) a novel algorithm for candidate retrieval, based on misspelled n-gram mappings, which gives up to 90% recall with just the top 10 candidates on Spoken Wikipedia; 2) a non-autoregressive neural model based on BERT architecture, where the initial transcript and ten candidates are combined into one input. The experiments on Spoken Wikipedia show 21.4% word error rate improvement compared to a baseline ASR system",
    "checked": true,
    "id": "843cfd1c36be2c0fed94f62795f9df9c5bab5a9c",
    "semantic_title": "spellmapper: a non-autoregressive neural spellchecker for asr customization with candidate retrieval based on n-gram mappings",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bijwadia23_interspeech.html": {
    "title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models",
    "volume": "main",
    "abstract": "Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall",
    "checked": true,
    "id": "c86bd94782bdfa25396f7bb5c108e522138f3192",
    "semantic_title": "text injection for capitalization and turn-taking prediction in speech models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gitman23_interspeech.html": {
    "title": "Confidence-based Ensembles of End-to-End Speech Recognition Models",
    "volume": "main",
    "abstract": "The number of end-to-end speech recognition models grows every year. These models are often adapted to new domains or languages resulting in a proliferation of expert systems that achieve great results on target data, while generally showing inferior performance outside of their domain of expertise. We explore combination of such experts via confidence-based ensembles: ensembles of models where only the output of the most-confident model is used. We assume that models' target data is not available except for a small validation set. We demonstrate effectiveness of our approach with two applications. First, we show that a confidence-based ensemble of 5 monolingual models outperforms a system where model selection is performed via a dedicated language identification block. Second, we demonstrate that it is possible to combine base and adapted models to achieve strong results on both original and target data. We validate all our results on multiple datasets and model architectures",
    "checked": true,
    "id": "2f61008b996599d10fc0f091415215378c0decc5",
    "semantic_title": "confidence-based ensembles of end-to-end speech recognition models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chi23_interspeech.html": {
    "title": "Unsupervised Code-switched Text Generation from Parallel Text",
    "volume": "main",
    "abstract": "There has been great interest in developing automatic speech recognition (ASR) systems that can handle code-switched (CS) speech to meet the needs of a growing bilingual population. However, existing datasets are limited in size. It is expensive and difficult to collect real transcribed spoken CS data due to the challenges of finding and identifying CS data in the wild. As a result, many attempts have been made to generate synthetic CS data. Existing methods either require the existence of CS data during training, or are driven by linguistic knowledge. We introduce a novel approach of forcing a multilingual MT system that was trained on non-CS data to generate CS translations. Comparing against two prior methods, we show that simply leveraging the shared representations of two languages (Mandarin and English) yields better CS text generation and, ultimately, better CS ASR",
    "checked": true,
    "id": "3c96fe235783a42a2061f7010da756531e2ba5b8",
    "semantic_title": "unsupervised code-switched text generation from parallel text",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23b_interspeech.html": {
    "title": "A Binary Keyword Spotting System with Error-Diffusion Based Feature Binarization",
    "volume": "main",
    "abstract": "Binary-neural-network based keyword spotting (KWS) for resource-constrained devices has gained much attention in recent years. Although several works proved their success, a fully binary KWS system is yet to come, considering high-precision speech feature maps are still required for satisfying accuracy. Such precision mismatch results in non-binary activation layers, thus leading to extra computational costs. In this paper, we present an extremely compact KWS system using a binary neural network and error-diffusion binarized speech features. The system eliminates all high-precision multiplications and requires only hardware-friendly bit-wise operations and additions for inference. Experiments on the Google speech commands show that our binary KWS system yields 98.54% accuracy on a 1-keyword task and 95.05% on a 2-keyword task, outperforming 8-bit KWS systems of bigger size. The result proves the feasibility of a fully binary KWS system and can be inspiring for hardware implementations",
    "checked": true,
    "id": "c908f8141cf59b1bdcba92f93ee3ce783988cac2",
    "semantic_title": "a binary keyword spotting system with error-diffusion based feature binarization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23b_interspeech.html": {
    "title": "Language-universal Phonetic Encoder for Low-resource Speech Recognition",
    "volume": "main",
    "abstract": "Multilingual training is effective in improving low-resource ASR, which may partially be explained by phonetic representation sharing between languages. In end-to-end (E2E) ASR systems, graphemes are often used as basic modeling units, however graphemes may not be ideal for multilingual phonetic sharing. In this paper, we leverage International Phonetic Alphabet (IPA) based language-universal phonetic model to improve low-resource ASR performances, for the first time within the attention encoder-decoder architecture. We propose an adaptation method on the phonetic IPA model to further improve the proposed approach on extreme low-resource languages. Experiments carried out on the open-source MLS corpus and our internal databases show our approach outperforms baseline monolingual models and most state-of-the-art works. Our main approach and adaptation are effective on extremely low-resource languages, even within domain- and language-mismatched scenarios",
    "checked": true,
    "id": "fb7d5882e98b9ea3b0b45711379029928b24cfd0",
    "semantic_title": "language-universal phonetic encoder for low-resource speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23g_interspeech.html": {
    "title": "A Lexical-aware Non-autoregressive Transformer-based ASR Model",
    "volume": "main",
    "abstract": "Non-autoregressive automatic speech recognition (ASR) has become a mainstream of ASR modeling because of its fast decoding speed and satisfactory result. To further boost the performance, relaxing the conditional independence assumption and cascading large-scaled pre-trained models are two active research directions. In addition to these strategies, we propose a lexical-aware non-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of an acoustic encoder, a speech-text shared encoder, and a speech-text shared decoder. The acoustic encoder is used to process the input speech features as usual, and the speech-text shared encoder and decoder are designed to train speech and text data simultaneously. By doing so, LA-NAT aims to make the ASR model aware of lexical information, so the resulting model is expected to achieve better results by leveraging the learned linguistic knowledge",
    "checked": true,
    "id": "25d825e6439e7c9f5619262ad551047a2b833ba6",
    "semantic_title": "a lexical-aware non-autoregressive transformer-based asr model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vanvuren23_interspeech.html": {
    "title": "Improving Under-Resourced Code-Switched Speech Recognition: Large Pre-trained Models or Architectural Interventions",
    "volume": "main",
    "abstract": "We present three approaches to improve language modelling of under-resourced code-switched speech. First, we challenge the practice of fine-tuning large pre-trained language models on small datasets. Secondly, we investigate the advantages of sub-word encodings for our multilingual code-switched speech. Thirdly, we propose an architectural innovation to the RNN language model that is specifically designed for code-switched text. We show a clear reduction in absolute word error rate of 0.17% for the adapted LSTM language model compared to M-BERT when employed in n-best rescoring experiments. Further, the LSTM models afford a seven-fold reduction in total number of parameters and reduces runtime during rescoring 100-fold. Contrary to recent research trends, our LSTM models do not outperform the word-level models when using sub-word vocabularies. Finally, the new architectural mechanism applied to the LSTM improves language prediction for a span of several words following a code-switch",
    "checked": true,
    "id": "2ddca6863b3291254546ff9db69a3c3d629610ee",
    "semantic_title": "improving under-resourced code-switched speech recognition: large pre-trained models or architectural interventions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bellegarda23_interspeech.html": {
    "title": "Pragmatic Pertinence: A Learnable Confidence Metric to Assess the Subjective Quality of LM-Generated Text",
    "volume": "main",
    "abstract": "To be perceived as trustworthy, artificially generated text must be sufficiently congruent with the available discourse history. Pre-trained language models (LMs) operating in generative mode are capable of predicting locally coherent phrases, but those do not always reflect salient syntactic, semantic, or pragmatic facets of prior content. This paper introduces a learnable evaluation metric to assess the pragmatic pertinence of LM-generated text for a given history. Pertinence is closely aligned with qualitative human judgments of acceptability, thereby emerging as a blend of sensibleness and specificity. Experiments conducted across different domains using different learning architectures show that this approach circumvents the issue of multiple valid ground-truths, while providing a reliable quantitative ranking of generated text completion candidates in context. Pertinence scoring could thus prove useful for the detection of hallucinations",
    "checked": true,
    "id": "71427159f87bd9b2534966b3f32dab73b869c589",
    "semantic_title": "pragmatic pertinence: a learnable confidence metric to assess the subjective quality of lm-generated text",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ea_interspeech.html": {
    "title": "ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition",
    "volume": "main",
    "abstract": "In Speech Emotion Recognition (SER), textual data is often used alongside audio signals to address their inherent variability. However, the reliance on human annotated text in most research hinders the development of practical SER systems. To overcome this challenge, we investigate how Automatic Speech Recognition (ASR) performs on emotional speech by analyzing the ASR performance on emotion corpora and examining the distribution of word errors and confidence scores in ASR transcripts to gain insight into how emotion affects ASR. We utilize four ASR systems, namely Kaldi ASR, wav2vec2, Conformer, and Whisper, and three corpora: IEMOCAP, MOSI, and MELD to ensure generalizability. Additionally, we conduct text-based SER on ASR transcripts with increasing word error rates to investigate how ASR affects SER. The objective of this study is to uncover the relationship and mutual impact of ASR and SER, in order to facilitate ASR adaptation to emotional speech and the use of SER in real world",
    "checked": true,
    "id": "c40e3331d374cf18e96878fafe6477b5030519aa",
    "semantic_title": "asr and emotional speech: a word-level investigation of the mutual impact of speech and emotion recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharma23_interspeech.html": {
    "title": "BASS: Block-wise Adaptation for Speech Summarization",
    "volume": "main",
    "abstract": "End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to computing restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline",
    "checked": true,
    "id": "3bd320ddb25886417ae90011b00f13f5d558097b",
    "semantic_title": "bass: block-wise adaptation for speech summarization",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shekar23_interspeech.html": {
    "title": "Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data: Fearless Steps Apollo-11 Audio Corpus",
    "volume": "main",
    "abstract": "Speaker tracking in spontaneous naturalistic data continues to be a major research challenge, especially for short turn-taking communications. The NASA Apollo-11 space mission brought astronauts to the moon and back, where team based voice communications were captured. Building robust speaker classification models for this corpus has significant challenges due to variability of speaker turns, imbalanced speaker classes, and time-varying background noise/distortions. This study proposes a novel approach for speaker classification and tracking, utilizing a graph attention network framework that builds upon pretrained speaker embeddings. The model's robustness is evaluated on a number of speakers (10-140), achieving classification accuracy of 90.78% for 10 speakers, and 79.86% for 140 speakers. Furthermore, a secondary investigation focused on tracking speakers-of-interest(SoI) during mission critical phases, essentially serves as a lasting tribute to the 'Heroes Behind the Heroes'",
    "checked": true,
    "id": "90f1640e143cfd5a822e0d0f1cc662b2108e422f",
    "semantic_title": "speaker tracking using graph attention networks with varying duration utterances across multi-channel naturalistic data: fearless steps apollo-11 audio corpus",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yan23_interspeech.html": {
    "title": "Combining language corpora in a Japanese electromagnetic articulography database for acoustic-to-articulatory inversion",
    "volume": "main",
    "abstract": "This paper presents an electromagnetic articulography database of Japanese sentences. The database includes aligned acoustics and articulatory data from seven males and three females, with a total of five recorded hours. The database is now in preparation for public release to further research in areas of acoustic-to-articulatory inversion, brain-machine interface communication systems, artificial speech synthesis, and dialect recognition. Moreover, based on this database we established an acoustic-to-articulatory inversion system using a deep, bidirectional, long short-term memory recurrent neural network structure. The results showed that, for the Japanese language, adding English corpora to the training was not beneficial for this speaker-independent model",
    "checked": true,
    "id": "ad0ec08e019b9ab8e2174a6184ddcfc3588a86d8",
    "semantic_title": "combining language corpora in a japanese electromagnetic articulography database for acoustic-to-articulatory inversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23g_interspeech.html": {
    "title": "A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition",
    "volume": "main",
    "abstract": "Multi-modal emotion recognition (MER) is an emerging research field in human-computer interactions. However, previous studies have explored several fusion methods to deal with the asynchronism and the heterogeneity of multimodal data but they mostly neglect the importance of discriminative unimodal information resulting in the ignorance of independence of uni-modality. Furthermore, the complementarity among different fusion strategies is seldom taken in consideration. To address these limitations, we propose a modality-collaborative fusion network (MCFN) consisting of three main components: a dual attention-based intra-modal learning module which is devoted to build the initial embedding spaces, a modality-collaborative learning approach is to reconcile the emotional information across modalities, and a two-stage fusion strategy to integrate multimodal features which are improved by a mutual adjustment approach. The proposed framework outperforms the state-of-the-art methods in overall experiments on two well-known public datasets. Our model will be available at https://github.com/zxiaohen/ Speech-emotion-recognition-MCFN",
    "checked": true,
    "id": "d6d2f6d901e83a2699b317b9a54c79658a1d064d",
    "semantic_title": "a dual attention-based modality-collaborative fusion network for emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chivriga23_interspeech.html": {
    "title": "Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm",
    "volume": "main",
    "abstract": "Large models (e.g., GPT-3, CLIP, DALL-E) show remarkable few-shot and zero-shot capabilities when trained on hundreds of millions of samples. Despite this trend, no publicly available synchronized music audio and lyrics dataset of sufficient scale exists, nor does a reliable evaluation benchmark to assess a model's performance. To address this issue, we build and release MusicLyric, a large public dataset with over 320k audio sequences and lyrics pairs for a total duration of 1,200 hours based on a collection of over 32,000 songs. The generation process is based on the teacher-student paradigm where the student seeks to outclass the teacher with more data available using the newly generated pseudo-alignments. The method is efficient and straightforward with at least 3 iterations needed to create high-quality data that can be scaled to a hundred thousand samples. We make our dataset, toolkit, and pre-trained models open-source",
    "checked": true,
    "id": "1cff88a49880a269d7017329fbd842c124183ef7",
    "semantic_title": "large dataset generation of synchronized music audio and lyrics at scale using teacher-student paradigm",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/banerjee23_interspeech.html": {
    "title": "Enc-Dec RNN Acoustic Word Embeddings learned via Pairwise Prediction",
    "volume": "main",
    "abstract": "Learning discriminative Acoustic Word Embeddings (AWEs) summarising variable length spoken word segments brings efficiency in speech retrieval tasks, notably, Query-by-Example (QbE) Speech or Spoken Term Detection (STD). In this paper, we add on to RNN based approaches for generating acoustic word embeddings. The model is trained in an encoder-decoder fashion on pairs of similar word segments by optimizing a pairwise self-supervised loss where the targets are generated offline via clustering. The pairs may be generated with word boundaries (weak supervision) or via augmentation of unlabelled word segments (no supervision). Experiments with word discrimination task on TIMIT and LibriSpeech show state of the art performance of the proposed approach outperforming popular RNN AWE approaches in both weakly supervised and unsupervised settings. The AWEs generated by our model generalise well to OOV words. On STD tasks performed on TIMIT, the proposed approach provides speed advantages",
    "checked": true,
    "id": "d6eeb0a6d2dc6a130fadf043c27ae2b4ecdfa27d",
    "semantic_title": "enc-dec rnn acoustic word embeddings learned via pairwise prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kotey23_interspeech.html": {
    "title": "Query Based Acoustic Summarization for Podcasts",
    "volume": "main",
    "abstract": "Podcasts are a rich storytelling medium of long diverse conversations. Typically, listeners preview an episode through an audio clip, before deciding to consume the content. An automatic system that produces promotional clips, by supporting acoustic queries would greatly benefit podcasters. Previous text based methods do not use the acoustic signal directly or incorporate acoustic defined queries. Therefore, we propose a query based summarization approach, to produce audio clip summaries from podcast data. Leveraging unsupervised clustering methods, we apply our framework to the Spotify podcasts dataset. Audio signals are transformed into acoustic word embeddings, along with a pre-selected candidate query. We initiate the cluster centroids with the query vector and obtain the final snippets by computing a global and local similarity score. Additionally, we apply our framework to the AMI meeting dataset and demonstrate how audio can successfully be utilized to perform summarization",
    "checked": true,
    "id": "7ae9630ae9ac1b78d1ee18107a26c8d6d4351174",
    "semantic_title": "query based acoustic summarization for podcasts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23f_interspeech.html": {
    "title": "Spot Keywords From Very Noisy and Mixed Speech",
    "volume": "main",
    "abstract": "Most existing keyword spotting research focuses on conditions with slight or moderate noise. In this paper, we try to tackle a more challenging task: detecting keywords buried under strong interfering speech (10 times higher than the keyword in amplitude), and even worse, mixed with other keywords. We propose a novel Mix Training (MT) strategy that encourages the model to discover low-energy keywords from noisy and mixed speech. Experiments were conducted with a vanilla CNN and two EfficientNet (B0/B2) architectures. The results evaluated with the Google Speech Command dataset demonstrated that the proposed mix training approach is highly effective and outperforms standard data augmentation and mixup training",
    "checked": true,
    "id": "e366833a629c8faa0a01ce802e0cadfa977302fd",
    "semantic_title": "spot keywords from very noisy and mixed speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nayem23_interspeech.html": {
    "title": "Knowledge Distillation on Joint Task End-to-End Speech Translation",
    "volume": "main",
    "abstract": "An End-to-End Speech Translation (E2E-ST) model takes input audio in one language and directly produces output text in another language. The model requires to learn both speech-to-text modality conversion and translation tasks, which demands a large architecture for effective learning of this joint task. Yet, to the best of our knowledge, we are the first to optimize compression of E2E-ST models. In this work, we explore knowledge distillation for a cross-modality joint-task E2E-ST system from 3 dimensions: 1) student architecture and weight initialization scheme, 2) importance of loss terms associated with different tasks and data modalities, 3) knowledge distillation training scheme customized for the multi-task/module model. Comparing with the full size model, our compressed model's encoder and decoder size are 50% smaller, while it retains 90% and > 95% performance on speech translation task and machine translation task respectively on MUST-C en→de testset",
    "checked": true,
    "id": "5011e031d2d4cc8ab83424256e53444d31d54ed4",
    "semantic_title": "knowledge distillation on joint task end-to-end speech translation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23d_interspeech.html": {
    "title": "Investigating Pre-trained Audio Encoders in the Low-Resource Condition",
    "volume": "main",
    "abstract": "Pre-trained speech encoders have been central to pushing state-of-the-art results across various speech understanding and generation tasks. Nonetheless, the capabilities of these encoders in low-resource settings are yet to be thoroughly explored. To address this, we conduct a comprehensive set of experiments using a representative set of 3 state-of-the-art encoders (Wav2vec2, WavLM, Whisper) in the low-resource setting across 7 speech understanding and generation tasks. We provide various quantitative and qualitative analyses on task performance, convergence speed, and representational properties of the encoders. We observe a connection between the pre-training protocols of these encoders and the way in which they capture information in their internal layers. In particular, we observe the Whisper encoder exhibits the greatest low-resource capabilities on content-driven tasks in terms of performance and convergence speed",
    "checked": true,
    "id": "9ee4bd748ecbc4f4b191189bdae48729d58c7fa9",
    "semantic_title": "investigating pre-trained audio encoders in the low-resource condition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23g_interspeech.html": {
    "title": "Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) is a task that aims to extract semantic information from spoken utterances. Previous research has made progress in end-to-end SLU by using paired speech-text data, such as pre-trained Automatic Speech Recognition (ASR) models or paired text as intermediate targets. However, acquiring paired transcripts is expensive and impractical for unwritten languages. On the other hand, Textless SLU extracts semantic information from speech without utilizing paired transcripts. However, the absence of intermediate targets and training guidance for textless SLU often leads to suboptimal performance. In this work, inspired by the content-disentangled discrete units from self-supervised speech models, we proposed to use discrete units as intermediate guidance to improve textless SLU performance. Our method surpasses the baseline method on five SLU benchmark corpora. Additionally, we find that unit guidance facilitates few-shot learning and enhances noise robustness",
    "checked": true,
    "id": "de3b0f07daa9c717ddf20247eb05a2d373ba4ec7",
    "semantic_title": "improving textless spoken language understanding with discrete units as intermediate target",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23m_interspeech.html": {
    "title": "Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test",
    "volume": "main",
    "abstract": "Automatic speech recognition systems based on deep learning are mainly trained under empirical risk minimization (ERM). Since ERM utilizes the averaged performance on the data samples regardless of a group such as healthy or dysarthric speakers, ASR systems are unaware of the performance disparities across the groups. This results in biased ASR systems whose performance differences among groups are severe. In this study, we aim to improve the ASR system in terms of group robustness for dysarthric speakers. To achieve our goal, we present a novel approach, sample reweighting with sample affinity test (Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given data sample and then mitigates the bias by debiasing helpfulness-based sample reweighting. Experimental results demonstrate that Re-SAT contributes to improved ASR performance on dysarthric speech without performance degradation on healthy speech",
    "checked": true,
    "id": "1b94e2e75d6be5884efbb725b540b50c7b97f4cb",
    "semantic_title": "debiased automatic speech recognition for dysarthric speech via sample reweighting with sample affinity test",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/papadimitriou23_interspeech.html": {
    "title": "Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a novel Transformer-based approach for continuous sign language recognition (CSLR) from videos, aiming to address the shortcomings of traditional Transformers in learning local semantic context of SL. Specifically, the proposed relies on two distinct components: (a) a window-based RNN module to capture local temporal context and (b) a Transformer encoder, enhanced with local modeling via Gaussian bias and relative position information, as well as with global structure modeling through multi-head attention. To further improve model performance, we design a multimodal framework that applies the proposed to both appearance and motion signing streams, aligning their posteriors through a guiding CTC technique. Further, we achieve visual feature and gloss sequence alignment by incorporating a knowledge distillation loss. Experimental evaluation on two popular German CSLR datasets, demonstrates the superiority of our model",
    "checked": true,
    "id": "6ead5cf59020f89638a0b2fc5d9cd4821d060897",
    "semantic_title": "multimodal locally enhanced transformer for continuous sign language recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gonzalezmachorro23_interspeech.html": {
    "title": "Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features",
    "volume": "main",
    "abstract": "Multiple sclerosis (MS) is a neuroinflammatory disease that affects millions of people worldwide. Since dysarthria is prominent in people with MS (pwMS), this paper aims to identify acoustic features that differ between people with MS and healthy controls (HC). Additionally, we develop automatic classification methods to distinguish between pwMS and HC. In this work, we present a new dataset of a German-speaking cohort which contains 39 patients with low disability of relapsing MS and 16 HC. Findings suggest that certain interpretable speech features could be useful in diagnosing MS, and that machine learning methods could potentially support fast and unobtrusive screening in clinical practice. The study emphasises the importance of analysing free speech compared to read speech",
    "checked": true,
    "id": "1c6e47c14c3fca53e2e6f2a61a6935043eac6634",
    "semantic_title": "towards supporting an early diagnosis of multiple sclerosis using vocal features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rathod23_interspeech.html": {
    "title": "Whisper Features for Dysarthric Severity-Level Classification",
    "volume": "main",
    "abstract": "Dysarthria is a speech disorder caused by improper coordination between the brain and the muscles that produce intelligible speech. Accurately diagnosing the severity of dysarthria is critical for determining the appropriate treatment and directing speech to suitable Automatic Speech Recognition systems. Recently, various methods have been employed to investigate the classification of dysarthria severity-levels using advanced features, including STFT and MFCC. This study proposes utilizing Web-scale Supervised Pretraining for Speech Recognition (WSPSR), also known as Whisper, encoder module for dysarthric severity-level classification using transfer learning approach. Whisper model is an advanced machine learning model used for speech recognition, which is trained on a large scale of 680,000 hours of labeled audio data. The proposed approach demonstrated a high accuracy rate of 98.02%, surpassing the accuracies achieved by MFCC (95.2%) and LFCC (96.05%)",
    "checked": true,
    "id": "fd01dc873d3593507879bc3cbf9100505565cef1",
    "semantic_title": "whisper features for dysarthric severity-level classification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tang23b_interspeech.html": {
    "title": "A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning",
    "volume": "main",
    "abstract": "Aphasia is a language disorder that affects the speaking ability of millions of patients. This paper presents a new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset. Specifically, we introduce two multi-task learning methods based on the CTC/Attention architecture to perform both tasks simultaneously. Our system achieves state-of-the-art speaker-level detection accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia patients. In addition, we demonstrate the generalizability of our approach by applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing",
    "checked": true,
    "id": "4d35540aaf993c8fa7e1fa5fc6a990f1eb830263",
    "semantic_title": "a new benchmark of aphasia speech recognition and detection based on e-branchformer and multi-task learning",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yue23_interspeech.html": {
    "title": "Dysarthric Speech Recognition, Detection and Classification using Raw Phase and Magnitude Spectra",
    "volume": "main",
    "abstract": "In this paper, we explore the effectiveness of deploying the raw phase and magnitude spectra for dysarthric speech recognition, detection and classification. In particular, we scrutinise the usefulness of various raw phase-based representations along with their combinations with the raw magnitude spectrum and filterbank features. We employed single and multi-stream architectures consisting of a cascade of convolutional, recurrent and fully-connected layers for acoustic modelling. Furthermore, we investigate various configurations and fusion schemes as well as their training dynamics. In addition, the accuracies of the raw phase and magnitude based systems in the detection and classification tasks are studied and discussed. We report the performance on the UASpeech and TORGO dysarthric speech databases and for different severity levels. Our best system achieved WERs of 31.2% and 9.1% for dysarthric and typical speech on TORGO and 30.2% on UASpeech, respectively",
    "checked": true,
    "id": "6c4677fc175694ceaeda613f22ea315b5a17693d",
    "semantic_title": "dysarthric speech recognition, detection and classification using raw phase and magnitude spectra",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bayerl23_interspeech.html": {
    "title": "A Stutter Seldom Comes Alone – Cross-Corpus Stuttering Detection as a Multi-label Problem",
    "volume": "main",
    "abstract": "Most stuttering detection and classification research has viewed stuttering as a multi-class classification problem or a binary detection task for each dysfluency type; however, this does not match the nature of stuttering, in which one dysfluency seldom comes alone but rather co-occurs with others. This paper explores multi-language and cross-corpus end-to-end stuttering detection as a multi-label problem using a modified wav2vec 2.0 system with an attention-based classification head and multi-task learning. We evaluate the method using combinations of three datasets containing English and German stuttered speech, one containing speech modified by fluency shaping. The experimental results and an error analysis show that multi-label stuttering detection systems trained on cross-corpus and multi-language data achieve competitive results but performance on samples with multiple labels stays below overall detection results",
    "checked": false,
    "id": "ac2ca47a9c0c547b4cedb4d6a7c39e8616823bbc",
    "semantic_title": "a stutter seldom comes alone - cross-corpus stuttering detection as a multi-label problem",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhattacharjee23_interspeech.html": {
    "title": "Transfer Learning to Aid Dysarthria Severity Classification for Patients with Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "A major challenge involved in automatic dysarthria severity classification for patients with Amyotrophic Lateral Sclerosis (ALS) is the difficulty to build a speech corpus which is large enough to train accurate and generalizable classifiers. To overcome this constraint, we employ transfer learning approaches, specifically, fine-tuning from an auxiliary task and multi-task learning. Input feature reconstruction and gender classification, on the same ALS speech dataset or other healthy speech corpora, are explored as the auxiliary tasks. We use temporal statistics of mel-frequency cepstral coefficients as the features and dense neural networks for performing the primary and auxiliary tasks. Experiments suggest that transfer learning aids severity classification with up to 11.03% absolute increase in the average classification accuracy as compared to direct single task learning. The improvement is attributed mainly to better classification of the mild class than severe/normal classes",
    "checked": true,
    "id": "bcbe04bc505e5bfe3d10ea30b40834361718d268",
    "semantic_title": "transfer learning to aid dysarthria severity classification for patients with amyotrophic lateral sclerosis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23qa_interspeech.html": {
    "title": "DuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach with Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "We present a novel typical-to-atypical voice conversion approach (DuTa-VC), which (i) can be trained with nonparallel data (ii) first introduces diffusion probabilistic model (iii) preserves the target speaker identity (iv) is aware of the phoneme duration of the target speaker. DuTa-VC consists of three parts: an encoder transforms the source mel-spectrogram into a duration-modified speaker-independent mel-spectrogram, a decoder performs the reverse diffusion to generate the target mel-spectrogram, and a vocoder is applied to reconstruct the waveform. Objective evaluations conducted on the UASpeech show that DuTa-VC is able to capture severity characteristics of dysarthric speech, reserves speaker identity, and significantly improves dysarthric speech recognition as a data augmentation. Subjective evaluations by two expert speech pathologists validate that DuTa-VC can preserve the severity and type of dysarthria of the target speakers in the synthesized speech",
    "checked": true,
    "id": "bf6339e920466f2dc7dc0da5edde5b3187cf9d0d",
    "semantic_title": "duta-vc: a duration-aware typical-to-atypical voice conversion approach with diffusion probabilistic model",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hedeshy23_interspeech.html": {
    "title": "CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice",
    "volume": "main",
    "abstract": "Non-verbal voice expressions (NVVEs) have been adopted as a means of human-computer interaction in research studies. However, exploring non-verbal voice-based interactions has been constrained by the limited availability of suitable training data and computational methods for classifying such expressions, leading to a focus on simple binary inputs. We address this issue with a new dataset containing 950 audio samples comprising 6 classes of voice expressions. The data were collected from 42 speakers who donated voice recordings. The classifier was trained on the data using features derived from mel-spectrograms. Furthermore, we studied the effectiveness of data augmentation and improved over the baseline model accuracy significantly with a test accuracy of 96.6% in a 5-fold cross-validation. We have made CNVVE publicly accessible in the hope that it will serve as a benchmark for future research",
    "checked": true,
    "id": "a820d95886178b82ec595146d46f2e98acf75f1e",
    "semantic_title": "cnvve: dataset and benchmark for classifying non-verbal voice",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baali23_interspeech.html": {
    "title": "Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based Augmentation",
    "volume": "main",
    "abstract": "Despite major advancements in Automatic Speech Recognition (ASR), the state-of-the-art ASR systems struggle to deal with impaired speech even with high-resource languages. In Arabic, this challenge gets amplified, with added complexities in collecting data from dysarthric speakers. In this paper, we aim to improve the performance of Arabic dysarthric automatic speech recognition through a multi-stage augmentation approach. To this effect, we first propose a signal-based approach to generate dysarthric Arabic speech from healthy Arabic speech by modifying its speed and tempo. We also propose a second stage Parallel Wave Generative (PWG) adversarial model that is trained on an English dysarthric dataset to capture language-independant dysarthric speech patterns and further augment the signal-adjusted speech samples. Furthermore, we propose a fine-tuning and text-correction strategies for Arabic Conformer at different dysarthric speech severity levels. Our fine-tuned Conformer achieved 18% Word Error Rate (WER) and 17.2% Character Error Rate (CER) on synthetically generated dysarthric speech from the Arabic common voice speech dataset. This shows significant WER improvement of 81.8% compared to the baseline model trained solely on healthy data. We perform further validation on real English dysarthric speech showing a WER improvement of 124% compared to the baseline trained only on healthy English LJSpeech dataset",
    "checked": true,
    "id": "c175253d07a07e62815d9d902da8b92adaf1ab31",
    "semantic_title": "arabic dysarthric speech recognition using adversarial and signal-based augmentation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kouzelis23_interspeech.html": {
    "title": "Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling",
    "volume": "main",
    "abstract": "The study of speech disorders can benefit greatly from time-aligned data. However, audio-text mismatches in disfluent speech cause rapid performance degradation for modern speech aligners, hindering the use of automatic approaches. In this work, we propose a simple and effective modification of alignment graph construction of CTC-based models using Weighted Finite State Transducers. The proposed weakly-supervised approach alleviates the need for verbatim transcription of speech disfluencies for forced alignment. During the graph construction, we allow the modeling of common speech disfluencies, i.e. repetitions and omissions. Further, we show that by assessing the degree of audio-text mismatch through the use of Oracle Error Rate, our method can be effectively used in the wild. Our evaluation on a corrupted version of the TIMIT test set and the UCLASS dataset shows significant improvements, particularly for recall, achieving a 23-25% relative improvement over our baselines",
    "checked": true,
    "id": "c03d509344651cc7334693ab419bd36ec7d490f2",
    "semantic_title": "weakly-supervised forced alignment of disfluent speech using phoneme-level modeling",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/novotny23_interspeech.html": {
    "title": "Glottal source analysis of voice deficits in basal ganglia dysfunction: evidence from de novo Parkinson's disease and Huntington's disease",
    "volume": "main",
    "abstract": "Dysphonia is a common speech disruption in people with Parkinson's (PD) and Huntington's (HD). Though the glottal source analysis (GSS) yielded promising results in PD, no study analyzed utility of the GSS in HD. In addition, the potential GSS sex-dependency remains unknown. This study examines sustained vowel phonations provided by 40 PD, 40 HD and 40 age- and sex-matched healthy participants using six GSS features including normalized amplitude quotient, quasi-open quotient, magnitude difference of first two spectral peaks, harmonic richness factor, maximum dispersion quotient (MDQ), and peak slope. Our results showed significant differences in HD men and women compared to the healthy counterpart, suggesting breathiness (p<0.01), tension (p<0.001), and decreased timbre (p<0.01) in HD. Reported sex-related differences highlighted the sensitivity of the GSS towards the speaker's sex. The correlation analysis revealed significant relationship between disease severity and MDQ in HD men",
    "checked": true,
    "id": "449d9ef13b24b5df16435969745cd8cb4480f6e5",
    "semantic_title": "glottal source analysis of voice deficits in basal ganglia dysfunction: evidence from de novo parkinson's disease and huntington's disease",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mun23b_interspeech.html": {
    "title": "An Analysis of Glottal Features of Chronic Kidney Disease Speech and Its Application to CKD Detection",
    "volume": "main",
    "abstract": "Chronic kidney disease (CKD) causes a continuous decline in kidney function and structural damage to the kidneys. The speech characteristics of CKD speakers will be different from those of non-CKD speakers because the typical characteristics of CKD, which are impairment of respiratory and laryngeal muscles, can affect respiration, the primary source of speech. In this paper, we identify the glottal characteristics of CKD speech and then investigate whether CKD can be automatically detected using the glottal features. Statistical analysis shows significant differences between groups in glottal source features, representing the breathy characteristic of CKD speech. Through the classification experiment, we compare the performance of solely using voice quality features (baseline) against additional glottal and spectral features. When glottal source features and voice quality features are used together, an F1-score of 0.88 with a 76% relative increase compared to the baseline is obtained",
    "checked": true,
    "id": "9729f24f79bb5ccb2a4ededa68fdfc3aef1fc505",
    "semantic_title": "an analysis of glottal features of chronic kidney disease speech and its application to ckd detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/belagali23_interspeech.html": {
    "title": "Weakly supervised glottis segmentation in high-speed videoendoscopy using bounding box labels",
    "volume": "main",
    "abstract": "Analysis of vocal fold vibration in high-speed videoendoscopy can aid in the assessment of voice disorders. Glottis segmentation is a preliminary step of this analysis. Previous deep learning approaches have focused on fully supervised learning methods for glottis segmentation which require pixel-level annotation. Collection of pixel-level annotated data is time consuming and tedious. To overcome this challenge, in this work, we explore the use of bounding box labels for weakly supervised glottis segmentation. As such, bounding box labels are relatively easier to annotate. The proposed method uses multiple instance learning to leverage bounding box labels in the form of bag labels. The method outperforms the baseline method (trained with bounding box as mask) by 0.20 in terms of dice score, and matches the performance of fully supervised version after fine-tuning",
    "checked": true,
    "id": "a8ace6742cdf9bc3e5dfbc87e6edf9dedb25fb20",
    "semantic_title": "weakly supervised glottis segmentation in high-speed videoendoscopy using bounding box labels",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23k_interspeech.html": {
    "title": "An Efficient and Noise-Robust Audiovisual Encoder for Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "Boosted by self-supervised learning (SSL) on large amounts of unlabeled data, computationally demanding transformer-based audiovisual ASR (AV-ASR) achieves state-of-the-art performance. In this work, we are the first to propose teacher-student model distillation for an efficient and noise-robust AV encoder for AV-ASR. First, we compare two options for the teacher, a non-task-specific and a task-specific one. Second, we investigate the design and the components in the student neural network. Third, we explore loss function choices during distillation. By distillation with a simplified loss function, the final efficient conformer-based student has 69% fewer parameters and 23% less computational power than the teacher, but excels the baseline student with a WER of 4.6% (11.4%) in clean condition, and with 20.2% (35.7%) in 0dB babble noise. On average over noise types in 0dB SNR, our proposed student even achieves more than 50% relative WER reduction compared to the baseline student",
    "checked": true,
    "id": "47b16dd124960faa7d1529d0a8063f3c7995fde8",
    "semantic_title": "an efficient and noise-robust audiovisual encoder for audiovisual speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23b_interspeech.html": {
    "title": "A Novel Self-training Approach for Low-resource Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a self-training approach for automatic speech recognition (ASR) for low-resource settings. While self-training approaches have been extensively developed and evaluated for high-resource languages such as English, their applications to low-resource languages like Punjabi have been limited, despite the language being spoken by millions globally. The scarcity of annotated data has hindered the development of accurate ASR systems, especially for low-resource languages (e.g., Punjabi and Māori languages). To address this issue, we propose an effective self-training approach that generates highly accurate pseudo-labels for unlabeled low-resource speech. Our experimental analysis demonstrates that our approach significantly improves word error rate, achieving a relative improvement of 14.94% compared to a baseline model across four real-speech datasets. Further, our proposed approach reports the best results on the Common Voice Punjabi dataset",
    "checked": true,
    "id": "6c72d274fe460c3f0a4b6d0e66c8ddc698c6943e",
    "semantic_title": "a novel self-training approach for low-resource speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23g_interspeech.html": {
    "title": "FunASR: A Fundamental End-to-End Speech Recognition Toolkit",
    "volume": "main",
    "abstract": "This paper introduces FunASR, an open-source speech recognition toolkit designed to bridge the gap between academic research and industrial applications. FunASR offers models trained on large-scale industrial corpora and the ability to deploy them in applications. The toolkit's flagship model, Paraformer, is a non-autoregressive end-to-end speech recognition model that has been trained on a manually annotated Mandarin speech recognition dataset that contains 60,000 hours of speech. To improve the performance of Paraformer, we have added timestamp prediction and hotword customization capabilities to the standard Paraformer backbone. In addition, to facilitate model deployment, we have open-sourced a voice activity detection model based on the Feedforward Sequential Memory Network (FSMN-VAD) and a text post-processing punctuation model based on the controllable time-delay Transformer (CT-Transformer), both of which were trained on industrial corpora. These functional modules provide a solid foundation for building high-precision long audio speech recognition services. Compared to other models trained on open datasets, Paraformer demonstrates superior performance",
    "checked": true,
    "id": "5dc148a0548f63e125acb37badfda87deacd28a4",
    "semantic_title": "funasr: a fundamental end-to-end speech recognition toolkit",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23_interspeech.html": {
    "title": "Streaming Audio-Visual Speech Recognition with Alignment Regularization",
    "volume": "main",
    "abstract": "In this work, we propose a streaming AV-ASR system based on a hybrid connectionist temporal classification (CTC)/attention neural network architecture. The audio and the visual encoder neural networks are both based on the conformer architecture, which is made streamable using chunk-wise self-attention (CSA) and causal convolution. Streaming recognition with a decoder neural network is realized by using the triggered attention technique, which performs time-synchronous decoding with joint CTC/attention scoring. Additionally, we propose a novel alignment regularization technique that promotes synchronization of the audio and visual encoder, which in turn results in better word error rates (WERs) at all SNR levels for streaming and offline AV-ASR models. The proposed AV-ASR model achieves WERs of 2.0% and 2.6% on the Lip Reading Sentences 3 (LRS3) dataset in an offline and online setup, respectively, which both present state-of-the-art results when no external training data are used",
    "checked": true,
    "id": "2b2843ac65f370fcf64c1e8fe07d7da9304c1287",
    "semantic_title": "streaming audio-visual speech recognition with alignment regularization",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fernandezlopez23_interspeech.html": {
    "title": "SparseVSR: Lightweight and Noise Robust Visual Speech Recognition",
    "volume": "main",
    "abstract": "Recent advances in deep neural networks have achieved unprecedented success in visual speech recognition. However, there remains substantial disparity between current methods and their deployment in resource-constrained devices. In this work, we explore different magnitude-based pruning techniques to generate a lightweight model that achieves higher performance than its dense model equivalent, especially under the presence of visual noise. Our sparse models achieve state-of-the-art results at 10% sparsity on the LRS3 dataset and outperform the dense equivalent up to 70% sparsity. We evaluate our 50% sparse model on 7 different visual noise types and achieve an overall absolute improvement of more than 2% WER compared to the dense equivalent. Our results confirm that sparse networks are more resistant to noise than dense networks",
    "checked": true,
    "id": "531ae1f7fcb29911a6d519c454e7035a79a3abf9",
    "semantic_title": "sparsevsr: lightweight and noise robust visual speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23c_interspeech.html": {
    "title": "Multimodal Speech Recognition for Language-Guided Embodied Agents",
    "volume": "main",
    "abstract": "Benchmarks for language-guided embodied agents typically assume text-based instructions, but deployed agents will encounter spoken instructions. While Automatic Speech Recognition (ASR) models can bridge the input gap, erroneous ASR transcripts can hurt the agents' ability to complete tasks. We propose training a multimodal ASR model that utilizes the accompanying visual context to reduce errors in spoken instruction transcripts. We train our model on a dataset of synthetic spoken instructions, derived from the ALFRED household task dataset, where we simulate acoustic noise by systematically masking spoken words. We find that utilizing visual observations facilitates masked word recovery, with multimodal ASR models recovering up to 30% more masked words than unimodal baselines. We also find that spoken instructions transcribed by multimodal ASR models result in higher task completion success rates for a language-guided embodied agent. github.com/Cylumn/embodied-multimodal-asr",
    "checked": true,
    "id": "ab6081fd1b399bbe722bf44d7b6ada004f55cca6",
    "semantic_title": "multimodal speech recognition for language-guided embodied agents",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nishu23_interspeech.html": {
    "title": "Matching Latent Encoding for Audio-Text based Keyword Spotting",
    "volume": "main",
    "abstract": "Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown high-quality results, but the key challenge of how to semantically align two embeddings for multi-word keywords of different sequence lengths remains largely unsolved. In this paper, we propose an audio-text-based end-to-end model architecture for flexible keyword spotting (KWS), which builds upon learned audio and text embeddings. Our architecture uses a novel dynamic programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally partition the audio sequence into the same length as the word-based text sequence using the monotonic alignment of spoken content. Our proposed model consists of an encoder block to get audio and text embeddings, a projector block to project individual embeddings to a common latent space, and an audio-text aligner containing a novel DSP algorithm, which aligns the audio and text embeddings to determine if the spoken content is the same as the text. Experimental results show that our DSP is more effective than other partitioning schemes, and the proposed architecture outperformed the state-of-the-art results on the public dataset in terms of Area Under the ROC Curve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively",
    "checked": true,
    "id": "1198506b8dfa2a001ed0bab82a5403f231a3431e",
    "semantic_title": "matching latent encoding for audio-text based keyword spotting",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/p23_interspeech.html": {
    "title": "Self-Paced Pattern Augmentation for Spoken Term Detection in Zero-Resource",
    "volume": "main",
    "abstract": "The spoken term detection task is challenging when a large volume of spoken content is generated without annotation. The pattern discovery approach aims to overcome the challenges by capturing the pattern similarities directly from the representation of the speech signal. A challenge to the pattern discovery task is handling the variabilities in natural speech. In the proposed approach, we aim to overcome the pattern variability challenges in the spoken term similarity region in three stages. At first, the pattern similarities between two spoken terms were captured using our heuristic search, and the pattern variabilities in the similarity region were observed. In the second stage, the observed pattern variabilities were augmented to the Siamese network to learn the relationship. Finally, the learned network is used to identify the matches between spoken query and document. Based on the experimental studies, it is observed that the proposed approach reduces the false alarms by 17.7% and improves the spoken term detection accuracy by 7.1% against the Microsoft Low-Resource Language corpus",
    "checked": true,
    "id": "2836db777602f1911406c7d6bd0efc83e6a874f4",
    "semantic_title": "self-paced pattern augmentation for spoken term detection in zero-resource",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23y_interspeech.html": {
    "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
    "volume": "main",
    "abstract": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints",
    "checked": true,
    "id": "44c72391f943ac7106484fc6eee405df0d87d629",
    "semantic_title": "on-device constrained self-supervised speech representation learning for keyword spotting via knowledge distillation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/michieli23_interspeech.html": {
    "title": "Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics",
    "volume": "main",
    "abstract": "Keyword Spotting (KWS) models on embedded devices should adapt fast to new user-defined words without forgetting previous ones. Embedded devices have limited storage and computational resources, thus, they cannot save samples or update large models. We consider the setup of embedded online continual learning (EOCL), where KWS models with frozen backbone are trained to incrementally recognize new words from a non-repeated stream of samples, seen one at a time. To this end, we propose Temporal Aware Pooling (TAP) which constructs an enriched feature space computing high-order moments of speech features extracted by a pre-trained backbone. Our method, TAP-SLDA, updates a Gaussian model for each class on the enriched feature space to effectively use audio representations. In experimental analyses, TAP-SLDA outperforms competitors on several setups, backbones, and baselines, bringing a relative average gain of 11.3% on the GSC dataset",
    "checked": true,
    "id": "e6c7f8f3089a46d1efacc3b9c2612f1534b83719",
    "semantic_title": "online continual learning in keyword spotting for low-resource devices via pooling high-order temporal statistics",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23j_interspeech.html": {
    "title": "Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data",
    "volume": "main",
    "abstract": "Few-shot keyword spotting (FS-KWS) models usually require large-scale annotated datasets to generalize to unseen target keywords. However, existing KWS datasets are limited in scale and gathering keyword-like labeled data is costly undertaking. To mitigate this issue, we propose a framework that uses easily collectible, unlabeled reading speech data as an auxiliary source. Self-supervised learning has been widely adopted for learning representations from unlabeled data; however, it is known to be suitable for large models with enough capacity and is not practical for training a small footprint FS-KWS model. Instead, we automatically annotate and filter the data to construct a keyword-like dataset, LibriWord, enabling supervision on auxiliary data. We then adopt multi-task learning that helps the model to enhance the representation power from out-of-domain auxiliary data. Our method notably improves the performance over competitive methods in the FS-KWS benchmark",
    "checked": true,
    "id": "d658235e1878f7cbef6e40e6fa9585fe2cc5891b",
    "semantic_title": "improving small footprint few-shot keyword spotting with supervision on auxiliary data",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23t_interspeech.html": {
    "title": "Robust Keyword Spotting for Noisy Environments by Leveraging Speech Enhancement and Speech Presence Probability",
    "volume": "main",
    "abstract": "Although various deep keyword spotting (KWS) systems have demonstrated promising performance under relatively noiseless environments, accurate keyword detection in the presence of strong noise remains challenging. Room acoustics and noise conditions can be highly diverse, leading to drastic performance degradation if not handled carefully. In this paper, we propose a noise management front-end called SE-SPP Net performing speech enhancement (SE) and speech presence probability (SPP) estimation jointly for robust KWS in noise. The SE-SPP Net estimates both the denoised Mel spectrogram and the position of the speech utterance in the noisy signal, where the latter is estimated as the probability of a particular time-frequency bin containing speech. Further, it comes at relatively no cost in model size when compared to a model estimating the denoised speech. Our SE-SPP Net can improve noisy KWS performance by up to 7% compared to a similar sized state-of-the-art model at SNR -10dB",
    "checked": true,
    "id": "9d880267e6f8db8719ce226aef3dd5b707cb0c1e",
    "semantic_title": "robust keyword spotting for noisy environments by leveraging speech enhancement and speech presence probability",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23m_interspeech.html": {
    "title": "Enhancing the Unified Streaming and Non-streaming Model with Contrastive Learning",
    "volume": "main",
    "abstract": "The unified streaming and non-streaming speech recognition model has achieved great success due to its comprehensive capabilities. In this paper, we propose to improve the accuracy of the unified model by bridging the inherent representation gap between the streaming and non-streaming modes with a contrastive objective. Specifically, the top-layer hidden representation at the same frame of the streaming and non-streaming modes are regarded as a positive pair, encouraging the representation of the streaming mode close to its non-streaming counterpart. The multiple negative samples are randomly selected from the rest frames of the same sample under the non-streaming mode. Experimental results demonstrate that the proposed method achieves consistent improvements toward the unified model in both streaming and non-streaming modes. Our method achieves CER of 4.66% in the streaming mode and CER of 4.31% in the non-streaming mode, which sets a new state-of-the-art on the AISHELL-1 benchmark",
    "checked": true,
    "id": "c4d4ff1c6bd7363e24245c73894fabc9c552170a",
    "semantic_title": "enhancing the unified streaming and non-streaming model with contrastive learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23c_interspeech.html": {
    "title": "ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs",
    "volume": "main",
    "abstract": "In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding Prompt-and-Refine strategy (Figure 3), two simple but effective training-free methods to decrease the Token Display Time (TDT) of streaming ASR models without any accuracy loss. The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken. We argue that streaming acoustic encoders naturally have the modeling ability of Masked Language Models and our experiments demonstrate that ZeroPrompt is engineering cheap and can be applied to streaming acoustic encoders on any dataset without any accuracy loss. Specifically, compared with our baseline models, we achieve 350~700ms reduction on First Token Display Time (TDT-F) and 100~400ms reduction on Last Token Display Time (TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and Librispeech datasets",
    "checked": true,
    "id": "635e5a007dd2e31503c9a5b0668f44fd6b10c767",
    "semantic_title": "zeroprompt: streaming acoustic encoders are zero-shot masked lms",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23_interspeech.html": {
    "title": "Improved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation",
    "volume": "main",
    "abstract": "Punctuated text prediction is crucial for automatic speech recognition as it enhances readability and impacts downstream natural language processing tasks. In streaming scenarios, the ability to predict punctuation in real-time is particularly desirable but presents a difficult technical challenge. In this work, we propose a method for predicting punctuated text from input speech using a chunk-based Transformer encoder trained with Connectionist Temporal Classification (CTC) loss. The acoustic model trained with long sequences by concatenating the input and target sequences can learn punctuation marks attached to the end of sentences more effectively. Additionally, by combining CTC losses on the chunks and utterances, we achieved both the improved F1 score of punctuation prediction and Word Error Rate (WER)",
    "checked": true,
    "id": "22dc06d4ca6591ac6f23490c9d2e434330a6e1ce",
    "semantic_title": "improved training for end-to-end streaming automatic speech recognition model with punctuation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huybrechts23_interspeech.html": {
    "title": "DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer",
    "volume": "main",
    "abstract": "Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings",
    "checked": true,
    "id": "ba99f8a00047f12d803f49325f6e5b8cef154b08",
    "semantic_title": "dctx-conformer: dynamic context carry-over for low latency unified streaming and non-streaming conformer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23_interspeech.html": {
    "title": "Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer",
    "volume": "main",
    "abstract": "Streaming automatic speech recognition (ASR) models are restricted from accessing future context, which results in worse performance compared to the non-streaming models. To improve the performance of streaming ASR, knowledge distillation (KD) from the non-streaming to streaming model has been studied, mainly focusing on aligning the output token probabilities. In this paper, we propose a layer-to-layer KD from the teacher encoder to the student encoder. To ensure that features are extracted using the same context, we insert auxiliary non-streaming branches to the student and perform KD from the non-streaming teacher layer to the non-streaming auxiliary layer. We design a special KD loss that leverages the autoregressive predictive coding (APC) mechanism to encourage the streaming model to predict unseen future contexts. Experimental results show that the proposed method can significantly reduce the word error rate compared to previous token probability distillation methods",
    "checked": true,
    "id": "448071368f036bc8178d0be8ef1c3c3db127b33e",
    "semantic_title": "knowledge distillation from non-streaming to streaming asr encoder using auxiliary non-streaming layer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23d_interspeech.html": {
    "title": "Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition",
    "volume": "main",
    "abstract": "By incorporating additional contextual information, deep biasing methods have emerged as a promising solution for speech recognition of personalized words. However, for real-world voice assistants, always biasing on personalized words with high prediction scores can degrade the performance of recognizing common words. To address this issue, we propose an adaptive contextual biasing method based on Context-Aware Transformer Transducer (CATT) that utilizes the biased encoder and predictor embeddings to perform streaming prediction of contextual phrase occurrences. This prediction is then used to switch the bias list on and off, enabling the model to adapt to both personalized and common scenarios. Experiments on Librispeech and internal voice assistant datasets show that our approach can achieve up to 6.7% and 20.7% relative reduction in WER and CER compared to the baseline respectively, mitigating up to 96.7% and 84.9% of the relative WER and CER increase for common cases",
    "checked": true,
    "id": "53103c3746965b40f95a5870a45d9e3ea28d5d8e",
    "semantic_title": "adaptive contextual biasing for transducer based streaming speech recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martel23_interspeech.html": {
    "title": "Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model",
    "volume": "main",
    "abstract": "We propose Audio-Visual Lightweight ITerative model (AVLIT), an effective and lightweight neural network that uses Progressive Learning (PL) to perform audio-visual speech separation in noisy environments. To this end, we adopt the Asynchronous Fully Recurrent Convolutional Neural Network (A-FRCNN), which has shown successful results in audio-only speech separation. Our architecture consists of an audio branch and a video branch, with iterative A-FRCNN blocks sharing weights for each modality. We evaluated our model in a controlled environment using the NTCD-TIMIT dataset and in-the-wild using a synthetic dataset that combines LRS3 and WHAM!. The experiments demonstrate the superiority of our model in both settings with respect to various audio-only and audio-visual baselines. Furthermore, the reduced footprint of our model makes it suitable for low resource applications",
    "checked": true,
    "id": "eb02d89acb27f523d7b1505b24ea9bfd3d27ab6f",
    "semantic_title": "audio-visual speech separation in noisy environments with a lightweight iterative model",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saijo23_interspeech.html": {
    "title": "Remixing-based Unsupervised Source Separation from Scratch",
    "volume": "main",
    "abstract": "We propose an unsupervised approach for training separation models from scratch using RemixIT and Self-Remixing, which are recently proposed self-supervised learning methods for refining pre-trained models. They first separate mixtures with a teacher model and create pseudo-mixtures by shuffling and remixing the separated signals. A student model is then trained to separate the pseudo-mixtures using either the teacher's outputs or the initial mixtures as supervision. To refine the teacher's outputs, the teacher's weights are updated with the student's weights. While these methods originally assumed that the teacher is pre-trained, we show that they are capable of training models from scratch. We also introduce a simple remixing method to stabilize training. Experimental results demonstrate that the proposed approach outperforms mixture invariant training, which is currently the only available approach for training a monaural separation model from scratch",
    "checked": true,
    "id": "49604747961bc89e85e90079806e77840b68663f",
    "semantic_title": "remixing-based unsupervised source separation from scratch",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/okamoto23_interspeech.html": {
    "title": "CAPTDURE: Captioned Sound Dataset of Single Sources",
    "volume": "main",
    "abstract": "In conventional studies on environmental sound separation and synthesis using captions, datasets consisting of multiple-source sounds with their captions were used for model training. However, when we collect the captions for multiple-source sound, it is not easy to collect detailed captions for each sound source, such as the number of sound occurrences and timbre. Therefore, it is difficult to extract only the single-source target sound by the model-training method using a conventional captioned sound dataset. In this work, we constructed a dataset with captions for a single-source sound named CAPTDURE, which can be used in various tasks such as environmental sound separation and synthesis. Our dataset consists of 1,044 sounds and 4,902 captions. We evaluated the performance of environmental sound extraction using our dataset. The experimental results show that the captions for single-source sounds are effective in extracting only the single-source target sound from the mixture sound",
    "checked": true,
    "id": "028bb4bd56a29ee955d46f28f640b4c34bafdecd",
    "semantic_title": "captdure: captioned sound dataset of single sources",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/munakata23_interspeech.html": {
    "title": "Recursive Sound Source Separation with Deep Learning-based Beamforming for Unknown Number of Sources",
    "volume": "main",
    "abstract": "We propose a recursive separation model for an unknown number of sound sources based on deep learning-based beamforming. Recursive separation models have been investigated as a way to separate a mixture signal composed of an unknown number of sources in a single-channel condition. The mixture signal is separated with residual information in a recursive manner. Although the recursive separation model can be extended to a multi-channel condition using a beamforming-based filter, the separation performance is degraded because the beamforming-based filter tends to accumulate estimation errors in the recursions. To address this problem, we introduce a local Gaussian model (LGM)-based recursive separation model. The proposed method mitigates the accumulation of errors by reusing estimated parameters and applying only one filter to the mixture signal. Experimental results show that our proposed method outperforms a separation model using an accumulative filter",
    "checked": true,
    "id": "a95917f32e83120ed9527d87fc1c1993861fb74f",
    "semantic_title": "recursive sound source separation with deep learning-based beamforming for unknown number of sources",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mosner23_interspeech.html": {
    "title": "Multi-Channel Speech Separation with Cross-Attention and Beamforming",
    "volume": "main",
    "abstract": "Originally, single-channel source separation gained more research interest. It resulted in immense progress. Multi-channel (MC) separation comes with new challenges posed by adverse indoor conditions making it an important field of study. We seek to combine promising ideas from the two worlds. First, we build MC models by extending current single-channel time-domain separators relying on their strength. Our approach allows reusing pre-trained models by inserting designed lightweight reference channel attention (RCA) combiner, the only trained module. It comprises two blocks: the former allows attending to different parts of other channels w.r.t. the reference one, and the latter provides an attention-based combination of channels. Second, like many successful MC models, our system incorporates beamforming and allows for the fusion of the network and beamformer outputs. We compare our approach with the SOTA models on the SMS-WSJ dataset and show better or similar performance",
    "checked": true,
    "id": "59a438952214794f978593e72f56430357e75b7a",
    "semantic_title": "multi-channel speech separation with cross-attention and beamforming",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eom23_interspeech.html": {
    "title": "Background-Sound Controllable Voice Source Separation",
    "volume": "main",
    "abstract": "There have been various approaches to separate mixed voices. In the real world, input voices contain many different kinds of background sounds but existing methods have not considered the background sounds in model architectures. These approaches are difficult to control the background sounds directly and the voice separation results include the background sounds randomly. In this paper, we propose an extended voice separation framework, background-sound controllable voice source separation that can control the degrees of background sounds of voice separation outputs using a control parameter that ranges from 0 to 1 without additional mixing procedures. Several experiments show the controllability of background sounds on various real world datasets with preserving voice separation performances",
    "checked": true,
    "id": "bec608753780f2505559a010abb500ccacbae345",
    "semantic_title": "background-sound controllable voice source separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/escobargrisales23_interspeech.html": {
    "title": "An Automatic Multimodal Approach to Analyze Linguistic and Acoustic Cues on Parkinson's Disease Patients",
    "volume": "main",
    "abstract": "Early detection and monitoring of Parkinson's disease are crucial for properly treating and managing the symptoms. Automatic speech and language analysis has emerged as a promising non-invasive method to monitor the patient's state. This study analyzed different speech and language representations for automatic classification between Parkinson's disease patients and healthy controls. First, each modality is analyzed independently. General representations such as Wav2vec or BETO are used together with representations oriented to model disease traits such as phonemic identifiability in speech modality and grammatical units analysis in language modality. The best speech and language representations were combined using a fusion strategy based on Gated Multimodal Units. The best results are achieved with the multimodal approach, outperforming all results obtained with unimodal representations and the traditional fusion strategy",
    "checked": true,
    "id": "655d462fc9955620257053801d6253e9fe56f723",
    "semantic_title": "an automatic multimodal approach to analyze linguistic and acoustic cues on parkinson's disease patients",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23_interspeech.html": {
    "title": "Personalization for Robust Voice Pathology Detection in Sound Waves",
    "volume": "main",
    "abstract": "Automatic voice pathology detection is promising for non-invasive screening and early intervention using sound signals. Nevertheless, existing methods are susceptible to covariate shifts due to background noises, human voice variations, and data selection biases leading to severe performance degradation in real-world scenarios. Hence, we propose a non-invasive framework that contrastively learns personalization from sound waves as a pre-train and predicts latent-spaced profile features through semi-supervised learning. It allows all subjects from various distributions (e.g., regionality, gender, age) to benefit from personalized predictions for robust voice pathology in a privacy-fulfilled manner. We extensively evaluate the framework on four real-world respiratory illnesses datasets, including Coswara, COUGHVID, ICBHI and our private dataset - ASound, under multiple covariate shift settings (i.e., cross-dataset), improving up to 4.12% in overall performance",
    "checked": true,
    "id": "dd4f872c22a6b7c28a81b9ea1f803f1b34bb29e1",
    "semantic_title": "personalization for robust voice pathology detection in sound waves",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23d_interspeech.html": {
    "title": "Integrated and Enhanced Pipeline System to Support Spoken Language Analytics for Screening Neurocognitive Disorders",
    "volume": "main",
    "abstract": "This paper presents an enhanced pipeline system for automated screening of neurocognitive disorders, e.g. Alzheimer's Disease (AD), using spoken language technologies. To ensure local relevance, the pipeline is applied to two-way interactions between clinical assessors and older adult participants in spoken Cantonese, the predominant language used in Hong Kong. The pipeline includes: (i) Speaker diarization using speaker-turn-aware scoring to capture the temporal structure of conversations. (ii) ASR using XLS-R wav2vec 2.0 models further pre-trained on Cantonese speech data and fine-tuned. (iii) Language modelling using RoBERTa with further fine-tuning. (iv) AD screening with neural network classification. A reference benchmark is obtained using the ADReSS corpus where no diarization is needed, and the partial pipeline attained a competitive detection accuracy of 87.5%",
    "checked": true,
    "id": "e5346f94b1e6138cbbebc6b7a3c01d666e29755a",
    "semantic_title": "integrated and enhanced pipeline system to support spoken language analytics for screening neurocognitive disorders",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niu23b_interspeech.html": {
    "title": "Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder",
    "volume": "main",
    "abstract": "Emotion is a complex behavioral phenomenon, which is expressed and perceived through various modalities, such as language, vocal and facial expressions. Psychiatric research has suggested that the lack of emotional alignment between modalities is a symptom of emotion disorders. In this work, we quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional MisMatch (EMM), as an intermediate step for mood identification. We use a longitudinal dataset collected from people with Bipolar Disorder (BP) and show that symptomatic mood episodes show significantly more EMM, compared to euthymic moods. We propose a fully automatic mood identification pipeline with automatic speech transcription, emotion recognition, and EMM feature extraction. We find that EMM features, although smaller in size, outperform a language-based baseline, and consistently provide improvement when combined with language and/or raw emotion features on mood classification",
    "checked": true,
    "id": "015ee16bb1c149c53f486c60c44bea54b7812fe0",
    "semantic_title": "capturing mismatch between textual and acoustic emotion expressions for mood identification in bipolar disorder",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23d_interspeech.html": {
    "title": "FTA-net: A Frequency and Time Attention Network for Speech Depression Detection",
    "volume": "main",
    "abstract": "Depression is one of the most common mental diseases nowadays, which seriously affects the health of individuals. Some researchers have shown an association between the level of depression and speech features in individuals, so a lot of automatic speech-based depression detection systems have been proposed. A number of studies utilized convolutional neural network (CNN) to realize the speech depression detection. However, most of these studies did not take into account that different frequencies and time steps in the speech spectrum features contribute unequally to the detection of depression. In order to extract more significant and distinctive features, this paper proposes an effective frequency-time attention (FTA) module for CNN, which is based on squeeze and excitation operations and can emphasize the time steps and frequencies associated with depression. Experimental results based on the AVEC 2013 and AVEC 2014 benchmarks demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "369c69aea9cfd1d6d2bf5bfb6def022ee43c85ee",
    "semantic_title": "fta-net: a frequency and time attention network for speech depression detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fara23_interspeech.html": {
    "title": "Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data",
    "volume": "main",
    "abstract": "Predicting the presence of major depressive disorder (MDD) using speech is highly non-trivial. The heterogeneous clinical profile of MDD means that any given speech pattern may be associated with a unique combination of depressive symptoms. Conventional discriminative machine learning models may lack the complexity to robustly model this heterogeneity. Bayesian networks, however, are well-suited to such a scenario. They provide further advantages over standard discriminative modeling by offering the possibility to (i) fuse with other data streams; (ii) incorporate expert opinion into the models; (iii) generate explainable model predictions, inform about the uncertainty of predictions, and (iv) handle missing data. In this study, we apply a Bayesian framework to capture the relationships between depression, depression symptoms, and features derived from speech, facial expression and cognitive game data. Presented results also highlight our model is not subject to demographic biases",
    "checked": true,
    "id": "48e3815ac637d6d1d4f38119c4e2a77dea855661",
    "semantic_title": "bayesian networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23y_interspeech.html": {
    "title": "Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "Automatic recognition of disordered and elderly speech remains highly challenging tasks to date due to data scarcity. Parameter fine-tuning is often used to exploit the large quantities of non-aged and healthy speech pre-trained models, while neural architecture hyper-parameters are set using expert knowledge and remain unchanged. This paper investigates hyper-parameter adaptation for Conformer ASR systems that are pre-trained on the Librispeech corpus before being domain adapted to the DementiaBank elderly and UASpeech dysarthric speech datasets. Experimental results suggest that hyper-parameter adaptation produced word error rate (WER) reductions of 0.45% and 0.67% over parameter-only fine-tuning on DBank and UASpeech tasks respectively. An intuitive correlation is found between the performance improvements by hyper-parameter domain adaptation and the relative utterance length ratio between the source and target domain data",
    "checked": true,
    "id": "daaf51daf49a7e05458ffe686f2972c0fe41cdc3",
    "semantic_title": "hyper-parameter adaptation of conformer asr systems for elderly and dysarthric speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/campbell23_interspeech.html": {
    "title": "Classifying depression symptom severity: Assessment of speech representations in personalized and generalized machine learning models",
    "volume": "main",
    "abstract": "There is an urgent need for new methods that improve the management and treatment of Major Depressive Disorder (MDD). Speech has long been regarded as a promising digital marker in this regard, with many works highlighting that speech changes associated with MDD can be captured through machine learning models. Typically, findings are based on cross-sectional data, with little work exploring the advantages of personalization in building more robust and reliable models. This work assesses the strengths of different combinations of speech representations and machine learning models, in personalized and generalized settings in a two-class depression severity classification paradigm. Key results on a longitudinal dataset highlight the benefits of personalization. Our strongest performing model set-up utilized self-supervised learning features and convolutional neural network (CNN) and long short-term memory (LSTM) back-end",
    "checked": true,
    "id": "524989f8798a4e5424ce912545646aadf5390e56",
    "semantic_title": "classifying depression symptom severity: assessment of speech representations in personalized and generalized machine learning models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghaffarzadegan23_interspeech.html": {
    "title": "Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma",
    "volume": "main",
    "abstract": "Existing audio-based asthma monitoring solutions rely on feature engineering designs paired with contact-based auscultation which are brittle in practice and do not scale beyond point of care setups. Data-driven methods utilizing contactless microphones have the potential to address such limitations. These solutions are under-explored in healthcare due to high cost of data curation requiring physicians-in-the-loop. Here, we propose an active learning (AL) system to facilitate audio data collection and annotation. It detects lung sound abnormalities in asthma. AL reduces the annotation cost while increasing the model performance under a constrained annotation budget. It automatically extracts interesting audio segments from the continuous recordings, and efficiently annotates and trains anomaly detector model. The experimental results confirm the effectiveness of the proposed system as an enabler for larger scale data curation on a newly collected audio corpus for pediatric asthma",
    "checked": true,
    "id": "8d3e9790f07f70fc661a4319516a6fb8d32fb58e",
    "semantic_title": "active learning for abnormal lung sound data curation and detection in asthma",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pereztoro23_interspeech.html": {
    "title": "Automatic Assessment of Alzheimer's across Three Languages Using Speech and Language Features",
    "volume": "main",
    "abstract": "With the increasing prevalence of Alzheimer's Disease (AD) worldwide, it is essential to develop non-invasive methods to monitor the progression of the disease. Speech and language analyses are suitable for detecting the cognitive impairment of AD patients; thus, by analyzing changes in speech patterns and language use, researchers can develop methods to monitor AD remotely. In this paper, we investigated several speech and language techniques commonly used for the automatic detection of AD. Furthermore, we considered speech recordings of 448 patients in three different languages: Spanish (57), German (205), and English (186). Cross-lingual analysis was carried out using two classification approaches: (1) training/testing in one or more languages and (2) training in one language and testing in another. We obtained unweighted average recall values of up to 83% to classify AD using the first classification approach and up to 70% with the second",
    "checked": true,
    "id": "5e485d7b072b3cf5b8ef260039d5c4b106c652a9",
    "semantic_title": "automatic assessment of alzheimer's across three languages using speech and language features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geng23_interspeech.html": {
    "title": "On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition",
    "volume": "main",
    "abstract": "Accurate recognition of dysarthric and elderly speech remain challenging tasks to date. Speaker-level heterogeneity attributed to accent or gender, when aggregated with age and speech impairment, create large diversity among these speakers. Scarcity of speaker-level data limits the practical use of data-intensive model based speaker adaptation methods. To this end, this paper proposes two novel forms of data-efficient, feature-based on-the-fly speaker adaptation methods: variance-regularized spectral basis embedding (SVR) and spectral feature driven f-LHUC transforms. Experiments conducted on UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest the proposed on-the-fly speaker adaptation approaches consistently outperform baseline iVector adapted hybrid DNN/TDNN and E2E Conformer systems by statistically significant WER reduction of 2.48%-2.85% absolute (7.92%-8.06% relative), and offline model based LHUC adaptation by 1.82% absolute (5.63% relative) respectively",
    "checked": true,
    "id": "403db1c9d91c4e99fd82b77d4cd7b0d09624faff",
    "semantic_title": "on-the-fly feature based rapid speaker adaptation for dysarthric and elderly speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/svihlik23_interspeech.html": {
    "title": "Relationship between LTAS-based spectral moments and acoustic parameters of hypokinetic dysarthria in Parkinson's disease",
    "volume": "main",
    "abstract": "Although long-term averaged spectrum (LTAS) descriptors can detect the change in dysarthria of patients with Parkinson's disease (PD) due to subthalamic nucleus deep brain stimulation (STN-DBS), the relationship between LTAS variables with measures that relate to laryngeal physiology remain unknown. We aimed to find connections between LTAS-based moments and the main acoustic characteristics of hypokinetic dysarthria in PD as the response to STN-DBS stimulation changes. We analyzed reading passages of 23 PD patients in ON and OFF STN-DBS states compared to 23 healthy controls. We found a relation between the stimulation-induced change in several spectral moments and acoustic parameters representing voice quality, articulatory decay, net speech rate, and mean fundamental frequency. While the difference between PD and controls was significant across most acoustic descriptors, only the spectral mean and fundamental frequency variability could differentiate between ON and OFF conditions",
    "checked": true,
    "id": "311d95c6600b6cdca5d07b4e4e9842a5f33a296c",
    "semantic_title": "relationship between ltas-based spectral moments and acoustic parameters of hypokinetic dysarthria in parkinson's disease",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alvarado23_interspeech.html": {
    "title": "Respiratory distress estimation in human-robot interaction scenario",
    "volume": "main",
    "abstract": "Social robotics and human-robot partnership are becoming very relevant topics in the next decades defining many challenges for speech technology. In addition, the COVID pandemic imposed an awareness of technology challenges to fight massive health problems. In this paper, the first system to estimate respiratory distress in a human-robot interaction (HRI) environment is presented. The training procedure of the dyspnea estimation models by simulating the HRI acoustic environment with real room impulse responses (estimated with a PR2 robot) and additive noise is described. The training and testing data were processed using two beamforming techniques: delay-and-sum and MVDR. The results suggest that it should be possible to reduce significantly the degradation in precision of estimates of respiratory distress in a real HRI scenario. The improvements in accuracy and AUC with MVDR when compared to baseline processing without beamforming are 7% and 4%, respectively",
    "checked": true,
    "id": "0b5193f9a2ac1ce8a6225ebf99bc1d7a8f22f84b",
    "semantic_title": "respiratory distress estimation in human-robot interaction scenario",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/reynerfuentes23_interspeech.html": {
    "title": "Prediction of the Gender-based Violence Victim Condition using Speech: What do Machine Learning Models rely on?",
    "volume": "main",
    "abstract": "Women who have experienced gender-based violence (GBV) are at an increased risk of developing mental illnesses such as depression, anxiety, and post-traumatic stress disorder (PTSD). Recently, Artificial Intelligence (AI) has provided new tools to assist mental health clinical diagnosis, including speech-based detection. However, there is not much work done on the GBV victim (GBVV) condition detection. This study aims to identify specific speech features that aid this detection, analyse the relationship of such results with the user's psychological evaluation, and evaluate whether the models rely on the speaker identity or self-reported emotions to predict the GBVV condition. Our results indicate that it is possible to distinguish GBVV with controlled sequelae from non-victims, which may suggest that such differentiation for GBVV with more severe mental aftereffects-such as PTSD-may be even more meaningful. We believe that our work can help future mental health AI therapy assistants",
    "checked": true,
    "id": "e60da852d8948e172b963a2dd875683e82db8064",
    "semantic_title": "prediction of the gender-based violence victim condition using speech: what do machine learning models rely on?",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/charola23_interspeech.html": {
    "title": "Whisper Encoder features for Infant Cry Classification",
    "volume": "main",
    "abstract": "Identifying the pathology in infant cry is an important and socially relevant research problem, as it can save the lives of many infants. This study proposes the use of transfer learning based approach using Whisper Encoder Module which is compared against state-of-the art MFCC feature set for classification of normal vs. pathological infant cry. Moreover, we also present multi-class pathological infant cry classification using CNN and Bi-LSTM networks. Our study finds that whisper encoder module coupled with DNN classifiers such as CNN and Bi-LSTM outperform MFCC features with absolute increment of 4% and 1% on CNN and Bi-LSTM respectively. Furthermore, whisper encoder features are analysed using statistical parameters and t-SNE plots. The experiments are performed using the 10-fold cross-validation on Baby Chillanto dataset, In-house DA-IICT dataset, and also on the datasets formed by combining these two datasets",
    "checked": true,
    "id": "929d422e098c08a0edb20bf91572b195e1241c15",
    "semantic_title": "whisper encoder features for infant cry classification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jurov23_interspeech.html": {
    "title": "A neural architecture for selective attention to speech features",
    "volume": "main",
    "abstract": "Speech perception is complex and demands constant adaptations to the speaker and the environment (i.e. noisy speech, accent, etc.). To adapt, the listener relies on one speech feature more than another. This cognitive mechanism is called selective attention. We present a model that captures the idea of selective attention: we show that this dynamic adaptation process can be captured in a neural architecture by using a multiple encoder beta variational auto encoder (beta-ME-VAE), which is based on rate distortion theory. This model implements the idea that optimal feature weighting looks different under different listening conditions and provides insight into how listeners can adapt their listening strategy on a moment-to-moment basis, even in listening situations they haven't experienced before",
    "checked": true,
    "id": "a845c05ecc36a90596fa68db8917674448fb2fac",
    "semantic_title": "a neural architecture for selective attention to speech features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huo23_interspeech.html": {
    "title": "Quantifying Informational Masking due to Masker Intelligibility in Same-talker Speech-in-speech Perception",
    "volume": "main",
    "abstract": "Intelligibility of the competing speech plays a significant role in causing informational masking (IM) to the target speech during speech-in-speech perception, especially in same-talker conditions where the target and the masker share a large number of similarities in acoustics. Few studies have quantitatively measured IM as a function of intelligibility of competing speech. Evidence shows that voiced segments are robust cues for speech intelligibility. In this study, the contribution of masker intelligibility to IM was studied by adjusting the voice-to-noise ratio (VNR) on voiced segments of the competing speech, while maintaining energetic masking (EM) at different target-to-masker ratios. Although model estimations suggested that the intelligibility due to EM converged when VNR<0 dB, listener performance showed that more release from IM was received with a further decrease in VNR. It was projected that masker intelligibility could lead to target intelligibility decreased by 50%",
    "checked": true,
    "id": "0651d3c1fe5dc628697300c6d1670aa63f2ab0af",
    "semantic_title": "quantifying informational masking due to masker intelligibility in same-talker speech-in-speech perception",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cuervo23_interspeech.html": {
    "title": "On the Benefits of Self-supervised Learned Speech Representations for Predicting Human Phonetic Misperceptions",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) trained by self-supervised learning (SSL) have recently been shown to produce representations similar to brain activations for the same speech input. Can SSL representations help to explain human speech perception errors? Aiming to shed light on this question, we study their use for phonetic misperception prediction. We extract representations from wav2vec 2.0, a recent SSL architecture for speech, and use them to compute features for a model predicting the presence of phonetic perception errors in speech-in-noise signals. We perform our experiments on a corpus of over 3000 consistent word-in-noise confusions in English. We consider multiple SSL-based features and compare them against conventional acoustic baselines and features obtained from DNNs fine-tuned through supervised learning for ASR. Our results show the superiority of SSL representations when extracted from the proper layer, further suggesting their potential to model human speech perception",
    "checked": true,
    "id": "fab47c9386872f92dc8c7a83447783012da85f46",
    "semantic_title": "on the benefits of self-supervised learned speech representations for predicting human phonetic misperceptions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schulz23_interspeech.html": {
    "title": "Predicting Perceptual Centers Located at Vowel Onset in German Speech Using Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "Perceptual centers (p-centers) can be defined as the perceived centers of a syllable. Previous research regarding the location of p-centers in speech relied on experimental methods, and among the suggested acoustic features contributing to the location of p-centers in Germanic languages is the transition of the consonant to the vowel onset. The current study investigates the prediction of the location of p-centers in German, by means of machine learning. Machine learning is a promising tool to capture possible non-linear relationships that may occur among the acoustic features used in the complexity that is the human perception. Therefore, an LSTM neural network approach was used for the identification of p-centers in a set of spoken German sentences, with input data features being Mel Frequency Cepstral Coefficients (MFCC), amplitude envelope and root mean squared energy. The model was able to achieve a balanced accuracy of 84% with MFCCs being the best predictor of p-center location",
    "checked": true,
    "id": "f8f4ddfb50b76b881e7f27b271330b634893e1c0",
    "semantic_title": "predicting perceptual centers located at vowel onset in german speech using long short-term memory networks",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cooke23_interspeech.html": {
    "title": "Exploring the mutual intelligibility breakdown caused by sculpting speech from a competing speech signal",
    "volume": "main",
    "abstract": "Passing noise through a binary mask representing speech leads to remarkably intelligible speech. However, if the mask input is a competing speech signal, both the competing speech and the target speech represented by the mask are rendered unintelligible. The current study considers potential explanations for this abrupt breakdown. Competing speech was modified to reduce the influence of properties that may have interacted adversely with those of the target, including speaker, language, F0 and spectral detail. Properties were modified by noise-vocoding, envelope substitution and preservation of temporal modulations. The outcome of a listening experiment indicated that the impact of competing speech is largely due to conflicting formant-scale spectral detail and the absence of sufficient energy in specific temporal epochs, while conflicting F0 plays no role. These findings contribute to a broader understanding of the minimal representational basis that underlies speech perception",
    "checked": true,
    "id": "a3b060c86a8f3c63cf94f45962d15fc8f05b9c3e",
    "semantic_title": "exploring the mutual intelligibility breakdown caused by sculpting speech from a competing speech signal",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kitahara23_interspeech.html": {
    "title": "Perception of Incomplete Voicing Neutralization of Obstruents in Tohoku Japanese",
    "volume": "main",
    "abstract": "Intervocalic voicing neutralization has been generally accepted as a peculiar feature of Tohoku dialects. The present paper reports the results of perception experiments on this phenomenon. Natural and resynthesized stimuli spoken by Tohoku speakers were presented to both Tohoku and Tokyo listeners in a series of online experiments. A comparison between these two listener groups reveals that, for Tohoku listeners whose perception was biased by the voicing neutralization in their phonology, the boundary between voiced and voiceless tokens was more blurred compared to Tokyo listeners whose phonology had no such neutralization. These results suggest that neutralization can be bidirectional: i.e., voiced tokens become less voiced and voiceless tokens become less voiceless in contrast to the traditional view of neutralization which assumes a unidirectional process where one category remains intact and the other category merges with the former",
    "checked": true,
    "id": "39a688d168daaffc2a87e3aba5a01ce0509539eb",
    "semantic_title": "perception of incomplete voicing neutralization of obstruents in tohoku japanese",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pohnlein23_interspeech.html": {
    "title": "The emergence of obstruent-intrinsic f0 and VOT as cues to the fortis/lenis contrast in West Central Bavarian",
    "volume": "main",
    "abstract": "This study examines the effects of underlying voicing in word initial and medial stops on f0 at the onset of the following stressed and unstressed vowel (CF0), respectively, in Standard German (SG) and West Central Bavarian (WB). As opposed to SG, WB hitherto did not contrast fortis and lenis stops by VOT, but the importance of this cue increases in younger WB speakers. The replacement of VOT by f0 as acoustic cue in connection with voicing mergers and tonogenesis is well-studied but not the emergence of CF0 effects together with an evolving VOT contrast. An acoustic analysis of twenty SG speakers as well as ten older and ten younger WB speakers showed higher f0 after fortis compared to lenis stops in SG but only in initial position where VOT was much longer. Younger but not older WB speakers showed signs of developing CF0 effects in initial stops as found in SG which may forecast VOT differences in this position at the population level possibly due to speaker-specific cue enhancement",
    "checked": true,
    "id": "8cd448ede105cebb732610457c959b814121eb7b",
    "semantic_title": "the emergence of obstruent-intrinsic f0 and vot as cues to the fortis/lenis contrast in west central bavarian",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/havard23_interspeech.html": {
    "title": "〈'〉 in Tsimane': a Preliminary Investigation",
    "volume": "main",
    "abstract": "Tsimane' is a language spoken in Bolivia by several thousand people. Yet, it has not been described in detail. We aim to take a step towards a better description by focusing on an aspect of language: the sound represented in spelling with 〈'〉, informally described as a glottal stop. We recorded two adult speakers of Tsimane' producing (near-)minimal pairs involving this sound. Perceptual analyses suggested 〈'〉 is very rarely realised as a full glottal stop, and is more often cued by creaky-voiced vow- els and nasals. Despite the variability in implementation, presentation of syllabic minimal pairs to these two informants and two other adult Tsimane' listeners revealed evidence that they could easily perceive when 〈'〉 was intended. Together, these data suffice to rule out the hypothesis that 〈'〉 is systematically realised as a full stop, and suggests instead a more complex set of perceptual cues may be at speakers' and listeners' disposal",
    "checked": true,
    "id": "44ef277670749de4289317e909b37de4100237f4",
    "semantic_title": "〈'〉 in tsimane': a preliminary investigation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hoffmann23_interspeech.html": {
    "title": "Segmental features of Brazilian (Santa Catarina) Hunsrik",
    "volume": "main",
    "abstract": "This paper explores the segmentals of Brazilian Hunsrik, Santa Catarina (SC). The account proposed here is merely the starting point for a more comprehensive analysis of this unique, under-resourced dialect of German for which no written standard and only limited prior linguistic descriptions exist. The goal is to characterise the segmental features of SC Hunsrik and compare them to those reported for the Hunsrik of Rio Grande do Sul (RS). The description is based on two Praat-analysed recordings of three speakers: one saying prayers, and two chatting about daily chores. The recurrent features include vowel raising, de-rounding and schwa epenthesis; consonantal aspects involve obstruent voicing assimilation, consonant deletion and /l/-velarisation. The next steps will involve unravelling the features of German Hunsrück and the Brazilian Portuguese influences as well as analysing the dialect's prosody",
    "checked": true,
    "id": "ef88d67eeb539a12d344ec691a86be65f25a9b97",
    "semantic_title": "segmental features of brazilian (santa catarina) hunsrik",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ratko23_interspeech.html": {
    "title": "Opening or Closing? An Electroglottographic Analysis of Voiceless Coda Consonants in Australian English",
    "volume": "main",
    "abstract": "In voiceless sounds, the glottis may be spread or constricted. Glottal spreading is associated with breathiness, and constriction with glottalisation. In many dialects of English, glottalisation often occurs with coda /t/ and sometimes with /p, k/, suggesting coda stop voicelessness is achieved through glottal constriction. Conversely, voiceless coda fricatives are associated with breathiness of the preceding vowel, with voicelessness achieved through glottal spreading. However, analyses specifically measuring glottal activity in coda consonant contexts in English are sparse. We conducted an electroglottographic analysis of vowels preceding voiceless codas /p, t, k, s/ to examine how coda voicelessness is achieved in Australian English (AusE). We found that coda /t/ and /p/ show glottal constriction towards vowel offset. Conversely, /k/ patterns with /s/ and exhibits glottal spreading. This suggests that different glottal configurations are used to achieve coda voicelessness in AusE",
    "checked": true,
    "id": "08c531064e40a81b91656faea94270b242f1f904",
    "semantic_title": "opening or closing? an electroglottographic analysis of voiceless coda consonants in australian english",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zebe23_interspeech.html": {
    "title": "Increasing aspiration of word-medial fortis plosives in Swiss Standard German",
    "volume": "main",
    "abstract": "There is evidence for a sound change in progress in German-speaking Switzerland: Namely, Swiss German speakers of Alemannic increasingly use aspiration in fortis plosives, particularly in word-initial position. This study aims to extend the research by investigating word-medial plosives in Swiss Standard German (SSG). Using the apparent-time paradigm, the main goal is to compare younger to older speakers. Since the increasing aspiration is probably driven by the contact to German Standard German (GSG), this study focuses on speakers from both rural and urban areas, assuming that the latter have more contact to speakers of GSG than the former. Results show that younger urban speakers produce longer VOT values in alveolar plosives than the other speakers, while all younger speakers show this pattern for bilabial plosives. Furthermore, only the younger speakers from the urban group produce shorter closure durations in fortis plosives",
    "checked": true,
    "id": "90a01d4e87178b945e22b8706aaae6f226f5b9bc",
    "semantic_title": "increasing aspiration of word-medial fortis plosives in swiss standard german",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shao23_interspeech.html": {
    "title": "Lexical Stress and Velar Palatalization in Italian: A spatio-temporal Interaction",
    "volume": "main",
    "abstract": "Palatalization is the process whereby a velar stop is fronted to a palatal affricate or fricative. In Italian, it takes place at the boundary between the root and /i/ suffixes. In nouns and adjectives, palatalization occurs in words with antepenultimate stress ([ˈko.mi.t͡ʃi]), while it is much rarer in words with penultimate stress ([ka.ˈdu.ki]) Based on one acoustic and one articulatory study (EMA), we postulate that the resistance of post-tonic /k, g/ to palatalize is related to the stressed vowel directly preceding. In the acoustic domain, post-tonic consonants show longer closure duration. This increase in closure duration is directly related to a larger and longer tongue dorsum movement in the articulatory domain. We show an interaction between temporal (closure duration) and spatial (tongue dorsum displacement) aspects of lexical stress, which we interpret as the cause of resistance to palatalization in post-tonic velars. The findings are discussed within the μ-gesture framework",
    "checked": true,
    "id": "48acad41f7b7f28f466b90f987ae92223ec833a5",
    "semantic_title": "lexical stress and velar palatalization in italian: a spatio-temporal interaction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23i_interspeech.html": {
    "title": "Speaker Embeddings as Individuality Proxy for Voice Stress Detection",
    "volume": "main",
    "abstract": "Since the mental states of the speaker modulate speech, stress introduced by cognitive or physical loads could be detected in the voice. The existing voice stress detection benchmark has shown that the audio embeddings extracted from the Hybrid BYOL-S self-supervised model perform well. However, the benchmark only evaluates performance separately on each dataset, but does not evaluate performance across the different types of stress and different languages. Moreover, previous studies found strong individual differences in stress susceptibility. This paper presents the design and development of voice stress detection, trained on more than 100 speakers from 9 language groups and five different types of stress. We address individual variabilities in voice stress analysis by adding speaker embeddings to the hybrid BYOL-S features. The proposed method significantly improves voice stress detection performance with an input audio length of only 3-5 seconds",
    "checked": true,
    "id": "24e7803d58a7ed3f6279bb90ccd21e9ba58f11a8",
    "semantic_title": "speaker embeddings as individuality proxy for voice stress detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23j_interspeech.html": {
    "title": "From Interval to Ordinal: A HMM based Approach for Emotion Label Conversion",
    "volume": "main",
    "abstract": "Ordinal labels along affect dimensions are garnering increasing interest in computation paralinguistics. However, they are rarely obtained directly from raters, and instead typically obtained by conversion from interval labels. Current approaches to such conversion map interval labels to either absolute ordinal labels (AOL) (e.g., low and high), or to relative ordinal labels (ROL) (e.g., one has higher arousal than the other), but never take both into account. This paper presents a novel approach to map time-continuous interval labels to time-continuous ordinal labels. It simultaneously considers both inter-rater ambiguity about where AOLs sit on the interval label scale and the consistency amongst different raters in terms of ROLs. We validate the proposed approach by comparing the converted ordinal labels to original interval labels and the categorical labels for the same speech using the publicly available MSP-Podcast and MSP-Conversation corpora",
    "checked": true,
    "id": "0cc978e550da47a48d39020d0618d2439a22f5a4",
    "semantic_title": "from interval to ordinal: a hmm based approach for emotion label conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23l_interspeech.html": {
    "title": "Turbo your multi-modal classification with contrastive learning",
    "volume": "main",
    "abstract": "Contrastive learning has become one of the most impressive approaches for multi-modal representation learning. However, previous multi-modal works mainly focused on cross-modal understanding, ignoring in-modal contrastive learning, which limits the representation of each modality. In this paper, we propose a novel contrastive learning strategy, called Turbo, to promote multi-modal understanding by joint in-modal and cross-modal contrastive learning. Specifically, multi-modal data pairs are sent through the forward pass twice with different hidden dropout masks to get two different representations for each modality. With these representations, we obtain multiple in-modal and cross-modal contrastive objectives for training. Finally, we combine the self-supervised Turbo with the supervised multi-modal classification and demonstrate its effectiveness on two audio-text classification tasks, where the state-of-the-art performance is achieved on a speech emotion recognition benchmark dataset",
    "checked": true,
    "id": "0dc58873c1765110311faf0afeb9dbde8d6fca44",
    "semantic_title": "turbo your multi-modal classification with contrastive learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ioannides23_interspeech.html": {
    "title": "Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition",
    "volume": "main",
    "abstract": "We propose a methodology for information aggregation from the various transformer layer outputs of a generic speech Encoder (e.g. WavLM, HuBERT) for the downstream task of Speech Emotion Recognition (SER). The proposed methodology significantly reduces the dependency of model predictions on linguistic content, while leading to competitive performance without requiring costly Encoder re-training. The proposed paradigm is evaluated via Accuracy, Positive Pointwise Mutual Information, and visualization of the learned attention weights. This methodology generalizes well to a multi-language SER setting in addition to single-language SER, suggesting existing cultural commonalities in the paralinguistic domain between different languages. Experimental results demonstrate this ability by testing our model on unseen languages in a zero-shot fashion, suggesting our proposed method is inclusive in the context of speech and language, thus, making it applicable to a wide audience of speakers",
    "checked": true,
    "id": "469416e547656f2fb1e1218f1ccbaef631ec83c5",
    "semantic_title": "towards paralinguistic-only speech representations for end-to-end speech emotion recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23y_interspeech.html": {
    "title": "SOT: Self-supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In cross-domain speech emotion recognition (SER), reducing the global probability distribution distance (GPDD) between different domains plays a crucial role in unsupervised domain adaptation (UDA), which can be naturally measured by optimal transport (OT). However, owing to the large intra-variations of emotion categories, samples distributed in overlap may induce negative transports. Moreover, OT only considers the GPDD and therefore cannot efficiently transport hard-discriminative samples without utilizing the local structures from intra-class distributions. We propose a self-supervised learning (SSL)-assisted optimal transport (SOT) algorithm for cross-domain SER. First, we regularized OT's transport coupling to mitigate negative transports; then, we designed an SSL module to emphasize local intra-class structure to assist OT in capturing those nontransferable acknowledge. Cross-domain SER experimental results showed that SOT dramatically outperformed state-of-the-art UDAs",
    "checked": true,
    "id": "8de6d819fda2a243da79e20eba9b266d44b7633f",
    "semantic_title": "sot: self-supervised learning-assisted optimal transport for unsupervised adaptive speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bansal23_interspeech.html": {
    "title": "On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition",
    "volume": "main",
    "abstract": "New-age conversational agent systems perform both speech-emotion recognition (SER) and automatic speech recognition (ASR) using two separate and often independent approaches for real-world application in noisy environments. In this paper, we investigate a joint ASR-SER multitask learning approach in a low-resource setting and show that improvements are observed not only in SER but also in ASR. We also investigate the robustness of such jointly trained models to the presence of background noise, babble, and music. Experimental results on the IEMOCAP dataset show that joint learning can improve ASR word error rate (WER) and SER classification accuracy by 10.7% and 2.3% respectively in clean scenarios. In noisy scenarios, results on data augmented with MUSAN show that the joint approach outperforms the independent ASR and SER approaches across many noisy conditions. Overall, the joint ASR-SER approach yielded more noise-resistant models than the independent ASR and SER approaches",
    "checked": true,
    "id": "60bd26b2b926bcc9817506dff9895540a7b0a856",
    "semantic_title": "on the efficacy and noise-robustness of jointly learned speech emotion and automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23b_interspeech.html": {
    "title": "Speaking State Decoder with Transition Detection for Next Speaker Prediction",
    "volume": "main",
    "abstract": "Next speaker prediction and turn change prediction are two important tasks in group interaction and human-agent interaction. In order to carry out a fluent conversation, we need to identify who is currently speaking, who is the next speaker and when the next speaker starts to speak. These questions are computationally designed as the task of next speaker prediction. Behaviors such as gaze direction, speaking prosody or gestures have been modeled to perform this task. In this work, we propose a decoder-based speaking state decoder (SSD) for next speaker prediction, which jointly considers current behavior features, past history of talking and speaking state transition detection model. Our decoder approach achieves next speaker prediction with UAR of 78.11%, which is 3.41% improvement over the champion model in MultiMediate challenge 2021",
    "checked": true,
    "id": "12425c65dcf4bb80e122f73780a3935ce51d5692",
    "semantic_title": "speaking state decoder with transition detection for next speaker prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kitagishi23_interspeech.html": {
    "title": "What are differences? Comparing DNN and Human by Their Performance and Characteristics in Speaker Age Estimation",
    "volume": "main",
    "abstract": "We compare speaker age estimation results obtained by human listeners and a latest deep neural network (DNN) model to reveal differences in their estimation characteristics. A DNN model can achieve high speaker age estimation performance and is expected to be utilized in practical applications. Only a few studies compared speaker-age estimation performance between human listeners and machine learning models. However, the differences in their estimation characteristics have yet to be revealed. Our experimental results reveal that the DNN model performs comparable or superior to the listeners but is more sensitive to elderly speech, acoustic characteristics, and lengths of speech samples than the listeners. The results also reveal that the speakers' gender and some specific acoustic features negatively affect the listeners' estimation performance",
    "checked": true,
    "id": "d311ae30da7189f70c32fae911db719e99ab2e16",
    "semantic_title": "what are differences? comparing dnn and human by their performance and characteristics in speaker age estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arts23_interspeech.html": {
    "title": "Effects of perceived gender on the perceived social function of laughter",
    "volume": "main",
    "abstract": "Previously, the sex of the speaker has been found to play an important role in how we perceive human and artificial voices. It was found, for example, that sex mediates in how we perceive the social function of laughter. We, however, are interested in how socially-formed concepts of gender influence how laughter is perceived. To investigate this, we carried out a within-subjects study of listeners who judged social functions of the same laugh stimulus twice, which was framed as produced by either a man or woman. Mixed-effects ordinal regression modelling showed no statistically significant relations between the perceived gender of a laugh and its perceived social functions",
    "checked": true,
    "id": "e8979be87ba6fabd07afaa9a5147492a8900f053",
    "semantic_title": "effects of perceived gender on the perceived social function of laughter",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/purohit23_interspeech.html": {
    "title": "Implicit phonetic information modeling for speech emotion recognition",
    "volume": "main",
    "abstract": "This study investigates the efficacy of utilizing embedding spaces to model phonetic information in emotion utterances for speech emotion recognition. Our approach involves implicit modeling of phone information by deriving phone-based embeddings from networks specifically trained for phone recognition and pre-trained models fine-tuned for phone/character recognition. The results from evaluating our approach on three speech emotion databases, using both intra-corpus and inter-corpus evaluation methods demonstrate the competitive performance of implicit modeling of phonetic information compared to knowledge-based handcrafted features",
    "checked": true,
    "id": "1ec3a6312aefc68da51618a04c648c64b219a16a",
    "semantic_title": "implicit phonetic information modeling for speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/leem23_interspeech.html": {
    "title": "Computation and Memory Efficient Noise Adaptation of Wav2Vec2.0 for Noisy Speech Emotion Recognition with Skip Connection Adapters",
    "volume": "main",
    "abstract": "An appealing approach for speech emotion recognition (SER) is to pre-train a large speech representation model such as Wav2Vec2.0 or HuBERT. However, this large model should be adapted to different environments when deployed on real-world applications. This approach demands additional training time and stored parameters for each target environment. This paper proposes a computation and memory-efficient adaptation method. The approach trains skip connection adapters that generate environmental representations from the convolutional encoder, and denoise the self-supervised speech representations. Our experiments with the clean and contaminated version of the MSP-Podcast corpus show that our adapter-based approach not only improves the performance of the original fine-tuned SER model, but also reduces the computation and memory requirements. For each environment, the approach requires 59.16% decreased adaptation time and only 0.98% of the parameters of the transformer encoder",
    "checked": true,
    "id": "bcb740386b0b2d3756ec5d948c2f91ae966eff17",
    "semantic_title": "computation and memory efficient noise adaptation of wav2vec2.0 for noisy speech emotion recognition with skip connection adapters",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23b_interspeech.html": {
    "title": "Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) performance deteriorates significantly in the presence of noise, making it challenging to achieve competitive performance in noisy conditions. To this end, we propose a multi-level knowledge distillation (MLKD) method, which aims to transfer the knowledge from a teacher model trained on clean speech to a simpler student model trained on noisy speech. Specifically, we use clean speech features extracted by the wav2vec-2.0 as the learning goal and train the distil wav2vec-2.0 to approximate the feature extraction ability of the original wav2vec-2.0 under noisy conditions. Furthermore, we leverage the multi-level knowledge of the original wav2vec-2.0 to supervise the single-level output of the distil wav2vec-2.0. We evaluate the effectiveness of our proposed method by conducting extensive experiments using five types of noise-contaminated speech on the IEMOCAP dataset, which show promising results compared to state-of-the-art models",
    "checked": true,
    "id": "bfd3a01166217bb5776edfe536c4320699171d19",
    "semantic_title": "multi-level knowledge distillation for speech emotion recognition in noisy conditions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naini23_interspeech.html": {
    "title": "Preference Learning Labels by Anchoring on Consecutive Annotations",
    "volume": "main",
    "abstract": "An important task in human-computer interaction is to rank speech samples according to their expressive content. A preference learning framework is appropriate for obtaining an emotional rank for a set of speech samples. However, obtaining reliable labels for training a preference learning framework is a challenging task. Most existing databases provide sentence-level absolute attribute scores annotated by multiple raters, which have to be transformed to obtain preference labels. Previous studies have shown that evaluators anchor their absolute assessments on previously annotated samples. Hence, this study proposes a novel formulation for obtaining preference learning labels by only considering annotation trends assigned by a rater to consecutive samples within an evaluation session. The experiments show that the use of the proposed anchor-based ordinal labels leads to significantly better performance than models trained using existing alternative labels",
    "checked": true,
    "id": "52ceaffd6a8460f0a20714a516f2a62f79a74d17",
    "semantic_title": "preference learning labels by anchoring on consecutive annotations",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chetiaphukan23_interspeech.html": {
    "title": "Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is a field that has drawn a lot of attention due to its applications in diverse fields. A cur- rent trend in methods used for SER is to leverage embeddings from pre-trained models (PTMs) as input features to down- stream models. However, the use of embeddings from speaker recognition PTMs hasn't garnered much focus in comparison to other PTM embeddings. To fill this gap and in order to understand the efficacy of speaker recognition PTM embed- dings, we perform a comparative analysis of five PTM embed- dings. Among all, x-vector embeddings performed the best possibly due to its training for speaker recognition leading to capturing various components of speech such as tone, pitch, etc. Our modeling approach which utilizes x-vector embed- dings and mel-frequency cepstral coefficients (MFCC) as input features is the most lightweight approach while achieving com- parable accuracy to previous state-of-the-art (SOTA) methods in the CREMA-D benchmark",
    "checked": true,
    "id": "4cbf260c8dd0a401b1577ce22d0d9a5a92f0e50d",
    "semantic_title": "transforming the embeddings: a lightweight technique for speech emotion recognition tasks",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23_interspeech.html": {
    "title": "Learning Local to Global Feature Aggregation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Transformer has emerged in speech emotion recognition (SER) at present. However, its equal patch division not only damages frequency information but also ignores local emotion correlations across frames, which are key cues to represent emotion. To handle the issue, we propose a Local to Global Feature Aggregation learning (LGFA) for SER, which can aggregate long-term emotion correlations at different scales both inside frames and segments with entire frequency information to enhance the emotion discrimination of utterance-level speech features. For this purpose, we nest a Frame Transformer inside a Segment Transformer. Firstly, Frame Transformer is designed to excavate local emotion correlations between frames for frame embeddings. Then, the frame embeddings and their corresponding segment features are aggregated as different-level complements to be fed into Segment Transformer for learning utterance-level global emotion features. Experimental results show that the performance of LGFA is superior to the state-of-the-art methods",
    "checked": true,
    "id": "3a6550e3aee316f68156b1e7d5e4c6caf30b600e",
    "semantic_title": "learning local to global feature aggregation for speech emotion recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23q_interspeech.html": {
    "title": "Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) is a challenging task due to limited data and blurred boundaries of certain emotions. In this paper, we present a comprehensive approach to improve the SER performance throughout the model lifecycle, including pre-training, fine-tuning, and inference stages. To address the data scarcity issue, we utilize a pre-trained model, wav2vec2.0. During fine-tuning, we propose a novel loss function that combines cross-entropy loss with supervised contrastive learning loss to improve the model's discriminative ability. This approach increases the inter-class distances and decreases the intra-class distances, mitigating the issue of blurred boundaries. Finally, to leverage the improved distances, we propose an interpolation method at the inference stage that combines the model prediction with the output from a k-nearest neighbors model. Our experiments on IEMOCAP demonstrate that our proposed methods outperform current state-of-the-art results",
    "checked": true,
    "id": "9a4aa8375fb30114b4d0b358f0c72e306b94f2cb",
    "semantic_title": "supervised contrastive learning with nearest neighbor search for speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pham23b_interspeech.html": {
    "title": "Vietnam-Celeb: a large-scale dataset for Vietnamese speaker recognition",
    "volume": "main",
    "abstract": "The success of speaker recognition systems heavily depends on large training datasets collected under real-world conditions. While common languages like English or Chinese have vastly available datasets, low-resource ones like Vietnamese remain limited. This paper presents a large-scale spontaneous dataset gathered under noisy environments, with over 87,000 utterances from 1,000 Vietnamese speakers of many professions, covering 3 main Vietnamese dialects. To build the dataset, we propose a sophisticated construction pipeline that can also be applied to other languages, with efficient visual-aided processing techniques to boost data precision. With the state-of-the-art x-vector model, training with the proposed dataset shows an average absolute and relative EER improvement of 5.48% and 41.61% when compared to the model trained on VLSP 2021, a publicly available Vietnamese speaker dataset",
    "checked": true,
    "id": "8b673225d277d3928dab3c1725210040c6ea34df",
    "semantic_title": "vietnam-celeb: a large-scale dataset for vietnamese speaker recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23v_interspeech.html": {
    "title": "What Can an Accent Identifier Learn? Probing Phonetic and Prosodic Information in a Wav2vec2-based Accent Identification Model",
    "volume": "main",
    "abstract": "This study is focused on understanding and quantifying the change in phoneme and prosody information encoded in the Self-Supervised Learning (SSL) model, brought by an accent identification (AID) fine-tuning task. This problem is addressed based on model probing. Specifically, we conduct a systematic layer-wise analysis of the representations of the Transformer layers on a phoneme correlation task, and a novel word-level prosody prediction task. We compare the probing performance of the pre-trained and fine-tuned SSL models. Results show that the AID fine-tuning task steers the top 2 layers to learn richer phoneme and prosody representation. These changes share some similarities with the effects of fine-tuning with an Automatic Speech Recognition task. In addition, we observe strong accent-specific phoneme representations in layer 9. To sum up, this study provides insights into the understanding of SSL features and their interactions with fine-tuning tasks",
    "checked": true,
    "id": "f098620d3e19388937839afd9c9c856a821ca3ac",
    "semantic_title": "what can an accent identifier learn? probing phonetic and prosodic information in a wav2vec2-based accent identification model",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23c_interspeech.html": {
    "title": "The 2022 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "In 2022, the U.S. National Institute of Standards and Technology (NIST) conducted the latest Language Recognition Evaluation (LRE) in an ongoing series administered by NIST since 1996 to foster research in language recognition and to measure state-of-the-art technology. Similar to previous LREs, LRE22 focused on conversational telephone speech (CTS) and broadcast narrowband speech (BNBS) data. LRE22 also introduced new evaluation features, such as an emphasis on African languages, including low resource languages, and a test set consisting of segments containing between 3s and 35s of speech randomly sampled and extracted from longer recordings. A total of 21 research organizations, forming 16 teams, participated in this 3-month long evaluation and made a total of 65 valid system submissions to be evaluated. This paper presents an overview of LRE22 and an analysis of system performance over different evaluation conditions. The evaluation results suggest that Oromo and Tigrinya are easier to detect while Xhosa and Zulu are more challenging. A greater confusability is seen for some language pairs. When speech duration increased, system performance significantly increased up to a certain duration, and then a diminishing return on system performance is observed afterward",
    "checked": true,
    "id": "ae444d811900fbd3d211c94683c7660eab521870",
    "semantic_title": "the 2022 nist language recognition evaluation",
    "citation_count": 90,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarni23_interspeech.html": {
    "title": "Description and analysis of the KPT system for NIST Language Recognition Evaluation 2022",
    "volume": "main",
    "abstract": "This paper presents an analysis of the KPT system for the 2022 NIST Language Recognition Evaluation. The KPT submission focuses on the fixed training condition where only specific speech data can be used to develop all the modules and auxiliary systems used to build the language recognizer. Our solution consists of several sub-systems based on different neural network front-ends and a common back-end for classification and fusion. The goal of each front-end is to extract language-related embeddings. Gaussian linear models are used to classify the embeddings of each front-end, followed by multi-class logistic regression to calibrate and fuse the different sub-systems. Experimental results from the NIST LRE 2022 evaluation task show that our approach achieves competitive performance",
    "checked": true,
    "id": "0814d74df7c3d486bcf3b442816251ba5e752147",
    "semantic_title": "description and analysis of the kpt system for nist language recognition evaluation 2022",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yip23_interspeech.html": {
    "title": "ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention",
    "volume": "main",
    "abstract": "In this paper, we propose ACA-Net, a lightweight, global context-aware speaker embedding extractor for Speaker Verification (SV) that improves upon existing work by using Asymmetric Cross Attention (ACA) to replace temporal pooling. ACA is able to distill large, variable-length sequences into small, fixed-sized latents by attending a small query to large key and value matrices. In ACA-Net, we build a Multi-Layer Aggregation (MLA) block using ACA to generate fixed-sized identity vectors from variable-length inputs. Through global attention, ACA-Net acts as an efficient global feature extractor that adapts to temporal variability unlike existing SV models that apply a fixed function for pooling over the temporal dimension which may obscure information about the signal's nonstationary temporal variability. Our experiments on the WSJ0- 1talker show ACA-Net outperforms a strong baseline by 5% relative improvement in EER using only 1/5 of the parameters",
    "checked": true,
    "id": "c83a635dd31bdf7332fddf1dd0767863b5b13ae6",
    "semantic_title": "aca-net: towards lightweight speaker verification using asymmetric cross attention",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yao23_interspeech.html": {
    "title": "Branch-ECAPA-TDNN: A Parallel Branch Architecture to Capture Local and Global Features for Speaker Verification",
    "volume": "main",
    "abstract": "Currently, ECAPA-TDNN is one of the state-of-the-art deep models for automatic speaker verification (ASV). However, it focuses too much on local feature extraction with fixed local ranges, without paying much attention to global feature extraction. To deal with this issue, in this paper, we propose Branch-ECAPA-TDNN, which uses two parallel branches to extract features with various ranges and abstract levels. One branch employs multi-head self-attention to capture long-range dependencies, while the other branch utilizes an SE-Res2Block module to model local multi-scale characteristics. To improve the feature fusion, we further apply different merging methods to aggregate features from both branches. Experimental results demonstrate that the proposed Branch-ECAPA-TDNN achieves a relative EER reduction of 24.10% and 7.92% over ECAPA-TDNN on the VoxCeleb and CN-Celeb datasets, respectively",
    "checked": true,
    "id": "2b8f227dc8187616167bd4267d2e1fada7051790",
    "semantic_title": "branch-ecapa-tdnn: a parallel branch architecture to capture local and global features for speaker verification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23d_interspeech.html": {
    "title": "Speaker Verification Across Ages: Investigating Deep Speaker Embedding Sensitivity to Age Mismatch in Enrollment and Test Speech",
    "volume": "main",
    "abstract": "In this paper, we study the impact of the ageing on modern deep speaker embedding based automatic speaker verification (ASV) systems. We have selected two different datasets to examine ageing on the state-of-the-art ECAPA-TDNN system. The first dataset, used for addressing short-term aging (up to 10 years time difference between enrollment and test) under un-controlled conditions, is VoxCeleb. The second dataset, used for addressing long-term aging effect (up to 40 years difference) of Finnish speakers under a more controlled setup, is Longitudinal Corpus of Finnish Spoken in Helsinki (LCFSH). Our study provides new insights into the impact of speaker ageing on modern ASV systems. Specifically, we establish a quantitative measure between ageing and ASV scores. Further, our research indicates that ageing affects female English speakers to a greater degree than male English speakers, while in the case of Finnish, it has a greater impact on male speakers than female speakers",
    "checked": true,
    "id": "852eaf5fc69a0ec67211824ec94763f54c40a59e",
    "semantic_title": "speaker verification across ages: investigating deep speaker embedding sensitivity to age mismatch in enrollment and test speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dey23_interspeech.html": {
    "title": "Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification",
    "volume": "main",
    "abstract": "Commonly used features in spoken language identification (LID), such as mel-spectrogram or MFCC, lose high-frequency information due to windowing. The loss further increases for longer temporal contexts. To improve generalization of the low-resourced LID systems, we investigate an alternate feature representation, wavelet scattering transform (WST), that compensates for the shortcomings. To our knowledge, WST is not explored earlier in LID tasks. We first optimize WST features for multiple South Asian LID corpora. We show that LID requires low octave resolution and frequency-scattering is not useful. Further, cross-corpora evaluations show that the optimal WST hyper-parameters depend on both train and test corpora. Hence, we develop fused ECAPA-TDNN based LID systems with different sets of WST hyper-parameters to improve generalization for unknown data. Compared to MFCC, EER is reduced upto 14.05% and 6.40% for same-corpora and blind VoxLingua107 evaluations, respectively",
    "checked": true,
    "id": "ae668f5d1570bc46d1cb431d3c9c44b5704f2254",
    "semantic_title": "wavelet scattering transform for improving generalization in low-resourced spoken language identification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/radhakrishnan23_interspeech.html": {
    "title": "A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model",
    "volume": "main",
    "abstract": "In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open-source code at https://github.com/Srijith-rkr/KAUST-Whisper-Adapter",
    "checked": true,
    "id": "27179342d752e044aa9705114026ca7f4a98d2e9",
    "semantic_title": "a parameter-efficient learning approach to arabic dialect identification with pre-trained general-purpose speech model",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tamayoflorez23_interspeech.html": {
    "title": "HABLA: A Dataset of Latin American Spanish Accents for Voice Anti-spoofing",
    "volume": "main",
    "abstract": "Research on improving automatic speaker verification systems to detect speech spoofing has focused mainly on English, with little attention given to other languages creating a significant gap in language coverage. This paper introduces HABLA, the first voice anti-spoofing dataset in the Spanish language including Argentinian, Colombian, Peruvian, Venezuelan, and Chilean accents. The dataset provided by HABLA comprises over 22,000 authentic speech samples from male and female speakers hailing from five distinct Latin American nations as well as 58,000 spoof samples that were generated through the use of six different speech synthesis strategies, including recent voice conversion and text-to-speech algorithms. Finally, initial findings on the efficacy of pre-existing Antispoofing Systems models are presented along with concerns regarding their performance in languages other than English",
    "checked": true,
    "id": "3052be3179f7f6660a397f2a81a8fbc4392b2a4a",
    "semantic_title": "habla: a dataset of latin american spanish accents for voice anti-spoofing",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23aa_interspeech.html": {
    "title": "Self-supervised Learning Representation based Accent Recognition with Persistent Accent Memory",
    "volume": "main",
    "abstract": "Accent recognition (AR) is challenging due to the lack of training data as well as the accents are entangled with speakers and regional characteristics. This paper aims to improve AR performance from two perspectives. First, to alleviate the data insufficiency problem, we employ the self-supervised learning representations (SSLRs) extracted from a pre-trained model to build the AR models. With the help of SSLRs, it gains significant performance improvement compared with the traditional acoustic features. Secondly, we proposed a persistent accent memory (PAM) as contextual knowledge to bias the AR model. The accent embeddings that are extracted from all training data by the encoder of AR models are clustered to form an accent codebook, i.e. PAM. In addition, we propose diverse attention mechanisms to investigate the optimal utilization of PAM. We observe that the best performance is obtained by selecting the most relevant accent embeddings",
    "checked": true,
    "id": "4d5ab653df0e74e0460d2413588d564471eb942a",
    "semantic_title": "self-supervised learning representation based accent recognition with persistent accent memory",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23g_interspeech.html": {
    "title": "Extremely Low Bit Quantization for Mobile Speaker Verification Systems Under 1MB Memory",
    "volume": "main",
    "abstract": "How to develop lightweight systems customized for mobile devices is an urgent and intriguing topic for speaker verification. In this paper, we investigate extremely low bit quantization for small-footprint speaker verification. Specifically, two different binary quantization schemes are proposed, namely static and adaptive quantizer. By applying them to the pre-trained full-precision ResNet, we successfully obtain binarized variants named as b-vector with a model size of under 1MB memory. Experiments on Voxceleb dataset illustrate that compared with the previous best small-footprint system, our best b-vector system achieves 38%, 36% and 30% relative improvements on Vox1-O, E and H respectively, while maintaining almost identical model size. In addition, the analysis of the binarized weight histograms reveals that adaptive quantization scheme, when compared to the static method, can better match the real-valued distribution, and hence presents more effective representation ability",
    "checked": true,
    "id": "42a9a61094b9b30c97c7fc0bc614c0d29a16a600",
    "semantic_title": "extremely low bit quantization for mobile speaker verification systems under 1mb memory",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/das23_interspeech.html": {
    "title": "Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance",
    "volume": "main",
    "abstract": "Dialect classification is used in a variety of applications, such as machine translation and speech recognition, to improve the overall performance of the system. In a real-world scenario, a deployed dialect classification model can encounter anomalous inputs that differ from the training data distribution, also called out-of-distribution (OOD) samples. Those OOD samples can lead to unexpected outputs, as dialects of those samples are unseen during model training. Out-of-distribution detection is a new research area that has received little attention in the context of dialect classification. Towards this, we proposed a simple yet effective unsupervised Mahalanobis distance feature-based method to detect out-of-distribution samples. We utilize the latent embeddings from all intermediate layers of a wav2vec 2.0 transformer-based dialect classifier model for multi-task learning. Our proposed approach outperforms other state-of-the-art OOD detection methods significantly",
    "checked": true,
    "id": "c6d71ea890cccb99626ba6ec9d3928d98704c537",
    "semantic_title": "unsupervised out-of-distribution dialect detection with mahalanobis distance",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bredin23_interspeech.html": {
    "title": "pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe",
    "volume": "main",
    "abstract": "pyannote.audio is an open-source toolkit written in Python for speaker diarization. Version 2.1 introduces a major overhaul of pyannote.audio default speaker diarization pipeline, made of three main stages: speaker segmentation applied to a short sliding window, neural speaker embedding of each (local) speakers, and (global) agglomerative clustering. One of the main objectives of the toolkit is to democratize speaker diarization. Therefore, on top of a pretrained speaker diarization pipeline that gives good results out of the box, we also provide a recipe that practitioners can follow to improve its performance on their own (manually annotated) dataset. It has been used for various challenges and reached 1st place at Ego4D 2022, 1st place at Albayzin 2022, and 6th place at VoxSRC 2022",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23t_interspeech.html": {
    "title": "Model Compression for DNN-based Speaker Verification Using Weight Quantization",
    "volume": "main",
    "abstract": "DNN-based speaker verification (SV) models demonstrate significant performance at relatively high computation costs. Model compression can be applied to reduce the model size for lower resource consumption. The present study exploits weight quantization to compress two widely-used SV models, namely ECAPA-TDNN and ResNet. Experimental results on VoxCeleb show that weight quantization is effective for compressing SV models. The model size can be reduced multiple times without noticeable degradation in performance. Compression of ResNet shows more robust results than ECAPA-TDNN with lower-bitwidth quantization. Analysis of the layer weights suggests that the smooth weight distribution of ResNet may be related to its better robustness. The generalization ability of the quantized model is validated via a language-mismatched SV task. Furthermore, analysis by information probing reveals that the quantized models can retain most of the speaker-relevant knowledge learned by the original models",
    "checked": true,
    "id": "c5c892b85e4cdfff061ce5e0f9c2f5309f352783",
    "semantic_title": "model compression for dnn-based speaker verification using weight quantization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vachhani23_interspeech.html": {
    "title": "Multi-resolution Approach to Identification of Spoken Languages and To Improve Overall Language Diarization System Using Whisper Model",
    "volume": "main",
    "abstract": "This research paper investigates the effectiveness of the Whisper decoder for Language Identification (LI) and Language Diarization (LD) tasks. An audio accent detection system was used as an attention mechanism to narrow down the Whisper LI output classes. The LI system was tested on different audio resolutions ranging from 1.0 to 11.0 seconds, and the segments obtained were combined to generate RTTM per audio resolution. Lastly, we ensemble different multi-resolution diarization systems using DOVER-Lap algorithm. This work was part of DISPLACE challenge organized in INTERSPEECH 2023 and hence the challenge dataset was utilized for all the experiments. It shows that 5-second of audio resolution (i.e.,S-1) yield optimum result of 38.12% and 42.45% DER on development and evaluation data respectively. Furthermore, combining multi-resolution diarization systems (i.e.,S-2) produced an absolute improvement of 3.22% over S-1 and 11.66% over the challenge baseline, with a total DER of 34.9% on the Development set",
    "checked": true,
    "id": "825635531b01812cca1364d5e953799c776819d5",
    "semantic_title": "multi-resolution approach to identification of spoken languages and to improve overall language diarization system using whisper model",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23_interspeech.html": {
    "title": "Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms",
    "volume": "main",
    "abstract": "The ability of countermeasure models to generalize from seen speech synthesis methods to unseen ones has been investigated in the ASVspoof challenge. However, a new mismatch scenario in which fake audio may be generated from real audio with unseen genres has not been studied thoroughly. To this end, we first use five different vocoders to create a new dataset called CN-Spoof based on the CN-Celeb1&2 datasets. Then, we design two auxiliary objectives for regularization via meta-optimization and a genre alignment module, respectively, and combine them with the main anti-spoofing objective using learnable weights for multiple loss terms. The results on our cross-genre evaluation dataset for anti-spoofing show that the proposed method significantly improved the generalization ability of the countermeasures compared with the baseline system in the genre mismatch scenario",
    "checked": true,
    "id": "a24f7d8d68e2142953da0a627cd09e94e253dde2",
    "semantic_title": "improving generalization ability of countermeasures for new mismatch scenario by combining multiple advanced regularization terms",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23b_interspeech.html": {
    "title": "Dynamic Fully-Connected Layer for Large-Scale Speaker Verification",
    "volume": "main",
    "abstract": "Recently, the mainstream x-vector for speaker verification usually adopts a one-hot encoded fully-connected (FC) layer for classification at the training stage. Suppose a large-scale dataset (e.g., one million speakers) is prepared to optimize the network. The unbearable computation cost and memory requirement are mainly from the FC layer. We propose a dynamic fully-connected (Dynamic FC) layer for speaker verification to achieve a tradeoff between hardware resources and system performance. The proposed Dynamic FC uses a dynamic class queue (DCQ) to store a subset of speaker identity centers and uses an identity-based data loading mechanism to realize memory and time savings. The virtue of the proposed method is that the required memory only depends on the size of the DCQ and does not increase with the number of speakers in the training dataset. The proposed method on the VoxCeleb dataset achieves an EER of 2.345% and a minDCF of 0.261 at a low memory and computation cost",
    "checked": true,
    "id": "215d6e789d2a94fa7d298a9471e4cd8104fe4350",
    "semantic_title": "dynamic fully-connected layer for large-scale speaker verification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schroter23b_interspeech.html": {
    "title": "DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "Multi-frame algorithms for single-channel speech enhancement are able to take advantage from short-time correlations within the speech signal. Deep Filtering (DF) was proposed to directly estimate a complex filter in frequency domain to take advantage of these correlations. In this work, we present a real-time speech enhancement demo using DeepFilterNet. DeepFilterNet's efficiency is enabled by exploiting domain knowledge of speech production and psychoacoustic perception. Our model is able to match state-of-the-art speech enhancement benchmarks while achieving a real-time-factor of 0.19 on a single threaded notebook CPU. The framework as well as pretrained weights have been published under an open source license",
    "checked": true,
    "id": "dcb4c9e7c47fec600c3208adaf20e9254f698ea9",
    "semantic_title": "deepfilternet: perceptually motivated real-time speech enhancement",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burkhardt23_interspeech.html": {
    "title": "Nkululeko: Machine Learning Experiments on Speaker Characteristics Without Programming",
    "volume": "main",
    "abstract": "We would like to present Nkululeko, a template based system that lets users perform machine learning experiments in the speaker characteristics domain. It is mainly targeted on users not being familiar with machine learning, or computer programming at all, to being used as a teaching tool or a simple entry level tool to the field of artificial intelligence",
    "checked": true,
    "id": "969fb0afb445f27e635c6a7b3e2df6aea557c482",
    "semantic_title": "nkululeko: machine learning experiments on speaker characteristics without programming",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lemaguer23_interspeech.html": {
    "title": "Sp1NY: A Quick and Flexible Speech Visualisation Tool in Python",
    "volume": "main",
    "abstract": "In this submission, we describe Sp1NY, a Python toolkit to visualise and annotate speech. Inspired by Praat and music notation software, we designed Sp1NY to be accessible and flexible. By introducing a control panel, Sp1NY provides a quick way for the user to interact with it. By focusing Sp1NY only on visualisation and annotation and, by reducing the core of the software to a minimum, we ensure that the software will remain stable. Finally, Sp1NY integrates a plugin mechanism which allows researchers to adapt the tool to their needs",
    "checked": true,
    "id": "c28eb9af3ba947bf68f74de58f513080239b2cae",
    "semantic_title": "sp1ny: a quick and flexible speech visualisation tool in python",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/corkey23_interspeech.html": {
    "title": "Intonation Control for Neural Text-to-Speech Synthesis with Polynomial Models of F0",
    "volume": "main",
    "abstract": "We present a novel, user-friendly approach for controlling patterns of intonation (a fundamental aspect of prosody) within a neural TTS system. This involves concisely representing F0 contours with the coefficients of their Legendre polynomial series expansion, and implementing a model (based on FastPitch) which is conditioned on these sets of coefficients during training. At inference time the model will explicitly predict a coefficient set, or a user (eg. human-in-the-loop) can provide a target coefficient set which audibly alters the intonation of the output speech, based on just a few values. This is particularly effective for intonation transfer: where these coefficient targets are extracted from a ground truth recording, making the synthesised utterance more closely reflect the intonation of the real speaker",
    "checked": false,
    "id": "146efa59a307373b8cfe8f3b06e5624cf7a55dee",
    "semantic_title": "edinburgh research explorer intonation control for neural text-to-speech synthesis with polynomial models of f0",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szekely23b_interspeech.html": {
    "title": "So-to-Speak: An Exploratory Platform for Investigating the Interplay between Style and Prosody in TTS",
    "volume": "main",
    "abstract": "In recent years, numerous speech synthesis systems have been proposed that feature multi-dimensional controllability, generating a level of variability that surpasses traditional TTS systems by orders of magnitude. However, it remains challenging for developers to comprehend and demonstrate the potential of these advanced systems. We introduce So-to-Speak, a customisable interface tailored for showcasing the capabilities of different controllable TTS systems. The interface allows for the generation, synthesis, and playback of hundreds of samples simultaneously, displayed on an interactive grid, with variation both low level prosodic features and high level style controls. To offer insights into speech quality, automatic estimates of MOS scores are presented for each sample. So-to-Speak facilitates the audiovisual exploration of the interaction between various speech features, which can be useful in a range of applications in speech technology",
    "checked": true,
    "id": "0c0b4a31e280b8fe6048d179427b50862cd09fba",
    "semantic_title": "so-to-speak: an exploratory platform for investigating the interplay between style and prosody in tts",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arai23_interspeech.html": {
    "title": "Comparing /b/ and /d/ with a Single Physical Model of the Human Vocal Tract to Visualize Droplets Produced while Speaking",
    "volume": "main",
    "abstract": "The BMW-RL model, a physical model of the human vocal tract that produces /b/, /m/, /w/, /r/, and /l/, has been utilized to investigate not only each single sound but also consonant clusters such as /br/. This model started gaining attention in 2021 in light of the COVID-19 pandemic, as it can demonstrate how droplets are expelled from the lips when producing the /b/ sound by applying a laser sheet. In this study, we redesigned the model to produce both /b/ and /d/, since both are voiced plosives and the only difference is the place of articulation. With the original BMW-RL model, the first half of the tongue rotates and produces /r/ and /l/, while in the newly proposed model, the width of the tongue is wide enough to make a complete closure at the alveolar position for producing /d/. We tested how the different place of articulation affects the ways of expelling droplets by using the single model producing /b/ and /d/ and found that more droplets were expelled with /b/ than /d/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ekstedt23b_interspeech.html": {
    "title": "Show & Tell: Voice Activity Projection and Turn-taking",
    "volume": "main",
    "abstract": "We present Voice Activity Projection (VAP), a model trained on spontaneous spoken dialog with the objective to incrementally predict future voice activity. Similar to a language model, it is trained through self-supervised learning and outputs a probability distribution over discrete states that corresponds to the joint future voice activity of the dialog interlocutors. The model is well-defined over overlapping speech regions, resilient towards microphone \"bleed-over\" and considers the speech of both speakers (e.g., a user and an agent) to provide the most likely next speaker. VAP is a general turn-taking model which can serve as the base for turn-taking decisions in spoken dialog systems, an automatic tool useful for linguistics and conversational analysis, an automatic evaluation metric for conversational text-to-speech models, and possibly many other tasks related to spoken dialog interaction",
    "checked": true,
    "id": "fba111227d84a298690ee760ecfffa77a1e83490",
    "semantic_title": "show & tell: voice activity projection and turn-taking",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cordourier23_interspeech.html": {
    "title": "Real Time Detection of Soft Voice for Speech Enhancement",
    "volume": "main",
    "abstract": "People in remote meetings in open spaces might choose to speak with a restrained voice due to concerns around privacy or disturbing others. Research shows that persons prefer to use soft voice (voice with lower amplitude and pitch, but with harmonic tones in its spectrum) over whispered voice (voice with the lowest amplitude, and no harmonics at all) to avoid being overheard during such calls. We present a lightweight classifier based in a simple feed-forward neural network, which uses normalized Log-Mel spectrum of voice captured by a headset as input, and can detect if the person is using soft voice. This allows to enhance soft voice with more precision and responsiveness than regular amplitude compensation (\"auto-gain\") systems. In this show and tell, we present a real-time demo of the voice classifier. Viewers will see our algorithm detect in real-time soft voice vs other voice types, in a regular PC, with voice captured with a headset",
    "checked": true,
    "id": "ff4853f3404e6b9a4143de691a9bb54ef541aea6",
    "semantic_title": "real time detection of soft voice for speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tanna23_interspeech.html": {
    "title": "Data Augmentation for Diverse Voice Conversion in Noisy Environments",
    "volume": "main",
    "abstract": "Voice conversion (VC) models have demonstrated impressive few-shot conversion quality on the clean, native speech populations they're trained on. However, when source or target speech accents, background noise conditions, or microphone characteristics differ from training, quality voice conversion is not guaranteed. These problems are often left unexamined in VC research, giving rise to frustration in users trying to use pretrained VC models on their own data. We are interested in accent-preserving voice conversion for name pronunciation from self-recorded examples, a domain in which all three of the aforementioned conditions are present, and posit that demonstrating higher performance in this domain correlates with creating VC models that are more usable by otherwise frustrated users. We demonstrate that existing SOTA encoder-decoder VC models can be made robust to these varations and endowed with natural denoising capabilities using more diverse data and simple data augmentation techniques in pretraining",
    "checked": true,
    "id": "9848df593e72aa4374238575b2fdae86d4d1bb7e",
    "semantic_title": "data augmentation for diverse voice conversion in noisy environments",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gogate23_interspeech.html": {
    "title": "Application for Real-time Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "This short paper demonstrates a first of its kind audio-visual (AV) speech enhancement (SE) desktop application that isolates, in real-time, the voice of a target speaker from noisy audio input. The deep neural network model integrated in this application exploits the AV nature of speech from the target speaker to suppress all speech and non-speech background sounds. In the context of a growing need for video conferencing solutions, AV SE enables the practical deployment such technology in challenging acoustic environments with multiple competing background noise sources. In these scenarios, classical audio-only SE typically fails as they are usually trained to isolate speech from non-speech noises. The application comprises a graphical user interface and modules for real-time AV speech acquisition, preprocessing, and enhancement. The participants will experience a significant improvement in the speech quality and intelligibility of a target speaker who will be physically situated in a real noisy environment with a range of real-world noises. Moreover, participants can evaluate the performance of the application with their own voice by recording videos in challenging multi-talker conversational environments",
    "checked": true,
    "id": "ac53d1df85c3ed54ccbae3e9495ad3ec41bd0256",
    "semantic_title": "application for real-time audio-visual speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23d_interspeech.html": {
    "title": "Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction",
    "volume": "main",
    "abstract": "Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method",
    "checked": true,
    "id": "064af570792c61f5cc46814c648fd3969f0999e7",
    "semantic_title": "mitigating the exposure bias in sentence-level grapheme-to-phoneme (g2p) transduction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23_interspeech.html": {
    "title": "Streaming Parrotron for on-device speech-to-speech conversion",
    "volume": "main",
    "abstract": "We present a fully on-device streaming Speech2Speech conversion model that normalizes a given input speech directly to synthesized output speech. Deploying such a model on mobile devices pose significant challenges in terms of memory footprint and computation requirements. We present a streaming-based approach to produce an acceptable delay, with minimal loss in speech conversion quality, when compared to a reference state of the art non-streaming approach. Our method consists of first streaming the encoder in real time while the speaker is speaking. Then, as soon as the speaker stops speaking, we run the spectrogram decoder in streaming mode along the side of a streaming vocoder to generate output speech. To achieve an acceptable delay-quality trade-off, we propose a novel hybrid approach for look-ahead in the encoder which combines a look-ahead feature stacker with a look-ahead self-attention. We show that our streaming approach is 2x faster than real time on the Pixel4 CPU",
    "checked": true,
    "id": "1f89e67be1fc3bdcd8e6de807068f33f183aafd2",
    "semantic_title": "streaming parrotron for on-device speech-to-speech conversion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shaheen23_interspeech.html": {
    "title": "Exploiting Emotion Information in Speaker Embeddings for Expressive Text-to-Speech",
    "volume": "main",
    "abstract": "Text-to-Speech (TTS) systems have recently seen great progress in synthesizing high-quality speech. However, the prosody of generated utterances often is not as diverse as prosody of the natural speech. In the case of multi-speaker or voice cloning systems, this problem becomes even worse as information about prosody may be present in the input text and the speaker embedding. In this paper, we study the phenomenon of the presence of emotional information in speaker embeddings recently revealed for i-vectors and x-vectors. We show that the produced embeddings may include devoted components encoding prosodic information. We further propose a technique for finding such components and generating emotional speaker embeddings by manipulating them. We then demonstrate that the emotional TTS system based on the proposed method shows good performance and has a smaller number of trained parameters compared to solutions based on fine-tuning",
    "checked": true,
    "id": "e9f0608602e1d9cc17edd1f80f8a598b4de33a0c",
    "semantic_title": "exploiting emotion information in speaker embeddings for expressive text-to-speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/okamoto23b_interspeech.html": {
    "title": "E2E-S2S-VC: End-To-End Sequence-To-Sequence Voice Conversion",
    "volume": "main",
    "abstract": "This paper proposes end-to-end (E2E) non-autoregressive sequence-to-sequence (S2S) voice conversion (VC) models that extend two E2E text-to-speech models, VITS and JETS. In the proposed E2E-S2S-VC models, VITS-VC and JETS-VC, the input text sequences of VITS and JETS are replaced by the source speaker's acoustic feature sequences, and E2E models (including HiFi-GAN waveform synthesizers) are trained using monotonic alignment search (MAS) without external aligners. To successfully train MAS for VC, the proposed models use a reduction factor only for the encoder. The voice of a source speaker is converted directly to that of a target speaker using a single neural network in the proposed models in an S2S manner; the duration and prosody between the source and target speech can be directly converted. The results of experiments using 1,000 parallel utterances of Japanese male and female speakers demonstrate that the proposed JETS-VC outperformed cascade non-autoregressive S2S VC models",
    "checked": true,
    "id": "338f92e4041fd832f2454e68afe9d31453587674",
    "semantic_title": "e2e-s2s-vc: end-to-end sequence-to-sequence voice conversion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23f_interspeech.html": {
    "title": "DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer",
    "volume": "main",
    "abstract": "Despite the huge successes made in neutral TTS, content-leakage remains a challenge. In this paper, we propose a new input representation and simple architecture to achieve improved prosody modeling. Inspired by the recent success in the use of discrete code in TTS, we introduce discrete code to the input of the reference encoder. Specifically, we leverage the vector quantizer from the audio compression model to exploit the diverse acoustic information it has already been trained on. In addition, we apply the modified MLP-Mixer to the reference encoder, making the architecture lighter. As a result, we train the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of our method through both subjective and objective evaluations. We demonstrate that the reference encoder learns better speaker-independent prosody when discrete code is utilized as input in the experiments. In addition, we obtain comparable results even when fewer parameters are inputted",
    "checked": true,
    "id": "9f1516735be1648a2f283528eba7ffb7067761fd",
    "semantic_title": "dc comix tts: an end-to-end expressive tts with discrete code collaborated with mixer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baas23_interspeech.html": {
    "title": "Voice Conversion With Just Nearest Neighbors",
    "volume": "main",
    "abstract": "Any-to-any voice conversion aims to transform source speech into a target voice with just a few examples of the target speaker as a reference. Recent methods produce convincing conversions, but at the cost of increased complexity – making results difficult to reproduce and build on. Instead, we keep it simple. We propose k-nearest neighbors voice conversion (kNN-VC): a straightforward yet effective method for any-to-any conversion. First, we extract self-supervised representations of the source and reference speech. To convert to the target speaker, we replace each frame of the source representation with its nearest neighbor in the reference. Finally, a pretrained vocoder synthesizes audio from the converted representation. Objective and subjective evaluations show that kNN-VC improves speaker similarity with similar intelligibility scores to existing methods. Code, samples, trained models: https://bshall.github.io/knn-vc",
    "checked": true,
    "id": "cc5c8defe3023a45b7425375c3ebda5fccbfcb66",
    "semantic_title": "voice conversion with just nearest neighbors",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tanaka23_interspeech.html": {
    "title": "CFVC: Conditional Filtering for Controllable Voice Conversion",
    "volume": "main",
    "abstract": "This paper describes a many-to-many voice conversion model that filters the speaker vector to control high-level attributes such as speaking rate while preserving voice timbre. In order to control only the speaking rate, it is essential to decompose the speaker vector into a speaking rate vector and others. The challenge is to train such disentangled representations with no/few annotation data. Motivated by this difficulty, we propose an approach combining the conditional filtering method with data augmentation. The experimental results showed that our method disentangled complex attributes without annotation and separately controlled speaking rate and voice timbre. Audio samples can be accessed on our web page",
    "checked": true,
    "id": "e99758ec9cea00717475c87fac118fc10ae68506",
    "semantic_title": "cfvc: conditional filtering for controllable voice conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ning23_interspeech.html": {
    "title": "DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding",
    "volume": "main",
    "abstract": "Real-time applications require voice conversion models with streaming conversion capabilities, and streaming voice conversion faces significant challenges due to limited future information. To address this challenge, we propose DualVC, a dual-mode neural voice conversion approach that supports both streaming and non-streaming modes using jointly trained separate network parameters. Furthermore, we propose intra-model knowledge distillation and hybrid predictive coding (HPC) to enhance the performance of streaming conversion.Additionally, we incorporate data augmentation to train a noise-robust autoregressive decoder, improving the model's performance on long-form speech conversion. Experimental results demonstrate that the proposed model outperforms the baseline models in the context of streaming voice conversion, while maintaining comparable performance to the non-streaming topline system that leverages the complete context, albeit with a latency of only 252ms",
    "checked": true,
    "id": "cfa76e619225b0689dbccfa4c361911e209f8ba2",
    "semantic_title": "dualvc: dual-mode voice conversion using intra-model knowledge distillation and hybrid predictive coding",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23_interspeech.html": {
    "title": "Attention-based Interactive Disentangling Network for Instance-level Emotional Voice Conversion",
    "volume": "main",
    "abstract": "Emotional Voice Conversion aims to manipulate a speech according to a given emotion while preserving non-emotion components. Existing approaches cannot well express fine-grained emotional attributes. In this paper, we propose an Attention-based Interactive diseNtangling Network (AINN) that leverages instance-wise emotional knowledge for voice conversion. We introduce a two-stage pipeline to effectively train our network: Stage I utilizes inter-speech contrastive learning to model fine-grained emotion and intra-speech disentanglement learning to better separate emotion and content. In Stage II, we propose to regularize the conversion with a multi-view consistency mechanism. This technique helps us transfer fine-grained emotion and maintain speech content. Extensive experiments show that our AINN outperforms state-of-the-arts in both objective and subjective metrics",
    "checked": true,
    "id": "038cde0278fc3f25e208ba537c796e39179bebf2",
    "semantic_title": "attention-based interactive disentangling network for instance-level emotional voice conversion",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23p_interspeech.html": {
    "title": "ALO-VC: Any-to-any Low-latency One-shot Voice Conversion",
    "volume": "main",
    "abstract": "This paper presents ALO-VC, a non-parallel low-latency one-shot phonetic posteriorgrams (PPGs) based voice conversion method. ALO-VC enables any-to-any voice conversion using only one utterance from the target speaker, with only 47.5 ms future look-ahead. The proposed hybrid signal processing and machine learning pipeline combines a pre-trained speaker encoder, a pitch predictor to predict the converted speech's prosody, and positional encoding to convey the phoneme's location information. We introduce two system versions: ALO-VC-R, which uses a pre-trained d-vector speaker encoder, and ALO-VC-E, which improves performance using the ECAPA-TDNN speaker encoder. The experimental results demonstrate both ALO-VC-R and ALO-VC-E can achieve comparable performance to non-causal baseline systems on the VCTK dataset and two out-of-domain datasets. Furthermore, both proposed systems can be deployed on a single CPU core with 55 ms latency and 0.78 real-time factor. Our demo is available online",
    "checked": true,
    "id": "5830528b8eda61534d7912f3b3be51b62ee93cb1",
    "semantic_title": "alo-vc: any-to-any low-latency one-shot voice conversion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/minixhofer23_interspeech.html": {
    "title": "Evaluating and reducing the distance between synthetic and real speech distributions",
    "volume": "main",
    "abstract": "While modern Text-to-Speech (TTS) systems can produce natural-sounding speech, they remain unable to reproduce the full diversity found in natural speech data. We consider the distribution of all possible real speech samples that could be generated by these speakers alongside the distribution of all synthetic samples that could be generated for the same set of speakers, using a particular TTS system. We set out to quantify the distance between real and synthetic speech via a range of utterance-level statistics related to properties of the speaker, speech prosody and acoustic environment. Differences in the distribution of these statistics are evaluated using the Wasserstein distance. We reduce these distances by providing ground-truth values at generation time, and quantify the improvements to the overall distribution distance, approximated using an automatic speech recognition system. Our best system achieves a 10% reduction in distribution distance",
    "checked": true,
    "id": "da7c0e5afb152b35defb1ee6a6926503c648f36a",
    "semantic_title": "evaluating and reducing the distance between synthetic and real speech distributions",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/quamer23_interspeech.html": {
    "title": "Decoupling Segmental and Prosodic Cues of Non-native Speech through Vector Quantization",
    "volume": "main",
    "abstract": "Accent conversion (AC) seeks to transform utterances from a non-native speaker to appear native-like. Compared to voice conversion, which generally treats accent and voice quality as one, AC provides a finer-grained decomposition of speech. This paper presents an AC system that further decomposes an accent into its segmental and prosodic characteristics, and provides independent control of both channels. The system uses conventional modules (acoustic model, speaker/prosody encoders, seq2seq model) to generate accent conversions that combine (1) the segmental characteristics from a source utterance, (2) the voice characteristics from a target utterance, and (3) the prosody of a reference utterance. However, naive application of this idea prevents the system from learning and transferring prosody. We show that vector quantization and removal of repeated codewords allows the system to transfer prosody and improve voice similarity, as verified by objective and perceptual measures",
    "checked": true,
    "id": "65ed82c466e2228658a8dd23fed56c0a181f04b9",
    "semantic_title": "decoupling segmental and prosodic cues of non-native speech through vector quantization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kanagawa23_interspeech.html": {
    "title": "VC-T: Streaming Voice Conversion Based on Neural Transducer",
    "volume": "main",
    "abstract": "A conventional sequence-to-sequence voice conversion (seq2seq VC), i.e., attentional encoder-decoder, can be trained without the speech sequence pre-aligning normally used to counter the different lengths of the source and target speakers. However, if alignments rendered by attention are not monotonic, speech drops and repeats will happen, and the linguistic contents will not be kept. To address this issue, we propose VC-T, a novel streaming VC framework based on a neural transducer (RNNT); RNNT is effective in the automatic speech recognition field as it offers robust alignment against collapse. We also introduce an alignment design scheme for VC-T training. Experiments show that our offline and streaming VC-T variants outperform two modern seq2seq parallel VCs while offering a lower character error rate as a result of the proposal robust alignment. Our VC-T also achieves better naturalness the drastic degradation suffered by the conventional alternatives, especially for streaming VC",
    "checked": true,
    "id": "4cd3e8bb37b02295c98e86f03552f888cc57b581",
    "semantic_title": "vc-t: streaming voice conversion based on neural transducer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghosh23_interspeech.html": {
    "title": "Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion-Preserving Voice Conversion",
    "volume": "main",
    "abstract": "Speech anonymisation prevents misuse of spoken data by removing any personal identifier while preserving at least linguistic content. However, emotion preservation is crucial for natural human-computer interaction. The well-known voice conversion technique StarGANv2-VC achieves anonymisation but fails to preserve emotion. This work presents an any-to-many semi-supervised StarGANv2-VC variant trained on partially emotion-labelled non-parallel data. We propose emotion-aware losses computed on the emotion embeddings and acoustic features correlated to emotion. Additionally, we use an emotion classifier to provide direct emotion supervision. Objective and subjective evaluations show that the proposed approach significantly improves emotion preservation over the vanilla StarGANv2-VC. This considerable improvement is seen over diverse datasets, emotions, target speakers, and inter-group conversions without compromising intelligibility and anonymisation",
    "checked": true,
    "id": "0105c8443d029e857c08f8cb9d9ca4a5b1abb1a6",
    "semantic_title": "emo-stargan: a semi-supervised any-to-many non-parallel emotion-preserving voice conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23r_interspeech.html": {
    "title": "ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed",
    "volume": "main",
    "abstract": "Recent advancements in neural speech synthesis have renewed interest in voice conversion (VC) to go beyond timbre transfer. Achieving controllability of para-linguistic parameters like pitch and speed is crucial in various applications. However, existing studies either lack interpretability or only provide global control at the utterance level. This paper introduces ControlVC, the first neural voice conversion system to enable time-varying controls on pitch and speed. ControlVC uses pre-trained encoders to generate pitch and linguistic embeddings, combined and converted to speech using a vocoder. Speed control is achieved by TD-PSOLA pre-processing, while pitch control is achieved by manipulating the pitch contour before feeding it into the encoder. Systematic subjective and objective evaluations show that this work significantly outperforms self-constructed baselines on speech quality and controllability for non-parallel zero-shot conversion while achieving time-varying control",
    "checked": true,
    "id": "6270e7e4a33b0c53c4b92f3755d599d318b760a1",
    "semantic_title": "controlvc: zero-shot voice conversion with time-varying controls on pitch and speed",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23e_interspeech.html": {
    "title": "Reverberation-Controllable Voice Conversion Using Reverberation Time Estimator",
    "volume": "main",
    "abstract": "Recent trends have emerged to implement voice conversion (VC) in real-world scenarios where background sounds and reverberation are inevitable. However, most VC studies mainly focus on clean speech conversion, where high-quality speech data are required for training and testing. Moreover, the background sounds and reverberation are treated as interferences to be discarded, despite being informative to be retained in some scenarios, such as movie dubbing and singing VC. In this paper, we propose a reverberation-robust VC framework consisting of a reverberation time (T60) estimation module and a VC module. The T60 estimator is introduced to provide the VC module with the reverberation information to model the reverberant speech. Experimental results show that 1) our framework can disentangle and control the speaker identity and reverberation from the speech, and 2) we can get acceptable VC performances dealing with reverberation, even when clean training data are not available",
    "checked": true,
    "id": "b5edf316b94a95ee63e5bfc35116248bddc958ee",
    "semantic_title": "reverberation-controllable voice conversion using reverberation time estimator",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23d_interspeech.html": {
    "title": "Cross-utterance Conditioned Coherent Speech Editing",
    "volume": "main",
    "abstract": "Text-based speech editing systems are developed to enable users to modify speech based on the transcript. Existing state-of-the-art editing systems based on neural networks do partial inferences with no exception, that is, only generate new words that need to be replaced or inserted. This manner usually leads to the prosody of the edited part being inconsistent with the surrounding speech and a failure to handle the alteration of intonation. To address these problems, we propose a cross-utterance conditioned coherent speech editing system, that first does the entire reasoning at the inference time. Our proposed system can generate speech by utilizing speaker information, context, acoustic features, and the mel-spectrogram from the original audio. Experiments conducted on subjective and objective metrics demonstrate that our approach outperforms the baseline on various editing operations regarding naturalness and prosody consistency",
    "checked": true,
    "id": "6e0d7809007b9699e20530e3cee092315e2de43e",
    "semantic_title": "cross-utterance conditioned coherent speech editing",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23o_interspeech.html": {
    "title": "MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information",
    "volume": "main",
    "abstract": "Audio-visual speech recognition (AVSR) gains increasing attention from researchers as an important part of human-computer interaction. However, the existing available Mandarin audio-visual datasets are limited and lack the depth information. To address this issue, this work establishes the MAVD, a new large-scale Mandarin multimodal corpus comprising 12,484 utterances spoken by 64 native Chinese speakers. To ensure the dataset covers diverse real-world scenarios, a pipeline for cleaning and filtering the raw text material has been developed to create a well-balanced reading material. In particular, the latest data acquisition device of Microsoft, Azure Kinect is used to capture depth information in addition to the traditional audio signals and RGB images during data acquisition. We also provide a baseline experiment, which could be used to evaluate the effectiveness of the dataset. The dataset and code will be released at https://github.com/SpringHuo/MAVD",
    "checked": true,
    "id": "2eefbcb4fabf8804940a89bc62559860628a5942",
    "semantic_title": "mavd: the first open large-scale mandarin audio-visual dataset with depth information",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23y_interspeech.html": {
    "title": "CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition",
    "volume": "main",
    "abstract": "Audio-visual person recognition (AVPR) has received extensive attention. However, most datasets used for AVPR research so far are collected in constrained environments, and thus cannot reflect the true performance of AVPR systems in real-world scenarios. To meet the request for research on AVPR in unconstrained conditions, this paper presents a multi-genre AVPR dataset collected 'in the wild', named CN-Celeb-AV. This dataset contains more than 420k video segments from 1,136 persons from public media. In particular, we put more emphasis on two real-world complexities: (1) data in multiple genres; (2) segments with partial information. A comprehensive study was conducted to compare CN-Celeb-AV with two popular public AVPR benchmark datasets, and the results demonstrated that CN-Celeb-AV is more in line with real-world scenarios and can be regarded as a new benchmark dataset for AVPR research. The dataset also involves a development set that can be used to boost the performance of AVPR systems in real-life situations. The dataset is free for researchers and can be downloaded from http://cnceleb.org/",
    "checked": true,
    "id": "f95f276ee1b753be80032b2ab36bb8e7aa28a103",
    "semantic_title": "cn-celeb-av: a multi-genre audio-visual dataset for person recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ca_interspeech.html": {
    "title": "Improving Zero-shot Cross-domain Slot Filling via Transformer-based Slot Semantics Fusion",
    "volume": "main",
    "abstract": "Slot filling is an essential component in task-oriented dialogue systems. Due to the scarcity of annotated data, zero-shot slot filling has been studied to transfer knowledge from source domains to a target domain. Previous methods adopt slot descriptions or questions as slot semantics, where they utilize slot descriptions to calculate similarity scores, or reformat the task as a question-answering problem. However, these methods do not fully exploit the token-level dependency between the slot semantics and utterances. In this study, we propose a Transformer-based Slot semantics fusion method for Slot Filling (TSSF). We first adopt two encoders with shared weights to obtain the representations of utterances and slot semantics. Then, we design a transformer-based fusion module for effectively integrating slot semantics into utterances. Experimental results on the public benchmark SNIPS show that our model significantly outperforms the state-of-the-art model by 6.09% in terms of slot F1",
    "checked": true,
    "id": "d7af953e4ede906dd677257a902d22b5ead13cdb",
    "semantic_title": "improving zero-shot cross-domain slot filling via transformer-based slot semantics fusion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shin23_interspeech.html": {
    "title": "Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer",
    "volume": "main",
    "abstract": "The performance of automated audio captioning (AAC) has been improved considerably through a transformer-based encoder and transfer learning. However, their performance improvement is constrained by the following problems: (1) discrepancy in the input patch size between pretraining and fine-tuning steps. (2) lack of local-level relations between inputs and captions. In this paper, we propose a simple transfer learning scheme that maintains input patch sizes, unlike previous methods, to avoid input discrepancies. Furthermore, we propose a patch-wise keyword estimation branch that utilizes an attention pooling method to effectively represent both global- and local-level information. The results on the AudioCaps dataset reveal that the proposed learning scheme and method considerably contribute to performance gain. Finally, the visualization results demonstrate that the proposed attention-pooling method effectively detects local-level information in the AAC system",
    "checked": true,
    "id": "bf7469a65ae60077a2bc478e594a3203d82d0268",
    "semantic_title": "rethinking transfer and auxiliary learning for improving audio captioning transformer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23c_interspeech.html": {
    "title": "Boosting Punctuation Restoration with Data Generation and Reinforcement Learning",
    "volume": "main",
    "abstract": "Punctuation restoration is an important task in automatic speech recognition (ASR) which aim to restore the syntactic structure of generated ASR texts to improve readability. While punctuated texts are abundant from written documents, the discrepancy between written punctuated texts and ASR texts limits the usability of written texts in training punctuation restoration systems for ASR texts. This paper proposes a reinforcement learning method to exploit in-topic written texts and recent advances in large pre-trained generative language models to bridge this gap. The experiments show that our method achieves state-of-the-art performance on the ASR test set on two benchmark datasets for punctuation restoration. The source code of this work is publicly accessible at https://github.com/laiviet/pr-rl",
    "checked": true,
    "id": "d32b8a89d7385aa7db6e95ba7d920cff8bc746ff",
    "semantic_title": "boosting punctuation restoration with data generation and reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23e_interspeech.html": {
    "title": "J-ToneNet: A Transformer-based Encoding Network for Improving Tone Classification in Continuous Speech via F0 Sequences",
    "volume": "main",
    "abstract": "Currently, tone classification studies mainly focus on training classifiers by using intrinsic features of isolated segments, i.e. often the syllables. Mostly, the works are not merely in use of fundamental frequency (f0) but utilizing more information on the spectrograms, MFCCs, or energy to improve model accuracy. However, as we know, more challenges on tone classification lie on modeling the complex f0 variations from the tonal coarticulations and the interactive effects among tonality in continuous speech. To tackle down this issue, we first aim at in using the sequence of f0 samples in speech utterance only. In addition, we propose a transformer based network with an extendable BERT input architecture and a joint learning technique to consolidate the contour representations of consecutive tones. Leveraging or fusing more information affected from speech rhythm in utterance, the experiments show that the proposed J-ToneNet is very robust for read speech",
    "checked": true,
    "id": "a0dd2506538ad0655154108d5cc2fc0d708be935",
    "semantic_title": "j-tonenet: a transformer-based encoding network for improving tone classification in continuous speech via f0 sequences",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/avila23_interspeech.html": {
    "title": "Towards Cross-Language Prosody Transfer for Dialog",
    "volume": "main",
    "abstract": "Speech-to-speech translation systems today do not adequately support use for dialog purposes. In particular, nuances of speaker intent and stance can be lost due to improper prosody transfer. We present an exploration of what needs to be done to overcome this. First, we developed a data collection protocol in which bilingual speakers re-enact utterances from an earlier conversation in their other language, and used this to collect an English-Spanish corpus, so far comprising 1871 matched utterance pairs. Second, we developed a simple prosodic dissimilarity metric based on Euclidean distance over a broad set of prosodic features. We then used these to investigate cross-language prosodic differences, measure the likely utility of three simple baseline models, and identify phenomena which will require more powerful modeling. Our findings should inform future research on cross-language prosody and the design of speech-to-speech translation systems capable of effective prosody transfer",
    "checked": true,
    "id": "e4b977595a3e96581642059402ef657133c6f1ae",
    "semantic_title": "towards cross-language prosody transfer for dialog",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kesiraju23_interspeech.html": {
    "title": "Strategies for Improving Low Resource Speech to Text Translation Relying on Pre-trained ASR Models",
    "volume": "main",
    "abstract": "This paper presents techniques and findings for improving the performance of low-resource speech to text translation (ST). We conducted experiments on both simulated and real-low resource setups, on language pairs English - Portuguese, and Tamasheq - French respectively. Using the encoder-decoder framework for ST, our results show that a multilingual automatic speech recognition system acts as a good initialization under low-resource scenarios. Furthermore, using the CTC as an additional objective for translation during training and decoding helps to reorder the internal representations and improves the final translation. Through our experiments, we try to identify various factors (initializations, objectives, and hyper-parameters) that contribute the most for improvements in low-resource setups. With only 300 hours of pre-training data, our model achieved 7.3 BLEU score on Tamasheq - French data, outperforming prior published works from IWSLT 2022 by 1.6 points",
    "checked": true,
    "id": "f12b04da6de0995583d4e8aea21a5fefc54ecd2b",
    "semantic_title": "strategies for improving low resource speech to text translation relying on pre-trained asr models",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koudounas23_interspeech.html": {
    "title": "ITALIC: An Italian Intent Classification Dataset",
    "volume": "main",
    "abstract": "Recent large-scale Spoken Language Understanding datasets focus predominantly on English and do not account for language-specific phenomena such as particular phonemes or words in different lects. We introduce ITALIC, the first large-scale speech dataset designed for intent classification in Italian. The dataset comprises 16,521 crowdsourced audio samples recorded by 70 speakers from various Italian regions and annotated with intent labels and additional metadata. We explore the versatility of ITALIC by evaluating current state-of-the-art speech and text models. Results on intent classification suggest that increasing scale and running language adaptation yield better speech models, monolingual text models outscore multilingual ones, and that speech recognition on ITALIC is more challenging than on existing Italian benchmarks. We release both the dataset and the annotation scheme to streamline the development of new Italian SLU models and language-specific datasets",
    "checked": true,
    "id": "7d8ce3eefbddeb72c1a1c78cc8a775d96426c81c",
    "semantic_title": "italic: an italian intent classification dataset",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rugayan23_interspeech.html": {
    "title": "Perceptual and Task-Oriented Assessment of a Semantic Metric for ASR Evaluation",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems have become a vital part of our everyday lives through their many applications. However, as much as we have developed in this regard, our most common evaluation method for ASR systems still remains to be word error rate (WER). WER does not give information on the severity of errors, which strongly impacts practical performance. As such, we examine a semantic-based metric called Aligned Semantic Distance (ASD) against WER and demonstrate its advantage over WER in two facets. First, we conduct a survey asking participants to score reference text and ASR transcription pairs. We perform a correlation analysis and show that ASD is more correlated to the human evaluation scores compared to WER. We also explore the feasibility of predicting human perception using ASD. Second, we demonstrate that ASD is more effective than WER as an indicator of performance on downstream NLP tasks such as named entity recognition and sentiment classification",
    "checked": true,
    "id": "babdea20aa5eed770cde3e7946574dccc90c8d37",
    "semantic_title": "perceptual and task-oriented assessment of a semantic metric for asr evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23s_interspeech.html": {
    "title": "How ChatGPT is Robust for Spoken Language Understanding?",
    "volume": "main",
    "abstract": "Large language models (LLMs), e.g. ChatGPT, have shown super performance on various NLP tasks. There is a doubt whether these LLMs, which are trained on a large corpus of written text, can show the robustness of understanding the spoken text. Therefore in this paper, we give a detailed investigation of robust spoken language understanding (SLU) with ChatGPT. In our experiments, we evaluate ChatGPT on two sets of public datasets, Spoken SQuAD and ASR-GLUE, in which there are ASR errors in the text. Quantitative and qualitative analyses on the experimental results are conducted to show that ChatGPT not only performs very well for SLU tasks but also can recover some ASR errors with its super reason ability",
    "checked": true,
    "id": "fc667b8705a35a637142dc1d9270326ebc952475",
    "semantic_title": "how chatgpt is robust for spoken language understanding?",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ye23b_interspeech.html": {
    "title": "GigaST: A 10,000-hour Pseudo Speech Translation Corpus",
    "volume": "main",
    "abstract": "This paper introduces GigaST, a large-scale pseudo speech-to-text translation (ST) corpus. We create the corpus by translating the transcript in GigaSpeech, an English ASR corpus, into German and Chinese. The training set is translated by a strong machine translation system and the test set is translated by human. ST models trained with an addition of our corpus obtain new state-of-the-art results on the MuST-C English-German benchmark test set. We provide a detailed description of the translation process and verify its quality. We make the translated text data public and hope to facilitate research in speech translation. Additionally, we also release the training scripts on NeurST1 to make it easy to replicate our systems. GigaST dataset is available at https://st-benchmark.github.io/resources/GigaST",
    "checked": false,
    "id": "141d1e80096b0fa67904257b8270afa3d5180510",
    "semantic_title": "gigast: a 10, 000-hour pseudo speech translation corpus",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fan23b_interspeech.html": {
    "title": "Boosting Chinese ASR Error Correction with Dynamic Error Scaling Mechanism",
    "volume": "main",
    "abstract": "Chinese Automatic Speech Recognition (ASR) error correction presents significant challenges due to the Chinese language's unique features, including a large character set and borderless, morpheme-based structure. Current mainstream models often struggle with effectively utilizing word-level features and phonetic information. This paper introduces a novel approach that incorporates a dynamic error scaling mechanism to detect and correct phonetically erroneous text generated by ASR output. This mechanism operates by dynamically fusing word-level features and phonetic information, thereby enriching the model with additional semantic data. Furthermore, our method implements unique error reduction and amplification strategies to address the issues of matching wrong words caused by incorrect characters. Experimental results indicate substantial improvements in ASR error correction, demonstrating the effectiveness of our proposed method and yielding promising results on established datasets",
    "checked": true,
    "id": "8358530fae3ed689f9a688c6f1077a95f6e9f96b",
    "semantic_title": "boosting chinese asr error correction with dynamic error scaling mechanism",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fallgren23_interspeech.html": {
    "title": "Crowdsource-based Validation of the Audio Cocktail as a Sound Browsing Tool",
    "volume": "main",
    "abstract": "We conduct two crowdsourcing experiments designed to examine the usefulness of audio cocktails to quickly find out information on the contents of large audio data. Several thousand crowd workers were engaged to listen to audio cocktails with systematically varied composition. They were then asked to state either which sound out of four categories (Children, Women, Men, Orchestra) they heard the most of, or if they heard anything of a specific category at all. The results show that their responses have high reliability and provide information as to whether a specific task can be performed using audio cocktails. We also propose that the combination of crowd workers and audio cocktails can be used directly as a tool to investigate the contents of large audio data",
    "checked": true,
    "id": "309ec568cea3b99123545ace000924212d055d59",
    "semantic_title": "crowdsource-based validation of the audio cocktail as a sound browsing tool",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23z_interspeech.html": {
    "title": "PunCantonese: A Benchmark Corpus for Low-Resource Cantonese Punctuation Restoration from Speech Transcripts",
    "volume": "main",
    "abstract": "Punctuation restoration from unsegmented speech transcripts is an essential task to improve the readability of transcripts and can facilitate various downstream NLP tasks. However, there is still lack of systematic studies on punctuation restoration for Cantonese as a low-resource language. This paper introduces a new Cantonese punctuation corpus named PunCantonese, which consists of annotated spoken transcripts and written-style Wikipedia sentences, covering the major punctuations such as \",.?!\" and code-switched sentences in Cantonese and English. We also propose a Transformer-based punctuation model which exploits pre-trained multilingual language models, adopts multitask learning for style and punctuation prediction, and introduces a novel Jyutping embedding layer to inject the phonetic features not explicitly available in Cantonese characters. Experimental results show that these methods are effective in improving punctuation restoration, and the Jyutping embedding layer brings an absolute F1 increase by more than 2%",
    "checked": true,
    "id": "c20f97155221a72625bfd6c6572d97b26138fce1",
    "semantic_title": "puncantonese: a benchmark corpus for low-resource cantonese punctuation restoration from speech transcripts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kato23_interspeech.html": {
    "title": "Speech-to-Face Conversion Using Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "Speech-to-face conversion is the task of generating face images from speech signals. Many studies have been conducted to address this task, and achieved good performances. In this paper, we introduce denoising diffusion probabilistic models (DDPMs) to generate face images instead of generative adversarial networks (GANs) or autoencoders, which are used in most of the prior studies. Moreover, unlike prior studies, several components of our system are designed to use high-resolution face image datasets instead of audio-visual paired data. As a result, our system can generate high-resolution face images from speech signals with an architecture that is simpler and more flexible than the ones used in prior studies. In addition, introducing DDPMs enables us to utilize techniques that control out- puts of DDPMs or improve performance of them in succeeding studies",
    "checked": true,
    "id": "573eb81bf5d37fb10588fafa33ccd852d2ca65ca",
    "semantic_title": "speech-to-face conversion using denoising diffusion probabilistic models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nishikawa23_interspeech.html": {
    "title": "Inter-connection: Effective Connection between Pre-trained Encoder and Decoder for Speech Translation",
    "volume": "main",
    "abstract": "In end-to-end speech translation, speech and text pre-trained models improve translation quality. Recently proposed models simply connect the pre-trained models of speech and text as encoder and decoder. Therefore, only the information from the final layer of encoders is input to the decoder. Since it is clear that the speech pre-trained model outputs different information from each layer, the simple connection method cannot fully utilize the information that the speech pre-trained model has. In this study, we propose an inter-connection mechanism that aggregates the information from each layer of the speech pre-trained model by weighted sums and inputs into the decoder. This mechanism increased BLEU by approximately 2 points in en-de, en-ja, and en-zh by increasing parameters by 2K when the speech pre-trained model was frozen. Furthermore, we investigated the contribution of each layer for each language by visualizing layer weights and found that the contributions were different",
    "checked": true,
    "id": "14a01b6001c5301a2d89a86a9a35d4013fc17cae",
    "semantic_title": "inter-connection: effective connection between pre-trained encoder and decoder for speech translation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/radfar23_interspeech.html": {
    "title": "Conmer: Streaming Conformer Without Self-attention for Interactive Voice Assistants",
    "volume": "main",
    "abstract": "Conformer is an extension of transformer-based neural ASR models whose fundamental component is the self-attention module. In this paper, we show that we can remove the self-attention module from Conformer and achieve the same or even better recognition performance for utterances whose length is up to around 10 seconds. This is particularly important for streaming interactive voice assistants as input is often very short and a fast response is expected. Since the computational complexity of self-attention is quadratic, this modification allows for faster, smaller sized models, two requirements for on-device applications. Using this finding, we propose Conmer, a neural architecture based on Conformer but without self-attention for streaming interactive voice assistants. We conduct experiments on public and real-world data and show the streaming Conmer reduces the WER and computational complexity relatively by 4.03% and 10%, respectively",
    "checked": true,
    "id": "7bd6a6d55c55f5b66ab04f2f5fa6659bad928c68",
    "semantic_title": "conmer: streaming conformer without self-attention for interactive voice assistants",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23e_interspeech.html": {
    "title": "Intra-ensemble: A New Method for Combining Intermediate Outputs in Transformer-based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Deep learning models employ various regularization techniques to prevent overfitting and enhance generalization. In particular, an auxiliary loss, as proposed for connectionist temporal classification (CTC) models, demonstrated the potential for intermediate prediction to be useful by enabling sub-models to recognize speech accurately. We propose a new method called Intra-ensemble, which combines these accurate intermediate outputs into a single output for both training and inference, considering the importance of the intermediate layer using learnable parameters. Our approach is applicable to CTC models, attention-based encoder-decoder models, and transducer structures and demonstrated performance improvements of 13.5%, 3.0%, and 4.1% respectively, in the LibriSpeech evaluation. Furthermore, through various analytical experiments, we found that the sub-models contributed significantly to performance improvement",
    "checked": true,
    "id": "16f9d44d9b83e806837276dddb3967af0d0596b6",
    "semantic_title": "intra-ensemble: a new method for combining intermediate outputs in transformer-based automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23b_interspeech.html": {
    "title": "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks",
    "volume": "main",
    "abstract": "Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community",
    "checked": true,
    "id": "14f5fd91d75bc10d9fff53dfe7ee73484fc4273b",
    "semantic_title": "a comparative study on e-branchformer vs conformer in speech recognition, translation, and understanding tasks",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mai23_interspeech.html": {
    "title": "HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition",
    "volume": "main",
    "abstract": "State-of-the-art ASR systems have achieved promising results by modeling local and global interactions separately. While the former can be computed efficiently, global interactions are usually modeled via attention mechanisms, which are expensive for long input sequences. Here, we address this by extending HyperMixer, an efficient alternative to attention exhibiting linear complexity, to the Conformer architecture for speech recognition, leading to HyperConformer. In particular, multi-head HyperConformer achieves comparable or higher recognition performance while being more efficient than Conformer in terms of inference speed, memory, parameter count, and available training data. HyperConformer achieves a word error rate of 2.9% on LibriSpeech test-clean with less than 8M neural parameters and a peak memory during training of 5.7GB, hence trainable with accessible hardware. Inference speed is between 38% on mid-length speech and 56% on long speech faster than an equivalent Conformer",
    "checked": true,
    "id": "6702329634ed6876fc03bcc8ea68bdbf34c254e4",
    "semantic_title": "hyperconformer: multi-head hypermixer for efficient speech recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/carvalho23_interspeech.html": {
    "title": "Memory-augmented conformer for improved end-to-end long-form ASR",
    "volume": "main",
    "abstract": "Conformers have recently been proposed as a promising modelling approach for automatic speech recognition (ASR), outperforming recurrent neural network-based approaches and transformers. Nevertheless, in general, the performance of these end-to-end models, especially attention-based models, is particularly degraded in the case of long utterances. To address this limitation, we propose adding a fully-differentiable memory-augmented neural network between the encoder and decoder of a conformer. This external memory can enrich the generalization for longer utterances since it allows the system to store and retrieve more information recurrently. Notably, we explore the neural Turing machine (NTM) that results in our proposed Conformer-NTM model architecture for ASR. Experimental results using Librispeech train-clean-100 and train-960 sets show that the proposed system outperforms the baseline conformer without memory for long utterances",
    "checked": true,
    "id": "c1ca1e4953bd0a3ce553fae7f954813575b13416",
    "semantic_title": "memory-augmented conformer for improved end-to-end long-form asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23_interspeech.html": {
    "title": "Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems",
    "volume": "main",
    "abstract": "Current ASR systems are mainly trained and evaluated at the utterance level. Long range cross utterance context can be incorporated. A key task is to derive a suitable compact representation of the most relevant history contexts. In contrast to previous researches based on either LSTM-RNN encoded histories that attenuate the information from longer range contexts, or frame level concatenation of transformer context embeddings, in this paper compact low-dimensional cross utterance contextual features are learned in the Conformer-Transducer Encoder using specially designed attention pooling layers that are applied over efficiently cached preceding utterances' history vectors. Experiments on the 1000-hr Gigaspeech corpus demonstrate that the proposed contextualized streaming Conformer-Transducers outperform the baseline using utterance internal context only with statistically significant WER reductions of 0.7% to 0.5% absolute (4.3% to 3.1% relative) on the dev and test data",
    "checked": true,
    "id": "1abae0addac14b4889fa4396a31b407ffd3f22c5",
    "semantic_title": "towards effective and compact contextual representation for conformer transducer speech recognition systems",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23o_interspeech.html": {
    "title": "An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification",
    "volume": "main",
    "abstract": "Effective fusion of multi-scale features is crucial for improving speaker verification performance. While most existing methods aggregate multi-scale features in a layer-wise manner via simple operations, such as summation or concatenation. This paper proposes a novel architecture called Enhanced Res2Net (ERes2Net), which incorporates both local and global feature fusion techniques to improve the performance. The local feature fusion (LFF) fuses the features within one single residual block to extract the local signal. The global feature fusion (GFF) takes acoustic features of different scales as input to aggregate global signal. To facilitate effective feature fusion in both LFF and GFF, an attentional feature fusion module is employed in the ERes2Net architecture, replacing summation or concatenation operations. A range of experiments conducted on the VoxCeleb datasets demonstrate the superiority of the ERes2Net in speaker verification",
    "checked": true,
    "id": "db22321ee7316104b7e69cca6babcfbc7ee0f4b3",
    "semantic_title": "an enhanced res2net with local and global feature fusion for speaker verification",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23x_interspeech.html": {
    "title": "A Study on Visualization of Voiceprint Feature",
    "volume": "main",
    "abstract": "Despite the remarkable success of convolutional neural networks (CNNs) in voiceprint recognition, we still lack a comprehensive understanding of the specific features extracted by these models. To address this issue, we adopt an attribution approach in this paper to explain the voiceprint identification model and visualize the relevant features. Using five attribution methods, we successfully identify the features extracted by the ECAPA-TDNN model and confirm the reliability of our attribution techniques.We also explore two distinct methods for visualizing voiceprint features, with one approach aimed at interpreting features in unknown speech and the other focused on known speech. Through the attribution method, we are able to more precisely capture voiceprint features within speech data without significantly impacting the performance of the voiceprint recognition model. It would help us to do a more detailed study of the voiceprint features in the future",
    "checked": true,
    "id": "02945f6eaa65762651cf9f409e1e7e5d53469257",
    "semantic_title": "a study on visualization of voiceprint feature",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yakovlev23_interspeech.html": {
    "title": "VoxTube: a multilingual speaker recognition dataset",
    "volume": "main",
    "abstract": "The objective of this paper is to advance the development of technologies in the fields of speaker recognition and speaker identification by introducing a large labeled audio database VoxTube collected from the open-source media. We propose a fully automated unsupervised approach for audio labeling that requires any pre-trained speaker recognition model. Collected with this approach from videos with CC BY license the VoxTube dataset contains more than 5.000 speakers with more than 4 million utterances pronounced in more than 10 languages. In our paper we show the VoxTube's high generalization ability across multiple domains by evaluating the accuracy metrics on various speaker recognition benchmarks. We also show how well this dataset complements an already existing VoxCeleb2 dataset",
    "checked": true,
    "id": "1a67c57dcf3f0d4517b5abe2509870e586191e65",
    "semantic_title": "voxtube: a multilingual speaker recognition dataset",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23p_interspeech.html": {
    "title": "Visualizing Data Augmentation in Deep Speaker Recognition",
    "volume": "main",
    "abstract": "Visualization is of great value in understanding the internal mechanisms of neural networks. Previous work found that LayerCAM is a reliable visualization tool for deep speaker models. In this paper, we use LayerCAM to analyze the widely-adopted data augmentation (DA) approach, to understand how it leads to model robustness. We conduct experiments on the VoxCeleb1 dataset for speaker identification, which shows that both vanilla and activation-based (Act) DA approaches enhance robustness against interference, with Act DA being consistently superior. Visualization with LayerCAM suggests DA helps models learn to delete temporal-frequency (TF) bins that are corrupted by interference. The ‘learn to delete' behavior explained why DA models are more robust than clean models, and why the Act DA is superior over the vanilla DA when the interference is non-target speech. However, LayerCAM still cannot clearly explain the superiority of Act DA in other situations, suggesting further research",
    "checked": true,
    "id": "c40b9b1e751d7e9cddcfbfa53e03d9c8943e2433",
    "semantic_title": "visualizing data augmentation in deep speaker recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ba_interspeech.html": {
    "title": "Fast and Efficient Multilingual Self-Supervised Pre-training for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "Recent advances in self-supervised learning (SSL) have remarkably improved speech recognition performance for low-resource languages. On the other hand, with data of an increasingly larger scale required for SSL, the pretraining process has become extremely time-consuming. To address this problem, we propose an unsupervised data selection method based on utterance-level language similarity and a curriculum learning strategy to boost the efficiency of multilingual SSL pretraining while maintaining performance. We conduct experiments on five languages in COMMONVOICE dataset. Compared to the baseline with all data for pretraining, we pretrained on only 25% of the data and saved 60% of the training steps with even better performance on the target low-resource language",
    "checked": true,
    "id": "f570c974088d8aa621ecdb16334a9c94e15d1c52",
    "semantic_title": "fast and efficient multilingual self-supervised pre-training for low-resource speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23z_interspeech.html": {
    "title": "UniSplice: Universal Cross-Lingual Data Splicing for Low-Resource ASR",
    "volume": "main",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) has made remarkable progress thanks to the abundant annotated data for a few rich-resource languages. However, data scarcity remains a challenge for the majority of the world's languages. To address this issue, we propose UniSplice, a novel cross-lingual speech synthesis framework based on data splicing that leverages self-supervised learning (SSL) units from Hidden Unit BERT (HuBERT) as universal phonetic units. Our approach involves splicing speech fragments from rich-resource languages into complete speech that conforms acoustically to text from low-resource languages. UniSplice eliminates the need for computationally expensive neural text-to-speech (TTS) models, enabling the training of ASR models using on-the-fly synthesized speech. Experimental results on the COMMON-VOICE dataset show 20-30% relative improvement for four Indo-European languages and about 15% for Turkish with a 4-gram language model for rescoring, in a 10-hour low-resource setup",
    "checked": true,
    "id": "db4eb16e11488479d28a750df53032f1c4a3362c",
    "semantic_title": "unisplice: universal cross-lingual data splicing for low-resource asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/glocker23_interspeech.html": {
    "title": "Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes",
    "volume": "main",
    "abstract": "This paper proposes Allophant, a multilingual phoneme recognizer. It requires only a phoneme inventory for cross-lingual transfer to a target language, allowing for low-resource recognition. The architecture combines a compositional phone embedding approach with individually supervised phonetic attribute classifiers in a multi-task architecture. We also introduce Allophoible, an extension of the PHOIBLE database. When combined with a distance based mapping approach for grapheme-to-phoneme outputs, it allows us to train on PHOIBLE inventories directly. By training and evaluating on 34 languages, we found that the addition of multi-task learning improves the model's capability of being applied to unseen phonemes and phoneme inventories. On supervised languages we achieve phoneme error rate improvements of 11 percentage points (pp.) compared to a baseline without multi-task learning. Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of 2.63 pp. over the baseline",
    "checked": true,
    "id": "340a65d3bd9d2e12edc2ee77041451cf41049409",
    "semantic_title": "allophant: cross-lingual phoneme recognition with articulatory attributes",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23_interspeech.html": {
    "title": "Phonetic-assisted Multi-Target Units Modeling for Improving Conformer-Transducer ASR system",
    "volume": "main",
    "abstract": "Exploiting effective target modeling units is very important and has always been a concern in end-to-end automatic speech recognition (ASR). In this work, we propose a phonetic-assisted multi-target units (PMU) modeling approach, to enhance the Conformer-Transducer ASR system in a progressive representation learning manner. Specifically, PMU first uses the pronunciation-assisted subword modeling (PASM) and byte pair encoding (BPE) to produce phonetic-induced and text-induced target units separately; Then, three new frameworks are investigated to enhance the acoustic encoder, including a basic PMU, a paraCTC and a paCTC, they integrate the PASM and BPE units at different levels for CTC and transducer multi-task training. Experiments on both LibriSpeech and accented ASR tasks show that, the proposed PMU significantly outperforms the conventional BPE, it reduces the WER of LibriSpeech clean, other, and six accented ASR testsets by relative 12.7%, 4.3% and 7.7%, respectively",
    "checked": true,
    "id": "276d8a49c459635693d63a5de26735b3d34a3e50",
    "semantic_title": "phonetic-assisted multi-target units modeling for improving conformer-transducer asr system",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rouditchenko23_interspeech.html": {
    "title": "Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages",
    "volume": "main",
    "abstract": "Recent models such as XLS-R and Whisper have made multilingual speech technologies more accessible by pre-training on audio from around 100 spoken languages each. However, there are thousands of spoken languages worldwide, and adapting to new languages is an important problem. In this work, we aim to understand which model adapts better to languages unseen during pre-training. We fine-tune both models on 13 unseen languages and 18 seen languages. Our results show that the number of hours seen per language and language family during pre-training is predictive of how the models compare, despite the significant differences in the pre-training methods",
    "checked": true,
    "id": "5f5dd6a960b61d978253856bbb487c81cee16ce3",
    "semantic_title": "comparison of multilingual self-supervised and weakly-supervised speech pre-training for adaptation to unseen languages",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ea_interspeech.html": {
    "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
    "volume": "main",
    "abstract": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models",
    "checked": true,
    "id": "100d625fcc926cbce2282197a920d589b3eae11c",
    "semantic_title": "distilxlsr: a light weight cross-lingual speech representation model",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23b_interspeech.html": {
    "title": "Emotional Voice Conversion with Semi-Supervised Generative Modeling",
    "volume": "main",
    "abstract": "Emotional Voice Conversion (EVC) is a task that aims to convert the emotional state of speech from one to another while preserving the linguistic information and identity of the speaker. However, many studies are limited by the requirement for parallel speech data between different emotional patterns, which is not widely available in real-life applications. Furthermore, the annotation of emotional data is highly time-consuming and labor-intensive. To address these problems, in this paper, we propose SGEVC, a novel semi-supervised generative model for emotional voice conversion. This paper demonstrates that using as little as 1% supervised data is sufficient to achieve EVC. Experimental results show that our proposed model achieves state-of-the-art (SOTA) performance and consistently outperforms EVC baseline frameworks",
    "checked": true,
    "id": "27c4d28829b7c089f5c7e6e5fd79e5c865cd7eac",
    "semantic_title": "emotional voice conversion with semi-supervised generative modeling",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23d_interspeech.html": {
    "title": "Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation",
    "volume": "main",
    "abstract": "Although voice conversion (VC) systems have shown a remarkable ability to transfer voice style, existing methods still have an inaccurate pitch and low speaker adaptation quality. To address these challenges, we introduce Diff-HierVC, a hierarchical VC system based on two diffusion models. We first introduce DiffPitch, which can effectively generate F0 with the target voice style. Subsequently, the generated F0 is fed to DiffVoice to convert the speech with a target voice style. Furthermore, using the source-filter encoder, we disentangle the speech and use the converted Mel-spectrogram as a data-driven prior in DiffVoice to improve the voice style transfer capacity. Finally, by using the masked prior in diffusion models, our model can improve the speaker adaptation quality. Experimental results verify the superiority of our model in pitch generation and voice style transfer performance, and our model also achieves a CER of 0.83% and EER of 3.29% in zero-shot VC scenarios",
    "checked": true,
    "id": "69f1ed9c41db6ab3fd505c5cc6fdcc3ee1233b04",
    "semantic_title": "diff-hiervc: diffusion-based hierarchical voice conversion with robust pitch generation and masked prior for zero-shot speaker adaptation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23_interspeech.html": {
    "title": "S2CD: Self-heuristic Speaker Content Disentanglement for Any-to-Any Voice Conversion",
    "volume": "main",
    "abstract": "In this paper, we propose a Selfheuristic Speaker Content Disentanglement (S2CD) model for any to any voice conversion without using any external resources, e.g., speaker labels or vectors, linguistic models, and transcriptions. S2CD is built on the disentanglement sequential variational autoencoder (DSVAE), but improves DSVAE at the model architecture level from three perspectives. Specifically, we develop different structures for speaker and content encoders based on their underlying static/dynamic property. We further propose a generative graph, modelled by S2CD, so as to make S2CD well mimic the multi-speaker speech generation process. Finally, we propose a self-heuristic way to introduce bias to the prior modelling. Extensive empirical evaluations show the effectiveness of S2CD for any to any voice conversion",
    "checked": true,
    "id": "bc5ceb87dd6f77111ee1707ef71251915ce8ff89",
    "semantic_title": "s2cd: self-heuristic speaker content disentanglement for any-to-any voice conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23g_interspeech.html": {
    "title": "Flow-VAE VC: End-to-End Flow Framework with Contrastive Loss for Zero-shot Voice Conversion",
    "volume": "main",
    "abstract": "Voice conversion (VC) seeks to modify one speaker's voice to generate speech as if it came from another speaker. It is challenging especially when source and target speakers are unseen during training (zero-shot VC). Recent work in this area made progress with disentanglement methods that separate utterance content and speaker characteristics from speech audio recordings. However, these models either lack adequate disentanglement ability or rely on the use of a trained vocoder to reconstruct the speech from acoustic features. We propose Flow-VAE VC, which is an end-to-end system processing directly on the raw audio waveform for zero-shot tasks. Flow-VAE VC adopts a conditional Variational Autoencoder (VAE) with normalizing flows and an adversarial training process to improve the expressive power of generative modeling. Specifically, we learn context-invariant representations by applying frame-level contrastive loss to speech different augment samples. The experiments show that the proposed method achieves a decent performance on zero-shot voice conversion and significantly improves converted speech naturalness and speaker similarity. Readers can get the source code and listen to some audio samples on the demo webpage",
    "checked": true,
    "id": "0c24b4199b42557ff6b0b54c87281b58cbff464f",
    "semantic_title": "flow-vae vc: end-to-end flow framework with contrastive loss for zero-shot voice conversion",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23s_interspeech.html": {
    "title": "Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation",
    "volume": "main",
    "abstract": "Voice Conversion (VC) converts the voice of a source speech to that of a target while maintaining the source's content. Speech can be mainly decomposed into four components: content, timbre, rhythm and pitch. Unfortunately, most related works only take into account content and timbre, which results in less natural speech. Some recent works are able to disentangle speech into several components, but they require laborious bottleneck tuning or various hand-crafted features, each assumed to contain disentangled speech information. In this paper, we propose a VC model that can automatically disentangle speech into four components using only two augmentation functions, without the requirement of multiple hand-crafted features or laborious bottleneck tuning. The proposed model is straightforward yet efficient, and the empirical results demonstrate that our model can achieve a better performance than the baseline, regarding disentanglement effectiveness and speech naturalness",
    "checked": true,
    "id": "76933d7de40927127a313911d97f2061de36ea69",
    "semantic_title": "automatic speech disentanglement for voice conversion using rank module and speech augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23b_interspeech.html": {
    "title": "End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions",
    "volume": "main",
    "abstract": "Zero-shot voice conversion is becoming an increasingly popular research topic, as it promises the ability to transform speech to sound like any speaker. However, relatively little work has been done on end-to-end methods for this task, which are appealing because they remove the need for a separate vocoder to generate audio from intermediate features. In this work, we propose LVC-VC, an end-to-end zero-shot voice conversion model that uses location-variable convolutions (LVCs) to jointly model the conversion and speech synthesis processes. LVC-VC utilizes carefully designed input features that have disentangled content and speaker information, and it uses a neural vocoder-like architecture that utilizes LVCs to efficiently combine them and perform voice conversion while directly synthesizing time domain audio. Experiments show that our model achieves especially well balanced performance between voice style transfer and speech intelligibility compared to several baselines",
    "checked": true,
    "id": "5ee405675554a192ee452b3bf9d12b65fcbd582a",
    "semantic_title": "end-to-end zero-shot voice conversion with location-variable convolutions",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/braun23_interspeech.html": {
    "title": "Classifying Dementia in the Presence of Depression: A Cross-Corpus Study",
    "volume": "main",
    "abstract": "Automated dementia screening enables early detection and intervention, reducing costs to healthcare systems and increasing quality of life for those affected. Depression has shared symptoms with dementia, adding complexity to diagnoses. The research focus so far has been on binary classification of dementia (DEM) and healthy controls (HC) using speech from picture description tests from a single dataset. In this work, we apply established baseline systems to discriminate cognitive impairment in speech from the semantic Verbal Fluency Test and the Boston Naming Test using text, audio and emotion embeddings in a 3-class classification problem (HC vs. MCI vs. DEM). We perform cross-corpus and mixed-corpus experiments on two independently recorded German datasets to investigate generalization to larger populations and different recording conditions. In a detailed error analysis, we look at depression as a secondary diagnosis to understand what our classifiers actually learn",
    "checked": true,
    "id": "0404aebc2dda1b968ea12ccc8d77b4ba829fd954",
    "semantic_title": "classifying dementia in the presence of depression: a cross-corpus study",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23b_interspeech.html": {
    "title": "Exploiting Cross-Domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "Articulatory features (AFs) are inherently invariant to acoustic signal distortion. Their practical application to atypical domains such as elderly, disordered speech across languages is limited by data scarcity. This paper presents a cross-domain and cross-lingual Acoustic-to-Articulatory (A2A) inversion approach that utilizes the parallel audio and ultrasound tongue imaging (UTI) data of the 24-hour TaL corpus in A2A model training before being adapted to three datasets: the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora; and the English TORGO dysarthric speech data, to produce UTI based AFs. Experiments suggest incorporating the generated AFs consistently outperforms the baseline TDNN/Conformer ASR systems using acoustic features only by statistically significant word/character error rate reductions up to 4.75%, 2.59% and 2.07% absolute (14.69%, 10.64% and 22.72% relative) after data augmentation, speaker adaptation and cross system multi-pass decoding",
    "checked": true,
    "id": "72a6db85488881e312f1d366fd670f326567b5c7",
    "semantic_title": "exploiting cross-domain and cross-lingual ultrasound tongue imaging features for elderly and dysarthric speech recognition",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wagner23_interspeech.html": {
    "title": "Multi-class Detection of Pathological Speech with Latent Features: How does it perform on unseen data?",
    "volume": "main",
    "abstract": "The detection of pathologies from speech features is usually defined as a binary classification task with one class representing a specific pathology and the other class representing healthy speech. In this work, we train neural networks, large margin classifiers, and tree boosting machines to distinguish between four pathologies: Parkinson's disease, laryngeal cancer, cleft lip and palate, and oral squamous cell carcinoma. We show that latent representations extracted at different layers of a pre-trained wav2vec 2.0 system can be effectively used to classify these types of pathological voices. We evaluate the robustness of our classifiers by adding room impulse responses to the test data and by applying them to unseen speech corpora. Our approach achieves unweighted average F1-Scores between 74.1% and 97.0%, depending on the model and the noise conditions used. The systems generalize and perform well on unseen data of healthy speakers sampled from a variety of different sources",
    "checked": true,
    "id": "d2b4d7208dd5c564262801ff90e64dd99c7d2324",
    "semantic_title": "multi-class detection of pathological speech with latent features: how does it perform on unseen data?",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kothare23_interspeech.html": {
    "title": "Responsiveness, Sensitivity and Clinical Utility of Timing-Related Speech Biomarkers for Remote Monitoring of ALS Disease Progression",
    "volume": "main",
    "abstract": "In this study, we describe the responsiveness of timing-related measures extracted from read speech in persons with ALS (pALS) collected via a remote patient monitoring platform in an effort to quantify how long it takes to detect a clinically-meaningful change associated with disease progression. We found that the timing alignment of pALS speech relative to a canonical elicitation of the same prompt is the most responsive measure, of the ones considered in this study, at detecting such change in both pALS with bulbar (n = 35) and non-bulbar onset (n = 94). We further evaluated the sensitivity of speech metrics in tracking disease progression in pALS while their ALSFRS-R speech score remained unchanged at 3 out of a total possible score of 4. We observed that timing-related speech metrics showed significant longitudinal changes even after accounting for learning effects. The findings of this study have the potential to inform disease prognosis and functional outcomes of clinical trials",
    "checked": true,
    "id": "164985ced1ba9de4f05b103ce0e93ac4da17635a",
    "semantic_title": "responsiveness, sensitivity and clinical utility of timing-related speech biomarkers for remote monitoring of als disease progression",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geng23b_interspeech.html": {
    "title": "Use of Speech Impairment Severity for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "A key challenge in dysarthric speech recognition is the speaker-level diversity attributed to both speaker-identity related factors (e.g. gender) and speech impairment severity. Most prior researches on addressing this issue focused on using speaker-identity only. To this end, this paper proposes a novel set of techniques to use both severity and speaker-identity in dysarthric speech recognition: a) multitask training incorporating severity prediction error; b) speaker-severity aware auxiliary feature adaptation; and c) structured LHUC transforms separately conditioned on speaker-identity and severity. Experiments conducted on UASpeech suggest incorporating additional speech impairment severity into state-of-the-art hybrid DNN, E2E Conformer and pre-trained Wav2vec 2.0 ASR systems produced statistically significant WER reductions up to 4.78% (14.03% relative). Using the best system the lowest published WER of 17.82% (51.25% on very low intelligibility) was obtained on UASpeech",
    "checked": true,
    "id": "9f4844c79226025f5bb61f2ee62fca8ffdaf3cce",
    "semantic_title": "use of speech impairment severity for dysarthric speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mosuily23_interspeech.html": {
    "title": "MMLung: Moving Closer to Practical Lung Health Estimation using Smartphones",
    "volume": "main",
    "abstract": "Long-term respiratory illnesses like Chronic Obstructive Pulmonary Disease (COPD) and Asthma are commonly diagnosed with the gold standard spirometry, which is a lung health test that requires specialized equipment and trained healthcare experts, making it expensive and difficult to scale. Moreover, blowing into a spirometer can be quite hard for people suffering from pulmonary illnesses. To solve the aforementioned limitations, we introduce MMLung, an approach that leverages information obtained from multiple audio signals by combining multiple tasks and different modalities performed on the microphone of a smartphone to estimate lung function. Our proposed approach achieves the best mean absolute percentage error (MAPE) of 1.3% on a cohort of 40 participants. Compared to the reported performances (5%-10% MAPE) on lung health estimation using smartphones, MMLung shows that practical lung health estimation is viable by combining multiple tasks utilizing multiple modalities",
    "checked": true,
    "id": "dff184f8b52489c6d53fefca7dfa4e739ba5ec68",
    "semantic_title": "mmlung: moving closer to practical lung health estimation using smartphones",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23i_interspeech.html": {
    "title": "Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization",
    "volume": "main",
    "abstract": "Large-scale pre-training has been a successful strategy for training transformer models. However, maintaining a large clinical dataset for pre-training is not always possible, and access to data in this domain can be time-limited and costly. We explore using synthetic data in pre-training sequence-to-sequence (seq-to-seq) transformer models to generate clinical notes from Doctor-Patient-Conversations (DoPaCos). Using a generative language model fine-tuned on authentic conversations, a synthetic DoPaCo dataset was created and used with a corpus of clinical notes to pre-train a Longformer-Encoder-Decoder (LED) model. Results show that synthetic data leads to comparable performance in the downstream summarization task compared to pre-training with authentic data. Pre-training on synthetic conversations first, followed by clinical notes, yields higher performance across most of our evaluation metrics",
    "checked": true,
    "id": "bad2eef437f08e7c67e9455134327328e5a3e528",
    "semantic_title": "investigating the utility of synthetic data for doctor-patient conversation summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23pa_interspeech.html": {
    "title": "Non-uniform Speaker Disentanglement For Depression Detection From Raw Speech Signals",
    "volume": "main",
    "abstract": "While speech-based depression detection methods that use speaker-identity features, such as speaker embeddings, are popular, they often compromise patient privacy. To address this issue, we propose a speaker disentanglement method that utilizes a non-uniform mechanism of adversarial SID loss maximization. This is achieved by varying the adversarial weight between different layers of a model during training. We find that a greater adversarial weight for the initial layers leads to performance improvement. Our approach using the ECAPA-TDNN model achieves an F1-score of 0.7349 (a 3.7% improvement over audio-only SOTA) on the DAIC-WoZ dataset, while simultaneously reducing the speaker-identification accuracy by 50%. Our findings suggest that identifying depression through speech signals can be accomplished without placing undue reliance on a speaker's identity, paving the way for privacy-preserving approaches of depression detection",
    "checked": true,
    "id": "67fb87e9475118e09863291c34241823b701f49a",
    "semantic_title": "non-uniform speaker disentanglement for depression detection from raw speech signals",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/demir23_interspeech.html": {
    "title": "PoCaPNet: A Novel Approach for Surgical Phase Recognition Using Speech and X-Ray Images",
    "volume": "main",
    "abstract": "Surgical phase recognition is a challenging and necessary task for the development of context-aware intelligent systems that can support medical personnel for better patient care and effective operating room management. In this paper, we present a surgical phase recognition framework that employs a Multi-Stage Temporal Convolution Network using speech and X-Ray images for the first time. We evaluate our proposed approach using our dataset that comprises 31 port-catheter placement operations and report 82.56 % frame-wise accuracy with eight surgical phases. Additionally, we investigate the design choices in the temporal model and solutions for the class-imbalance problem. Our experiments demonstrate that speech and X-Ray data can be effectively utilized for surgical phase recognition, providing a foundation for the development of speech assistants in operating rooms of the future",
    "checked": true,
    "id": "e3769ece1c4fcc201aa455b8e1f4ecd85399bf70",
    "semantic_title": "pocapnet: a novel approach for surgical phase recognition using speech and x-ray images",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/neumann23_interspeech.html": {
    "title": "Combining Multiple Multimodal Speech Features into an Interpretable Index Score for Capturing Disease Progression in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "Multiple speech biomarkers have been shown to carry useful information regarding Amyotrophic Lateral Sclerosis (ALS) pathology. We propose a two-step framework to compute optimal linear combinations (indexes) of these biomarkers that are more discriminative and noise-robust than the individual markers, which is important for clinical care and pharmaceutical trial applications. First, we use a hierarchical clustering based method to select representative speech metrics from a dataset comprising 143 people with ALS and 135 age- and sex-matched healthy controls. Second, we analyze three methods of index computation that optimize linear discriminability, Youden Index, and sparsity of logistic regression model weights, respectively, and evaluate their performance with 5-fold cross validation. We find that the proposed indexes are generally more discriminative of bulbar vs non-bulbar onset in ALS than their individual component metrics as well as an equally-weighted baseline",
    "checked": true,
    "id": "774ed9112b2c4fa7b64c85e42a132a25742d2513",
    "semantic_title": "combining multiple multimodal speech features into an interpretable index score for capturing disease progression in amyotrophic lateral sclerosis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mallolragolta23_interspeech.html": {
    "title": "The MASCFLICHT Corpus: Face Mask Type and Coverage Area Recognition from Speech",
    "volume": "main",
    "abstract": "We present a novel speech dataset for face mask type and coverage area recognition collected with a smartphone. The dataset contains 2h 27m 55s of data from 30 German speakers (15f, 15m). The baseline results exploit the functionals of the eGeMAPS feature set, the Mel-spectrogram, and the spectrogram representations of the audio samples. To model the one-dimensional features, we investigate Support Vector Classifiers (SVC) and a neural network classifier. We extract salient information from the two-dimensional representations with Convolutional Neural Network (CNN) based encoders, coupled with a classification block. We use the Unweighted Average Recall (UAR) as the evaluation metric. For the face mask type and the coverage area recognition tasks (3-class problems), the best models on the test partition score a UAR of 49.3% and 47.8%, respectively. For the face mask type and coverage area recognition task (5-class problem), the optimal model on the test partition obtains a UAR of 35.0%",
    "checked": true,
    "id": "2e610972854ef00908769f11d1e8014178f6d657",
    "semantic_title": "the mascflicht corpus: face mask type and coverage area recognition from speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/botelho23_interspeech.html": {
    "title": "Towards Reference Speech Characterization for Health Applications",
    "volume": "main",
    "abstract": "Speech has been used as a biomarker for the binary classification of multiple diseases, with promising results. However these speech affecting diseases often co-exist in the same individual and produce similar manifestations in the speech signal. Thus we propose to characterize normative speech using reference intervals for interpretable speech features (acoustic and linguistic), as a first step towards the adoption of speech analysis for multidisease screening in health applications. We discuss the impact of demographics and speech tasks. Finally, we compare the reference intervals with subjects suffering from Parkinson's disease, Alzheimer's disease and depression",
    "checked": true,
    "id": "9c836950492d3c43e17c1eb11020298e7a73a992",
    "semantic_title": "towards reference speech characterization for health applications",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/riosurrego23_interspeech.html": {
    "title": "Automatic Classification of Hypokinetic and Hyperkinetic Dysarthria based on GMM-Supervectors",
    "volume": "main",
    "abstract": "Hypokinetic and hyperkinetic dysarthria are motor speech disorders that appear in patients with Parkinson's and Huntington's disease, respectively. They are caused due to progressive lesions or alterations in the basal ganglia. In particular, Huntington's disease (HD) is known to be more invasive and difficult to treat than Parkinson's disease (PD), producing more aggressive motor and cognitive alterations. Since speech production requires the movement and control of many different muscles and limbs, it constitutes a highly complex motor activity that may reflect relevant aspects of the patient's health state. This paper proposes the discrimination between patients with PD, HD, and healthy controls (HC) based on different speech dimensions. Speaker models based on Gaussian-mixture model supervectors are created with the features extracted from each speech dimension. The results suggest that it is possible to distinguish between PD and HD patients using the supervectors-based approach",
    "checked": true,
    "id": "f5514993b1c5bc25a7ff4a55117f221fbe0dd464",
    "semantic_title": "automatic classification of hypokinetic and hyperkinetic dysarthria based on gmm-supervectors",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dineley23_interspeech.html": {
    "title": "Towards robust paralinguistic assessment for real-world mobile health (mHealth) monitoring: an initial study of reverberation effects on speech",
    "volume": "main",
    "abstract": "Speech is promising as an objective, convenient tool to monitor health remotely over time using mobile devices. Numerous paralinguistic features have been demonstrated to contain salient information related to an individual's health. However, mobile device specification and acoustic environments vary widely, risking the reliability of the extracted features. In an initial step towards quantifying these effects, we report the variability of 13 exemplar paralinguistic features commonly reported in the speech-health literature and extracted from the speech of 42 healthy volunteers recorded consecutively in rooms with low and high reverberation with one budget and two higher-end smartphones, and a condenser microphone. Our results show reverberation has a clear effect on several features, in particular voice quality markers. Our findings point to new research directions investigating how best to record and process in-the-wild speech for reliable longitudinal mobile health state assessment",
    "checked": true,
    "id": "e1cad4e1650b0f7b55e5255f0d97cb36da9a712b",
    "semantic_title": "towards robust paralinguistic assessment for real-world mobile health (mhealth) monitoring: an initial study of reverberation effects on speech",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simmatis23_interspeech.html": {
    "title": "Multimodal Assessment of Bulbar Amyotrophic Lateral Sclerosis (ALS) Using a Novel Remote Speech Assessment App",
    "volume": "main",
    "abstract": "Speech is a valuable marker of disease onset and progression in amyotrophic lateral sclerosis (ALS). Acoustic and kinematic data have characterized speech impairments in ALS previously, and there is growing interest in combining these modalities in novel analytical platforms. We explored the use of a multimodal (audio/video) speech assessment pipeline in ALS patients with varying severities. Participants performed a passage reading task, and clinical outcomes of e.g., speech function were collected. Speech data were analyzed using a custom automated acoustic and kinematic pipeline. Sparse canonical correlation analysis (SCCA) was then used. Both acoustic and kinematic features loaded strongly with clinical data (loadings ≥|0.50|), indicating that multimodal features captured complementary speech function information. This reinforces the value of multimodal assessment techniques and points the way towards future remote assessment development steps",
    "checked": true,
    "id": "1b727de8e19ed1a2fbdbdc7d2e02c555e5c61696",
    "semantic_title": "multimodal assessment of bulbar amyotrophic lateral sclerosis (als) using a novel remote speech assessment app",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martinez23_interspeech.html": {
    "title": "On the Use of High Frequency Information for Voice Pathology Classification",
    "volume": "main",
    "abstract": "We have observed significant differences in the high frequency content of the spectrum between healthy and pathological voices. Pathologies like larynx cancer, vocal fold lesions, and patients with larynx or vocal fold removal are examples of this. This finding invites to use high sampling frequencies in voice pathology classification systems to benefit from this high frequency information, which has been traditionally ignored. With a GMM classifier fed with MFCCs and a sampling frquency of 48 kHz we are able to improve AUC almost a 5% compared to a system using a sampling frequency of 8 kHz and more than 2% compared to a system using a sampling frequency of 16 kHz",
    "checked": true,
    "id": "c1c33c4f240aa74462a7635c574e74555a6ee2f5",
    "semantic_title": "on the use of high frequency information for voice pathology classification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/favaro23_interspeech.html": {
    "title": "Do Phonatory Features Display Robustness to Characterize Parkinsonian Speech Across Corpora?",
    "volume": "main",
    "abstract": "Sustained vowels have been largely used to quantify vocal impairment in Parkinson's disease (PD), with most studies focusing on a single corpus. Presumably, features obtained from sustained vowels are language-independent, but how findings generalize across cohorts is unclear. This work analyzes 61 phonatory features from 5 corpora in American English, Italian, Castilian Spanish, Colombian Spanish, and German, respectively, by conducting a statistical and correlation analysis. We use robustness as a criterion in which a feature displays the same behavior across corpora. The statistical analysis showed that the features provided good separability between PD and controls in only two out of five corpora, and none of the features displayed robustness. However, experiments report significant correlations between feature values and clinical scores. These findings provide valuable insights into the acoustic corpora-based dissimilarities, which should be considered when generalizing findings",
    "checked": true,
    "id": "562d06b0cddb553a76e6b68f6f2ba470a17bb5d4",
    "semantic_title": "do phonatory features display robustness to characterize parkinsonian speech across corpora?",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kadiri23_interspeech.html": {
    "title": "Severity Classification of Parkinson's Disease from Speech using Single Frequency Filtering-based Features",
    "volume": "main",
    "abstract": "Developing objective methods for assessing the severity of Parkinson's disease (PD) is crucial for improving the diagnosis and treatment. This study proposes two sets of novel features derived from the single frequency filtering (SFF) method: (1) SFF cepstral coefficients (SFFCC) and (2) MFCCs from the SFF (MFCC-SFF) for the severity classification of PD. Prior studies have demonstrated that SFF offers greater spectro-temporal resolution compared to the short-time Fourier transform. The study uses the PC-GITA database, which includes speech of PD patients and healthy controls produced in three speaking tasks (vowels, sentences, text reading). Experiments using the SVM classifier revealed that the proposed features outperformed the conventional MFCCs in all three speaking tasks. The proposed SFFCC and MFCC-SFF features gave a relative improvement of 5.8% & 2.3% for the vowel task, 7.0% & 1.8% for the sentence task, and 2.4% & 1.1% for the read text task, in comparison to MFCC features",
    "checked": true,
    "id": "75ce6a82935bf18bc9beb1b667306520cd3ed6e2",
    "semantic_title": "severity classification of parkinson's disease from speech using single frequency filtering-based features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simek23_interspeech.html": {
    "title": "Comparison of acoustic measures of dysphonia in Parkinson's disease and Huntington's disease: Effect of sex and speaking task",
    "volume": "main",
    "abstract": "This study investigated whether voice quality is differentially affected in two distinct basal ganglia disorders causing hypokinetic and hyperkinetic dysarthria, including effects of gender and speaking task. The sustained vowel phonations and monologues of 40 de novo Parkinson's disease (PD) patients, 40 Huntington's disease (HD) patients, and 40 healthy control participants were evaluated. Using cepstral peak prominence extracted from sustained phonation, differences from controls were found for male and female HD patients (p < 0.05) but only male PD patients (p < 0.05). Using the glottal-to-noise excitation ratio obtained from monologue, differences from controls were detected for male and female PD groups (p < 0. 05) but only male HD group (p < 0.05). In general, female patients show better voice quality. Our findings highlight that selecting suitable acoustic measures and speaking material is essential for adequate evaluation of dysphonia severity across differing etiologies",
    "checked": true,
    "id": "826bed7d291ca97822d48c425a50d245f3e89136",
    "semantic_title": "comparison of acoustic measures of dysphonia in parkinson's disease and huntington's disease: effect of sex and speaking task",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gomezzaragoza23_interspeech.html": {
    "title": "Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses",
    "volume": "main",
    "abstract": "Alzheimer's Disease (AD) is the world's leading neurodegenerative disease, which often results in communication difficulties. Analysing speech can serve as a diagnostic tool for identifying the condition. The recent ADReSS challenge provided a dataset for AD classification and highlighted the utility of manual transcriptions. In this study, we used the new state-of-the-art Automatic Speech Recognition (ASR) model Whisper to obtain the transcriptions, which also include automatic punctuation. The classification models achieved test accuracy scores of 0.854 and 0.833 combining the pretrained FastText word embeddings and recurrent neural networks on manual and ASR transcripts respectively. Additionally, we explored the influence of including pause information and punctuation in the transcriptions. We found that punctuation only yielded minor improvements in some cases, whereas pause encoding aided AD classification for both manual and ASR transcriptions across all approaches investigated",
    "checked": true,
    "id": "9b13b17f817de5f6fd315d3a113edd1562b9e2ff",
    "semantic_title": "alzheimer disease classification through asr-based transcriptions: exploring the impact of punctuation and pauses",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23c_interspeech.html": {
    "title": "LanSER: Language-Model Supported Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) models typically rely on costly human-labeled data for training, making scaling methods to large speech datasets and nuanced emotion taxonomies difficult. We present LanSER, a method that enables the use of unlabeled data by inferring weak emotion labels via pre-trained large language models through weakly-supervised learning. For inferring weak labels constrained to a taxonomy, we use a textual entailment approach that selects an emotion label with the highest entailment score for a speech transcript extracted via automatic speech recognition. Our experimental results show that models pre-trained on large datasets with this weak supervision outperform other baseline models on standard SER datasets when fine-tuned, and show improved label efficiency. Despite being pre-trained on labels derived only from text, we show that the resulting representations appear to model the prosodic content of speech",
    "checked": true,
    "id": "74381622d5931fb073deccf758d37cf45e41820f",
    "semantic_title": "lanser: language-model supported speech emotion recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23_interspeech.html": {
    "title": "Fine-tuned RoBERTa Model with a CNN-LSTM Network for Conversational Emotion Recognition",
    "volume": "main",
    "abstract": "Textual emotion recognition in conversations has gained increasing attention in recent years for the growing amount of applications it can serve, e.g., human-robot interactions, recommended systems. However, most existing approaches are either based on BERT-based model which fail to exploit crucial information about the long-text context, or resort to complex entanglement of neural network architectures resulting in less stable training procedures and slower inference time. To bridge this gap, we first propose a fast, compact and parameter-efficient framework based on fine-tuned pre-trained RoBERTa model with a CNN-LSTM network for textual emotion recognition in conversations. First, we fine-tune the pre-tranined RoBERTa model to effectively learn long-term emotion-relevant context information. Second, convolutional neural network coupled with the bidirectional long short-term memory and joint reinforced blocks are utilized to recognize emotion in conversations. Extensive experiments are conducted on benchmark emotion MELD dataset, and the results show that our model outperforms a wide range of strong baselines and achieves competitive results with the state-of-art approaches",
    "checked": true,
    "id": "8a2bb15380530066cd385b2f94cc7c98f1547656",
    "semantic_title": "fine-tuned roberta model with a cnn-lstm network for conversational emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/stanley23_interspeech.html": {
    "title": "Emotion Label Encoding Using Word Embeddings for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) is an important and challenging task for human-computer interaction. Human emotions are complex and nuanced, hence difficult to represent. The standard representations of emotions, categorical or continuous, tend to oversimplify the problem. Recently, the label encoding approach has been proposed, where vectors are used to represent the emotion space. In this paper, we hypothesise that using a pre-existing vector space that encodes semantic information about emotion is beneficial for the task. To this aim, we propose using word embeddings obtained from a Language Model (LM) as labels for SER. We evaluate the performance of the proposed approach on the IEMOCAP corpus and show that it yields better performance than a standard baseline. We also present a method to combine free text labels, which are unusable in conventional approaches, and by doing so we show that the model can learn more nuanced representations of emotions",
    "checked": true,
    "id": "7f3a77502d719ad6ba275d6951868cc220518f49",
    "semantic_title": "emotion label encoding using word embeddings for speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ia_interspeech.html": {
    "title": "Discrimination of the Different Intents Carried by the Same Text Through Integrating Multimodal Information",
    "volume": "main",
    "abstract": "Many intent understanding studies neglect the impact of paralinguistic information, resulting in misunderstandings during speech interactions, particularly when different intentions are conveyed by the same text with varying paralinguistic information. To address this issue, this study developed a Chinese multimodal spoken language intention understanding dataset that features different spoken intentions for identical texts. Our proposed attention-based BiLSTM model integrates textual and acoustic features and introduces an acoustic information gate mechanism to supplement or correct linguistic intention with paralinguistic intention. Experimental results demonstrate that our multimodal integration model improves intent discrimination accuracy by 11.0% compared to models that incorporate only linguistic information. The result highlights the effectiveness of our proposed model for intent discrimination, particularly in cases with identical text but varying intentions",
    "checked": true,
    "id": "fc54740746ceb8e98f9d2c659b7d2e5673f19432",
    "semantic_title": "discrimination of the different intents carried by the same text through integrating multimodal information",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23f_interspeech.html": {
    "title": "Meta-domain Adversarial Contrastive Learning for Alleviating Individual Bias in Self-sentiment Predictions",
    "volume": "main",
    "abstract": "Self-sentiment provides direct feedback from users and is vital in accurately evaluating and improving the quality of dialogue systems. However, few studies focus on self-sentiment prediction, and the works on third-party sentiment prediction suffer from two problems when predicting self-sentiments: Self-sentiment annotations are labeled by the speakers themselves, leading to solid individual bias in annotations and a sub-optimal prediction; The hardness of collecting sufficient data with self-sentiment annotations limits the size of the data, resulting in the overlapping problem. This work hence proposes a novel meta-learning domain adversarial contrastive neural network (MetaDACNN) that extracts user-shared prior knowledge and learns user-specific classifiers to handle individual bias and to alleviate overfitting. Experimental results on two public datasets show that MetaDACNN improves the prediction performance and alleviates individual bias compared to state-of-the-art models",
    "checked": true,
    "id": "da83dfcddcd6729e40947c821a34ec613f3d80a5",
    "semantic_title": "meta-domain adversarial contrastive learning for alleviating individual bias in self-sentiment predictions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23b_interspeech.html": {
    "title": "SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "To achieve efficient feature fusion, existing research tends to employ cross-attention to control the contributions of different modalities in fusion. However, this inevitably causes high computational effort and introduces noise weights due to redundant computations. Therefore, this paper proposes sliding window attention (SliWa) to control the feature perception range and dynamically model the modality fusion at different granularities. In addition, we present a novel feature map classifier (FMC) based on high-response feature reuse (HRFR), which explicitly preserves the deep emotional feature structure, thus preventing the submersion of the crucial classification information after average flattening and the negative impacts of parameter flooding. We unify the mentioned modules in the SWRR framework, and the experimental results on the commonly used datasets IEMOCAP and CMU-MOSEI reveal the effectiveness of SWRR in improving the performance of emotion recognition",
    "checked": true,
    "id": "9439dccba5ad90d30cee18785b5cd03bc9a43990",
    "semantic_title": "swrr: feature map classifier based on sliding window attention and high-response feature reuse for multimodal emotion recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23f_interspeech.html": {
    "title": "PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNN) and Transformer have wildly succeeded in multimedia applications. However, more effort needs to be made to harmonize these two architectures effectively to satisfy speech enhancement. This paper aims to unify these two architectures and presents a Parallel Conformer for speech enhancement. In particular, the CNN and the self-attention (SA) in the Transformer are fully exploited for local format patterns and global structure representations. Based on the small receptive field size of CNN and the high computational complexity of SA, we specially designed a multi-branch dilated convolution (MBDC) and a self-channel-time-frequency attention (Self-CTFA) module. MBDC contains three convolutional layers with different dilation rates for the feature from local to non-local processing. Experimental results show that our method performs better than state-of-the-art methods in most evaluation criteria while maintaining the lowest model parameters",
    "checked": true,
    "id": "3b4ead117a5fce4e73dd55ce446f4a89647d3d25",
    "semantic_title": "pcnn: a lightweight parallel conformer neural network for efficient monaural speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23b_interspeech.html": {
    "title": "Exploring the Interactions Between Target Positive and Negative Information for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Acoustic echo cancellation (AEC) aims to remove interference signals while leaving near-end speech least distorted. As the indistinguishable patterns between near-end speech and interference signals, near-end speech can't be separated completely, causing speech distortion and interference signals residual. We observe that besides target positive information, e.g., ground-truth speech and features, the target negative information, such as interference signals and features, helps make pattern of target speech and interference signals more discriminative. Therefore, we present a novel AEC model encoder-decoder architecture with the guidance of negative information termed as CMNet. A collaboration module (CM) is designed to establish the correlation between the target positive and negative information in a learnable manner via three blocks: target-positive, target-negative, and interactive block. Experimental results demonstrate our CMNet achieves superior performance than recent methods",
    "checked": true,
    "id": "745280a794d1a53f167830d0f742db62554a289e",
    "semantic_title": "exploring the interactions between target positive and negative information for acoustic echo cancellation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/andreev23_interspeech.html": {
    "title": "Iterative autoregression: a novel trick to improve your low-latency speech enhancement model",
    "volume": "main",
    "abstract": "Streaming models are an essential component of real-time speech enhancement tools. The streaming regime constrains speech enhancement models to use only a tiny context of future information. As a result, the low-latency streaming setup is generally considered a challenging task and has a significant negative impact on the model's quality. However, the sequential nature of streaming generation offers a natural possibility for autoregression, that is, utilizing previous predictions while making current ones. The conventional method for training autoregressive models is teacher forcing, but its primary drawback lies in the training-inference mismatch that can lead to a substantial degradation in quality. In this study, we propose a straightforward yet effective alternative technique for training autoregressive low-latency speech enhancement models. We demonstrate that the proposed approach leads to stable improvement across diverse architectures and training scenarios",
    "checked": true,
    "id": "27ce54919d7b81f6f949ee43180e33132c3eb5d5",
    "semantic_title": "iterative autoregression: a novel trick to improve your low-latency speech enhancement model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ku23_interspeech.html": {
    "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
    "volume": "main",
    "abstract": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer to build a new model with a small footprint that also achieve good performances. We explore several S4-based deep architectures in both time (T) and time-frequency (TF) domains. The 2-D S4 layer can be thought of as a special convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18",
    "checked": true,
    "id": "154d5cf9a40d313cdf62372621f554a809fbb103",
    "semantic_title": "a multi-dimensional deep structured state space approach to speech enhancement using small-footprint models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/frenkel23_interspeech.html": {
    "title": "Domain Adaptation for Speech Enhancement in a Large Domain Gap",
    "volume": "main",
    "abstract": "Speech enhancement approaches based on neural networks, aim to learn a noisy-to-clean transformation using a supervised learning paradigm. However, networks trained in this way may not be effective at handling languages and types of noise that were not present in the training data. To address this issue, this study focuses on unsupervised domain adaptation, specifically for large-domain-gap cases. In this setup, we have noisy speech data from the new domain but the corresponding clean speech data are not available. We propose an adaptation method that is based on domain-adversarial training followed by iterative self-training where the quality of the estimated speech used as pseudo labels is monitored by the performance of the adapted network on labeled data from the source domain. Experimental results show that our method effectively mitigates the domain mismatch between training and test sets, and surpasses the current baseline",
    "checked": true,
    "id": "fc5424fe6021a9554e00ee3f8214f645521d171a",
    "semantic_title": "domain adaptation for speech enhancement in a large domain gap",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zadorozhnyy23_interspeech.html": {
    "title": "SCP-GAN: Self-Correcting Discriminator Optimization for Training Consistency Preserving Metric GAN on Speech Enhancement Tasks",
    "volume": "main",
    "abstract": "In recent years, Generative Adversarial Networks (GANs) have produced significantly improved speech enhancement (SE) task results. However, they are challenging to train. In this work, we introduce several improvements to GAN training schemes, which can be applied to most GAN-based SE models. We propose using consistency loss functions, which target the inconsistency in time and time-frequency domains caused by Fourier and Inverse Fourier Transforms. We also present self-correcting optimization for training a GAN discriminator on SE tasks which helps avoid \"harmful\" training directions for parts of the discriminator loss function. We have tested our proposed methods on several state-of-the-art GAN-based SE models and obtained consistent improvements, including new state-of-the-art results for the Voice Bank+DEMAND dataset",
    "checked": true,
    "id": "6d644c54b07dcb53acf977b796c49cf27b698c38",
    "semantic_title": "scp-gan: self-correcting discriminator optimization for training consistency preserving metric gan on speech enhancement tasks",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23c_interspeech.html": {
    "title": "A Mask Free Neural Network for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "In speech enhancement, the lack of clear structural characteristics in the target speech phase requires the use of conservative and cumbersome network frameworks. It seems difficult to achieve competitive performance using direct methods and simple network architectures. However, we propose the MFNet, a direct and simple network that can not only map speech but also map reverse noise. This network is constructed by stacking global local former blocks (GLFBs), which combine the advantages of Mobileblock for global processing and Metaformer architecture for local interaction. Our experimental results demonstrate that our network using mapping method outperforms masking methods, and direct mapping of reverse noise is the optimal solution in strong noise environments. In a horizontal comparison on the 2020 Deep Noise Suppression (DNS) challenge test set without reverberation, to the best of our knowledge, MFNet is the current state-of-the-art (SOTA) mapping model",
    "checked": true,
    "id": "8bbcf7df910cd6eacf92168ec81220fd62fcf21b",
    "semantic_title": "a mask free neural network for monaural speech enhancement",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23p_interspeech.html": {
    "title": "A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech",
    "volume": "main",
    "abstract": "The lack of clean speech is a practical challenge to the development of speech enhancement systems, which means that there is an inevitable mismatch between their training criterion and evaluation metric. In response to this unfavorable situation, we propose a training and inference strategy that additionally uses enhanced speech as a target by improving the previously proposed noisy-target training (NyTT). Because homogeneity between in-domain noise and extraneous noise is the key to the effectiveness of NyTT, we train various student models by remixing 1) the teacher model's estimated speech and noise for enhanced-target training or 2) raw noisy speech and the teacher model's estimated noise for noisy-target training. Experimental results show that our proposed method outperforms several baselines, especially with the teacher/student inference, where predicted clean speech is derived successively through the teacher and final student models",
    "checked": true,
    "id": "e685129b7dac8f2b460ec7f024f4fbdeb8be069f",
    "semantic_title": "a training and inference strategy using noisy and enhanced speech as target for speech enhancement without clean speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pandey23b_interspeech.html": {
    "title": "A Simple RNN Model for Lightweight, Low-compute and Low-latency Multichannel Speech Enhancement in the Time Domain",
    "volume": "main",
    "abstract": "Deep learning has led to unprecedented advances in speech enhancement. However, deep neural networks (DNNs) typically require large amount of computation, memory, signal buffer and processing time to achieve strong performance. Designing a DNN to meet a given resource constraint requires dedicated efforts. This study proposes a novel recurrent neural network (RNN) based model for time-domain multichannel speech enhancement that can be easily tuned to meet a given constraint. We present results of training the model at different scales, where algorithmic latency varies from 1 ms to 16 ms, model size varies from 100 Thousand to 25 Million parameters, and compute to process one second of speech varies from 100 Mega to 25 Giga multiply-accumulates (MACs). Experimental results demonstrate that the proposed model can obtain similar or better performance using fewer computes and parameters than competitive approaches to low-latency multichannel speech enhancement",
    "checked": true,
    "id": "cf371579ee406fda6847b29e32b747297cfbb6c9",
    "semantic_title": "a simple rnn model for lightweight, low-compute and low-latency multichannel speech enhancement in the time domain",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23b_interspeech.html": {
    "title": "High Fidelity Speech Enhancement with Band-split RNN",
    "volume": "main",
    "abstract": "Despite the rapid progress in speech enhancement (SE) research, improving the intelligibility and perceptual quality of desired speech in noisy environments with interfering speakers remains challenging. This paper attempts to achieve high-fidelity full-band SE and personalized SE (PSE) by modifying the recently proposed band-split RNN (BSRNN) model. To reduce the negative impact of unstable high-frequency components in full-band speech recording, we perform bi-directional and uni-directional band-level modeling to low-frequency and high-frequency subbands, respectively. For the PSE task, an additional speaker enrollment module is added to BSRNN to make use of the target speaker information for suppressing the interfering speech. Moreover, we utilize a MetricGAN discriminator (MGD) and a multi-resolution spectrogram discriminator (MRSD) to further improve the human auditory perceptual quality of the enhanced speech. Experimental results show that our system outperforms various top-ranking SE systems, achieves state-of-the-art (SOTA) SE performance on the DNS-2020 test set, and ranks among the top 3 in the DNS-2023 challenge on the PSE task",
    "checked": true,
    "id": "6d8358a9e819eb0088f229da45b953a977e83c93",
    "semantic_title": "high fidelity speech enhancement with band-split rnn",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23b_interspeech.html": {
    "title": "Focus on the Sound around You: Monaural Target Speaker Extraction via Distance and Speaker Information",
    "volume": "main",
    "abstract": "Previously, Target Speaker Extraction (TSE) has yielded outstanding performance in certain application scenarios for speech enhancement and source separation. However, obtaining auxiliary speaker-related information is still challenging in noisy environments with significant reverberation. inspired by the recently proposed distance-based sound separation, we propose the near sound (NS) extractor, which leverages distance information for TSE to reliably extract speaker information without requiring previous speaker enrolment, called speaker embedding self-enrollment (SESE). Full- & sub-band modeling is introduced to enhance our NS-Extractor's adaptability towards environments with significant reverberation. Experimental results on several cross-datasets demonstrate the effectiveness of our improvements and the excellent performance of our proposed NS-Extractor in different application scenarios",
    "checked": true,
    "id": "fe665dcac3c7d48aa0111c63aa6d84b048aaf748",
    "semantic_title": "focus on the sound around you: monaural target speaker extraction via distance and speaker information",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kovalyov23_interspeech.html": {
    "title": "DFSNet: A Steerable Neural Beamformer Invariant to Microphone Array Configuration for Real-Time, Low-Latency Speech Enhancement",
    "volume": "main",
    "abstract": "Invariance to microphone array configuration is a rare attribute in neural beamformers. Filter-and-sum (FS) methods in this class define the target signal with respect to a reference channel. However, this not only complicates formulation in reverberant conditions but also the network, which must have a mechanism to infer what the reference channel is. To address these issues, this study presents Delay-Filter-and-Sum Network (DFSNet), a steerable neural beamformer invariant to microphone number and array geometry for causal speech enhancement. In DFSNet, acquired signals are first steered toward the speech source direction prior to the FS operation, which simplifies the task into the estimation of delay-and-sum clean reverberant speech. The proposed model is designed to incur low latency, distortion, and memory and computational burden, giving rise to high potential in hearing aid applications. Simulation results reveal comparable performance to noncausal state-of-the-art",
    "checked": true,
    "id": "c62781a17717d115068f42adf20031e1fc83de59",
    "semantic_title": "dfsnet: a steerable neural beamformer invariant to microphone array configuration for real-time, low-latency speech enhancement",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23o_interspeech.html": {
    "title": "Speaker-Aware Anti-spoofing",
    "volume": "main",
    "abstract": "We address speaker-aware anti-spoofing, where prior knowledge of the target speaker is incorporated into a voice spoofing countermeasure (CM). In contrast to the frequently used speaker-independent solutions, we train the CM in a speaker-conditioned way. As a proof of concept, we consider speaker-aware extension to the state-of-the-art AASIST (audio anti-spoofing using integrated spectro-temporal graph attention networks) model. To this end, we consider two alternative strategies to incorporate target speaker information at the frame and utterance levels, respectively. The experimental results on a custom protocol based on ASVspoof 2019 dataset indicates the efficiency of the speaker information via enrollment: we obtain maximum relative improvements of 25.1% and 11.6% in equal error rate (EER) and minimum tandem detection cost function (t-DCF) over a speaker-independent baseline, respectively",
    "checked": true,
    "id": "cf63c74d496ee64921ea493955649018bd3f4ae2",
    "semantic_title": "speaker-aware anti-spoofing",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/araki23_interspeech.html": {
    "title": "Impact of Residual Noise and Artifacts in Speech Enhancement Errors on Intelligibility of Human and Machine",
    "volume": "main",
    "abstract": "Single-channel (1-ch) speech enhancement (SE) has been widely studied, and high accuracy has been achieved recently. However, enhanced speech still includes some errors that affect human hearing quality and SE applications, e.g., automatic speech recognition (ASR). Previously, [Iwamoto et al., in Interspeech 2022, pp. 5418-5422] decomposed the errors in an enhanced signal into residual noise and artifact components and analyzed their impacts on ASR performance. They showed that the artifacts have a greater impact than the residual noise on ASR. Although the impact on human intelligibility has not been investigated yet, it is essential to get the knowledge to develop SE techniques suitable for both humans and machines. This paper, therefore, investigates the effects of such error factors on human listening. Our subjective test results show that the artifacts have a large impact on human intelligibility, and that residual noise has a lesser impact",
    "checked": true,
    "id": "40a78f647aa9365746598a359c4c7804bcf7182f",
    "semantic_title": "impact of residual noise and artifacts in speech enhancement errors on intelligibility of human and machine",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sach23_interspeech.html": {
    "title": "EffCRN: An Efficient Convolutional Recurrent Network for High-Performance Speech Enhancement",
    "volume": "main",
    "abstract": "Fully convolutional recurrent neural networks (FCRNs) have shown state-of-the-art performance in single-channel speech enhancement. However, the number of parameters and the FLOPs/second of the original FCRN are restrictively high. A further important class of efficient networks is the CRUSE topology, serving as reference in our work. By applying a number of topological changes at once, we propose both an efficient FCRN (FCRN15), and a new family of efficient convolutional recurrent neural networks (EffCRN23, EffCRN23lite). We show that our FCRN15 (875K parameters) and EffCRN23lite (396K) outperform the already efficient CRUSE5 (85M) and CRUSE4 (7.2M) networks, respectively, w.r.t. PESQ, DNSMOS and ΔSNR, while requiring about 94% less parameters and about 20% less #FLOPs/frame. Thereby, according to these metrics, the FCRN/EffCRN class of networks provides new best-in-class network topologies for speech enhancement",
    "checked": true,
    "id": "a0e1ade0b2187d56f0c9284b31b528b273671fbe",
    "semantic_title": "effcrn: an efficient convolutional recurrent network for high-performance speech enhancement",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23e_interspeech.html": {
    "title": "HAD-ANC: A Hybrid System Comprising an Adaptive Filter and Deep Neural Networks for Active Noise Control",
    "volume": "main",
    "abstract": "Our study proposes a novel hybrid active noise control (ANC) system, called HAD-ANC, that combines an adaptive filter with deep neural networks. HAD-ANC employs a cascade design comprising the frequency-domain block least mean square algorithm and two gated convolutional recurrent networks (GCRNs). The first GCRN follows the adaptive filter to handle nonlinear distortion by reducing the residual error of linear filtering and models the reverse of both loudspeaker and secondary path. The second GCRN models the loudspeaker and secondary path to force the adaptive filter to estimate the primary path. Additionally, we utilize a delay-compensated reference signal to consider the causal constraints of frequency-domain ANC system. Experimental results based on NOISEX-92 dataset show that the proposed system outperforms recent ANC methods, enables wideband noise reduction, and indicates robustness to path changes",
    "checked": true,
    "id": "d41ebfbaa531d906aeb57bc048e10e5bc10ddb54",
    "semantic_title": "had-anc: a hybrid system comprising an adaptive filter and deep neural networks for active noise control",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chu23_interspeech.html": {
    "title": "MSAF: A Multiple Self-Attention Field Method for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement (SE) systems, based on generative adversarial networks (GANs), are limited in improving speech quality and intelligibility. In this study, we propose a novel multiple self-attention field method for speech enhancement (MSAF). The models with different positions of the self-attention layers focus on different features. The output of each model is assigned a different feature weight, which is obtained by training. Then, we fuse the models according to the feature weights to obtain a clean speech signal. For speech quality, the proposed method improves by 8.22%, 8.52%, 9.28%, and 9.40% in CBAK, CSIG, COVL, and PESQ on average compared with the baseline SASEGANs. The results show that the MSAF comprehensively improves the performance of the baseline SASEGAN and performs better than the mainstream GAN-based SE methods. Importantly, the proposed method can be extended to other GAN-based SE methods",
    "checked": true,
    "id": "4e276d1aa1da695673c93b2a52d7b229bb9609c4",
    "semantic_title": "msaf: a multiple self-attention field method for speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23t_interspeech.html": {
    "title": "Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression",
    "volume": "main",
    "abstract": "Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet",
    "checked": true,
    "id": "86f5ddb176f159ac26e6dde5abb4d4ebcb2ad5a5",
    "semantic_title": "ultra dual-path compression for joint echo cancellation and noise suppression",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wan23_interspeech.html": {
    "title": "ABC-KD: Attention-Based-Compression Knowledge Distillation for Deep Learning-Based Noise Suppression",
    "volume": "main",
    "abstract": "Noise suppression (NS) models have been widely applied to enhance speech quality. Recently, Deep Learning-Based NS, which we denote as Deep Noise Suppression (DNS), became the mainstream NS method due to its excelling performance over traditional ones. However, DNS models face 2 major challenges for supporting the real-world applications. First, high-performing DNS models are usually large in size, causing deployment difficulties. Second, DNS models require extensive training data, including noisy audios as inputs and clean audios as labels. It is often difficult to obtain clean labels for training DNS models. We propose the use of knowledge distillation (KD) to resolve both challenges. Our study serves 2 main purposes. First, we are among the first to comprehensively investigate mainstream KD techniques on DNS models to resolve the challenges. Furthermore, we propose a novel Attention-Based-Compression KD method that outperforms all investigated mainstream KD frameworks on DNS task",
    "checked": true,
    "id": "a3ade1e455423118c3820259ffdc41ebcc37c97d",
    "semantic_title": "abc-kd: attention-based-compression knowledge distillation for deep learning-based noise suppression",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/diener23_interspeech.html": {
    "title": "PLCMOS – A Data-driven Non-intrusive Metric for The Evaluation of Packet Loss Concealment Algorithms",
    "volume": "main",
    "abstract": "Speech quality assessment is a problem for every researcher working on models that produce or process speech. Human subjective ratings, the gold standard in speech quality assessment, are expensive and time-consuming to acquire in a quantity that is sufficient to get reliable data, while automated objective metrics show a low correlation with gold standard ratings. This paper presents PLCMOS, a non-intrusive data-driven tool for generating a robust, accurate estimate of the mean opinion score a human rater would assign an audio file that has been processed by being transmitted over a degraded packet-switched network with missing packets being healed by a packet loss concealment algorithm. Our new model shows a model-wise Pearson's correlation of ~0.97 and rank correlation of ~0.95 with human ratings, substantially above all other available intrusive and non-intrusive metrics",
    "checked": false,
    "id": "2f90c238092e7889e198d1c280d178c47cf32b64",
    "semantic_title": "plcmos - a data-driven non-intrusive metric for the evaluation of packet loss concealment algorithms",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wagner23b_interspeech.html": {
    "title": "Effects of Meter, Genre and Experience on Pausing, Lengthening and Prosodic Phrasing in German Poetry Reading",
    "volume": "main",
    "abstract": "The adequate and pleasant delivery of poetic speech remains a challenge for humans and machines alike. The present corpus study analyzes factors and strategies that characterize the stylistic expression of poetry and prose by professional actors as well as laypersons with musical training, focusing on their pausing, lengthening and intonation at verse boundaries. Our results show a clear influence on speakers' experience in modulating their speech with respect to prosodic timing: professional actors systematically insert more and more diverse prosodic boundaries and pauses than laypersons, and make strategic use of lengthening at verse endings in poetic speech. Our results further point out the relevance of pausing and lengthening as a time-buying strategy that enhances speech fluency, and we make tentative suggestions for modeling (poetic) speech in expressive speech synthesis",
    "checked": true,
    "id": "8255182f999b77a8a4331e41a5a86bebbd6f49c7",
    "semantic_title": "effects of meter, genre and experience on pausing, lengthening and prosodic phrasing in german poetry reading",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szalay23_interspeech.html": {
    "title": "Comparing first spectral moment of Australian English /s/ between straight and gay voices using three analysis window sizes",
    "volume": "main",
    "abstract": "Men producing /s/ with higher first spectral moment (M1) are more likely to be perceived as gay, yet it is unclear if M1 differs in production. Inconsistent results might be caused by inherent change in M1 over time. Therefore, we explored M1 change over time and tested if the length and location of the analysis window affects results on gay-straight differences. 37 gay and 29 straight male speakers of Australian English produced two /s/ tokens in continuous speech. M1 was extracted in each quarter of /s/ to explore change over time and in three windows: the entire fricative, the mid-50 ms, and the third quarter. Gay and straight men produced lower M1 in the first and last quarters relative to the midpoint. Despite the M1 change over time, we found no effect of analysis window on gay-straight differences, as gay men consistently showed higher M1. The lack of effect of analysis window on M1 is attributed to the overlap between the analysis windows caused by the duration of /s/",
    "checked": true,
    "id": "dd266e37aa668507f72dc00945cbc16a8b9efb81",
    "semantic_title": "comparing first spectral moment of australian english /s/ between straight and gay voices using three analysis window sizes",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/taguchi23_interspeech.html": {
    "title": "Universal Automatic Phonetic Transcription into the International Phonetic Alphabet",
    "volume": "main",
    "abstract": "This paper presents a state-of-the-art model for transcribing speech in any language into the International Phonetic Alphabet (IPA). Transcription of spoken languages into IPA is an essential yet time-consuming process in language documentation, and even partially automating this process has the potential to drastically speed up the documentation of endangered languages. Like the previous best speech-to-IPA model (Wav2Vec2Phoneme), our model is based on wav2vec 2.0 and is fine-tuned to predict IPA from audio input. We use training data from seven languages from CommonVoice 11.0, transcribed into IPA semi-automatically. Although this training dataset is much smaller than Wav2Vec2Phoneme's, its higher quality lets our model achieve comparable or better results. Furthermore, we show that the quality of our universal speech-to-IPA models is close to that of human annotators",
    "checked": true,
    "id": "a79bca31fc93e6a9b6754aae4bb4ad91dde7de12",
    "semantic_title": "universal automatic phonetic transcription into the international phonetic alphabet",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gerlach23_interspeech.html": {
    "title": "Voice Twins: Discovering Extremely Similar-sounding, Unrelated Speakers",
    "volume": "main",
    "abstract": "This paper deals with extremely similar-sounding, unrelated speakers ('voice twins') and presents an automatic approach to voice twin discovery applied to different speaker databases. An automatic speaker recognition system relying on perceptually relevant phonetic features including formants and a tuned clustering algorithm DBSCAN was used to group recordings within diverse datasets. 18 voice twin pairs selected from 2-speaker clusters were evaluated by 50 listeners in a 2-alternative forced choice experiment. Same/different decisions and confidence ratings were collected for same-speaker, random different-speaker and voice twin comparisons. Listeners were unable to differentiate between the candidate voice twin pairs much better than chance level while they performed well (80% accuracy) for random same- or different-speaker comparisons indicating the voice twin speakers were perceptually very similar. The implications and forensic relevance of identifying voice twins are discussed",
    "checked": true,
    "id": "6a735c94b52b72d500d68aef6fe4352367a99678",
    "semantic_title": "voice twins: discovering extremely similar-sounding, unrelated speakers",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hedegard23_interspeech.html": {
    "title": "Filling the population statistics gap: Swiss German reference data on F0 and speech tempo for forensic contexts",
    "volume": "main",
    "abstract": "The increased focus on big data in phonetics, and Bayesian statistics in the forensic sciences, prompt a fundamental issue in common applications of forensic phonetics. Relevant population distributions for most features, a key element when evaluating the similarity and distinctiveness of voices, remain lacking for a substantial number of languages and dialects. This paper provides population statistics for two phonetic features in the Swiss German context, speech tempo and F0, and outlines a potential method for big data analysis. The speech data is taken from 1000 SwG speakers and include two different style conditions: spontaneous and read speech. Results indicate significant variation for both parameters: we contradict previous findings on gender differences in speech tempo and note discrepancies for both features between the two styles. These findings constitute an important contribution to the field of forensic phonetics, as well as the field of general phonetics more broadly",
    "checked": true,
    "id": "477228f8a466febaff48befa18dd1596c2281cc9",
    "semantic_title": "filling the population statistics gap: swiss german reference data on f0 and speech tempo for forensic contexts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hutin23_interspeech.html": {
    "title": "Investigating the Syntax-Discourse Interface in the Phonetic Implementation of Discourse Markers",
    "volume": "main",
    "abstract": "Discourse markers (DMs) are (chunks of) words stemming from the diachronic development of other parts-of-speech that tag the discourse's organization (ex. \"well then\", \"innit\"...). However, in synchrony, the formal accounts for the DM class vary from purely discourse-oriented definitions to models relying on a combination of lexico-grammatical and discursive information. We propose to bring new evidence into this debate by comparing the phonetic realizations of 4 DM types: stemming originally from adverbs, coordinators, subordinators and interjections. A discourse-only account would predict that the 4 types would be realized similarly, while a syntactic-discursive account predicts that subordinators would stand out, as they are less prone to syntactic independence. The analysis of various acoustic parameters (segment duration, F0, F1, F2 and HNR) in a finely-annotated 4-hour long corpus of French indicates that a hybrid approach may indeed be more accurate",
    "checked": true,
    "id": "caf85e2a0e73bd4153c190f6776b3ac8095148ea",
    "semantic_title": "investigating the syntax-discourse interface in the phonetic implementation of discourse markers",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/essery23_interspeech.html": {
    "title": "Evaluation of a Forensic Automatic Speaker Recognition System with Emotional Speech Recordings",
    "volume": "main",
    "abstract": "In forensic contexts, speakers often feel emotional, which will likely influence their speech. Emotional mismatch between samples is therefore a source of variability which could have substantial effects on the performance of a forensic automatic speaker recognition system. This paper examines the issue of emotional speech in forensic casework, both in terms of emotional match and mismatch between test samples and in terms of the data used to calibrate the system (i.e. the reference population). Specifically, we tested system performance on samples of neutral and acted angry and fearful speech data across 37 test conditions. The best system performance was achieved when the test data and reference population conditions matched exactly. However, in 16 of the 37 tests, the system produced a Cllr greater than 0.8, 10 of which also exceeded a Cllr of 1. As a result, caution should be used to interpret the results of automatic and semi-automatic forensic analysis on emotional speech data",
    "checked": true,
    "id": "d3740056c2869da7f1d693ee6e886c38976b3ad5",
    "semantic_title": "evaluation of a forensic automatic speaker recognition system with emotional speech recordings",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahn23_interspeech.html": {
    "title": "An Outlier Analysis of Vowel Formants from a Corpus Phonetics Pipeline",
    "volume": "main",
    "abstract": "With the growing availability of large-scale spoken databases, linguists are increasingly relying on automated tools to obtain time alignments of sound units to the speech signal. A typical automated pipeline may involve grapheme-to-phoneme conversion, forced alignment, and acoustic-phonetic measurement, and each of these stages requires a strong assumption regarding the output quality. We investigate these assumptions by auditing outliers in vowel formants from two multilingual read speech corpora, CMU Wilderness and Mozilla Common Voice, across three languages: Hausa, Kazakh, and Swedish. From this audit, we develop a novel outlier taxonomy that includes the broad outlier categories of transcript errors, alignment errors, formant tracking errors, linguistic variations, and fine samples. We show the utility of this outlier analysis in identifying weaknesses in corpus-specific and corpus-general pipeline assumptions, and discovering characteristics of particular languages",
    "checked": true,
    "id": "844e4c88fcd590e979d1a804d0494a07c6c9cb0b",
    "semantic_title": "an outlier analysis of vowel formants from a corpus phonetics pipeline",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qu23_interspeech.html": {
    "title": "The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features",
    "volume": "main",
    "abstract": "This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech - phoneme corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes vs. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning",
    "checked": true,
    "id": "a6e3a10a6286967413e3406374bbeea533640030",
    "semantic_title": "the hidden dance of phonemes and visage: unveiling the enigmatic link between phonemes and facial features",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/blaylock23_interspeech.html": {
    "title": "Beatboxing Kick Drum Kinematics",
    "volume": "main",
    "abstract": "Study of the vocal movements of beatboxing can benefit speech science in a number of ways; but while there are established models of speech motor control that make deterministic predictions about vocal kinematics, little is known about beatboxing motor control. A region of interest method was applied to real-time MRI videos of beatboxing Kick Drums to measure the time to peak velocity-a measurement that is used to assess how well models of speech action predict actual movement trajectories. The average time to peak velocity for Kick Drums is about half of the way (58%) through the total movement duration, similar to the times to peak velocity reported for speech actions. However, while the times to peak velocity for Kick Drums tend to be just above 50%, the times to peak velocity reported for speech sounds are usually a bit below 50%. Further study is needed to assess whether this difference reflects a more extreme constriction goal or a qualitatively different movement pattern",
    "checked": true,
    "id": "16efca1d110ea1b3fbc6708d580d522281f6291e",
    "semantic_title": "beatboxing kick drum kinematics",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23b_interspeech.html": {
    "title": "Effects of hearing loss and amplification on Mandarin consonant perception",
    "volume": "main",
    "abstract": "This study investigates the effects of hearing loss and amplification on Mandarin consonant perception. 44 listeners with varying degrees of hearing loss were tested, both with and without the use of hearing aids. Consonant recognition was strongly correlated with the hearing threshold (r = -0.87), and was significantly improved by hearing-aid amplification (by more than 20% in group means) but was still not perfect. The underlying reasons are discussed. Furthermore, confusion patterns were analyzed and compared with those of normal-hearing listeners in the literature. The most challenging Mandarin consonants for hearing-impaired listeners include consonants with a spectral center of gravity in the high-frequency range (such as s and z), consonants with short duration (such as b, d, and g), and aspirated stops (such as p, t, and k). The findings of this study contribute to a better understanding of the difficulties experienced by Mandarin-speaking listeners with hearing loss",
    "checked": true,
    "id": "2523b7dec797b65bd700fd3879c33670f6b6ab29",
    "semantic_title": "effects of hearing loss and amplification on mandarin consonant perception",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/adams23_interspeech.html": {
    "title": "An Acoustic Analysis of Fricative Variation in Three Accents of English",
    "volume": "main",
    "abstract": "This study reports on an analysis of accent and gender differences in the realisation of the fricative /s/ within three accents of English: London, Cambridge, Belfast. There were 30 speakers in the study. Using multilevel modelling, significant differences between accents in the dynamic acoustics of the alveolar fricative /s/ are evident. Significant gender differences in the fricative energy measure trajectories are also found, within each accent. We further discuss the implications of these differences to our understanding of the role of gender and accent in the realisation of spectra movements in English fricatives, highlighting the necessity of a dynamic approach to sociophonetic acoustic variation",
    "checked": true,
    "id": "66d18ef1010a598db68f1ae55f0ef596b5f6296a",
    "semantic_title": "an acoustic analysis of fricative variation in three accents of english",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bros23_interspeech.html": {
    "title": "Acoustic cues to stress perception in Spanish – a mismatch negativity study",
    "volume": "main",
    "abstract": "In this paper we investigate the cues governing stress perception in Spanish - an issue that has been subject to debate and remains largely unresolved. While there is general agreement as to the ability of Spanish listeners to detect and reliably produce stress contrasts, there is some disagreement on the roles played by individual stress cues. In this study, we focus on early stress processing in a passive oddball paradigm aimed at eliciting a mismatch negativity response to changes in stress. Individual features (spectral tilt and f0) rather than feature bundles are used to induce stress shift. A vowel change is used as a control condition. The results show that while both spectral tilt and f0 manipulations result in mismatch negativity, the latter evokes a stronger response that equals the effect of a change in the quality of the stressed vowel. The results are in line with previous studies on stress correlates in other languages, pointing to a possible cross-linguistic pattern",
    "checked": true,
    "id": "0709fc902a6de0fb35cb070b62218af3a4d204fd",
    "semantic_title": "acoustic cues to stress perception in spanish – a mismatch negativity study",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sabev23_interspeech.html": {
    "title": "Bulgarian Unstressed Vowel Reduction: Received Views vs Corpus Findings",
    "volume": "main",
    "abstract": "Bulgarian is often cited in phonological work for its vowel reduction, with the assumption that the six-vowel stressed inventory, /ɛ a ɔ i ɤ u/, shrinks to three unstressed contrastive vowels, /i ɤ u/, by virtue of /ɛ a ɔ/ raising and merging with /i ɤ u/. The literature in Bulgarian, on the other hand, maintains that /ɛ-i/ do not merge in Standard Bulgarian; that vowels are less reduced in immediately pretonic syllables than elsewhere; that unstressed high vowels are lowered, while nonhigh vowels are raised; and that /a-ɤ/ are more likely to merge than /ɔ-u/. These claims have been challenged in recent work, and we present a new investigation based on 11,615 vowel tokens from 140 speakers in the BulPhonC speech corpus. MANOVA and GLMM results provide clear evidence that there is no unstressed high-vowel lowering, no difference between pretonic vs other unstressed vowels, and that both unstressed /a-ɤ/ and /ɔ-u/ merge completely, while /ɛ-i/ remain spectrally distinct",
    "checked": true,
    "id": "1207d703d3e003a8e90bddd518f0487ff887f9ff",
    "semantic_title": "bulgarian unstressed vowel reduction: received views vs corpus findings",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jain23b_interspeech.html": {
    "title": "An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations",
    "volume": "main",
    "abstract": "Speech systems are sensitive to accent variations. This is a challenge in India, which has numerous languages but few linguistic studies on pronunciation variation. The growing number of L2 English speakers reinforces the need to study accents and L1-L2 interactions. We investigate Indian English (IE) accents and report our observations on regional and shared features. Specifically, we observe phonemic variations and phonotactics in speakers' native languages and apply this to their English pronunciations. We demonstrate the influence of 18 Indian languages on IE by comparing native language features with IE pronunciations obtained from literature studies and phonetically annotated speech. Hence, we validate Indian language influences on IE by justifying pronunciation rules from the perspective of Indian language phonology. We obtain a comprehensive description of generalised and region-specific IE characteristics, which facilitates accent adaptation of existing speech systems",
    "checked": true,
    "id": "46c231d8fe8b349c8f9efc2c735e6e47cb876bed",
    "semantic_title": "an investigation of indian native language phonemic influences on l2 english pronunciations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23b_interspeech.html": {
    "title": "Identifying Stable Sections for Formant Frequency Extraction of French Nasal Vowels Based on Difference Thresholds",
    "volume": "main",
    "abstract": "Formant frequencies of a vowel are generally extracted from midpoints or central sections on the time axis. Nasal vowels present a challenge for obtaining stable formant frequencies, as the midpoint often falls in an anti-formant section where vocal energy is lost through the nasal cavity. This study proposes a stable section for extracting nasal vowel formant frequencies using difference thresholds, which identify a vowel as being distinct when F1 is above 60 Hz and/or F2 is above 200 Hz. For the experiment, 481 disyllabic words (232 nasal vowels and 294 oral counterparts) are selected from an online French-Korean Dictionary. Each vowel is divided into 10 intervals, and the stable section is identified as one or more continuous intervals with lower frequencies than the difference thresholds. The results show that the stable section for nasal vowels is identified in 20%~50% of the vowels, while the stable section for oral vowels is identified in 20%~80% of the vowels",
    "checked": true,
    "id": "ffc0917fa476bd69e07807bd7fd5c6e04c3ee97c",
    "semantic_title": "identifying stable sections for formant frequency extraction of french nasal vowels based on difference thresholds",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/audibert23_interspeech.html": {
    "title": "Evaluation of delexicalization methods for research on emotional speech",
    "volume": "main",
    "abstract": "Perceptual evaluation of non-controlled emotional speech requires delexicalization to neutralize semantic variation. However, most existing methods imply losing spectral cues crucial to emotional attribution, related to both laryngeal and supralaryngeal settings. We propose a method relying on voice morphing to retain part of the spectral information of the original stimuli, as an additional step to diphone synthesis delexicalization. After previous assessment of intelligibility loss, this study evaluates the naturalness of angry and neutral expressions in French films, delexicalized using low-pass filtering and the proposed method implemented with MBROLA and STRAIGHT. Results show that morphing-based delexicalization, which leads to accurate emotional attribution, is rated with a higher degree of naturalness than low-pass filtering. Implications for research in affective speech are discussed with regards to other delexicalization methods proposed in the literature",
    "checked": true,
    "id": "265234299bdb877577147edb9988999d87d0ed1d",
    "semantic_title": "evaluation of delexicalization methods for research on emotional speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kejriwal23b_interspeech.html": {
    "title": "Relationship between auditory and semantic entrainment using Deep Neural Networks (DNN)",
    "volume": "main",
    "abstract": "The tendency of people to engage in similar, matching, or synchronized behaviour when interacting is known as entrainment. Many studies examined linguistic (syntactic and lexical structures) and paralinguistic (pitch, intensity) entrainment, but less attention was given to finding the relationship between them. In this study, we utilized state-of-the-art DNN embeddings such as BERT and TRIpLet Loss network (TRILL) vectors to extract features for measuring semantic and auditory similarities of turns within dialogues in two comparable spoken corpora of two different languages. We found people's tendency to entrain on semantic features more when compared to auditory features. Additionally, we found that entrainment in semantic and auditory linguistic features are positively correlated. The findings of this study might assist in implementing the mechanism of entrainment in human-machine interaction (HMI)",
    "checked": true,
    "id": "586faa3c2f190c8535dabaf5a50326b8091e4ca7",
    "semantic_title": "relationship between auditory and semantic entrainment using deep neural networks (dnn)",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kejriwal23_interspeech.html": {
    "title": "Unsupervised Auditory and Semantic Entrainment Models with Deep Neural Networks",
    "volume": "main",
    "abstract": "Speakers tend to engage in adaptive behavior, known as entrainment, when they become similar to their interlocutor in various aspects of speaking. We present an unsupervised deep learning framework that derives meaningful representation from textual features for developing semantic entrainment. We investigate the model's performance by extracting features using different variations of the BERT model (DistilBERT and XLM-RoBERTa) and Google's universal sentence encoder (USE) embeddings on two human-human (HH) corpora (The Fisher Corpus English Part 1, Columbia games corpus) and one human-machine (HM) corpus (Voice Assistant Conversation Corpus (VACC)). In addition to semantic features we also trained DNN-based models utilizing two auditory embeddings (TRIpLet Loss network (TRILL) vectors, Low-level descriptors (LLD) features) and two units of analysis (Inter pausal unit and Turn). The results show that semantic entrainment can be assessed with our model, that models can distinguish between HH and HM interactions and that the two units of analysis for extracting acoustic features provide comparable findings",
    "checked": true,
    "id": "b7ed57a9b5643889007f909f6d9fa3eb586dbcf7",
    "semantic_title": "unsupervised auditory and semantic entrainment models with deep neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nielsen23_interspeech.html": {
    "title": "Parsing dialog turns with prosodic features in English",
    "volume": "main",
    "abstract": "Parsing spoken dialogue presents challenges that parsing text does not, including a lack of clear sentence boundaries. We know from previous work that prosody helps in parsing single sentences (Tran et al. 2018), but we want to show the effect of prosody on parsing speech that isn't segmented into sentences. In experiments on the English Switchboard corpus, we find prosody helps our model both with parsing and with accurately identifying sentence boundaries. However, we find that the best-performing parser is not necessarily the parser that produces the best sentence segmentation performance. We suggest that the best parses instead come from modelling sentence boundaries jointly with other constituent boundaries",
    "checked": true,
    "id": "ace1bbfa1475cacc40fa9bcd7735c8ff3f072fd0",
    "semantic_title": "parsing dialog turns with prosodic features in english",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muromachi23_interspeech.html": {
    "title": "Estimation of Listening Response Timing by Generative Model and Parameter Control of Response Substantialness Using Dynamic-Prompt-Tune",
    "volume": "main",
    "abstract": "A spoken dialogue system is required to continuously listen to a human user for smooth conversation. We propose a method that simultaneously performs response generation and listener response timing estimation. Our proposed method estimates listener response timing by adding pseudo-samples where listener response should be irrelevant, which allows using text-only conversation dataset without audio information. Furthermore, our proposed method can control substantialness of responses by user-specified parameter integrated with the Dynamic-Prompt-Tune method, which uses prompt token embedding dynamically generated from the parameter. Our automatic and manual evaluation showed that the proposed method can generate responses with more natural timing and more in line with the response substantialness parameter compared to the baseline model",
    "checked": true,
    "id": "7e79bb2d459fc45d41eb1e450867ebfb150f63c1",
    "semantic_title": "estimation of listening response timing by generative model and parameter control of response substantialness using dynamic-prompt-tune",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chowdhury23_interspeech.html": {
    "title": "Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "The diagnosis of autism spectrum disorder (ASD) is a complex, challenging task as it depends on the analysis of interactional behaviors by psychologists rather than the use of biochemical diagnostics. In this paper, we present a modeling approach to ASD diagnosis by analyzing acoustic/prosodic and linguistic features extracted from diagnostic conversations between a psychologist and children who either are typically developing (TD) or have ASD. We compare the contributions of different features across a range of conversation tasks. We focus on finding a minimal set of parameters that characterize conversational behaviors of children with ASD. Because ASD is diagnosed through conversational interaction, in addition to analyzing the behavior of the children, we also investigate whether the psychologist's conversational behaviors vary across diagnostic groups. Our results can facilitate fine-grained analysis of conversation data for children with ASD to support diagnosis and intervention",
    "checked": true,
    "id": "f48c0c9a314df02c45901e84c5fb83234e5b3123",
    "semantic_title": "parameter selection for analyzing conversations with autism spectrum disorder",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/buddi23_interspeech.html": {
    "title": "Efficient Multimodal Neural Networks for Trigger-less Voice Assistants",
    "volume": "main",
    "abstract": "The adoption of multimodal interactions by Voice Assistants (VAs) is growing rapidly to enhance human-computer interactions. Smartwatches have now incorporated trigger-less methods of invoking VAs, such as Raise To Speak (RTS), where the user raises their watch and speaks to VAs without an explicit trigger. Current state-of-the-art RTS systems rely on heuristics and engineered Finite State Machines to fuse gesture and audio data for multimodal decision-making. However, these methods have limitations, including limited adaptability, scalability, and induced human biases. In this work, we propose a neural network based audio-gesture multimodal fusion system that (1) Better understands temporal correlation between audio and gesture data, leading to precise invocations (2) Generalizes to a wide range of environments and scenarios (3) Is lightweight and deployable on low-power devices, such as smartwatches, with quick launch times (4) Improves productivity in asset development processes",
    "checked": true,
    "id": "4d3bfb15afee927cd63dc3640acd0ed6f1a446a2",
    "semantic_title": "efficient multimodal neural networks for trigger-less voice assistants",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ostrand23_interspeech.html": {
    "title": "Rapid Lexical Alignment to a Conversational Agent",
    "volume": "main",
    "abstract": "Conversational partners modify their language to be more similar to each other during interactions. This phenomenon, known as alignment, has been shown in human-human interactions, but there is little work on lexical alignment in human-computer interactions. We investigate whether people lexically align to a conversational agent, and whether the degree of alignment depends on feedback from the agent. This study compared three feedback conditions for how the agent responded to users' word choice: (1) the agent only understood the specific words that it produced itself; (2) the agent understood the words that it produced as well as more appropriate synonyms; (3) the agent's understanding of words that it did not produce was random. Participants significantly aligned to the agent in all conditions, and aligned more when they learned that the agent's comprehension was contingent on their alignment. Thus, inducing lexical alignment may be an effective way to increase dialogue success",
    "checked": true,
    "id": "c9bfaf9a2830e79bca9e64a8f8c4d08f843a2815",
    "semantic_title": "rapid lexical alignment to a conversational agent",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kurata23_interspeech.html": {
    "title": "Multimodal Turn-Taking Model Using Visual Cues for End-of-Utterance Prediction in Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "In this study, we propose a multimodal model for predicting the end-of-utterance probability in spoken dialogue systems, highlighting the unique role of visual cues in addition to acoustic and linguistic information. Although the effectiveness of visual cues, such as gaze, mouth, and head movements, has been suggested, few studies have fully incorporated them into turn-taking models, and the relative importance of these visual cues has also been underresearched. To address these issues, we first conducted an ablation study on visual features, showing the larger contribution of eye movements than mouth and head movements. Additionally, an end-to-end visual feature extraction model utilizing 3D-CNN is employed to comprehensively capture these visual cues. By combining visual features with acoustic and verbal information, AUC score for end-of-utterance prediction improved from 0.896 to 0.920, demonstrating the effectiveness of incorporating these visual cues in turn-taking models",
    "checked": true,
    "id": "539a253a8fcebbf54499a8ee0aaade8a6b788697",
    "semantic_title": "multimodal turn-taking model using visual cues for end-of-utterance prediction in spoken dialogue systems",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hojo23_interspeech.html": {
    "title": "Audio-Visual Praise Estimation for Conversational Video based on Synchronization-Guided Multimodal Transformer",
    "volume": "main",
    "abstract": "This study investigates praise estimation, the task of estimating the existence of preferable behaviors of a speaker in a conversational video. To estimate praises from multimodal information, considering synchronized behavior across modalities is important. Such cross-modal synchronization can be modeled by the conventional multimodal Transformer in a time-axis concatenation architecture because it models relevance between all time steps of all input modalities using attention matrices. However, the attention matrices are so high-dimensional that the model training can be difficult with a limited amount of training data. To alleviate this problem, we propose introducing a loss function representing the prior knowledge that the attention should link around the synchronized time steps across the input modalities. Our experiments on a business negotiation conversation corpus showed that the proposed method could improve the praise estimation's macro F1",
    "checked": true,
    "id": "7b24d9f4367e9413e9f892a9d836e1ffc8ff6001",
    "semantic_title": "audio-visual praise estimation for conversational video based on synchronization-guided multimodal transformer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sakuma23_interspeech.html": {
    "title": "Improving the response timing estimation for spoken dialogue systems by reducing the effect of speech recognition delay",
    "volume": "main",
    "abstract": "In conversational systems, the proper timing of the system's response is critical to maintaining a comfortable conversation. To achieve appropriate timing estimation, it is important to know what the users have said, including their most recent words, but ASR delay usually prevents the use of full user utterance. In this paper, we attempted to employ an extremely low latency ASR model called Multi-Look-Ahead ASR by Zhao et al. to enable near full utterance for response timing estimation. Additionally, we examined the effectiveness of using low latency ASR in combination with a parameter called Estimates of Syntactic Completeness (ESC), which indicates how soon the user's speech is completed. We evaluated on a Japanese simulated dialog database of a restaurant information center. The results confirmed that reducing ASR delay improves the accuracy of response timing estimation. This effect also appeared when the method using ESC is combined with the use of low latency ASR",
    "checked": true,
    "id": "26ecebd086c6b7ece6e321400d741dc62e965cb5",
    "semantic_title": "improving the response timing estimation for spoken dialogue systems by reducing the effect of speech recognition delay",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23c_interspeech.html": {
    "title": "Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Recognizing emotions in speech is essential for improving human-computer interactions, which require understanding and responding to the users' emotional states. Integrating multiple modalities, such as speech and text, enhances the performance of speech emotion recognition systems by providing a varied source of emotional information. In this context, we propose a model that enhances cross-modal transformer fusion by applying focus attention mechanisms to align and combine the salient features of two different modalities, namely, speech and text. The analysis of the disentanglement of the emotional representation various multiple embedding spaces using deep metric learning confirmed that our method shows enhanced emotion recognition performance. Furthermore, the proposed approach was evaluated on the IEMOCAP dataset. Experimental results demonstrated that our model achieves the best performance among other relevant multimodal speech emotion recognition systems",
    "checked": true,
    "id": "09d1fa85766d64f8db42a72d7c117d2356848f02",
    "semantic_title": "focus-attention-enhanced crossmodal transformer with metric learning for multimodal speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23la_interspeech.html": {
    "title": "A Multiple-Teacher Pruning Based Self-Distillation (MT-PSD) Approach to Model Compression for Audio-Visual Wake Word Spotting",
    "volume": "main",
    "abstract": "We propose a novel model compression approach using multiple-teacher pruning based self-distillation for audio-visual wake word spotting, facilitating compact neural network implementations without sacrificing system performances. In each stage of the proposed framework, we prune a teacher model obtained in the previous stage to generate a student model, then fine-tune it with teacher-student learning and use it as a new teacher model for following stages. A normalized intra-class loss is designed to optimize this pruning based self-distillation (PSD) process. Both single-teacher PSD (ST-PSD) and multi-teacher PSD (MT-PSD) are adopted in the fine-tuning process each stage. When tested on audio-visual wake word spotting in MISP2021 Challenge, the two proposed techniques outperform state-of-the-art methods in both system performances and model efficiencies. Moreover, MT-PSD that leverages upon the complementarity of multiple teachers obtained in different stages also outperforms ST-PSD",
    "checked": true,
    "id": "9ffffa0547892da041c60c6bac37073b271ef3e4",
    "semantic_title": "a multiple-teacher pruning based self-distillation (mt-psd) approach to model compression for audio-visual wake word spotting",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/spiesberger23_interspeech.html": {
    "title": "Abusive Speech Detection in Indic Languages Using Acoustic Features",
    "volume": "main",
    "abstract": "Abusive content in online social networks is a well-known problem that can cause serious psychological harm and incite hatred. The ability to upload audio data increases the importance of developing methods to detect abusive content in speech recordings. However, simply transferring the mechanisms from written abuse detection would ignore relevant information such as emotion and tone. In addition, many current algorithms require training in the specific language for which they are being used. This paper proposes to use acoustic and prosodic features to classify abusive content. We used the ADIMA data set, which contains recordings from ten Indic languages, and trained different models in multilingual and cross-lingual settings. Our results show that it is possible to classify abusive and non-abusive content using only acoustic and prosodic features. The most important and influential features are discussed",
    "checked": true,
    "id": "56cbba328eb0be875798f24b5dd52b04655329b5",
    "semantic_title": "abusive speech detection in indic languages using acoustic features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ingle23_interspeech.html": {
    "title": "Listening To Silences In Contact Center Conversations Using Textual Cues",
    "volume": "main",
    "abstract": "Contact center conversations often consist of silent segments, where neither the customer nor the agent is speaking. These silences if continued beyond an acceptable level can negatively impact contact center KPIs. Thus, understanding silences and defining measures to handle them better via appropriate coach- ing and alerting for agents is one of the key focus areas for contact centers. In this paper, we demonstrate how dialogue turns around silences could be used to understand the characteristics of silences (expected vs unexpected and agent vs cusomer-caused silences) via two text classification tasks. We propose a methodology to pre-train a silence-aware language model in contact center domain, called Silence-RoBERTa and demonstrate its ability to better capture the conversational characteristics around silences. Finally, we discuss the application of the above methodology in real-time and post-call settings and demonstrate its usability to reduce silences via a reallife case study",
    "checked": true,
    "id": "564b74b682a3919f945c77e91af54913e1ddd202",
    "semantic_title": "listening to silences in contact center conversations using textual cues",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeen23_interspeech.html": {
    "title": "I Learned Error, I Can Fix It! : A Detector-Corrector Structure for ASR Error Calibration",
    "volume": "main",
    "abstract": "Speech recognition technology has improved recently. However, in the context of spoken language understanding (SLU), containing automatic speech recognition (ASR) errors causes significant downstream performance degradation. To address this issue, various ASR error correction methodologies have been proposed. ASR error correction mainly focuses on correcting and generating only the error span using a conditional decoding method. To this end, we propose a structure with a Detector that uses collaborative training to predict various error patterns and a Corrector that corrects the detected error span by Detector. This pipeline reduces Word Error Rate (WER) and shows less performance degradation in downstream tasks compared with the original ASR hypotheses. In addition, it was shown that it could be generalized to various downstream data. By leveraging this Detector-Corrector pipeline, we expect to achieve effective ASR error correction and enable high-quality SLU downstream tasks",
    "checked": true,
    "id": "1e1df3c1e57f86d5b9eaa77cb73ec0ae35bc05a3",
    "semantic_title": "i learned error, i can fix it! : a detector-corrector structure for asr error calibration",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/garnier23b_interspeech.html": {
    "title": "Verbal and nonverbal feedback signals in response to increasing levels of miscommunication",
    "volume": "main",
    "abstract": "This study aims to explore in detail how listeners respond to communication disruptions in a task-oriented dialogue. We conducted an experiment with participants playing a map task with a partner via a video conferencing system that showed seemingly random breakdowns. In fact, the breakdowns were scripted to induce increasing levels of miscommunication. After an initial interactive session, a second non-interactive session was recorded with one-sided communication from the task leader. Among the fifty or so verbal and nonverbal feedback signals observed, twelve were produced by more than half of the participants. A detailed analysis of their use in different situations, their timing and their co-occurrence, supported that they may have different functions: some appear to be personal reactions of uncertainty, misunderstanding, or inability to complete the task, whereas others were clear repair initiators or turn-taking signals deliberately addressed to the interlocutor",
    "checked": true,
    "id": "9f23c2e883b72f76495823c9f891041f7c337b73",
    "semantic_title": "verbal and nonverbal feedback signals in response to increasing levels of miscommunication",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/amiriparian23_interspeech.html": {
    "title": "Speech-Based Classification of Defensive Communication: A Novel Dataset and Results",
    "volume": "main",
    "abstract": "Defensive communication is known to have detrimental effects on the quality of social interactions. Hence, recognising and reducing defensive behaviour is crucial to improving professional and personal communication. We introduce DefComm-DB, a novel multimodal dataset comprising video recordings in which one of the following types of defensive communication is present: (i) verbally attacking the conversation partner, (ii) withdrawing from the communication, (iii) making oneself greater, and (iv) making oneself smaller. Subsequently, we present a machine learning approach for the automatic classification of DefComm-DB. In particular, we utilise wav2vec2, autoencoders, a pre-trained CNN and openSMILE for feature extraction from the audio modality. For the text stream, we apply ELECTRA and SBERT. On the unseen test set, our models achieve an Unweighted Average Recall of 49.4 % and 52.2 % for the audio and text modalities, respectively, showing the feasibility of the introduced challenge",
    "checked": true,
    "id": "3c77a9bfe90a4ff9abfe9634cb6051ef5e2e9c7d",
    "semantic_title": "speech-based classification of defensive communication: a novel dataset and results",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wallbridge23_interspeech.html": {
    "title": "Quantifying the perceptual value of lexical and non-lexical channels in speech",
    "volume": "main",
    "abstract": "Speech is a fundamental means of communication that can be seen to provide two channels for transmitting information: the lexical channel of which words are said, and the non-lexical channel of how they are spoken. Both channels shape listener expectations of upcoming communication; however, directly quantifying their relative effect on expectations is challenging. Previous attempts require spoken variations of lexically-equivalent dialogue turns or conspicuous acoustic manipulations. This paper introduces a generalised paradigm to study the value of non-lexical information in dialogue across unconstrained lexical content. By quantifying the perceptual value of the non-lexical channel with both accuracy and entropy reduction, we show that non-lexical information produces a consistent effect on expectations of upcoming dialogue: even when it leads to poorer discriminative turn judgements than lexical content alone, it yields higher consensus among participants",
    "checked": true,
    "id": "9b9bfca7ac8b4a44c68a755901435c8d96f345b0",
    "semantic_title": "quantifying the perceptual value of lexical and non-lexical channels in speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tsubokura23_interspeech.html": {
    "title": "Relationships Between Gender, Personality Traits and Features of Multi-Modal Data to Responses to Spoken Dialog Systems Breakdown",
    "volume": "main",
    "abstract": "Automated dialog systems are currently being used in various applications, but it is unclear if they will ever be able to converse as naturally as humans do. One challenge is avoiding breakdowns during conversations due to inappropriate system utterances. Although many studies have focused on dialog breakdown detection, the influence of differences among individual users on dialog breakdowns and breakdown detection has not been sufficiently examined. In this study, we focus on individual differences thought to be related to emotional responses after breakdowns, specifically language, acoustic, and facial features, as well as gender and BigFive personality traits, to analyze differences in user responses to breakdowns. Our results suggest that gender and personality traits influence user responses to dialog breakdowns. For example, users with low Openness scores were more likely to express anger, while women were less likely to do so",
    "checked": true,
    "id": "91883360bfd0dcb77bc13264a43af8a600a2153f",
    "semantic_title": "relationships between gender, personality traits and features of multi-modal data to responses to spoken dialog systems breakdown",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23e_interspeech.html": {
    "title": "Speaker-aware Cross-modal Fusion Architecture for Conversational Emotion Recognition",
    "volume": "main",
    "abstract": "Conversational Emotion Recognition (CER) is an important topic in the construction of intelligent human-machine interaction systems. The emotion is mainly influenced by the conversational context and the speakers. In addition, sufficient utilization of the relevant features of both speech and text modes is also crucial to the performance of CER. Based on the above considerations, we propose a novel Speaker-aware Cross-modal Fusion Architecture (SCFA). Within a single modality, we design a conversation encoder, including a context encoder and a speaker-aware encoder, to model the conversational content and the intra- and inter-speaker influence, respectively. On this basis, cross-modal fusion attention is introduced to extract the cross-modal characteristics of the conversation, so as to better detect the emotions in conversation. We conduct experiments on the IEMOCAP and MELD datasets. Compared with state-of-the-art baselines, SCFA achieves better performance on average",
    "checked": true,
    "id": "585a679992bbb908c7c7f9ce3cdd033a832ea47e",
    "semantic_title": "speaker-aware cross-modal fusion architecture for conversational emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liao23_interspeech.html": {
    "title": "Blind Estimation of Room Impulse Response from Monaural Reverberant Speech with Segmental Generative Neural Network",
    "volume": "main",
    "abstract": "This paper presents a generative neural network to estimate room impulse response (RIR) directly from the received reverberant speech in single-channel scenario. Complex spectrogram of the reverberant speech is used as the input of an encoder to produce the compact acoustic embedding, which is then fed to a generator to construct the related time-domain acoustic response. To avoid a large model to generate the RIR with long taps, we propose SG-RIR, a novel segmental generative network that splits the RIR into segments and shares the network parameters across segments for blind RIR estimation. Experimental results show that the proposed model is capable of estimating the time-domain RIR with mean error of 0.008 in terms of both simulated and measured RIR test sets. The effectiveness is further verified by the achieved competitive estimation accuracy of two key room acoustic parameters (the reverberation time RT and the direct-to-reverberant ratio DRR) as compared to state-of-the-art approaches that are specific for RT and DRR estimation",
    "checked": true,
    "id": "6faeffd6dfc853a89248feaf78856e770651ed90",
    "semantic_title": "blind estimation of room impulse response from monaural reverberant speech with segmental generative neural network",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ren23_interspeech.html": {
    "title": "Emotion-Aware Audio-Driven Face Animation via Contrastive Feature Disentanglement",
    "volume": "main",
    "abstract": "In this paper, we tackle the problem of audio-driven face animation which aims to synthesize a realistic talking face given a piece of driven speech. Directly modeling the mapping from audio feature to facial expression is challenging, since people tend to have different talking styles with momentary emotion states as well as identity-dependent vocal characteristics. To address this challenge, we propose a contrastive feature disentanglement method for emotion-aware face animation. The key idea is to disentangle the features for speech content, momentary emotion and identity-dependent vocal characteristics from audio features with a contrastive learning strategy. Experiments on public datasets show that our method can generate more realistic facial expression and enables synthesis of diversified face animation with different emotion",
    "checked": true,
    "id": "551829633213b7a42ccc471884885a091ac14ea2",
    "semantic_title": "emotion-aware audio-driven face animation via contrastive feature disentanglement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shimonishi23_interspeech.html": {
    "title": "Anomalous Sound Detection Based on Sound Separation",
    "volume": "main",
    "abstract": "This paper proposes an unsupervised anomalous sound detection method using sound separation. In factory environments, background noise and non-objective sounds obscure desired machine sounds, making it challenging to detect anomalous sounds. Therefore, using sounds not mixed with background noise or non-purpose sounds in the detection system is desirable. We compared two versions of our proposed method, one using sound separation as a pre-processing step and the other using separation-based outlier exposure that uses the error between two separated sounds. Based on the assumption that differences in separation performance between normal and anomalous sounds affect detection results, a sound separation model specific to a particular product type was used in both versions. Experimental results indicate that the proposed method improved anomalous sound detection performance for all Machine IDs, achieving a maximum improvement of 39%",
    "checked": true,
    "id": "03658a4a0f5a9f9fa7609f46b699d224e16ecce1",
    "semantic_title": "anomalous sound detection based on sound separation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fahed23_interspeech.html": {
    "title": "Random Forest Classification of Breathing Phases from Audio Signals Recorded using Mobile Devices",
    "volume": "main",
    "abstract": "Respiration rate (RR) and other respiratory features, such as inhale-to-exhale ratio (IER) and duration of breathing phases can be used as marker of respiratory and lung conditions and to modulate autonomic function in biofeedback applications. In this study, audio respiration signals were recorded by 112 participants using smartphones. RR was estimated using a frequency-domain method. Acoustic features were extracted from the audio signals and random forest was used to classify inhales, exhales and respiratory pauses, with ROC AUCs of 0.84 and 0.95 for inhales and exhales respectively. RR was estimated with a mean absolute error (MAE) of 0.63 bpm. IER was estimated with a MAE of 0.37, with 76% of the dataset reporting a MAE of less than 0.20. The results demonstrate a computationally efficient approach to estimate respiratory features from audio signals recorded using smartphones that can be easily implemented in real-time for large-scale home monitoring or biofeedback applications",
    "checked": true,
    "id": "4af40d14e5cbd97a3701b042d10a5d3c994fe678",
    "semantic_title": "random forest classification of breathing phases from audio signals recorded using mobile devices",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahn23b_interspeech.html": {
    "title": "GRAVO: Learning to Generate Relevant Audio from Visual Features with Noisy Online Videos",
    "volume": "main",
    "abstract": "Given a video, previous video-to-audio generation methods use a hierarchical auto-regressive language model to produce a sequence of audio tokens to be decoded into a waveform. The audio generation depends only on the previous audio token and the current image but ignores the surrounding images that may have useful information. To learn the relationships between image frames, in this paper, we introduce GRAVO (Generate Relevant Audio from Visual features with Online videos), which employs multi-head attention (MHA) to encode rich context information and guide the audio decoder to produce more accurate audio tokens. Moreover, two auxiliary losses are introduced to explicitly supervise the MHA behavior, maximizing the similarity between the MHA output vector and the target waveform representation while preserving the original visual semantic information. Experimental results demonstrate that GRAVO surpasses state-of-the-art models on ImageHear and VGG-Sound datasets",
    "checked": true,
    "id": "d23ce9a9db54a747b73d811b8eaf4dba6721c748",
    "semantic_title": "gravo: learning to generate relevant audio from visual features with noisy online videos",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhai23_interspeech.html": {
    "title": "Wav2ToBI: a new approach to automatic ToBI transcription",
    "volume": "main",
    "abstract": "ToBI is a prosody labeling system that transcribes American English prosody in terms of phonological tones and break indices. Previous works on automatic ToBI transcription require additional information such as word boundaries and use modular feature extraction with separately optimized feature detectors and classifiers. We are interested in investigating if a neural network-based approach would also result in high performance on automatic ToBI transcription without additional information. In this paper, we investigate the problem of pitch accent detection and prosody boundary detection using the Wav2vec 2.0 model with only acoustic information. Our model is trained on the Boston University Radio News Corpus and evaluated on both the Boston University Radio News Corpus and the Boston Directions Corpus. We show that it achieves an F1 score of 0.82 on pitch accent detection and 0.86 on phrase boundary detection. Code and model weights are available",
    "checked": true,
    "id": "c9644183da9c149073baf94a2fecc6a3910df439",
    "semantic_title": "wav2tobi: a new approach to automatic tobi transcription",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23b_interspeech.html": {
    "title": "Joint-Former: Jointly Regularized and Locally Down-sampled Conformer for Semi-supervised Sound Event Detection",
    "volume": "main",
    "abstract": "Semi-supervised Sound Event Detection (SSED) is to recognize the categories of events and mark their onset and offset times using a small amount of weakly-labeled and a large-scale of unlabeled data. To exploit unlabeled data effectively and reduce over-fitting, regularization techniques play a critical role in SSED. In this paper, we proposed a novel jointly regularized and locally down-sampled Conformer (Joint-Former) model for SSED. Joint-Former first locally down-samples the spectrogram and learns the token representations with high temporal resolution and low computational cost. Then, Joint-Former effectively exploits unlabelled data in SSED by integrating Mean-Teacher and Masked Spectrogram Modeling using joint regularization through a multitask learning framework. Extensive experiments on DCASE 2019, DCASE 2020, and DCASE 2021 task4 SSED datasets show that Joint-Former greatly outperformed existing methods",
    "checked": true,
    "id": "71f15440c6f9369eeb2b9399416a7c329a8dca5c",
    "semantic_title": "joint-former: jointly regularized and locally down-sampled conformer for semi-supervised sound event detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/goel23_interspeech.html": {
    "title": "Towards Attention-based Contrastive Learning for Audio Spoof Detection",
    "volume": "main",
    "abstract": "Vision transformers (ViT) have made substantial progress for classification tasks in computer vision. Recently, Gong et. al. '21, introduced attention-based modeling for several audio tasks. However, relatively unexplored is the use of a ViT for audio spoof detection task. We bridge this gap and introduce ViTs for this task. A vanilla baseline built on fine-tuning the SSAST (Gong et. al. '22) audio ViT model achieves sub-optimal equal error rates (EERs). To improve performance, we propose a novel attention-based contrastive learning framework (SSAST-CL) that uses cross-attention to aid the representation learning. Experiments show that our framework successfully disentangles the bonafide and spoof classes and helps learn better classifiers for the task. With appropriate data augmentations policy, a model trained on our framework achieves competitive performance on the ASVSpoof 2021 challenge. We provide comparisons and ablation studies to justify our claim",
    "checked": true,
    "id": "7e97e03e9c4a8dfb45bfaeb3296716f7b261850d",
    "semantic_title": "towards attention-based contrastive learning for audio spoof detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23d_interspeech.html": {
    "title": "Masked Audio Modeling with CLAP and Multi-Objective Learning",
    "volume": "main",
    "abstract": "Most existing masked audio modeling (MAM) methods learn audio representations by masking and reconstructing local spectrogram patches. However, the reconstruction loss mainly accounts for the signal-level quality of the reconstructed spectrogram and is still limited in extracting high-level audio semantics. In this paper, we propose to enhance the semantic modeling of MAM by distilling cross-modality knowledge from contrastive language-audio pretraining (CLAP) representations for both masked and unmasked regions (MAM-CLAP) and leveraging a multi-objective learning strategy with a supervised classification branch (SupMAM), thereby providing more semantic knowledge for MAM and enabling it to effectively learn global features from labels. Experiments show that our methods significantly improve the performance on multiple downstream tasks. Furthermore, by combining our MAM-CLAP with SupMAM, we can achieve new state-of-the-art on various audio and speech classification tasks, exceeding previous self-supervised and supervised pretraining methods",
    "checked": true,
    "id": "664edaab7a29dda2c59be94f05c654c3e61f6c3f",
    "semantic_title": "masked audio modeling with clap and multi-objective learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rusci23_interspeech.html": {
    "title": "Few-Shot Open-Set Learning for On-Device Customization of KeyWord Spotting Systems",
    "volume": "main",
    "abstract": "A personalized KeyWord Spotting (KWS) pipeline typically requires the training of a Deep Learning model on a large set of user-defined speech utterances, preventing fast customization directly applied on-device. To fill this gap, this paper investigates few-shot learning methods for open-set KWS classification by combining a deep feature encoder with a prototype-based classifier. With user-defined keywords from 10 classes of the Google Speech Command dataset, our study reports an accuracy of up to 76% in a 10-shot scenario while the false acceptance rate of unknown data is kept to 5%. In the analyzed settings, the usage of the triplet loss to train an encoder with normalized output features performs better than the prototypical networks jointly trained with a generator of dummy unknown-class prototypes. This design is also more effective than encoders trained on a classification problem and features fewer parameters than other iso-accuracy approaches",
    "checked": true,
    "id": "190f2b31d19003f02e25ea4a153fb0dfbb381dfc",
    "semantic_title": "few-shot open-set learning for on-device customization of keyword spotting systems",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/azeemi23_interspeech.html": {
    "title": "Self-Supervised Dataset Pruning for Efficient Training in Audio Anti-spoofing",
    "volume": "main",
    "abstract": "The computational cost for training neural anti-spoofing models has rapidly increased due to larger network architectures. Several dataset-pruning metrics have been proposed to increase the training efficiency of these models. However, these metrics require example labels and an initial training step to compute example scores which is computationally intensive. We propose a novel self-supervised pruning metric for efficient dataset pruning in neural anti-spoofing models. Our method identifies important examples and prunes the dataset in an efficient, self-supervised manner using the clustered embedding representation of audios. We demonstrate that our method exceeds the performance of four other pruning metrics on the ASVSpoof 2019 dataset across two anti-spoofing models while being 91% computationally more efficient. We also find differences in the distribution of certain attacks, which helps explain the better performance of self-supervised pruning over other metrics",
    "checked": true,
    "id": "00e19f0636f49ea712087972b710b0ff467a4787",
    "semantic_title": "self-supervised dataset pruning for efficient training in audio anti-spoofing",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23b_interspeech.html": {
    "title": "Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR",
    "volume": "main",
    "abstract": "We propose a method of segmenting long-form speech by separating semantically complete sentences within the utterance. This prevents the ASR decoder from needlessly processing faraway context while also preventing it from missing relevant context within the current sentence. Semantically complete sentence boundaries are typically demarcated by punctuation in written text; but unfortunately, spoken real-world utterances rarely contain punctuation. We address this limitation by distilling punctuation knowledge from a bidirectional teacher language model (LM) trained on written, punctuated text. We compare our segmenter, which is distilled from the LM teacher, against a segmenter distilled from a acoustic-pause-based teacher used in other works, on a streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2% relative WER gain along with a 60 ms median end-of-segment latency reduction on a YouTube captioning task",
    "checked": true,
    "id": "a18c778ec3b8aa2e0846c7d81f260db0013bc6bd",
    "semantic_title": "semantic segmentation with bidirectional language models improves long-form asr",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mariotte23_interspeech.html": {
    "title": "Multi-microphone Automatic Speech Segmentation in Meetings Based on Circular Harmonics Features",
    "volume": "main",
    "abstract": "Speaker diarization is the task of answering Who spoke and when? in an audio stream. Pipeline systems rely on speech segmentation to extract speakers' segments and achieve robust speaker diarization. This paper proposes a common framework to solve three segmentation tasks in the distant speech scenario: Voice Activity Detection (VAD), Overlapped Speech Detection (OSD), and Speaker Change Detection (SCD). In the literature, a few studies investigate the multi-microphone distant speech scenario. In this work, we propose a new set of spatial features based on direction-of-arrival estimations in the circular harmonic domain (CH-DOA). These spatial features are extracted from multi-microphone audio data and combined with standard acoustic features. Experiments on the AMI meeting corpus show that CH-DOA can improve the segmentation while being robust in case of deactivated microphones",
    "checked": true,
    "id": "8730f1ae4447bfad9372a58625dac229da43087f",
    "semantic_title": "multi-microphone automatic speech segmentation in meetings based on circular harmonics features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23h_interspeech.html": {
    "title": "Advanced RawNet2 with Attention-based Channel Masking for Synthetic Speech Detection",
    "volume": "main",
    "abstract": "Automatic speaker verification (ASV) systems are often vulnerable to spoofing attacks, particularly unseen attacks. Due to the diversity of text-to-speech and voice conversion algorithms, how to improve the generalization ability of synthetic speech detection systems is a challenging issue. To address this issue, we propose an advanced RawNet2 (ARawNet2) by introducing an attention-based channel masking (ACM) block to improve the RawNet2, with three main components: the squeeze-and-excitation, the channel masking, and a global-local feature aggregation. The effectiveness of the proposed system is evaluated on both the ASVspoof 2019 and ASVspoof 2021 datasets. Specifically, the ARawNet2 achieves an EER of 4.61% on the ASVspoof 2019 logical access (LA) task, and on the ASVspoof 2021 LA and speech deepfake (DF) tasks, it achieves EER of 8.36% and 19.03%, which obtains relative 12.00% and 14.97% EER reductions over the RawNet2 baseline, respectively",
    "checked": true,
    "id": "1568e8059166e695436c05d7af619f130793f7b0",
    "semantic_title": "advanced rawnet2 with attention-based channel masking for synthetic speech detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martinezsevilla23_interspeech.html": {
    "title": "Insights into end-to-end audio-to-score transcription with real recordings: A case study with saxophone works",
    "volume": "main",
    "abstract": "Neural end-to-end Audio-to-Score (A2S) transcription aims to retrieve a score that encodes the music content of an audio recording in a single step. Due to the recentness of this formulation, the existing works have exclusively addressed controlled scenarios with synthetic data that fail to provide conclusions applicable to real-world cases. In response to this gap in the literature, this work introduces a novel assortment of real saxophone recordings---together with their digital scores---and poses several experimental scenarios involving real and synthetic data. The obtained results confirm the adequacy of this A2S framework to deal with real data as well as proving the relevance of leveraging synthetic interpretations to improve the recognition rate in scenarios with real-data scarcity",
    "checked": true,
    "id": "1ab7b812887b4b1073ed532bcc005d63aad8f5f2",
    "semantic_title": "insights into end-to-end audio-to-score transcription with real recordings: a case study with saxophone works",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23d_interspeech.html": {
    "title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers",
    "volume": "main",
    "abstract": "In this paper, we focus on Whisper, a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions. We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type. With this finding, we build a unified audio tagging and speech recognition model Whisper-AT by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it. With <1% extra computational cost, Whisper-AT can recognize audio events, in addition to spoken text, in a single forward pass",
    "checked": true,
    "id": "98bd673440c579bcca2d434bca86f97c9b40c74c",
    "semantic_title": "whisper-at: noise-robust automatic speech recognizers are also strong general audio event taggers",
    "citation_count": 15,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23b_interspeech.html": {
    "title": "Synthetic Voice Spoofing Detection based on Feature Pyramid Conformer",
    "volume": "main",
    "abstract": "In speech anti-spoofing, artefacts used to detect spoofed speech are often located in specific sub-bands. Previous works often use Convolution Neural Networks (CNNs) as backbone which are good at capturing local features. However, if artefacts simultaneously exist in different sub-bands, CNNs cannot model this kind of information. Thus, we propose to use Feature Pyramid Conformer to solve this issue. Conformer can capture both local and global features. We aggregate the outputs of each Conformer block with Feature Pyramid Module. Through addition and lateral connection, the aggregation can be better integrated. Besides, to improve generalization of detecting unknown attacks, we propose to adopt Elastic penalty Margin Softmax. It can enhance intra-class compactness and inter-class discrepancy flexibly. Without data augmentaion, our system achieve an Equal Error Rate (EER) of 1.65% on the evaluation set of ASVspooof 2019 logical access, outperforming most existing systems",
    "checked": true,
    "id": "4071c9a56c536bf39d4c107edb1231150a09db38",
    "semantic_title": "synthetic voice spoofing detection based on feature pyramid conformer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23c_interspeech.html": {
    "title": "Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection",
    "volume": "main",
    "abstract": "In recent years, Audio Deepfake Detection (ADD) models have shown promising results in intra-domain. However, they do not perform well in cross-domain scenarios. This is mainly due to the limited variety of domain types and attack methods in training data, as well as insufficient research on hidden feature representation. To address these issues, we present W2VASDG, a generalized ADD system including a self-supervised representation front-end and a domain generalization backbone. Furthermore, we try to learn an ideal feature space which aggregates real speech and separates fake speech. Fake speech varies significantly by different forgery methods, while real speech varies less. In light of this, we further propose the aggregation and separation domain generalization (ASDG) method as the back-end to learn a domain invariant feature representation. Experiments show that our W2V-ASDG outperforms baseline models in cross-domains and gets the lowest average equal error rates (EER) of 4.60%",
    "checked": true,
    "id": "8d48b4395ad4f61717e5fc202d0c45ac4f1f1365",
    "semantic_title": "learning a self-supervised domain-invariant feature representation for generalized audio deepfake detection",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kerpicci23_interspeech.html": {
    "title": "Application of Knowledge Distillation to Multi-Task Speech Representation Learning",
    "volume": "main",
    "abstract": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models",
    "checked": true,
    "id": "925ac929940720f16d0c54cd3ec0cba341f95bdf",
    "semantic_title": "application of knowledge distillation to multi-task speech representation learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23g_interspeech.html": {
    "title": "DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes",
    "volume": "main",
    "abstract": "Lifelong audio feature extraction involves learning new sound classes incrementally, which is essential for adapting to new data distributions over time. However, optimizing the model only on new data can lead to catastrophic forgetting of previously learned tasks, which undermines the model's ability to perform well over the long term. This paper introduces a new approach to continual audio representation learning called DeCoR. Unlike other methods that store previous data, features, or models, DeCoR indirectly distills knowledge from an earlier model to the latest by predicting quantization indices from a delayed codebook. We demonstrate that DeCoR improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning. Our approach introduces minimal storage and computation overhead, making it a lightweight and efficient solution for continual learning",
    "checked": true,
    "id": "39f1ee5c35c7f514d57c8465dba2d26502ee140c",
    "semantic_title": "decor: defy knowledge forgetting by predicting earlier audio codes",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/almudevar23_interspeech.html": {
    "title": "Variational Classifier for Unsupervised Anomalous Sound Detection under Domain Generalization",
    "volume": "main",
    "abstract": "Unsupervised anomalous sound detection typically involves using a classifier with the last layer removed to extract embeddings. After that the cosine distance between train and test embeddings as anomaly score is used. In this paper, we propose a new idea which we call variational classifier that force the embeddings to follow a distribution imposed by design that can depend on the class of the input among other factors. To achieve this goal, in addition to the cross-entropy, we add to the loss function the KL divergence between these distributions and the one followed by the training embeddings. This enhances the ability of the system to differentiate between classes and it allows us to use sampling methods and to calculate the log-likelihood of a test embedding in the train embeddings distributions. We tested this proposal on the DCASE 2022 Task 2 dataset and observed improvements in both classification and unsupervised anomaly detection, which is the primary task",
    "checked": true,
    "id": "a07379ca1717c16fbe9c505f6435472aa99bcd0c",
    "semantic_title": "variational classifier for unsupervised anomalous sound detection under domain generalization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23c_interspeech.html": {
    "title": "FlexiAST: Flexibility is What AST Needs",
    "volume": "main",
    "abstract": "The objective of this work is to give patch-size flexibility to Audio Spectrogram Transformers (AST). Recent advancements in ASTs have shown superior performance in various audio-based tasks. However, the performance of standard ASTs drastically drops when evaluated at different patch sizes than they were trained at. As a result, AST models are typically retrained to accommodate changes in patch sizes. To overcome this limitation, this paper demonstrates a training procedure to provide flexibility to standard AST models without architectural changes, allowing them to work with various patch sizes at the inference stage - FlexiAST. This proposed training approach simply utilizes random patch size selection and resizing of patch and positional embedding weights. Our experiments show that FlexiAST gives similar performance to standard AST models while maintaining its evaluation ability at various patch sizes on different datasets for audio classification tasks",
    "checked": true,
    "id": "b61b85112c5553a6546102480758712c82163d13",
    "semantic_title": "flexiast: flexibility is what ast needs",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23c_interspeech.html": {
    "title": "MCR-Data2vec 2.0: Improving Self-supervised Speech Pre-training via Model-level Consistency Regularization",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has shown significant progress in speech processing tasks. However, despite the intrinsic randomness in the Transformer structure, such as dropout variants and layer-drop, improving the model-level consistency remains under-explored in the speech SSL literature. To address this, we propose a new pre-training method that uses consistency regularization to improve Data2vec 2.0, the recent state-of-the-art (SOTA) SSL model. Specifically, the proposed method involves sampling two different student sub-models within the Data2vec 2.0 framework, enabling two output variants derived from a single input without additional parameters. Subsequently, we regularize the outputs from the student sub-models to be consistent and require them to predict the representation of the teacher model. Our experimental results demonstrate that the proposed approach improves the SSL model's robustness and generalization ability, resulting in SOTA results on the SUPERB benchmark",
    "checked": true,
    "id": "b5409e42cb24bca75569183e07f23b0b5c4c06c3",
    "semantic_title": "mcr-data2vec 2.0: improving self-supervised speech pre-training via model-level consistency regularization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23l_interspeech.html": {
    "title": "Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention",
    "volume": "main",
    "abstract": "Audio captioning aims to generate text descriptions of audio clips. In the real world, many objects produce similar sounds. How to accurately recognize ambiguous sounds is a major challenge for audio captioning. In this work, inspired by inherent human multimodal perception, we propose visually-aware audio captioning, which makes use of visual information to help the description of ambiguous sounding objects. Specifically, we introduce an off-the-shelf visual encoder to extract video features and incorporate the visual features into an audio captioning system. Furthermore, to better exploit complementary audio-visual contexts, we propose an audio-visual attention mechanism that adaptively integrates audio and visual context and removes the redundant information in the latent space. Experimental results on AudioCaps, the largest audio captioning dataset, show that our proposed method achieves state-of-the-art results on machine translation metrics",
    "checked": true,
    "id": "2b6e41761ac23c106963677304a899a57c7d75e0",
    "semantic_title": "visually-aware audio captioning with adaptive audio-visual attention",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ditthapron23_interspeech.html": {
    "title": "Masking Kernel for Learning Energy-Efficient Representations for Speaker Recognition and Mobile Health",
    "volume": "main",
    "abstract": "Modern smartphones possess hardware for audio acquisition and to perform speech processing tasks such as speaker recognition and health assessment. However, energy consumption remains a concern, especially for resource-intensive DNNs. Prior work has improved the DNN energy efficiency by utilizing a compact model or reducing the dimensions of speech features. Both approaches reduced energy consumption during DNN inference but not during speech acquisition. This paper proposes using a masking kernel integrated into gradient descent during DNN training to learn the most energy-efficient speech length and sampling rate for windowing, a common step for sample construction. To determine the most energy-optimal parameters, a masking function with non-zero derivatives was combined with a low-pass filter. The proposed approach minimizes the energy consumption of both data collection and inference by 57%, and is competitive with speaker recognition and traumatic brain injury detection baselines",
    "checked": true,
    "id": "78b492e24eec182694f24aac575c66f9abe71565",
    "semantic_title": "masking kernel for learning energy-efficient representations for speaker recognition and mobile health",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiang23_interspeech.html": {
    "title": "eSTImate: A Real-time Speech Transmission Index Estimator With Speech Enhancement Auxiliary Task Using Self-Attention Feature Pyramid Network",
    "volume": "main",
    "abstract": "The Speech Transmission Index (STI) is a crucial metric for evaluating speech intelligibility, but its standard measurement method is too complicated for real-time applications. Though recently proposed deep learning based STI estimation schemes can effectively address the problem, existing methods still fall short of covering all possible STI scenarios. This paper presents eSTImate: an end-to-end deep learning system for real-time STI blind estimation that integrates the tasks of STI estimation and speech enhancement through a feature pyramid auxiliary learning architecture and incorporates multi-head attention mechanisms. The proposed model demonstrates the performance of state-of-the-art, achieving a low mean absolute error of 0.016 and root mean square error of 0.021 on the constructed dataset that covers the whole range of STI, highlighting its potential to provide accurate and consistent real-time STI estimation across diverse real-world scenarios",
    "checked": true,
    "id": "12a5d188e284a0a3671ab48ace8fbee63caa3d0f",
    "semantic_title": "estimate: a real-time speech transmission index estimator with speech enhancement auxiliary task using self-attention feature pyramid network",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23n_interspeech.html": {
    "title": "Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement",
    "volume": "main",
    "abstract": "Current speech enhancement (SE) research has largely neglected channel attention and spatial attention, and encoder-decoder architecture-based networks have not adequately considered how to provide efficient inputs to the intermediate enhancement layer. To address these issues, this paper proposes a time-frequency (T-F) domain SE network (DPCFCS-Net) that incorporates improved densely connected blocks, dual-path modules, convolution-augmented transformers (conformers), channel attention, and spatial attention. Compared with previous models, our proposed model has a more efficient encoder-decoder and can learn comprehensive features. Experimental results on the VCTK+DEMAND dataset demonstrate that our method outperforms existing techniques in SE performance. Furthermore, the improved densely connected block and two dimensions attention module developed in this work are highly adaptable and easily integrated into existing networks",
    "checked": true,
    "id": "e20c3e13cff70666a17fa8c246124adeb6218cfa",
    "semantic_title": "efficient encoder-decoder and dual-path conformer for comprehensive feature learning in speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23b_interspeech.html": {
    "title": "Privacy-preserving Representation Learning for Speech Understanding",
    "volume": "main",
    "abstract": "Existing privacy-preserving speech representation learning methods target a single application domain. In this paper, we present a novel framework to anonymize utterance-level speech embeddings generated by pre-trained encoders and show its effectiveness for a range of speech classification tasks. Specifically, given the representations from a pre-trained encoder, we train a Transformer to estimate the representations for the same utterances spoken by other speakers. During inference, the extracted representations can be converted into different identities to preserve privacy. We compare the results with the voice anonymization baselines from the VoicePrivacy 2022 challenge. We evaluate our framework on speaker identification for privacy and emotion recognition, depression classification, and intent classification for utility. Our method outperforms the baselines on privacy and utility in paralinguistic tasks and achieves comparable performance for intent classification",
    "checked": true,
    "id": "ea281a8cc0e071e33592627c31dce77ed88e46e7",
    "semantic_title": "privacy-preserving representation learning for speech understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panariello23_interspeech.html": {
    "title": "Vocoder drift in x-vector–based speaker anonymization",
    "volume": "main",
    "abstract": "State-of-the-art approaches to speaker anonymization typically employ some form of perturbation function to conceal speaker information contained within an x-vector embedding, then resynthesize utterances in the voice of a new pseudo-speaker using a vocoder. Strategies to improve the x-vector anonymization function have attracted considerable research effort, whereas vocoder impacts are generally neglected. In this paper, we show that the impact of the vocoder is substantial and sometimes dominant. The vocoder drift, namely the difference between the x-vector vocoder input and that which can be extracted subsequently from the output, is learnable and can hence be reversed by an attacker; anonymization can be undone and the level of privacy protection provided by such approaches might be weaker than previously thought. The findings call into question the focus upon x-vector anonymization, prompting the need for greater attention to vocoder impacts and stronger attack models alike",
    "checked": true,
    "id": "ad5dcdd727278dbaa5b12f893554770f640ecb61",
    "semantic_title": "vocoder drift in x-vector–based speaker anonymization",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panariello23b_interspeech.html": {
    "title": "Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems",
    "volume": "main",
    "abstract": "We present Malafide, a universal adversarial attack against automatic speaker verification (ASV) spoofing countermeasures (CMs). By introducing convolutional noise using an optimised linear time-invariant filter, Malafide attacks can be used to compromise CM reliability while preserving other speech attributes such as quality and the speaker's voice. In contrast to other adversarial attacks proposed recently, Malafide filters are optimised independently of the input utterance and duration, are tuned instead to the underlying spoofing attack, and require the optimisation of only a small number of filter coefficients. Even so, they degrade CM performance estimates by an order of magnitude, even in black-box settings, and can also be configured to overcome integrated CM and ASV subsystems. Integrated solutions that use self-supervised learning CMs, however, are more robust, under both black-box and white-box settings",
    "checked": true,
    "id": "247c9fcb129c21d4441d19c44aef97a40cdbf7e5",
    "semantic_title": "malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zaiem23b_interspeech.html": {
    "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models",
    "checked": true,
    "id": "08a62562feed7b3709f939092021fcc5b7794d07",
    "semantic_title": "speech self-supervised representation benchmarking: are we doing it right?",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23d_interspeech.html": {
    "title": "An extension of disentanglement metrics and its application to voice",
    "volume": "main",
    "abstract": "In representation learning, the promise of disentanglement methods is to decompose an input signal into a set of independent and interpretable attributes. Some metrics, such as the DCI or MIG scores, have been proposed to evaluate how much this goal is reached. They analyse the relationship between the representation components and the desirable attributes. This paper shows that, even when applied to synthetic datasets generated from a closed list of generative factors, these metrics can be too optimistic. In particular, it reports that a generative factor can be recovered from an altered disentangled representation from which it has been supposedly removed, according to the metrics. Based on this observation, a new criterion called latent decimation is proposed to evaluate disentanglement through the accuracy of factors prediction from subsets of latents. A new metric called MIDCI is defined, and its relevance is demonstrated on voice data",
    "checked": true,
    "id": "7420178b45d43b8486465c4fbaa527262cd8cb2b",
    "semantic_title": "an extension of disentanglement metrics and its application to voice",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/abdullah23_interspeech.html": {
    "title": "An Information-Theoretic Analysis of Self-supervised Discrete Representations of Speech",
    "volume": "main",
    "abstract": "Self-supervised representation learning for speech often involves a quantization step that transforms the acoustic input into discrete units. However, it remains unclear how to characterize the relationship between these discrete units and abstract phonetic categories such as phonemes. In this paper, we develop an information-theoretic framework whereby we represent each phonetic category as a distribution over discrete units. We then apply our framework to two different self-supervised models (namely, wav2vec 2.0 and XLSR) and use American English speech as a case study. Our study demonstrates that the entropy of phonetic distributions reflects the variability of the underlying speech sounds, with phonetically similar sounds exhibiting similar distributions. While our study confirms the lack of direct one-to-one correspondence, we find an intriguing indirect relationship between phonetic categories and discrete units",
    "checked": true,
    "id": "2c64730917a56fa78ce37abde56a5c78685c0de8",
    "semantic_title": "an information-theoretic analysis of self-supervised discrete representations of speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ashihara23_interspeech.html": {
    "title": "SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) for speech representation has been successfully applied in various downstream tasks, such as speech and speaker recognition. More recently, speech SSL models have also been shown to be beneficial in advancing spoken language understanding tasks, implying that the SSL models have the potential to learn not only acoustic but also linguistic information. In this paper, we aim to clarify if speech SSL techniques can well capture linguistic knowledge. For this purpose, we introduce SpeechGLUE, a speech version of the General Language Understanding Evaluation (GLUE) benchmark. Since GLUE comprises a variety of natural language understanding tasks, SpeechGLUE can elucidate the degree of linguistic ability of speech SSL models. Experiments demonstrate that speech SSL models, although inferior to text-based SSL models, perform better than baselines, suggesting that they can acquire a certain amount of general linguistic knowledge from just unlabeled speech data",
    "checked": true,
    "id": "21fe2b5fb20f3327baa602fe4171c66284dd1e16",
    "semantic_title": "speechglue: how well can self-supervised speech models capture linguistic knowledge?",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sasou23_interspeech.html": {
    "title": "Comparison of GIF- and SSL-based Features in Pathological-voice Detection",
    "volume": "main",
    "abstract": "A system that automatically detects voice pathology from acoustic signals enables non-invasive, low cost, and objective assessment of speech disorders. Therefore, it is expected to accelerate and improve the diagnosis and clinical treatment of patients. Pathological voices are symptoms of impairments in the articulation of speech sound, fluency, and/or voice. We consider that direct extraction of features from the glottal flow estimated by glottal inverse filtering (GIF) is a promising approach to pathological-voice detection. To precisely estimate the glottal flow, we propose a novel GIF method that combines constrained autoregressive hidden Markov model (CAR–HMM) analysis with automatic topology generation of the excitation HMM. To evaluate the effectiveness of the features extracted from the estimated glottal flow during pathological-voice detection, we employ the Saarbrücken Voice Database. We also compare the features obtained by the proposed CAR–HMM with those obtained by pre-trained models based on self-supervised learning (SSL). The experimental results confirmed that the CAR–HMM-based method can outperform the SSL-based methods",
    "checked": true,
    "id": "c3b1e1518f4de6fad99f76d52fb21560c8d898f9",
    "semantic_title": "comparison of gif- and ssl-based features in pathological-voice detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23c_interspeech.html": {
    "title": "What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions",
    "volume": "main",
    "abstract": "There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of speech processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end. In this paper, we investigate this question on keyword spotting, speech-based emotion recognition and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt. Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions. This in turn enables a system trained on clean speech to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper",
    "checked": true,
    "id": "f278036c2546252222c2e8df454b2cd45789a9e6",
    "semantic_title": "what is learnt by the learnable front-end (leaf)? adapting per-channel energy normalisation (pcen) to noisy conditions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/masumura23_interspeech.html": {
    "title": "End-to-End Joint Target and Non-Target Speakers ASR",
    "volume": "main",
    "abstract": "This paper proposes a novel automatic speech recognition (ASR) system that can transcribe individual speaker's speech while identifying whether they are target or non-target speakers from multi-talker overlapped speech. Target-speaker ASR systems are a promising way to only transcribe a target speaker's speech by enrolling the target speaker's information. However, in conversational ASR applications, transcribing both the target speaker's speech and non-target speakers' ones is often required to understand interactive information. To naturally consider both target and non-target speakers in a single ASR model, our idea is to extend autoregressive modeling-based multi-talker ASR systems to utilize the enrollment speech of the target speaker. Our proposed ASR is performed by recursively generating both textual tokens and tokens that represent target or non-target speakers. Our experiments demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "10b1fa066de6e14c42ed04b7b770575bb462b79e",
    "semantic_title": "end-to-end joint target and non-target speakers asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23c_interspeech.html": {
    "title": "Improving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end (E2E) systems have shown comparable performance to hybrid systems for automatic speech recognition (ASR). Word timings, as a by-product of ASR, are essential in many applications, especially for subtitling and computer-aided pronunciation training. In this paper, we improve the frame-level classifier for word timings in E2E system by introducing label priors in connectionist temporal classification (CTC) loss, which is adopted from prior works, and combining low-level Mel-scale filter banks with high-level ASR encoder output as input feature. On the internal Chinese corpus, the proposed method achieves 95.68%/94.18% compared to the hybrid system 93.0%/90.22% on the word timing accuracy metrics. It also surpass a previous E2E approach with an absolute increase of 4.80%/8.02% on the metrics on 7 languages. In addition, we further improve word timing accuracy by delaying CTC peaks with frame-wise knowledge distillation, though only experimenting on LibriSpeech",
    "checked": true,
    "id": "88567faceac81933a545532f41448e31a0c33b0c",
    "semantic_title": "improving frame-level classifier for word timings with non-peaky ctc in end-to-end automatic speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/makishima23_interspeech.html": {
    "title": "Joint Autoregressive Modeling of End-to-End Multi-Talker Overlapped Speech Recognition and Utterance-level Timestamp Prediction",
    "volume": "main",
    "abstract": "This paper proposes autoregressive modeling of the joint multi-talker automatic speech recognition (ASR) and timestamp prediction. Autoregressive modeling of multi-talker ASR is a simple and promising approach. However, it does not predict utterance timestamp information despite its being important in practice. To address this problem, our key idea is to extend autoregressive-modeling-based multi-talker ASR to predict quantized timestamp tokens representing the start and end time of an utterance. Our method estimates transcription and utterance-level timestamp tokens of multiple speakers one after another. This enables joint modeling of multi-talker ASR and timestamps prediction without changing the simple autoregressive modeling of the conventional multi-talker ASR. Experimental results show that our method outperforms the ASR performance of conventional autoregressive multi-talker ASR without timestamp prediction and achieves promising timestamp prediction accuracy",
    "checked": true,
    "id": "693dc3da48b2babd66ff85d43f546e04710b3055",
    "semantic_title": "joint autoregressive modeling of end-to-end multi-talker overlapped speech recognition and utterance-level timestamp prediction",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23_interspeech.html": {
    "title": "Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems degrade significantly under noisy conditions. Recently, speech enhancement (SE) is introduced as front-end to reduce noise for ASR, but it also suppresses some important speech information, i.e., over-suppression. To alleviate this, we propose a dual-path style learning approach for end-to-end noise-robust speech recognition (DPSL-ASR). Specifically, we first introduce clean speech feature along with the fused feature from IFF-Net as dual-path inputs to recover the suppressed information. Then, we propose style learning to map the fused feature close to clean feature, in order to learn latent speech information from the latter, i.e., clean \"speech style\". Furthermore, we also minimize the distance of final ASR outputs in two paths to improve noise-robustness. Experiments show that the proposed approach achieves relative word error rate (WER) reductions of 10.6% and 8.6% over the best IFF-Net baseline, on RATS and CHiME-4 datasets respectively",
    "checked": true,
    "id": "14098f596e2fec819bed7490a7436fe3584966c9",
    "semantic_title": "dual-path style learning for end-to-end noise-robust speech recognition",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23_interspeech.html": {
    "title": "Multi-pass Training and Cross-information Fusion for Low-resource End-to-end Accented Speech Recognition",
    "volume": "main",
    "abstract": "Low-resource accented speech recognition is one of the important challenges faced by current ASR technology in practical applications. In this study, we propose a Conformer-based architecture, called Aformer, to leverage both the acoustic information from large non-accented and limited accented training data. Specifically, a general encoder and an accent encoder are designed in the Aformer to extract complementary acoustic information. Moreover, we propose to train the Aformer in a multi-pass manner, and investigate three cross-information fusion methods to effectively combine the information from both general and accent encoders. All experiments are conducted on both the accented English and Mandarin ASR tasks. Results show that our proposed methods outperform the strong Conformer baseline by relative 10.2% to 24.5% word/character error rate reduction on six in-domain and out-of-domain accented test sets",
    "checked": true,
    "id": "e40636e20e17f80cc9ea4a917710819b045bc20e",
    "semantic_title": "multi-pass training and cross-information fusion for low-resource end-to-end accented speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bataev23_interspeech.html": {
    "title": "Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator",
    "volume": "main",
    "abstract": "We propose an end-to-end Automatic Speech Recognition (ASR) system that can be trained on transcribed speech data, text-only data, or a mixture of both. The proposed model uses an integrated auxiliary block for text-based training. This block combines a non-autoregressive multi-speaker text-to-mel-spectrogram generator with a GAN-based enhancer to improve the spectrogram quality. The proposed system can generate a mel-spectrogram dynamically during training. It can be used to adapt the ASR model to a new domain by using text-only data from this domain. We demonstrate that the proposed training method significantly improves ASR accuracy compared to the system trained on transcribed speech only. It also surpasses cascade TTS systems with the vocoder in the adaptation quality and training speed",
    "checked": true,
    "id": "be1f6e3160d44bee221e96f4fee6f02fa0adaf0c",
    "semantic_title": "text-only domain adaptation for end-to-end asr using integrated text-to-mel-spectrogram generator",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23_interspeech.html": {
    "title": "Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling",
    "volume": "main",
    "abstract": "We study speech intent classification and slot filling (SICSF) by proposing to use an encoder pretrained on speech recognition (ASR) to initialize an end-to-end (E2E) Conformer-Transformer model, which achieves the new state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and 82.27% SLURP-F1. We compare our model with encoders pre-trained on self-supervised learning (SSL), and show that ASR pretraining is much more effective than SSL for SICSF. To explore parameter efficiency, we freeze the encoder and add Adapter modules, and show that parameter efficiency is only achievable with an ASR-pretrained encoder, while the SSL encoder needs full finetuning to achieve comparable results. In addition, we provide an in-depth comparison on end-to-end models versus cascading models (ASR+NLU), and show that E2E models are better than cascaded models unless an oracle ASR model is provided. Last but not least, our model is the first E2E model that achieves the same performance as cascading models with oracle ASR. Code, checkpoints and configs are available",
    "checked": true,
    "id": "9e8fa730a4bdb65b140afd59881a6655780b7df6",
    "semantic_title": "leveraging pretrained asr encoders for effective and efficient end-to-end speech intent classification and slot filling",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23b_interspeech.html": {
    "title": "Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models",
    "volume": "main",
    "abstract": "Although pre-trained language models show good performance on various natural language processing tasks, they often rely on non-causal features and patterns to determine the outcome. For natural language inference tasks, previous results have shown that even a model trained on a large number of data fails to perform well on counterfactually revised data, indicating that the model is not robustly learning the semantics of the classes. In this paper, we propose a method in which we use token-based and sentence-based augmentation methods to generate counterfactual sentence pairs that belong to each class, and apply contrastive learning to help the model learn the difference between sentence pairs of different classes with similar contexts. Evaluation results with counterfactually-revised dataset and general NLI datasets show that the proposed method can improve the performance and robustness of the NLI model",
    "checked": true,
    "id": "fb8d0982d76945e136836a57e7a23907c21c2fb2",
    "semantic_title": "relation-based counterfactual data augmentation and contrastive learning for robustifying natural language inference models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matsuura23_interspeech.html": {
    "title": "Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization",
    "volume": "main",
    "abstract": "End-to-end speech summarization (E2E SSum) directly summarizes input speech into easy-to-read short sentences with a single model. This approach is promising because it, in contrast to the conventional cascade approach, can utilize full acoustical information and mitigate to the propagation of transcription errors. However, due to the high cost of collecting speech-summary pairs, an E2E SSum model tends to suffer from training data scarcity and output unnatural sentences. To overcome this drawback, we propose for the first time to integrate a pre-trained language model (LM), which is highly capable of generating natural sentences, into the E2E SSum decoder via transfer learning. In addition, to reduce the gap between the independently pre-trained encoder and decoder, we also propose to transfer the baseline E2E SSum encoder instead of the commonly used automatic speech recognition encoder. Experimental results show that the proposed model outperforms baseline and data augmented models",
    "checked": true,
    "id": "3b1fce8bbd9c326d00e912bef3def5a6455daa3c",
    "semantic_title": "transfer learning from pre-trained language models improves end-to-end speech summarization",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deshmukh23_interspeech.html": {
    "title": "Audio Retrieval with WavText5K and CLAP Training",
    "volume": "main",
    "abstract": "Text-based audio retrieval takes a natural language query to retrieve relevant audio files in a database. Most retrieval models are trained, optimized, and evaluated on a single dataset. In this paper, we quantify the effect of adding training data using three datasets and the effect on performance by evaluating the same model on two datasets. For our study, first, we introduce a new collection of about 5000 audio-text pairs called WavText5K. We qualitatively show how WavText5K differs from audio-text datasets and quantitatively show its effectiveness for retrieval. Our results show that adding more audio-text pairs does not necessarily improve performance. Second, we compare two effective audio encoders: CNN and audio transformers. We propose an architecture that demonstrates that utilizing both encoders improves the individual model's performance. Overall, using WavText5K and the proposed encoder combination outperforms the benchmark for AudioCaps and Clotho by 6% and 23%",
    "checked": true,
    "id": "c822486b8f1dcbef3b96b136c85d48d0dc580f31",
    "semantic_title": "audio retrieval with wavtext5k and clap training",
    "citation_count": 23,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cappellazzo23b_interspeech.html": {
    "title": "Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "The ability to learn new concepts sequentially is a major weakness for modern neural networks, which hinders their use in non-stationary environments. Their propensity to fit the current data distribution to the detriment of the past acquired knowledge leads to the catastrophic forgetting issue. In this work we tackle the problem of Spoken Language Understanding applied to a continual learning setting. We first define a class-incremental scenario for the SLURP dataset. Then, we propose three knowledge distillation (KD) approaches to mitigate forgetting for a sequence-to-sequence transformer model: the first KD method is applied to the encoder output (audio-KD), and the other two work on the decoder output, either directly on the token-level (tok-KD) or on the sequence-level (seq-KD) distributions. We show that the seq-KD substantially improves all the performance metrics, and its combination with the audio-KD further decreases the average WER and enhances the entity prediction metric",
    "checked": true,
    "id": "100926ca1253a5e7673cca246405db089a75b798",
    "semantic_title": "sequence-level knowledge distillation for class-incremental end-to-end spoken language understanding",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chien23b_interspeech.html": {
    "title": "Contrastive Disentangled Learning for Memory-Augmented Transformer",
    "volume": "main",
    "abstract": "This paper developed a new memory-augmented sequential learning based on a contrastive disentangled transformer. Conventionally, transformer is insufficient to characterize long sequences since the sequence length is restricted to avoid the requirement of overlarge memory. A direct solution to handle this issue is to divide long sequence into short segments, but the context fragmentation will happen. In this paper, the contrastive disentangled memory is exploited to deal with the increasing computation cost as well as the overlarge memory requirement due to long sequences. In particular, an informative selection over the disentangled memory slots is proposed for iterative updating in a large-span sequence representation. This paper maximizes the semantic diversity of memory slots and captures the contextual semantics via contrastive learning. The experiments on language understanding show that the context fragmentation is mitigated by the proposed method with reduced computation",
    "checked": true,
    "id": "abfa407b9c4702f4caf7ec49ea98c4b844ee6772",
    "semantic_title": "contrastive disentangled learning for memory-augmented transformer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deseyssel23_interspeech.html": {
    "title": "ProsAudit, a prosodic benchmark for self-supervised speech models",
    "volume": "main",
    "abstract": "We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, and an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when evaluated on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks",
    "checked": true,
    "id": "75ef5877acbd3fff7bce0af51dadfa42f6e4c9c0",
    "semantic_title": "prosaudit, a prosodic benchmark for self-supervised speech models",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23j_interspeech.html": {
    "title": "Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces",
    "volume": "main",
    "abstract": "Self-supervised speech representations are known to encode both speaker and phonetic information, but how they are distributed in the high-dimensional space remains largely unexplored. We hypothesize that they are encoded in orthogonal subspaces, a property that lends itself to simple disentanglement. Applying principal component analysis to representations of two predictive coding models, we identify two subspaces that capture speaker and phonetic variances, and confirm that they are nearly orthogonal. Based on this property, we propose a new speaker normalization method which collapses the subspace that encodes speaker information, without requiring transcriptions. Probing experiments show that our method effectively eliminates speaker information and outperforms a previous baseline in phone discrimination tasks. Moreover, the approach generalizes and can be used to remove information of unseen speakers",
    "checked": true,
    "id": "5376990833dd410f30faf48a673da17263b57067",
    "semantic_title": "self-supervised predictive coding models encode speaker and phonetic information in orthogonal subspaces",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hallap23_interspeech.html": {
    "title": "Evaluating context-invariance in unsupervised speech representations",
    "volume": "main",
    "abstract": "Unsupervised speech representations have taken off with benchmarks demonstrating major progress on semi-supervised speech recognition, speech synthesis, and speech-only language modelling. Inspiration comes from the promise of discovering the phonemes of a language or a similar low-bitrate encoding. However, one of the critical properties of phoneme transcriptions is context-invariance: the phonetic context of a speech sound can have massive influence on the way it is pronounced while text remains stable. This is why tokens of the same word have the same transcriptions---key to language understanding. Current benchmarks do not measure context-stability. We develop a new version of the ZeroSpeech ABX benchmark that does, and apply it to recent self-supervised representations. We show that context-independence of representations is predictive of the stability of word-level representations. We suggest research concentrate on improving context-independence of unsupervised representations",
    "checked": true,
    "id": "629d7e7b9fc7b6d6918d384038b1d4dd8c74aec4",
    "semantic_title": "evaluating context-invariance in unsupervised speech representations",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23_interspeech.html": {
    "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
    "volume": "main",
    "abstract": "Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT",
    "checked": true,
    "id": "9efd5e9d81ca498cc14a13b8c9681501d28d3c13",
    "semantic_title": "cobert: self-supervised speech representation learning through code representation learning",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23_interspeech.html": {
    "title": "Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering",
    "volume": "main",
    "abstract": "Self-supervised speech representation models have succeeded in various tasks, but improving them for content-related problems using unlabeled data is challenging. We propose speaker-invariant clustering (Spin), a novel self-supervised learning method that clusters speech representations and performs swapped prediction between the original and speaker-perturbed utterances. Spin disentangles speaker information and preserves content representations with just 45 minutes of fine-tuning on a single GPU. Spin improves pre-trained networks and outperforms prior methods in speech recognition and acoustic unit discovery",
    "checked": true,
    "id": "f78b17020c8949e9c91e8e3239dcde2cb1bf91d3",
    "semantic_title": "self-supervised fine-tuning for improved content representations by speaker-invariant clustering",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23d_interspeech.html": {
    "title": "Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder",
    "volume": "main",
    "abstract": "Acoustic word embeddings (AWEs) aims to map a variable-length speech segment into a fixed-dimensional representation. High-quality AWEs should be invariant to variations, such as duration, pitch and speaker. In this paper, we introduce a novel self-supervised method to learn robust AWEs from a large-scale unlabelled speech corpus. Our model, named Correspondence Transformer Encoder (CTE), employs a teacher-student learning framework. We train the model based on the idea that different realisations of the same word should be close in the underlying embedding space. Specifically, we feed the teacher and student encoder with different acoustic instances of the same word and pre-train the model with a word-level loss. Our experiments show that the embeddings extracted from the proposed CTE model are robust to speech variations, e.g. speakers and domains. Additionally, when evaluated on Xitsonga, a low-resource cross-lingual setting, the CTE model achieves new state-of-the-art performance",
    "checked": true,
    "id": "7715d31722988c5f9fca16997d2ab38ae100a3c7",
    "semantic_title": "self-supervised acoustic word embedding learning via correspondence transformer encoder",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/paterson23_interspeech.html": {
    "title": "A Pipeline to Evaluate the Effects of Noise on Machine Learning Detection of Laryngeal Cancer",
    "volume": "main",
    "abstract": "Cases of laryngeal cancer are rising, with diagnosis often involving invasive biopsy procedures. An alternate approach is to identify high-risk patients by analysis of voice recordings which can alert clinical teams to those patients that need prioritisation. We propose a pipeline for evaluating speech classifier performance in the presence of noise. We perform experiments using the pipeline with several classifiers and denoising techniques. Random forest classifier performed best with an accuracy of 81.2% on clean data dropping to 63.8% when noise was added to recordings. The accuracy of all classifiers was reduced by added noise, signal denoising improved classifier accuracy but could not fully reverse the effects of noise. The effects of noise on classification is a complex issue which must be resolved for these detection systems to be implemented in clinical practice. We show that the proposed pipeline allows for the evaluation of classifier performance in the presence of noise",
    "checked": true,
    "id": "56a59dfab72fdb5205442a070aaa3e217f4d99c4",
    "semantic_title": "a pipeline to evaluate the effects of noise on machine learning detection of laryngeal cancer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ga_interspeech.html": {
    "title": "ReCLR: Reference-Enhanced Contrastive Learning of Audio Representation for Depression Detection",
    "volume": "main",
    "abstract": "Contrastive self-supervised learning has seen great success in computer vision while been less investigated in the audio processing field, in particular depression detection, a socially critical challenge. Detecting depression from one's speech has been examined via various audio representations, including acoustic feature combinations and model-based ones. This paper proposes to obtain depressive audio representations by departing speech via reference features from an emotion recognition model. Furthermore, we propose a reference-enhanced contrastive learning (ReCLR) to select fine-grained positive instances and allocate weight to negative instances. The depression detection results indicate that contrastive learning is effective in such an audio task. Moreover, our modified ReCLR strategy has outperformed contrastive training without references",
    "checked": true,
    "id": "9e0688d5553edadb9b47ce8aec11a5e2ab70cd98",
    "semantic_title": "reclr: reference-enhanced contrastive learning of audio representation for depression detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/egaslopez23_interspeech.html": {
    "title": "Automated Multiple Sclerosis Screening Based on Encoded Speech Representations",
    "volume": "main",
    "abstract": "Multiple Sclerosis (MS) is a chronic disease affecting over 2.5 million people worldwide. Its early detection is crucial for the management and treatment of the disease. Here we present an approach for automatic MS screening based on encoded speech representations. Our methods rely on Wav2Vec2 models to extract relevant traits from speech recordings of patients, which are then fed into a Support Vector Machine. Besides employing Wav2Vec2 models pre-trained on large public corpora, we also fine-tune them on 85 hours of the target language (Hungarian) in two distinct ways: for ASR and for speaker identification. Both variations outperformed the original models and conventional methods (ComParE functionals, x-vectors, and ECAPA-TDNN). Our findings suggest that fine-tuning for the actual speaker provides more advantages than the typical approach of fine-tuning for ASR purposes. Still, we improved our best MS discrimination performance when we fused features from our two fine-tuned models",
    "checked": true,
    "id": "0ecc9271879f70bb0440c2d88e775fd16eb0fc1d",
    "semantic_title": "automated multiple sclerosis screening based on encoded speech representations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/melistas23_interspeech.html": {
    "title": "Cross-Lingual Features for Alzheimer's Dementia Detection from Speech",
    "volume": "main",
    "abstract": "Alzheimer's dementia is a neurodegenerative disease that affects millions of people worldwide. Early detection of Alzheimer's dementia is crucial for effective treatment and management of the disease. In this paper, we present a cross-lingual approach for detecting Alzheimer's dementia from speech, based on multiple feature streams that capture the individual's speech and conversational interactions. In order to validate the ability of the features to perform well in cross-linguistic scenarios, we evaluate in a zero-shot setup, where the target domain is a language that was not available during training and a few-shot setup, where only limited data is available. Experimental results show that an ensemble system using the features trained on English and evaluated on Greek outperforms the baseline system by 4.4 %. Further experiments show promising zero-shot and few-shot performance on a similar Spanish task",
    "checked": true,
    "id": "96f8abfad2dc63136b4408ddb761525197ace431",
    "semantic_title": "cross-lingual features for alzheimer's dementia detection from speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zusag23_interspeech.html": {
    "title": "Careful Whisper - leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification",
    "volume": "main",
    "abstract": "This paper presents a fully automated approach for identifying speech anomalies from voice recordings to aid in the as- sessment of speech impairments. By combining Connectionist Temporal Classification (CTC) and encoder-decoder-based automatic speech recognition models, we generate rich acoustic and clean transcripts. We then apply several natural language processing methods to extract features from these transcripts to produce prototypes of healthy speech. Basic distance measures from these prototypes serve as input features for standard machine learning classifiers, yielding human-level accuracy for the distinction between recordings of people with aphasia and a healthy control group. Furthermore, the most frequently occurring aphasia types can be distinguished with 90% accuracy. The pipeline is directly applicable to other diseases and languages, showing promise for robustly extracting diagnostic speech biomarkers",
    "checked": true,
    "id": "7fbe44f2d9c48794be12bab8832d5a789c34287b",
    "semantic_title": "careful whisper - leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thienpondt23_interspeech.html": {
    "title": "Behavioral Analysis of Pathological Speaker Embeddings of Patients During Oncological Treatment of Oral Cancer",
    "volume": "main",
    "abstract": "In this paper, we analyze the behavior of speaker embeddings of patients during oral cancer treatment. First, we found that pre- and post-treatment speaker embeddings differ significantly, notifying a substantial change in voice characteristics. However, a partial recovery to pre-operative voice traits is observed after 12 months post-operation. Secondly, the same-speaker similarity at distinct treatment stages is similar to healthy speakers, indicating that the embeddings can capture characterizing features of even severely impaired speech. Finally, a speaker verification analysis signifies a stable false positive rate and variable false negative rate when combining speech samples of different treatment stages. This indicates robustness of the embeddings towards other speakers, while still capturing the changing voice characteristics during treatment. To the best of our knowledge, this is the first analysis of speaker embeddings during oral cancer treatment of patients",
    "checked": true,
    "id": "9ab59ced6320180a236a472a61f69fdcfe5e592e",
    "semantic_title": "behavioral analysis of pathological speaker embeddings of patients during oncological treatment of oral cancer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23b_interspeech.html": {
    "title": "Adversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech",
    "volume": "main",
    "abstract": "To simplify the generation process, several text-to-speech (TTS) systems implicitly learn intermediate latent representations instead of relying on predefined features (e.g., mel-spectrogram). However, their generation quality is unsatisfactory as these representations lack speech variances. In this paper, we improve TTS performance by adding prosody embeddings to the latent representations. During training, we extract reference prosody embeddings from mel-spectrograms, and during inference, we estimate these embeddings from text using generative adversarial networks (GANs). Using GANs, we reliably estimate the prosody embeddings in a fast way, which have complex distributions due to the dynamic nature of speech. We also show that the prosody embeddings work as efficient features for learning a robust alignment between text and acoustic features. Our proposed model surpasses several publicly available models with less parameters and computational complexity in comparative experiments",
    "checked": false,
    "id": "ed9e97ff828513d670ca117a61e4453a16cfc7e3",
    "semantic_title": "ailtts: adversarial learning of intermediate acoustic feature for end-to-end lightweight text-to-speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hsieh23_interspeech.html": {
    "title": "Adapter-Based Extension of Multi-Speaker Text-To-Speech Model for New Speakers",
    "volume": "main",
    "abstract": "Fine-tuning is a popular method for adapting text-to-speech (TTS) models to new speakers. However, this approach has some challenges. Usually, fine-tuning requires several hours of high quality speech per speaker. Fine-tuning might negatively affect the quality of speech synthesis for previously learned speakers. In this paper, we propose an alternative approach for TTS adaptation based on using parameter-efficient adapter modules. In the proposed approach, a few adapter modules are added between the layers of the pretrained network. The pretrained model is frozen, and only the adapters are fine-tuned to the speech of a new speaker. Our approach will produce a new model with a high level of parameter sharing with the original model. Our experiments on LibriTTS, HiFi-TTS and VCTK datasets validate our adapter-based method through objective and subjective metrics. The code is open-sourced and the audio samples are available on our demo page",
    "checked": true,
    "id": "7d42d1a27129c51e618e5127132c32f35260b4c4",
    "semantic_title": "adapter-based extension of multi-speaker text-to-speech model for new speakers",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sivaguru23_interspeech.html": {
    "title": "SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "While FastSpeech2 aims to integrate aspects of speech such as pitch, energy, and duration as conditional inputs, it still leaves scope for richer representations. As a part of this work, we leverage representations from various Self-Supervised Learning (SSL) models to enhance the quality of the synthesized speech. In particular, we pass the FastSpeech2 encoder's length-regulated outputs through a series of encoder layers with the objective of reconstructing the SSL representations. In the SALTTS-parallel implementation, the representations from this second encoder are used for an auxiliary reconstruction loss with the SSL features. The SALTTS-cascade implementation, however, passes these representations through the decoder in addition to having the reconstruction loss. The richness of speech characteristics from the SSL features reflects in the output speech quality, with the objective and subjective evaluation measures of the proposed approach outperforming the baseline FastSpeech2",
    "checked": true,
    "id": "bc20f9517fe8e8895f1cd9b0624290c76d4e91d4",
    "semantic_title": "saltts: leveraging self-supervised speech representations for improved text-to-speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23k_interspeech.html": {
    "title": "UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data",
    "volume": "main",
    "abstract": "We propose UnitSpeech, a speaker-adaptive speech synthesis method that fine-tunes a diffusion-based text-to-speech (TTS) model using minimal untranscribed data. To achieve this, we use the self-supervised unit representation as a pseudo transcript and integrate the unit encoder into the pre-trained TTS model. We train the unit encoder to provide speech content to the diffusion-based decoder and then fine-tune the decoder for speaker adaptation to the reference speaker using a single",
    "checked": true,
    "id": "0faf1ee64fe60140e360fea4e85a6c0e715e94e1",
    "semantic_title": "unitspeech: speaker-adaptive speech synthesis with untranscribed data",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dang23b_interspeech.html": {
    "title": "LightVoc: An Upsampling-Free GAN Vocoder Based On Conformer And Inverse Short-time Fourier Transform",
    "volume": "main",
    "abstract": "Most neural vocoders based on generative adversarial networks (GANs) rely on iterative upsampling to generate audio sequences from mel-spectrograms as well as dilated convolution to expand their receptive fields. Nevertheless, iterative upsampling increases the network's complexity and thus decreases the inference speed. Moreover, convolution neural networks are geared towards extracting fine-grained local information and still struggle to capture long-term dependencies. In this work, we propose LightVoc, an efficient and high-quality GAN-based neural vocoder that replaces all upsampling blocks with a stack of Conformer blocks and uses a novel combination of discriminators to generate high-resolution waveforms over the full-band. From our experiments on LJSpeech dataset, LightVoc produces comparable audio quality while being 52.5 times faster in terms of CPU-based inference speed in comparison to HiFi-GAN V1",
    "checked": true,
    "id": "5856df08c211d595d0b0e3b8d850aa4bf0fc3b10",
    "semantic_title": "lightvoc: an upsampling-free gan vocoder based on conformer and inverse short-time fourier transform",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saito23_interspeech.html": {
    "title": "ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings",
    "volume": "main",
    "abstract": "We propose ChatGPT-EDSS, an empathetic dialogue speech synthesis (EDSS) method using ChatGPT for extracting dialogue context. ChatGPT is a chatbot that can deeply understand the content and purpose of an input prompt and appropriately respond to the user's request. We focus on ChatGPT's reading comprehension and introduce it to EDSS, a task of synthesizing speech that can empathize with the interlocutor's emotion. Our method first gives chat history to ChatGPT and asks it to generate three words representing the intention, emotion, and speaking style for each line in the chat. Then, it trains an EDSS model using the embeddings of ChatGPT-derived context words as the conditioning features. The experimental results demonstrate that our method performs comparably to ones using emotion labels or neural network-derived context embeddings learned from chat histories. The collected ChatGPT-derived context information is available at our project page",
    "checked": true,
    "id": "00554edbbe20423cbf7a2f7e3a130c1cb4f56203",
    "semantic_title": "chatgpt-edss: empathetic dialogue speech synthesis trained from chatgpt-derived context word embeddings",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23f_interspeech.html": {
    "title": "Human Transcription Quality Improvement",
    "volume": "main",
    "abstract": "High quality transcription data is crucial for training automatic speech recognition (ASR) systems. However, the existing industry-level data collection pipelines are expensive to researchers, while the quality of crowdsourced transcription is low. In this paper, we propose a reliable method to collect speech transcriptions. We introduce two mechanisms to improve transcription quality: confidence estimation based reprocessing at labeling stage, and automatic word error correction at post-labeling stage. We collect and release LibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100 hours of English speech. Experiment shows the Transcription WER is reduced by over 50%. We further investigate the impact of transcription error on ASR model performance and found a strong correlation. The transcription quality improvement provides over 10% relative WER reduction for ASR models. We release the dataset and code to benefit the research community",
    "checked": true,
    "id": "f3ffd5d42df1f8f75a796290d3ca592caf420f5e",
    "semantic_title": "human transcription quality improvement",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simantiraki23_interspeech.html": {
    "title": "The effect of masking noise on listeners' spectral tilt preferences",
    "volume": "main",
    "abstract": "Speech enhancement algorithms often focus on optimising intelligibility while neglecting other aspects of speech such as naturalness, quality and listening effort which may affect a listener's experience. This paper investigates the impact of spectral tilt on listeners' preferences, using a new corpus of Greek utterances. Participants adjusted spectral tilt with real-time feedback to select their preferred tilt in quiet and in the presence of speech-shaped noise at eight signal-to-noise ratios. Listeners displayed distinct preferences, with a tendency to select flatter tilts with increasing noise. Preferences were not random even for constant intelligibility, indicating that their adjustments were influenced by factors beyond the need to maintain comprehensibility. These findings have the potential to inform the design of speech enhancement algorithms that jointly optimise intelligibility and a listener's overall experience",
    "checked": true,
    "id": "8c776b3976d01f9eab2b3719cb88f22300482efe",
    "semantic_title": "the effect of masking noise on listeners' spectral tilt preferences",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tranngoc23_interspeech.html": {
    "title": "The Effect of Whistled Vowels on Whistled Word Categorization for Naive Listeners",
    "volume": "main",
    "abstract": "In this paper, we explore whistled word perception by naive French speakers. In whistled words of non-tonal languages, vowels are transposed to relatively stable pitches, which contrast with consonant movements or interruptions. Previous studies on whistled speech with naive listeners have tested vowels and consonants separately. Other studies on spoken word recognition have found that vowels and consonants contribute differently to intelligibility, where the role of vowels was highly mediated by the context. Here, naive participants recognize disyllabic whistled words above chance, and vowels are shown to contribute differently than consonants. When focusing on the role of vowels, we found different scales of performance between the vowels tested, mediated by their position in the word. We also highlighted the importance of the vowels' relative frequency difference (called 'interval') in the word",
    "checked": true,
    "id": "ad34c19adbe8afef6efdabb63de21327f935f520",
    "semantic_title": "the effect of whistled vowels on whistled word categorization for naive listeners",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bharati23_interspeech.html": {
    "title": "Automatic Deep Neural Network-Based Segmental Pronunciation Error Detection of L2 English Speech (L1 Bengali)",
    "volume": "main",
    "abstract": "In the last few decades, English has become a popular language as it helps us to communicate with the global world. A large population of English learners find it challenging to achieve an 'acceptable' and 'intelligible' pronunciation. To overcome these issues, various computer-assisted pronunciation training tools are designed where automatic pronunciation error detection (APED) is a core component of the system. Most of the works of APED are based on European English speech, but there is no such work reported for Bengali English speech. This paper proposes a system for pronunciation error detection of L2 English speech (L1 Bengali) at phoneme/segmental level using a hybrid convolutional neural network and long short-term memory modules with CTC loss. Experiments are done based on newly created L2 English speaker (L1 Bengali) speech data. The results demonstrate that the proposed system outperforms the goodness of pronunciation-based methods by 15% in terms of F1 score using fbank",
    "checked": true,
    "id": "99991481355364f3dcf3a68ef21b6b9bc7b7f2bc",
    "semantic_title": "automatic deep neural network-based segmental pronunciation error detection of l2 english speech (l1 bengali)",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hao23_interspeech.html": {
    "title": "The effect of stress on Mandarin tonal perception in continuous speech for Spanish-speaking learners",
    "volume": "main",
    "abstract": "The perception of lexical tones is a well-known challenge for L2 learners especially in continuous speech where the tonal variations are complicated. To investigate whether the stress, by affecting the acoustic manifestations of tones, has an effect on Mandarin tonal perception for L2 learners, we carried out a perceptual experiment based on the Annotated Speech Corpus of Chinese Discourse with 25 native Spanish-speaking participants. The results indicate that: the perceptual accuracy of stressed tones is significantly higher than that of the unstressed ones in general; T3 is the most difficult one to be perceived among the four Mandarin tones (T1-T4) in both stressed and unstressed syllables, and presumably the Spanish-speaking learners' perceptual order of Mandarin tones is T4-T1-T2-T3; the significant interactive effect found between tone and tonal context in continuous speech may lead to great confusion of tonal perception, especially when T2 and T3 are adjacent with one another",
    "checked": true,
    "id": "188daccc97de0e83437a912b69ef6b1dd7e91e67",
    "semantic_title": "the effect of stress on mandarin tonal perception in continuous speech for spanish-speaking learners",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elmerich23_interspeech.html": {
    "title": "Combining acoustic and aerodynamic data collection: A perceptual evaluation of acoustic distortions",
    "volume": "main",
    "abstract": "Combining acoustic and aerodynamic data acquisitions is challenging. Devices for aerodynamic measurements often create severe acoustic distortions, which make it impossible to analyse the simultaneously recorded acoustic data. An improved technique, a pneumotachograph mask made of synthetic fibers, is acoustically transparent while ensuring a high-quality aerodynamic data acquisition. A previous acoustic study confirms the minimal acoustic distortions caused by this technique. The present study evaluates the impact of different aerodynamic devices on the human perception of vowels. Results show that vowels recorded with a fiber mask are almost as accurately categorised as acoustic-only recordings, compared with rigid masks that result in perceptual confusions. Listeners are also less likely to perceive the presence of a mask. Overall, our study provides the perceptual validation of the fiber mask technique, which will be of a great value in the field of speech sciences",
    "checked": true,
    "id": "d9be8791612ae64004afa111a091f9f453fdf6a5",
    "semantic_title": "combining acoustic and aerodynamic data collection: a perceptual evaluation of acoustic distortions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elie23b_interspeech.html": {
    "title": "Estimating virtual targets for lingual stop consonants using general Tau theory",
    "volume": "main",
    "abstract": "This paper investigates the existence and position of virtual targets during the production of stop consonants. Using the equations from general Tau theory to model the time-course of tongue constriction formation movements, targets were estimated by fitting these equations on observed tongue constriction variables extracted from real EMA data from 2 native speakers of English. Results suggest that targets are virtual for 50 to 60% of movements. For these movements, virtual targets of the tongue tip constriction are predicted to occur around 0.1 cm beyond the palate, and virtual targets for the tongue dorsum constriction are predicted to occur between 0.05 and 0.2 cm beyond the palate. Our results suggest that the time-course of movement is planned so that the onset of closure occurs with relatively high velocity: closure onset is generally located very close in time to the time of peak velocity",
    "checked": true,
    "id": "59aae1527082e117b19d8e3d5da2562ec9300bfa",
    "semantic_title": "estimating virtual targets for lingual stop consonants using general tau theory",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gibson23_interspeech.html": {
    "title": "Using Random Forests to classify language as a function of syllable timing in two groups: children with cochlear implants and with normal hearing",
    "volume": "main",
    "abstract": "We trained a series of Random Forest models in a supervised learning environment on different temporal parameters related to syllable structure: voice onset time (VOT), vowel duration following simplex and complex onsets, and lateral duration in word initial (/lV) position and as the second consonant in a C1C2 cluster (where C means consonant). Capitalizing on previous work we trained the models on data from monolingual Spanish- and English-speaking adults. We asked whether the timing productions used by bilingual children with normal hearing (NH) and children with cochlear implants (CI) can be classified as pertaining to the same timing system (i.e. language), or whether the children are applying the same basic timing plan to two different languages. We also asked whether there were differences between the CI and NH groups. Our results indicate that the children from both groups produce qualitatively distinct timing plans for each language with no interference from the other language",
    "checked": true,
    "id": "e6d2b45c3a580e9e2ee38feeebfade94c1938286",
    "semantic_title": "using random forests to classify language as a function of syllable timing in two groups: children with cochlear implants and with normal hearing",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23w_interspeech.html": {
    "title": "An Improved End-to-End Audio-Visual Speech Recognition Model",
    "volume": "main",
    "abstract": "By incorporating lip language, audio-visual speech recognition can effectively improve the recognition effect in noisy environments, and will slightly improve the recognition effect in quiet environments. we use a frequency domain attention based residual network (Fca-Net) as the model of the vision front-end module, which extracts more features that are helpful to the AVSR and VSR system at a small cost. And use the powerful speech pre-training model Hu-BERT as the recognition front-end model of ASR. We compare the impact of different model as visual back-end modules and fusion modules on the AVSR system. Our experiments show that the model selection of the fusion module is critical to the performance of the AVSR system. Ultimately, our proposed model achieves state-of-the-art results on audio-visual speech recognition tasks using the LRS2 dataset",
    "checked": true,
    "id": "eda5321a9897bceebaf1b2a726b5bc22fdcbf134",
    "semantic_title": "an improved end-to-end audio-visual speech recognition model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wesoek23_interspeech.html": {
    "title": "What influences the foreign accent strength? Phonological and grammatical errors in the perception of accentedness",
    "volume": "main",
    "abstract": "The present study investigates the influence of grammatical and phonological errors on the perceived degree of foreign accent strength. German and Polish participants listened to speech in their native language produced with foreign and native accent. They rated the accent strength of each sentence on a 7-point scale. Grammatical errors consisted of gender agreement violations and phonological errors consisted of controlled vowel substitutions. Both error types significantly affected the perception of accent strength in the foreign and native-accented condition. In Polish, phonological anomalies had significantly more impact than grammatical violations in native-accented sentences. In German, there was no significant difference between phonological and grammatical violations. The study provides evidence that the presence of phonological and grammatical errors increases the perceived accentedness of speech. The weighting of both errors for accent perception can vary between languages",
    "checked": true,
    "id": "705b8611aef3beab246fb6e9a97d1c639b471c33",
    "semantic_title": "what influences the foreign accent strength? phonological and grammatical errors in the perception of accentedness",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huttner23_interspeech.html": {
    "title": "Investigating the Perception Production Link through Perceptual Adaptation and Phonetic Convergence",
    "volume": "main",
    "abstract": "Speech perception and production may be linked. In this pilot study, we aim to investigate the nature of this link by examining how perception and production of VOT adjust in social interaction. In an online experiment with 2x2 conditions participants were asked to categorize nine stimuli on a VOT continuum between /tin/ and /din/ and produce the words din and tin before and after playing a game with a bot. During the game participants were trained on a sound to word correspondence with a bias either towards /din/ or /tin/. In two conditions participants alternated categorizing and producing the stimuli, while in the other two participants only categorized the stimuli. Significant differences in categorization between conditions occurred only during the game. Whereas significant changes in VOT can be observed between pre and post-test for three conditions. This could mean that perception and production do not adjust symmetrically over the course of an interaction",
    "checked": true,
    "id": "198ee250e02d88d20b7dbd5c6c980626a301a16a",
    "semantic_title": "investigating the perception production link through perceptual adaptation and phonetic convergence",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23f_interspeech.html": {
    "title": "Emotion Prompting for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) classifies speech into emotion categories such as: Happy, Angry. Most prior works for SER focused on how to mine compelling features to improve performance. However, these methods ignore the influence of emotional label information on SER. Recent studies have attempted to prompt pre-trained language models and yield good performance for NLP tasks. Nevertheless, few works have attempted to prompt pre-trained speech models (PSM) on speech tasks. In light of these, we propose a simple but effective prompt-based method that prompts PSM for SER. Firstly, we reframe SER as an entailment task. Next, we generate speech prompts and combine them with the raw audio to form the input for PSM. Finally, we build a multi-task learning framework to extract more compelling features by simultaneously performing automatic speech recognition (ASR) and SER. Experiments on the IEMOCAP benchmark show that our method outperforms state-of-the-art baselines on the SER task",
    "checked": true,
    "id": "0d070b78fed9c485959c20012f089b9f390b8504",
    "semantic_title": "emotion prompting for speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chin23_interspeech.html": {
    "title": "Speech-in-Speech Recognition is Modulated by Familiarity to Dialect",
    "volume": "main",
    "abstract": "Listening to speech in competing background speech can be difficult due to elements such as the linguistic content of the signal. Linguistic release from masking occurs when altering the masker language results in less interference in speech recognition. The greater the linguistic differences between the target and masker, the higher the speech recognition accuracy. However, for dialectal variations of the same language, these patterns are less consistent. This study examined speech-in-speech recognition in Australian English monolinguals when the target speech was in either Australian (AU) or American English, and when the masker speech was in either of the dialects or a foreign language (Swedish). Speech recognition performance was greatest when AU was the target and poorest when AU was the masker. There were fewer differences in performance between the Swedish and dialect maskers. Results indicate that speech recognition is modulated by a listener's familiarity to a dialect",
    "checked": true,
    "id": "87c754368d3e9bbcbbcf547e1b14112e6028fe86",
    "semantic_title": "speech-in-speech recognition is modulated by familiarity to dialect",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23m_interspeech.html": {
    "title": "BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions",
    "volume": "main",
    "abstract": "Time-domain single-channel speech enhancement (SE) still remains challenging to extract the target speaker without any prior information on multi-talker conditions. It has been shown via auditory attention decoding that the brain activity of the listener contains the auditory information of the attended speaker. In this paper, we thus propose a novel time-domain brain-assisted SE network (BASEN) incorporating electroencephalography (EEG) signals recorded from the listener for extracting the target speaker from monaural speech mixtures. The proposed BASEN is based on the fully-convolutional time-domain audio separation network. In order to fully leverage the complementary information contained in the EEG signals, we further propose a convolutional multi-layer cross attention module to fuse the dual-branch features. Experimental results on a public dataset show that the proposed model outperforms the state-of-the-art method in several evaluation metrics. The reproducible code is available at https://github.com/jzhangU/Basen",
    "checked": true,
    "id": "a4a8bcf45750cfe4b4e1cfa6e739727c205b194d",
    "semantic_title": "basen: time-domain brain-assisted speech enhancement network with convolutional cross attention in multi-talker conditions",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/miodonska23_interspeech.html": {
    "title": "Are retroflex-to-dental sibilant substitutions in Polish children's speech an example of a covert contrast? A preliminary acoustic study",
    "volume": "main",
    "abstract": "The study aimed to investigate whether the dental substitutions of retroflex voiceless fricatives (/ʂ/ to [s]) in Polish children's speech are an example of a covert contrast. We analyzed speech samples collected through a picture naming test from 11 children showing this retroflex-to-dental production pattern. The language material included words with /ʂ/ and /s/ in diversified word positions. We extracted a set of spectrum-based acoustic features from the recorded sibilants and conducted the analysis using linear mixed-effect models. The models showed that significant acoustic differences (p < 0.05) can be found between realizations of /s/ and /ʂ/ substituted by [s]. The main differences were detected in the amplitudes of fricative formants and the energy levels in specific subbands of the frication noise. The study provides preliminary evidence of the existence of covert contrasts in the analyzed substitutions",
    "checked": true,
    "id": "476f3583d68b90b2d168d6d35641f41155df7947",
    "semantic_title": "are retroflex-to-dental sibilant substitutions in polish children's speech an example of a covert contrast? a preliminary acoustic study",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23i_interspeech.html": {
    "title": "Reversible Neural Networks for Memory-Efficient Speaker Verification",
    "volume": "main",
    "abstract": "Training large-scale speaker verification systems on consumer GPUs is difficult due to the memory consumption of the existing networks being proportional to the number of layers. In this paper, a novel family of Reversible Neural Networks (RevNets) is proposed for memory-efficient speaker verification. Specifically, we introduce two types of RevNets, namely partially and fully reversible networks, which alleviate the need to store activations in memory during back-propagation. Consequently, RevNets require nearly constant memory costs as the network depth increases. Experiments on Voxceleb show that RevNets achieve up to 15.7x memory savings, while maintaining nearly identical parameters and performance when compared to the vanilla ResNets. To our knowledge, this is the first work to investigate memory-efficient training for speaker verification. Our results indicate the potential of reversible networks as a more efficient backbone for resource-limited training scenarios",
    "checked": true,
    "id": "9d3e89a4cafc6ad96ff865c85c34cbb56bb63960",
    "semantic_title": "reversible neural networks for memory-efficient speaker verification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23f_interspeech.html": {
    "title": "ECAPA++: Fine-grained Deep Embedding Learning for TDNN Based Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we aim to bridge the performance gap between TDNN and 2D CNN based speaker verification systems. Specifically, three types of architectural enhancements to ECAPA-TDNN are proposed: 1) follow depth-first design to significantly increase network depth while maintaining its complexity. 2) introduce recursive convolution to better capture fine-grained speaker information. 3) propose pyramid-based multi-path feature enhancement module to yield more discriminative speaker representation. Experiments on Voxceleb show that our final model, named ECAPA++, achieves 25%, 23% and 24% relative improvements on Vox1-O, E and H respectively, while with 2.4x fewer parameters and 2.3x fewer FLOPs over the previous best TDNN-based system. Meanwhile, it is comparable to the state-of-the-art ResNet-based systems with higher computational efficiency. In addition, further performance gains can be achieved by fusing ECAPA++ and ResNet-based systems",
    "checked": true,
    "id": "1ea4ab514e59f78cd971745b322eefc9c4723d20",
    "semantic_title": "ecapa++: fine-grained deep embedding learning for tdnn based speaker verification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23w_interspeech.html": {
    "title": "TO-Rawnet: Improving RawNet with TCN and Orthogonal Regularization for Fake Audio Detection",
    "volume": "main",
    "abstract": "Current fake audio detection relies on hand-crafted features, which lose information during extraction. To overcome this, recent studies use direct feature extraction from raw audio signals. For example, RawNet is one of the representative works in end-to-end fake audio detection. However, existing work on RawNet does not optimize the parameters of the Sinc-conv during training, which limited its performance. In this paper, we propose to incorporate orthogonal convolution into RawNet, which reduces the correlation between filters when optimizing the parameters of Sinc-conv, thus improving discriminability. Additionally, we introduce temporal convolutional networks (TCN) to capture long-term dependencies in speech signals. Experiments on the ASVspoof 2019 show that the Our TO-RawNet system can relatively reduce EER by 66.09% on logical access scenario compared with the RawNet, demonstrating its effectiveness in detecting fake audio attacks",
    "checked": true,
    "id": "292da433d540c50d1beac4b330e32ac7f76a326c",
    "semantic_title": "to-rawnet: improving rawnet with tcn and orthogonal regularization for fake audio detection",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zuo23_interspeech.html": {
    "title": "Fooling Speaker Identification Systems with Adversarial Background Music",
    "volume": "main",
    "abstract": "Speaker identification (SI) systems are widely used in real-world scenarios but are vulnerable to attacks from malicious users. Although existing attacks mainly focus on speech-shaped inputs, SI models can also be broken by speech-unrelated background music (BGM) in practical use. In this paper, we propose a new attack, called BGM Attack (BGMA), that generates auditorily natural music to deceive SI models. BGMA integrates a music generation model and a SI model to modify the music-level semantic features. We propose a linear transform called differentiable spectrogram reconstruction (DSR) that acts as a bridge for conveying gradient information between the two models in BGMA. Our experiments show that BGMA can effectively break state-of-the-art SI models with generated auditorily natural music. The result of this paper highlights the need for SI models to be robust against attacks from non-speech inputs and provides a novel attack method for testing the security of SI systems",
    "checked": true,
    "id": "eb2defea28d138d945137d5bb3cca96de492e90b",
    "semantic_title": "fooling speaker identification systems with adversarial background music",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23r_interspeech.html": {
    "title": "Mutual Information-based Embedding Decoupling for Generalizable Speaker Verification",
    "volume": "main",
    "abstract": "Domain shift is a challenging problem in speaker verification, especially when dealing with unseen target domains. Recently, embedding decoupling-based methods have shown their effectiveness. Typically, domain information is extracted by a domain classification loss and then decoupled from speaker embeddings. However, the domain classification loss fails to ensure that only domain information is encoded in domain embeddings. This paper proposes a novel mutual information-based embedding decoupling framework, in which the domain information is extracted by maximizing the mutual information between different speaker sample pairs in the same domain. Then the domain information is removed from speaker embeddings by minimizing mutual information between speaker and domain embeddings. Experiments indicate that our method can improve the generalization and outperform domain classification-based decoupling methods",
    "checked": true,
    "id": "d2d563cb3e209cf501e35c6a1a3e77143093721b",
    "semantic_title": "mutual information-based embedding decoupling for generalizable speaker verification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23c_interspeech.html": {
    "title": "Target Active Speaker Detection with Audio-visual Cues",
    "volume": "main",
    "abstract": "In active speaker detection (ASD), we would like to detect whether an on-screen person is speaking based on audio-visual cues. Previous studies have primarily focused on modeling audio-visual synchronization cue, which depends on the video quality of the lip region of a speaker. In real-world applications, it is possible that we can also have the reference speech of the on-screen speaker. To benefit from both facial cue and reference speech, we propose the Target Speaker TalkNet (TS-TalkNet), which leverages a pre-enrolled speaker embedding to complement the audio-visual synchronization cue in detecting whether the target speaker is speaking. Our framework outperforms the popular model, TalkNet on two datasets, achieving absolute improvements of 1.6% in mAP on the AVA-ActiveSpeaker validation set, and 0.8%, 0.4%, and 0.8% in terms of AP, AUC and EER on the ASW test set, respectively.Code is available at https://github.com/Jiang-Yidi/TS-TalkNet/",
    "checked": true,
    "id": "8ce2887342e558d625d7b13823d3d686b6b63d2c",
    "semantic_title": "target active speaker detection with audio-visual cues",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/broughton23_interspeech.html": {
    "title": "Improving End-to-End Neural Diarization Using Conversational Summary Representations",
    "volume": "main",
    "abstract": "Speaker diarization is a task concerned with partitioning an audio recording by speaker identity. End-to-end neural diarization with encoder-decoder based attractor calculation (EEND-EDA) aims to solve this problem by directly outputting diarization results for a flexible number of speakers. Currently, the EDA module responsible for generating speaker-wise attractors is conditioned on zero vectors providing no relevant information to the network. In this work, we extend EEND-EDA by replacing the input zero vectors to the decoder with learned conversational summary representations. The updated EDA module sequentially generates speaker-wise attractors based on utterance-level information. We propose three methods to initialize the summary vector and conduct an investigation into varying input recording lengths. On a range of publicly available test sets, our model achieves an absolute DER performance improvement of 1.90 % when compared to the baseline",
    "checked": true,
    "id": "3ec0a75841b846d0b57e99750da739653699de2c",
    "semantic_title": "improving end-to-end neural diarization using conversational summary representations",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zang23_interspeech.html": {
    "title": "Phase perturbation improves channel robustness for speech spoofing countermeasures",
    "volume": "main",
    "abstract": "In this paper, we aim to address the problem of channel robustness in speech countermeasure (CM) systems, which are used to distinguish synthetic speech from human natural speech. On the basis of two hypotheses, we suggest an approach for perturbing phase information during the training of time-domain CM systems. Communication networks often employ lossy compression codec that encodes only magnitude information, therefore heavily phase information. Also, state-of-the-art CM systems rely on phase information to identify spoofed speech. Thus, we believe the information loss in the phase domain induced by lossy compression codec degrades the performance of the unseen channel. We first establish the dependence of time-domain CM systems on phase information by perturbing phase in evaluation, showing strong degradation. Then, we demonstrated that perturbing phase during training leads to a significant performance improvement, whereas perturbing magnitude leads to further degradation",
    "checked": true,
    "id": "a1f3b07ac1e2c9c726ac8130f29d5ea8e3b9590e",
    "semantic_title": "phase perturbation improves channel robustness for speech spoofing countermeasures",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bousquet23_interspeech.html": {
    "title": "Improving training datasets for resource-constrained speaker recognition neural networks",
    "volume": "main",
    "abstract": "Tackling the increase in complexity, which is now the main factor of improvement in deep learning, this paper proposes a new algorithm of data selection for training speaker recognition systems. The method starts from an initial training dataset, then the algorithm scans new data to determine the most useful speakers for completing the initial ones. The resulting training dataset improves the model, in terms of accuracy and ability to generalize, while maintaining the learning complexity reasonable. This algorithm is unsupervised, as it does not need any metadata on the new utterances and, therefore, compatible with self-supervised learning. By selecting only 30% of the speakers from a new database, the proposed algorithm is able to achieve very similar performances to the system with all speakers added",
    "checked": true,
    "id": "5eaae3fb1d3aaa488d67196e16e7359ff6657a3e",
    "semantic_title": "improving training datasets for resource-constrained speaker recognition neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lertpetchpun23_interspeech.html": {
    "title": "Instance-based Temporal Normalization for Speaker Verification",
    "volume": "main",
    "abstract": "One of the challenges in speaker verification is domain mismatch and other effects such as language and emotion. Normalization techniques such as Batch Normalization (BN) have been proven effective in improving neural network training and are a popular choice in many speaker verification networks. However, BN may not be able to adequately normalize the feature map for speaker verification. In this work, we investigate several instance-based normalization methods which are more suitable for speaker verification. We propose the Temporal Normalization layer, which normalizes along the time dimension, and show its effectiveness on four different datasets. Experiments on VoxCeleb2 show a relative improvement of 24.3% and 46.15% in terms of EER and DCF over fwSE-ResNet34 in VoxCeleb1-O. Furthermore, we present a systematic evaluation of our networks against three other datasets, namely Thai-Central, THAI-SER, and CREMA-D to show its robustness on language and emotional variants",
    "checked": true,
    "id": "75903b14b2eb61ecfeb381b09f31e055946767d4",
    "semantic_title": "instance-based temporal normalization for speaker verification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/novoselov23_interspeech.html": {
    "title": "On the robustness of wav2vec 2.0 based speaker recognition systems",
    "volume": "main",
    "abstract": "Recent advances in unsupervised speech representation learning discover new approaches and provide new state-of-the-art for diverse types of speech processing tasks. This paper extends the investigation of using wav2vec 2.0 deep speech representations for the speaker recognition task. It focuses on the robustness issues in different domains and considers the effectiveness of wav2vec not only on telephone and microphone speaker verification protocols but also for cross-channel task. It is concluded that powerful transformer-based speaker recognition systems can be well-generalized across variable conditions. It is concluded that powerful transformer-based speaker recognition systems can be well-generalized across variable conditions. In this study speaker recognition systems were analyzed on a wide range of well-known verification protocols. According to the results obtained in this paper we recommend to use data augmentation for fine-tuning of wav2vec based systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23i_interspeech.html": {
    "title": "P-vectors: A Parallel-coupled TDNN/Transformer Network for Speaker Verification",
    "volume": "main",
    "abstract": "Typically, the Time-Delay Neural Network (TDNN) and Transformer can serve as a backbone for Speaker Verification (SV). Both of them have advantages and disadvantages from the perspective of global and local feature modeling. How to effectively integrate these two style features is still an open issue. In this paper, we explore a Parallel-coupled TDNN/Transformer Network (p-vectors) to replace the serial hybrid networks. The p-vectors allows TDNN and Transformer to learn the complementary information from each other through Soft Feature Alignment Interaction (SFAI) under the premise of preserving local and global features. Also, p-vectors uses the Spatial Frequency-channel Attention (SFA) to enhance the spatial interdependence modeling for input features. Finally, the outputs of dual branches of p-vectors are combined by Embedding Aggregation Layer (EAL). Experiments1 show that p-vectors outperforms MACCIF-TDNN and MFA-Conformer with relative improvements of 11.5% and 13.9% in EER on VoxCeleb1-O",
    "checked": true,
    "id": "66e770fe847c458a6c97a9a82061e4b6f2b91da7",
    "semantic_title": "p-vectors: a parallel-coupled tdnn/transformer network for speaker verification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lei23_interspeech.html": {
    "title": "Group GMM-ResNet for Detection of Synthetic Speech Attacks",
    "volume": "main",
    "abstract": "The CNN-based models have achieved a remarkable success for speaker recognition and spoofing speech detection. We propose the group GMM-ResNet for synthesis speech detection. The grouping technique is used to improve classification accuracy by exposing the group cardinality while reducing both the number of parameters and the training time. The grouping technique allows the model to jointly attend to information from different representation subspaces. We propose two grouping methods, which are based on the Gaussian components in GMM. And the GMM is trained using binary splitting method. On the ASVspoof 2021 LA task, the group GMM-ResNet achieves a minimum t-DCF of 0.2450 and an EER of 2.53%, which relatively reduces by 28.9% and 72.7% compared with the LFCC-LCNN baseline. On the ASVspoof 2021 DF task, the group GMM-ResNet achieves an EER of 15.96%, which relatively reduces by 28.7% compared with the RawNet2 baseline",
    "checked": true,
    "id": "1166def2459b185e3729f4a93893c30da65e6fd5",
    "semantic_title": "group gmm-resnet for detection of synthetic speech attacks",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fang23_interspeech.html": {
    "title": "Robust Training for Speaker Verification against Noisy Labels",
    "volume": "main",
    "abstract": "The deep learning models used for speaker verification rely heavily on large amounts of data and correct labeling. However, noisy (incorrect) labels often occur, which degrades the performance of the system. In this paper, we propose a novel two-stage learning method to filter out noisy labels from speaker datasets. Since a DNN will first fit data with clean labels, we first train the model with all data for several epochs. Then, based on this model, the model predictions are compared with the labels using our proposed the OR-Gate with top-k mechanism to select the data with clean labels and the selected data is used to train the model. This process is iterated until the training is completed. We have demonstrated the effectiveness of this method in filtering noisy labels through extensive experiments and have achieved excellent performance on the VoxCeleb (1 and 2) with different added noise rates",
    "checked": true,
    "id": "61c5058554bf7bdcb4349045feeec2afc24be0cd",
    "semantic_title": "robust training for speaker verification against noisy labels",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jeoung23_interspeech.html": {
    "title": "Self-Distillation into Self-Attention Heads for Improving Transformer-based End-to-End Neural Speaker Diarization",
    "volume": "main",
    "abstract": "In this study, we explore self-distillation (SD) techniques to improve the performance of the transformer-encoder-based self-attentive (SA) end-to-end neural speaker diarization (EEND). We first apply the SD approaches, introduced in the automatic speech recognition field, to the SA-EEND model to confirm their potential for speaker diarization. Then, we propose two novel SD methods for the SA-EEND, which distill the prediction output of the model or the SA heads of the upper blocks into the SA heads of the lower blocks. Consequently, we expect the high-level speaker-discriminative knowledge learned by the upper blocks to be shared across the lower blocks, thereby enabling the SA heads of the lower blocks to effectively capture the discriminative patterns of overlapped speech of multiple speakers. Experimental results on the simulated and CALLHOME datasets show that the SD generally improves the baseline performance, and the proposed methods outperform the conventional SD approaches",
    "checked": true,
    "id": "04f08e8c3ecb300812aaf8dc36cc7fa037e4a8b0",
    "semantic_title": "self-distillation into self-attention heads for improving transformer-based end-to-end neural speaker diarization",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23m_interspeech.html": {
    "title": "Build a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022",
    "volume": "main",
    "abstract": "Many speaker recognition challenges have been held to assess the speaker verification system in the wild and probe the performance limit. Voxceleb Speaker Recognition Challenge (VoxSRC), based on the voxceleb, is the most popular. Besides, another challenge called CN-Celeb Speaker Recognition Challenge (CNSRC) is also held this year, which is based on the Chinese celebrity multi-genre dataset CN-Celeb. Last year, our team participated in both speaker verification closed tracks in CNSRC 2022 and VoxSRC 2022, and achieved the 1st place and 3rd place respectively. In most system reports, the authors usually only provide a description of their systems but lack an effective analysis of their methods. In this paper, we will outline how to build a strong speaker verification challenge system and give a detailed analysis of each method compared with some other popular technical means",
    "checked": true,
    "id": "7e53dc0a6e70ff6373ce0b53691b8038faefe8c8",
    "semantic_title": "build a sre challenge system: lessons from voxsrc 2022 and cnsrc 2022",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benamor23_interspeech.html": {
    "title": "Describing the phonetics in the underlying speech attributes for deep and interpretable speaker recognition",
    "volume": "main",
    "abstract": "Deep neural networks have dominated speaker recognition, with a sharp increase in performance associated with increasingly complex models. This comes at the cost of transparency, which poses serious problems for informed decision making. In response, an intrinsically interpretable scoring approach, BA-LR, was recently presented. This method uses an attribute-based bottom-up representation of speech, linked with a transparent scoring scheme. For the sake of explainability, the present work adds an analysis of the nature of the attributes, by selecting and quantifying the contributions of the phonetic variables that describe it. We propose two methods based on statistical and surrogate models, respectively. The results reveal that the speech attributes are each well described by a set of descriptive variables. This allows us to propose the first transparent scoring scheme in speaker recognition, where the weights of the phonetic variables contributing to each decision item are known",
    "checked": true,
    "id": "6c6a77236a9fa1f373986b4c875ac642f2ea1976",
    "semantic_title": "describing the phonetics in the underlying speech attributes for deep and interpretable speaker recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23v_interspeech.html": {
    "title": "Range-Based Equal Error Rate for Spoof Localization",
    "volume": "main",
    "abstract": "Spoof localization, also called segment-level detection, is a crucial task that aims to locate spoofs in partially spoofed audio. The equal error rate (EER) is widely used to measure performance for such biometric scenarios. Although EER is the only threshold-free metric, it is usually calculated in a point-based way that uses scores and references with a pre-defined temporal resolution and counts the number of misclassified segments. Such point-based measurement overly relies on this resolution and may not accurately measure misclassified ranges. To properly measure misclassified ranges and better evaluate spoof localization performance, we upgrade point-based EER to range-based EER. Then, we adapt the binary search algorithm for calculating range-based EER and compare it with the classical point-based EER. Our analyses suggest utilizing either range-based EER, or point-based EER with a proper temporal resolution can fairly and properly evaluate the performance of spoof localization",
    "checked": true,
    "id": "58e0903a81867685f6bfcb1a7a97b21f23d50b70",
    "semantic_title": "range-based equal error rate for spoof localization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tabassum23_interspeech.html": {
    "title": "Exploring the English Accent-independent Features for Speech Emotion Recognition using Filter and Wrapper-based Methods for Feature Selection",
    "volume": "main",
    "abstract": "In Speech Emotion Recognition (SER), significant progress has been made. Despite cutting-edge developments, faultless human-computer interaction remains a distant goal since established SoTA models cannot perceive the speaker's emotional state flawlessly. On the contrary, several studies in SER uncovered the possibility of language and culture-specific differences in this domain. Emotion recognition in speech can vary from person to person based on age, gender, language, and accent, amongst others. In this study, we explore and investigate how assorted accents of the English language influence SER. We employ four different English accents: American, British, Canadian, and Bengali English. Then we extracted a subset of best-performing accent-neutral features by incorporating filter and wrapper-based feature selection methods. Our investigations reveal that pitch, intensity, and MFCC-related features more effectively recognize emotions regardless of accent",
    "checked": true,
    "id": "ce2589c0a5ea10bf87826c9d41f8a99305a47e24",
    "semantic_title": "exploring the english accent-independent features for speech emotion recognition using filter and wrapper-based methods for feature selection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/plaquet23_interspeech.html": {
    "title": "Powerset multi-class cross entropy loss for neural speaker diarization",
    "volume": "main",
    "abstract": "Since its introduction in 2019, the whole end-to-end neural diarization (EEND) line of work has been addressing speaker diarization as a frame-wise multi-label classification problem with permutation-invariant training. Despite EEND showing great promise, a few recent works took a step back and studied the possible combination of (local) supervised EEND diarization with (global) unsupervised clustering. Yet, these hybrid contributions did not question the original multi-label formulation. We propose to switch from multi-label (where any two speakers can be active at the same time) to powerset multi-class classification (where dedicated classes are assigned to pairs of overlapping speakers). Through extensive experiments on 9 different benchmarks, we show that this formulation leads to significantly better performance (mostly on overlapping speech) and robustness to domain mismatch, while eliminating the detection threshold hyper-parameter, critical for the multi-label formulation",
    "checked": true,
    "id": "2ab5f1d4fa37a5d329fc2817c97b68ef10b55c1b",
    "semantic_title": "powerset multi-class cross entropy loss for neural speaker diarization",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23_interspeech.html": {
    "title": "A Method of Audio-Visual Person Verification by Mining Connections between Time Series",
    "volume": "main",
    "abstract": "It has already been observed that audio-visual embedding is more robust than uni-modality embedding for person verification. But the relationship of keyframes in time series between modalities seems to be unexplored. Hence, we proposed a novel audio-visual strategy that considers connections between time series from a generative perspective. First, we introduced weight-enhanced attentive statistics pooling to extend the salience of the keyframe weights. Then, joint attentive pooling incorporating 3 popular generative supervision models is proposed. Finally, each modality is fused with a gated attention mechanism to gain robust embedding. All the proposed models are trained on the VoxCeleb2 dev dataset and the best system obtains 0.14%, 0.21%, and 0.37% EER on three official trial lists of VoxCeleb1 respectively, which is to our knowledge the best-published results for person verification",
    "checked": true,
    "id": "1effbe9ff6f1189009b41ac8518abfde1d60e863",
    "semantic_title": "a method of audio-visual person verification by mining connections between time series",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fish23_interspeech.html": {
    "title": "A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization",
    "volume": "main",
    "abstract": "Recent advancement in Automatic Speech Recognition (ASR) has produced large AI models, which become impractical for deployment in mobile devices. Model quantization is effective to produce compressed general-purpose models, however such models may only be deployed to a restricted sub-domain of interest. We show that ASR models can be personalized during quantization while relying on just a small set of unlabelled samples from the target domain. To this end, we propose myQASR, a mixed-precision quantization method that generates tailored quantization schemes for diverse users under any memory requirement with no fine-tuning. myQASR automatically evaluates the quantization sensitivity of network layers by analyzing the full-precision activation values. We are then able to generate a personalized mixed-precision quantization scheme for any predetermined memory budget. Results for large-scale ASR models show how myQASR improves performance for specific genders, languages, and speakers",
    "checked": true,
    "id": "a3cc2fe45ade50197179384694874e389c4e5678",
    "semantic_title": "a model for every user and budget: label-free and personalized mixed-precision quantization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23_interspeech.html": {
    "title": "Modeling Dependent Structure for Utterances in ASR Evaluation",
    "volume": "main",
    "abstract": "The bootstrap resampling method has been popular for performing significance analysis on word error rate (WER) in automatic speech recognition (ASR) evaluation. To deal with dependent speech data, the blockwise bootstrap approach is also introduced. By dividing utterances into uncorrelated blocks, this approach resamples these blocks instead of original data. However, it is typically nontrivial to uncover the dependent structure among utterances and identify the blocks, which might lead to subjective conclusions in statistical testing. In this paper, we present graphical lasso based methods to explicitly model such dependency and estimate uncorrelated blocks of utterances in a rigorous way, after which blockwise bootstrap is applied on top of the inferred blocks. We show the resulting variance estimator of WER in ASR evaluation is statistically consistent under mild conditions. We also demonstrate the validity of proposed approach on the LibriSpeech dataset",
    "checked": true,
    "id": "b320400ce22a4a491d4c7a602c820b099a557db5",
    "semantic_title": "modeling dependent structure for utterances in asr evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/verma23_interspeech.html": {
    "title": "ASR for Low Resource and Multilingual Noisy Code-Mixed Speech",
    "volume": "main",
    "abstract": "Developing reliable Automatic Speech Recognition (ASR) system for Indian Languages has been challenging due to the limited availability of large-scale, high-quality speech datasets. This problem is even more pronounced when dealing with noisy code-mixed settings with different grapheme vocabularies. This paper proposes a novel ASR system for low-resource noisy speech code mixed with Indian languages. Our approach involves fine-tuning pre-trained models using text transliterated to Devanagari and mapping similar-sounding characters into one character group. Experiments show the model's effectiveness for low-resource Indian languages, including noisy, code-mixed, and multilingual settings. The approach outperforms several baseline models and demonstrates the potential for adapting state-of-the-art ASR models to new languages with limited resources. The proposed system has been deployed in production, where call centers use it to transcribe customer calls",
    "checked": true,
    "id": "a0e1c6cc9598c0f7bd04a19ce73e84aa27083366",
    "semantic_title": "asr for low resource and multilingual noisy code-mixed speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23b_interspeech.html": {
    "title": "Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "Estimating confidence scores for recognition results is a classic task in ASR field and of vital importance for kinds of downstream tasks and training strategies. Previous end-to-end (E2E) based confidence estimation models (CEM) predict score sequences of equal length with input transcriptions, leading to unreliable estimation when deletion and insertion errors occur. In this paper we proposed CIF-Aligned confidence estimation model (CA-CEM) to achieve accurate and reliable confidence estimation based on novel non-autoregressive E2E ASR model - Paraformer. CA-CEM utilizes the modeling character of continuous integrate-and-fire (CIF) mechanism to generate token-synchronous acoustic embedding, which solves the estimation failure issue above. We measure the quality of estimation with AUC and RMSE in token level and ECE-U - a proposed metrics in utterance level. CA-CEM gains 24% and 19% relative reduction on ECE-U and also better AUC and RMSE on two test sets. Furthermore, we conduct analysis to explore the potential of CEM for different ASR related usage",
    "checked": true,
    "id": "36ee30709a4955bcda7d9b7cada656f1d6c159ca",
    "semantic_title": "accurate and reliable confidence estimation based on non-autoregressive end-to-end speech recognition system",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mateju23_interspeech.html": {
    "title": "Combining Multilingual Resources and Models to Develop State-of-the-Art E2E ASR for Swedish",
    "volume": "main",
    "abstract": "In terms of automatic speech recognition (ASR), Swedish belongs to the group of less-resourced languages, as publicly available training data is limited to a few hundred hours of mostly read speech. To acquire larger amounts of more realistic data, we investigate the existing multilingual approaches, and also propose two new ones, which combine Swedish with previously created Norwegian data and models. We use them for efficient automatic harvesting of spoken Swedish from broadcast, parliament, YouTube, and audiobook archives. The combined models significantly speed up the harvesting process and improve the final Swedish end-to-end (E2E) ASR system. We evaluate it on datasets covering various applications and domains; they provide performance better than the state-of-the-art commercial cloud services. We have made all of our test datasets publicly available for future comparative experiments",
    "checked": true,
    "id": "d1e7b3732891a8da4b20741117670cd4c181ba1d",
    "semantic_title": "combining multilingual resources and models to develop state-of-the-art e2e asr for swedish",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23o_interspeech.html": {
    "title": "Two Stage Contextual Word Filtering for Context Bias in Unified Streaming and Non-streaming Transducer",
    "volume": "main",
    "abstract": "It is difficult for an E2E ASR system to recognize words such as entities appearing infrequently in the training data. A widely used method to mitigate this issue is feeding contextual information into the acoustic model. Previous works have proven that a compact and accurate contextual list can boost the performance significantly. In this paper, we propose an efficient approach to obtain a high quality contextual list for a unified streaming/non-streaming based E2E model. Specifically, we make use of the phone-level streaming output to first filter the predefined contextual word list then fuse it into non-casual encoder and decoder to generate the final recognition results. Our approach improve the accuracy of the contextual ASR system and speed up the inference process. Experiments on two datasets demonstrates over 20% CER reduction comparing to the baseline system. Meanwhile, the RTF of our system can be stabilized within 0.15 when the size of the contextual word list grows over 6,000",
    "checked": true,
    "id": "a97f711597d004e4b7cb048f2f94c597cbff7489",
    "semantic_title": "two stage contextual word filtering for context bias in unified streaming and non-streaming transducer",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pham23_interspeech.html": {
    "title": "Towards continually learning new languages",
    "volume": "main",
    "abstract": "Multilingual speech recognition with neural networks is often implemented with batch-learning, when all of the languages are available before training. An ability to add new languages after the prior training sessions can be economically beneficial, but the main challenge is catastrophic forgetting. In this work, we combine the qualities of weight factorization and elastic weight consolidation in order to counter catastrophic forgetting and facilitate learning new languages quickly. Such combination allowed us to eliminate catastrophic forgetting while still achieving performance for the new languages comparable with having all languages at once, in experiments of learning from an initial 10 languages to achieve 26 languages without catastrophic forgetting and a reasonable performance compared to training all languages from scratch",
    "checked": true,
    "id": "5cde1c29780b2f7ad134b4e57e56bbc948bd033d",
    "semantic_title": "towards continually learning new languages",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23e_interspeech.html": {
    "title": "N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space",
    "volume": "main",
    "abstract": "Error correction models form an important part of Automatic Speech Recognition (ASR) post-processing to improve the readability and quality of transcriptions. Most prior works use the 1-best ASR hypothesis as input and therefore can only perform correction by leveraging the context within one sentence. In this work, we propose a novel N-best T5 model for this task, which is fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By transferring knowledge from the pre-trained language model and obtaining richer information from the ASR decoding space, the proposed approach outperforms a strong Conformer-Transducer baseline. Another issue with standard error correction is that the generation process is not well-guided. To address this a constrained decoding process, either based on the N-best list or an ASR lattice, is used which allows additional information to be propagated",
    "checked": true,
    "id": "740b6c5c8c4ad4892a51bdc9da433f3177dbc6f3",
    "semantic_title": "n-best t5: robust asr error correction using multiple input hypotheses and constrained decoding space",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23g_interspeech.html": {
    "title": "SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge",
    "volume": "main",
    "abstract": "Recently, excellent progress has been made in speech recognition. However, pure data-driven approaches have struggled to solve the problem in domain-mismatch and long-tailed data. Considering that knowledge-driven approaches can help data-driven approaches alleviate their flaws, we introduce sememe-based semantic knowledge information to speech recognition (SememeASR). Sememe, according to the linguistic definition, is the minimum semantic unit in a language and is able to represent the implicit semantic information behind each word very well. Our experiments show that the introduction of sememe information can improve the effectiveness of speech recognition. In addition, our further experiments show that sememe knowledge can improve the model's recognition of long-tailed data and enhance the model's domain generalization ability",
    "checked": true,
    "id": "1bd11964d4ef12d28c67a76feaee8e5786391460",
    "semantic_title": "sememeasr: boosting performance of end-to-end speech recognition against domain and long-tailed data shift with sememe semantic knowledge",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gulzar23_interspeech.html": {
    "title": "miniStreamer: Enhancing Small Conformer with Chunked-Context Masking for Streaming ASR Applications on the Edge",
    "volume": "main",
    "abstract": "Real-time applications of Automatic Speech Recognition (ASR) on user devices on the edge require streaming processing. Conformer model has achieved state-of-the-art performance in ASR for the non-streaming task. Conventional approaches have tried to achieve streaming ASR with Conformer using causal operations, but it leads to quadratic increase in the computational cost as the utterance length increases. In this work, we propose a chunked-context masking approach to perform streaming ASR with Conformer, which limits the computational cost from quadratic to a constant value. Our approach allows self-attention in Conformer encoder to attend the limited past information in form of chunked context. It achieves close to the full context causal performance for Conformer-Transducer, while significantly reducing the computational cost and maintains a low Real Time Factor (RTF) which is highly desirable trait for resource-constrained low-power edge devices",
    "checked": true,
    "id": "3beb439741a881905b696c4255979148671ab682",
    "semantic_title": "ministreamer: enhancing small conformer with chunked-context masking for streaming asr applications on the edge",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23p_interspeech.html": {
    "title": "CoMFLP: Correlation Measure Based Fast Search on ASR Layer Pruning",
    "volume": "main",
    "abstract": "Transformer-based speech recognition (ASR) model with deep layers exhibited significant performance improvement. However, the model is inefficient for deployment on resource constrained devices. Layer pruning (LP) is a commonly used compression method to remove redundant layers. Previous studies on LP usually identify the redundant layers according to a task-specific evaluation metric. They are time-consuming for models with a large number of layers, even in a greedy search manner. To address this problem, we propose CoM-FLP, a fast search LP algorithm based on correlation measure. The correlation between layers is computed to generate a correlation matrix, which identifies the redundancy among layers. The search process is carried out in two steps: (1) coarse search: to determine top K candidates by pruning the most redundant layers based on the correlation matrix; (2) fine search: to select the best pruning proposal among K candidates using a task-specific evaluation metric. Experiments on an ASR task show that the pruning proposal determined by CoMFLP outperforms existing LP methods while only requiring constant time complexity. The code is publicly available at https://github.com/louislau1129/CoMFLP",
    "checked": true,
    "id": "a402c9b48238fb9755d8117f7c57eed039906939",
    "semantic_title": "comflp: correlation measure based fast search on asr layer pruning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23h_interspeech.html": {
    "title": "Exploration on HuBERT with Multiple Resolution",
    "volume": "main",
    "abstract": "Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals",
    "checked": false,
    "id": "fa75ef55e04e3b25b8af56435478c2fd17403ce8",
    "semantic_title": "exploration on hubert with multiple resolutions",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23s_interspeech.html": {
    "title": "Quantization-aware and Tensor-compressed Training of Transformers for Natural Language Understanding",
    "volume": "main",
    "abstract": "Fine-tuned transformer models have shown superior performances in many natural language tasks. However, the large model size prohibits deploying high-performance transformer models on resource-constrained devices. This paper proposes a quantization-aware tensor-compressed training approach to reduce the model size, arithmetic operations, and runtime latency of transformer-based models. We compress the embedding and linear layers of transformers into small low-rank tensor cores, significantly reducing model parameters. A quantization-aware training with learnable scales factors is used to further obtain low-precision representations of the tensor-compressed models. The developed approach can be used for both end-to-end training and distillation-based training. To improve the convergence, layer-by-layer distillation is applied to distill a quantized tensor-compressed student model from a pre-trained transformer. The performance is demonstrated in two natural language understanding tasks, showing up to 63 times compression ratio with little accuracy loss and remarkable inference and training speedup",
    "checked": true,
    "id": "c7f3aa64da445f0f3db7f337a9caffeba9d90bbb",
    "semantic_title": "quantization-aware and tensor-compressed training of transformers for natural language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naowarat23b_interspeech.html": {
    "title": "Word-level Confidence Estimation for CTC Models",
    "volume": "main",
    "abstract": "Measuring confidence in Automatic Speech Recognition (ASR) is important for ensuring the reliability of downstream applications. Previous works proposed Confidence Estimation Module (CEM) for predicting confidences for autoregressive attention-based and neural transducer architectures. However, CEM for connectionist temporal classification (CTC) models have not been explored. In this work, we expand the idea of CEM to CTC models and further propose considering surrounding words for estimating confidences. Our experiments on four test sets in two languages demonstrate that our proposed method significantly reduces calibration errors of both common and rare words compared to naive confidences from CTC softmax. Moreover, we show that the approach is also effective for hard words and out-of-domain test sets, indicating its potential to be used as a reliable trigger for human intervention",
    "checked": true,
    "id": "6ebf4541e74440cc553e0cb131987ef036fe45fd",
    "semantic_title": "word-level confidence estimation for ctc models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kulshreshtha23_interspeech.html": {
    "title": "Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification (CTC) models are popular for their balance between speed and performance for Automatic Speech Recognition (ASR). However, these CTC models still struggle in other areas, such as personalization towards custom words. A recent approach explores Contextual Adapters, wherein an attention-based biasing model for CTC is used to improve the recognition of custom entities. While this approach works well with enough data, we showcase that it isn't an effective strategy for low-resource languages. In this work, we propose a supervision loss for smoother training of the Contextual Adapters. Further, we explore a multilingual strategy to improve performance with limited training data. Our method achieves 48% F1 improvement in retrieving unseen custom entities for a low-resource language. Interestingly, as a by-product of training the Contextual Adapters, we see a 5-11% Word Error Rate (WER) reduction in the performance of the base CTC model as well",
    "checked": true,
    "id": "538bcc850b49aa0bdeb98d9cc095c3933e7e6e2e",
    "semantic_title": "multilingual contextual adapters to improve custom word recognition in low-resource languages",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23_interspeech.html": {
    "title": "Unsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In recent years, speech-based self-supervised learning (SSL) has made significant progress in various tasks, including automatic speech recognition (ASR). An ASR model with decent performance can be realized by fine-tuning an SSL model with a small fraction of labeled data. Reducing the demand for labeled data is always of great practical value. In this paper, we further extend the use of SSL to cut down labeling costs with active learning. Three types of units on different granularities are derived from speech signals in an unsupervised way, and their effects are compared by applying a contrastive data selection method. The experimental results show that our proposed data selection framework can effectively improve the word error rate (WER) by more than 11% with the same amount of labeled data, or halve the labeling cost while maintaining the same WER, compared to random selection",
    "checked": true,
    "id": "2719ea7f7a69c553c2597bf3bc5794388702a674",
    "semantic_title": "unsupervised active learning: optimizing labeling cost-effectiveness for automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23_interspeech.html": {
    "title": "4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders",
    "volume": "main",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) can be classified into several models, including connectionist temporal classification (CTC), recurrent neural network transducer (RNN-T), attention mechanism, and mask-predict models. There are pros and cons to each of these architectures, and thus practitioners may switch between these different models depending on application requirements. Instead of building separate models, we propose a joint modeling scheme where four different decoders (CTC, attention, RNN-T, mask-predict) share an encoder - we refer to this as 4D modeling. Additionally, we propose to 1) train 4D models using a two-stage strategy which stabilizes multitask learning and 2) decode 4D models using a novel time-synchronous one-pass beam search. We demonstrate that jointly trained 4D models improve the performances of each individual decoder. Further, we show that our joint CTC/RNN-T/attention decoding surpasses the previously proposed CTC/attention decoding",
    "checked": true,
    "id": "6fb5a73e498845945080916f7b24c045b010d9dd",
    "semantic_title": "4d asr: joint modeling of ctc, attention, transducer, and mask-predict decoders",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yen23_interspeech.html": {
    "title": "Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition",
    "volume": "main",
    "abstract": "We propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR) and build an AR-SCR system. The AR procedure aims at repurposing a pretrained SCR model (from the source domain) to modify the acoustic signals (from the target domain). To solve the label mismatches between source and target domains and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained acoustic model trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Lithuanian and Arabic datasets, with only a limited amount of training data",
    "checked": true,
    "id": "1386eba753466911aa586a0d672f3990c58a54da",
    "semantic_title": "neural model reprogramming with similarity based mapping for low-resource spoken command recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fan23_interspeech.html": {
    "title": "Language-specific Boundary Learning for Improving Mandarin-English Code-switching Speech Recognition",
    "volume": "main",
    "abstract": "Code-switching speech recognition (CSSR) transcribes speech that switches between multiple languages or dialects within a single sentence. The main challenge in this task is that different languages often have similar pronunciations, making it difficult for models to distinguish between them. In this paper, we propose a method for solving the CSSR task from the perspective of language-specific acoustic boundary learning. We introduce language-specific weight estimators (LSWE) to model acoustic boundary learning in different languages separately. Additionally, a non-autoregressive (NAR) decoder and a language change detection (LCD) module are employed to assist in training. Evaluated on the SEAME corpus, our method achieves a state-of-the-art mixed error rate (MER) of 16.29% and 22.81% on the testman and testsge sets. We also demonstrate the effectiveness of our method on a 9000-hour in-house meeting code-switching dataset, where our method achieves a relatively 7.9% MER reduction",
    "checked": true,
    "id": "997ec959e8619112f19612ac894c1727e0f3e012",
    "semantic_title": "language-specific boundary learning for improving mandarin-english code-switching speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23c_interspeech.html": {
    "title": "Mixture-of-Expert Conformer for Streaming Multilingual ASR",
    "volume": "main",
    "abstract": "End-to-end models with large capacity have significantly improved multilingual automatic speech recognition, but their computation cost poses challenges for on-device applications. We propose a streaming truly multilingual Conformer incorporating mixture-of-expert (MoE) layers that learn to only activate a subset of parameters in training and inference. The MoE layer consists of a softmax gate which chooses the best two experts among many in forward propagation. The proposed MoE layer offers efficient inference by activating a fixed number of parameters as the number of experts increases. We evaluate the proposed model on a set of 12 languages, and achieve an average 11.9% relative improvement in WER over the baseline. Compared to an adapter model using ground truth information, our MoE model achieves similar WER and activates similar number of parameters but without any language information. We further show around 3% relative WER improvement by multilingual shallow fusion",
    "checked": true,
    "id": "f27c4fc3e68b2477fa1ca74988f43aa0d9613139",
    "semantic_title": "mixture-of-expert conformer for streaming multilingual asr",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23x_interspeech.html": {
    "title": "Lossless 4-bit Quantization of Architecture Compressed Conformer ASR Systems on the 300-hr Switchboard Corpus",
    "volume": "main",
    "abstract": "State-of-the-art end-to-end automatic speech recognition (ASR) systems are becoming increasingly complex and expensive for practical applications. This paper develops a high-performance and low-footprint 4-bit quantized Conformer ASR system. A key feature of the system design is to account for the fine-grained, varying performance sensitivity at different Conformer components to quantization errors. Neural architectural compression and mixed precision quantization approaches were used to auto-configure the optimal substructures and quantization bit-widths within each Conformer submodule. Experiments conducted on the 300-hr Switchboard data suggest that the obtained auto-configured systems consistently outperform the uniform precision quantized baseline Conformer of comparable bit-widths in terms of word error rate (WER). An overall \"lossless\" compression ratio of 16.2 times was obtained over the 32-bit full-precision baseline while incurring no statistically significant WER increase",
    "checked": true,
    "id": "0d24dc9f34b90875da254db8bab2e19fecd4f2e0",
    "semantic_title": "lossless 4-bit quantization of architecture compressed conformer asr systems on the 300-hr switchboard corpus",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23c_interspeech.html": {
    "title": "Compressed MoE ASR Model Based on Knowledge Distillation and Quantization",
    "volume": "main",
    "abstract": "The mixture of experts (MoE)-based automatic speech recognition (ASR) model can achieve remarkable performance, but pose greater challenges to model deployment for its huge model size. Therefore, it is important to compress the model size and reduce the computational cost. In this paper, we propose a compressed MoE (CMoE) ASR model that simplifies the MoE structure by knowledge distillation and reduces parameter bit-width through quantization, and provide two pipelines (one-stage and two-stage pipelines) to deploy the compression. In quantization, we use binary weight network to quantize the weights to 1-bit for reducing the quantization error and use learned step size quantization to quantize the activations to 4-bit. Experimental results show that the quantized dense network compressed from the MoE based ASR model by our method reduces the size by 150x with very small accuracy loss. The proposed model is expected to be deployed on embedded devices",
    "checked": true,
    "id": "bbd6b5d6150fd7538eadd29eaafcb6173bd31e93",
    "semantic_title": "compressed moe asr model based on knowledge distillation and quantization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deng23b_interspeech.html": {
    "title": "Factorised Speaker-environment Adaptive Training of Conformer Speech Recognition Systems",
    "volume": "main",
    "abstract": "Rich sources of variability in natural speech present significant challenges to current data intensive speech recognition technologies. To model both speaker and environment level diversity, this paper proposes a novel Bayesian factorised speaker-environment adaptive training and test time adaptation approach for Conformer ASR models. Speaker and environment level characteristics are separately modeled using compact hidden output transforms, which are then linearly or hierarchically combined to represent any speaker-environment combination. Bayesian learning is further utilized to model the adaptation parameter uncertainty. Experiments on the 300-hr WHAM noise corrupted Switchboard data suggest that factorised adaptation consistently outperforms the baseline and speaker label only adapted Conformers by up to 3.1% absolute (10.4% relative) word error rate reductions. Further analysis shows the proposed method offers potential for rapid adaption to unseen speaker-environment conditions",
    "checked": true,
    "id": "ad85c0e1ac7db070ca363d21cc904a99c0c72c06",
    "semantic_title": "factorised speaker-environment adaptive training of conformer speech recognition systems",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23aa_interspeech.html": {
    "title": "Text Only Domain Adaptation with Phoneme Guided Data Splicing for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Adaptation of end-to-end (E2E) automatic speech recognition (ASR) models to unseen domains remains a challenge due to their monolithic construction, which typically necessitates paired data for customization. While neural text-to-speech (TTS) approaches have shown effectiveness for domain adaptation, they come with the drawback of high computational costs during training and inference. In this paper, we propose a model-free audio synthesis pipeline for domain adaptation, which synthesizes audio with text from the target domain and audio pieces from the source domain, allowing ASR models to be adapted with the on-the-fly synthesized audio. Additionally, we apply layer-wise regularization between speech encodings generated by adapted and unadapted models to prevent overfitting. Our experiments adapt from LIBRI SPEECH to various domains in GIGA SPEECH. The results show a 15-30% relative improvement in target domains compared to shallow fusion, with almost no degradation in the source domain",
    "checked": true,
    "id": "28ffc23a5ab97309640d7fc47aa24f714c7a3348",
    "semantic_title": "text only domain adaptation with phoneme guided data splicing for end-to-end speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cahyawijaya23_interspeech.html": {
    "title": "Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition plays a crucial role in human-computer interactions. However, most speech emotion recognition research is biased toward English-speaking adults, which hinders its applicability to other demographic groups in different languages and age groups. In this work, we analyze the transferability of emotion recognition across three different languages--English, Mandarin Chinese, and Cantonese; and 2 different age groups--adults and the elderly. To conduct the experiment, we develop an English-Mandarin speech emotion benchmark for adults and the elderly, BiMotion, and a Cantonese speech emotion dataset, YueMotion. This study concludes that different language and age groups require specific speech features, thus making cross-lingual inference an unsuitable method. However, cross-group data augmentation is still beneficial to regularize the model, with linguistic distance being a significant influence on cross-lingual transferability",
    "checked": true,
    "id": "5c112d7afee764a1466755417aa02e11bbec7c9c",
    "semantic_title": "cross-lingual cross-age adaptation for low-resource elderly speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23fa_interspeech.html": {
    "title": "Modular Domain Adaptation for Conformer-Based Streaming ASR",
    "volume": "main",
    "abstract": "Speech data from different domains has distinct acoustic and linguistic characteristics. It is common to train a single multidomain model such as a Conformer transducer for speech recognition on a mixture of data from all domains. However, changing data in one domain or adding a new domain would require the multidomain model to be retrained. To this end, we propose a framework called modular domain adaptation (MDA) that enables a single model to process multidomain data while keeping all parameters domain-specific, i.e., each parameter is only trained by data from one domain. On a streaming Conformer transducer trained only on video caption data, experimental results show that an MDA-based model can reach similar performance as the multidomain model on other domains such as voice search and dictation by adding per-domain adapters and per-domain feed-forward networks in the Conformer encoder",
    "checked": true,
    "id": "675c58390b58ee4fb0ac154c40e8586ffd617e4c",
    "semantic_title": "modular domain adaptation for conformer-based streaming asr",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhatia23_interspeech.html": {
    "title": "Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters",
    "volume": "main",
    "abstract": "Speech representations learned in a self-supervised fashion from massive unlabeled speech corpora have been adapted successfully toward several downstream tasks. However, such representations may be skewed toward canonical data characteristics of such corpora and perform poorly on atypical, non-native accented speaker populations. With the state-of-the-art HuBERT model as a baseline, we propose and investigate self-supervised adaptation of speech representations to such populations in a parameter-efficient way via training accent-specific residual adapters. We experiment with 4 accents and choose automatic speech recognition (ASR) as the downstream task of interest. We obtain strong word error rate reductions (WERR) over HuBERT-large for all 4 accents, with a mean WERR of 22.7% with accent-specific adapters and a mean WERR of 25.1% if the entire encoder is accent-adapted. While our experiments utilize HuBERT and ASR as the downstream task, our proposed approach is both model and task-agnostic",
    "checked": true,
    "id": "b8478841dddd0febd8b13a0c685f5ebea41fdbb5",
    "semantic_title": "don't stop self-supervision: accent adaptation of speech representations via residual adapters",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23f_interspeech.html": {
    "title": "SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts",
    "checked": true,
    "id": "b2d71f05c64a6a55b65073931090dc3fa1de0a2e",
    "semantic_title": "sgem: test-time adaptation for automatic speech recognition via sequential-level generalized entropy minimization",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mori23_interspeech.html": {
    "title": "A Generative Framework for Conversational Laughter: Its 'Language Model' and Laughter Sound Synthesis",
    "volume": "main",
    "abstract": "As the phonetic and acoustic manifestations of laughter in conversation are highly diverse, laughter synthesis should be capable of accommodating such diversity while maintaining high controllability. This paper proposes a generative model of laughter in conversation that can produce a wide variety of laughter by utilizing the emotion dimension as a conversational context. The model comprises two parts: the laughter \"phones generator,\" which generates various, but realistic, combinations of laughter components for a given speaker ID and emotional state, and the laughter \"sound synthesizer,\" which receives the laughter phone sequence and produces acoustic features that reflect the speaker's individuality and emotional state. The results of a listening experiment indicated that conditioning both the phones generator and the sound synthesizer on emotion dimensions resulted in the most effective control of the perceived emotion in synthesized laughter",
    "checked": true,
    "id": "a052d47d5b22682c19169104b97c9bafafead287",
    "semantic_title": "a generative framework for conversational laughter: its 'language model' and laughter sound synthesis",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ba_interspeech.html": {
    "title": "Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "The spontaneous behavior that often occurs in conversations makes speech more human-like compared to reading-style. However, synthesizing spontaneous-style speech is challenging due to the lack of high-quality spontaneous datasets and the high cost of labeling spontaneous behavior. In this paper, we propose a semi-supervised pre-training method to increase the amount of spontaneous-style speech and spontaneous behavioral labels. In the process of semi-supervised learning, both text and speech information are considered for detecting spontaneous behaviors labels in speech. Moreover, a linguistic-aware encoder is used to model the relationship between each sentence in the conversation. Experimental results indicate that our proposed method achieves superior expressive speech synthesis performance with the ability to model spontaneous behavior in spontaneous-style speech and predict reasonable spontaneous behavior from text",
    "checked": true,
    "id": "a1fac1c5fe5e09f8e5c0dd6acb314a373f766ba9",
    "semantic_title": "towards spontaneous style modeling with semi-supervised pre-training for conversational text-to-speech synthesis",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lameris23_interspeech.html": {
    "title": "Beyond Style: Synthesizing Speech with Pragmatic Functions",
    "volume": "main",
    "abstract": "With recent advances in generative modelling, conversational systems are becoming more lifelike and capable of long, nuanced interactions. Text-to-Speech (TTS) is being tested in territories requiring natural-sounding speech that can mimic the complexities of human conversation. Hyper-realistic speech generation has been achieved, but a gap remains between the verbal behavior required for upscaled conversation, such as paralinguistic information and pragmatic functions, and comprehension of the acoustic prosodic correlates underlying these. Without this knowledge, reproducing these functions in speech has little value. We use prosodic correlates including spectral peaks, spectral tilt, and creak percentage for speech synthesis with the pragmatic functions of small talk, self-directed speech, advice, and instructions. We perform a MOS evaluation, and a suitability experiment in which our system outperforms a read-speech and conversational baseline",
    "checked": true,
    "id": "ffe8bc7ea78838e8f35b2bc518f33037374ae0ef",
    "semantic_title": "beyond style: synthesizing speech with pragmatic functions",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/abbas23_interspeech.html": {
    "title": "eCat: An End-to-End Model for Multi-Speaker TTS & Many-to-Many Fine-Grained Prosody Transfer",
    "volume": "main",
    "abstract": "We present eCat, a novel end-to-end multispeaker model capable of: a) generating long-context speech with expressive and contextually appropriate prosody, and b) performing fine-grained prosody transfer between any pair of seen speakers. eCat is trained using a two-stage training approach. In Stage I, the model learns speaker-independent word-level prosody representations in an end-to-end fashion from speech. In Stage II, we learn to predict the prosody representations using the contextual information available in text. We compare eCat to CopyCat2, a model capable of both fine-grained prosody transfer (FPT) and multi-speaker TTS. We show that eCat statistically significantly reduces the gap in naturalness between CopyCat2 and human recordings by an average of 46.7% across 2 languages, 3 locales, and 7 speakers, along with better target-speaker similarity in FPT. We also compare eCat to VITS, and show a statistically significant preference",
    "checked": true,
    "id": "3590cf6ae100683c0fc9f56b8d0869017f0adab7",
    "semantic_title": "ecat: an end-to-end model for multi-speaker tts & many-to-many fine-grained prosody transfer",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deb23_interspeech.html": {
    "title": "BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion",
    "volume": "main",
    "abstract": "Spoken languages often utilise intonation, rhythm, intensity, and structure, to communicate intention, which can be interpreted differently depending on the rhythm of speech of their utterance. These speech acts provide the foundation of communication and are unique in expression to the language. Recent advancements in attention-based models, demonstrating their ability to learn powerful representations from multilingual datasets, have performed well in speech tasks and are ideal to model specific tasks in low resource languages. Here, we develop a novel multimodal approach combining two models, wav2vec2.0 for audio and MarianMT for text translation, by using multimodal attention fusion to predict speech acts in our prepared Bengali speech corpus. We also show that our model BeAts (Bengali speech acts recognition using Multimodal Attention Fusion) significantly outperforms both the unimodal baseline using only speech data and a simpler bimodal fusion using both speech and text data",
    "checked": true,
    "id": "2dcf860da0d7c8ae7eebdecd58fde999d8883fb1",
    "semantic_title": "beats: bengali speech acts recognition using multimodal attention fusion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kashiwagi23_interspeech.html": {
    "title": "Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning",
    "volume": "main",
    "abstract": "This paper presents a novel metric learning approach to address the performance gap between normal and silent speech in visual speech recognition (VSR). The difference in lip movements between the two poses a challenge for existing VSR models, which exhibit degraded accuracy when applied to silent speech. To solve this issue and tackle the scarcity of training data for silent speech, we propose to leverage the shared literal content between normal and silent speech and present a metric learning approach based on visemes. Specifically, we aim to map the input of two speech types close to each other in a latent space if they have similar viseme representations. By minimizing the Kullback-Leibler divergence of the predicted viseme probability distributions between and within the two speech types, our model effectively learns and predicts viseme identities. Our evaluation demonstrates that our method improves the accuracy of silent VSR, even when limited training data is available",
    "checked": true,
    "id": "13747244b5436dd4bb39286c93ab5a5ab58aa03f",
    "semantic_title": "improving the gap in visual speech recognition between normal and silent speech based on metric learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jakubiak23_interspeech.html": {
    "title": "Whistle-to-text: Automatic recognition of the Silbo Gomero whistled language",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) is a rapidly developing field of study. However, ASR for other types of speech than the regular spoken speech-for example, whispering or shouting-remains difficult, as it requires specific models trained to recognise these types of speech. A lesser-known type of speech than those is the whistled speech, in which speech is transformed into whistling. In this paper, I will describe how I created the first-ever ASR model designed to recognise a whistled language. It was trained, using the HMM-GMM approach to ASR, to recognise the whistled dialect of Spanish, Silbo Gomero. This model learned to recognise Silbo Gomero, though its performance was somewhat worse than that of spoken speech recognition models trained on data sets of similar size. It appears that methods used to create spoken language ASR models can be used to create whistled language ASR models, with only small changes-which will be explained in this paper-required",
    "checked": true,
    "id": "97bf185af43bda52daa039ae1c85675316d8c07d",
    "semantic_title": "whistle-to-text: automatic recognition of the silbo gomero whistled language",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23c_interspeech.html": {
    "title": "A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus",
    "volume": "main",
    "abstract": "Cued Speech (CS) is a multi-modal visual coding system combining lip reading with several hand cues at the phonetic level to make the spoken language visible to the hearing impaired. Previous studies solved asynchronous problems between lip and hand movements by a cuer-dependent piecewise linear model for English and French CS. In this work, we innovatively propose three statistical measure on the lip stream to build an interpretable and generalizable model for predicting hand preceding time (HPT), which achieves cuer-independent by a proper normalization. Particularly, we build the first Mandarin CS corpus comprising annotated videos from five speakers including three normal and two hearing impaired individuals. Consequently, we show that the hand preceding phenomenon exists in Mandarin CS production with significant differences between normal and hearing impaired people. Extensive experiments demonstrate that our model outperforms the baseline and the previous state-of-the-art methods",
    "checked": true,
    "id": "a6d33b367403b24b4f327278c2cb4d8f697752b3",
    "semantic_title": "a novel interpretable and generalizable re-synchronization model for cued speech based on a multi-cuer corpus",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nortje23_interspeech.html": {
    "title": "Visually grounded few-shot word acquisition with fewer shots",
    "volume": "main",
    "abstract": "We propose a visually grounded speech model that acquires new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelled speech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new model, we achieve better performance with fewer shots than any existing approach",
    "checked": true,
    "id": "b33a042767addf736de16fe0e5166e470096075f",
    "semantic_title": "visually grounded few-shot word acquisition with fewer shots",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23_interspeech.html": {
    "title": "JAMFN: Joint Attention Multi-Scale Fusion Network for Depression Detection",
    "volume": "main",
    "abstract": "Recently, with the widespread popularity of the Internet, social networks have become an indispensable part of people's lives. As social networks contain information about users' daily moods and states, their development provides a new avenue for detecting depression. Although most current approaches focus on the fusion of multimodal features, the importance of fine-grained behavioral information is ignored. In this paper, we propose the Joint Attention Multi-Scale Fusion Network (JAMFN), a model that reflects the multiscale behavioral information of depression and leverages the proposed Joint Attention Fusion (JAF) module to extract the temporal importance of multiple modalities to guide the fusion of multiscale modal pairs. Our experiment is conducted on D-vlog dataset, and the experimental results demonstrate that the proposed JAMFN model outperforms all the benchmark models, indicating that our proposed JAMFN model can effectively mine the potential depressive behavior",
    "checked": true,
    "id": "fe50157627e3564d221a9cdbbb857c3cbfd37b68",
    "semantic_title": "jamfn: joint attention multi-scale fusion network for depression detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23z_interspeech.html": {
    "title": "Prompt Guided Copy Mechanism for Conversational Question Answering",
    "volume": "main",
    "abstract": "Conversational Question Answering (CQA) is a challenging task that aims to generate natural answers for conversational flow questions. In this paper, we propose a pluggable approach for extractive methods that introduces a novel prompt-guided copy mechanism to improve the fluency and appropriateness of the extracted answers. Our approach uses prompts to link questions to answers and employs attention to guide the copy mechanism to verify the naturalness of extracted answers, making necessary edits to ensure that the answers are fluent and appropriate. The three prompts, including a question-rationale relationship prompt, a question description prompt, and a conversation history prompt, enhance the copy mechanism's performance. Our experiments demonstrate that this approach effectively promotes the generation of natural answers and achieves good results in the CoQA challenge",
    "checked": true,
    "id": "e5275eb9f4404e0acd7e89c2b240af7a050b9dc8",
    "semantic_title": "prompt guided copy mechanism for conversational question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/faustini23_interspeech.html": {
    "title": "Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants",
    "volume": "main",
    "abstract": "The adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users instant access to information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement in voice-based settings. To enable this, we tackle the novel task of suggesting uestions with compact and natural voice hints to allow users to ask follow-up questions. We first define the task of composing speech-based hints, ground it in syntactic theory, and outline linguistic desiderata for spoken hints. We propose a sequence-to-sequence approach to generate spoken hints from a list of questions. Using a new dataset of 6, 681 input questions and human written hints, we evaluate models with automatic metrics and human evaluation. Results show that a naive approach of concatenating suggested questions creates poor voice hints. Our most sophisticated approach applies a linguistically-motivated pretraining task and was strongly preferred by humans for producing the most natural hints",
    "checked": true,
    "id": "9aa8da79117e73e3c55f79be1236d190af9592bb",
    "semantic_title": "composing spoken hints for follow-on question suggestion in voice assistants",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23c_interspeech.html": {
    "title": "On Monotonic Aggregation for Open-domain QA",
    "volume": "main",
    "abstract": "Question answering (QA) is a critical task for speech-based retrieval from knowledge sources, by sifting only the answers without requiring to read supporting documents. Specifically, open-domain QA aims to answer user questions on unrestricted knowledge sources. Ideally, adding a source should not decrease the accuracy, but we find this property (denoted as \"monotonicity\") does not hold for current state-of-the-art methods. We identify the cause, and based on that we propose Judge-Specialist framework. Our framework consists of (1) specialist retrievers/readers to cover individual sources, and (2) judge, a dedicated language model to select the final answer. Our experiments show that our framework not only ensures monotonicity, but also outperforms state-of-the-art multi-source QA methods on Natural Questions. Additionally, we show that our models robustly preserve the monotonicity against noise from speech recognition. We publicly release our code and setting",
    "checked": true,
    "id": "f04bc23e086894262a8327887880ade568c1c42a",
    "semantic_title": "on monotonic aggregation for open-domain qa",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nguyen23b_interspeech.html": {
    "title": "Question-Context Alignment and Answer-Context Dependencies for Effective Answer Sentence Selection",
    "volume": "main",
    "abstract": "Answer sentence selection (AS2) in open-domain question answering finds answer for a question by ranking candidate sentences extracted from web documents. Recent work exploits answer context, i.e., sentences around a candidate, by incorporating them as additional input string to the Transformer models to improve the correctness scoring. In this paper, we propose to improve the candidate scoring by explicitly incorporating the dependencies between question-context and answer-context into the final representation of a candidate. Specifically, we use Optimal Transport to compute the question-based dependencies among sentences in the passage where the answer is extracted from. We then represent these dependencies as edges in a graph and use Graph Convolutional Network to derive the representation of a candidate, a node in the graph. Our proposed model achieves significant improvements on popular AS2 benchmarks, i.e., WikiQA and WDRASS, obtaining new state-of-the-art on all benchmarks",
    "checked": true,
    "id": "a9a0036c5e7a249bd9ccf983313cee25fac7caf1",
    "semantic_title": "question-context alignment and answer-context dependencies for effective answer sentence selection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23v_interspeech.html": {
    "title": "Multi-Scale Attention for Audio Question Answering",
    "volume": "main",
    "abstract": "Audio question answering (AQA), acting as a widely used proxy task to explore scene understanding, has got more attention. The AQA is challenging for it requires comprehensive temporal reasoning from different scales' events of an audio scene. However, existing methods mostly extend the structures of visual question answering task to audio ones in a simple pattern but may not perform well when perceiving a finegrained audio scene. To this end, we present a Multi-scale Window Attention Fusion Model (MWAFM) consisting of an asynchronous hybrid attention module and a multi-scale window attention module. The former is designed to aggregate unimodal and cross-modal temporal contexts, while the latter captures sound events of varying lengths and their temporal dependencies for a more comprehensive understanding. Extensive experiments are conducted to demonstrate that the proposed MWAFM can effectively explore temporal information to facilitate AQA in the fine-grained scene",
    "checked": true,
    "id": "309074a33c9a96bdcdde6bf8ad35fc03e716a2cd",
    "semantic_title": "multi-scale attention for audio question answering",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23f_interspeech.html": {
    "title": "Enhancing Visual Question Answering via Deconstructing Questions and Explicating Answers",
    "volume": "main",
    "abstract": "A compositional question refers to a question that involves multiple visual objects, as well as their attributes and relationships, which requires compositional reasoning to answer. Existing VQA models can well answer a compositional question, but few works can give the reasoning process and explain why this answer is given. In this paper, we propose a novel model (DEEX) to enhance visual question answering via DEconstructing questions and EXplicating answers when answering compositional questions. Specifically, DEEX aims to accomplish three sub-tasks: (1) Compositional Question Answering (CQA), (2) Question Deconstructing (QD), and (3) Answer Explicating (AE). We utilize prompt-based multi-task learning to train the proposed DEEX to be able to answer questions and give explanations simultaneously. Experimental results on the GQA dataset demonstrate our method's effectiveness, which can enhance visual question answering by giving corresponding reasoning processes and explanations",
    "checked": true,
    "id": "2075effd2f498456d871664d06362ff615878ec1",
    "semantic_title": "enhancing visual question answering via deconstructing questions and explicating answers",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23c_interspeech.html": {
    "title": "SEF-Net: Speaker Embedding Free Target Speaker Extraction Network",
    "volume": "main",
    "abstract": "Most target speaker extraction methods use the target speaker embedding as reference information. However, the speaker embedding extracted by a speaker recognition module may not be optimal for the target speaker extraction tasks. In this paper, we proposes Speaker Embedding Free target speaker extraction Network (SEF-Net), a novel target speaker extraction model without relying on speaker embedding. SEF-Net uses cross multi-head attention in the transformer decoder to implicitly utilize the speaker information in the reference speech's conformer encoding outputs. Experimental results show that our proposed model achieves comparable performance to other target speaker extraction models. SEF-Net provides a feasible new solution to perform target speaker extraction without using a speaker embedding extractor or speaker recognition loss function",
    "checked": true,
    "id": "f087b3c1db5c0e853d12df002c659e8fce3818f0",
    "semantic_title": "sef-net: speaker embedding free target speaker extraction network",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rose23_interspeech.html": {
    "title": "Cascaded encoders for fine-tuning ASR models on overlapped speech",
    "volume": "main",
    "abstract": "Multi-talker automatic speech recognition (MT-ASR) has been shown to improve ASR performance on speech containing overlapping utterances from more than one speaker. While MT-ASR models have typically been trained from scratch using simulated overlapping speech datasets, there is generally an underlying goal that these models also obtain state of the art performance on single speaker utterances as well. This implies that they must be competitive with the best available fine-tuned speech models that have been trained using massive datasets collected from a wide variety of task domains. This paper presents an MT-ASR model formed by combining a well-trained foundation model with a multi-talker mask model in a cascaded RNN-T encoder configuration. Experimental results show that the cascade configuration provides improved WER on overlapping speech utterances with respect to a baseline multi-talker model without sacrificing the performance achievable by the foundation model on non-overlapping utterances",
    "checked": true,
    "id": "1e5301da613db7cff7ad98d17eedf9a05db4d31a",
    "semantic_title": "cascaded encoders for fine-tuning asr models on overlapped speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/erdogan23_interspeech.html": {
    "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
    "volume": "main",
    "abstract": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model",
    "checked": true,
    "id": "b7bd040a9d9165efe6fc213382a0d0f97128331a",
    "semantic_title": "tokensplit: using discrete speech representations for direct, refined, and transcript-conditioned speech separation and recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23b_interspeech.html": {
    "title": "Unified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator",
    "volume": "main",
    "abstract": "Multi-talker overlapped speech poses a significant challenge for speech recognition and diarization. Recent research indicated that these two tasks are inter-dependent and complementary, motivating us to explore a unified modeling method to address them in the context of overlapped speech. A recent study proposed a cost-effective method to convert a single-talker automatic speech recognition (ASR) system into a multi-talker one, by inserting a Sidecar separator into the frozen well-trained ASR model. Extending on this, we incorporate a diarization branch into the Sidecar, allowing for unified modeling of both ASR and diarization with a negligible overhead of only 768 parameters. The proposed method yields better ASR results compared to the baseline on LibriMix and LibriSpeechMix datasets. Moreover, without sophisticated customization on the diarization task, our method achieves acceptable diarization results on the two-speaker subset of CALLHOME with only a few adaptation steps",
    "checked": true,
    "id": "1d3f808908489fdeccd55ae405cf24e7c225f222",
    "semantic_title": "unified modeling of multi-talker overlapped speech recognition and diarization with a sidecar separator",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahmadikalkhorani23_interspeech.html": {
    "title": "Time-domain Transformer-based Audiovisual Speaker Separation",
    "volume": "main",
    "abstract": "In this study, we propose a transformer-based architecture for talker-independent audiovisual speaker separation in the time-domain. Inputs to the proposed architecture are the noisy mixtures of multiple talkers and their corresponding cropped faces. Using a cross-attention mechanism, these two streams are fused together. The fusion layer is followed by a masking net that estimates one mask per talker and multiplies the mixed feature matrix by these masks to separate speaker features. Finally, the separated features are converted to the time domain at the decoder layer. Moreover, we propose a novel training strategy to increase the role of the video stream which starts with a relatively noisy condition and gradually increases audio stream quality during training. Experimental results demonstrate that the proposed method outperforms existing techniques according to multiple metrics on several commonly used audiovisual datasets",
    "checked": true,
    "id": "0b520898002cff988db3353ea7d53842a2b27108",
    "semantic_title": "time-domain transformer-based audiovisual speaker separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/delcroix23_interspeech.html": {
    "title": "Multi-Stream Extension of Variational Bayesian HMM Clustering (MS-VBx) for Combined End-to-End and Vector Clustering-based Diarization",
    "volume": "main",
    "abstract": "Combining end-to-end neural speaker diarization (EEND) with vector clustering (VC), known as EEND-VC, has gained interest for leveraging the strengths of both methods. EEND-VC estimates activities and speaker embeddings for all speakers within an audio chunk and uses VC to associate these activities with speaker identities across different chunks. EEND-VC generates thus multiple streams of embeddings, one for each speaker in a chunk. We can cluster these embeddings using constrained agglomerative hierarchical clustering (cAHC), ensuring embeddings from the same chunk belong to different clusters. This paper introduces an alternative clustering approach, a multi-stream extension of the successful Bayesian HMM clustering of x-vectors (VBx), called MS-VBx. Experiments on three datasets demonstrate that MS-VBx outperforms cAHC in diarization and speaker counting performance",
    "checked": true,
    "id": "f3381ba845714c9effe316bb1c3285911a6c7934",
    "semantic_title": "multi-stream extension of variational bayesian hmm clustering (ms-vbx) for combined end-to-end and vector clustering-based diarization",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niu23_interspeech.html": {
    "title": "Unsupervised Adaptation with Quality-Aware Masking to Improve Target-Speaker Voice Activity Detection for Speaker Diarization",
    "volume": "main",
    "abstract": "We propose an unsupervised adaptation approach to improve target-speaker voice activity detection (TS-VAD) in speaker diarization (SD) based on quality-aware masking (QM) in order to reduce potential errors in the generated pseudo-labels. Furthermore, the QM-TS-VAD adapted model can be used as a teacher model to fine-tune a student SD model through knowledge distillation (KD) to further mitigate the over-fitting issue. Evaluated on the eight different domains in the DIHARD-III evaluation corpus, our experimental results show that the proposed QM-TS-VAD approach effectively enhances SD performances, and the introduced KD method can further reduce errors in seven of the eight domains. Finally, the proposed framework outperforms the unsupervised adaptation approach in the top-ranked system submitted to the DIHARD-III Challenge",
    "checked": true,
    "id": "9e2d9aad080d4f8c52c922629fe7fd450408ed99",
    "semantic_title": "unsupervised adaptation with quality-aware masking to improve target-speaker voice activity detection for speaker diarization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23e_interspeech.html": {
    "title": "BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR",
    "volume": "main",
    "abstract": "The recently proposed serialized output training (SOT) simplifies multi-talker automatic speech recognition (ASR) by generating speaker transcriptions separated by a special token. However, frequent speaker changes can make speaker change prediction difficult. To address this, we propose boundary-aware serialized output training (BA-SOT), which explicitly incorporates boundary knowledge into the decoder via a speaker change detection task and boundary constraint loss. We also introduce a two-stage connectionist temporal classification (CTC) strategy that incorporates token-level SOT CTC to restore temporal context information. Besides typical character error rate (CER), we introduce utterance-dependent character error rate (UD-CER) to further measure the precision of speaker change prediction. Compared to original SOT, BA-SOT reduces CER/UD-CER by 5.1%/14.0%, and leveraging a pre-trained ASR model for BA-SOT model initialization further reduces CER/UD-CER by 8.4%/19.9%",
    "checked": true,
    "id": "ba7f05e8169ba2751aa86441b370f8b8cf0aa76d",
    "semantic_title": "ba-sot: boundary-aware serialized output training for multi-talker asr",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23e_interspeech.html": {
    "title": "Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation",
    "volume": "main",
    "abstract": "In supervised speech separation, permutation invariant training (PIT) is widely used to handle label ambiguity by selecting the best permutation to update the model. Despite its success, previous studies showed that PIT is plagued by excessive label assignment switching in adjacent epochs, impeding the model to learn better label assignments. To address this issue, we propose a novel training strategy, dynamic sample dropout (DSD), which considers previous best label assignments and evaluation metrics to exclude the samples that may negatively impact the learned label assignments during training. Additionally, we include layer-wise optimization (LO) to improve the performance by solving layer-decoupling. Our experiments showed that combining DSD and LO outperforms the baseline and solves excessive label assignment switching and layer-decoupling issues. The proposed DSD and LO approach is easy to implement, requires no extra training sets or steps, and shows generality to various speech separation tasks",
    "checked": true,
    "id": "0b2aaa3466ef21eb8ea65eaabaa1c02e7a6d89d4",
    "semantic_title": "improving label assignments learning by dynamic sample dropout combined with layer-wise optimization in speech separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gaultier23_interspeech.html": {
    "title": "Joint compensation of multi-talker noise and reverberation for speech enhancement with cochlear implants using one or more microphones",
    "volume": "main",
    "abstract": "Following speech in noisy and reverberant situations is difficult for cochlear implant (CI) users. This study investigates single- and multi-microphone deep neural network (DNN) speech enhancement algorithms on the joint task of denoising and dereverberation. The DNN algorithms were trained and tested on simulated sound scenes from behind-the-ear hearing devices. Performance was assessed using objective measures and a listening study for reverberant mixtures of speech in multi-talker babble noise. We compare results for signal distortion, predicted intelligibility and speech reception thresholds measured in a listening experiment with 15 typically hearing participants using cochlear implant simulations. Objective metrics indicated listening benefits for both single- and multi-microphone approaches while the listening study results confirmed significant improvements in speech intelligibility for the multi-microphone approaches, holding strong promise to benefit CI listeners",
    "checked": true,
    "id": "3cec569eff56d655b49a87bab52b2887e58af676",
    "semantic_title": "joint compensation of multi-talker noise and reverberation for speech enhancement with cochlear implants using one or more microphones",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yousefi23_interspeech.html": {
    "title": "Speaker Diarization for ASR Output with T-vectors: A Sequence Classification Approach",
    "volume": "main",
    "abstract": "This paper considers applying speaker diarization (SD) to the output tokens of automatic speech recognition (ASR). We formulate the task to be solved as a sequence classification problem, where we estimate the correct speaker label for each ASR output token based on a sequence of token-level speaker embeddings and candidate speaker profiles. To leverage the information from the ASR model, we utilize a recently proposed t-vector for the speaker embedding estimation. Whereas previous studies performed t-vector classification using cosine similarities with ad hoc post-processing, we propose to use a sequence classification model to leverage the sequential nature of the task more effectively. To handle a variable number of speakers, we use a classification model inspired by a target speaker voice activity detection based on transformers. We conduct experiments using the AMI meeting corpus in both speaker identification and diarization settings and show the effectiveness of our approach",
    "checked": true,
    "id": "9fb16253791bcd69f907f782c88a55a2a7bb7a6d",
    "semantic_title": "speaker diarization for asr output with t-vectors: a sequence classification approach",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raj23_interspeech.html": {
    "title": "GPU-accelerated Guided Source Separation for Meeting Transcription",
    "volume": "main",
    "abstract": "Guided source separation (GSS) is a target-speaker extraction method that uses pre-computed speaker activities and blind source separation to perform front-end enhancement of overlapped speech signals. First proposed during the CHiME-5 challenge, it provided significant improvements over the delay-and-sum beamforming baseline. Despite its strengths, the method has seen limited adoption for meeting transcription benchmarks primarily due to its high computation time. In this paper, we describe our improved implementation of GSS that leverages the power of modern GPU-based pipelines, such as batched processing of frequencies and segments, to provide 300x speed-up over CPU-based inference. This allows us to perform detailed ablation studies over several parameters of the GSS algorithm -- context duration, number of channels, and noise class, to name a few. We provide reproducible pipelines for speaker-attributed transcription of popular meeting benchmarks: LibriCSS, AMI, and AliMeeting",
    "checked": true,
    "id": "ad9199a35c5ce4738c39bb4af8abf7caee418365",
    "semantic_title": "gpu-accelerated guided source separation for meeting transcription",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23c_interspeech.html": {
    "title": "Overlap Aware Continuous Speech Separation without Permutation Invariant Training",
    "volume": "main",
    "abstract": "Continuous speech separation (CSS) aims to separate a long-form signal with multiple partially overlapped utterances into a set of non-overlapped speech signals. While most existing CSS methods rely on the permutation invariant training (PIT) algorithm for training and inference, we argue that one may not need PIT at all to achieve promising CSS performance. In this paper, we propose a novel overlap aware CSS method, which explicitly identifies the non-overlapped segments in the long-form input to guide the separation of overlapped segments. We show that with the help of an external overlapping speech detection (OSD) model, an overlap-aware CSS model can be trained without PIT. In addition, an overlap-aware inference algorithm is proposed to greatly reduce the computational cost while preserving strong performance. Experiment results show that our proposed methods outperform the conventional stitching-based CSS approach, with over 1 dB signal-to-noise ratio (SNR) improvement",
    "checked": true,
    "id": "7c644f515560667de454614f4be0ba86e04b67b3",
    "semantic_title": "overlap aware continuous speech separation without permutation invariant training",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23w_interspeech.html": {
    "title": "Weakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) based speech pre-training has attracted much attention for its capability of extracting rich representations learned from massive unlabeled data. On the other hand, the use of weakly-supervised data is less explored for speech pre-training. To fill this gap, we propose a weakly-supervised speech pre-training method based on speaker-aware speech data. It adopts a similar training procedure to the widely-used masked speech prediction based SSL framework, while incorporating additional target-speaker enrollment information as an auxiliary input. In this way, the learned representation is steered towards the target speaker even in the presence of highly overlapping interference, allowing potential applications to tasks such as target speech recognition. Our experiments on Libri2Mix and WSJ0-2mix datasets show that the proposed model achieves significantly better ASR performance compared to WavLM, the state-of-the-art SSL model with denoising capability",
    "checked": true,
    "id": "6dfa3a8692284493a867c972e2a79f682fb56d66",
    "semantic_title": "weakly-supervised speech pre-training: a case study on target speech recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23j_interspeech.html": {
    "title": "Directional Speech Recognition for Speaker Disambiguation and Cross-talk Suppression",
    "volume": "main",
    "abstract": "With advances in mobile computing, smart glasses are becoming powerful enough to generate real-time closed captions of live conversations. Such system must distinguish speech from the conversation partner from the wearer's, and in public places it must not transcribe speech from unrelated bystanders to avoid confusion and to honor privacy. We propose an end-to-end modeling approach that leverages the smart glasses' microphone array. But we go beyond beamforming for improved target-speaker SNR: We feed multiple audio channels simultaneously to a single ASR model as a basis for speaker-attributed transcription and suppression of bystander cross-talk. Our proposed multi-channel directional ASR model processes multiple beamformer outputs for different steering directions simultaneously and combines it with serialized output training. Under room-acoustics and noise simulation, we demonstrate near perfect wearer/conversation-partner disambiguation and suppression of cross-talk speech from non-target directions",
    "checked": true,
    "id": "45b9fd5c381c986a93de2fc59a2355eefd5c4716",
    "semantic_title": "directional speech recognition for speaker disambiguation and cross-talk suppression",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/berger23_interspeech.html": {
    "title": "Mixture Encoder for Joint Speech Separation and Recognition",
    "volume": "main",
    "abstract": "Multi-speaker automatic speech recognition (ASR) is crucial for many real-world applications, but it requires dedicated modeling techniques. Existing approaches can be divided into modular and end-to-end methods. Modular approaches separate speakers and recognize each of them with a single-speaker ASR system. End-to-end models process overlapped speech directly in a single, powerful neural network. This work proposes a middle-ground approach that leverages explicit speech separation similarly to the modular approach but also incorporates mixture speech information directly into the ASR module in order to mitigate the propagation of errors made by the speech separator. We also explore a way to exchange cross-speaker context information through a layer that combines information of the individual speakers. Our system is optimized through separate and joint training stages and achieves a relative improvement of 7% in word error rate over a purely modular setup on the SMS-WSJ task",
    "checked": true,
    "id": "19ca1a8f951ef63465d84ce0a160c71612cba44a",
    "semantic_title": "mixture encoder for joint speech separation and recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hejna23_interspeech.html": {
    "title": "Aberystwyth English Pre-aspiration in Apparent Time",
    "volume": "main",
    "abstract": "Do younger speakers of Aberystwyth English (Wales) pre-aspirate more than older ones? Previous research reports that they do, but finds a high degree of individual variation. We build on this work by enlarging the database with the inclusion of younger speakers. We confirm that pre-aspiration increases in frequency and duration in apparent time. We further investigate whether duration analyses are affected when zero duration values are excluded, that is, whether pre-aspiration is indeed longer in younger speakers, or whether it applies more frequently. We find that pre-aspiration applies obligatorily for the majority of speakers, so that excluding zero values does not affect the statistical results. Finally, we examine the interaction of pre-aspiration with pre-glottalisation, and show that pre-glottalisation tends to block the application of pre-aspiration, with individual-specific patterns. The interaction between the two is nevertheless not accounted for by age",
    "checked": true,
    "id": "c995e8b28c379712f0299e588104845759916504",
    "semantic_title": "aberystwyth english pre-aspiration in apparent time",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23c_interspeech.html": {
    "title": "Speech Entrainment in Chinese Story-Style Talk Shows: The Interaction Between Gender and Role",
    "volume": "main",
    "abstract": "Speech entrainment is evident in short-and-short turn-taking, but entrainment in long-and-short turn-taking, like talk shows, is expected to be different but also evident. We examined three prosodic feature sets of pitch, intensity, and duration (speaking rate) to explore the impact of gender and role interaction between the host and guests on speech entrainment in a Mandarin Chinese talk show corpus. This research consistently showed that intensity remained the robust entraining feature, and the speaking rate was steadily divergent. Another vital result was that the rare occurrence of the final-rising pitch in question types led to dynamic local positive proximity. Besides, it was interesting to note that mixed-gender pairs with different roles showed more dynamic local positive proximity and synchrony on intensity and pitch than same-gender pairs. Taken together, these results suggest the complexity of gender and role interaction on speech entrainment in long-and-short turn-taking",
    "checked": true,
    "id": "7a1b9fa94b6917267761ffd0f84159f46fb1def1",
    "semantic_title": "speech entrainment in chinese story-style talk shows: the interaction between gender and role",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/steiner23_interspeech.html": {
    "title": "Sociodemographic and Attitudinal Effects on Dialect Speakers' Articulation of the Standard Language: Evidence from German-Speaking Switzerland",
    "volume": "main",
    "abstract": "In Switzerland, the way of speaking Standard German is subject to heated debates on the extent to which speakers phonetically mark their dialectal origin. However, there is little evidence of factors influencing the articulation of the standard language. This paper focuses on two variables (/ç/ and /k/) which may indicate the degree of dialect- vs. norm-oriented articulation of the standard. We analyzed data from 1,000 speakers from 125 localities using auditory coding and acoustic measures. Besides sociodemographic factors, our models suggested influences of language attitudes and political leaning: less favorable attitudes to the standard language and a right-leaning orientation were associated with higher rates of dialectal articulation of the standard. These findings contribute to our understanding of the links between attitudinal factors and speech, suggesting that phonetic features may be indexical of local dialect identities or political positioning",
    "checked": true,
    "id": "3df0548435e70785bd9e2079fccde163fc617668",
    "semantic_title": "sociodemographic and attitudinal effects on dialect speakers' articulation of the standard language: evidence from german-speaking switzerland",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burridge23_interspeech.html": {
    "title": "Vowel Normalisation in Latent Space for Sociolinguistics",
    "volume": "main",
    "abstract": "To study variations in vowel sounds between different sociolinguistic groups, sounds must be normalized to minimize variations caused by physical factors. The Lobanov method, for example, standardizes formant distributions by speaker. Since formants are often difficult to measure, and offer only a partial description of sounds, a robust and reproducible normalisation method based on the whole spectrum would be useful. One candidate is speaker-level standardization in the latent space of a variational auto-encoder, trained on a large sample of vowel spectra. We show that whole spectrum transformations induced by latent normalisation shift formants similarly to direct formant normalisation. We also show that formant-based normalisation procedures can be used to induce whole-spectrum transformations via latent space",
    "checked": true,
    "id": "fbc9d623beb0b031f5458dc2cd722a94b91d3690",
    "semantic_title": "vowel normalisation in latent space for sociolinguistics",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23n_interspeech.html": {
    "title": "Attention-based Encoder-Decoder Network for End-to-End Neural Speaker Diarization with Target Speaker Attractor",
    "volume": "main",
    "abstract": "This paper proposes a novel Attention-based Encoder-Decoder network for End-to-End Neural speaker Diarization (AED-EEND). In AED-EEND system, we incorporate the target speaker enrollment information used in target speaker voice activity detection (TS-VAD) to calculate the attractor, which can mitigate the speaker permutation problem and facilitate easier model convergence. In the training process, we propose a teacher-forcing strategy to obtain the enrollment information using the ground-truth label. Furthermore, we propose three heuristic decoding methods to identify the enrollment area for each speaker during the evaluation process. Additionally, we enhance the attractor calculation network LSTM used in the end-to-end encoder-decoder based attractor calculation (EEND-EDA) system by incorporating an attention-based model. By utilizing such an attention-based attractor decoder, our proposed AED-EEND system outperforms both the EEND-EDA and TS-VAD systems with only 0.5s of enrollment data",
    "checked": true,
    "id": "5a4923078a76ac97960e4bfa2f56522fb2619c99",
    "semantic_title": "attention-based encoder-decoder network for end-to-end neural speaker diarization with target speaker attractor",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lahiri23_interspeech.html": {
    "title": "Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism",
    "volume": "main",
    "abstract": "We address the problem of detecting who spoke when in child-inclusive spoken interactions i.e., automatic child-adult speaker classification. Interactions involving children are richly heterogeneous due to developmental differences. The presence of neurodiversity e.g., due to Autism, contributes additional variability. We investigate the impact of additional pre-training with more unlabelled child speech on the child-adult classification performance. We pre-train our model with child-inclusive interactions, following two recent self-supervision algorithms, Wav2vec 2.0 and WavLM, with a contrastive loss objective. We report 9-13% relative improvement over the state-of-the-art baseline with regards to classification F1 scores on two clinical interaction datasets involving children with Autism. We also analyze the impact of pre-training under different conditions by evaluating our model on interactions involving different subgroups of children based on various demographic factors",
    "checked": true,
    "id": "6a45165ff77d4792e4bcbd77dc4ae2d823f8c313",
    "semantic_title": "robust self supervised speech embeddings for child-adult classification in interactions involving children with autism",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baghel23_interspeech.html": {
    "title": "The DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments",
    "volume": "main",
    "abstract": "In multilingual societies, social conversations often involve code-mixed speech. The current speech technology may not be well equipped to extract information from multi-lingual multi-speaker conversations. The DISPLACE challenge entails a first-of-kind task to benchmark speaker and language diarization on the same data, as the data contains multi-speaker conversations in multilingual code-mixed speech. The challenge attempts to highlight outstanding issues in speaker diarization (SD) in multilingual settings with code-mixing. Further, language diarization (LD) in multi-speaker settings also introduces new challenges, where the system has to disambiguate speaker switches with code switches. For this challenge, a natural multilingual, multi-speaker conversational dataset is distributed for development and evaluation purposes. The systems are evaluated on single-channel far-field recordings. We also release a baseline system and report the highlights of the system submissions",
    "checked": true,
    "id": "cfd4dda52012cc2effa07e76eb88a2654a0b86b9",
    "semantic_title": "the displace challenge 2023 - diarization of speaker and language in conversational environments",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/paturi23_interspeech.html": {
    "title": "Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction",
    "volume": "main",
    "abstract": "Speaker diarization (SD) is typically used with an automatic speech recognition (ASR) system to ascribe speaker labels to recognized words. The conventional approach reconciles outputs from independently optimized ASR and SD systems, where the SD system typically uses only acoustic information to identify the speakers in the audio stream. This approach can lead to speaker errors especially around speaker turns and regions of speaker overlap. In this paper, we propose a novel second- pass speaker error correction system using lexical information, leveraging the power of modern language models (LMs). Our experiments across multiple telephony datasets show that our approach is both effective and robust. Training and tuning only on the Fisher dataset, this error correction approach leads to relative word-level diarization error rate (WDER) reductions of 15-30% on three telephony datasets: RT03-CTS, Callhome American English and held-out portions of Fisher",
    "checked": true,
    "id": "99da5ada1b6244f2caafad55e8f61fe8e6dc1e49",
    "semantic_title": "lexical speaker error correction: leveraging language models for speaker diarization error correction",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pirlogeanu23_interspeech.html": {
    "title": "The SpeeD--ZevoTech submission at DISPLACE 2023",
    "volume": "main",
    "abstract": "This paper describes our team's collaborative efforts in participating in the Track1 of the Diarization of Speaker and Language in Conversation Environments (DISPLACE) Challenge 2023. Our submission focuses on speaker diarization in multilingual scenarios, dealing with overlapping speech segments with significant noise ratios. To achieve our goal, we fine-tuned the parameters of two speaker diarization toolkits, Pyannote and NeMo, and retrained some components using the DISPLACE development sets and subsets from the MUSAN speech database. The experiments show promising results, we managed to make improvements over the pretrained voice activity detection (VAD) model, as well as training the Multi-scale Speaker Diarization Decoder (MSDD) by using the DISPLACE development datasets. Best systems are combined using DOVER-Lap. Our approach achieves a diarization error rate (DER) of 28.97% on Phase 1 Eval set, compared to the baseline diarization error rate of 40%",
    "checked": true,
    "id": "bce79bb87b455588aeefd1c140fea0e4d561a682",
    "semantic_title": "the speed--zevotech submission at displace 2023",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23g_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with Absolute Speaker Loss",
    "volume": "main",
    "abstract": "End-to-end neural speaker diarization (EEND) has proved to be a very promising method in speaker diarization, especially in tackling overlapping speech recordings. In this paper, we propose a new approach to EEND that incorporates an absolute speaker loss function, which can force the network to consider global speaker identity information in the training phase, and keeps one-stage inference at the same time. Besides, we modify the pre-processing module and do not need feature splice, which results in longer contextual information and supports longer recording input when inferencing. As a result, with our proposed one-stage system, we achieve better results in simulated librispeech conversation-like data sets compared to EEND-VC, a two-stage system. We evaluate our experiments in different chunkings, different durations and different overlap ratios, and achieve up to 70% relative improvement in terms of DER over baseline EEND-VC on short recordings and up to 7.5% on long recordings",
    "checked": true,
    "id": "5ccca0c3e6fce5a25b2c7842343fd6715c4ab43c",
    "semantic_title": "end-to-end neural speaker diarization with absolute speaker loss",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23d_interspeech.html": {
    "title": "A Context-Constrained Sentence Modeling for Deception Detection in Real Interrogation",
    "volume": "main",
    "abstract": "Detecting deception in real interrogations for criminal cases is critically important. Interrogation is composed of evidence-driven conversation that calls for a need for proper integration of context, where most prior works treat it as a sequence modeling task. In this work, we propose a context-constrained sentence modeling approach for deception detection. Specifically, we introduce the use of a global context label that is defined on multi-sentences, i.e., a context label is marked as deception if any of its sentences are deceptive. Then, by using a contextual integrator that aggregates predictions on local sentences for context label prediction, we improve deception detection by jointly optimizing global and local labels. Our approach significantly outperforms other models and achieves 76.38% and 73.15% in Unweighted Average Recall (UAR) at the local and global levels, respectively. We also conducted two analyses to further demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "f159cba7796af84c2105bb35317180fcfd1ede81",
    "semantic_title": "a context-constrained sentence modeling for deception detection in real interrogation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23c_interspeech.html": {
    "title": "MetricAug: A Distortion Metric-Lead Augmentation Strategy for Training Noise-Robust Speech Emotion Recognizer",
    "volume": "main",
    "abstract": "Noise-robust speech emotion recognition (SER) systems are important in real world applications. Conventionally, noise robustness is achieved by training on a noise-augmented dataset. In this work, instead of pre-defining noise SNRs to augment the clean set, we propose an augment-while-train strategy while referencing speech distortion metric. This strategy (MetricAug) constructs an augmented set per each training epoch by assessing the effect of different distortion levels have on degrading the SER performances. That is, we augment more of those noisy data that degrade the SER performance the most dynamically at each learning epoch. We evaluate our framework on two databases, MSP-Podcast and MELD. Our framework shows consistent robustness against varying levels and even unseen noise types. Further analysis reveals that by choosing STOI as the metric of noise distortion, it leads the construction of augmented sets better than metrics of PESQ and fwSNRseg",
    "checked": true,
    "id": "7db697dba5f3249152de2e8dddb12cd471c5f639",
    "semantic_title": "metricaug: a distortion metric-lead augmentation strategy for training noise-robust speech emotion recognizer",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ludusan23_interspeech.html": {
    "title": "The co-use of laughter and head gestures across speech styles",
    "volume": "main",
    "abstract": "The multimodal expression of laughter has been studied in various fields, with previous work focused on the characterization of the visual component during laughter. We investigate here the interaction between laughter and the gestures preceding it, considering that communicative gestures usually occur before their associated verbal message. Employing two German datasets of dyadic interactions, one containing narratives and the other dialogues, we analysed the distribution of head gestures preceding laughter. The results showed high individual variation in the production of head gestures, but also an effect of speech style and of addressee characteristics. Narratives contained more gestures and, in these materials, the familiarity between interlocutors decreased gesture incidence. Examining also the link between laughter and the corresponding co-gestures, we observed a significant effect of gesture presence on laughter form",
    "checked": true,
    "id": "9cd27b7e51062d4f86cd6f8a00286a629e3b927c",
    "semantic_title": "the co-use of laughter and head gestures across speech styles",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23d_interspeech.html": {
    "title": "EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is an important research topic in human-computer interaction. Existing works mainly rely on human expertise to design models. Despite their success, different datasets often require distinct structures and hyperparameters. Searching for an optimal model for each dataset is time-consuming and labor-intensive. To address this problem, we propose a two-stream neural architecture search (NAS) based framework, called \"EmotionNAS\". Specifically, we take two-stream features (i.e., handcrafted and deep features) as the inputs, followed by NAS to search for the optimal structure for each stream. Furthermore, we incorporate complementary information in different streams through an efficient information supplement module. Experimental results demonstrate that our method outperforms existing manually-designed and NAS-based models, setting the new state-of-the-art record",
    "checked": true,
    "id": "a49e1393d52adb3e5eefa01eba3b43e75111fea9",
    "semantic_title": "emotionnas: two-stream neural architecture search for speech emotion recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23b_interspeech.html": {
    "title": "Pre-Finetuning for Few-Shot Emotional Speech Recognition",
    "volume": "main",
    "abstract": "Speech models have long been known to overfit individual speakers for many classification tasks. This leads to poor generalization in settings where the speakers are out-of-domain or out-of-distribution, as is common in production environments. We view speaker adaptation as a few-shot learning problem and propose investigating transfer learning approaches inspired by recent success with pre-trained models in natural language tasks. We propose pre-finetuning speech models on difficult tasks to distill knowledge into few-shot downstream classification objectives. We pre-finetune Wav2Vec2.0 on every permutation of four multiclass emotional speech recognition corpora and evaluate our pre-finetuned models through 33,600 few-shot fine-tuning trials on the Emotional Speech Dataset",
    "checked": true,
    "id": "b8bea82e7a8028fbe79942c4331546701976dd36",
    "semantic_title": "pre-finetuning for few-shot emotional speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23_interspeech.html": {
    "title": "Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations",
    "volume": "main",
    "abstract": "Although automatic emotion recognition (AER) has recently drawn significant research interest, most current AER studies use manually segmented utterances, which are usually unavailable for dialogue systems. This paper proposes integrating AER with automatic speech recognition (ASR) and speaker diarisation (SD) in a jointly-trained system. Distinct output layers are built for four sub-tasks including AER, ASR, voice activity detection and speaker classification based on a shared encoder. Taking the audio of a conversation as input, the integrated system finds all speech segments and transcribes the corresponding emotion classes, word sequences, and speaker identities. Two metrics are proposed to evaluate AER performance with automatic segmentation based on time-weighted emotion and speaker classification errors. Results on the IEMOCAP dataset show that the proposed system consistently outperforms two baselines with separately trained single-task systems on AER, ASR and SD",
    "checked": true,
    "id": "5e5e5c59b96a37b097f91c9bda3206210a7e7c7c",
    "semantic_title": "integrating emotion recognition with speech recognition and speaker diarisation for conversations",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lavania23_interspeech.html": {
    "title": "Utility-Preserving Privacy-Enabled Speech Embeddings for Emotion Detection",
    "volume": "main",
    "abstract": "Audio privacy has been undertaken using adversarial task training or adversarial models based on GANs, where the models also suppress scoring of other attributes (e.g., emotion, etc.), but embeddings still retain enough information to bypass speaker privacy. We use methods for feature importance from the explainability literature to modify embeddings from adversarial task training, providing a simple and accurate approach to generating embeddings for preserving speaker privacy while not attenuating utility for related tasks (e.g., emotion recognition). This enables better adherence with privacy regulations around biometrics and voiceprints, while retaining the usefulness of audio representation learning",
    "checked": true,
    "id": "c3b305b0e2fcab25d525b095328b628f32ceb9b4",
    "semantic_title": "utility-preserving privacy-enabled speech embeddings for emotion detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burdisso23_interspeech.html": {
    "title": "Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "We propose a simple approach for weighting self-connecting edges in a Graph Convolutional Network (GCN) and show its impact on depression detection from transcribed clinical interviews. To this end, we use a GCN for modeling non-consecutive and long-distance semantics to classify the transcriptions into depressed or control subjects. The proposed method aims to mitigate the limiting assumptions of locality and the equal importance of self-connections vs. edges to neighboring nodes in GCNs, while preserving attractive features such as low computational cost, data agnostic, and interpretability capabilities. We perform an exhaustive evaluation in two benchmark datasets. Results show that our approach consistently outperforms the vanilla GCN model as well as previously reported results, achieving an F1=0.84% on both datasets. Finally, a qualitative analysis illustrates the interpretability capabilities of the proposed approach and its alignment with previous findings in psychology",
    "checked": true,
    "id": "e16fd0756daca091a09dc4dd65e589ba0482baee",
    "semantic_title": "node-weighted graph convolutional network for depression detection in transcribed clinical interviews",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/branco23_interspeech.html": {
    "title": "Laughter in task-based settings: whom we talk to affects how, when, and how often we laugh",
    "volume": "main",
    "abstract": "Map task corpora are not typically used to study laughter, but they allow an interesting analysis of multiple factors such as familiarity between the participants, their gender, and eye contact. We conducted linear/generalized mixed-effects analysis to study if co-laughter, laughter rate, and the percentage of voiced frames in laughs are influenced by such factors. Our results show that, in conversations without eye contact, the gender of the participant was statistically relevant regarding laughter rate and the percentage of voiced frames, and the difference in gender was relevant regarding co-laughter. On the other hand, with eye contact, familiarity was statistically relevant with respect to co-laughter, laughter rate, and the percentage of voiced frames. Most of our results align and extend what has been previously found, except for voiced laughs between friends. This study emphasizes the highly variable character of laughter and its dependence on interlocutors' characteristics",
    "checked": true,
    "id": "57ce72bc24b300dfbafb0ef0d6195877674d8da4",
    "semantic_title": "laughter in task-based settings: whom we talk to affects how, when, and how often we laugh",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fang23b_interspeech.html": {
    "title": "Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Huge progress has been made in self-supervised audio representation learning recently, and transformer based downstream model using Multi-head Self-Attention and Feed-Forward Network (MSA-FFN) as the basic block delivered promising transfer performance on downstream speech tasks. However, it is unclear whether the traditional transformer architecture is appropriate for downstream transfer. In this paper, we adopt a block architecture search strategy (BAS) to explore this issue, taking speech emotion recognition as an example. We found that 1) it is crucial to incorporate an FFN-like representation learning module without MSA design in the early stages of the downstream model; 2) with the use of self-supervised features, it is good enough to use a simple FFN for the downstream task. This work can serve as a source of inspiration for all other downstream speech tasks that utilize self-supervised features",
    "checked": true,
    "id": "73ad2661288b0d6d2ae3ada686c3ed4ec735b9d4",
    "semantic_title": "exploring downstream transfer of self-supervised features for speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deoliveira23_interspeech.html": {
    "title": "Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models",
    "volume": "main",
    "abstract": "In large part due to their implicit semantic modeling, self-supervised learning (SSL) methods have significantly increased the performance of valence recognition in speech emotion recognition (SER) systems. Yet, their large size may often hinder practical implementations. In this work, we take HuBERT as an example of an SSL model and analyze the relevance of each of its layers for SER. We show that shallow layers are more important for arousal recognition while deeper layers are more important for valence. This observation motivates the importance of additional textual information for accurate valence recognition, as the distilled framework lacks the depth of its large-scale SSL teacher. Thus, we propose an audio-textual distilled SSL framework that, while having only ~20% of the trainable parameters of a large SSL model, achieves on par performance across the three emotion dimensions (arousal, valence, dominance) on the MSP-Podcast v1.10 dataset",
    "checked": true,
    "id": "565b42dcb2b1f38019a0bab78e13d2e1e55497aa",
    "semantic_title": "leveraging semantic information for efficient self-supervised emotion recognition with audio-textual distilled models",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23d_interspeech.html": {
    "title": "Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining",
    "volume": "main",
    "abstract": "This paper addresses effective pretraining of automatic speech recognition (ASR) and gender recognition to improve wav2vec 2.0 embedding for speech emotion recognition (SER). Specifically, we propose a two-stage finetuning method, which first pretrains the self-supervised learning (SSL) model with ASR to learn the linguistic information and address the gradient conflict problem of conventional multi-task learning. Experimental results on the IEMOCAP dataset show that ASR pretraining can significantly outperform the simple MTL with ASR, and thus demonstrate the effectiveness of the two-stage finetuning method. We also investigate how to combine gender recognition with ASR pretraining to derive more effective embedding for SER. As the upper layers of the SSL model are focused on ASR, incorporating skip-connection can effectively embed the gender information. Compared with the single-task learning baseline, our method achieves a UA of 76.10% with an absolute improvement of 3.97%",
    "checked": true,
    "id": "35542e70c1abca996222e3ccf58e25534c6a64fe",
    "semantic_title": "two-stage finetuning of wav2vec 2.0 for speech emotion recognition with asr and gender pretraining",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thakran23_interspeech.html": {
    "title": "Investigating Acoustic Cues for Multilingual Abuse Detection",
    "volume": "main",
    "abstract": "This work focuses on audio abuse detection from an acoustic cue perspective in a multilingual social media setting. While textual abuse detection has been widely researched, comparatively, abuse detection from audio remains unexplored. Our key hypothesis is based on the fact that abusive behavior leads to distinct acoustic cues. Such cues can help detect abuse directly from audio signals without the need to transcribe them. We first demonstrate that employing a generic large pre-trained acoustic/language model is suboptimal. This proves that incorporating the right acoustic cues might be the way forward to improve performance and achieve generalization in a large-scale setting. Our proposed method explicitly focuses on two modalities: the underlying emotions expressed and the language features of audio. On the recently proposed ADIMA benchmark for this task, our approach achieves the state-of-the-art performance of 96% on the test set and outperforms existing best models by a large margin",
    "checked": true,
    "id": "4b934c58df8c65103718493997ab04c5889e5f6d",
    "semantic_title": "investigating acoustic cues for multilingual abuse detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23c_interspeech.html": {
    "title": "A novel frequency warping scale for speech emotion recognition",
    "volume": "main",
    "abstract": "We investigate an optimised non-linear frequency warping scale for speech emotion recognition (SER). The proposed scale maps the speech spectrogram onto another time-frequency domain which is invariant to speaker-specific variations. Generally, the famous mel-scale designed on human audio perception is considered the de facto standard of frequency warping. However, designed mainly for speech recognition, the generalisability of mel on other speech processing tasks is debatable. Our experiments show that an emotion-specific scale designed on an SER database outperforms the standard mel-scale. Along with performance improvement, the proposed approach also provides insight into the emotion-relevant frequency regions for SER. Despite the database-dependent design of our approach, we find that the scale obtained from our experiments also shows SER performance improvement when tested on two other databases",
    "checked": true,
    "id": "b59f54e37e31e7a458a792901acec9357e72bae7",
    "semantic_title": "a novel frequency warping scale for speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23m_interspeech.html": {
    "title": "Multi-Scale Temporal Transformer For Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition plays a crucial role in human-machine interaction systems. Recently various optimized Transformers have been successfully applied to speech emotion recognition. However, the existing Transformer architectures focus more on global information and require large computation. On the other hand, abundant speech emotional representations exist locally on different parts of the input speech. To tackle these problems, we propose a Multi-Scale TRansfomer (MSTR) for speech emotion recognition. It comprises of three main components: (1) a multi-scale temporal feature operator, (2) a fractal self-attention module, and (3) a scale mixer module. These three components can effectively enhance the transformer's ability to learn multi-scale local emotion representations. Experimental results demonstrate that the proposed MSTR model significantly outperforms a vanilla Transformer and other state-of-the-art methods across three speech emotion datasets: IEMOCAP, MELD and, CREMAD. In addition, it can greatly reduce the computational cost",
    "checked": true,
    "id": "1bfb9fc9dc072b95a0c40ccd3f05462bf01228f2",
    "semantic_title": "multi-scale temporal transformer for speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grageda23_interspeech.html": {
    "title": "Distant Speech Emotion Recognition in an Indoor Human-robot Interaction Scenario",
    "volume": "main",
    "abstract": "Social robotics and human-robot partnership are becoming very relevant topics defining many challenges for state-of-the-art speech technology. This paper presents the first evaluation of speech emotion recognition (SER) technology with nonacted speech data recorded in a real indoor humanrobot interaction (HRI) scenario. The challenge is typified by distant speech processing, reverberation, and additive external and robot engine noise. We train and evaluate a machine learning-based based on simulated acoustic modelling that includes room impulse responses (RIRs), external noise, and beamforming response. We observe increased performance in the prediction of arousal, valence, and dominance with the proposed training procedure combined with delayandsum and minimum variance distortionless response (MVDR), with gain as high as 180%, compared with the result obtained with the model trained with the original data in controlled environments. Moreover, the degradation achieved when compared with the original matched training/testing condition is just 39%",
    "checked": true,
    "id": "7081ff17e308508ec364cba9c960ea70f410b94d",
    "semantic_title": "distant speech emotion recognition in an indoor human-robot interaction scenario",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tao23b_interspeech.html": {
    "title": "A Study on Prosodic Entrainment in Relation to Therapist Empathy in Counseling Conversation",
    "volume": "main",
    "abstract": "Counseling is carried out as spoken conversation between a therapist and a client. The empathy level expressed by the therapist is considered an important index of the quality of counseling and often assessed by an observer or the client. This research investigates the entrainment of speech prosody in relation to subjectively rated empathy. Experimental results show that the entrainment of intensity is more influential to empathy observation than that of pitch or speech rate in client-therapist interaction. The observer and the client have different perceptions of therapist empathy with the same entrained phenomena in pitch and intensity. The client's intention to make adjustment on pitch variation and intensity of speech is considered an indicator of the client's perception of counseling quality",
    "checked": true,
    "id": "4ff4cb0eefa7ae35d50363da09b99b041d645b19",
    "semantic_title": "a study on prosodic entrainment in relation to therapist empathy in counseling conversation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/minematsu23_interspeech.html": {
    "title": "A Unified Framework to Improve Learners' Skills of Perception and Production Based on Speech Shadowing and Overlapping",
    "volume": "main",
    "abstract": "A unified framework to improve learners' skills in perceiving and producing L2 sounds is demonstrated based on speech shadowing and overlapping. Speech shadowing is a training method, where learners are asked to reproduce a given model speech (M) as immediately as possible, and it was proved to be effective in enhancing their L2 speech perception. After several trials of shadowing, the learners are provided with M's script to continue shadowing with no delay, called overlapping. By comparing the shadowing speech (S) and the script-shadowing speech (SS), shadowing breakdowns are measured sequentially, which can characterize listening breakdowns. By comparing M and SS, the prosodic and segmental gaps are analyzed sequentially and presented visually to learners along with imitation scores. All the tasks are implemented as interactive speech games, which help learners to become more proficient in L2 speech perception and production",
    "checked": false,
    "id": "5cb4be24d7dc0823fad79a4d236faf4d51f3e5db",
    "semantic_title": "a uniﬁed framework to improve learners' skills of perception and production based on speech shadowing and overlapping",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nicholls23_interspeech.html": {
    "title": "Speak & Improve: L2 English Speaking Practice Tool",
    "volume": "main",
    "abstract": "A problem for building and studying approaches to automatically assess and give feedback to L2 English learners on their speaking ability is a lack of suitable data sets and platforms to run experiments on. This paper describes the Speak & Improve (S&I) speaking practice tool, which has been designed to meet the needs of researchers while also offering learners the opportunity to practise their English speaking and improve their confidence. S&I has tasks that allow the learner to demonstrate and improve their proficiency across the English speaking construct. Most of the tasks are free speaking, that is, the learner is not constrained in what they have to say. Over 400,000 learners worldwide have tried the alpha version of S&I. This paper presents the next version which provides more choice and flexibility to the users and more feedback on performance",
    "checked": false,
    "id": "a132a41d90b4071041cb29796c41cb52dc187719",
    "semantic_title": "acquisition of l2 english spatial deixes by arabic-speaking children",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nicolao23_interspeech.html": {
    "title": "Measuring prosody in child speech using SoapBox Fluency API",
    "volume": "main",
    "abstract": "SoapBox Fluency API uses automatic speech recognition (ASR) to return data points that enable educators to assess a child's oral reading fluency (ORF). Prosody is a key component of ORF assessment together with accuracy and speed, but manual assessment of prosody is largely subjective. A quantitative prosody score would benefit fair assessment. In this show and tell paper and accompanying video, we describe the new prosody feature of the API and demonstrate how the outputs can be used to assess expressiveness in ORF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nissen23_interspeech.html": {
    "title": "Teaching Non-native Sound Contrasts using Visual Biofeedback",
    "volume": "main",
    "abstract": "This demonstration will explain how non-native sound contrasts can be taught to second language (L2) learners using real-time visual biofeedback through an electropalatographic (EPG) sensor. How the EPG sensor is created from a dental mold and is subsequently produced will be shown during the presentation. An explanation of how a student can visualize the contact patterns of their speech and how a tutor might use the various features of the associated software to customize the instruction to an individual speaker will be illustrated during the presentation. Possible limitations, costs, and drawbacks to using the technology will also be discussed",
    "checked": false,
    "id": "1d141d316fff18bb23d7cfe273b6417eef10838d",
    "semantic_title": "the impact of electropalatography in teaching the /r/-/l/ sound contrast for native japanese language learners of english: evidence from lingua-palatal contact",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/walsh23_interspeech.html": {
    "title": "Large-Scale Automatic Audiobook Creation",
    "volume": "main",
    "abstract": "An audiobook can dramatically improve a work of literature's accessibility and improve reader engagement. However, audiobooks can take hundreds of hours of human effort to create, edit, and publish. In this work, we present a system that can automatically generate high-quality audiobooks from online e-books. In particular, we leverage recent advances in neural text-to-speech to create and release thousands of human-quality, open-license audiobooks from the Project Gutenberg e-book collection. Our method can identify the proper subset of e-book content to read for a wide collection of diversely structured books and can operate on hundreds of books in parallel. Our system allows users to customize an audiobook's speaking speed and style, emotional intonation, and can even match a desired voice using a small amount of sample audio. This work contributed over five thousand open-license audiobooks and an interactive demo that allows users to quickly create their own customized audiobooks. To listen to the audiobook collection visit https://aka.ms/audiobook",
    "checked": true,
    "id": "cfc416129b1fb3f2b755c7a577f577fa5e5fa13e",
    "semantic_title": "large-scale automatic audiobook creation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elkheir23_interspeech.html": {
    "title": "QVoice: Arabic Speech Pronunciation Learning Application",
    "volume": "main",
    "abstract": "This paper introduces a novel Arabic pronunciation learning application QVoice, powered with end-to-end mispronunciation detection and feedback generator module. The application is designed to support non-native Arabic speakers in enhancing their pronunciation skills, while also helping native speakers mitigate any potential influence from regional dialects on their Modern Standard Arabic (MSA) pronunciation. QVoice employs various learning cues to aid learners in comprehending meaning, drawing connections with their existing knowledge of English language, and offers detailed feedback for pronunciation correction, along with contextual examples showcasing word usage. The learning cues featured in QVoice encompass a wide range of meaningful information, such as visualizations of phrases/words and their translations, as well as phonetic transcriptions and transliterations. QVoice provides pronunciation feedback at the character level and assesses performance at the word level",
    "checked": true,
    "id": "98b2463a9b3312776c84ecc2a2b0c6bb8d351634",
    "semantic_title": "qvoice: arabic speech pronunciation learning application",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/svec23_interspeech.html": {
    "title": "Asking Questions: an Innovative Way to Interact with Oral History Archives",
    "volume": "main",
    "abstract": "The paper describes our initial effort to use Transformer-based neural networks for understanding and presenting oral history archives. Such archives of interviews often contain large passages of the interviewee's speech. Our approach automatically generates relevant questions, which enrich such monotonous parts and allows the listener to better orient in the interview. The generated questions also allow for finding interesting parts of the interview without changing the original meaning of the testimony. We present our working pipeline consisting of a Wav2Vec speech recognizer, BERT-based punctuation detection, T5 asking questions model and BERT-based semantic continuity model",
    "checked": true,
    "id": "0024e994ce76ecb81c16fefea171108544377cd4",
    "semantic_title": "asking questions: an innovative way to interact with oral history archives",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhat23_interspeech.html": {
    "title": "DisfluencyFixer: A tool to enhance Language Learning through Speech To Speech Disfluency Correction",
    "volume": "main",
    "abstract": "Conversational speech often consists of deviations from the speech plan, producing disfluent utterances that affect downstream NLP tasks. Removing these disfluencies is necessary to create fluent and coherent speech. This paper presents DisfluencyFixer, a tool that performs speech-to-speech disfluency correction in English and Hindi using a pipeline of Automatic Speech Recognition (ASR), Disfluency Correction (DC) and Text-To-Speech (TTS) models. Our proposed system removes disfluencies from input speech and returns fluent speech as output along with its transcript, disfluency type and total disfluency count in source utterance, providing a one-stop destination for language learners to improve the fluency of their speech. We evaluate the performance of our tool subjectively and receive scores of 4.26, 4.29 and 4.42 out of 5 in ASR performance, DC performance and ease-of-use of the system. Our tool can be accessed openly at the following link",
    "checked": true,
    "id": "e28edbd8066266960d048154f63a4ce892f15c99",
    "semantic_title": "disfluencyfixer: a tool to enhance language learning through speech to speech disfluency correction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/prakash23_interspeech.html": {
    "title": "Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages",
    "volume": "main",
    "abstract": "Cross-lingual dubbing of lecture videos requires the transcription of the original audio, correction and removal of disfluencies, domain term discovery, text-to-text translation into the target language, text-to-speech synthesis followed by isochronous lipsyncing to the original video. This task becomes challenging when the source and target languages belong to different language families, resulting in differences in generated audio duration. This is further compounded by the original speaker's rhythm, especially for extempore speech. This paper describes the challenges in regenerating English lecture videos in Indian languages semi-automatically. A prototype is developed for dubbing lectures into Indian languages. A demo for dubbing with supervision is available online",
    "checked": true,
    "id": "97e75517af9fbad1eecb32410fb2321f3880cba8",
    "semantic_title": "technology pipeline for large scale cross-lingual dubbing of lecture videos into multiple indian languages",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elshahawy23_interspeech.html": {
    "title": "MyVoice: Arabic Speech Resource Collaboration Platform",
    "volume": "main",
    "abstract": "We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data",
    "checked": true,
    "id": "3627dc4fd36a5f4511ea8de9e07c553efb6e0bce",
    "semantic_title": "myvoice: arabic speech resource collaboration platform",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hromada23_interspeech.html": {
    "title": "Personal Primer Prototype 1: Invitation to Make Your Own Embooked Speech-Based Educational Artifact",
    "volume": "main",
    "abstract": "In our show & tell contribution, we present more closely seven properties of a digital educational artifact known as Personal Primer. These are: speech-based, embooked, voluminous, modular, circadian, SDG-compliant, edge-computing. Furthermore, we provide link to code repository as well as enumerate off-the-shelf components which can be combined together in order to yield personal Primer prototypes of the first generation. Thus, this show& tell contribution can be understood as an invitation addressing any motivated teacher, student, parent or engineer willing to make an own copy of a Primer for oneself or others",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deng23_interspeech.html": {
    "title": "Time-frequency Domain Filter-and-sum Network for Multi-channel Speech Separation",
    "volume": "main",
    "abstract": "Learning-based methods have made impressive strides in speech separation, and the implicit filter-and-sum network (iFaSNet) stands out as a reliable multi-channel solution. Meanwhile, the TF-GridNet has achieved state-of-the-art performance on the WSJ0-2mix dataset, indicating the underlying capability of time-frequency (T-F) domain speech separation methods. This paper investigates the possibility of constructing a T-F domain filter-and-sum network that improves upon the iFaSNet. In addition to optimizing the separation module, we develop a narrow-band spatial feature as a cross-channel feature and a convolution module for context decoding. With these enhancements, we redesign each module under the iFaSNet architecture, which entirely operates in the T-F domain. Thus, the proposed method is referred to as the TF-FaSNet. Experimental results on fixed microphone array geometries show that the TF-FaSNet outperforms the standard iFaSNet under all conditions with similar model complexity",
    "checked": true,
    "id": "0e7818b852c769ea1c2e4738a3a235b0895e4fb0",
    "semantic_title": "time-frequency domain filter-and-sum network for multi-channel speech separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23h_interspeech.html": {
    "title": "Audio-Visual Fusion using Multiscale Temporal Convolutional Attention for Time-Domain Speech Separation",
    "volume": "main",
    "abstract": "Audio-only speech separation methods cannot fully exploit audio-visual correlation information of speaker, which limits separation performance. Additionally, audio-visual separation methods usually adopt traditional idea of feature splicing and linear mapping to fuse audio-visual features, this approach requires us to think more about fusion process. Therefore, in this paper, combining with the changes of speaker mouth landmarks, we propose a time-domain audio-visual temporal convolution attention speech separation method (AVTA). In AVTA, we design a multiscale temporal convolutional attention (MTCA) to better focus on contextual dependencies of time sequences. We then use sequence learning and fusion network composed of MTCA to build a separation model for speech separation task. On different datasets, AVTA achieves competitive performance, and compared to baseline methods, AVTA is better balanced in training cost, computational complexity and separation performance",
    "checked": true,
    "id": "685cac16d71e922509c76442a666c57709fef6e3",
    "semantic_title": "audio-visual fusion using multiscale temporal convolutional attention for time-domain speech separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ca_interspeech.html": {
    "title": "An Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention",
    "volume": "main",
    "abstract": "We present an efficient speech separation neural network, ARFDCN, which combines dilated convolutions, multi-scale fusion (MSF), and channel attention to overcome the limited receptive field of convolution-based networks and the high computational cost of transformer-based networks. The suggested network architecture is encoder-decoder based. By using dilated convolutions with gradually increasing dilation value to learn local and global features and fusing them at adjacent stages, the model can learn rich feature content. Meanwhile, by adding channel attention modules to the network, the model can extract channel weights, learn more important features, and thus improve its expressive power and robustness. Experimental results indicate that the model achieves a decent balance between performance and computational efficiency, making it a promising alternative to current mainstream models for practical applications",
    "checked": true,
    "id": "7dfaeb85b8b8095cb60338a003e341df7e879057",
    "semantic_title": "an efficient speech separation network based on recurrent fusion dilated convolution and channel attention",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/phokhinanan23_interspeech.html": {
    "title": "Binaural Sound Localization in Noisy Environments Using Frequency-Based Audio Vision Transformer (FAViT)",
    "volume": "main",
    "abstract": "Binaural sound source localization (BSSL) aims to locate sound as the way human does, but it falls short due to acoustic interferences. While Convolutional Neural Networks (CNNs) have shown promise in localizing sounds corrupted by noise, their large parameter and training data requirements make them unsuitable for real-time processing on devices like hearing aids and robots. In this paper, we propose an adapted Vision Transformer (ViT) model for BSSL in noisy environments. Inspired by the Duplex Theory, our model uses selective attention mechanisms to the frequency range of binaural features to aid in sound localization. Our model outperformed recent CNNs and standard audio ViT models in localizing speech in unseen noises and speakers, even in challenging conditions with low training data and parameters. The attention heatmap results suggest differences in how humans and machines process binaural cues, opening up for further investigation",
    "checked": true,
    "id": "0e0e0643592733052a2ef1c5a1df85233c8ef95a",
    "semantic_title": "binaural sound localization in noisy environments using frequency-based audio vision transformer (favit)",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23i_interspeech.html": {
    "title": "Contrastive Learning based Deep Latent Masking for Music Source Separation",
    "volume": "main",
    "abstract": "Recent studies on music source separation have extended their applicability to generic audio signals. Real-time applications for music source separation are necessary to provide services such as custom equalizers or to improve the sound of live streaming with diverse effects. However, most prior methods are unsuitable for real-time applications due to their high computational complexity, large memory usage, or long latency. To overcome these problems, we propose a Wave-U-Net type of music source separation network that utilizes high-dimensional masking for the deep latent domain features. We also introduce a contrastive learning technique to estimate the salient latent space embedding of each target source using a masking-based approach. The performance of our proposed model is evaluated on the MUSDB18HQ dataset in comparison with several baselines. The experiments confirm that our proposed model is capable of real-time processing and outperforms existing models",
    "checked": true,
    "id": "c620a98e7dfbc78c9455921368f70211d5c84789",
    "semantic_title": "contrastive learning based deep latent masking for music source separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23k_interspeech.html": {
    "title": "Speaker Extraction with Detection of Presence and Absence of Target Speakers",
    "volume": "main",
    "abstract": "Target speaker extraction extracts a target voice from a given cocktail party mixture signal. Most studies are restricted to conditions in which the target speaker is present in the mixture (PT), which often fail when the target speaker is absent (AT). Training on both PT and AT situations helps, but degrades the PT performance as the model intrinsically tries to detect the target presence. We propose a new model, called TSEJoint, that jointly performs target speaker detection and extraction. Both tasks share the low-level modules, allowing the detection branch to use a pre-separated signal and keeping the overall processing pipeline length similar, while at the high-level they have different branches to ensure the performance of each task. We evaluate our proposed methods under PT and AT conditions comprising one and two talkers. The TSEJoint model shows better extraction performance under the PT condition and better detection performance on all conditions compared with the baseline",
    "checked": true,
    "id": "8fd9be042815e0cdaff0d7f440a15b6d4e3e21bf",
    "semantic_title": "speaker extraction with detection of presence and absence of target speakers",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23k_interspeech.html": {
    "title": "PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network",
    "volume": "main",
    "abstract": "It is common in everyday spoken communication that we look at the turning head of a talker to listen to his/her voice. Humans see the talker to listen better, so do machines. However, previous studies on audio-visual speaker extraction have not effectively handled the varying talking face. This paper studies how to take full advantage of the varying talking face. We propose a Pose-Invariant Audio-Visual Speaker Extraction Network (PIAVE) that incorporates an additional pose-invariant view to improve audio-visual speaker extraction. Specifically, we generate the pose-invariant view from each original pose orientation, which enables the model to receive a consistent frontal view of the talker regardless of his/her head pose, therefore, forming a multi-view visual input for the speaker. Experiments on the multi-view MEAD and in-the-wild LRS3 dataset demonstrate that PIAVE outperforms the state-of-the-art and is more robust to pose variations",
    "checked": true,
    "id": "3aa567846bcc855e224ab4a4355af4f95bf9a395",
    "semantic_title": "piave: a pose-invariant audio-visual speaker extraction network",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarabia23_interspeech.html": {
    "title": "Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning",
    "volume": "main",
    "abstract": "We present Spatial LibriSpeech, a spatial audio dataset with over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor noise. Spatial LibriSpeech is designed for machine learning model training, and it includes labels for source position, speaking direction, room acoustics and geometry. Spatial LibriSpeech is generated by augmenting LibriSpeech samples with 200k+ simulated acoustic conditions across 8k+ synthetic rooms. To demonstrate the utility of our dataset, we train models on four spatial audio tasks, resulting in a median absolute error of 6.60° on 3D source localization, 0.43m on distance, 90.66ms on T30, and 2.74dB on direct-to-reverberant ratio estimation. We show that the same models generalize well to widely-used evaluation datasets, e.g., obtaining a median absolute error of 12.43° on 3D source localization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACE Challenge",
    "checked": true,
    "id": "8e5d66e802084898fc628685a3b8d8cc331b2276",
    "semantic_title": "spatial librispeech: an augmented dataset for spatial audio learning",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23q_interspeech.html": {
    "title": "Image-driven Audio-visual Universal Source Separation",
    "volume": "main",
    "abstract": "This paper introduces an image-driven audio-visual universal source separation (ID-USS) and proposes ID-USS-Conformer. ID-USS aims to separate a target source from the mixture based on the input image that is consistent with the target. Importantly, ID-USS only focuses on the sound made by the target in this image, not on the description of the target or the semantic information of the picture. In detail, ID-USS-Conformer mainly consists of an Efficient-b3-based visual branch and a Conformer-based audio branch. The visual branch extracts the visual clue of the target from the input image. After the audio branch fuses the visual features, ID-USS-Conformer separates the target source from the mixture. We launch an ID-USS dataset and verify the effectiveness of ID-USS-Conformer on it. The ID-USS-Conformer has achieved a 10.139 dB signal-to-distortion ratio improvement in the test set and outperformed the compared methods",
    "checked": true,
    "id": "7aa1cb84053341a91b050e4e5153c35eda2cef6b",
    "semantic_title": "image-driven audio-visual universal source separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fras23_interspeech.html": {
    "title": "Joint Blind Source Separation and Dereverberation for Automatic Speech Recognition using Delayed-Subsource MNMF with Localization Prior",
    "volume": "main",
    "abstract": "Overlapping speech and high room reverberation deteriorate the accuracy of automatic speech recognition (ASR). This paper proposes a method for jointly optimum source separation and dereverberation using delayed subsource multichannel nonnegative matrix factorization (MNMF). We formulate a subsource-based signal model that accounts for late room reverberation using time-delayed microphone signals from several past time frames. We then propose a maximum a posteriori (MaP) estimator based on MNMF with localization prior on the mixing matrix suitable for direct-path and reverberant signal components estimation. Finally, two algorithms are derived, namely the original and simplified delayed subsource MNMF, which are shown to outperform many state-of-the-art approaches. The results of experimental evaluations, performed using real and simulated data, indicate superior performance of the proposed processing in terms of the word error rate (WER) as well as signal-to-distortion ratio (SDR)",
    "checked": true,
    "id": "ddd34b9e20ce3d4a271ed13284b26c79f927d2ea",
    "semantic_title": "joint blind source separation and dereverberation for automatic speech recognition using delayed-subsource mnmf with localization prior",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23na_interspeech.html": {
    "title": "SDNet: Stream-attention and Dual-feature Learning Network for Ad-hoc Array Speech Separation",
    "volume": "main",
    "abstract": "Considerable progress has been made in multi-channel speech separation for fixed arrays. In this paper, we aim to develop a robust system for ad-hoc arrays to deal with uncertainties of microphone locations and numbers. Previous works commonly used the averaging method for ad-hoc arrays, overlooking the diversity of microphones in various positions. Some studies suggest that microphones with high signal-to-noise ratio(SNR) are more helpful in improving speech quality. Motivated by this, we propose stream-attention and dual-feature learning network called SDNet. The key points are as follows: 1) We propose a dual-feature learning block with fewer parameters to learn the long-term dependency better. 2) Based on this high-quality speech representation, we further propose stream attention that effectively handles microphone variability and allocates more attention to microphones with higher SNR. Experiments show that our proposed model outperforms other advanced baselines",
    "checked": true,
    "id": "ddf9ed7c3bdc8e6facece3763b1c7a0d5f8f5e2e",
    "semantic_title": "sdnet: stream-attention and dual-feature learning network for ad-hoc array speech separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baek23_interspeech.html": {
    "title": "Deeply Supervised Curriculum Learning for Deep Neural Network-based Sound Source Localization",
    "volume": "main",
    "abstract": "Deep neural network (DNN) has made impressive progress in sound source localization (SSL) tasks with the hard n-hot labels that represent specific directions-of-arrivals (DOAs). However, recent study suggested soft DOA labels, considering the correlations between targets and nearby DOAs. In this study, to effectively train a DNN using soft labels, we propose deeply supervised curriculum learning (DSCL) by adopting the two techniques for the DNN, deep supervision (DS) and curriculum learning (CL). We train a DNN to solve SSL problems progressing from easier to harder, expecting the DNN would gradually reduce the angular region of the target DOAs. It is gained by various resolution soft targets for the different DNN layers to deeply supervise the DNN, while increasing the angular selectivity of the targets from the early to late stages of training by CL. Proposed method was verified on datasets with multi-speakers, and exceeded the hard-label methods with great improvements",
    "checked": true,
    "id": "1da3f494365978ade6ff934bef21bc238048df90",
    "semantic_title": "deeply supervised curriculum learning for deep neural network-based sound source localization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fujimura23_interspeech.html": {
    "title": "Multi-channel separation of dynamic speech and sound events",
    "volume": "main",
    "abstract": "We propose a multi-channel separation method for moving sound sources. We build upon a recent beamformer for a moving speaker using attention-based tracking. This method uses an attention mechanism to compute the time-varying spatial statistics which enables tracking the moving source. While this prior work aimed to extract a single target source, we simultaneously estimate multiple sources. Our main technical contribution is to introduce attention-based tracking into the iterative source steering algorithm for independent vector analysis (IVA), enabling joint estimation of multiple sources. We experimentally show that the proposed method greatly improves the separation performance for moving speakers, including an absolute reduction of 27.2% in word error rate compared to time-invariant IVA. In addition, we demonstrate that the proposed method is effective as a pre-processing for sound event detection, showing an improvement in F1 scores of up to 4.7% in real recordings",
    "checked": true,
    "id": "b71ae4f3f62a8d147d0bfa328610e5e1ee04e60e",
    "semantic_title": "multi-channel separation of dynamic speech and sound events",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ja_interspeech.html": {
    "title": "Rethinking the Visual Cues in Audio-Visual Speaker Extraction",
    "volume": "main",
    "abstract": "The Audio-Visual Speaker Extraction (AVSE) algorithm employs parallel video recording to leverage two visual cues, namely speaker identity and synchronization, to enhance performance compared to audio-only algorithms. However, the visual front-end in AVSE is often derived from a pre-trained model or end-to-end trained, making it unclear which visual cue contributes more to the speaker extraction performance. This raises the question of how to better utilize visual cues. To address this issue, we propose two training strategies that decouple the learning of the two visual cues. Our experimental results demonstrate that both visual cues are useful, with the synchronization cue having a higher impact. We introduce a more explainable model, the Decoupled Audio-Visual Speaker Extraction (DAVSE) model, which leverages both visual cues",
    "checked": true,
    "id": "8e8a241084b55dbdc8f243a03cd449c706955c34",
    "semantic_title": "rethinking the visual cues in audio-visual speaker extraction",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dang23_interspeech.html": {
    "title": "Using Semi-supervised Learning for Monaural Time-domain Speech Separation with a Self-supervised Learning-based SI-SNR Estimator",
    "volume": "main",
    "abstract": "Speech separation aims to decompose mixed speeches into independent signals. Prior research on monaural timedomain speech separation has made great progress in supervised manners. Almost all of these works are trained on simulated mixed speech signals since obtaining ground truth for real-world mixed signals is problematic. To this end, we propose a novel semi-supervised learning method for speech separation (SSLM-SS), which leverages mixed speeches without ground truth. In particular, for this type of data, we further put forward a non-intrusive separated speech quality prediction network (SSQP-Net) based on self-supervised learning. According to the results, the linear correlation coefficient between the predicted results of SSQP-Net and the ground truth achieves 0.9. Moreover, the performance of SSLM-SS equipped with SSQP-Net exhibits an improvement of 0.2 dB and 1.1 dB compared to the mixture invariant training (MixIT) in the conditions of involving 10% and 50% labeled data respectively, and rivals supervised learning",
    "checked": true,
    "id": "c8f8307a962009ddfa149156316e1fe04fed8fe2",
    "semantic_title": "using semi-supervised learning for monaural time-domain speech separation with a self-supervised learning-based si-snr estimator",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23d_interspeech.html": {
    "title": "Investigation of Training Mute-Expressive End-to-End Speech Separation Networks for an Unknown Number of Speakers",
    "volume": "main",
    "abstract": "In speech separation, there have been a limited number of prior works for an unknown number of speakers in a speech mixture. To address this situation, one simple solution is to constitute the sufficient number of output channels greater than or equal to the expected number of speakers and ignore invalid outputs containing meaningless signals when the number of speakers is less than the output channels. To detect such invalid outputs, it is an ideal scenario for the meaningless signals to be muted. In this paper, we investigate several training methods by which separation models can mute the invalid outputs. We first introduce an on-the-fly data mixing scheme adding small random noises to the speech mixtures. As a training criterion, we analyze why the well-known scale-invariant signal-to-noise ratio is not suitable for muting the invalid outputs because of its power amplification problem and also explain why we use the signal-to-noise ratio criterion to avoid the problem",
    "checked": true,
    "id": "84ce970242c28c102aaa8e6863192af63e16b512",
    "semantic_title": "investigation of training mute-expressive end-to-end speech separation networks for an unknown number of speakers",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cho23_interspeech.html": {
    "title": "SR-SRP: Super-Resolution based SRP-PHAT for Sound Source Localization and Tracking",
    "volume": "main",
    "abstract": "Sound source localization and tracking have been extensively studied. Recently, there has been considerable interest in highly reverberant scenarios and steered response power with phase transform (SRP-PHAT) based models have shown a good performance. However, these models still have limitations because the SRP-PHAT algorithm cannot represent the direction of the source in such adverse environments. In this paper, we propose a novel structure combining a super-resolution model and a single sound source localization model that allows to improve direction estimation performance. The proposed method generates a robust power map that accurately represents the direction of the source, even in poor scenarios. Furthermore, the proposed structure has a lower computational cost because it uses a low-resolution map. Experimental results on simulation-based and real-world data show that the proposed method outperforms the state-of-the-art model, Cross3D",
    "checked": true,
    "id": "0aa83be007001291478e5130c8405aad3b7c551a",
    "semantic_title": "sr-srp: super-resolution based srp-phat for sound source localization and tracking",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23g_interspeech.html": {
    "title": "Dual-Memory Multi-Modal Learning for Continual Spoken Keyword Spotting with Confidence Selection and Diversity Enhancement",
    "volume": "main",
    "abstract": "Enabling continual learning (CL) from an ever-changing environment is highly valuable, but it poses significant challenges for spoken keyword spotting (KWS), which simultaneously deals with both variability in acoustic characteristics of speech signals and catastrophic forgetting issues. In this paper, we propose a novel framework for replay-based CL in KWS that uses a Dual-Memory Multi-Modal (DM3) structure to enhance generalizability and robustness. Our approach leverages short-term and long-term models to learn near-term and long-term knowledge in an adaptive manner with a dual-memory structure, while also exploiting the consistency of multiple speech perturbations to improve the robustness with a multi-modal structure. Additionally, we introduce a class-balanced selection strategy that uses confidence scores to sort training samples. Experiments demonstrate the effectiveness of our method over competitive baselines in class incremental learning and domain incremental learning KWS settings",
    "checked": true,
    "id": "c718a9c97bf63104aeebf33feb2dfc69a58e25c9",
    "semantic_title": "dual-memory multi-modal learning for continual spoken keyword spotting with confidence selection and diversity enhancement",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23j_interspeech.html": {
    "title": "FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization",
    "volume": "main",
    "abstract": "Extracting direct-path spatial features is critical for sound source localization in adverse acoustic environments. This paper proposes a full-band and narrow-band fusion network for estimating direct-path inter-channel phase difference (DP-IPD) from microphone signals. The alternating full-band and narrow-band layers are responsible for learning the full-band correlation and narrow-band extraction of DP-IPD, respectively. Experiments show that the proposed network noticeably outperforms other advanced methods on both simulated and real-world data",
    "checked": true,
    "id": "11ca9b1b12d8a54dd4b1eacf262828fa01b90aed",
    "semantic_title": "fn-ssl: full-band and narrow-band fusion for sound source localization",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23g_interspeech.html": {
    "title": "A Neural State-Space Modeling Approach to Efficient Speech Separation",
    "volume": "main",
    "abstract": "In this work, we introduce S4M, a new efficient speech separation framework based on neural state-space models (SSM). Motivated by linear time-invariant systems for sequence modeling, our SSM-based approach can efficiently model input signals into a format of linear ordinary differential equations (ODEs) for representation learning. To extend the SSM technique into speech separation tasks, we first decompose the input mixture into multi-scale representations with different resolutions. This mechanism enables S4M to learn globally coherent separation and reconstruction. The experimental results show that S4M performs comparably to other separation backbones in terms of SI-SDRi, while having a much lower model complexity with significantly fewer trainable parameters. In addition, our S4M-tiny model (1.8M parameters) even surpasses attention-based Sepformer (26.0M parameters) in noisy conditions with only 9.2% of multiply-accumulate operation (MACs)",
    "checked": true,
    "id": "0f1fe30bd9176ac58c8ed72dc2d562168eee6d48",
    "semantic_title": "a neural state-space modeling approach to efficient speech separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23c_interspeech.html": {
    "title": "Locate and Beamform: Two-dimensional Locating All-neural Beamformer for Multi-channel Speech Separation",
    "volume": "main",
    "abstract": "Recently, stunning improvements on multi-channel speech separation have been achieved by neural beamformers when direction information is available. However, most of them neglect to utilize speaker's 2-dimensional (2D) location cues contained in mixture signal, which limits the performance when two sources come from close directions. In this paper, we propose an end-to-end beamforming network for 2D location guided speech separation merely given mixture signal. It first estimates discriminable direction and 2D location cues, which imply directions the sources come from in multi views of microphones and their 2D coordinates. These cues are then integrated into location-aware neural beamformer, thus allowing accurate reconstruction of two sources' speech signals. Experiments show that our proposed model not only achieves a comprehensive decent improvement compared to baseline systems, but avoids inferior performance on spatial overlapping cases",
    "checked": true,
    "id": "166b3406c0a9137604497c64102396d187a58706",
    "semantic_title": "locate and beamform: two-dimensional locating all-neural beamformer for multi-channel speech separation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23f_interspeech.html": {
    "title": "Monaural Speech Separation Method Based on Recurrent Attention with Parallel Branches",
    "volume": "main",
    "abstract": "In many speech separation methods, the contextual information contained in the feature sequence is mainly modeled by recurrent layer and/or self-attention mechanism. However, how to combine these two powerful approaches more effectively needs to be explored. In this paper, a recurrent attention with parallel branches is proposed to first fully exploit the contextual information contained in the time-frequency (T-F) features. Then, this information is further modeled by the recurrent modules in a conventional manner. Specifically, the proposed recurrent attention with parallel branches uses two attention modules stacked sequentially. Each attention module has two parallel branches of self-attention to model dependencies along two axes and one convolutional layer for feature fusion. Thus, the contextual information contained in the T-F features can be fully exploited and further modeled by the recurrent modules. Experimental results showed the effectiveness of our proposed method",
    "checked": true,
    "id": "138dd775e634f7662baa26c5a9106f6bb84f9035",
    "semantic_title": "monaural speech separation method based on recurrent attention with parallel branches",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23m_interspeech.html": {
    "title": "Ontology-aware Learning and Evaluation for Audio Tagging",
    "volume": "main",
    "abstract": "This study defines a new evaluation metric for audio tagging tasks to alleviate the limitation of the mean average precision (mAP) metric. The mAP metric treats different kinds of sound as independent classes without considering their relations. The proposed metric, ontology-aware mean average precision (OmAP), addresses the weaknesses of mAP by utilizing additional ontology during evaluation. Specifically, we reweight the false positive events in the model prediction based on the AudioSet ontology graph distance to the target classes. The OmAP also provides insights into model performance by evaluating different coarse-grained levels in the ontology graph. We conduct a human assessment and show that OmAP is more consistent with human perception than mAP. We also propose an ontology-based loss function (OBCE) that reweights binary cross entropy (BCE) loss based on the ontology distance. Our experiment shows that OBCE can improve both mAP and OmAP metrics on the AudioSet tagging task",
    "checked": true,
    "id": "92fc5f32f48947729ad9e8ad52fc16be5159aa55",
    "semantic_title": "ontology-aware learning and evaluation for audio tagging",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23c_interspeech.html": {
    "title": "Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing",
    "volume": "main",
    "abstract": "Audio anti-spoofing for automatic speaker verification aims to safeguard users' identities from spoofing attacks. Although state-of-the-art spoofing countermeasure(CM) models perform well on specific datasets, they lack generalization when evaluated with different datasets. To address this limitation, previous studies have explored large pre-trained models, which require significant resources and time. We aim to develop a compact but well-generalizing CM model that can compete with large pre-trained models. Our approach involves multi-dataset co-training and sharpness-aware minimization, which has not been investigated in this domain. Extensive experiments reveal that proposed method yield competitive results across various datasets while utilizing 4,000 times less parameters than the large pre-trained models",
    "checked": true,
    "id": "612a5d14767dccba176cf4c2713e4a9eed844c6f",
    "semantic_title": "multi-dataset co-training with sharpness-aware optimization for audio anti-spoofing",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lay23_interspeech.html": {
    "title": "Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement",
    "volume": "main",
    "abstract": "Recently, score-based generative models have been successfully employed for the task of speech enhancement. A stochastic differential equation is used to model the iterative forward process, where at each step environmental noise and white Gaussian noise are added to the clean speech signal. While in limit the mean of the forward process ends at the noisy mixture, in practice it stops earlier and thus only at an approximation of the noisy mixture. This results in a discrepancy between the terminating distribution of the forward process and the prior used for solving the reverse process at inference. In this paper, we address this discrepancy and propose a forward process based on a Brownian bridge. We show that such a process leads to a reduction of the mismatch compared to previous diffusion processes. More importantly, we show that our approach improves in objective metrics over the baseline process with only half of the iteration steps and having one hyperparameter less to tune",
    "checked": true,
    "id": "8b7c874f88f43377c7aa9108cf08b4884251aac4",
    "semantic_title": "reducing the prior mismatch of stochastic differential equations for diffusion-based speech enhancement",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muller23_interspeech.html": {
    "title": "Complex-valued neural networks for voice anti-spoofing",
    "volume": "main",
    "abstract": "Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the \"In-the-Wild\" anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing",
    "checked": true,
    "id": "a376c59322c5d6f6ee086f1c62a29017a9a8671d",
    "semantic_title": "complex-valued neural networks for voice anti-spoofing",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ristea23_interspeech.html": {
    "title": "DeepVQE: Real Time Deep Voice Quality Enhancement for Joint Acoustic Echo Cancellation, Noise Suppression and Dereverberation",
    "volume": "main",
    "abstract": "Acoustic echo cancellation (AEC), noise suppression (NS) and dereverberation (DR) are an integral part of modern full-duplex communication systems. As the demand for teleconferencing systems increases, addressing these tasks is required for an effective and efficient online meeting experience. Most prior research proposes solutions for these tasks separately, combining them with digital signal processing (DSP) based components, resulting in complex pipelines that are often impractical to deploy in real-world applications. This paper proposes a real-time cross-attention deep model, named DeepVQE, based on residual convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to simultaneously address AEC, NS, and DR. DeepVQE achieves state-of-the-art performance on non-personalized tracks from the 2023 AEC and 2023 DNS Challenge test sets. Moreover, the model runs in real-time and has been successfully tested for the Microsoft Teams platform",
    "checked": true,
    "id": "eeec97727582fb10bd49565fbc7a0fc097913ff7",
    "semantic_title": "deepvqe: real time deep voice quality enhancement for joint acoustic echo cancellation, noise suppression and dereverberation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sawata23_interspeech.html": {
    "title": "Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement",
    "volume": "main",
    "abstract": "Although deep neural network (DNN)-based speech enhancement (SE) methods outperform the previous non-DNN-based ones, they often degrade the perceptual quality of generated outputs. To tackle this problem, we introduce a DNN-based generative refiner, Diffiner, aiming to improve perceptual speech quality pre-processed by an SE method. We train a diffusion based generative model by utilizing a dataset consisting of clean speech only. Then, our refiner effectively mixes clean parts newly generated via denoising diffusion restoration into the degraded and distorted parts caused by a preceding SE method, resulting in refined speech. Once our refiner is trained on a set of clean speech, it can be applied to various SE methods without additional training specialized for each SE module. Therefore, our refiner can be a versatile post-processing module w.r.t. SE methods and has high potential in terms of modularity. Experimental results show that our method improved perceptual speech quality regardless of the preceding SE methods used. Our code is available at https://github.com/sony/diffiner",
    "checked": false,
    "id": "53e3b4ed1ed6a4038f74b55dea9245bb9dffff87",
    "semantic_title": "a versatile diffusion-based generative refiner for speech enhancement",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23h_interspeech.html": {
    "title": "HD-DEMUCS: General Speech Restoration with Heterogeneous Decoders",
    "volume": "main",
    "abstract": "This paper introduces an end-to-end neural speech restoration model, HD-DEMUCS, demonstrating efficacy across multiple distortion environments. Unlike conventional approaches that employ cascading frameworks to remove undesirable noise first and then restore missing signal components, our model performs these tasks in parallel using two heterogeneous decoder networks. Based on the U-Net style encoder-decoder framework, we attach an additional decoder so that each decoder network performs noise suppression or restoration separately. We carefully design each decoder architecture to operate appropriately depending on its objectives. Additionally, we improve performance by leveraging a learnable weighting factor, aggregating the two decoder output waveforms. Experimental results with objective metrics across various environments clearly demonstrate the effectiveness of our approach over a single decoder or multi-stage systems for general speech restoration task",
    "checked": true,
    "id": "356c90133d2e3c5555c5eef1e94902e1436f94d3",
    "semantic_title": "hd-demucs: general speech restoration with heterogeneous decoders",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23e_interspeech.html": {
    "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
    "volume": "main",
    "abstract": "This paper proposes MP-SENet, a novel Speech Enhancement Network which directly denoises Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by convolution-augmented transformers. The encoder aims to encode time-frequency representations from the input noisy magnitude and phase spectra. The decoder is composed of parallel magnitude mask decoder and phase decoder, directly recovering clean magnitude spectra and clean-wrapped phase spectra by incorporating learnable sigmoid activation and parallel phase estimation architecture, respectively. Multi-level losses defined on magnitude spectra, phase spectra, short-time complex spectra, and time-domain waveforms are used to train the MP-SENet model jointly. Experimental results show that our proposed MP-SENet achieves a PESQ of 3.50 on the public VoiceBank+DEMAND dataset and outperforms existing advanced speech enhancement methods",
    "checked": true,
    "id": "85e4a0b4b33c6c78cf1eaec61613136c0a1146ac",
    "semantic_title": "mp-senet: a speech enhancement model with parallel denoising of magnitude and phase spectra",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yin23_interspeech.html": {
    "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
    "volume": "main",
    "abstract": "This paper presents TridentSE, a new and innovative architecture for speech enhancement that efficiently combines local details and global information. The architecture uses time-frequency bin level representation for capturing detailed information and a small number of global tokens for processing global information. It employs cross attention modules to transfer information between the local and global representation, and separates the global tokens into two groups to process inter- and intra-frame information. A metric discriminator is utilized to increase perceptual quality and achieve improved performance compared to previous speech enhancement methods. With lower computational cost, TridentSE achieved a PESQ of 3.47 on the VoiceBank+DEMAND dataset and a PESQ of 3.44 on the DNS no-reverb test set, outperforming most previous methods. Visualization shows that the global tokens demonstrate diverse and interpretable global patterns",
    "checked": true,
    "id": "4f0304ac826ee0c15328deb6eaa7b4d8d5b32c68",
    "semantic_title": "tridentse: guiding speech enhancement with 32 global tokens",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23x_interspeech.html": {
    "title": "Detection of Cross-Dataset Fake Audio Based on Prosodic and Pronunciation Features",
    "volume": "main",
    "abstract": "Existing fake audio detection systems perform well in in-domain testing, but still face many challenges in out-of-domain testing. This is due to the mismatch between the training and test data, as well as the poor generalizability of features extracted from limited views. To address this, we propose multi-view features for fake audio detection, which aim to capture more generalized features from prosodic, pronunciation, and wav2vec dimensions. Specifically, the phoneme duration features are extracted from a pre-trained model based on a large amount of speech data. For the pronunciation features, a Conformer-based phoneme recognition model is first trained, keeping the acoustic encoder part as a deeply embedded feature extractor. FFurthermore, the prosodic and pronunciation features are fused with wav2vec features based on an attention mechanism to improve the generalization of fake audio detection models. Results show that the proposed method achieves significant performance gains in several cross-dataset experiments",
    "checked": true,
    "id": "6c4fe348287b94b02c81e450525c53ea68d4129a",
    "semantic_title": "detection of cross-dataset fake audio based on prosodic and pronunciation features",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dowerah23_interspeech.html": {
    "title": "Self-supervised learning with Diffusion-based multichannel speech enhancement for speaker verification under noisy conditions",
    "volume": "main",
    "abstract": "The paper introduces Diff-Filter, a multichannel speech enhancement approach based on the diffusion probabilistic model, for improving speaker verification performance under noisy and reverberant conditions. It also presents a new two-step training procedure that takes the benefit of self-supervised learning. In the first stage, the Diff-Filter is trained by conducting time-domain speech filtering using a scoring-based diffusion model. In the second stage, the Diff-Filter is jointly optimized with a pre-trained ECAPA-TDNN speaker verification model under a self-supervised learning framework. We present a novel loss based on equal error rate. This loss is used to conduct self-supervised learning on a dataset that is not labelled in terms of speakers. The proposed approach is evaluated on MultiSV, a multichannel speaker verification dataset, and shows significant improvements in performance under noisy multichannel conditions",
    "checked": true,
    "id": "12117a75e8963d1883fc8f269bdf25423fe078dc",
    "semantic_title": "self-supervised learning with diffusion-based multichannel speech enhancement for speaker verification under noisy conditions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nespoli23_interspeech.html": {
    "title": "Two-Stage Voice Anonymization for Enhanced Privacy",
    "volume": "main",
    "abstract": "In recent years, the need for privacy preservation when manipulating or storing personal data, including speech , has become a major issue. In this paper, we present a system addressing the speaker-level anonymization problem. We propose and evaluate a two-stage anonymization pipeline exploiting a state-of-the-art anonymization model described in the Voice Privacy Challenge 2022 in combination with a zero-shot voice conversion architecture able to capture speaker characteristics from a few seconds of speech. We show this architecture can lead to strong privacy preservation while preserving pitch information. Finally, we propose a new compressed metric to evaluate anonymization systems in privacy scenarios with different constrains on privacy and utility",
    "checked": true,
    "id": "4475dea9debb5f8294faa89e1d568a97b34e9172",
    "semantic_title": "two-stage voice anonymization for enhanced privacy",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23h_interspeech.html": {
    "title": "Personalized Dereverberation of Speech",
    "volume": "main",
    "abstract": "Classic non-blind speech dereverberation methods produce high-quality results only when the precise impulse response is known. Alternatively, learning-based blind methods cannot ensure adequate dereverberation in all environments. We propose an environment- and speaker-specific approach combining the advantages of both approaches. With a simple, one-time personalization step, our model generalizes a single measured impulse response to its spatial neighborhood. Specifically, the two-stage model performs feature-based Wiener deconvolution followed by a network-based refinement. Extensive experimental results indicate that our approach quantitatively and qualitatively outperforms the state-of-the-art methods. Additional user studies confirm that our method is overwhelmingly favored by listeners",
    "checked": true,
    "id": "bf6687d4c3e5e2b99f6d5e1704b808e312c8d445",
    "semantic_title": "personalized dereverberation of speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/binhthien23_interspeech.html": {
    "title": "Weighted Von Mises Distribution-based Loss Function for Real-time STFT Phase Reconstruction Using DNN",
    "volume": "main",
    "abstract": "This paper presents improvements to real-time phase reconstruction using deep neural networks (DNNs). The advantage of DNN-based approaches in phase reconstruction is that they can leverage prior knowledge from data and are adaptable to real-time applications by using causal models. However, conventional DNN-based methods do not consider the varying properties of the phase at different time-frequency bins. Our paper proposes loss functions for phase reconstruction that incorporate frequency-specific and amplitude weights to distinguish the importance of phase elements based on their properties. We also use an extension of the group delay to improve the phase connections along the frequency. To improve the generalization, we augment the data by randomly shifting the signals in the time domain for each epoch during training. Experimental results show the superior performance of the proposed methods compared to conventional DNN-based and non-DNN real-time phase reconstruction methods",
    "checked": true,
    "id": "f3c6c14bc231c4c9efc015b2537f6d74e364d832",
    "semantic_title": "weighted von mises distribution-based loss function for real-time stft phase reconstruction using dnn",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schroter23_interspeech.html": {
    "title": "Deep Multi-Frame Filtering for Hearing Aids",
    "volume": "main",
    "abstract": "Multi-frame algorithms for single-channel speech enhancement are able to take advantage from short-time correlations within the speech signal. Deep filtering (DF) recently demonstrated its capabilities for low-latency scenarios like hearing aids with its complex multi-frame (MF) filter. Alternatively, the complex filter can be estimated via an MF minimum variance distortionless response (MVDR), or MF Wiener filter (WF). Previous studies have shown that incorporating algorithm domain knowledge using an MVDR filter might be beneficial compared to the direct filter estimation via DF. In this work, we compare the usage of various multi-frame filters such as DF, MF-MVDR, or MF-WF for HAs. We assess different covariance estimation methods for both MF-MVDR and MF-WF and objectively demonstrate an improved performance compared to direct DF estimation, significantly outperforming related work while improving the runtime performance",
    "checked": true,
    "id": "e68dcbc90134007fee2e22d575a36b67d3a24fae",
    "semantic_title": "deep multi-frame filtering for hearing aids",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiong23_interspeech.html": {
    "title": "Aligning Speech Enhancement for Improving Downstream Classification Performance",
    "volume": "main",
    "abstract": "Speech-based classification models in the cloud are gaining large-scale adoption. In many applications where post-deployment background noise conditions mismatch those used during model training, fine-tuning the original model on local data would likely improve performance. However, this is not always possible as the local user may not be authorized to modify the cloud-based model or the local user may be unable to share the data and corresponding labels required for fine-tuning. In this paper, we propose a denoiser stored locally on edge devices with an application-specific training scheme. It learns a custom speech enhancement scheme that aligns the local denoiser with the downstream model, without requiring access to the cloud-based weights. We evaluate the denoiser with a common classification task - keyword spotting - and demonstrate using two different architectures that the proposed scheme outperforms common speech enhancement models for different types of background noise",
    "checked": true,
    "id": "a0a50bd6d96e116a12c2d18b16748bdeffeb3de1",
    "semantic_title": "aligning speech enhancement for improving downstream classification performance",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23b_interspeech.html": {
    "title": "DNN-based Parameter Estimation for MVDR Beamforming and Post-filtering",
    "volume": "main",
    "abstract": "Multi-channel speech enhancement systems usually consist of spatial filtering such as minimum-variance distortionless-response (MVDR) beamforming and post-processing, which require acoustic parameters including relative transfer function (RTF), noise spatial covariance matrix (SCM), and a priori and a posteriori signal-to-noise ratios (SNRs). In this paper, we propose a deep neural network (DNN)-based parameter estimation for MVDR beamforming and post-filtering. Specifically, we propose to use a DNN to estimate the interchannel phase differences of the clean speech and the speech presence probability (SPP), which are used to estimate the RTF and the noise SCM for MVDR beamforming. As for the post-processing, we adopt the iDeepMMSE framework in which another DNN is employed to estimate the a priori SNR, speech power spectral density, and SPP used to compute spectral gains. The proposed method outperformed several previous approaches especially in the PESQ scores for the CHiME-4 dataset",
    "checked": true,
    "id": "a691c60cdd26713bc87ecc45eefcfc8cae91b57b",
    "semantic_title": "dnn-based parameter estimation for mvdr beamforming and post-filtering",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23b_interspeech.html": {
    "title": "FRA-RIR: Fast Random Approximation of the Image-source Method",
    "volume": "main",
    "abstract": "The training of modern speech processing systems often requires a large amount of simulated room impulse response (RIR) data to generalize well in real-world environments. However, simulating realistic RIR typically requires accurate physical modeling, and the acceleration of such process typically requires certain computational platforms. In this paper, we propose fast random approximation of room impulse response (FRA-RIR) to efficiently generate realistic RIR data without specific computational devices. FRA-RIR replaces the physical simulation by a series of random approximations, which significantly speeds up the simulation process and enables fully on-the-fly simulation when training neural networks. Experiments show that FRA-RIR is not only significantly faster than other existing ISM-based tools on standard platforms, but also improves the performance of speech denoising systems evaluated on real-world RIRs. The implementation is available online",
    "checked": true,
    "id": "98e640d75abb8c1511c053ca5b27d33b567c13bd",
    "semantic_title": "fra-rir: fast random approximation of the image-source method",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23b_interspeech.html": {
    "title": "Rethinking Complex-Valued Deep Neural Networks for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Despite efforts made to adopt complex-valued deep neural networks (CVDNNs), it remains unclear whether CVDNNs are generally more effective than real-valued DNNs (RVDNNs) for speech enhancement. This study systematically examines CVDNNs against their real-valued counterparts in monaural scenarios. We first investigate atomic units of CVDNNs against those of RVDNNs. We find the use of complex-valued operations hinders model capacity when model size is small. Moreover, we show that two notable CVDNNs, deep complex convolutional recurrent network (DCCRN) and deep complex U-Net (DCUNET), produce identical performance to their real-valued counterparts while requiring more computation. Our experimental results show that those CVDNNs do not provide a performance gain over RVDNNs for monaural speech enhancement, and are less desirable due to higher computational cost. This study suggests that it is more than nontrivial to rethink the efficacy of CVDNNs for speech enhancement",
    "checked": true,
    "id": "a99421382884171ced632c00f2fd7d2b22d7e540",
    "semantic_title": "rethinking complex-valued deep neural networks for monaural speech enhancement",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/le23_interspeech.html": {
    "title": "Harmonic enhancement using learnable comb filter for light-weight full-band speech enhancement model",
    "volume": "main",
    "abstract": "With fewer feature dimensions, filter banks are often used in light-weight full-band speech enhancement models. In order to further enhance the coarse speech in the sub-band domain, it is necessary to apply a post-filtering for harmonic retrieval. The signal processing-based comb filters used in RNNoise and PecepNet have limited performance and may cause speech quality degradation due to inaccurate fundamental frequency estimation. To tackle this problem, we propose a learnable comb filter to enhance harmonics. Based on the sub-band model, we design a DNN-based fundamental frequency estimator to estimate the discrete fundamental frequencies and a comb filter for harmonic enhancement, which are trained via an end-to-end pattern. The experiments show the advantages of our proposed method over state-of-the-art PecepMet amd DeepFilterNet",
    "checked": true,
    "id": "b782db39470683fa434893f306c79cd06d02e160",
    "semantic_title": "harmonic enhancement using learnable comb filter for light-weight full-band speech enhancement model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23e_interspeech.html": {
    "title": "How Does Pretraining Improve Discourse-Aware Translation?",
    "volume": "main",
    "abstract": "Pretrained language models (PLMs) have produced substantial improvements in discourse-aware neural machine translation (NMT), for example, improved coherence in spoken language translation. However, the underlying reasons for their strong performance have not been well explained. To bridge this gap, we introduce a probing task to interpret the ability of PLMs to capture discourse relation knowledge. We validate three state-of-the-art PLMs across encoder-, decoder-, and encoder-decoder-based models. The analysis shows that (1) the ability of PLMs on discourse modelling varies from architecture and layer; (2) discourse elements in a text lead to different learning difficulties for PLMs. Besides, we investigate the effects of different PLMs on spoken language translation. Through experiments on IWSLT2017 Chinese-English dataset, we empirically reveal that NMT models initialized from different layers of PLMs exhibit the same trends with the probing task",
    "checked": true,
    "id": "b46aabea46dda8bd3c8a82af1143411872d94be2",
    "semantic_title": "how does pretraining improve discourse-aware translation?",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23t_interspeech.html": {
    "title": "PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction",
    "volume": "main",
    "abstract": "Speech-to-text errors made by automatic speech recognition (ASR) systems negatively impact downstream models. Error correction models as a post-processing text editing method have been recently developed for refining the ASR outputs. However, efficient models that meet the low latency requirements of industrial grade production systems have not been well studied. We propose PATCorrect-a novel non-autoregressive (NAR) approach based on multi-modal fusion leveraging representations from both text and phoneme modalities, to reduce word error rate (WER) and perform robustly with varying input transcription quality. We demonstrate that PATCorrect consistently outperforms state-of-the-art NAR method on English corpus across different upstream ASR systems, with an overall 11.62% WER reduction (WERR) compared to 9.46% WERR achieved by other methods using text only modality. Besides, its inference latency is at tens of milliseconds, making it ideal for systems with low latency requirements",
    "checked": true,
    "id": "54d3de259781262e586218dcf05a15cf67441987",
    "semantic_title": "patcorrect: non-autoregressive phoneme-augmented transformer for asr error correction",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tseng23_interspeech.html": {
    "title": "Model-assisted Lexical Tone Evaluation of three-year-old Chinese-speaking Children by also Considering Segment Production",
    "volume": "main",
    "abstract": "This paper presents a hybrid workflow for lexical tone evaluation of 3-year-old Chinese-speaking children. The speech data of 123 children were phonetically transcribed for phoneme accuracy as well as perceptually evaluated for tone accuracy by human judgement. A transformer-based tone model with a BERT input architecture was built using the speech data and tested on twelve children with low speech performance. The accuracy rates between the judged tones and the predicted tones output by our model were high for the overall evaluation. More consistent patterns between judged and predicted tones were observed for high-register Tone 1 and Tone 4 for low-register Tone 2 and Tone 3. We also found that a child's tone production ability is consistently reflected in relation to consonants, vowels, and syllables. Tone accuracy is more related to vowel accuracy than consonant accuracy. In particular, the most diverse differences in tone, consonant, and vowel accuracies were observed for Tone 3",
    "checked": true,
    "id": "277acd215301d54ec1d0ec92b199205c85ee5f2b",
    "semantic_title": "model-assisted lexical tone evaluation of three-year-old chinese-speaking children by also considering segment production",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23b_interspeech.html": {
    "title": "Sentence Embedder Guided Utterance Encoder (SEGUE) for Spoken Language Understanding",
    "volume": "main",
    "abstract": "The pre-trained speech encoder wav2vec 2.0 performs very well on various spoken language understanding (SLU) tasks. However, on many tasks, it trails behind text encoders with textual input. To improve the understanding capability of SLU encoders, various studies have used knowledge distillation to transfer knowledge from natural language understanding (NLU) encoders. We use a very simple method of distilling from a textual sentence embedder directly into wav2vec 2.0 as pre-training, utilizing paired audio-text datasets. We observed that this method is indeed capable of improving SLU task performance in fine-tuned settings, as well as full-data and few-shot transfer on a frozen encoder. However, the model performs worse on certain tasks highlighting the strengths and weaknesses of our approach",
    "checked": true,
    "id": "10c4d3cd98981bbff20d7da7109afb4f1fe927aa",
    "semantic_title": "sentence embedder guided utterance encoder (segue) for spoken language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23u_interspeech.html": {
    "title": "Joint Time and Frequency Transformer for Chinese Opera Classification",
    "volume": "main",
    "abstract": "Transformer has recently gained more attention and is widely used in audio tasks. Most tasks compute attention directly over the entire time-frequency space or only in the temporal. This paper presents a joint time and frequency model for Chinese opera classification. A shallow convolutional block is used to get localized low-level semantic features and reduce the feature map size. Moreover, the criss-cross attention and the factorised self-attention are employed in the model to extract the time and frequency space representation. The experiment results demonstrate that the proposed model achieves state-of-the-art performance on a large Chinese opera dataset with fewer model parameters",
    "checked": true,
    "id": "0038b90c77ec940c0412a3d14a49e1f822af3009",
    "semantic_title": "joint time and frequency transformer for chinese opera classification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23_interspeech.html": {
    "title": "AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination",
    "volume": "main",
    "abstract": "Many recent loss functions in deep metric learning are expressed with logarithmic and exponential forms, and they involve margin and scale as essential hyper-parameters. Since each data class has an intrinsic characteristic, several previous works have tried to learn embedding space close to the real distribution by introducing adaptive margins. However, there was no work on adaptive scales at all. We argue that both margin and scale should be adaptively adjustable during the training. In this paper, we propose a method called Adaptive Margin and Scale (AdaMS), where hyper-parameters of margin and scale are replaced with learnable parameters of adaptive margins and adaptive scales for each class. Our method is evaluated on Wall Street Journal dataset, and we achieve outperforming results for word discrimination tasks",
    "checked": false,
    "id": "940e8e726c7f1bcc8aa09e415d4161515fb21b41",
    "semantic_title": "deep metric learning with adaptive margin and adaptive scale for acoustic word discrimination",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arvan23_interspeech.html": {
    "title": "Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective",
    "volume": "main",
    "abstract": "Reproducibility is a key aspect for scientific advancement across disciplines, and reducing barriers for open science is a focus area for the theme of Interspeech 2023. Availability of source code is one of the indicators that facilitates reproducibility. However, less is known about the rates of reproducibility at Interspeech conferences in comparison to other conferences in the field. In order to fill this gap, we have surveyed 27,717 papers at seven conferences across speech and language processing disciplines. We find that despite having a close number of accepted papers to the other conferences, Interspeech has up to 40% less source code availability. In addition to reporting the difficulties we have encountered during our research, we also provide recommendations and possible directions to increase reproducibility for further studies",
    "checked": true,
    "id": "d8c78b91533f1705894c123d3a41d1861e06ab87",
    "semantic_title": "investigating reproducibility at interspeech conferences: a longitudinal and comparative perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/biswas23_interspeech.html": {
    "title": "An Efficient Approach for the Automated Segmentation and Transcription of the People's Speech Sorpus",
    "volume": "main",
    "abstract": "Advancements in speech technology have led to the integration of modern ASR systems into various applications such as chatbots, medical dictation, video transcription etc. Conversational ASR training requires speech that captures the acoustic cues of spontaneous speech. With its 30k hours of conversational speech, the People's Speech corpus is the largest available spontaneous and conversational corpus and an invaluable resource for such training. In addition, it comes with a commercial friendly license. The corpus is packaged in uniform 15-second segments, but this can lead to abrupt cutting off of speech and transcription that is not always accurate. This paper presents an effective method for automatic data mining from a small subset of 973 raw original records used by the People's Speech corpus. The paper also proposes an approach for outlier detection and automatic data curation. Results show a 19.7% relative improvement in WER compared to the original segments",
    "checked": true,
    "id": "7c23a818f88936e86855539b8022b1f792f6d6a4",
    "semantic_title": "an efficient approach for the automated segmentation and transcription of the people's speech sorpus",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23g_interspeech.html": {
    "title": "Diverse Feature Mapping and Fusion via Multitask Learning for Multilingual Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In addition to linguistic information, speech contains non-lexical information, such as emotion, gender, and speaker identity. Recent self-supervised learning methods for speech representation can provide powerful initial feature spaces. However, a few training samples in speech emotion recognition cannot fully utilize the vast pretrained feature space. Herein, we propose an effective use of the feature space. First, to obtain more complementary information, diverse features are extracted by mapping the same utterance to different clusters via multitask learning. Thereafter, fusion methods are investigated according to the correlation among the diversely mapped features. The proposed methods are evaluated on two emotional speech corpora. The experimental results show that the proposed methods can effectively utilize the vast pretrained feature space and achieve state-of-the-art performance, with an unweighted average recall of 78.45% on the benchmark IEMOCAP corpus",
    "checked": true,
    "id": "1d86b2ac9443fde0b3ce116e013035a6f0cd2983",
    "semantic_title": "diverse feature mapping and fusion via multitask learning for multilingual speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bahar23_interspeech.html": {
    "title": "Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text",
    "volume": "main",
    "abstract": "Automatic Arabic diacritization is useful in many applications, ranging from reading support for language learners to accurate pronunciation predictor for downstream tasks like speech synthesis. While most of the previous works focused on models that operate on raw non-diacritized text, production systems can gain accuracy by first letting humans partly annotate ambiguous words. In this paper, we propose 2SDiac, a multi-source model that can effectively support optional diacritics in input to inform all predictions. We also introduce Guided Learning, a training scheme to leverage given diacritics in input with different levels of random masking. We show that the provided hints during test affect more output positions than those annotated. Moreover, experiments on two common benchmarks show that our approach i) greatly outperforms the baseline also when evaluated on non-diacritized text; and ii) achieves state-of-the-art results while reducing the parameter count by over 60%",
    "checked": true,
    "id": "ebcc5d24fc09b04689a3756042bba2a495da824b",
    "semantic_title": "take the hint: improving arabic diacritization with partially-diacritized text",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23e_interspeech.html": {
    "title": "Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin",
    "volume": "main",
    "abstract": "Developing effective spoken language processing systems for low-resource languages poses several challenges due to the lack of parallel data and limited resources for fine-tuning models. In this work, we target on improving upon both text classification and translation of Nigerian Pidgin (Naija) by collecting a large-scale parallel English-Pidgin corpus and further propose a framework of cross-lingual adaptive training that includes both continual and task adaptive training so as to adapt a base pre-trained model to low-resource languages. Our studies show that English pre-trained language models serve as a stronger prior than multilingual language models on English-Pidgin tasks with up to 2.38 BLEU improvements; and demonstrate that augmenting orthographic data and using task adaptive training with back-translation can have a significant impact on model performance",
    "checked": true,
    "id": "72c4cb0dceadfcbc985d13204a31e4a9e785731b",
    "semantic_title": "low-resource cross-lingual adaptive training for nigerian pidgin",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23j_interspeech.html": {
    "title": "Efficient Adaptation of Spoken Language Understanding based on End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In production scenarios that require frequent change, it is inefficient to repeatedly train and update the entire End-to-end (E2E) model for spoken language understanding (SLU). In this paper, we present a study on efficiently adapting E2E SLU models based on pre-trained ASR model. Specifically, we propose the ASR-based E2E SLU model integrating an additional decoder for SLU and a fusion module that incorporates acoustic representation from the shared encoder and text transcript representation from ASR decoder. Furthermore, we investigate the effectiveness of an adapter module that fine-tunes only a small number of parameters for semantic and tran- script predictions. The experimental results show that the proposed model outperforms other competitive baselines in intent accuracy, SLU F1 score and word error rate (WER) on FSC, SLURP, and Samsung in-house SLU datasets",
    "checked": true,
    "id": "56abe7d4034220d788f4491e3aa82247a4f6c46c",
    "semantic_title": "efficient adaptation of spoken language understanding based on end-to-end automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23d_interspeech.html": {
    "title": "PhonMatchNet: Phoneme-Guided Zero-Shot Keyword Spotting for User-Defined Keywords",
    "volume": "main",
    "abstract": "This study presents a novel zero-shot user-defined keyword spotting model that utilizes the audio-phoneme relationship of the keyword to improve performance. Unlike the previous approach that estimates at utterance level, we use both utterance and phoneme level information. Our proposed method comprises a two-stream speech encoder architecture, self-attention-based pattern extractor, and phoneme-level detection loss for high performance in various pronunciation environments. Based on experimental results, our proposed model outperforms the baseline model and achieves competitive performance compared with full-shot keyword spotting models. Our proposed model significantly improves the EER and AUC across all datasets, including familiar words, proper nouns, and indistinguishable pronunciations, with an average relative improvement of 67% and 80%, respectively. The implementation code of our proposed model is available at https://github.com/ncsoft/PhonMatchNet",
    "checked": true,
    "id": "60a068271f0bb7d8b171d25a5cc64f44829cd555",
    "semantic_title": "phonmatchnet: phoneme-guided zero-shot keyword spotting for user-defined keywords",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23_interspeech.html": {
    "title": "Mix before Align: Towards Zero-shot Cross-lingual Sentiment Analysis via Soft-Mix and Multi-View Learning",
    "volume": "main",
    "abstract": "Due to the insufficient sentiment corpus in many languages, recent studies have proposed cross-lingual sentiment analysis to transfer sentiment analysis models from rich-resource languages to low-resource ones. However, existing models heavily rely on code-switched sentences to reduce the alignment discrepancy of cross-lingual embeddings, which could be limited by their inherent constraints. In this paper, we propose a novel method dubbed SOUL (short for Softmix and Multiview learning) to enhance zero-shot cross-lingual sentiment analysis. Instead of using the embeddings of code-switched sentences directly, SOUL first mixes them softly with the embeddings of original sentences. Furthermore, SOUL utilizes multi-view learning to encourage contextualized embeddings to align into a refined language-invariant space. Experimental results on four cross-lingual benchmarks across five languages clearly verify the effectiveness of our proposed SOUL",
    "checked": true,
    "id": "0987c12e280c39e27245969eedc33b900cfa019b",
    "semantic_title": "mix before align: towards zero-shot cross-lingual sentiment analysis via soft-mix and multi-view learning",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/papi23_interspeech.html": {
    "title": "AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "Attention is the core mechanism of today's most used architectures for natural language processing and has been analyzed from many perspectives, including its effectiveness for machine translation-related tasks. Among these studies, attention resulted to be a useful source of information to get insights about word alignment also when the input text is substituted with audio segments, as in the case of the speech translation (ST) task. In this paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that exploits the attention information to generate source-target alignments that guide the model during inference. Through experiments on the 8 language pairs of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art SimulST policies applied to offline-trained models with gains in terms of BLEU of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8 languages",
    "checked": true,
    "id": "e16da0a803c7a2df4820dd48bd5eb785ddc55232",
    "semantic_title": "alignatt: using attention-based audio-translation alignments as a guide for simultaneous speech translation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/polak23_interspeech.html": {
    "title": "Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff",
    "volume": "main",
    "abstract": "Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed - this scheme cannot directly show a single incremental translation to users. Further, this method lacks mechanisms for controlling the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-n policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality",
    "checked": true,
    "id": "d24d60719e90e69749a75c160cb760d1d9fca44a",
    "semantic_title": "incremental blockwise beam search for simultaneous speech translation with controllable quality-latency tradeoff",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sikasote23_interspeech.html": {
    "title": "Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages",
    "volume": "main",
    "abstract": "This work introduces Zambezi Voice, an open-source multilingual speech resource for Zambian languages. It contains two collections of datasets: unlabelled audio recordings of radio news and talk shows programs (160 hours) and labelled data (over 80 hours) consisting of read speech recorded from text sourced from publicly available literature books. The dataset is created for speech recognition but can be extended to multilingual speech processing research for both supervised and unsupervised learning approaches. To our knowledge, this is the first multilingual speech dataset created for Zambian languages. We exploit pretraining and cross-lingual transfer learning by finetuning the Wav2Vec2.0 large-scale multilingual pretrained model to build end-to-end (E2E) speech recognition models for our baseline models. The dataset is released publicly under a Creative Commons BY-NC-ND 4.0 license and can be accessed through the project repository",
    "checked": true,
    "id": "4488203f1aa489c6e4ebd48d87b3feb2e211261b",
    "semantic_title": "zambezi voice: a multilingual speech corpus for zambian languages",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mun23_interspeech.html": {
    "title": "Towards Single Integrated Spoofing-aware Speaker Verification Embeddings",
    "volume": "main",
    "abstract": "This study aims to develop a single integrated spoofing-aware speaker verification (SASV) embeddings that satisfy two aspects. First, rejecting non-target speakers' input as well as target speakers' spoofed inputs should be addressed. Second, competitive performance should be demonstrated compared to the fusion of automatic speaker verification (ASV) and countermeasure (CM) embeddings, which outperformed single embedding solutions by a large margin in the SASV2022 challenge. We analyze that the inferior performance of single SASV embeddings comes from insufficient amount of training data and distinct nature of ASV and CM tasks. To this end, we propose a novel framework that includes multi-stage training and a combination of loss functions. Copy synthesis, combined with several vocoders, is also exploited to address the lack of spoofed data. Experimental results show dramatic improvements, achieving an SASV-EER of 1.06% on the evaluation protocol of the SASV2022 challenge",
    "checked": true,
    "id": "1245bd948db146d9cbe3445fe713f8ea5273e5c9",
    "semantic_title": "towards single integrated spoofing-aware speaker verification embeddings",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ba_interspeech.html": {
    "title": "Pseudo-Siamese Network based Timbre-reserved Black-box Adversarial Attack in Speaker Identification",
    "volume": "main",
    "abstract": "In this study, we propose a timbre-reserved adversarial attack approach for speaker identification (SID) to not only exploit the weakness of the SID model but also preserve the timbre of the target speaker in a black-box attack setting. Particularly, we generate timbre-reserved fake audio by adding an adversarial constraint during the training of the voice conversion model. Then, we leverage a pseudo-Siamese network architecture to learn from the black-box SID model constraining both intrinsic similarity and structural similarity simultaneously. The intrinsic similarity loss is to learn an intrinsic invariance, while the structural similarity loss is to ensure that the substitute SID model shares a similar decision boundary to the fixed black-box SID model. The substitute model can be used as a proxy to generate timbre-reserved fake audio for attacking. Experimental results on the Audio Deepfake Detection (ADD) challenge dataset indicate that the attack success rate of our proposed approach yields up to 60.58% and 55.38% in the white-box and black-box scenarios, respectively, and can deceive both human beings and machines",
    "checked": true,
    "id": "a010d9e871f1b2423b3076983aefea8c82e656a8",
    "semantic_title": "pseudo-siamese network based timbre-reserved black-box adversarial attack in speaker identification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23v_interspeech.html": {
    "title": "Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion",
    "volume": "main",
    "abstract": "Audio Deepfake Detection (ADD) aims to detect the fake audio generated by text-to-speech (TTS), voice conversion (VC) and replay, etc., which is an emerging topic. Traditionally we take the mono signal as input and focus on robust feature extraction and effective classifier design. However, the dual-channel stereo information in the audio signal also includes important cues for deepfake, which has not been studied in the prior work. In this paper, we propose a novel ADD model, termed as M2S-ADD, that attempts to discover audio authenticity cues during the mono-to-stereo conversion process. We first projects the mono to a stereo signal using a pretrained stereo synthesizer, then employs a dual-branch neural architecture to process the left and right channel signals, respectively. In this way, we effectively reveal the artifacts in the fake audio, thus improve the ADD performance. The experiments on the ASVspoof2019 database show that M2S-ADD outperforms all baselines that input mono. We release the source code at https://github.com/AI-S2-Lab/M2S-ADD",
    "checked": true,
    "id": "7ebff41444a4deaf34f4d233e121ba8bfd273580",
    "semantic_title": "betray oneself: a novel audio deepfake detection model via mono-to-stereo conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23v_interspeech.html": {
    "title": "Robust Audio Anti-spoofing Countermeasure with Joint Training of Front-end and Back-end Models",
    "volume": "main",
    "abstract": "The accuracy and reliability of many speech processing systems may deteriorate under noisy conditions. This paper discusses robust audio anti-spoofing countermeasure for audio in noisy environments. Firstly, we attempt to use a pre-trained speech enhancement model as the front-end module and build a cascaded system. However, the independent denoising process of enhancement models may distort the synthesis artifacts or anti-spoofing related information included in utterances, leading to performance degradation. Therefore, we proposes a new framework for robust audio anti-spoofing by joint training the integrated speech enhancement front-end and anti-spoofing back-end. The final results demonstrate that the joint training framework is more effective than the cascaded framework. Additionally, we propose a cross-joint training scheme, which allows the single-model performance to exceed the result of score level fusion, making the joint framework more effective and efficient",
    "checked": true,
    "id": "fb91b6fdce692c3d97022bed202bdc61b1ea1bd9",
    "semantic_title": "robust audio anti-spoofing countermeasure with joint training of front-end and back-end models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kawa23b_interspeech.html": {
    "title": "Improved DeepFake Detection Using Whisper Features",
    "volume": "main",
    "abstract": "With a recent influx of voice generation methods, the threat introduced by audio DeepFake (DF) is ever-increasing. Several different detection methods have been presented as a countermeasure. Many methods are based on so-called front-ends, which, by transforming the raw audio, emphasize features crucial for assessing the genuineness of the audio sample. Our contribution contains investigating the influence of the state-of-the-art Whisper automatic speech recognition model as a DF detection front-end. We compare various combinations of Whisper and well-established front-ends by training 3 detection models (LCNN, SpecRNet, and MesoNet) on a widely used ASVspoof 2021 DF dataset and later evaluating them on the DF In-The-Wild dataset. We show that using Whisper-based features improves the detection for each model and outperforms recent results on the In-The-Wild dataset by reducing Equal Error Rate by 21%",
    "checked": true,
    "id": "8d87f4c7abab966ed86403d5b4b21222dfb6c3da",
    "semantic_title": "improved deepfake detection using whisper features",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23c_interspeech.html": {
    "title": "DoubleDeceiver: Deceiving the Speaker Verification System Protected by Spoofing Countermeasures",
    "volume": "main",
    "abstract": "Automatic Speaker Verification (ASV) systems are vulnerable to various attacks, especially spoofing attacks, and therefore are typically protected by spoofing countermeasures. However, both spoofing countermeasures and ASV models are vulnerable to adversarial attacks. We propose DoubleDeceiver - a novel black-box attack method that incorporates text-to-speech synthesis and adversarial attack to deceive ASV systems even with the protection of spoofing countermeasures. Although the surrogate models and victim models differ in architectures, DoubleDeceiver achieved a successful attack rate (SAR) as high as 98.3%. DoubleDeceiver identified the vulnerabilities of ASV systems and issued a warning that solely relying on the spoofing countermeasures is not reliable to protect ASV systems' security. This work encourages the development of more secure anti-spoofing and ASV systems by highlighting the need to consider composite attacks in future designs",
    "checked": true,
    "id": "68d92126af0400732e06fb018bfecd23d473275e",
    "semantic_title": "doubledeceiver: deceiving the speaker verification system protected by spoofing countermeasures",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panchapagesan23_interspeech.html": {
    "title": "On Training a Neural Residual Acoustic Echo Suppressor for Improved ASR",
    "volume": "main",
    "abstract": "Acoustic Echo Cancellation (AEC) is critical for accurate recognition of speech directed at a smart device playing audio. Previous work has showed that neural AEC models can significantly improve Automatic Speech Recognition (ASR) accuracy. In this paper, we train a conformer-based waveform-domain neural model to perform residual acoustic echo suppression (RAES) on the output of a linear AEC. We focus specifically on improving ASR accuracy in realistic mismatched test conditions, when training on large-scale simulated training data, as needed for production voice-interaction systems. Our key finding is that instead of naively using the best evaluation-time linear AEC configuration during neural RAES model training, using a weaker linear AEC generalizes significantly better, with 17-30% lower word error rate (WER) on a realistic re-recorded test set. Overall, the neural RAES model yields 38-53% WER reduction over the linear AEC alone",
    "checked": true,
    "id": "ead352ed127e4e54103c74f8a47dcc8c18eedad2",
    "semantic_title": "on training a neural residual acoustic echo suppressor for improved asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lemercier23_interspeech.html": {
    "title": "Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation",
    "volume": "main",
    "abstract": "In this paper, we present a scheme for extending deep neural network-based multiplicative maskers to deep subband filters for speech restoration in the time-frequency domain. The resulting method can be generically applied to any deep neural network providing masks in the time-frequency domain, while requiring only few more trainable parameters and a computational overhead that is negligible for state-of-the-art neural networks. We demonstrate that the resulting deep subband filtering scheme outperforms multiplicative masking for dereverberation, while leaving the denoising performance virtually the same. We argue that this is because deep subband filtering in the time-frequency domain fits the subband approximation often assumed in the dereverberation literature, whereas multiplicative masking corresponds to the narrowband approximation generally employed for denoising",
    "checked": true,
    "id": "56c69ac3a35e3ab3bbe0422bc1307f24a7ceffd1",
    "semantic_title": "extending dnn-based multiplicative masking to deep subband filtering for improved dereverberation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23b_interspeech.html": {
    "title": "UnSE: Unsupervised Speech Enhancement Using Optimal Transport",
    "volume": "main",
    "abstract": "Most deep learning-based speech enhancement methods usually use supervised learning, which requires massive noisy-to-clean training pairs. However, the synthesized training data can only partially cover some realistic environments, and it is generally difficult or almost impossible to collect pairs of noisy and ground-truth clean speech in some scenarios. To address this problem, we propose an unsupervised speech enhancement method that does not require any paired noisy-to-clean training data. Specifically, based on the optimal transport criterion, the speech enhancement model is trained in an unsupervised manner only using a noisy speech based fidelity loss and a distribution divergence loss, by which the divergence between the output and (unpaired) clean speech is minimized. Experimental results show that the proposed unsupervised method can achieve competitive performance with supervised methods on the VCTK + DEMAND benchmark and better performance on the CHiME4 benchmark",
    "checked": true,
    "id": "f56623f1febf6c4b11ad5d7f7098c0b0d682f12f",
    "semantic_title": "unse: unsupervised speech enhancement using optimal transport",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23k_interspeech.html": {
    "title": "MC-SpEx: Towards Effective Speaker Extraction with Multi-Scale Interfusion and Conditional Speaker Modulation",
    "volume": "main",
    "abstract": "The previous SpEx+ has yielded outstanding performance in speaker extraction and attracted much attention. However, it still encounters inadequate utilization of multi-scale information and speaker embedding. To this end, this paper proposes a new effective speaker extraction system with multi-scale interfusion and conditional speaker modulation (ConSM), which is called MC-SpEx. First of all, we design the weight-share multi-scale fusers (ScaleFusers) for efficiently leveraging multi-scale information as well as ensuring consistency of the model's feature space. Then, to consider different scale information while generating masks, the multi-scale interactive mask generator (ScaleInterMG) is presented. Moreover, we introduce ConSM module to fully exploit speaker embedding in the speech extractor. Experimental results on the Libri2Mix dataset demonstrate the effectiveness of our improvements and the state-of-the-art performance of our proposed MC-SpEx",
    "checked": true,
    "id": "86550f28c3d43ffc7a6b8477545fecf994384ad4",
    "semantic_title": "mc-spex: towards effective speaker extraction with multi-scale interfusion and conditional speaker modulation",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bartolewska23_interspeech.html": {
    "title": "Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement",
    "volume": "main",
    "abstract": "The aim of speech enhancement is to improve speech signal quality and intelligibility from a noisy microphone signal. In many applications, it is crucial to enable processing with small computational complexity and minimal requirements regarding access to future signal samples (look-ahead). This paper presents signal-based causal DCCRN that improves online single-channel speech enhancement by reducing the required look-ahead and the number of network parameters. The proposed modifications include complex filtering of the signal, application of overlapped-frame prediction, causal convolutions and deconvolutions, and modification of the loss function. Results of performed experiments indicate that the proposed model with overlapped signal prediction and additional adjustments, achieves similar or better performance than the original DCCRN in terms of various speech enhancement metrics, while it reduces the latency and network parameter number by around 30%",
    "checked": true,
    "id": "a129eff35330cf099072de77e4e359c5a68b52b3",
    "semantic_title": "causal signal-based dccrn with overlapped-frame prediction for online speech enhancement",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23q_interspeech.html": {
    "title": "Gesper: A Restoration-Enhancement Framework for General Speech Reconstruction",
    "volume": "main",
    "abstract": "This paper describes a real-time General Speech Reconstruction (Gesper) system submitted to the ICASSP 2023 Speech Signal Improvement (SSI) Challenge. This novel proposed system is a two-stage architecture, in which the speech restoration is performed, and then cascaded by speech enhancement. We propose a complex spectral mapping-based generative adversarial network (CSM-GAN) as the speech restoration module for the first time. For noise suppression and dereverberation, the enhancement module is performed with fullband-wideband parallel processing. On the blind test set of ICASSP 2023 SSI Challenge, the proposed Gesper system, which satisfies the real-time condition, achieves 3.27 P.804 overall mean opinion score (MOS) and 3.35 P.835 overall MOS, ranked 1st in both track 1 and track 2",
    "checked": true,
    "id": "a140ba3e4781f44e1be503381438264d00b6e257",
    "semantic_title": "gesper: a restoration-enhancement framework for general speech reconstruction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ryumina23_interspeech.html": {
    "title": "Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech",
    "volume": "main",
    "abstract": "Automatic personality traits assessment (PTA) provides high-level, intelligible predictive inputs for subsequent critical downstream tasks, such as job interview recommendations and mental healthcare monitoring. In this work, we introduce a novel Multimodal Personality Traits Assessment (MuPTA) corpus. Our MuPTA corpus is unique in that it contains both spontaneous and read speech collected in the midly-resourced Russian language. We present a novel audio-visual approach for PTA that is used in order to set up baseline results on this corpus. We further analyze the impact of both spontaneous and read speech types on the PTA predictive performance. We find that for the audio modality, the PTA predictive performances on short signals are almost equal regardless of the speech type, while PTA using video modality is more accurate with spontaneous speech compared to read one regardless of the signal length",
    "checked": true,
    "id": "95ce2fc7cda6f48314d44bf80f49c034dd91c30a",
    "semantic_title": "multimodal personality traits assessment (mupta) corpus: the impact of spontaneous and read speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pudo23_interspeech.html": {
    "title": "MOCKS 1.0: Multilingual Open Custom Keyword Spotting Testset",
    "volume": "main",
    "abstract": "The main purpose of this work is to create a comprehensive audio testset that can be used to evaluate custom keyword spotting (KWS) models and to benchmark different KWS solutions. We also propose a set of requirements that should be followed while creating testsets to evaluate custom KWS models. We consider multiple versions of the problem: text and audio-based keyword spotting, as well as offline and online (streaming) modes. Our testset named MOCKS is based on LibriSpeech and Mozilla Common Voice datasets. We used automatically generated alignments to extract parts of the recordings, which were split into keywords and test samples. The resulting testset contains almost 50,000 keywords. It contains audio data in English, French, German, Italian, and Spanish, but can be easily extended to other languages. MOCKS has been made publicly available to the research community. Initial KWS experiments run on MOCKS suggest that it can serve as a challenging testset for future research",
    "checked": true,
    "id": "6c49e30dc90d27d06e51e1fcf5c0581fd375e6c4",
    "semantic_title": "mocks 1.0: multilingual open custom keyword spotting testset",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eisenstein23_interspeech.html": {
    "title": "MD3: The Multi-Dialect Dataset of Dialogues",
    "volume": "main",
    "abstract": "We introduce a new dataset of conversational speech representing English from India, Nigeria, and the United States. The Multi-Dialect Dataset of Dialogues (MD3) strikes a new balance between open-ended conversational speech and task-oriented dialogue by prompting participants to perform a series of short information-sharing tasks. This facilitates quantitative cross-dialectal comparison, while avoiding the imposition of a restrictive task structure that might inhibit the expression of dialect features. Preliminary analysis of the dataset reveals significant differences in syntax and in the use of discourse markers. The dataset, which will be made publicly available with the publication of this paper, includes more than 20 hours of audio and more than 200,000 orthographically-transcribed tokens",
    "checked": true,
    "id": "19bd9ff0e8843b3fc18cf204d29c61d53ed2fdb3",
    "semantic_title": "md3: the multi-dialect dataset of dialogues",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/anwar23_interspeech.html": {
    "title": "MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation",
    "volume": "main",
    "abstract": "We introduce MuAViC, a multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation providing 1200 hours of audio-visual speech in 9 languages. It is fully transcribed and covers 6 English-to-X translation as well as 6 X-to-English translation directions. To the best of our knowledge, this is the first open benchmark for audio-visual speech-to-text translation and the largest open benchmark for multilingual audio-visual speech recognition. Our baseline results show that MuAViC is effective for building noise-robust speech recognition and translation models. We make the corpus available at https://github.com/facebookresearch/muavic",
    "checked": true,
    "id": "a474501612900c3b8540841cb2a76fa30f703089",
    "semantic_title": "muavic: a multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/suwanbandit23_interspeech.html": {
    "title": "Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition",
    "volume": "main",
    "abstract": "We release 840 hours of read speech multi-dialect ASR corpora consisting of 700 hours of main Thai dialect, named Thai-central, and 40 hours for each local dialect , named Thai-dialect, with transcripts and their translations to Thai. The dialects, selected to represent different regions of Thailand, are Khummuang, Korat, and Pattani. We also release the baseline dialectal ASR models trained using the curriculum learning approach. We found that the pre-training with the high-resource main dialect and target dialect generally yields the best performance. We believe that the availability of our corpora would contribute to the problem of low-resource Thai dialects. The corpus data will be available on Github",
    "checked": true,
    "id": "f06b6ec53fa77d16071498abb1c85175e83bbf6f",
    "semantic_title": "thai dialect corpus and transfer-based curriculum learning investigation for dialect automatic speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23d_interspeech.html": {
    "title": "HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation",
    "volume": "main",
    "abstract": "We introduce HK-LegiCoST, a new three-way parallel corpus of Cantonese-English translations, containing 600+ hours of Cantonese audio, its standard traditional Chinese transcript, and English translation, segmented and aligned at the sentence level. We describe the notable challenges in corpus preparation: segmentation, alignment of long audio recordings, and sentence-level alignment with non-verbatim transcripts. Such transcripts make the corpus suitable for speech translation research when there are significant differences between the spoken and written forms of the source language. Due to its large size, we are able to demonstrate competitive speech translation baselines on HK-LegiCoST and extend them to promising cross-corpus results on the FLEURS Cantonese subset. These results deliver insights into speech recognition and translation research in languages for which non-verbatim or ''noisy'' transcription is common due to various factors, including vernacular and dialectal speech",
    "checked": true,
    "id": "74173dec94055d7f4051aa2e80be31ccd2bde596",
    "semantic_title": "hk-legicost: leveraging non-verbatim transcripts for speech translation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bekal23_interspeech.html": {
    "title": "A Metric-Driven Approach to Conformer Layer Pruning for Efficient ASR Inference",
    "volume": "main",
    "abstract": "Conformer-based end-to-end automatic speech recognition (ASR) models have gained popularity in recent years due to their exceptional performance at scale. However, there are significant computation, memory and latency costs associated with running inference on such models. With the aim of mitigating these issues, we evaluate the efficacy of pruning Conformer layers while fine-tuning only on 20% of the data used for the pre-trained model. We score Conformer layers using correlation, energy, and gradient-based metrics and rank them to identify candidate layers for pruning. We also propose an iterative pruning strategy which monitors and prunes layers that are consistently ranked low by the metrics during training. Using our methods, we prune large pre-trained offline and online (streaming) models by 20% and 40% with little impact on performance, while outperforming a strong knowledge distillation baseline",
    "checked": true,
    "id": "aa1df95ee0de13d2b19b5df778f404ee0a95eeb9",
    "semantic_title": "a metric-driven approach to conformer layer pruning for efficient asr inference",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gurunathshivakumar23_interspeech.html": {
    "title": "Distillation Strategies for Discriminative Speech Recognition Rescoring",
    "volume": "main",
    "abstract": "Second-pass rescoring is employed in most state-of-the-art speech recognition systems. Recently, BERT based models have gained popularity for re-ranking the n-best hypothesis by exploiting the knowledge from masked language model pre-training. Further, fine-tuning with discriminative loss such as minimum word error rate (MWER) has shown to perform better than likelihood-based loss. Streaming applications with low latency requirements impose significant constraints on the size of the models, thereby limiting the word error rate (WER) performance gains. In this paper, we propose effective strategies for distilling from large models discriminatively trained with the MWER objective. We experiment on Librispeech and production scale internal dataset for voice-assistant. Our results demonstrate relative improvements of upto 7% WER over student models trained with MWER. We also show that the proposed distillation can reduce the WER gap between the student and the teacher by 62% upto 100%",
    "checked": true,
    "id": "4f803e3a190c8749b378fb6abf7d6c4cb08439ad",
    "semantic_title": "distillation strategies for discriminative speech recognition rescoring",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pouthier23_interspeech.html": {
    "title": "Another Point of View on Visual Speech Recognition",
    "volume": "main",
    "abstract": "Standard Visual Speech Recognition (VSR) systems directly process images as input features without any apriori link between raw pixel data and facial traits. Pixel information is smartly sieved when facial landmarks are extracted from pictures and repurposed as graph nodes. Their evolution through time is thus modeled by a Graph Convolutional Network. However, with graph-based VSR being in its infancy, the selection of points and their correlation are still ill-defined and often bound to aprioristic knowledge and handcrafted techniques. In this paper, we investigate the graph approach for VSR and its ability to learn the correlation between points beyond the mouth region. We also study the different contributions that each facial region brings to the system accuracy, proving that more scattered but better connected graphs can be both computationally light and accurate",
    "checked": true,
    "id": "a69b8616ea360c81ec2a25c167d864e5c3442421",
    "semantic_title": "another point of view on visual speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23e_interspeech.html": {
    "title": "RASR2: The RWTH ASR Toolkit for Generic Sequence-to-sequence Speech Recognition",
    "volume": "main",
    "abstract": "Modern public ASR tools usually provide rich support for training various sequence-to-sequence (S2S) models, but rather simple support for decoding open-vocabulary scenarios only. For closed-vocabulary scenarios, public tools supporting lexical-constrained decoding are usually only for classical ASR, or do not support all S2S models. To eliminate this restriction on research possibilities such as modeling unit choice, we present RASR2 in this work, a research-oriented generic S2S decoder implemented in C++. It offers a strong flexibility/compatibility for various S2S models, language models, label units/topologies and neural network architectures. It provides efficient decoding for both open- and closed-vocabulary scenarios based on a generalized search framework with rich support for different search modes and settings. We evaluate RASR2 with a wide range of experiments on both switchboard and Librispeech corpora. Our source code is public online",
    "checked": true,
    "id": "93cddfc01d6c1429a0bd3b367cd7d5ecec6b5437",
    "semantic_title": "rasr2: the rwth asr toolkit for generic sequence-to-sequence speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/filimonov23_interspeech.html": {
    "title": "Streaming Speech-to-Confusion Network Speech Recognition",
    "volume": "main",
    "abstract": "In interactive automatic speech recognition (ASR) systems, low-latency requirements limit the amount of search space that can be explored during decoding, particularly in end-to-end neural ASR. In this paper, we present a novel streaming ASR architecture that outputs a confusion network while maintaining limited latency, as needed for interactive applications. We show that 1-best results of our model are on par with a comparable RNN-T system, while the richer hypothesis set allows second-pass rescoring to achieve 10-20% lower word error rate on the LibriSpeech task. We also show that our model outperforms a strong RNN-T baseline on a far-field voice assistant task",
    "checked": true,
    "id": "5d13986bfd27f5b0bd28c7fce97d76c31f1dc61c",
    "semantic_title": "streaming speech-to-confusion network speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23d_interspeech.html": {
    "title": "Accurate and Structured Pruning for Efficient Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) has seen remarkable advancements with deep neural networks, such as Transformer and Conformer. However, these models typically have large model sizes and high inference costs, posing a challenge to deploy on resource-limited devices. In this paper, we propose a novel compression strategy that leverages structured pruning and knowledge distillation to reduce the model size and inference cost of the Conformer model while preserving high recognition performance. Our approach utilizes a set of binary masks to indicate whether to retain or prune each Conformer module, and employs L0 regularization to learn the optimal mask values. To further enhance pruning performance, we use a layerwise distillation strategy to transfer knowledge from unpruned to pruned models. Our method outperforms all pruning baselines on the widely used LibriSpeech benchmark, achieving a 50% reduction in model size and a 28% reduction in inference cost with minimal performance loss",
    "checked": true,
    "id": "9af24564bca1d4a9fe0ee1ba7d57417b68922f7c",
    "semantic_title": "accurate and structured pruning for efficient automatic speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chua23_interspeech.html": {
    "title": "MERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization",
    "volume": "main",
    "abstract": "To enhance the reliability and robustness of language identification(LID) and language diarization(LD) systems for heterogeneous populations and scenarios, there is a need for speech processing models to be trained on datasets that feature diverse language registers and speech patterns. We present the MERLIon CCS challenge, featuring a first-of-its-kind Zoom video call dataset of parent-child shared book reading, of over 30 hours with over 300 recordings, annotated by multilingual transcribers using a high-fidelity linguistic transcription protocol. The audio corpus features spontaneous and in-the-wild English-Mandarin code-switching, child-directed speech in non-standard accents with diverse language-mixing patterns recorded in a variety of home environments. This report describes the corpus, as well as LID and LD results for our baseline and several systems submitted to the MERLIon CCS challenge using the corpus",
    "checked": true,
    "id": "ea701359c6e2b2ba5c36c4849d4144d318171418",
    "semantic_title": "merlion ccs challenge: a english-mandarin code-switching child-directed speech corpus for language identification and diarization",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gupta23_interspeech.html": {
    "title": "Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech",
    "volume": "main",
    "abstract": "This work focuses on improving the Spoken Language Identification (LangId) system for a challenge that focuses on developing robust language identification systems that are reliable for non-standard, accented (Singaporean accent), spontaneous code-switched, and child-directed speech collected via Zoom. We propose a two-stage Encoder-Decoder-based E2E model. The encoder module consists of 1D depth-wise separable convolutions with Squeeze-and-Excitation (SE) layers with a global context. The decoder module uses an attentive temporal pooling mechanism to get fixed length time-independent feature representation. The total number of parameters in the model is around 22.1 M, which is relatively light compared to using some large-scale pre-trained speech models. We achieved an EER of 15.6% in the closed track and 11.1% in the open track (baseline system 22.1%). We also curated additional LangId data from YouTube videos (having Singaporean speakers), which will be released for public use",
    "checked": true,
    "id": "cde9b61b27b68765a6879aeaa9ad6443582d45da",
    "semantic_title": "spoken language identification system for english-mandarin code-switching child-directed speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shahin23_interspeech.html": {
    "title": "Improving wav2vec2-based Spoken Language Identification by Learning Phonological Features",
    "volume": "main",
    "abstract": "Spoken language identification (SLI) is a key component in speech-processing tools such as spoken language understanding. In code-switching conversational speech, speakers change languages for short durations posing an additional challenge to language identification techniques. In this work, we investigate the ability of a wav2vec2-based SLI method in identifying the spoken language of English/Mandarin code-switching child-directed conversational speech recorded via Zoom. The proposed system allows the pre-trained wav2vec2-based model to learn language-dependent phonological features by fine-tuning first on detecting manners and places of articulation, then on classifying between English and Mandarin speech segments. The proposed system was tested against parent-child Zoom recordings provided as a part of the MERLIon CCS challenge of language identification. The system achieved the best balanced accuracy of 81.3% and the second-lowest equal error rate of 10.6%",
    "checked": true,
    "id": "008f14282f99f2b3328f2a806224574096344d34",
    "semantic_title": "improving wav2vec2-based spoken language identification by learning phonological features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/praveen23_interspeech.html": {
    "title": "Language Identification Networks for Multilingual Everyday Recordings",
    "volume": "main",
    "abstract": "This paper describes the systems SRIB has proposed for task-1 of the inaugural MERLion CCS challenge in the closed domain and open domain. Our system for the closed task is based on an end-to-end conformer architecture trained for the task of automatic speech recognition using RNN-T loss, which is then transfer learned for the task of language classification. We train the ASR model initially to ease the task of learning the right features for the classification task. This system achieves a 13.9% Equal Error Rate (EER) and 81.7% Balanced Accuracy (BAC) on the evaluation set. For the open track, we use an ensemble of Open AI's Whisper model and one of the ASR models used our closed track. This system achieves 9.5% EER and 78.9% BAC on the evaluation set. Compared to the challenge baseline we observe relative improvements for EER of 35.9% in the closed track and 56.2% in the open track. We achieve 1st position on both the closed and the open track leaderboards",
    "checked": true,
    "id": "27b0852548c458adb316c9ba5563805151cf6421",
    "semantic_title": "language identification networks for multilingual everyday recordings",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/styles23_interspeech.html": {
    "title": "Investigating model performance in language identification: beyond simple error statistics",
    "volume": "main",
    "abstract": "Language development experts need tools that can automatically identify languages from fluent, conversational speech and provide reliable estimates of usage rates at the level of an individual recording. However, LID systems are typically evaluated on metrics such as equal error rate and balanced accuracy, applied at the level of an entire speech corpus. These overview metrics do not provide information about model performance at the level of individual speakers, recordings, or units of speech with different linguistic characteristics. Overview statistics may mask systematic errors in model performance for some subsets of the data, and consequently, have worse performance on data derived from some subsets of human speakers, creating a kind of algorithmic bias. Here, we investigate how well a number of LID systems perform on individual recordings and speech units with different linguistic properties in the MERLIon CCS Challenge featuring accented code-switched child-directed speech",
    "checked": true,
    "id": "5550d0db7030aa77480bfb10c4ec00862bb233eb",
    "semantic_title": "investigating model performance in language identification: beyond simple error statistics",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kodali23_interspeech.html": {
    "title": "Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings",
    "volume": "main",
    "abstract": "In speech communication, talkers regulate vocal intensity resulting in speech signals of different intensity categories (e.g., soft, loud). Intensity category carries important information about the speaker's health and emotions. However, many speech databases lack calibration information, and therefore sound pressure level cannot be measured from the recorded data. Machine learning, however, can be used in intensity category classification even though calibration information is not available. This study investigates pre-trained model embeddings (Wav2vec2 and Whisper) in classification of vocal intensity category (soft, normal, loud, and very loud) from speech signals expressed using arbitrary amplitude scales. We use a new database consisting of two speaking tasks (sentence and paragraph). Support vector machine is used as a classifier. Our results show that the pre-trained model embeddings outperformed three baseline features, providing improvements of up to 7%(absolute) in accuracy",
    "checked": true,
    "id": "ad97990a608b1d446d5f04983917a6167555e415",
    "semantic_title": "classification of vocal intensity category from speech using the wav2vec2 and whisper embeddings",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kathan23_interspeech.html": {
    "title": "The effect of clinical intervention on the speech of individuals with PTSD: features and recognition performances",
    "volume": "main",
    "abstract": "Post-traumatic stress disorder (PTSD) is an anxiety disorder that can occur as a response to traumatic experiences, such as catastrophic events, and can have a detrimental influence on mental wellbeing. Furthermore, PTSD is present in 5-10 % of the population, making it a prevalent disorder in our time, thus necessitating a timely diagnosis and proper treatment. In this paper, we present results for PTSD detection based on speech recordings on a newly collected dataset consisting of 15 participants, including speakers with PTSD and a control group. Moreover, the dataset includes speech data immediately before and after a clinical intervention (i.e., acupuncture-supported psychotherapy), allowing us to examine the effect of a single treatment session. In our experiments, we achieve a best area under the curve (AUC) of 82 % using solely pre-treatment data. Finally, we analyse prominent acoustic patterns of individuals with PTSD compared to the control group",
    "checked": true,
    "id": "289a1ceedaf062a794899fed8273585d60921f8c",
    "semantic_title": "the effect of clinical intervention on the speech of individuals with ptsd: features and recognition performances",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/triantafyllopoulos23_interspeech.html": {
    "title": "Analysis and automatic prediction of exertion from speech: Contrasting objective and subjective measures collected while running",
    "volume": "main",
    "abstract": "Monitoring runner exertion in real-time can provide unique insights that help improve training and reduce injuries. Most existing methods use heart rate (HR) as a physiological proxy of it, but this does not always correspond to self-perceived exertion. This is an additional factor in determining overall strain and is typically evaluated with the Borg rating of perceived exertion (RPE) scale. In recent years, speech has been one of the many modalities used to monitor exertion; however, mostly used to predict physiological measures using speech collected after a physical task. In this work, we contrast the manifestation of subjective vs objective exertion on speech signals obtained while running in real-life environments. We identify and interpret a set of prosodic and spectral features related to both markers, and proceed to train deep learning models that directly predict RPE and HR from speech, obtaining an average Pearson correlation of .341 and .418, respectively",
    "checked": true,
    "id": "c1c89a0e3272d5976986d71677279a6f36dedaa0",
    "semantic_title": "analysis and automatic prediction of exertion from speech: contrasting objective and subjective measures collected while running",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tao23_interspeech.html": {
    "title": "The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection",
    "volume": "main",
    "abstract": "This paper presents the Androids Corpus, a new benchmark for speech-based automatic depression detection. The corpus is a collection of 228 recordings uttered by 118 native Italian speakers, including 64 who were diagnosed with depression by professional psychiatrists. Out of the 228 recordings, 112 contain read speech (all speakers read the same text) and 116 contain spontaneous speech (all speakers answer the same questions posed by an interviewer). For 110 speakers, including 58 diagnosed with depression, the corpus includes both read and spontaneous speech samples. Overall, the total duration of the material is 1 hour, 33 minutes and 49 seconds for read speech and 7 hours, 24 minutes and 22 seconds for spontaneous speech. Besides the data, the corpus includes experimental protocols that can be replicated, thus ensuring reproducibility of the experiments performed over it and comparison of the results",
    "checked": true,
    "id": "5f8dbdf72576cf9dfd374e6ab790e51912eb22b0",
    "semantic_title": "the androids corpus: a new publicly available benchmark for speech based depression detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eni23_interspeech.html": {
    "title": "Comparing Hand-Crafted Features to Spectrograms for Autism Severity Estimation",
    "volume": "main",
    "abstract": "In this work, we compared two different input approaches to estimate autism severity using speech signals. We analyzed 127 audio recordings of young children obtained during the Autism Diagnostic Observation Schedule 2nd edition (ADOS-2) administration. Two different sets of features were extracted from each recording: 1) hand-crafted features, which included acoustic and prosodic features, and 2) log-mel spectrograms, which give the time-frequency representation. We examined two different Convolutional Neural Network (CNN) architectures for each of the two inputs and compared the autism severity estimation performance. We showed that the hand-crafted features yielded lower prediction error (normalized RMSE) in most examined configurations than the log-mel spectrograms. Moreover, fusing the estimated autism severity scores of the two feature extraction methods yielded the best results, where both architectures exhibited similar performance (Pearson R=0.66, normalized RMSE=0.24)",
    "checked": true,
    "id": "817a3ed98514cf0bebaada6d127cb5f85dc1b90b",
    "semantic_title": "comparing hand-crafted features to spectrograms for autism severity estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mijnders23_interspeech.html": {
    "title": "Acoustic characteristics of depression in older adults' speech: the role of covariates",
    "volume": "main",
    "abstract": "Depression in older adults is often associated with various physical conditions and is hence different from depression at a younger age. Ageing may come with cognitive decline, medication use, and frailty, which are known to be predictors of late-life depression. One common symptom of depression is psychomotor retardation, that may also affect speech production. Most speech studies on depression so far have focused on younger or middle-aged adults. In this study, we used speech data from a large longitudinal Dutch study on late-life depression and its comorbid symptoms to compare speech acoustics in persons with depression (PWD) and controls. We investigated whether groups differed by taking several covariates into account (e.g., frailty, slowness, and medication use). Group differences were found in within-vowel F2 range, speech rate and mean pause duration. These data indicate that speech acoustics can be used to differentiate PWDs and controls, even with low-quality speech data",
    "checked": true,
    "id": "dcc1944f910b7e3868f60c31d4204e40e5cc6e30",
    "semantic_title": "acoustic characteristics of depression in older adults' speech: the role of covariates",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23b_interspeech.html": {
    "title": "Dual Transformer Decoder based Features Fusion Network for Automated Audio Captioning",
    "volume": "main",
    "abstract": "Automated audio captioning (AAC) which generates textual descriptions of audio content. Existing AAC models achieve good results but only use the high-dimensional representation of the encoder. There is always insufficient information learning of high-dimensional methods owing to high-dimensional representations having a large amount of information. In this paper, a new encoder-decoder model called the Low- and High-Dimensional Feature Fusion (LHDFF) is proposed. LHDFF uses a new PANNs encoder called Residual PANNs (RPANNs) to fuse low- and high-dimensional features. Low-dimensional features contain limited information about specific audio scenes. The fusion of low- and high-dimensional features can improve model performance by repeatedly emphasizing specific audio scene information. To fully exploit the fused features, LHDFF uses a dual transformer decoder structure to generate captions in parallel. Experimental results show that LHDFF outperforms existing audio captioning models",
    "checked": true,
    "id": "781cd0f5251cbcbd6b226ac0ece9f10a378995c1",
    "semantic_title": "dual transformer decoder based features fusion network for automated audio captioning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pellegrini23_interspeech.html": {
    "title": "Adapting a ConvNeXt Model to Audio Classification on AudioSet",
    "volume": "main",
    "abstract": "In computer vision, convolutional neural networks (CNN) such as ConvNeXt, have been able to surpass state-of-the-art transformers, partly thanks to depthwise separable convolutions (DSC). DSC, as an approximation of the regular convolution, has made CNNs more efficient in time and memory complexity without deteriorating their accuracy, and sometimes even improving it. In this paper, we first implement DSC into the Pretrained Audio Neural Networks (PANN) family for audio classification on AudioSet, to show its benefits in terms of accuracy/model size trade-off. Second, we adapt the now famous ConvNeXt model to the same task. It rapidly overfits, so we report on techniques that improve the learning process. Our best ConvNeXt model reached 0.471 mean-average precision on AudioSet, which is better than or equivalent to recent large audio transformers, while using three times less parameters. We also achieved positive results in audio captioning and audio retrieval with this model",
    "checked": true,
    "id": "dffb1a21c349a13fb8850c27bd842c567d0ce303",
    "semantic_title": "adapting a convnext model to audio classification on audioset",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23w_interspeech.html": {
    "title": "Few-shot Class-incremental Audio Classification Using Stochastic Classifier",
    "volume": "main",
    "abstract": "It is generally assumed that number of classes is fixed in current audio classification methods, and the model can recognize pre-given classes only. When new classes emerge, the model needs to be retrained with adequate samples of all classes. If new classes continually emerge, these methods will not work well and even infeasible. In this study, we propose a method for few-shot class-incremental audio classification, which continually recognizes new classes and remember old ones. The proposed model consists of an embedding extractor and a stochastic classifier. The former is trained in base session and frozen in incremental sessions, while the latter is incrementally expanded in all sessions. Two datasets (NS-100 and LS-100) are built by choosing samples from audio corpora of NSynth and LibriSpeech, respectively. Results show that our method exceeds four baseline ones in average accuracy and performance dropping rate. Code is at https://github.com/vinceasvp/meta-sc",
    "checked": true,
    "id": "15f681e41ec1ea6857b678f875dbcbf2b5a37698",
    "semantic_title": "few-shot class-incremental audio classification using stochastic classifier",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23d_interspeech.html": {
    "title": "Enhance Temporal Relations in Audio Captioning with Sound Event Detection",
    "volume": "main",
    "abstract": "Automated audio captioning aims at generating natural language descriptions for given audio clips, not only detecting and classifying sounds, but also summarizing the relationships between audio events. Recent research advances in audio captioning have introduced additional guidance to improve the accuracy of audio events in generated sentences. However, temporal relations between audio events have received little attention while revealing complex relations is a key component in summarizing audio content. Therefore, this paper aims to better capture temporal relationships in caption generation with sound event detection (SED), a task that locates events' timestamps. We investigate the best approach to integrate temporal information in a captioning model and propose a temporal tag system to transform the timestamps into comprehensible relations. Results evaluated by the proposed temporal metrics suggest that great improvement is achieved in terms of temporal relation generation",
    "checked": true,
    "id": "4e2ad3cca9e561e836bda602b9ee8bf4db28e980",
    "semantic_title": "enhance temporal relations in audio captioning with sound event detection",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23s_interspeech.html": {
    "title": "First Language Effects on Second Language Perception: Evidence from English Low-vowel Nasal Sequences Perceived by L1 Mandarin Chinese Listeners",
    "volume": "main",
    "abstract": "First language (L1) sound systems can shape second language (L2) perception of non-native phonological contrasts. This study examines how L1 Mandarin listeners perceive English low vowel + nasal (VN) sequences that are not contrastive in Mandarin. A speeded AX discrimination task tested listeners' low-level processing of English VN sequences, and a perceptual similarity rating task assessed listeners' higher-level phonological knowledge. Despite high discrimination accuracy, reaction times showed that L1 Mandarin listeners had a harder time processing nasal-different sequences compared to L1 English listeners. Moreover, L1 Mandarin listeners perceived vowel-different sequences as more distinct than English listeners. Overall, listeners' L1 phonological system influences L2 perception, suggested by group-level differences in both perceived phonological distinctiveness and phonetic discrimination",
    "checked": true,
    "id": "3b03590f118877f1e2903c6f718a0d9288cee8fd",
    "semantic_title": "first language effects on second language perception: evidence from english low-vowel nasal sequences perceived by l1 mandarin chinese listeners",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/maity23_interspeech.html": {
    "title": "Motor Control Similarity Between Speakers Saying \"A Souk\" Using Inverse Atlas Tongue Modeling",
    "volume": "main",
    "abstract": "Finite element models (FEM) of the tongue have facilitated speech studies through analysis of internal muscle forces indirectly derived from imaging data. In this work, we build a uniform hexahedral FEM of a tongue atlas constructed from magnetic resonance imaging data of a healthy population. The FEM is driven by inverse internal tongue tissue kinematics of speakers temporally aligned and deformed into the same atlas space, while performing the speech task \"a souk\" allowing muscle activation predictions. This work aims to investigate the commonalities in tongue motor strategies in the articulation of \"a souk\" predicted by the inverse tongue atlas model. Our findings report variability among five speakers for estimated muscle activations with a similarity index using a dynamic time warp function. Two speakers show similarity index > 0.9 and two others < 0.7 with respect to a reference speaker for most tongue muscles. The relative motion tracking error of the model is less than 2% which is promising for speech study applications",
    "checked": true,
    "id": "aaac6e90397ed6765fb0da4ff6cd651c2b42df5a",
    "semantic_title": "motor control similarity between speakers saying \"a souk\" using inverse atlas tongue modeling",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23t_interspeech.html": {
    "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
    "volume": "main",
    "abstract": "This work introduces approaches to assessing phrase breaks in ESL learners' speech using pre-trained language models (PLMs) and large language models (LLMs). There are two tasks: overall assessment of phrase break for a speech clip and fine-grained assessment of every possible phrase break position. To leverage NLP models, speech input is first force-aligned with texts, and then pre-processed into a token sequence, including words and phrase break information. To utilize PLMs, we propose a pre-training and fine-tuning pipeline with the processed tokens. This process includes pre-training with a replaced break token detection module and fine-tuning with text classification and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The experiments show that with the PLMs, the dependence on labeled training data has been greatly reduced, and the performance has improved. Meanwhile, we verify that ChatGPT, a renowned LLM, has potential for further advancement in this area",
    "checked": true,
    "id": "b07fcd7d08b8426c23e787e6baf60444e8904f93",
    "semantic_title": "assessing phrase break of esl speech with pre-trained language models and large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoshinaga23_interspeech.html": {
    "title": "A Relationship Between Vocal Fold Vibration and Droplet Production",
    "volume": "main",
    "abstract": "While some aerosol droplets causing airborne transmissions are argued to be produced by vocal fold vibrations, the detailed production mechanisms were unclear due to the difficulty of direct observation. In this study, by using a transparent acrylic vocal fold model and high-speed imaging, we observed vocal fold vibrations that produce droplets of artificial mucus to clarify the relationship between the vocal fold vibration and the droplet production. The vocal fold model was set on a lung model which has a manual diaphragm. The artificial mucus in between the vocal folds was lighted up by a laser sheet. The results showed that droplets were produced when the sound amplitudes were decreased or the fundamental frequency became unstable. We observed that mucus drops attached to the middle of the vocal fold wall splashed and formed a droplet that flew from the vocal fold wall. This suggests that the shear forces of turbulent airflow passing on the mucus mainly produced the droplets",
    "checked": true,
    "id": "018190a633a4d3b9415e20984be97a919228422c",
    "semantic_title": "a relationship between vocal fold vibration and droplet production",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/garnier23_interspeech.html": {
    "title": "Audio, Visual and Audiovisual intelligibility of vowels produced in noise",
    "volume": "main",
    "abstract": "Why do speakers amplify articulatory movements when communicating in noisy environments? This study examines the hypothesis that hyper-articulation contributes to improved vowel intelligibility in audio, visual and audiovisual domains. A perceptual test was conducted with Audio-Only (AO), Visual-Only (VO) and Audiovisual (AV) stimuli of vowels produced in conversational and Lombard speech. The average score of vowel recognition was significantly increased in Lombard speech, compared to normal speech, for all perceptual modalities (AO, VO and AV). Specifically, the distinctive features of vowel height and backness were better perceived in Lombard speech in both the audio and visual domains. Changes in speech articulation in noise did not affect the perception of the rounding feature in the visual domain, but degraded it in the audio domain. On the contrary, the perception of the spreading feature was decreased in Lombard speech in the visual domain, but improved in the audio domain",
    "checked": true,
    "id": "711bfe0060cf354d0815da05606419f5a6ab13c6",
    "semantic_title": "audio, visual and audiovisual intelligibility of vowels produced in noise",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elie23_interspeech.html": {
    "title": "Optimal control of speech with context-dependent articulatory targets",
    "volume": "main",
    "abstract": "This paper presents a computational implementation of phonetic planning which consists of choosing the position of articulatory targets which satisfy conflicting linguistic and extra-linguistic requirements. We present a minimal model that considers intelligibility and least effort as task requirements. To achieve the context-dependent variability of targets, our model approximates intelligibility as a function of target phoneme recognition probability given a vector of articulatory parameters. Preliminary experiments show that our minimal computational model of phonetic planning is able to predict two types of hypoarticulation by adjusting the weight assigned to effort: vowel centralization and stop consonant lenition",
    "checked": true,
    "id": "6ddec359735b146a6bf1898b85bc65797c0d059e",
    "semantic_title": "optimal control of speech with context-dependent articulatory targets",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23d_interspeech.html": {
    "title": "Computational modeling of auditory brainstem responses derived from modified speech",
    "volume": "main",
    "abstract": "The auditory brainstem response (ABR) is a powerful neurophysiological measure to diagnose hearing deficits along the auditory pathway. Wave I of the ABRs is particularly critical for assessing early hearing loss, though hard to observe in humans. The major downside of ABR is that most protocols are very boring since they use thousands of clicks to elicit ABRs. Here, we derived modeled ABRs with continuous speech from an audiobook. Unlike other studies involving computationally intensive modification that made their speech stimuli unnatural-sounding and unlikely to be used in real-life applications, we applied a fast and efficient algorithm that enhances speech transients to better elicit ABRs. Using the auditory periphery model that simulates human brains, we derived ABRs from our transient speech and showed a significantly larger Wave I-V ratio compared to other stimuli. These results demonstrated a potential of assessing hearing conditions in a more objective and naturalistic way",
    "checked": true,
    "id": "c43c02dc14d0b69061cc26f4bb70f98da8f654dd",
    "semantic_title": "computational modeling of auditory brainstem responses derived from modified speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ma_interspeech.html": {
    "title": "Leveraging Label Information for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "Multimodal emotion recognition (MER) aims to detect the emotional status of a given expression by combining the speech and text information. Intuitively, label information should be capable of helping the model locate the salient tokens/frames relevant to the specific emotion, which finally facilitates the MER task. Inspired by this, we propose a novel approach for MER by leveraging label information. Specifically, we first obtain the representative label embeddings for both text and speech modalities, then learn the label-enhanced text/speech representations for each utterance via label-token and label-frame interactions. Finally, we devise a novel label-guided attentive fusion module to fuse the label-aware text and speech representations for emotion classification. Extensive experiments were conducted on the public IEMOCAP dataset, and experimental results demonstrate that our proposed approach outperforms existing baselines and achieves new state-of-the-art performance",
    "checked": true,
    "id": "49c56b3eabbf67b74993dec4769684e0d04abcd9",
    "semantic_title": "leveraging label information for multimodal emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23c_interspeech.html": {
    "title": "Improving End-to-End Modeling For Mandarin-English Code-Switching Using Lightweight Switch-Routing Mixture-of-Experts",
    "volume": "main",
    "abstract": "Code-switching is a common phenomenon in multilingual communities. In this paper, we study end-to-end model for Mandarin-English intra-sentential code-switching speech recognition. A lightweight Switch-Routing network is proposed, which includes two experts and a switch router. Two experts, representing Mandarin and English learners, implicitly provide language identification information and skillfully use monolingual data to assist code-switching task training, which solves the problem of data sparsity. In addition, our network is a lightweight structure, which makes use of the advantages of Switch Transformer and discards its weakness of increasing model capacity. Finally, we study the effect of using lightweight Switch Routing in different blocks of encoder and decoder. Compared with Bi-Encoder, proposed model has a better performance on the ASRU code-switching test set, and the most important thing is that it requires much less inference time with RTF decreasing by 31.39%",
    "checked": true,
    "id": "d7fb2acefec0b87bc49feef4805d440d46679846",
    "semantic_title": "improving end-to-end modeling for mandarin-english code-switching using lightweight switch-routing mixture-of-experts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ea_interspeech.html": {
    "title": "Frequency Patterns of Individual Speaker Characteristics at Higher and Lower Spectral Ranges",
    "volume": "main",
    "abstract": "Acoustic characteristics of speech exhibit variability across individuals, while preserving shared phonetic information to listeners. In this paper, the general time-frequency pattern of individual speaker characteristics is discussed based on our previous research. The main target here is set at speaker-specific acoustic effects of the vocal tract in both higher and lower frequency ranges. To address the under-explored phenomena, two experiments were conducted. Firstly, simulations based on the transmission line model are used to explore how resonances in higher frequencies vary with different hypopharyngeal-cavity shapes. Secondly, speech signals emitted from the mouth and nostrils are recorded separately to observe potential factors for spectral irregularity in lower frequencies. From our findings, a time-frequency model of individual speaker characteristics is proposed that provides insights into how individuality is manifested in speech spectral patterns",
    "checked": true,
    "id": "8a8d0aad29405d36f1dc306d0f1b1828ee2d4d14",
    "semantic_title": "frequency patterns of individual speaker characteristics at higher and lower spectral ranges",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gosselkeberthelsen23_interspeech.html": {
    "title": "Adaptation to predictive prosodic cues in non-native standard dialect",
    "volume": "main",
    "abstract": "Predicting word or sentence structure from prosodic cues (e.g., pitch, pauses, or duration) is an important mechanism in speech processing. The mechanism is readily employed by native listeners but considerably less so by second language learners except with extensive training. This prompts the question of how flexible and adaptive the predictive system is. Can listeners adjust to accommodate, for example, diverging predictive cues in a different dialect? The present paper tests this for prosody-based word structure prediction in mainland Scandinavian. Neurophysiological and behavioural results suggest that listeners from non-standard dialect areas can in fact predict word forms in the standard variety even when the predictive cues and their validity differ considerably. This suggests a certain degree of adaptability in the predictive system, potentially depending on factors like familiarity, prestige, and exposure onset",
    "checked": true,
    "id": "fd0cde3839d8123a90751ee3ddca8fe1c3b67317",
    "semantic_title": "adaptation to predictive prosodic cues in non-native standard dialect",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/archerboyd23_interspeech.html": {
    "title": "Head movements in two- and four-person interactive conversational tasks in noisy and moderately reverberant conditions",
    "volume": "main",
    "abstract": "Multi-modal processing schemes are of increasing importance for adaptive hearing devices. However, more data is required to understand interactions in complex application scenarios. In this study, the speech and head movements of eight normal-hearing participants were recorded in two- and four-person interactive conversational tasks, with and without 4-talker babble noise at 75 dB(A) and reverberation times of 0.25 and 0.6 s. Two-person conversations showed a head movement (yaw) interquartile range of 11.6° while four-person conversations showed a statistically significantly different interquartile range of 21.9°. No effect of acoustic condition was observed. The recorded data were also successfully used to test a previously published hearing-device direction of arrival estimation algorithm that utilized head movement information and correlation lag between acoustic signals from the left and right ear",
    "checked": true,
    "id": "b13bae98958e3b65b33389c51f82cef85e5412b3",
    "semantic_title": "head movements in two- and four-person interactive conversational tasks in noisy and moderately reverberant conditions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23d_interspeech.html": {
    "title": "Second language identification of Vietnamese tones by native Mandarin learners",
    "volume": "main",
    "abstract": "In this study, we examined native language phonological and phonetic factors in identification of second language (L2) tones by learners with different learning experience. Results show that when L2 tones were Categorised with high percent choice and goodness ratings into native categories, it can be accurately identified. Uncategorised L2 tones tend to be mis-identified as other L2 tones that were assimilated into overlapping native response categories. L2 tones with unique phonetic characteristics can be easily identified even though they were Uncategorised. Thus, our research has theoretical implications for L2 speech learning models and ramifications for L2 lexical tone teaching and learning",
    "checked": true,
    "id": "263795e40a4d1cfa0c6ecae1776f502d985af733",
    "semantic_title": "second language identification of vietnamese tones by native mandarin learners",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fagniart23_interspeech.html": {
    "title": "Nasal vowel production and grammatical processing in French-speaking children with cochlear implants and normal-hearing peers",
    "volume": "main",
    "abstract": "Our study investigates morphemic receptive skills, quality of phonological representations, and production abilities of the acoustic nasal feature in French, a language with nasal vocalic phonemes. We examine these skills in two groups: children with cochlear implants (CI group) and their normal hearing peers (NH group). The results reveal weaker receptive skills and more phonological errors in production among the CI group. Additionally, the CI group shows a specific use of different acoustic cues associated with vowel nasality, suggesting a perceptual-productive profile that focuses more on perceptually salient cues. This profile may be related to language skills, as the use of subtle acoustic cues is associated with better morphemic processing skills",
    "checked": true,
    "id": "a793bbd1746b2e28a159d7e16ac07b9d725b03af",
    "semantic_title": "nasal vowel production and grammatical processing in french-speaking children with cochlear implants and normal-hearing peers",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23f_interspeech.html": {
    "title": "Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech",
    "volume": "main",
    "abstract": "Emotion classification with EEG responses can be used in human-computer interaction, security, medical treatment, etc. Neural responses recorded via EEG can reflect more direct and objective emotional information than other behavioral signals (i.e., facial expression...). In most previous studies, only features of EEG were used as input for machine learning models. In this work, we assumed that the emotional features included in speech stimuli could assist in emotion recognition with EEG when the emotion is evoked by the emotional prosody of speech. An EEG data corpus was collected with specific speech stimuli, in which emotion was represented with only speech prosody and without semantic context. A novel EEG-Prosody CRNN model was proposed to classify four types of typical emotions. The classification accuracy can achieve at 82.85% when the prosody features of speech were integrated as input, which outperformed most audio-evoked EEG-based emotion classification methods",
    "checked": true,
    "id": "2163e8be52f4841e766fb6a56e658c2a61a81fc0",
    "semantic_title": "emotion classification with eeg responses evoked by emotional prosody of speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23b_interspeech.html": {
    "title": "L2-Mandarin regional accent variability during Mandarin tone-word training facilitates English listeners' subsequent tone categorizations",
    "volume": "main",
    "abstract": "We examined how accent variability during training on minimal-tone-contrast Mandarin words affects English listeners' subsequent generalization of tone categorization and discrimination to new talkers and accents. English listeners underwent 6 days of training on 16 pseudowords (4 tones 4 sets) produced by 12 talkers of either Beijing (n = 24) or a mix of Beijing, Yantai, and Guangzhou (n = 24) accents. In a post-training test, they used tone contour icons to categorize the tones produced by new talkers with a familiar and an unfamiliar accent, and to discriminate all possible tone contrasts for the new talkers. While both training groups discriminated all six tone contrasts for both new talkers, only the multiple accent group reliably categorized all four tones. The single accent group failed to correctly categorize the falling tone in both generalization tests. These results suggest that accent variability during tone-word training can facilitate subsequent tone categorization",
    "checked": true,
    "id": "ed261351ce7fe6a552f91fc8642704860d998b3c",
    "semantic_title": "l2-mandarin regional accent variability during mandarin tone-word training facilitates english listeners' subsequent tone categorizations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ueda23_interspeech.html": {
    "title": "HumanDiffusion: diffusion model using perceptual gradients",
    "volume": "main",
    "abstract": "We propose HumanDiffusion, a diffusion model trained from humans' perceptual gradients to learn an acceptable range of data for humans (i.e., human-acceptable distribution). Conventional HumanGAN aims to model the human-acceptable distribution wider than the real-data distribution by training a neural network-based generator with human-based discriminators. However, HumanGAN training tends to converge in a meaningless distribution due to the gradient vanishing or mode collapse and requires careful heuristics. In contrast, our HumanDiffusion learns the human-acceptable distribution through Langevin dynamics based on gradients of human perceptual evaluations. Our training iterates a process to diffuse real data to cover a wider human-acceptable distribution and can avoid the issues in the HumanGAN training. The evaluation results demonstrate that our HumanDiffusion can successfully represent the human-acceptable distribution without any heuristics for the training",
    "checked": true,
    "id": "034bb8e23d1095f7949449cef09c1aca610583f9",
    "semantic_title": "humandiffusion: diffusion model using perceptual gradients",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kachel23_interspeech.html": {
    "title": "Queer Events, Relationships, and Sports: Does Topic Influence Speakers' Acoustic Expression of Sexual Orientation?",
    "volume": "main",
    "abstract": "Studies on the acoustic parameterization of actual and perceived sexual orientation yielded inconclusive findings. One reason for this could be different linguistic and situational factors underlying these studies. In the present research, we aim to illuminate inconsistent findings by systematically varying the way sexual orientation was made salient in interview topics: Lesbian/gay and straight women and men (n = 72) were asked to answer questions referring to lesbian/gay issues, to their own sexual orientation, and to a non-sexual orientation topic. Applying a person perceptions approach that provides a holistic measure for phonetic variation across topics, raters (n = 35) were asked to judge speakers' sexual orientations. Overall, straight speakers were rated as straighter than lesbian/gay speakers. Contrary to expectations, this difference was largest in the control condition. Results are discussed in terms of the same topic having differential effects on different speaker groups",
    "checked": true,
    "id": "348b82e8a51ebf33dfc16cbd667dba61e949093b",
    "semantic_title": "queer events, relationships, and sports: does topic influence speakers' acoustic expression of sexual orientation?",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gunason23_interspeech.html": {
    "title": "Epoch-Based Spectrum Estimation for Speech",
    "volume": "main",
    "abstract": "An implicit assumption when using the discrete Fourier transform for spectrum estimation is that the time signal is periodic. This assumption clashes with the quasi-periodicity of voiced speech when the traditional short-time Fourier transform (STFT) is applied to it. This causes distortion and leads to a performance handicap in downstream processing. This work proposes a remedy to this by using epochs in the signal to determine better frame boundaries for the Fourier transform. The epochs are the estimated glottal closure instants in voiced speech and significant peaks in the unvoiced speech signal. The resulting coefficients are compared to the traditional STFT coefficients using copy-synthesis. An improvement of 15 dB signal-to-noise ratio and a PESQ score of 2.5 to 3.5 is achieved for copy-synthesis using 20 mel-filters. The results demonstrate that there is a great potential in improving down stream speech processing applications using this approach to spectrum estimation",
    "checked": true,
    "id": "fcd6b934d940c4ae603877f57a7f66fa1a028ea1",
    "semantic_title": "epoch-based spectrum estimation for speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mehta23_interspeech.html": {
    "title": "OverFlow: Putting flows on top of neural transducers for better TTS",
    "volume": "main",
    "abstract": "Neural HMMs are a type of neural transducer recently proposed for sequence-to-sequence modelling in text-to-speech. They combine the best features of classic statistical speech synthesis and modern neural TTS, requiring less data and fewer training updates, and are less prone to gibberish output caused by neural attention failures. In this paper, we combine neural HMM TTS with normalising flows for describing the highly non-Gaussian distribution of speech acoustics. The result is a powerful, fully probabilistic model of durations and acoustics that can be trained using exact maximum likelihood. Experiments show that a system based on our proposal needs fewer updates than comparable methods to produce accurate pronunciations and a subjective speech quality close to natural speech",
    "checked": true,
    "id": "ec3b95ec32d3487fd80b8aa9a541b7c623bed29b",
    "semantic_title": "overflow: putting flows on top of neural transducers for better tts",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mehrish23_interspeech.html": {
    "title": "ADAPTERMIX: Exploring the Efficacy of Mixture of Adapters for Low-Resource TTS Adaptation",
    "volume": "main",
    "abstract": "There are significant challenges for speaker adaptation in text- to-speech for languages that are not widely spoken or for speakers with accents or dialects that are not well-represented in the training data. To address this issue, we propose the use of the \"mixture of adapters\" method. This approach involves adding multiple adapters within a backbone-model layer to learn the unique characteristics of different speakers. Our approach outperforms the baseline, with a noticeable improvement of 5% observed in speaker preference tests when using only one minute of data for each new speaker. Moreover, following the adapter paradigm, we fine-tune only the adapter parameters (11% of the total model parameters). This is a significant achievement in parameter-efficient speaker adaptation, and one of the first models of its kind. Overall, our proposed approach offers a promising solution to the speech synthesis techniques, particularly for adapting to speakers from diverse backgrounds",
    "checked": true,
    "id": "2df789e62091d3e4868afc638d2dcb14d13165b9",
    "semantic_title": "adaptermix: exploring the efficacy of mixture of adapters for low-resource tts adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23c_interspeech.html": {
    "title": "Prior-free Guided TTS: An Improved and Efficient Diffusion-based Text-Guided Speech Synthesis",
    "volume": "main",
    "abstract": "Recently, diffusion models have exhibited higher sample quality with guidance, such as classifier guidance and classifier-free guidance. However, these guidances have limitations: they require extra classifiers or joint training, and incur additional sampling cost. In this study, we propose prior-free guidance diffusion model and prior-free guided text-to-speech (PfGuided-TTS) that can generate a speech at a quality as high as other guidances without extra training resources and computational cost. PfGuided-TTS can generate higher human perceptual quality speech than the existing autoregressive (AR) and non-AR models, including diffusion-based TTS on LJSpeech. In addition, we provide a schematic describing why and how classifier- and prior-free guided scores produce high-fidelity samples",
    "checked": true,
    "id": "a34d2a44cc52dbcd10b07c93b2d31cec5e466c4d",
    "semantic_title": "prior-free guided tts: an improved and efficient diffusion-based text-guided speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/iashchenko23_interspeech.html": {
    "title": "UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model",
    "volume": "main",
    "abstract": "This paper introduces UnDiff, a diffusion probabilistic model capable of solving various speech inverse tasks. Being once trained for speech waveform generation in an unconditional manner, it can be adapted to different tasks including degradation inversion, neural vocoding, and source separation. In this paper, we, first, tackle the challenging problem of unconditional waveform generation by comparing different neural architectures and preconditioning domains. After that, we demonstrate how the trained unconditional diffusion could be adapted to different tasks of speech processing by the means of recent developments in post-training conditioning of diffusion models. Finally, we demonstrate the performance of the proposed technique on the tasks of bandwidth extension, declipping, vocoding, and speech source separation and compare it to the baselines",
    "checked": true,
    "id": "f2928c6b1696c73bd2ef746da341717c777d0256",
    "semantic_title": "undiff: unsupervised voice restoration with unconditional diffusion model",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23_interspeech.html": {
    "title": "Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech",
    "volume": "main",
    "abstract": "For personalized speech generation, a neural text-to-speech (TTS) model must be successfully implemented with limited data from a target speaker. To this end, the baseline TTS model needs to be amply generalized to out-of-domain data (i.e., target speaker's speech). However, approaches to address this out-of-domain generalization problem in TTS have yet to be thoroughly studied. In this work, we propose an effective pruning method for a transformer known as sparse attention, to improve the TTS model's generalization abilities. In particular, we prune off redundant connections from self-attention layers whose attention weights are below the threshold. To flexibly determine the pruning strength for searching optimal degree of generalization, we also propose a new differentiable pruning method that allows the model to automatically learn the thresholds. Evaluations on zero-shot multi-speaker TTS verify the effectiveness of our method in terms of voice quality and speaker similarity",
    "checked": true,
    "id": "93a9f02a180543644689b294fbaed245507f8b98",
    "semantic_title": "pruning self-attention for zero-shot multi-speaker text-to-speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/guan23_interspeech.html": {
    "title": "Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge",
    "volume": "main",
    "abstract": "With the demand for autonomous control and personalized speech generation, the style control and transfer in Text-to-Speech (TTS) is becoming more and more important. In this paper, we propose a new TTS system that can perform style transfer with interpretability and high fidelity. Firstly, we design a TTS system that combines variational autoencoder (VAE) and diffusion refiner to get refined mel-spectrograms. Specifically, a two-stage and a one-stage system are designed respectively, to improve the audio quality and the performance of style transfer. Secondly, a diffusion bridge of quantized VAE is designed to efficiently learn complex discrete style representations and improve the performance of style transfer. To have a better ability of style transfer, we introduce ControlVAE to improve the reconstruction quality and have good interpretability simultaneously. Experiments on LibriTTS dataset demonstrate that our method is more effective than baseline models",
    "checked": true,
    "id": "4b579f6e5686f680ae84d6c7bac0b8d2a9cb8535",
    "semantic_title": "interpretable style transfer for text-to-speech with controlvae and diffusion bridge",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kogel23_interspeech.html": {
    "title": "Towards Robust FastSpeech 2 by Modelling Residual Multimodality",
    "volume": "main",
    "abstract": "State-of-the-art non-autoregressive text-to-speech (TTS) models based on FastSpeech 2 can efficiently synthesise high-fidelity and natural speech. For expressive speech datasets however, we observe characteristic audio distortions. We demonstrate that such artefacts are introduced to the vocoder reconstruction by over-smooth mel-spectrogram predictions, which are induced by the choice of mean-squared-error (MSE) loss for training the mel-spectrogram decoder. With MSE loss FastSpeech 2 is limited to learn conditional averages of the training distribution, which might not lie close to a natural sample if the distribution still appears multimodal after all conditioning signals. To alleviate this problem, we introduce TVC-GMM, a mixture model of Trivariate-Chain Gaussian distributions, to model the residual multimodality. TVC-GMM reduces spectrogram smoothness and improves perceptual audio quality in particular for expressive datasets as shown by both objective and subjective evaluation",
    "checked": true,
    "id": "d51872c84c40fb1378655da91c841cd0be6090f4",
    "semantic_title": "towards robust fastspeech 2 by modelling residual multimodality",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23c_interspeech.html": {
    "title": "Real time spectrogram inversion on mobile phone",
    "volume": "main",
    "abstract": "We present two methods of real time magnitude spectrogram inversion: streaming Griffin Lim(GL) and streaming MelGAN. We demonstrate the impact of looking ahead on perceptual quality of MelGAN. As little as one hop size (12.5ms) of lookahead is able to significantly improve perceptual quality in comparison to its causal version. We compare streaming GL with the streaming MelGAN and show different trade-offs in terms of perceptual quality, on-device latency, algorithmic delay, memory footprint and noise sensitivity. For fair quality assessment of the GL approach, we use input log magnitude spectrogram without mel transformation. We evaluate presented real time spectrogram inversion approaches on clean, noisy and atypical speech. We specified conditions when streaming GL has comparable quality with MelGAN: noisy audio and no mel transformation. Streaming GL is 2.4x faster than real time on the ARM CPU of a Pixel4 and has 250x times smaller memory footprint than MelGAN",
    "checked": true,
    "id": "19b020eda40c1b54a7ccf27b6ceba43fe1af3524",
    "semantic_title": "real time spectrogram inversion on mobile phone",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23_interspeech.html": {
    "title": "Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis",
    "volume": "main",
    "abstract": "Recently, zero-shot TTS and VC methods have gained attention due to their practicality of being able to generate voices even unseen during training. Among these methods, zero-shot modifications of the VITS model have shown superior performance, while having useful properties inherited from VITS. However, the performance of VITS and VITS-based zero-shot models vary dramatically depending on how the losses are balanced. This can be problematic, as it requires a burdensome procedure of tuning loss balance hyper-parameters to find the optimal balance. In this work, we propose a novel framework that finds this optimum without search, by inducing the decoder of VITS-based models to its full reconstruction ability. With our framework, we show superior performance compared to baselines in zero-shot TTS and VC, achieving state-of-the-art performance. Furthermore, we show the robustness of our framework in various settings. We provide an explanation for the results in the discussion",
    "checked": true,
    "id": "a2420378833122f1abdbda66c2a7d663be50abb4",
    "semantic_title": "automatic tuning of loss trade-offs without hyper-parameter search in end-to-end zero-shot speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wells23_interspeech.html": {
    "title": "A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic",
    "volume": "main",
    "abstract": "In this work we present an end-to-end pipeline for building a speech corpus and text-to-speech synthesis system for a new language without reference to any expert-defined linguistic resources. We segment and align over 85 hours of Scottish Gaelic recordings found online and select 2- and 8-hour subsets with comprehensive coverage of speech sounds based on self-supervised discrete acoustic unit sequences. We then compare FastPitch models trained on these relatively small data sets using character, acoustic unit and phone inputs. According to native speaker listening test judgements, characters serve well for Gaelic given its regular orthography, even in these limited data scenarios. We release our corpus building recipe so that others may easily apply our work to new languages",
    "checked": true,
    "id": "54a1ae69a2622bac5df75489d2b48189f6e502c2",
    "semantic_title": "a low-resource pipeline for text-to-speech from found data with application to scottish gaelic",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/krug23_interspeech.html": {
    "title": "Self-Supervised Solution to the Control Problem of Articulatory Synthesis",
    "volume": "main",
    "abstract": "Given an articulatory-to-acoustic forward model, it is a priori unknown how its motor control must be operated to achieve a desired acoustic result. This control problem is a fundamental issue of articulatory speech synthesis and the cradle of acoustic-to-articulatory inversion, a discipline which attempts to address the issue by the means of various methods. This work presents an end-to-end solution to the articulatory control problem, in which synthetic motor trajectories of Monte-Carlo-generated artificial speech are linked to input modalities (such as natural speech recordings or phoneme sequence input) via speaker-independent latent representations of a vector-quantized variational autoencoder. The proposed method is self-supervised and thus, in principle, synthesizer and speaker model independent",
    "checked": true,
    "id": "67501a1c7ed1f5d1bc543902e7ba6a6d95c02ac6",
    "semantic_title": "self-supervised solution to the control problem of articulatory synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23f_interspeech.html": {
    "title": "Hierarchical Timbre-Cadence Speaker Encoder for Zero-shot Speech Synthesis",
    "volume": "main",
    "abstract": "Although recent zero-shot text-to-speech (zs-TTS) models have shown high performance in terms of speech quality, speaker similarity is not up to par. Speaker similarity can be expressed in two different components: intra-speaker consistent component (timbre) and inter-utterance variate component (cadence). In this paper, we propose a timbre-cadence speaker encoder for zs-TTS that improves speaker similarity by modeling these components. To disentangle timbre and cadence more efficiently, we employ a hierarchical structure. The cadence embedding is first encoded with VICReg which enlarges the inter-utterance embedding within a batch. Next, timbre embedding is extracted after subtracting cadence embedding and using a loss between timbre embedding and speaker ID-based speaker embedding. Additionally, we propose an effective data augmentation called speaker mixing augmentation, where two short utterances from different speakers are concatenated for a more robust zs-TTS model",
    "checked": true,
    "id": "d3c664d42a136a25cf1271cd3cd5a6f1136f04b5",
    "semantic_title": "hierarchical timbre-cadence speaker encoder for zero-shot speech synthesis",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23_interspeech.html": {
    "title": "ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models",
    "volume": "main",
    "abstract": "Emotional Text-To-Speech (TTS) is an important task in the development of systems (e.g., human-like dialogue agents) that require natural and emotional speech. Existing approaches, however, only aim to produce emotional TTS for seen speakers during training, without consideration of the generalization to unseen speakers. In this paper, we propose ZET-Speech, a zero-shot adaptive emotion-controllable TTS model that allows users to synthesize any speaker's emotional speech using only a short, neutral speech segment and the target emotion label. Specifically, to enable a zero-shot adaptive TTS model to synthesize emotional speech, we propose domain adversarial learning and guidance methods on the diffusion model. Experimental results demonstrate that ZET-Speech successfully synthesizes natural and emotional speech with the desired emotion for both seen and unseen speakers. Samples are at https://ZET-Speech.github.io/ZET-Speech-Demo/",
    "checked": true,
    "id": "b1d637a6534f1d9ee22768118af9a1e9ae06c944",
    "semantic_title": "zet-speech: zero-shot adaptive emotion-controllable text-to-speech synthesis with diffusion and style-based models",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/du23_interspeech.html": {
    "title": "Improving WaveRNN with Heuristic Dynamic Blending for Fast and High-Quality GPU Vocoding",
    "volume": "main",
    "abstract": "Auto-regressive vocoders are typically less efficient at inference due to their serial nature, making it difficult to fully utilize graphics processing units (GPUs). In this context, batched inference with upsampled feature folding can be used to speed up vocoding. However, speech quality degradation caused by blending folded waveform segments making it hard to be applied to production. To address this issue, we propose a novel blending approach called heuristic dynamic blending (HDB), which effectively addresses the voice trembling and echo artifacts of conventional static blending. We also propose a parallel algorithm of HDB running on GPUs, which significantly reduces the additional time overhead introduced by the naive HDB algorithm. Experimental results demonstrate that using a multi-band WaveRNN with HDB can effectively improve parallelism for real-time GPU vocoding while maintaining high speech quality comparable to non-folding inference",
    "checked": true,
    "id": "751889eac2e35ed831dfb73114bbfa7827c35d59",
    "semantic_title": "improving wavernn with heuristic dynamic blending for fast and high-quality gpu vocoding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23_interspeech.html": {
    "title": "Intelligible Lip-to-Speech Synthesis with Speech Units",
    "volume": "main",
    "abstract": "In this paper, we propose a novel Lip-to-Speech synthesis (L2S) framework, for synthesizing intelligible speech from a silent lip movement video. Specifically, to complement the insufficient supervisory signal of the previous L2S model, we propose to use quantized self-supervised speech representations, named speech units, as an additional prediction target for the L2S model. Therefore, the proposed L2S model is trained to generate multiple targets, mel-spectrogram and speech units. As the speech units are discrete while mel-spectrogram is continuous, the proposed multi-target L2S model can be trained with strong content supervision, without using text-labeled data. Moreover, to accurately convert the synthesized mel-spectrogram into a waveform, we introduce a multi-input vocoder that can generate a clear waveform even from blurry and noisy mel-spectrogram by referring to the speech units. Extensive experimental results confirm the effectiveness of the proposed method in L2S",
    "checked": true,
    "id": "bffaa2ab41eb0d68fe60c9ac77a7fe3b132c9d62",
    "semantic_title": "intelligible lip-to-speech synthesis with speech units",
    "citation_count": 10,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23p_interspeech.html": {
    "title": "Parameter-Efficient Learning for Text-to-Speech Accent Adaptation",
    "volume": "main",
    "abstract": "This paper presents a parameter-efficient learning (PEL) to develop a low-resource accent adaptation for text-to-speech (TTS). A resource-efficient adaptation from a frozen pre-trained TTS model is developed by using only 1.2% to 0.8% of original trainable parameters to achieve competitive performance in voice synthesis. Motivated by a theoretical foundation of optimal transport (OT), this study carries out PEL for TTS where an auxiliary unsupervised loss based on OT is introduced to maximize a difference between the pre-trained source domain and the (unseen) target domain, in addition to its supervised training loss. Further, we leverage upon this unsupervised loss refinement to boost system performance via either sliced Wasser-stein distance or maximum mean discrepancy. The merit of this work is demonstrated by fulfilling PEL solutions based on residual adapter learning, and model reprogramming when evaluating the Mandarin accent adaptation. Experiment results show that the proposed methods can achieve competitive naturalness with parameter-efficient decoder fine-tuning, and the auxiliary unsupervised loss improves model performance empirically",
    "checked": true,
    "id": "796b30fecf6103420b713e4267ac1c4b4e335ba8",
    "semantic_title": "parameter-efficient learning for text-to-speech accent adaptation",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/khan23_interspeech.html": {
    "title": "Controlling formant frequencies with neural text-to-speech for the manipulation of perceived speaker age",
    "volume": "main",
    "abstract": "In this paper, we present a framework for formant-controllable neural text-to-speech. We train a model that predicts formant frequencies which then condition melspectrogram generation. We apply this to manipulate perceived speaker age in an indirect fashion, by modifying the predicted formants in a manner that affects perceived vocal tract length. Our ultimate goal is to allow for the control of perceived ageing in children's text-to-speech voices, since ageing in natural child speech is strongly linked to the growth of a child's vocal tract. However, our experiments indicate that our method shows strong age control capabilities for adult speech as well",
    "checked": true,
    "id": "00989aa3df58e3054a6ab1375d42c01b2129f2be",
    "semantic_title": "controlling formant frequencies with neural text-to-speech for the manipulation of perceived speaker age",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jang23b_interspeech.html": {
    "title": "FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs",
    "volume": "main",
    "abstract": "This paper presents FastFit, a novel neural vocoder architecture that replaces the U-Net encoder with multiple short-time Fourier transforms (STFTs) to achieve faster generation rates without sacrificing sample quality. We replaced each encoder block with an STFT, with parameters equal to the temporal resolution of each decoder block, leading to the skip connection. FastFit reduces the number of parameters and the generation time of the model by almost half while maintaining high fidelity. Through objective and subjective evaluations, we demonstrated that the proposed model achieves nearly twice the generation speed of baseline iteration-based vocoders while maintaining high sound quality. We further showed that FastFit produces sound qualities similar to those of other baselines in text-to-speech evaluation scenarios, including multi-speaker and zero-shot text-to-speech",
    "checked": true,
    "id": "3cc32c54d374fbd5a885dc1f33628abd434da5e2",
    "semantic_title": "fastfit: towards real-time iterative neural vocoder by replacing u-net encoder with multiple stfts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kaneko23_interspeech.html": {
    "title": "iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN",
    "volume": "main",
    "abstract": "The inverse short-time Fourier transform network (iSTFTNet) has garnered attention owing to its fast, lightweight, and high-fidelity speech synthesis. It obtains these characteristics using a fast and lightweight 1D CNN as the backbone and replacing some neural processes with iSTFT. Owing to the difficulty of a 1D CNN to model high-dimensional spectrograms, the frequency dimension is reduced via temporal upsampling. However, this strategy compromises the potential to enhance the speed. Therefore, we propose iSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and 2D CNNs to model temporal and spectrogram structures, respectively. We designed a 2D CNN that performs frequency upsampling after conversion in a few-frequency space. This design facilitates the modeling of high-dimensional spectrograms without compromising the speed. The results demonstrated that iSTFTNet2 made iSTFTNet faster and more lightweight with comparable speech quality",
    "checked": true,
    "id": "55594d1562eee4c83c1dae2504ab3aaaa38669fa",
    "semantic_title": "istftnet2: faster and more lightweight istft-based neural vocoder using 1d-2d cnn",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23_interspeech.html": {
    "title": "VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design",
    "volume": "main",
    "abstract": "Single-stage text-to-speech models have been actively studied recently, and their results have outperformed two-stage pipeline systems. Although the previous single-stage model has made great progress, there is room for improvement in terms of its intermittent unnaturalness, computational efficiency, and strong dependence on phoneme conversion. In this work, we introduce VITS2, a single-stage text-to-speech model that efficiently synthesizes a more natural speech by improving several aspects of the previous work. We propose improved structures and training mechanisms and present that the proposed methods are effective in improving naturalness, similarity of speech characteristics in a multi-speaker model, and efficiency of training and inference. Furthermore, we demonstrate that the strong dependence on phoneme conversion in previous works can be significantly reduced with our method, which allows a fully end-to-end single-stage approach",
    "checked": true,
    "id": "02120b301c25ee450cdd9dca9401f7b13681a708",
    "semantic_title": "vits2: improving quality and efficiency of single-stage text-to-speech with adversarial learning and architecture design",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luong23_interspeech.html": {
    "title": "Controlling Multi-Class Human Vocalization Generation via a Simple Segment-based Labeling Scheme",
    "volume": "main",
    "abstract": "As prompt-based generative models have received much attention, many studies have proposed a similar model for sound generation. While prompt-based generative models have an intuitive interface for non-professional users to experiment with, they lack the ability to control the generated sounds via a more direct means. In this work, we investigated the use of a simple segment-based labeling scheme for human vocalization generation, which is a specific subset of sound generation. By conditioning the generative models on the label sequence which marks the vocalization class of the segment, the generated sound can be controlled in a more detailed manner while maintaining a simple and intuitive input interface. Our experiments showed that simply switching the label scheme from global to segment-based does not degrade the quality of the generated samples in any way and provides a new method of controlling the generation process",
    "checked": true,
    "id": "ccdc2d8d84cd4cf6639cba7e37b6a96101be6582",
    "semantic_title": "controlling multi-class human vocalization generation via a simple segment-based labeling scheme",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhogale23_interspeech.html": {
    "title": "Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR",
    "volume": "main",
    "abstract": "Improving ASR systems is necessary to make new LLM-based use-cases accessible to people across the globe. In this paper, we focus on Indian languages, and make the case that diverse benchmarks are required to evaluate and improve ASR systems for Indian languages. To address this, we collate Vistaar as a set of 59 benchmarks across various language and domain combinations, on which we evaluate 3 publicly available ASR systems and 2 commercial systems. We also train IndicWhisper models by fine-tuning the Whisper models on publicly available training datasets across 12 Indian languages totalling to 10.7K hours. We show that IndicWhisper significantly improves on considered ASR systems on the Vistaar benchmark. Indeed, IndicWhisper has the lowest WER in 39 out of the 59 benchmarks, with an average reduction of 4.1 WER. We open-source all datasets, code and models - https://github.com/AI4Bharat/vistaar",
    "checked": true,
    "id": "2357a12c0cc43f01f576d25ab7c0f5a0622b6c32",
    "semantic_title": "vistaar: diverse benchmarks and training sets for indian language asr",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23_interspeech.html": {
    "title": "Domain Adaptive Self-supervised Training of Automatic Speech Recognition",
    "volume": "main",
    "abstract": "This paper explores domain adaptive self-supervised training of automatic speech recognition (ASR). Unlabeled data from the target domain can either be used in training the self-supervised pre-trained model or in the fine-tuning stage using semi-supervised approaches for the ASR task or both. Here we specifically focus on how semi-supervised approaches can enhance domain adaptation of pre-trained models built using self-supervised learning (SSL). For the purpose of this study, we use variants of English accents as the data from different domains. ASR experiments targeting single domain achieve relative word error rate (WER) reduction in the range 2.7-41.8% based on the extent of domain mismatch, while in the multiple-domain setting we achieve a relative WER reduction of 8% on average using semi-supervised fine-tuning on top of the model pre-trained with target domain using SSL",
    "checked": true,
    "id": "7e2b9882e018d6b0b4f31a49a92a64116b2e8c05",
    "semantic_title": "domain adaptive self-supervised training of automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/olivier23_interspeech.html": {
    "title": "There is more than one kind of robustness: Fooling Whisper with adversarial examples",
    "volume": "main",
    "abstract": "Whisper is a recent Automatic Speech Recognition (ASR) model displaying impressive robustness to both out-of-distribution inputs and random noise. In this work, we show that this robustness does not carry over to adversarial noise. We generate very small input perturbations with Signal Noise Ratio of up to 45dB, with which we can degrade Whisper performance dramatically, or even transcribe a target sentence of our choice. We also show that by fooling the Whisper language detector we can very easily degrade the performance of multilingual models. These vulnerabilities of a widely popular open-source model have practical security implications, and emphasize the need for adversarially robust ASR",
    "checked": true,
    "id": "286faebc2be7050c0ab4c049f9db7e9bdf81cbca",
    "semantic_title": "there is more than one kind of robustness: fooling whisper with adversarial examples",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heggan23_interspeech.html": {
    "title": "MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations",
    "volume": "main",
    "abstract": "Contrastive self-supervised learning has gained attention for its ability to create high-quality representations from large unlabelled data sets. A key reason that these powerful features enable data-efficient learning of downstream tasks is that they provide augmentation invariance, which is often a useful inductive bias. However, the amount and type of invariances preferred is not known apriori, and varies across different downstream tasks. We therefore propose a multi-task self-supervised framework (MT-SLVR) that learns both variant and invariant features in a parameter-efficient manner. Our multi-task representation provides a strong and flexible feature that benefits diverse downstream tasks. We evaluate our approach on few-shot classification tasks drawn from a variety of audio domains and demonstrate improved classification performance on all of them",
    "checked": true,
    "id": "8aee1d46e421c96a0b66e02827d8be50dc5d4dfa",
    "semantic_title": "mt-slvr: multi-task self-supervised learning for transformation in(variant) representations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23l_interspeech.html": {
    "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made opensource in ESPnet",
    "checked": true,
    "id": "1dc2d0f43df7f7a7847817203411357eca79a5b3",
    "semantic_title": "reducing barriers to self-supervised learning: hubert pre-training with academic compute",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23l_interspeech.html": {
    "title": "Blank-regularized CTC for Frame Skipping in Neural Transducer",
    "volume": "main",
    "abstract": "Neural Transducer and connectionist temporal classification (CTC) are popular end-to-end automatic speech recognition systems. Due to their frame-synchronous design, blank symbols are introduced to address the length mismatch between acoustic frames and output tokens, which might bring redundant computation. Previous studies managed to accelerate the training and inference of neural Transducers by discarding frames based on the blank symbols predicted by a co-trained CTC. However, there is no guarantee that the co-trained CTC can maximize the ratio of blank symbols. This paper proposes two novel regularization methods to explicitly encourage more blanks by constraining the self-loop of non-blank symbols in the CTC. It is interesting to find that the frame reduction ratio of the neural Transducer can approach the theoretical boundary. Experiments on LibriSpeech corpus show that our proposed method accelerates the inference of neural Transducer by 4 times without sacrificing performance",
    "checked": true,
    "id": "7ab3e30b28d7cc024d3334a00725854bd56aaaf9",
    "semantic_title": "blank-regularized ctc for frame skipping in neural transducer",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jayakumar23_interspeech.html": {
    "title": "The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR",
    "volume": "main",
    "abstract": "Building a multilingual Automated Speech Recognition (ASR) system in a linguistically diverse country like India can be a challenging task due to the differences in scripts and the limited availability of speech data. This problem can be solved by exploiting the fact that many of these languages are phonetically similar. These languages can be converted into a Common Label Set (CLS) by mapping similar sounds to common labels. In this paper, new approaches are explored and compared to improve the performance of CLS based multilingual ASR model. Specific language information is infused in the ASR model by giving Language ID or using CLS to Native script converter on top of the CLS Multilingual model. These methods give a significant improvement in Word Error Rate (WER) compared to the CLS baseline. These methods are further tried on out-of-distribution data to check their robustness",
    "checked": true,
    "id": "2398551c7433c80444ebd784394f01901b2ddbea",
    "semantic_title": "the tag-team approach: leveraging cls and language tagging for enhancing multilingual asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/unni23_interspeech.html": {
    "title": "Improving RNN-Transducers with Acoustic LookAhead",
    "volume": "main",
    "abstract": "RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end model for speech to text conversion because of their high accuracy and streaming capabilities. A typical RNN-T independently encodes the input audio and the text context, and combines the two encodings by a thin joint network. While this architecture provides SOTA streaming accuracy, it also makes the model vulnerable to strong LM biasing which manifests as multi-step hallucination of text without acoustic evidence. In this paper we propose LOOKAHEAD that makes text representations more acoustically grounded by looking ahead into the future within the audio input. This technique yields a significant 5% - 20% relative reduction in word error rate on both in-domain and out-of-domain evaluation sets",
    "checked": true,
    "id": "5d1e580f8e269620d5bf358250fbd27c257ce65d",
    "semantic_title": "improving rnn-transducers with acoustic lookahead",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/markl23_interspeech.html": {
    "title": "Everyone has an accent",
    "volume": "main",
    "abstract": "In this paper, we consider how the notion \"accent\" in particular in the context of \"accented speech\" has been discussed in Interspeech publications between 2004 and 2022. We contrast the way speech technology research published in the conference has conceptualised these terms with their usage in linguistics. The point of this comparison is to: highlight significant inter-disciplinary differences in the way apparently core terms are used, discuss disadvantages of using inexact language in research, and encourage researchers to be more mindful about the use of particular short-hands",
    "checked": true,
    "id": "07b8a981606dad40528a492e619739800feaf171",
    "semantic_title": "everyone has an accent",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/maison23_interspeech.html": {
    "title": "Some Voices are Too Common: Building Fair Speech Recognition Systems Using the CommonVoice Dataset",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems become increasingly efficient thanks to new advances in neural network training like self-supervised learning. However, they are known to be unfair toward certain groups, for instance, people speaking with an accent. In this work, we use the French Common Voice dataset to quantify the biases of a pre-trained wav2vec 2.0 model toward several demographic groups. By fine-tuning the pre-trained model on a variety of fixed-size, carefully crafted training sets, we demonstrate the importance of speaker diversity. We also run an in-depth analysis of the Common Voice corpus and identify important shortcomings that should be taken into account by users of this dataset",
    "checked": false,
    "id": "92efad297a183dc80507d9b8ba47a6a185d6ce0a",
    "semantic_title": "some voices are too common: building fair speech recognition systems using the common voice dataset",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23u_interspeech.html": {
    "title": "Information Magnitude Based Dynamic Sub-sampling for Speech-to-text",
    "volume": "main",
    "abstract": "Attention-based models have achieved new state-of-the-art in many tasks while the computational cost of these models increases drastically compared with previous methods. For most acoustic tasks, excessively long speech sequences exacerbate this problem and do not benefit a lot from the attention mechanism. We propose the information magnitude (IM) based dynamic stride convolution (IM-DSC) method. This method first calculates the IM according to the importance of each frame, then dynamically squeezes the redundant frames. We carry on experiments on speech translation and automatic speech recognition tasks. Our results show that we achieve 0.5 BLEU and 0.4 BLEU improvements on the MuST-C En-De and En-Fr data with a 22% compression ratio. For the ASR task, we gain a 0.2 WER reduction with a 21% compression ratio on the Librispeech data",
    "checked": true,
    "id": "e0364068f9bb56a77d81075b867417f535e3cdf6",
    "semantic_title": "information magnitude based dynamic sub-sampling for speech-to-text",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grice23_interspeech.html": {
    "title": "What's in a Rise? The Relevance of Intonation for Attention Orienting",
    "volume": "main",
    "abstract": "In this talk I will explore why and how intonational rises are used to orient attention towards the words and phrases bearing them. The attention orienting function of rising pitch is known outside the linguistic domain, with evidence from auditory looming, a phenomenon whereby a signal that increases in loudness or pitch appears to be approaching the listener and is perceived as an immediate threat This attention orienting function extends to speech communication, where rises in pitch are crucial for directing listeners' attention to the most important parts of the linguistic message. I will provide evidence from event related brain potentials that such rises affect both preattentive and conscious attention. Moreover, the lack of a rise can, in some situations, direct attention away from parts of the message, leading to information being missed. I will also discuss the influence of intonational rises on short-term memory, showing that rises can boost recall of items in a list. This effect can be local to a particular item if the rise is accentual, or more global if the rise is at the edge of a domain. However, despite the cross-linguistic effect of rises on attention, their influence can be impacted by language specific prosodic structure and linguistic expectations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23i_interspeech.html": {
    "title": "HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer",
    "volume": "main",
    "abstract": "Despite rapid progress in the voice style transfer (VST) field, recent zero-shot VST systems still lack the ability to transfer the voice style of a novel speaker. In this paper, we present HierVST, a hierarchical adaptive end-to-end zero-shot VST model. Without any text transcripts, we only use the speech dataset to train the model by utilizing hierarchical variational inference and self-supervised representation. In addition, we adopt a hierarchical adaptive generator that generates the pitch representation and waveform audio sequentially. Moreover, we utilize unconditional generation to improve the speaker-relative acoustic capacity in the acoustic representation. With a hierarchical adaptive structure, the model can adapt to a novel voice style and convert speech progressively. The experimental results demonstrate that our method outperforms other VST models in zero-shot VST scenarios. Audio samples are available at https://hiervst.github.io/",
    "checked": true,
    "id": "5425198786b9bd074d66191a1745e9573081f657",
    "semantic_title": "hiervst: hierarchical adaptive zero-shot voice style transfer",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23e_interspeech.html": {
    "title": "VISinger2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer",
    "volume": "main",
    "abstract": "End-to-end singing voice synthesis (SVS) model VISinger can achieve better performance than the typical two-stage model with fewer parameters. However, VISinger has several problems: text-to-phase problem, the end-to-end model learns the meaningless mapping of text-to-phase; glitches problem, the harmonic components corresponding to the periodic signal of the voiced segment occurs a sudden change with audible artefacts; low sampling rate, the sampling rate of 24KHz does not meet the application needs of high-fidelity generation with the full-band rate (44.1KHz or higher). In this paper, we propose VISinger 2 to address these issues by integrating the digital signal processing (DSP) methods with VISinger. Specifically, inspired by recent advances in differentiable digital signal processing (DDSP), we incorporate a DSP synthesizer into the decoder to solve the above issues. The DSP synthesizer consists of a harmonic synthesizer and a noise synthesizer to generate periodic and aperiodic signals, respectively, from the latent representation z in VISinger. It supervises the posterior encoder to extract the latent representation without phase information and avoid the prior encoder modelling text-to-phase mapping. To avoid glitch artefacts, the HiFiGAN is modified to accept the waveforms generated by the DSP synthesizer as a condition to produce the singing voice. Moreover, with the improved waveform decoder, VISinger 2 manages to generate 44.1kHz singing audio with richer expression and better quality. Experiments on OpenCpop corpus show that VISinger 2 outperforms VISinger, CpopSing and RefineSinger in both subjective and objective metrics. Our audio samples and source code are available",
    "checked": false,
    "id": "1a0a78cf184a80aaab8150f8435493c379633b2d",
    "semantic_title": "visinger 2: high-fidelity end-to-end singing voice synthesis enhanced by digital signal processing synthesizer",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23c_interspeech.html": {
    "title": "EdenTTS: A Simple and Efficient Parallel Text-to-speech Architecture with Collaborative Duration-alignment Learning",
    "volume": "main",
    "abstract": "In pursuit of high inference speed, many non-autoregressive neural text-to-speech (TTS) models have been proposed for parallel speech synthesis recently. A critical challenge of parallel speech generation lies in the learning of text-speech alignment. Existing methods usually require an external aligner for guidance or involve complex training process. In this work, we propose Eden-TTS, a simple and efficient parallel TTS architecture which jointly learns duration prediction, text-speech alignment and speech generation in a single fully-differentiable model. The alignment is learned implicitly in our architecture. A novel energy-modulated attention mechanism is proposed for alignment guidance which leads to fast and stable convergence of our model. Our model can be easily implemented and trained. Experiments demonstrate that our method can generate speech of high quality with high training efficiency",
    "checked": true,
    "id": "69b444bcb2b16af5cfd9403c74ba2b8a2985021a",
    "semantic_title": "edentts: a simple and efficient parallel text-to-speech architecture with collaborative duration-alignment learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23c_interspeech.html": {
    "title": "Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations",
    "volume": "main",
    "abstract": "While most research into speech synthesis has focused on synthesizing high-quality speech for in-dataset speakers, an equally essential yet unsolved problem is synthesizing speech for unseen speakers who are out-of-dataset with limited reference data, i.e., speaker adaptive speech synthesis. Many studies have proposed zero-shot speaker adaptive text-to-speech and voice conversion approaches aimed at this task. However, most current approaches suffer from the degradation of naturalness and speaker similarity when synthesizing speech for unseen speakers (i.e., speakers not in the training dataset) due to the poor generalizability of the model in out-of-distribution data. To address this problem, we propose GZS-TV, a generalizable zero-shot speaker adaptive text-to-speech and voice conversion model. GZS-TV introduces disentangled representation learning for both speaker embedding extraction and timbre transformation to improve model generalization and leverages the representation learning capability of the variational autoencoder to enhance the speaker encoder. Our experiments demonstrate that GZS-TV reduces performance degradation on unseen speakers and outperforms all baseline models in multiple datasets. The audio samples, detailed network architecture and pre-trained model are available at https://gzs-tv.github.io/",
    "checked": true,
    "id": "bc0f6ab69cdff091e3d7e75004a00dd24789638c",
    "semantic_title": "generalizable zero-shot speaker adaptive speech synthesis with disentangled representations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/montesinos23_interspeech.html": {
    "title": "Speech inpainting: Context-based speech synthesis guided by video",
    "volume": "main",
    "abstract": "Audio and visual modalities are inherently connected in speech signals: lip movements and facial expressions are correlated with speech sounds. This motivates studies that incorporate the visual modality to enhance an acoustic speech signal or even restore missing audio information. Specifically, this paper focuses on the problem of audio-visual speech inpainting, which is the task of synthesizing the speech in a corrupted audio segment in a way that it is consistent with the corresponding visual content and the uncorrupted audio context. We present an audio-visual transformer-based deep learning model that leverages visual cues that provide information about the content of the corrupted audio. It outperforms the previous state-of-the-art audio-visual model and audio-only baselines. We also show how visual features extracted with AV-HuBERT, a large audio-visual transformer for speech recognition, are suitable for synthesizing speech",
    "checked": true,
    "id": "b62eef28949a58169a9c15822c693507a3fb120f",
    "semantic_title": "speech inpainting: context-based speech synthesis guided by video",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23d_interspeech.html": {
    "title": "STEN-TTS: Improving Zero-shot Cross-Lingual Transfer for Multi-Lingual TTS with Style-Enhanced Normalization Diffusion Framework",
    "volume": "main",
    "abstract": "The prevalence of personalized multilingual tools plays an important role in learning aids and virtual assistants. The existing works on multilingual adaptive text-to-speech (TTS) mainly focus on fine-tuning models or extracting personal styles, such as prosody, emotion, and identity, with the aim of adapting to new speakers. This paper introduces the Style-Enhanced Normalization TTS (STEN-TTS) approach to synthesizing multilingual voice and maintaining personal styles with only 3 seconds of input reference. By presenting an integrated module (STEN) into the diffusion model, the proposed method can simulate the speaker's style and eliminate white noise in the synthesized speech. The experimental results show that our model achieves good performance, at above 3.5 on SMOS for cross-lingual switching. Furthermore, when using speaker verification to assess the similarity between the ground truth and synthesized voices, the accuracy reaches 82.4% with 3 seconds of audio reference",
    "checked": true,
    "id": "1557d266d649e0d6f2d7221447daad02c01aff11",
    "semantic_title": "sten-tts: improving zero-shot cross-lingual transfer for multi-lingual tts with style-enhanced normalization diffusion framework",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kano23_interspeech.html": {
    "title": "Average Token Delay: A Latency Metric for Simultaneous Translation",
    "volume": "main",
    "abstract": "In simultaneous translation, translation begins before the speaker has finished speaking. In its evaluation, we have to consider the latency of the translation in addition to the quality, with latency preferably as small as possible for users to comprehend what the speaker says with a small delay. Existing latency metrics focus on when the translation starts but do not consider adequately when the translation ends. This means such metrics do not penalize the latency caused by a long translation output, which delays user comprehension. In this work, we propose a novel latency evaluation metric called Average Token Delay (ATD) that focuses on the end timings of partial translations in simultaneous translation. We discuss the advantage of ATD using simulated examples and investigate the differences between ATD and Average Lagging with simultaneous translation experiments",
    "checked": true,
    "id": "c288d94e95166e3b8f4ce6ee836856cb66eeeb81",
    "semantic_title": "average token delay: a latency metric for simultaneous translation",
    "citation_count": 5,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qian23_interspeech.html": {
    "title": "Automatic Speech Recognition Transformer with Global Contextual Information Decoder",
    "volume": "main",
    "abstract": "Most current automatic speech recognition (ASR) models use decoders that do not have access to global contextual information at the token level. Therefore, we propose a decoder structure with text-level global contextual information. We construct the global information encoder based on non-autoregressive recognition. To eliminate the non-autoregressive independence assumption, we add a self-attention layer with rotary position encoding. The obtained text-level global contextual information and the decoder are fused as cross-attention to construct a decoder with contextual information. Our model can achieve a character error rate of 3.92% on the AISHELL-1 validation set and 4.35% on the test set, reducing the error rate by 1.72%(dev)/2.13%(test) compared to the baseline model, achieving SOTA performance. Finally, we also use visualization techniques to explain the role of global information in the decoder",
    "checked": true,
    "id": "1d7cc452c84b7c8b232d0d73eda9c06e20edfb64",
    "semantic_title": "automatic speech recognition transformer with global contextual information decoder",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23c_interspeech.html": {
    "title": "Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (ASR) has become an increasingly popular area of research, with two main models being online and offline ASR. Online models aim to provide real-time transcription with minimal latency, whereas offline models wait until the end of the speech utterance before generating a transcription. In this work, we explore three techniques to maximize the performance of each model by 1) proposing a joint parallel online and offline architecture for transducers; 2) introducing dynamic block (DB) training, which allows flexible block size selection and improves the robustness for the offline mode; and, 3) proposing a novel time-synchronous one-pass beam search using the online and offline decoders to further improve the performance of the offline mode. Experimental results show that the proposed method consistently improves the character/word error rates on the CSJ and LibriSpeech datasets",
    "checked": true,
    "id": "f3129ac25e5da769c4dd6f653ba4463e26a708f8",
    "semantic_title": "time-synchronous one-pass beam search for parallel online and offline transducers with dynamic block training",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/praveen23b_interspeech.html": {
    "title": "Prefix Search Decoding for RNN Transducers",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) has seen a surge in popularity for Recurrent Neural Network Transducers (RNN-T) in recent years and shows much promise. RNN-Ts were introduced as an extension of Connectionist Temporal Classification (CTC) models. While CTC models have prefix search as the widely used decoding strategy, it appears to have been overlooked in favour of other decoding strategies such as time-synchronous decoding (TSD) and alignment-synchronous decoding. In this work, we introduce prefix search decoding, looking at all prefixes in the decode lattice to score a candidate. We show that our technique aligns more closely to the training objective compared to the existing strategies. We compare our technique with the originally proposed TSD, using Librispeech and AMI-IHM datasets. We find that while prefix search is closer to the training objective, with larger datasets the performance improves significantly, while with lower size datasets the performance degrades",
    "checked": true,
    "id": "d8f361a888c6d9eae3b8d98b6a1ae1614cd5b925",
    "semantic_title": "prefix search decoding for rnn transducers",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bain23_interspeech.html": {
    "title": "WhisperX: Time-Accurate Speech Transcription of Long-Form Audio",
    "volume": "main",
    "abstract": "Large-scale, weakly-supervised speech recognition models, such as Whisper, have demonstrated impressive results on speech recognition across domains and languages. However, the predicted timestamps corresponding to each utterance are prone to inaccuracies, and word-level timestamps are not available out-of-the-box. Further, their application to long audio via buffered transcription prohibits batched inference due to their sequential nature. To overcome the aforementioned challenges, we present WhisperX, a time-accurate speech recognition system with word-level timestamps utilising voice activity detection and forced phoneme alignment. In doing so, we demonstrate state-of-the-art performance on long-form transcription and word segmentation benchmarks. Additionally, we show that pre-segmenting audio with our proposed VAD Cut & Merge strategy improves transcription quality and enables a twelve-fold transcription speedup via batched inference",
    "checked": true,
    "id": "a258a9837519cbca0cae650b02c41efd87116a5d",
    "semantic_title": "whisperx: time-accurate speech transcription of long-form audio",
    "citation_count": 34,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nigmatulina23_interspeech.html": {
    "title": "Implementing Contextual Biasing in GPU Decoder for Online ASR",
    "volume": "main",
    "abstract": "GPU decoding significantly accelerates the output of ASR predictions. While GPUs are already being used for online ASR decoding, post-processing and rescoring on GPUs have not been properly investigated yet. Rescoring with available contextual information can considerably improve ASR predictions. Previous studies have proven the viability of lattice rescoring in decoding and biasing language model (LM) weights in offline and online CPU scenarios. In real-time GPU decoding, partial recognition hypotheses are produced without lattice generation, which makes the implementation of biasing more complex. The paper proposes and describes an approach to integrate contextual biasing in real-time GPU decoding while exploiting the standard Kaldi GPU decoder. Besides the biasing of partial ASR predictions, our approach also permits dynamic context switching allowing a flexible rescoring per each speech segment directly on GPU. The code is publicly released and tested with open-sourced test sets",
    "checked": true,
    "id": "ac60e4aa24a5cfc2b7b174507e76bc1a6243b899",
    "semantic_title": "implementing contextual biasing in gpu decoder for online asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chung23_interspeech.html": {
    "title": "MF-PAM: Accurate Pitch Estimation through Periodicity Analysis and Multi-level Feature Fusion",
    "volume": "main",
    "abstract": "We introduce Multi-level feature Fusion-based Periodicity Analysis Model (MF-PAM), a novel deep learning-based pitch estimation model that accurately estimates pitch trajectory in noisy and reverberant acoustic environments. Our model leverages the periodic characteristics of audio signals and involves two key steps: extracting pitch periodicity using periodic non-periodic convolution (PNP-Conv) blocks and estimating pitch by aggregating multi-level features using a modified bi-directional feature pyramid network (BiFPN). We evaluate our model on speech and music datasets and achieve superior pitch estimation performance compared to state-of-the-art baselines while using fewer model parameters. Our model achieves 99.20 % accuracy in pitch estimation on a clean musical dataset. Overall, our proposed model provides a promising solution for accurate pitch estimation in challenging acoustic environments and has potential applications in audio signal processing",
    "checked": true,
    "id": "63760f42c52fd2d503a859a3b36049c3203fe4ab",
    "semantic_title": "mf-pam: accurate pitch estimation through periodicity analysis and multi-level feature fusion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/attia23_interspeech.html": {
    "title": "Enhancing Speech Articulation Analysis Using A Geometric Transformation of the X-ray Microbeam Dataset",
    "volume": "main",
    "abstract": "Accurate analysis of speech articulation is crucial for speech analysis. However, X-Y coordinates of articulators strongly depend on the anatomy of the speakers and variability of pellet placements, and existing methods for mapping anatomical landmarks in the X-ray Microbeam Dataset (XRMB) fail to capture the entire anatomy of the vocal tract. In this paper, we propose a new geometric transformation that improves the accuracy of these measurements. Our transformation maps anatomical landmarks' X-Y coordinates along the midsagittal plane onto six relative measures: Lip Aperture (LA), Lip Protrusion (LP), Tongue Body Constriction Location (TBCL), Degree (TBCD), Tongue Tip Constriction Location (TTCL), and Degree (TTCD). Our novel contribution is the extension of the palate trace towards the inferred anterior pharyngeal line, which improves measurements of tongue body constriction",
    "checked": true,
    "id": "ffb43a97c63cd372b819215dbda885fd398ceb0d",
    "semantic_title": "enhancing speech articulation analysis using a geometric transformation of the x-ray microbeam dataset",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jouaiti23_interspeech.html": {
    "title": "Matching Acoustic and Perceptual Measures of Phonation Assessment in Disordered Speech - A Case Study",
    "volume": "main",
    "abstract": "Speech/voice disorders are common in People Living with Dementia (PLwD). Fluctuations in speech quality can serve as biomarkers of cognitive deterioration but there is a gap in automated assessment of speech collected in unstructured environs. Our organisation has deployed Alexa in the households of 14 PLwD to track self-reported mental and physical state as well as use of language. In this work, we present a case study analysing highly variable speech over time, providing potential insights into cognitive changes. Alexa data gathered from the participant was manually annotated with speech assessment labels. Those labels are matched to openSMILE features by performing a feature importance analysis to isolate critical features that contribute to the perceptual ratings. We can assess phonation with a F1-score of 0.55, breathiness: 0.71, roughness: 0.60, asthenia: 0.65, strain: 0.74. This work is a first step towards automatic speech assessment to monitor cognitive impairment over time",
    "checked": true,
    "id": "75505773f973011f3b6d2e0f71702de096c61b1e",
    "semantic_title": "matching acoustic and perceptual measures of phonation assessment in disordered speech - a case study",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23_interspeech.html": {
    "title": "Improved Contextualized Speech Representations for Tonal Analysis",
    "volume": "main",
    "abstract": "We propose fine-tuning wav2vec2.0 with a cross-entropy loss to classify tones in an utterance on a frame-by-frame basis. Our study demonstrates that this approach not only improves tone classification accuracy but also generates frame-level representations suitable for tonal analysis. By using these representations, we established that the third-tone-sandhi-rising tone in Mandarin speech differs from the lexical rising tone, and the third tone that doesn't undergo sandhi differs from the third tone that's not in a sandhi context. Our findings suggest that third-tone sandhi in Mandarin Chinese involves a continuous shift from Tone3 to Tone2, rather than a categorical change",
    "checked": true,
    "id": "3b6d0eb3c7cd18bff18e90e0506d8fe413f416b5",
    "semantic_title": "improved contextualized speech representations for tonal analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chandrasekar23_interspeech.html": {
    "title": "A Study on the Importance of Formant Transitions for Stop-Consonant Classification in VCV Sequence",
    "volume": "main",
    "abstract": "This study analyzes formant transitions in six English stop-consonants in vowel-consonant-vowel (VCV) sequences. We investigate whether natural speech preserves formant patterns, and if not, how it affects stop-consonant perception and automatic classification. We specifically ask three questions: 1) To what extent these formant transition patterns are preserved in naturally produced VCV sequences? 2) If not preserved, does it have any effect on the perception of the stop-consonant? 3) How does the classification of stop-consonants by automatic classifiers change when formant transition patterns are not preserved? We found that 33.56% of the corpus deviate from the formant transition pattern. The perception test reveals an Unweighted Average Recall (UAR) of 91.97% in identifying the stop-consonants in the VCV sequences when the pattern is not preserved compared to 93.54% when it is preserved. The best UAR from an automatic classifier is 68.35% and 77.5% in these two cases, respectively",
    "checked": true,
    "id": "7c8be2061c7a1d4b244ae4d9573e3c3a266ebf26",
    "semantic_title": "a study on the importance of formant transitions for stop-consonant classification in vcv sequence",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eren23_interspeech.html": {
    "title": "FusedF0: Improving DNN-based F0 Estimation by Fusion of Summary-Correlograms and Raw Waveform Representations of Speech Signals",
    "volume": "main",
    "abstract": "DSP-based F0 estimation algorithms, such as multi-band summary-correlogram (MBSC), are robust to noisy speech. Recent studies show that mapping from raw waveform segments into F0 estimates by DNNs can outperform DSP-based methods in F0 estimation. However, generalization and noise robustness of DNNs have not been fully addressed previously. We propose a hybrid DSP and DNN based approach to F0 estimation. Key contributions include: (a) a modified version of MBSC that is substantially faster than the original algorithm while maintaining the accuracy of F0 estimates; (b) a method for fusing DSP features with raw waveform representations using a DNN architecture to obtain noise-robust F0 estimation; (c) demonstrating that auxiliary DSP features improve generalization with a relatively small number of DNN parameters. On the PTDB-TUG database, the proposed algorithm outperforms the MBSC and CREPE DNN baselines (including optimized versions) for clean and noisy speech at 20, 10, and 0 dB SNR",
    "checked": true,
    "id": "c7b623aa0fdbfe488c2268f52594745fcb5fedda",
    "semantic_title": "fusedf0: improving dnn-based f0 estimation by fusion of summary-correlograms and raw waveform representations of speech signals",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kyung23_interspeech.html": {
    "title": "Improving Joint Speech and Emotion Recognition Using Global Style Tokens",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) and speech emotion recognition (SER) are closely related in that the acoustic features of speech, such as pitch, tone, and intensity, can vary according to the speaker's emotional state. Our study focuses on a joint ASR and SER task, in which an emotion token is tagged and recognized along with the text. To further improve the joint recognition performance, we propose a novel training method that adopts the global style tokens (GSTs). The style embedding is extracted from the GSTs module to enhance the joint ASR and SER model to capture emotional information from speech. Specifically, a conformer-based joint ASR and SER model pre-trained on a large-scale dataset is jointly fine-tuned with style embedding to improve both ASR and SER. The experimental results on the IEMOCAP dataset showed that the proposed model achieves a word error rate of 15.8% and four emotion classification weighted and unweighted accuracy of 75.1% and 76.3%, respectively",
    "checked": true,
    "id": "35609baf9878a13471fc11b97e39364aa60736dc",
    "semantic_title": "improving joint speech and emotion recognition using global style tokens",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nagase23_interspeech.html": {
    "title": "Speech Emotion Recognition by Estimating Emotional Label Sequences with Phoneme Class Attribute",
    "volume": "main",
    "abstract": "In recent years, much research has been into speech emotion recognition (SER) using deep learning to predict emotions conveyed by speech. We studied the method that detected the emotion for the whole utterance using the frame-based SER, which estimates emotions in each frame rather than in a whole utterance. One of the problems with this method is that the emotional label sequence, which is used in training the frame-based SER, does not sufficiently consider phonemic characteristics. To solve this problem, we propose new methods of recognizing the emotion for the whole utterance using frame-based SER that considers the phoneme class attribute such as vowels, voiced consonants, unvoiced consonants, and other symbols in training. As a result, we found that the proposed methods significantly improve the performance of the result for the whole utterance compared to conventional methods",
    "checked": true,
    "id": "ee7ac39177fcce2d8ddda7ed3f3d96bfc3dfb157",
    "semantic_title": "speech emotion recognition by estimating emotional label sequences with phoneme class attribute",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23_interspeech.html": {
    "title": "Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Cross-domain speech emotion recognition (SER), which utilizes the source domain to recognize the emotions in the target domain, has received significant attention in recent years. In this paper, we propose a novel unsupervised transfer learning method named unsupervised transfer components learning (UTCL) for cross-domain SER. Specifically, we first learn a common projection for the cross-domain data, in which a PCA-like strategy is conducted for the source and target domains separately. Meanwhile, we design a simple strategy to ensure all cross-domain samples share similar manifold structures so that the learned common projection can preserve more transfer components. Furthermore, a novel adaptive structured graph strategy is designed to further narrow the gap between the cross-domain samples. Comprehensive experimental results on several benchmark datasets demonstrate that our method can achieve better performance in comparison with several state-of-the-art methods",
    "checked": true,
    "id": "7a074dbaf8c38ef831f09751cbbe935eaa3998c0",
    "semantic_title": "unsupervised transfer components learning for cross-domain speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/prisayad23_interspeech.html": {
    "title": "Dual Memory Fusion for Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Deep learning has been widely used in multi-modal Speech Emotion Recognition (SER) to learn sentiment-related features by aggregating representations from multiple modes. However, most SOTA methods use attentive fusion or late fusion of data which ignores the possibility of long-term dependencies among data. In this study, we propose a transformer-based SER architecture that fuses modality representations through explicit memory modules, where the information from current inputs is integrated with historical information allowing the model to understand the relative importance of modes over time. We have used Wav2Vec2 and BERT models to extract audio and text features which are then fused together by aggregating features from individual modes with information stored in memory, followed by downstream classification. Following state-of-the-art methods, we evaluate our proposed method on the IEMOCAP dataset and results indicate that memory-based fusion can achieve substantial improvements",
    "checked": true,
    "id": "d3842fb69381f714006036360d971be4830f154c",
    "semantic_title": "dual memory fusion for multimodal speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kondratenko23_interspeech.html": {
    "title": "Hybrid Dataset for Speech Emotion Recognition in Russian Language",
    "volume": "main",
    "abstract": "We present a new data set for speech emotion recognition (SER) tasks called Dusha. The corpus contains approximately 350 hours of data, more than 300 000 audio recordings of Russian speech, and their transcripts. Therefore it is the biggest open bi-modal data collection with an open license for SER tasks nowadays. This data set is the first speech emotion corpus in Russian, including both crowd-sourced acted and real-life emotions from podcasts, with multiple speakers and scalable data set size. Acted subset has a more balanced class distribution than the unbalanced real-life part consisting of audio podcasts. So the first one is suitable for model pre-training, and the second is elaborated for fine-tuning purposes, model approbation, and validation. This paper describes in detail our collecting procedure, pre-processing routine, annotation, and experiment with a baseline model to demonstrate some actual metrics which could be obtained with the Dusha data set",
    "checked": true,
    "id": "0d4e6c8ed41dd3a20978e226f104da6285c04d6c",
    "semantic_title": "hybrid dataset for speech emotion recognition in russian language",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hsu23_interspeech.html": {
    "title": "Speech Emotion Recognition using Decomposed Speech via Multi-task Learning",
    "volume": "main",
    "abstract": "In speech emotion recognition, most recent studies used powerful models to obtain robust features without considering the disentangled components, which contain diverse emotion-rich information helpful for speech emotion recognition. In this study, an autoencoder is used as the speech decomposition model to obtain the disentangled components, including content, timbre, pitch, and rhythm features, which are regarded as emotion-rich features, for speech emotion recognition. The mechanism of multi-task training is then used to train the tasks of speech emotion recognition, speaker recognition, speech recognition, and spectral reconstruction at the same time, while exploiting commonalities and differences across tasks. The model proposed in this study achieved an accuracy of 77.50% on the four-classes emotion recognition task of IEMOCAP. Experiments showed that the proposed methods can effectively improve speech emotion recognition performance, outperforming the SOTA approach",
    "checked": true,
    "id": "267bd4a37dd4f8ccaf5ef9edfee65c3487ff21a1",
    "semantic_title": "speech emotion recognition using decomposed speech via multi-task learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23b_interspeech.html": {
    "title": "Prospective Validation of Motor-Based Intervention with Automated Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders",
    "volume": "main",
    "abstract": "Because lab accuracy of clinical speech technology systems may be overoptimistic, clinical validation is vital to demonstrate system reproducibility-in this case, the ability of the PERCEPT-R Classifier to predict clinician judgment of American English /ɹ/ during ChainingAI motor-based speech sound disorder intervention. All five participants experienced statistically-significant improvement in untreated words following 10 sessions of combined human-ChainingAI treatment. These gains, despite a wide range of PERCEPT-human and human-human (F1-score) agreement, raise questions about best measuring classification performance for clinical speech that may be perceptually ambiguous",
    "checked": true,
    "id": "88bcfb9b0f28aa8b0935bc7c8e912b5f9186362a",
    "semantic_title": "prospective validation of motor-based intervention with automated mispronunciation detection of rhotics in residual speech sound disorders",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23_interspeech.html": {
    "title": "Classifying Rhoticity of /ɹ/ in Speech Sound Disorder using Age-and-Sex Normalized Formants",
    "volume": "main",
    "abstract": "Mispronunciation detection tools could increase treatment access for speech sound disorders impacting, e.g., /ɹ/. We show age-and-sex normalized formant estimation outperforms cepstral representation for detection of fully rhotic vs. derhotic /ɹ/ in the PERCEPT-R Corpus. Gated recurrent neural networks trained on this feature set achieve a mean test participant-specific F1-score =.81 (σx=.10, med = .83, n = 48), with post hoc modeling showing no significant effect of child age or sex",
    "checked": true,
    "id": "8e6cbaf6a8916cd8fd7f9391888dd8ba721698d9",
    "semantic_title": "classifying rhoticity of /ɹ/ in speech sound disorder using age-and-sex normalized formants",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23c_interspeech.html": {
    "title": "Acoustic-to-Articulatory Speech Inversion Features for Mispronunciation Detection of /ɹ/ in Child Speech Sound Disorders",
    "volume": "main",
    "abstract": "Acoustic-to-articulatory speech inversion could enhance automated clinical mispronunciation detection to provide detailed articulatory feedback unattainable by formant-based mispronunciation detection algorithms; however, it is unclear the extent to which a speech inversion system trained on adult speech performs in the context of (1) child and (2) clinical speech. In the absence of an articulatory dataset in children with rhotic speech sound disorders, we show that classifiers trained on tract variables from acoustic-to-articulatory speech inversion meet or exceed the performance of state-of-the-art features when predicting clinician judgment of rhoticity",
    "checked": true,
    "id": "c764a0b241448bd3ed9cdf255e81b356734920c9",
    "semantic_title": "acoustic-to-articulatory speech inversion features for mispronunciation detection of /ɹ/ in child speech sound disorders",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/piton23_interspeech.html": {
    "title": "Using Commercial ASR Solutions to Assess Reading Skills in Children: A Case Report",
    "volume": "main",
    "abstract": "Reading is an acquired skill that is essential for integrating and participating in today's society. Yet, becoming literate can be particularly laborious for some children. Identifying reading difficulties early enough is the first, necessary step toward remediation. Here we investigate the opportunities and limitations of integrating commercial, off-the-shelf automatic speech recognition (ASR) services from IBM Watson to ease the administration and evaluation of children's reading assessment tests in French and Italian",
    "checked": true,
    "id": "b4f98ef9b158e6f674eb48e6ad45e29aee71b6a3",
    "semantic_title": "using commercial asr solutions to assess reading skills in children: a case report",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gebauer23_interspeech.html": {
    "title": "Exploiting Diversity of Automatic Transcripts from Distinct Speech Recognition Techniques for Children's Speech",
    "volume": "main",
    "abstract": "The recent advances in automatic speech recognition (ASR) technologies using end-to-end machine learning do not transfer well to children's speech. One cause is the high pronunciation variability and frequent violations of grammatical or lexical rules, which impedes the successful usage of language models or powerful context-representations. Applying these methods affects the nature of the resulting transcript rather than improving the overall recognition performance. In this work we analyze the diversity of the transcripts from distinct ASR-systems for children's speech and exploit it by applying a common combination scheme. We consider systems with various degree of context: Greedily decoded and lexicon-constrained connectionist temporal classification-models, attention-based encoder decoders, and Wav2Vec 2.0, a powerful context-representation. By exploiting their diversity we achieve a relative improvement of 17.8 % on phone recognition compared to the best single system",
    "checked": true,
    "id": "99bc7cb15f78bf84bab2334fcd2d5bbc19870a73",
    "semantic_title": "exploiting diversity of automatic transcripts from distinct speech recognition techniques for children's speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rumberg23_interspeech.html": {
    "title": "Uncertainty Estimation for Connectionist Temporal Classification Based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Predictive uncertainty estimation of deep neural networks is important when their outputs are used for high stakes decision making. We investigate token-level uncertainty of connectionist temporal classification (CTC) based automatic speech recognition models. We propose an approach, which considers that not all changes at frame-level lead to a change at token-level after CTC decoding. The approach shows promising performance for prediction of recognition errors on TIMIT, Mozilla Common Voice (MCV) and kidsTALC, a corpus of children's speech, using two different model architectures, while introducing only negligible computational overhead. Our approach identifies over 80% of a wav2vec2.0 model's errors on MCV by selecting 10% of the tokens. We further show, that the predictive uncertainty estimate relates to the uncertainty of a human annotator, by re-annotating 500 utterances of kidsTALC",
    "checked": true,
    "id": "f03d438ac71f11d23aaaae96fb404e5f8f3d6c9d",
    "semantic_title": "uncertainty estimation for connectionist temporal classification based automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lavechin23_interspeech.html": {
    "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
    "volume": "main",
    "abstract": "Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech",
    "checked": true,
    "id": "8af2bffa267eeb4a3209c23dd3bbf5e7d4982809",
    "semantic_title": "babyslm: language-acquisition-friendly benchmark of self-supervised spoken language models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23c_interspeech.html": {
    "title": "Data augmentation for children ASR and child-adult speaker classification using voice conversion methods",
    "volume": "main",
    "abstract": "Many young children prefer speech based interfaces over text, as they are relatively slow and error-prone with text input. However, children ASR can be challenging due to the lack of transcribed children speech corpora. In this paper, we investigate a voice conversion method based on WORLD vocoder to generate childlike speech for data augmentation. Since noise may lead to severe artifacts in converted speech, we also investigate using speech enhancement to improve the quality of converted speech. On a publicly available children speech corpus, we evaluated the performance of the proposed data augmentation method against existing data augmentation methods based on linear prediction coefficients. Our proposed data augmentation method substantially outperformed the prior work on children ASR. Additionally, on a task to classify the speaker, adult or child, data generated using our proposed method was shown to mimic real children better compared to the reference methods",
    "checked": true,
    "id": "e80cf23e87fff7d2296b464f62a9e0b43e2e6ecf",
    "semantic_title": "data augmentation for children asr and child-adult speaker classification using voice conversion methods",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shetty23_interspeech.html": {
    "title": "Developmental Articulatory and Acoustic Features for Six to Ten Year Old Children",
    "volume": "main",
    "abstract": "In this paper, we study speech development in children using longitudinal acoustic and articulatory data. Data were collected yearly from grade 1 to grade 4 from four female and four male children. We analyze acoustic and articulatory properties of four corner vowels: /æ/, /i/, /u/, and /ɑ/, each occurring in two different words (different surrounding contexts). Acoustic features include formant frequencies and subglottal resonances (SGRs). Articulatory features include tongue curvature degree (TCD) and tongue curvature position (TCP). Based on the analyses, we observe the emergence of sex-based differences starting from grade 2. Similar to adults, the SGRs divide the vowel space into high, low, front, and back regions at least as early as grade 2. On average, TCD is correlated with vowel height and TCP with vowel frontness. Children in our study used varied articulatory configurations to achieve similar acoustic targets",
    "checked": true,
    "id": "44e260c8b95b9a4925090735e89b27d0e986553e",
    "semantic_title": "developmental articulatory and acoustic features for six to ten year old children",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23u_interspeech.html": {
    "title": "Automatically Predicting Perceived Conversation Quality in a Pediatric Sample Enriched for Autism",
    "volume": "main",
    "abstract": "Social interaction quality ratings derived from short natural conversations can differentiate children with and without autism at the group level. In this work, we explored conversations between children and an unfamiliar adult who rated their social interaction success on six dimensions. Using hand-crafted acoustic and lexical features, we built different classifiers to predict children's dimensional conversation quality. The best classifier achieved 61% accuracy, which outperformed human raters (49%). Follow-up analyses revealed that a subset of features determined communication quality scores. Additionally, we extracted acoustic features using a pretrained audio transformer and improved our prediction to 68%. This study suggests that automatically predicting conversation quality could be an inexpensive and objective way to monitor intervention progress in children with communication challenges, and could be used to identify intervention targets for improving conversational success",
    "checked": true,
    "id": "c7ca2f8ba7bf1bdc148309e5da023f8a9c1dfe1f",
    "semantic_title": "automatically predicting perceived conversation quality in a pediatric sample enriched for autism",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/johnson23_interspeech.html": {
    "title": "An Equitable Framework for Automatically Assessing Children's Oral Narrative Language Abilities",
    "volume": "main",
    "abstract": "This work proposes a novel framework for automatically scoring children's oral narrative language abilities. We use audio recordings from 3rd-8th graders of the Atlanta, Georgia area as they take a portion of the Test of Narrative Language. We design a system which extracts linguistic features and fine-tuned BERT-based self-supervised learning representation from state-of-the-art ASR transcripts. We predict manual test scores from the extracted features. This framework significantly outperforms a deterministic method based on the assessment's scoring rubric. Last, we evaluate the system performance across student's reading level, dialect, and diagnosed learning/language disabilities to establish fairness across diverse demographics of students. Using this system, we achieve approximately 98% classification accuracy of student scores. We are also able to identify key areas of improvement for this type of system across demographic areas and reading ability",
    "checked": true,
    "id": "d958a3f06184bce454fad7baae2dda0e6247e50e",
    "semantic_title": "an equitable framework for automatically assessing children's oral narrative language abilities",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cao23_interspeech.html": {
    "title": "An Analysis of Goodness of Pronunciation for Child Speech",
    "volume": "main",
    "abstract": "In this paper, we study the use of goodness of pronunciation (GOP) on child speech. We first compare the distributions of GOP scores on several open datasets representing various dimensions of speech variability. We show that the GOP distribution over CMU Kids, corresponding to young age, has larger spread than those on datasets representing other dimensions, i.e., accent, dialect, spontaneity and environmental conditions. We hypothesize that the increased variability of pronunciation in young age may impair the use of traditional mispronunciation detection methods for children. To support this hypothesis, we perform simulated mispronunciation experiments both for children and adults using different variants of the GOP algorithm. We also compare the results to real-case mispronunciations for native children showing that GOP is less effective for child speech than for adult speech",
    "checked": true,
    "id": "f7263ec9864e7384fc9d60d68089ff736e048720",
    "semantic_title": "an analysis of goodness of pronunciation for child speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sy23_interspeech.html": {
    "title": "Measuring Language Development From Child-centered Recordings",
    "volume": "main",
    "abstract": "Standard ways to measure child language development from spontaneous corpora rely on detailed linguistic descriptions of a language as well as exhaustive transcriptions of the child's speech, which today can only be done through costly human labor. We tackle both issues by proposing (1) a new language development metric (based on entropy) that does not require linguistic knowledge other than having a corpus of text in the language in question to train a language model, (2) a method to derive this metric directly from speech based on a smaller text-speech parallel corpus. Here, we present descriptive results on an open archive including data from six English-learning children as a proof of concept. We document that our entropy metric documents a gradual convergence of children's speech towards adults' speech as a function of age, and it also correlates moderately with lexical and morphosyntactic measures derived from morphologically-parsed transcriptions",
    "checked": true,
    "id": "ba61543cda917372ab744b0eca5c96889e2f6ed4",
    "semantic_title": "measuring language development from child-centered recordings",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hung23_interspeech.html": {
    "title": "Speaking Clearly, Understanding Better: Predicting the L2 Narrative Comprehension of Chinese Bilingual Kindergarten Children Based on Speech Intelligibility Using a Machine Learning Approach",
    "volume": "main",
    "abstract": "This study investigated the relationship of speech intelligibility and the narrative comprehension among bilingual kindergarten children and how well the speech intelligibility of second language (L2) predicted the L2 narrative comprehension using a machine learning approach. Fifty Chinese-English bilingual children aged 5-6 years old participated in this study by taking a narrative comprehension test. Their L2 narrative comprehension was assessed using the MAIN test. The speech intelligibility was assessed in terms of twenty-four features that encode confidence levels with respect to phoneme and word classifiers trained on native speaker speech data. Our hypothesis posits that it is possible to predict L2 narrative comprehension based on speech intelligibility features. By using seven out of the twenty-four considered features we were able to make predictions of the MAIN test scores with an RMSE of 2.13 and a Pearson correlation coefficient of 0.468 based on a data set of 50 bilingual kindergarten children. We conclude the paper by providing pedagogical implications for second language teaching as well as suggestions for future work",
    "checked": true,
    "id": "e838174ef3ed7f07373003dfee50a434a5ade3b0",
    "semantic_title": "speaking clearly, understanding better: predicting the l2 narrative comprehension of chinese bilingual kindergarten children based on speech intelligibility using a machine learning approach",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/charuau23_interspeech.html": {
    "title": "Speech Breathing Behavior During Pauses in Children",
    "volume": "main",
    "abstract": "The aim of this paper is to investigate speech breathing behaviors in children during the realization of pauses, breathing pauses or non-breathing pauses, depending on the syntactic location of the pause and the speech task. Thus, we will be able to observe the effects of cognitive-linguistic demands on breathing patterns in children. To do so, 10 French speakers, between 8 and 11 years old, were recorded while reading and spontaneous speech. The variation of respiratory movements was measured using inductive respiratory plethysmography. The respiratory signals were synchronized with acoustic data. The results show an effect of the speech task on the duration of inhalation and, to a lesser extent, on its amplitude. 'Partial inhalations' were observed at the syntactic boundaries, suggesting that they are integrated in the child's respiratory patterns. Finally, we observed occasional cessation of rib cage closure, essentially during non-syntactic pauses",
    "checked": true,
    "id": "751143aa70606661c1526f3ad4c06b0c7aacfaeb",
    "semantic_title": "speech breathing behavior during pauses in children",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23e_interspeech.html": {
    "title": "Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings",
    "volume": "main",
    "abstract": "Speech processing techniques are useful for analyzing speech and language development in children with Autism Spectrum Disorder (ASD), who are often varied and delayed in acquiring these skills. Early identification and intervention are crucial, but traditional assessment methodologies such as caregiver reports are not adequate for the requisite behavioral phenotyping. Natural Language Sample (NLS) analysis has gained attention as a promising complement. Researchers have developed benchmarks for spoken language capabilities in children with ASD, obtainable through the analysis of NLS. This paper proposes applications of speech processing technologies in support of automated assessment of children's spoken language development by classification between child and adult speech and between speech and nonverbal vocalization in NLS, with respective F1 macro scores of 82.6% and 67.8%, underscoring the potential for accurate and scalable tools for ASD research and clinical use",
    "checked": true,
    "id": "de66cc29c2b5396114c5672db7881af0743967d4",
    "semantic_title": "understanding spoken language development of children with asd using pre-trained speech embeddings",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ariasvergara23_interspeech.html": {
    "title": "Measuring Phonological Precision in Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "Children with Cleft Lip and Palate (CLP) may experience difficulties in oral communication, leading to other developmental problems such as delayed language acquisition and poor social skills; thus, early treatment is essential for successful speech rehabilitation. In this paper, we propose a methodology for automatically assessing the phonological precision of children with CLP. We propose to use the probabilities obtained from a phonological class recognizer to measure phonological precision during connected speech. Furthermore, we compute the nasal-to-sound ratio to improve the automatic detection of the nasality level. For this, we considered speech recordings of 88 children with CLP, assessed by a clinician according to four nasality levels: normal, mild, moderate, and severe. We obtained an F1-score of up to 0.54 for detecting the nasality level automatically. The results suggest that phonological analysis can be used for individualized speech rehabilitation",
    "checked": true,
    "id": "b04d663b35c95f5c8519510c2b495443f7a70c80",
    "semantic_title": "measuring phonological precision in children with cleft lip and palate",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23_interspeech.html": {
    "title": "A Study on Using Duration and Formant Features in Automatic Detection of Speech Sound Disorder in Children",
    "volume": "main",
    "abstract": "Speech sound disorder (SSD) in children is manifested by persistent articulation and phonological errors on specific phonemes of a language. Automatic SSD detection can be done using features extracted from deep neural network models. Interpretability of such learned features is a major concern. Motivated by clinical knowledge, the use of duration and formant features for SSD detection is investigated in this research. Acoustical analysis is performed to identify the acoustic features that differentiate between the speech of typical and disordered children. On the task of SSD detection in Cantonese-speaking children, the duration features are found to outperform the formant features and surpass previous methods that use paralinguistic feature set and speaker embeddings. Specifically, the duration features achieve a mean unweighted average recall of 71.0%. The results enhance the understanding of SSD, and motivate further use of temporal information of child speech in SSD detection",
    "checked": true,
    "id": "63e962e53a86dd2af1a5178d0f13b86208ec4a3b",
    "semantic_title": "a study on using duration and formant features in automatic detection of speech sound disorder in children",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baumann23_interspeech.html": {
    "title": "Influence of Utterance and Speaker Characteristics on the Classification of Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "Recent findings show that pre-trained wav2vec 2.0 models are reliable feature extractors for various speaker characteristics classification tasks. We show that latent representations extracted at different layers of a pre-trained wav2vec 2.0 system can be used as features for binary classification to distinguish between children with Cleft Lip and Palate (CLP) and a healthy control group. The results indicate that the distinction between CLP and healthy voices, especially with latent representations from the lower and middle encoder layers, reaches an accuracy of 100%. We test the classifier to find influencing factors for classification using unseen out-of-domain healthy and pathologic corpora with varying characteristics: age, spoken content, and acoustic conditions. Cross-pathology and cross-healthy tests reveal that the trained classifiers are unreliable if there is a mismatch between training and out-of-domain test data in, e.g., age, spoken content, or acoustic conditions",
    "checked": true,
    "id": "84c9a39e84064eb156be957a37c9e4efa8474280",
    "semantic_title": "influence of utterance and speaker characteristics on the classification of children with cleft lip and palate",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23g_interspeech.html": {
    "title": "Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning",
    "volume": "main",
    "abstract": "Dialogue state tracking (DST) is an important step in dialogue management to keep track of users' beliefs. Existing works fine-tune all language model (LM) parameters to tackle the DST task, which requires significant data and computing resources for training and hosting. The cost grows exponentially in the real-world deployment where dozens of fine-tuned LM are used for different domains and tasks. To reduce parameter size and better utilize cross-task shared information, we propose to use soft prompt token embeddings to learn task properties. Without tuning LM parameters, our method drastically reduces the number of parameters needed to less than 0.5% of prior works while achieving better low-resource DST performance",
    "checked": true,
    "id": "e65f6420f0876d4681dc11a0163e12a8a236dc5a",
    "semantic_title": "parameter-efficient low-resource dialogue state tracking by prompt tuning",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mcneill23_interspeech.html": {
    "title": "An Autoregressive Conversational Dynamics Model for Dialogue Systems",
    "volume": "main",
    "abstract": "Conversational partners adapt their speech to one another in a phenomenon called entrainment. While entrainment behaviors are associated with a variety of positive conversational outcomes, they are rarely implemented in dialogue systems due to their poorly understood mechanics. Conversational dynamics models could discover entrainment behavior in a dialogue corpus, but to date they have not been designed for or evaluated in dialogue systems. In this paper, we propose an autoregressive model specifically for use in a dialogue system. We evaluate its ability to predict features for upcoming conversational turns, and show it outperforms several baseline models. Additionally, we analyze its attention mechanism to explain which turns it finds useful for predicting upcoming speech features. Finally, we discuss its potential for future deployment in a live dialogue system",
    "checked": true,
    "id": "e13b0493fc2283fd39a42559f5036fd36cfe1494",
    "semantic_title": "an autoregressive conversational dynamics model for dialogue systems",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hori23_interspeech.html": {
    "title": "Style-transfer based Speech and Audio-visual Scene understanding for Robot Action Sequence Acquisition from Videos",
    "volume": "main",
    "abstract": "To realize human-robot collaboration, robots need to execute actions for new tasks according to human instructions given finite prior knowledge. Human experts can share their knowledge of how to perform a task with a robot through multi-modal instructions in their demonstrations, showing a sequence of shorthorizon steps to achieve a longhorizon goal. This paper introduces a method for robot action sequence generation from instruction videos using (1) an audio-visual Transformer that converts audio-visual features and instruction speech to a sequence of robot actions called dynamic movement primitives (DMPs) and (2) style-transfer-based training that employs multi-task learning with video captioning and weakly-supervised learning with a semantic classifier to exploit unpaired video-action data. We built a system that accomplishes various cooking actions, where an arm robot executes a DMP sequence acquired from a cooking video using the audio-visual Transformer. Experiments with EpicKitchen100, YouCookII, QuerYD, and inhouse instruction video datasets show that the proposed method improves the quality of DMP sequences by 2.3 times the METEOR score obtained with a baseline video-to-action Transformer. The model achieved 32% of the task success rate with the task knowledge of the object",
    "checked": true,
    "id": "e13e6ba3666ba8b499cda4457b00a2a8a7c53721",
    "semantic_title": "style-transfer based speech and audio-visual scene understanding for robot action sequence acquisition from videos",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/soltau23_interspeech.html": {
    "title": "Speech Aware Dialog System Technology Challenge (DSTC11)",
    "volume": "main",
    "abstract": "Most research on task oriented dialog modeling is based on written text input. However, practical dialog systems often use spoken input. Typically, input speech is converted into text using ASR, which are error-prone. Furthermore, most systems don't address the differences in written and spoken language. The research on this topic is stymied by the lack of a public corpus. Motivated by these considerations, we hosted a speech-aware dialog state tracking challenge and created a public corpus which can be used to investigate the performance gap between the written and spoken input. We created three spoken versions of the popular written-domain MultiWoz task and provide waveforms, ASR transcripts, and audio encodings to encourage wider participation from teams that may not have access to ASR systems. In this paper, we describe the corpus, report results from participating teams, provide preliminary analyses of their results, and summarize the current state-of-the-art in this domain",
    "checked": false,
    "id": "610b52dc88147c04e26a061ca905443937348116",
    "semantic_title": "overview of the eighth dialog system technology challenge: dstc8",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cai23_interspeech.html": {
    "title": "Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision",
    "volume": "main",
    "abstract": "Most existing task-oriented dialog (TOD) systems track dialog states in terms of slots and values and use them to query a database to get relevant knowledge to generate responses. In real life applications, user utterances are noisier, and thus it is more difficult to accurately track dialog states and correctly secure relevant knowledge. Recently, a progress in question answering and document-grounded dialog systems is retrieval-augmented methods with a knowledge retriever. Inspired by such progress, we propose a retrieval-based method to enhance knowledge selection in TOD systems, which significantly outperforms the traditional database query method for real-life dialogs. Further, we develop latent variable model based semi-supervised learning, which can work with the knowledge retriever to leverage both labeled and unlabeled dialog data. Joint Stochastic Approximation (JSA) algorithm is employed for semi-supervised model training, and the whole system is referred to as JSA-KRTOD. Experiments are conducted on a real-life dataset from China Mobile Custom-Service, called MobileCS, and show that JSA-KRTOD achieves superior performances in both labeled-only and semi-supervised settings",
    "checked": true,
    "id": "a52e8d7ab0c9882982908af5110362599f3bf8de",
    "semantic_title": "knowledge-retrieval task-oriented dialog systems with semi-supervision",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23k_interspeech.html": {
    "title": "Tracking Must Go On : Dialogue State Tracking with Verified Self-Training",
    "volume": "main",
    "abstract": "In task-oriented dialogues, dialogue state tracking (DST) is a critical component as it identifies specific information for the user's purpose. However, as annotating DST data requires a significant amount of human effort, leveraging raw dialogue is crucial. To address this, we propose a new self-training (ST) framework with a verification model. Unlike previous ST methods that rely on extensive hyper-parameter searching to filter out inaccurate data, our verification methodology ensures the accuracy and validity of the dataset without using a fixed threshold. Furthermore, to mitigate overfitting, we augment the dataset by generating diverse user utterances. Even when using only 10% of the labeled data, our approach achieves comparable results to a fully labeled MultiWOZ2.0 dataset. The evaluation of scalability also demonstrates enhanced robustness in predicting unseen values",
    "checked": true,
    "id": "45788a03a57cc8304fa715ac9775663706e24e63",
    "semantic_title": "tracking must go on : dialogue state tracking with verified self-training",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ja_interspeech.html": {
    "title": "Ordered and Binary Speaker Embedding",
    "volume": "main",
    "abstract": "Modern speaker recognition systems represent utterances by embedding vectors. Conventional embedding vectors are dense and non-structural. In this paper, we propose an ordered binary embedding approach that sorts the dimensions of the embedding vector via a nested dropout and converts the sorted vectors to binary codes via Bernoulli sampling. The resultant ordered binary codes offer some important merits such as hierarchical clustering, reduced memory usage, and fast retrieval. These merits were empirically verified by comprehensive experiments on a speaker identification task with the VoxCeleb and CN-Celeb datasets",
    "checked": true,
    "id": "f760c084dd45b1830e76686f9d66412edba61209",
    "semantic_title": "ordered and binary speaker embedding",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kataria23_interspeech.html": {
    "title": "Self-FiLM: Conditioning GANs with self-supervised representations for bandwidth extension based speaker recognition",
    "volume": "main",
    "abstract": "Speech super-resolution/Bandwidth Extension (BWE) can improve downstream tasks like Automatic Speaker Verification (ASV). We introduce a simple novel technique called Self-FiLM to inject self-supervision information in existing BWE models via Feature-wise Linear Modulation. We hypothesize that such information contains domain/environment information, which can help BWE deliver zero-shot generalization. Self-FiLM improves conditional GAN-based BWE by 18% (relative) in Equal Error Rate and 8.5% in minimum Decision Cost Function on the x-vector & Probabilistic Linear Discriminant Analysis based state-of-the-art ASV system on SRE21 test. We further improve it by using deep feature losses from time-domain models and re-training data2vec 2.0 models on naturalistic wideband (VoxCeleb) and telephone data (SRE Superset etc.). Lastly, we integrate Self-FiLM in CycleGAN to obtain a completely unsupervised solution that matches the CGAN-based semi-supervised performance",
    "checked": true,
    "id": "03e266795339008e9366daabfd2a2db2fbd51151",
    "semantic_title": "self-film: conditioning gans with self-supervised representations for bandwidth extension based speaker recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heo23b_interspeech.html": {
    "title": "Curriculum Learning for Self-supervised Speaker Verification",
    "volume": "main",
    "abstract": "The goal of this paper is to train effective self-supervised speaker representations without identity labels. We propose two curriculum learning strategies within a self-supervised learning framework. The first strategy aims to gradually increase the number of speakers in the training phase by enlarging the used portion of the train dataset. The second strategy applies various data augmentations to more utterances within a mini-batch as the training proceeds. A range of experiments conducted using the DINO self-supervised framework on the VoxCeleb1 evaluation protocol demonstrates the effectiveness of our proposed curriculum learning strategies. We report a competitive equal error rate of 4.47% with a single-phase training, and we also demonstrate that the performance further improves to 1.84% by fine-tuning on a small labelled dataset",
    "checked": true,
    "id": "4ecdbeef548b38355e8e58e8e80571ed71d2cd3a",
    "semantic_title": "curriculum learning for self-supervised speaker verification",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23aa_interspeech.html": {
    "title": "Introducing Self-Supervised Phonetic Information for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "This paper presents a novel multi-task learning framework by introducing self-supervised phonetic information for deep speaker embedding extraction. The primary task is still to classify speakers, but we consider an auxiliary task to identify phoneme boundaries in speech signals following the Noise Contrastive Estimation principle. To further utilize self-supervised information to assist speaker feature learning, the features of intermediate layers in the main task are refined by the features of corresponding layers in the auxiliary task through masking and biasing operations. We use the VoxCeleb1 and CN-Celeb datasets for performance evaluation, which consistently verifies the efficacy of the proposed method",
    "checked": true,
    "id": "402ed609ad58a14751cb47dc025c6210a80218ae",
    "semantic_title": "introducing self-supervised phonetic information for text-independent speaker verification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cordlandwehr23_interspeech.html": {
    "title": "A Teacher-Student Approach for Extracting Informative Speaker Embeddings From Speech Mixtures",
    "volume": "main",
    "abstract": "We introduce a monaural neural speaker embeddings extractor that computes an embedding for each speaker present in a speech mixture. To allow for supervised training, a teacher-student approach is employed: the teacher computes the target embeddings from each speaker's utterance before the utterances are added to form the mixture, and the student embedding extractor is then tasked to reproduce those embeddings from the speech mixture at its input. The system much more reliably verifies the presence or absence of a given speaker in a mixture than a conventional speaker embedding extractor, and even exhibits comparable performance to a multi-channel approach that exploits spatial information for embedding extraction. Further, it is shown that a speaker embedding computed from a mixture can be used to check for the presence of that speaker in another mixture",
    "checked": true,
    "id": "376a28b0079bed01db9def6406fbcd8244262a3b",
    "semantic_title": "a teacher-student approach for extracting informative speaker embeddings from speech mixtures",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lepage23_interspeech.html": {
    "title": "Experimenting with Additive Margins for Contrastive Self-Supervised Speaker Verification",
    "volume": "main",
    "abstract": "Most state-of-the-art self-supervised speaker verification systems rely on a contrastive-based objective function to learn speaker representations from unlabeled speech data. We explore different ways to improve the performance of these methods by: (1) revisiting how positive and negative pairs are sampled through a \"symmetric\" formulation of the contrastive loss; (2) introducing margins similar to AM-Softmax and AAM-Softmax that have been widely adopted in the supervised setting. We demonstrate the effectiveness of the symmetric contrastive loss which provides more supervision for the self-supervised task. Moreover, we show that Additive Margin and Additive Angular Margin allow reducing the overall number of false negatives and false positives by improving speaker separability. Finally, by combining both techniques and training a larger model we achieve 7.50% EER and 0.5804 minDCF on the VoxCeleb1 test set, which outperforms other contrastive self supervised methods on speaker verification",
    "checked": true,
    "id": "bf467ee469c4840a63b1824dfb137fc497270887",
    "semantic_title": "experimenting with additive margins for contrastive self-supervised speaker verification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hope23_interspeech.html": {
    "title": "Nonbinary American English speakers encode gender in vowel acoustics",
    "volume": "main",
    "abstract": "Encoding of gender in speech is a well-researched phenomenon, especially as it concerns men and women. Men tend to produce certain acoustic characteristics in certain ways (low fundamental frequency (F0), lower formant frequencies, lower center of gravity for [s] in English) compared to women, though these characteristics also differ based on other group memberships (e.g. race, sexuality, etc). Those who are more feminine, regardless of categorical gender, have been found to produce an increase in F0 and a larger vowel space. However, these previous studies used largely cisgender women and men or only examined encoding of binary gender in speech and did not consider encoding of \"other\" or nonbinary gender in speech. This study recruited American English nonbinary speakers to record 400 utterances and correlated acoustic characteristics with multidimensional gender. Masculine, feminine, and \"other\" gender are significantly correlated with vowel acoustics",
    "checked": true,
    "id": "9d4a5476a99764fdccb6630aea8b0a91ffc4734b",
    "semantic_title": "nonbinary american english speakers encode gender in vowel acoustics",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharp23_interspeech.html": {
    "title": "Coarticulation of Sibe Vowels and Dorsal Fricatives in Spontaneous Speech: An Acoustic Study",
    "volume": "main",
    "abstract": "Previous phonological analyses of dorsal segments in Sibe (Tungusic; Xinjiang, China) have treated the velar and uvular series as allophonic, with velar segments adjacent to /i ə u/ and uvular segments adjacent to /a o/. In this paper, we use a spontaneous speech corpus to examine the acoustic correlates of coarticulation of dorsal fricatives and vowels in Sibe. The first two spectral moments and mid-frequency spectral peak of dorsal fricatives were measured over the fricative onset in VC sequences and offset in CV sequences. Differences in spectral measures suggest that dorsal fricatives coarticulate with both preceding and following vowels primarily in terms of tongue dorsum backness, but with some role of height possible. These findings reflect a more complex relationship between Sibe vowels and dorsals than previously described; the distribution of labels used for the dorsal fricatives also suggests a gradient assimilation process",
    "checked": true,
    "id": "4fdcc570033d377ddb999cd579278694f0a5f9d3",
    "semantic_title": "coarticulation of sibe vowels and dorsal fricatives in spontaneous speech: an acoustic study",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/brown23_interspeech.html": {
    "title": "Using speech synthesis to explain automatic speaker recognition: a new application of synthetic speech",
    "volume": "main",
    "abstract": "Some speech synthesis systems make use of zero-shot adaptation to generate speech based on a target speaker. These systems produce speaker embeddings in the same way that speaker embeddings (often called 'x-vectors') are produced in automatic speaker recognition systems. This commonality between the two technologies could lower barriers that constrain the use of automatic speaker recognition systems in forensic speech analysis casework. A key barrier to the use of automatic speaker recognition in the forensic context is the issue of explainability, including what information about the voice a system uses in order to arrive at conclusions. This paper sets out a new approach that could be used to effectively communicate this type of information to audiences in the legal setting. Specifically, it is proposed that exposing listeners to synthetic speech produced by a zero-shot adaptation system could illustrate what aspects of the voice an automatic speaker recognition system captures",
    "checked": true,
    "id": "bcb965d1453cb6518f89c47f840c943dad31226f",
    "semantic_title": "using speech synthesis to explain automatic speaker recognition: a new application of synthetic speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23i_interspeech.html": {
    "title": "Same F0, Different Tones: A Multidimensional Investigation of Zhangzhou Tones",
    "volume": "main",
    "abstract": "This paper explores tonal encoding in Zhangzhou Southern Min, a Sinitic dialect spoken in South Fujian province of China. Results show that tones sharing an identical F0 contour can differ considerably in duration, vowel quality, voice quality, syllable coda type, and sandhi behavior. It is proper to consider tones are multidimensionally distinguished and treat each tone as a complex of phonetic features. The discussion stretches and deepens our understanding of the phonetic nature of tone, while questions the conventional definition of tone as a contrastive use of pitch. It contributes vital empirical data to the typology of tone as an important phenomenon in world's languages, and sheds important light on how tones should be defined",
    "checked": true,
    "id": "ede3b12a281d6f83a64361b2e17ae99139080d97",
    "semantic_title": "same f0, different tones: a multidimensional investigation of zhangzhou tones",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/english23_interspeech.html": {
    "title": "Discovering Phonetic Feature Event Patterns in Transformer Embeddings",
    "volume": "main",
    "abstract": "Domain-informed probing of large speech recognition transformer-based models offers an opportunity to investigate how phonetic information is captured and transformed in the information-rich embeddings that emerge as part of the recognition process. Previous work in this area has established the efficacy of probing these embeddings with simple multi-layer perceptron models to identify the information patterns encoded at each layer of the transformer. This paper explores phonetic feature event patterns which evolve at each layer of a transformer model. Probing models are trained with phonetic embeddings, which are averaged and labelled at the phone level using the TIMIT dataset, to detect the presence of certain phonetic features in time-steps of a speech signal. This paper demonstrates how the detection of phonetic features within the embeddings of transformer models, such as voicing, frication and nasal, provides insights in relation to the encoding of speech patterns in these models",
    "checked": true,
    "id": "b66f0ec91d9165fc907511a2111592dd2aa43d95",
    "semantic_title": "discovering phonetic feature event patterns in transformer embeddings",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ra_interspeech.html": {
    "title": "A System for Generating Voice Source Signals that Implements the Transformed LF-model Parameter Control",
    "volume": "main",
    "abstract": "This paper describes a system which fully implements the transformed LF glottal flow model, incorporating also the often-overlooked k-factors. A problem with the original proposal is that the global waveshape parameter Rd, central to the transformed model and used to predict some of the other parameters, cannot be directly controlled. Instead, a stylisation of the model was used to indirectly control Rd. However, this approach can yield substantial errors in the Rd value of the pulse, thus undermining the usefulness of the model. To overcome this problem, an iterative algorithm is presented, which ensures that for a given Rd input value, a pulse with that Rd value will be produced. Using this new Rd control, it transpired that some of the original parameter predictions give rise to combinations of values that are incompatible with the LF model. Modifications to the original predictions of the Rk parameter were incorporated to ensure model conformity",
    "checked": true,
    "id": "e6a6c4c588ee0054b9baccd7d9e2bb1dc5d565da",
    "semantic_title": "a system for generating voice source signals that implements the transformed lf-model parameter control",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/siriwardena23b_interspeech.html": {
    "title": "Speaker-independent Speech Inversion for Estimation of Nasalance",
    "volume": "main",
    "abstract": "The velopharyngeal (VP) valve regulates the opening between the nasal and oral cavities. This valve opens and closes through a coordinated motion of the velum and pharyngeal walls. Nasalance is an objective measure derived from the oral and nasal acoustic signals that correlate with nasality. In this work, we evaluate the degree to which the nasalance measure reflects fine-grained patterns of VP movement by comparison with simultaneously collected direct measures of VP opening using high-speed nasopharyngoscopy (HSN). We show that nasalance is significantly correlated with the HSN signal, and that both match expected patterns of nasality. We then train a temporal convolution-based speech inversion system in a speaker-independent fashion to estimate VP movement for nasality, using nasalance as the ground truth. In further experiments, we also show the importance of incorporating source features (from glottal activity) to improve nasality prediction",
    "checked": true,
    "id": "7dfc3a2bedec8f7ddb0af9b6864f25c845af2f80",
    "semantic_title": "speaker-independent speech inversion for estimation of nasalance",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23e_interspeech.html": {
    "title": "Effects of Tonal Coarticulation and Prosodic Positions on Tonal Contours of Low Rising Tones: In the Case of Xiamen Dialect",
    "volume": "main",
    "abstract": "Few studies have worked on the effects of tonal coarticulation and prosodic positions on the low rising tone in Xiamen Dialect. This study addressed such an issue. To do so, a new method, the Tonal Contour Analysis in Tonal Triangle, was proposed to measure the subtle curvature of the tonal contour. Findings are as follows: (1) The low rising tone in Xiamen Dialect has a tendency towards the falling-rising tone, which is significantly affected by the tonal coarticulation and prosodic positions. (2) The low rising tone presents as a falling-rising tone when preceded by a tone with a high offset, and as a low rising tone when preceded by a tone that ends up low. (3) The curvature of the low rising tone is greatest in the sentence-initial position, and is positively correlated to its own duration",
    "checked": true,
    "id": "7dbf0d40728fef855f89bb348f45d11514619469",
    "semantic_title": "effects of tonal coarticulation and prosodic positions on tonal contours of low rising tones: in the case of xiamen dialect",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/issa23_interspeech.html": {
    "title": "Durational and Non-durational Correlates of Lexical and Derived Geminates in Arabic",
    "volume": "main",
    "abstract": "This paper reports on the phonetic and phonological patterns of gemination in Tripolitanian Libyan Arabic (TLA). While previous studies on Arabic gemination have either focused on lexical geminates or reported results on data that contains both lexical and derived geminates, without investigating its effect on the phonetic output, the present study investigates the effect of the phonological status of a geminate on the phonetic realization. Several measurements were obtained including target segments duration, RMS amplitude and F1, F2 and F3 for the target consonants. Preliminary results suggest that the acoustic distinction between singleton and geminate consonants in TLA is dependent mainly on durational correlates. There was no evidence of differences in RMS amplitude between singleton and geminate consonants of any type. F1, F2 and F3 frequencies are found to show similar patterns for singleton and geminate types for all sounds, suggesting no gestural effects of gemination in TLA",
    "checked": true,
    "id": "aafb66919e2fb072b690dfd80104debe16a8004f",
    "semantic_title": "durational and non-durational correlates of lexical and derived geminates in arabic",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rao23_interspeech.html": {
    "title": "Mapping Phonemes to Acoustic Symbols and Codes Using Synchrony in Speech Modulation Vectors Estimated by the Travellingwave Filter Bank",
    "volume": "main",
    "abstract": "A hybrid vector representation for speech resonances is defined using the modulation model and the sum of sinusoids model. An adaptive filter bank, whose channels utilize resonance localized modulation tracking, to robustly estimate temporal variations in these vectors, is then presented. The synchrony in modulations, within and across resonance channels, is subsequently used to derive acoustic symbols and codes that map fundamental units of languages, phonemes. Such an acoustic-phonetic mapping has never been demonstrated before. It has potential applications in speech recognition and voice analytics",
    "checked": true,
    "id": "b5ec8c7593cecddf1166d0ea503f91e9e0268409",
    "semantic_title": "mapping phonemes to acoustic symbols and codes using synchrony in speech modulation vectors estimated by the travellingwave filter bank",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ge23_interspeech.html": {
    "title": "Rhythmic Characteristics of L2 German Speech by Advanced Chinese Learners",
    "volume": "main",
    "abstract": "The rhythm of speech produced by advanced Chinese learners can still be problematic. In order to improve their pronunciation, it is necessary to investigate L2 rhythmic deviance. In this study, read-aloud materials of 10 German L1 speakers selected from MULTEXT and parallel recordings of 14 Chinese L2 learners were analyzed by comparing six widely used rhythmic metrics, speech rate, and vowel deletion ratio. A correlation analysis was also conducted between these rhythmic features and the degree of perceived foreign accent. The results indicated that speech rate, varcoC, and vowel deletion ratio were significantly different between L1 and L2 speakers, and that speech rate and vowel deletion ratio had a strong correlation with perceived foreign accent within L2 speech. These findings suggest that complex consonantal intervals and vowel deletion are still challenging for advanced learners and should be given more attention in pronunciation training",
    "checked": true,
    "id": "7f86cbd168483439ac7d10a7d967ccc7dbe7f066",
    "semantic_title": "rhythmic characteristics of l2 german speech by advanced chinese learners",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kelterer23_interspeech.html": {
    "title": "(Dis)agreement and Preference Structure are Reflected in Matching Along Distinct Acoustic-prosodic Features",
    "volume": "main",
    "abstract": "This paper presents an investigation of acoustic-prosodic alignment in conversational speech and its relationship to functional inter-speaker alignment. While most previous research studied global alignment over whole conversations between strangers, the focus of this paper is on alignment between friends, partners and colleagues as a more local phenomenon related to affiliation and preference structure. Based on 359 turn-pairs from assessment sequences, we analyzed three prosodic matching features between adjacent turns in logistic and linear regression models. We found that disagreements tend to be produced with less F0 span matching than agreements and with less F0 median matching in some parts of the conversation. Preferred responses were more likely to be marked by higher F0 median matching than dispreferred responses. These results indicate that different aspects of functional inter-speaker alignment are reflected in matching along distinct acoustic-prosodic features",
    "checked": true,
    "id": "685325c35b722f6724b447b340f5d144e2587992",
    "semantic_title": "(dis)agreement and preference structure are reflected in matching along distinct acoustic-prosodic features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/christodoulidou23_interspeech.html": {
    "title": "Vowel reduction by Greek-speaking children: The effect of stress and word length",
    "volume": "main",
    "abstract": "This study examines vowel reduction as a function of stress and word length in 3 male and 3 female 3-, 5- and 7-year-olds, and adult controls. Participants produced triplets of two- and three-syllable words, where each Greek vowel /i, ε, ɐ, ο, u/ was studied in the first syllable in both stressed and unstressed conditions. Measurements were made for absolute and relative duration and (non-)normalized vowel space areas. The results showed that children had longer absolute and relative vowel duration and larger non-normalized vowel space areas than adults. Vowel space areas were lower and to the left in the acoustic vowel space in relation to adults. Longer duration and larger vowel space areas were found in stressed position across all ages with an adult-like spectral reduction already present at 3 years of age but an adult-like temporal reduction from 5 years of age and onwards for unstressed vowels in both two- and three-syllable words",
    "checked": true,
    "id": "2e2a6910fa376f2939b694670edd4a2b3a22ec56",
    "semantic_title": "vowel reduction by greek-speaking children: the effect of stress and word length",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lennes23_interspeech.html": {
    "title": "Pitch distributions in a very large corpus of spontaneous Finnish speech",
    "volume": "main",
    "abstract": "Speakers differ in the pitch range they use in their speech. In order to analyze the functional aspects of pitch, the typical pitch range of each individual is needed as reference. However, systematically collected pitch data from a sufficiently large corpus have not been previously available. We analyze the pitch distributions of individual speakers in a subset of the Donate Speech Corpus, collected from speakers of Finnish in 2020-2021. We report pitch analysis results based on samples from 8197 speakers and 1475 hours of speech. We compare the results obtained from male and female speakers in different age groups",
    "checked": true,
    "id": "e1467b6ebdc8e846a6d2f3ebd763d47c191ee0ab",
    "semantic_title": "pitch distributions in a very large corpus of spontaneous finnish speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kudera23_interspeech.html": {
    "title": "Speech Enhancement Patterns in Human-Robot Interaction: A Cross-Linguistic Perspective",
    "volume": "main",
    "abstract": "This paper presents the results of the human-robot interaction (HRI) study with German native speakers addressing the robot in their L1 and in L2 English. The aim of the experiment is to test the strategies of providing clarifications when talking to the voice assistant in a task involving teaching complex vocabulary. The analyses is based on spectral (F1, F2, and mean F0) and temporal (vowel length) features excerpted from the target words. With reference to a theoretical framework of hyperarticulation and hypoarticulation, these acoustic measures were compared across the iterations of the target words (first vs. second iteration). Results showed that participants, when asked for clarification by an inanimate interlocutor, do not hyperarticulate, but try to preserve the surface representation of target words across the iterations. These findings suggest that acoustic characteristics of clarifications directed to voice assistants differ from the ones directed to human interlocutors",
    "checked": true,
    "id": "014887264061d34c7edea38348e1f18d650f2aa7",
    "semantic_title": "speech enhancement patterns in human-robot interaction: a cross-linguistic perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html": {
    "title": "Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions",
    "volume": "main",
    "abstract": "Customizing voice and speaking style in a speech synthesis system with intuitive and fine-grained controls is challenging, given that little data with appropriate labels is available. Furthermore, editing an existing human's voice also comes with ethical concerns. In this paper, we propose a method to generate artificial speaker embeddings that cannot be linked to a real human while offering intuitive and fine-grained control over the voice and speaking style of the embeddings, without requiring any labels for speaker or style. The artificial and controllable embeddings can be fed to a speech synthesis system, conditioned on embeddings of real humans during training, without sacrificing privacy during inference",
    "checked": true,
    "id": "c043b2f78d3db1fa541e65933b65485c83aba01b",
    "semantic_title": "controllable generation of artificial speaker embeddings through discovery of principal directions",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ga_interspeech.html": {
    "title": "Dual Audio Encoders Based Mandarin Prosodic Boundary Prediction by Using Multi-Granularity Prosodic Representations",
    "volume": "main",
    "abstract": "Prosodic boundary prediction plays an important role in speech synthesis, phonetic understanding, etc. In previous studies, supra-segmental features such as pitch, energy, and duration have been widely used to explicitly model Mandarin prosodic boundaries. In this paper, we propose to refine implicit prosodic representations with fine-grained information from complex acoustic features including mel-spectrogram and context vectors obtained from a pre-trained model. Pitch and energy are encoded as explicit prosodic representations. These two representations extracted by dual audio encoders are fused by the decoder mainly composed of cross-attention layers. Then the fused representations are used to predict Mandarin prosodic boundaries. The results indicate that our proposed method outperforms the baselines in the Mandarin prosodic boundary prediction task, particularly for the minor prosodic phrases (#2)",
    "checked": true,
    "id": "ff7f089f8be670c1d47853a34c39804f118cd18b",
    "semantic_title": "dual audio encoders based mandarin prosodic boundary prediction by using multi-granularity prosodic representations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23i_interspeech.html": {
    "title": "NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS",
    "volume": "main",
    "abstract": "Expressive text-to-speech (TTS) can synthesize a new speaking style by imitating prosody and timbre from a reference audio, which faces the following challenges: (1) The highly dynamic prosody information in the reference audio is difficult to extract, especially, when the reference audio contains background noise. (2) The TTS systems should have good generalization for unseen speaking styles. In this paper, we present a noise-robust expressive TTS model (NoreSpeech), which can robustly transfer speaking style in a noisy reference utterance to synthesized speech. Specifically, our NoreSpeech includes several components: (1) a novel DiffStyle module, which leverages powerful probabilistic denoising diffusion models to learn noise-agnostic speaking style features from a teacher model by knowledge distillation; (2) a Vector Quantization (VQ) block, which maps the style features into a controllable quantized latent space for improving the generalization of style transfer; and (3) a straightforward but effective parameter-free text-style alignment module, which enables NoreSpeech to transfer style to a textual input from a length-mismatched reference utterance. Experiments demonstrate that NoreSpeech is more effective than previous expressive TTS models in noise environments",
    "checked": true,
    "id": "c29d2e98fda1bf772139da11814e313836df3704",
    "semantic_title": "norespeech: knowledge distillation based conditional diffusion model for noise-robust expressive tts",
    "citation_count": 12,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23n_interspeech.html": {
    "title": "MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy",
    "volume": "main",
    "abstract": "Many speech synthesis systems only consider the information within each sentence and ignore the contextual semantic and acoustic features. This makes it inadequate to generate high-expressiveness paragraph-level speech. In this paper, a context-aware speech synthesis system named MaskedSpeech is proposed, which considers both contextual semantic and acoustic features. Inspired by the masking strategy in speech editing research, the acoustic features of the current sentence are masked out and concatenated with those of contextual speech, and further used as additional model input. Furthermore, cross-utterance coarse-grained and fine-grained semantic features are employed to improve the prosody generation.The model is trained to reconstruct the masked acoustic features with the augmentation of both the contextual semantic and acoustic features.Experimental results demonstrate that the MaskedSpeech outperformed the baseline systems significantly in terms of naturalness and expressiveness",
    "checked": true,
    "id": "369b8ebae182b3b4ea8c55dfbe528b932a7c3e61",
    "semantic_title": "maskedspeech: context-aware speech synthesis with masking strategy",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pavankalyan23_interspeech.html": {
    "title": "Narrator or Character: Voice Modulation in an Expressive Multi-speaker TTS",
    "volume": "main",
    "abstract": "Current Text-to-Speech (TTS) systems are trained on audiobook data and perform well in synthesizing read-style speech. In this work, we are interested in synthesizing audio stories as narrated to children. The storytelling style is more expressive and requires perceptible changes of voice across the narrator and story characters. To address these challenges, we present a new TTS corpus of English audio stories for children with 32.7 hours of speech by a single female speaker with a UK accent. We provide evidence of the salient differences in the suprasegmentals of the narrator and character utterances in the dataset, motivating the use of a multi-speaker TTS for our application. We use a fine-tuned BERT model to label each sentence as being spoken by a narrator or character that is subsequently used to condition the TTS output. Experiments show our new TTS system is superior in expressiveness in both A-B preference and MOS testing compared to reading-style TTS and single-speaker TTS",
    "checked": true,
    "id": "672ea965415c09d78bc3c3e76ddd377460bd13ec",
    "semantic_title": "narrator or character: voice modulation in an expressive multi-speaker tts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23b_interspeech.html": {
    "title": "CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation",
    "volume": "main",
    "abstract": "Existing fine-grained intensity regulation methods rely on explicit control through predicted emotion probabilities. However, these high-level semantic probabilities are often inaccurate and unsmooth at the phoneme level, leading to bias in learning. Especially when we attempt to mix multiple emotion intensities for specific phonemes, resulting in markedly reduced controllability and naturalness of the synthesis. To address this issue, we propose the CAScaded Explicit and Implicit coNtrol framework (CASEIN), which leverages accurate disentanglement of emotion manifolds from the reference speech to learn the implicit representation at a lower semantic level. This representation bridges the semantical gap between explicit probabilities and the synthesis model, reducing bias in learning. In experiments, our CASEIN surpasses existing methods in both controllability and naturalness. Notably, we are the first to achieve fine-grained control over the mixed intensity of multiple emotions",
    "checked": true,
    "id": "a35daeb47e7632adc5bb742c436163221225dc64",
    "semantic_title": "casein: cascading explicit and implicit control for fine-grained emotion intensity regulation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oh23_interspeech.html": {
    "title": "Semi-supervised Learning for Continuous Emotional Intensity Controllable Speech Synthesis with Disentangled Representations",
    "volume": "main",
    "abstract": "Recent text-to-speech models have reached the level of generating natural speech similar to what humans say. But there still have limitations in terms of expressiveness. The existing emotional speech synthesis models have shown controllability using interpolated features with scaling parameters in emotional latent space. However, the emotional latent space generated from the existing models is difficult to control the continuous emotional intensity because of the entanglement of features like emotions, speakers, etc. In this paper, we propose a novel method to control the continuous intensity of emotions using semi-supervised learning. The model learns emotions of intermediate intensity using pseudo-labels generated from phoneme-level sequences of speech information. An embedding space built from the proposed model satisfies the uniform grid geometry with an emotional basis. The experimental results showed that the proposed method was superior in controllability and naturalness",
    "checked": true,
    "id": "11b791f3417a0c16a68a1010c8b6faf6b3ca5944",
    "semantic_title": "semi-supervised learning for continuous emotional intensity controllable speech synthesis with disentangled representations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nguyen23_interspeech.html": {
    "title": "Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis",
    "volume": "main",
    "abstract": "Recent work has shown that it is possible to resynthesize high-quality speech based, not on text, but on low bitrate discrete units that have been learned learned in a self-supervised fashion and can therefore capture expressive aspects of speech that are hard to transcribe. The adoption of these methods is still limited by the fact that most speech synthesis datasets are read, severely limiting spontaneity and expressivity. Here, we introduce EXPRESSO, a high-quality expressive speech dataset for textless speech synthesis that includes both read speech and improvised dialogues rendered in 26 expressive styles. We illustrate the challenges and potentials of this dataset with an expressive resynthesis benchmark where the task is to encode the input in low-bitrate units and resynthesize it in a target voice while preserving content and style. We evaluate resynthesis quality with automatic metrics for different self-supervised discrete encoders, and explore tradeoffs between quality, bitrate and invariance to speaker and style. The dataset, evaluation metrics and baseline models will be open sourced",
    "checked": true,
    "id": "b3193d1a48a0c9035c1e562643f44472575bcf44",
    "semantic_title": "expresso: a benchmark and analysis of discrete expressive speech resynthesis",
    "citation_count": 6,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23fa_interspeech.html": {
    "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
    "volume": "main",
    "abstract": "Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian",
    "checked": true,
    "id": "fae77c4caa606ed1b34473e76f0b1077f43d9957",
    "semantic_title": "comedicspeech: text to speech for stand-up comedies in low-resource scenarios",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kunesova23_interspeech.html": {
    "title": "Neural Speech Synthesis with Enriched Phrase Boundaries",
    "volume": "main",
    "abstract": "Prosodic phrasing is one of the factors influencing the naturalness of synthesized speech. In this paper, we enrich the phonetic representation for neural speech synthesis with additional markers denoting the strength of phrase breaks between words. These markers are assigned to the training data automatically, using our previously introduced model for audio-based phrase boundary detection. We tested the approach with two different levels of resolution for the break indices-either ten distinct levels (P10) or only \"ToBI-like\" four levels (P4). Listening tests with two different speaker voices show a statistically significant preference among listeners for P10 or P4 over the baseline speech synthesis without these markers (P0), although which version is judged as better depends on the voice",
    "checked": true,
    "id": "bd3aae544ad51b6f7c6a29807fbae467196100ff",
    "semantic_title": "neural speech synthesis with enriched phrase boundaries",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/swiatkowski23_interspeech.html": {
    "title": "Cross-lingual Prosody Transfer for Expressive Machine Dubbing",
    "volume": "main",
    "abstract": "Prosody transfer is well-studied in the context of expressive speech synthesis. Cross-lingual prosody transfer, however, is challenging and has been under-explored to date. In this paper, we present a novel solution to learn prosody representations that are transferable across languages and speakers for machine dubbing of expressive multimedia contents. Multimedia contents often contain field recordings. To enable prosody transfer from noisy audios, we introduce a novel noise modelling module that disentangles noise conditioning from prosody conditioning, and thereby gains independent control of noise levels in the synthesised speech. We augment noisy training data with clean data to improve the ability of the model to map the denoised reference audio to clean speech. Our proposed system can generate speech with context-matching prosody and closes the gap between a strong baseline and human expressive dialogs by 11.2%",
    "checked": true,
    "id": "31f039d280cbe011dfab865cfd6552abc8aadb13",
    "semantic_title": "cross-lingual prosody transfer for expressive machine dubbing",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elmers23_interspeech.html": {
    "title": "Synthesis after a couple PINTs: Investigating the Role of Pause-Internal Phonetic Particles in Speech Synthesis and Perception",
    "volume": "main",
    "abstract": "Pause-internal phonetic particles (PINTs), such as breath noises, tongue clicks and hesitations, play an important role in speech perception but are rarely modeled in speech synthesis. We developed two text-to-speech (TTS) systems: one with and one without PINTs labels in the training data. Both models produced fewer PINTs and had a lower total PINTs duration than natural speech. The labeled model generated more PINTs and longer total PINTs durations than the model without labels. In a listening experiment based on the labeled model we evaluated the influence of various PINTs combinations on the perception of speaker certainty. We tested a condition without PINTs material and three conditions that included PINTs. The condition without PINTs was perceived as significantly more certain than the PINTs conditions, suggesting that we can modify how certain TTS is perceived by including PINTs",
    "checked": true,
    "id": "d6ab22293c403523a1ea561c228d9a649aea9d0e",
    "semantic_title": "synthesis after a couple pints: investigating the role of pause-internal phonetic particles in speech synthesis and perception",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geneva23_interspeech.html": {
    "title": "Accentor: An Explicit Lexical Stress Model for TTS Systems",
    "volume": "main",
    "abstract": "The accurate placement of word stress is a critical component of the correct pronunciation of words. Contemporary publicly available text-to-speech (TTS) datasets have a relatively narrow coverage of unique words, which causes modern neural TTS systems to synthesize speech that often suffers from lexical stress errors. In this work, we propose an efficient approach for explicitly modeling lexical stress knowledge with a dedicated Accentor neural network. The Accentor is trained separately on a large lexically diverse stress-annotated text corpus that is automatically compiled using an automatic speech recognition system. We demonstrate that the Accentor can be combined with a TTS acoustic model to reliably control the word stress encoded in the generated acoustic features. Experiments show that our approach increases the stress prediction accuracy by a factor of 12 in comparison to other modern TTS systems and improves the naturalness and comprehensibility of the synthesized speech",
    "checked": true,
    "id": "9cc310d6a61345a663812cbf14e75b93fa6d96e8",
    "semantic_title": "accentor: an explicit lexical stress model for tts systems",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shechtman23_interspeech.html": {
    "title": "A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers",
    "volume": "main",
    "abstract": "Modern neural TTS systems are capable of generating natural and expressive speech when provided with sufficient amounts of training data. Such systems can be equipped with prosody-control functionality, allowing for more direct shaping of the speech output at inference time. In some TTS applications, it may be desirable to have an option that guides the TTS system with an ad-hoc speech recording exemplar to impose an implicit fine-grained, user-preferred prosodic realization for certain input prompts. In this work we present a first-of-its-kind neural TTS system equipped with such functionality to transfer the prosody from a parallel text recording from an unseen speaker. We demonstrate that the proposed system can precisely transfer the speech prosody from novel speakers to various trained TTS voices with no quality degradation, while preserving the target TTS speakers' identity, as evaluated by a set of subjective listening experiments",
    "checked": true,
    "id": "95e97659a24bd02f81f828a7dc7080a7848d402c",
    "semantic_title": "a neural tts system with parallel prosody transfer from unseen speakers",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23j_interspeech.html": {
    "title": "Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "Expressive human speech generally abounds with rich and flexible speech prosody variations. The speech prosody predictors in existing expressive speech synthesis methods mostly produce deterministic predictions, which are learned by directly minimizing the norm of prosody prediction error. Its unimodal nature leads to a mismatch with ground truth distribution and harms the model's ability in making diverse predictions. Thus, we propose a novel prosody predictor based on the denoising diffusion probabilistic model to take advantage of its high-quality generative modeling and training stability. Experiment results confirm that the proposed prosody predictor outperforms the deterministic baseline on both the expressiveness and diversity of prediction results with even fewer network parameters",
    "checked": true,
    "id": "78440bedd96e87e459b68ff986a4c2168c2bd922",
    "semantic_title": "diverse and expressive speech prosody prediction with denoising diffusion probabilistic model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23c_interspeech.html": {
    "title": "Prosody Modeling with 3D Visual Information for Expressive Video Dubbing",
    "volume": "main",
    "abstract": "The automatic video dubbing task is proposed to meet personal and industrial demands for dubbing. Current methods mostly focus on duration matching and overlook the synchronization of prosody, and thus lack expressiveness. In this paper, we introduce visual prosody modeling to promote expressiveness for video dubbing, defined as the expression and head pose in 3D space, which has the advantages of 1) high relevance to the tone and stress of utterances; 2) more accurate than 2D images; 3) disentanglement from irrelevant factors such as speaker identity. We propose a 3D-VD (3D Video Dubber) system to incorporate visual prosody, utilizing a visual-text step-wise aligner to control the generated prosody. Experiments demonstrate that the proposed method outperforms previous methods that only consider 2D face images in terms of naturalness, lip-speech alignment, and synchronization of visual and auditory prosody. The case study demonstrates the correlation between expression and pitch",
    "checked": true,
    "id": "9885071e16187b8f527f3c7eeda4cccfdfd5dcab",
    "semantic_title": "prosody modeling with 3d visual information for expressive video dubbing",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23f_interspeech.html": {
    "title": "LightClone: Speaker-guided Parallel Subnet Selection for Few-shot Voice Cloning",
    "volume": "main",
    "abstract": "Large-scale few-shot voice cloning service faces three main challenges: model storage for huge number of users, fast model training and real-time synthesis. They all involve model size directly. It is noted that few-shot voice cloning usually has much bigger model size than common TTS trained by one speaker corpus, since its source model needs more parameters to hold the characteristics of various speakers. It also indicates that a high quality TTS model for one voice could be much smaller. To reduce model size of voice cloning, speaker-guided parallel subnet selection (SG-PSS) is proposed in this paper. In adaptation phase, only one subnet is selected from parallel ones of source model for target speaker. By this method, adaptation training and inference can be much faster. Experiment results show that the proposed approach achieves 4x model compression ratio, 3x inference speedup and even slightly better performance in voice quality and speaker similarity in comparison with baseline",
    "checked": true,
    "id": "eec9309edcbfac69ed5e6ed1f7ded4c7c058cf58",
    "semantic_title": "lightclone: speaker-guided parallel subnet selection for few-shot voice cloning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhong23_interspeech.html": {
    "title": "EE-TTS: Emphatic Expressive TTS with Linguistic Information",
    "volume": "main",
    "abstract": "While Current TTS systems perform well in synthesizing high-quality speech, producing highly expressive speech remains a challenge. Emphasis, as a critical factor in determining the expressiveness of speech, has attracted more attention nowadays. Previous works usually enhance the emphasis by adding intermediate features, but they can not guarantee the overall expressiveness of the speech. To resolve this matter, we propose Emphatic Expressive TTS (EE-TTS), which leverages multi-level linguistic information from syntax and semantics. EE-TTS contains an emphasis predictor that can identify appropriate emphasis positions from text and a conditioned acoustic model to synthesize expressive speech with emphasis and linguistic information. Experimental results indicate that EE-TTS outperforms baseline with MOS improvements of 0.49 and 0.67 in expressiveness and naturalness. EE-TTS also shows strong generalization across different datasets according to AB test results",
    "checked": true,
    "id": "710d22dd8dd92cf449e39d480f686f0e3af3780a",
    "semantic_title": "ee-tts: emphatic expressive tts with linguistic information",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ogun23_interspeech.html": {
    "title": "Stochastic Pitch Prediction Improves the Diversity and Naturalness of Speech in Glow-TTS",
    "volume": "main",
    "abstract": "Flow-based generative models are widely used in text-to-speech (TTS) systems to learn the distribution of audio features (e.g., Mel-spectrograms) given the input tokens and to sample from this distribution to generate diverse utterances. However, in the zero-shot multi-speaker TTS scenario, the generated utterances lack diversity and naturalness. In this paper, we propose to improve the diversity of utterances by explicitly learning the distribution of fundamental frequency sequences (pitch contours) of each speaker during training using a stochastic flow-based pitch predictor, then conditioning the model on generated pitch contours during inference. The experimental results demonstrate that the proposed method yields a significant improvement in the naturalness and diversity of speech generated by a Glow-TTS model that uses explicit stochastic pitch prediction, over a Glow-TTS baseline and an improved Glow-TTS model that uses a stochastic duration predictor",
    "checked": true,
    "id": "c001972a50a951c2e33e1a254a1b6ba4155772cb",
    "semantic_title": "stochastic pitch prediction improves the diversity and naturalness of speech in glow-tts",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23_interspeech.html": {
    "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
    "volume": "main",
    "abstract": "While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/",
    "checked": true,
    "id": "d93e18676254c35abdbb92ab36d1f7cf3ab4c8cb",
    "semantic_title": "contextspeech: expressive and efficient text-to-speech for paragraph reading",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23t_interspeech.html": {
    "title": "PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions",
    "volume": "main",
    "abstract": "Style transfer TTS has shown impressive performance in recent years. However, style control is often restricted to systems built on expressive speech recordings with discrete style categories. In practical situations, users may be interested in transferring style by typing text descriptions of desired styles, without the reference speech in the target style. The text-guided content generation techniques have drawn wide attention recently. In this work, we explore the possibility of controllable style transfer with natural language descriptions. To this end, we propose PromptStyle, a text prompt-guided cross-speaker style transfer system. Specifically, PromptStyle consists of an improved VITS and a cross-modal style encoder. The crossmodal style encoder constructs a shared space of stylistic and semantic representation through a two-stage training process. Experiments show that PromptStyle can achieve proper style transfer with text prompts while maintaining relatively high stability and speaker similarity. Audio samples are available in our demo page",
    "checked": true,
    "id": "9b58ef6e29cccbc23578078e321f65848fde0335",
    "semantic_title": "promptstyle: controllable style transfer for text-to-speech with natural language descriptions",
    "citation_count": 9,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tian23b_interspeech.html": {
    "title": "Creating Personalized Synthetic Voices from Post-Glossectomy Speech with Guided Diffusion Models",
    "volume": "main",
    "abstract": "This paper is about developing personalized speech synthesis systems with recordings of mildly impaired speech. In particular, we consider consonant and vowel alterations resulted from partial glossectomy, the surgical removal of part of the tongue. The aim is to restore articulation in the synthesized speech and maximally preserve the target speaker's individuality. We propose to tackle the problem with guided diffusion models. Specifically, a diffusion-based speech synthesis model is trained on original recordings, to capture and preserve the target speaker's original articulation style. When using the model for inference, a separately trained phone classifier will guide the synthesis process towards proper articulation. Objective and subjective evaluation results show that the proposed method substantially improves articulation in the synthesized speech over original recordings, and preserves more of the target speaker's individuality than a voice conversion baseline",
    "checked": true,
    "id": "ed4ae0fe589c0c15e8a64d40b58edcab29f508dc",
    "semantic_title": "creating personalized synthetic voices from post-glossectomy speech with guided diffusion models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vaessen23_interspeech.html": {
    "title": "Towards Multi-task Learning of Speech and Speaker Recognition",
    "volume": "main",
    "abstract": "We study multi-task learning for two orthogonal speech technology tasks: speech and speaker recognition. We use wav2vec2 as a base architecture with two task-specific output heads. We experiment with different architectural decisions to mix speaker and speech information in the output sequence as well as different optimization strategies. Our multi-task learning networks can produce a shared speaker and speech embedding, which on first glance achieve a performance comparable to separate single-task models. However, we show that the multi-task networks have strongly degraded performance on out-of-distribution evaluation data compared to the single-task models. Code and model checkpoints are available at https://github.com/nikvaessen/disjoint-mtl",
    "checked": true,
    "id": "2a514a665da71f60139b8fddea313bc9ac8f85f1",
    "semantic_title": "towards multi-task learning of speech and speaker recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23f_interspeech.html": {
    "title": "Regarding Topology and Variant Frame Rates for Differentiable WFST-based End-to-End ASR",
    "volume": "main",
    "abstract": "End-to-end (E2E) Automatic Speech Recognition (ASR) has gained popularity in recent years, with most research focusing on designing novel neural network architectures, speech representations, and loss functions. However, the importance of topology in E2E ASR has been largely neglected. There are many aspects of topology to consider; in this paper, we focus on the relationship between topologies' minimum traversal time and output frame rate, the number of distinct states for each output unit, and the flexibility of alignments admitted. We ex- amine several different topologies on two datasets: WSJ and Librispeech. Our experiments reveal that different frame rates have varying optimal topologies and that the commonly used Connectionist Temporal Classification (CTC) topology is not always optimal. Our findings suggest that the choice of topology is an important consideration in the design of E2E ASR systems",
    "checked": true,
    "id": "9547cc3ca706e03bae35f18c19dd2867dc2abc1c",
    "semantic_title": "regarding topology and variant frame rates for differentiable wfst-based end-to-end asr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23b_interspeech.html": {
    "title": "2-bit Conformer quantization for automatic speech recognition",
    "volume": "main",
    "abstract": "Large speech models are rapidly gaining traction in research community. As a result, model compression has become an important topic, so that these models can fit in memory and be served with reduced cost. Practical approaches for compressing automatic speech recognition (ASR) model use int8 or int4 weight quantization. In this study, we propose to develop 2-bit ASR models. We explore the impact of symmetric and asymmetric quantization combined with sub-channel quantization and clipping on both LibriSpeech dataset and large-scale training data. We obtain a lossless 2-bit Conformer model with 32% model size reduction when compared to state of the art 4-bit Conformer model for LibriSpeech. With the large-scale training data, we obtain a 2-bit Conformer model with over 40% model size reduction against the 4-bit version at the cost of 17% relative word error rate degradation",
    "checked": true,
    "id": "214922c919cace747f5bd08d005204ef559d72cb",
    "semantic_title": "2-bit conformer quantization for automatic speech recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23_interspeech.html": {
    "title": "Time-Domain Speech Enhancement for Robust Automatic Speech Recognition",
    "volume": "main",
    "abstract": "It has been shown that the intelligibility of noisy speech can be improved by speech enhancement algorithms. However, speech enhancement has not been established as an effective frontend for robust automatic speech recognition (ASR) in noisy conditions compared to an ASR model trained on noisy speech directly. The divide between speech enhancement and ASR impedes the progress of robust ASR systems especially as speech enhancement has made big strides in recent years. In this work, we focus on eliminating this divide with an ARN (attentive recurrent network) based time-domain enhancement model. The proposed system fully decouples speech enhancement and an acoustic model trained only on clean speech. Results on the CHiME-2 corpus show that ARN enhanced speech translates to improved ASR results. The proposed system achieves 6.28% average word error rate, outperforming the previous best by 19.3% relatively",
    "checked": true,
    "id": "14ed4cefa3859a7e217a7775be261ed75b841b0b",
    "semantic_title": "time-domain speech enhancement for robust automatic speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yifan23_interspeech.html": {
    "title": "Multi-channel multi-speaker transformer for speech recognition",
    "volume": "main",
    "abstract": "With the development of teleconferencing and in-vehicle voice assistants, far-field multi-speaker speech recognition has become a hot research topic. Recently, a multi-channel transformer (MCT) has been proposed, which demonstrates the ability of the transformer to model far-field acoustic environments. However, MCT cannot encode high-dimensional acoustic features for each speaker from mixed input audio because of the interference between speakers. Based on these, we propose the multi-channel multi-speaker transformer (M2Former) for far-field multi-speaker ASR in this paper. Experiments on the SMS-WSJ benchmark show that the M2Former outperforms the neural beamformer, MCT, dual-path RNN with transform-average-concatenate and multi-channel deep clustering based end-to-end systems by 9.2%, 14.3%, 24.9%, and 52.2% respectively, in terms of relative word error rate reduction",
    "checked": true,
    "id": "ada72ff1fb57f40705d22cb1816b58883b86843f",
    "semantic_title": "multi-channel multi-speaker transformer for speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ye23_interspeech.html": {
    "title": "Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion",
    "volume": "main",
    "abstract": "Deep speech classification has achieved tremendous success and greatly promoted the emergence of many real-world applications. However, backdoor attacks present a new security threat to it, particularly with untrustworthy third-party platforms, as pre-defined triggers set by the attacker can activate the backdoor. Most of the triggers in existing speech backdoor attacks are sample-agnostic, and even if the triggers are designed to be unnoticeable, they can still be audible. This work explores a backdoor attack that utilizes sample-specific triggers based on voice conversion. Specifically, we adopt a pre-trained voice conversion model to generate the trigger, ensuring that the poisoned samples does not introduce any additional audible noise. Extensive experiments on two speech classification tasks demonstrate the effectiveness of our attack. Furthermore, we analyzed the specific scenarios that activated the proposed backdoor and verified its resistance against fine-tuning",
    "checked": true,
    "id": "7c4525725d15abfa65e4d6e3e53ca03f62613588",
    "semantic_title": "fake the real: backdoor attack on deep speech classification via voice conversion",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/miwa23_interspeech.html": {
    "title": "Dialect Speech Recognition Modeling using Corpus of Japanese Dialects and Self-Supervised Learning-based Model XLSR",
    "volume": "main",
    "abstract": "In order to utilize the large amount of historical speech resources for applications such as linguistic analysis and retrieval, automatic speech recognition technology that can handle a variety of dialects is required. Although there are many dialects in the Japanese language, there have been no reports of speech recognition models that cover almost all Japanese dialects using only shared dialect resources. This paper presents a baseline for dialect speech recognition of spoken Japanese using a nationwide corpus of Japanese dialects released in 2022. Specifically, the paper presents results on: 1) the effectiveness of adapting a self-supervised learning model, which has been shown to be effective for low-resource languages, to the dialect corpus; 2) the effectiveness of combining both automatic speech recognition and dialect region identification tasks, or when used in conjunction with a large-scale corpus of standard Japanese, within the framework of self-supervised learning",
    "checked": true,
    "id": "d051bc64243150db6327cdca832ff90f923f0457",
    "semantic_title": "dialect speech recognition modeling using corpus of japanese dialects and self-supervised learning-based model xlsr",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23d_interspeech.html": {
    "title": "Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network",
    "volume": "main",
    "abstract": "Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list",
    "checked": true,
    "id": "547a7f10f1fec6c77471139cbae23b21b120aa24",
    "semantic_title": "contextualized end-to-end speech recognition with contextual phrase prediction network",
    "citation_count": 8,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raissi23_interspeech.html": {
    "title": "Competitive and Resource Efficient Factored Hybrid HMM Systems are Simpler Than You Think",
    "volume": "main",
    "abstract": "Building competitive hybrid hidden Markov model (HMM) systems for automatic speech recognition (ASR) requires a complex multi-stage pipeline consisting of several training criteria. The recent sequence-to-sequence models offer the advantage of having simpler pipelines that can start from-scratch. We propose a purely neural based single-stage from-scratch pipeline for a context-dependent hybrid HMM that offers similar simplicity. We use an alignment from a full-sum trained zero-order posterior HMM with a BLSTM encoder. We show that with this alignment we can build a Conformer factored hybrid that performs even better than both a state-of-the-art classic hybrid and a factored hybrid trained with alignments taken from more complex Gaussian mixture based systems. Our finding is confirmed on Switchboard 300h and LibriSpeech 960h tasks with comparable results to other approaches in the literature, and by additionally relying on a responsible choice of available computational resources",
    "checked": true,
    "id": "3461f39488e0e3c7ba8e23677fdb597eb40bdb42",
    "semantic_title": "competitive and resource efficient factored hybrid hmm systems are simpler than you think",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23d_interspeech.html": {
    "title": "MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for speech recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a novel multi-modal multi-task encoder-decoder pre-training framework (MMSpeech) for Mandarin automatic speech recognition (ASR), which employs both unlabeled speech and text data. The main difficulty in speech-text joint pre-training comes from the significant difference between speech and text modalities, especially for Mandarin speech and text. Unlike English and other languages with an alphabetic writing system, Mandarin uses an ideographic writing system where character and sound are not tightly mapped to one another. Therefore, we propose to introduce the phoneme modality into pre-training, which can help capture modality-invariant information between Mandarin speech and text. In addition, a much larger amount of unsupervised text data 292G is utilized for pre-training, which brings significant improvements. Experiments on AISHELL-1 show that our proposed method achieves state-of-the-art performance, with a more than 40% relative improvement",
    "checked": true,
    "id": "88f00bc09d51eeebef8a660c957965c727ed4b51",
    "semantic_title": "mmspeech: multi-modal multi-task encoder-decoder pre-training for speech recognition",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kreyssig23_interspeech.html": {
    "title": "Biased Self-supervised Learning for ASR",
    "volume": "main",
    "abstract": "Self-supervised learning via masked prediction pre-training (MPPT) has shown impressive performance on a range of speech-processing tasks. This paper proposes a method to bias self-supervised learning towards a specific task. The core idea is to slightly finetune the model that is used to obtain the target sequence. This leads to better performance and a substantial increase in training speed. Furthermore, this paper proposes a variant of MPPT that allows low-footprint streaming models to be trained effectively by computing the MPPT loss on masked and unmasked frames. These approaches are evaluated for automatic speech recognition on the Librispeech corpus, where 100 hours of data served as the labelled data and 860 hours as the unlabelled data. The biased training outperforms the unbiased training by 15.5% after 250k updates and 23.8% after 100k updates on test-other. For the streaming models, the pre-training approach yields a reduction in word error rate of 44.1%",
    "checked": true,
    "id": "e2ad680160701884a6b92d48199ddd4710577c55",
    "semantic_title": "biased self-supervised learning for asr",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23q_interspeech.html": {
    "title": "A Unified Recognition and Correction Model under Noisy and Accent Speech Conditions",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) and its post-processing, such as recognition error correction, are usually cascaded in a pipeline ignoring their strong interconnection. Inspired by the recent progress of leveraging text data to improve linguistic modeling, we propose a Unified ASR and error Correction framework (UAC), coupling speech recognition and error correction to capture richer semantic information for improving the performance of speech recognition. The proposed framework established interaction between speech and textual representations via explicitly fusing their uni-modal embeddings in a shared encoder. Additionally, the proposed framework is flexible to operate in either synchronous or asynchronous variant and could be equipped with modality and task tags enhancing its adaptation to heterogeneous inputs. Experimental results on accented and noisy speech datasets demonstrate that our method effectively produces improved word error rate when compared against the pipeline baselines",
    "checked": true,
    "id": "cfaf207debecffa96be9b97d72cd849e685e5cc6",
    "semantic_title": "a unified recognition and correction model under noisy and accent speech conditions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23h_interspeech.html": {
    "title": "wav2vec 2.0 ASR for Cantonese-Speaking Older Adults in a Clinical Setting",
    "volume": "main",
    "abstract": "The lack of large-scale speech corpora for Cantonese and older adults has impeded the academia's research of automatic speech recognition (ASR) systems for the two. On the other hand, the recent success of self-supervised speech representation learning has shown its competitiveness in low-resource ASR. This work therefore studies the application of wav2vec 2.0 ASR using monolingual and cross-lingual pre-trained models on a developing speech corpus, CU-MARVEL, which is dedicated to the automated screening of neurocognitive disorders (NCD) for Cantonese-speaking older adults in Hong Kong. We detail our data preparation procedures for creating a monolingual wav2vec 2.0 model from scratch and further pre-training a cross-lingual model. We report the performance of our wav2vec 2.0 ASR models on the said corpus and present a preliminary analysis of the relationship between the ASR performance of older adult speech and various demographic characteristics",
    "checked": true,
    "id": "bce466291238a552b59cc12bc44dfd732ba5f81f",
    "semantic_title": "wav2vec 2.0 asr for cantonese-speaking older adults in a clinical setting",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/an23_interspeech.html": {
    "title": "BAT: Boundary aware transducer for memory-efficient and low-latency ASR",
    "volume": "main",
    "abstract": "Recently, recurrent neural network transducer (RNN-T) gains increasing popularity due to its natural streaming capability as well as superior performance. Nevertheless, RNN-T training requires large time and computation resources as RNN-T loss calculation is slow and consumes a lot of memory. Another limitation of RNN-T is that it tends to access more contexts for better performance, thus leading to higher emission latency in streaming ASR. In this paper we propose boundary-aware transducer (BAT) for memory-efficient and low-latency ASR. In BAT, the lattice for RNN-T loss computation is reduced to a restricted region selected by the alignment from continuous integrate-and-fire (CIF), which is jointly optimized with the RNN-T model. Extensive experiments demonstrate that compared to RNN-T, BAT reduces time and memory consumption significantly in training, and achieves good CER-latency trade-offs in inference for streaming ASR",
    "checked": true,
    "id": "34664bc7925487b12ddfa5ede598184ab8bd37e3",
    "semantic_title": "bat: boundary aware transducer for memory-efficient and low-latency asr",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tian23_interspeech.html": {
    "title": "Bayes Risk Transducer: Transducer with Controllable Alignment Prediction",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) based on transducers is widely used. In training, a transducer maximizes the summed posteriors of all paths. The path with the highest posterior is commonly defined as the predicted alignment between the speech and the transcription. While the vanilla transducer does not have a prior preference for any of the valid paths, this work intends to enforce the preferred paths and achieve controllable alignment prediction. Specifically, this work proposes Bayes Risk Transducer (BRT), which uses a Bayes risk function to set lower risk values to the preferred paths so that the predicted alignment is more likely to satisfy specific desired properties. We further demonstrate that these predicted alignments with intentionally designed properties can provide practical advantages over the vanilla transducer. Experimentally, the proposed BRT saves inference cost by up to 46% for non-streaming ASR and reduces overall system latency by 41% for streaming ASR",
    "checked": true,
    "id": "d2897d70e1bceaf4799937e4b4aab0a45fc6e20c",
    "semantic_title": "bayes risk transducer: transducer with controllable alignment prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alastruey23_interspeech.html": {
    "title": "Multi-View Frequency-Attention Alternative to CNN Frontends for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Convolutional frontends are a typical choice for Transformer-based asr to preprocess the spectrogram, reduce its sequence length, and combine local information in time and frequency similarly. However, the width and height of an audio spectrogram denote different information, e.g., due to reverberation as well as the articulatory system, the time axis has a clear left-to-right dependency. On the contrary, vovals and consonants demonstrate very different patterns and occupy almost disjoint frequency ranges. Therefore, we hypothesize, global attention over frequencies is beneficial over local convolution. We obtain 2.4 % rWERR on a production scale Conformer transducer replacing its CNN frontend by the proposed F-Attention module on Alexa traffic. To demonstrate generalizability, we validate this on public LibriSpeech data with an LSTM-based LAS architecture obtaining 4.6 % rWERR and demonstrate robustness to (simulated) noisy conditions",
    "checked": true,
    "id": "eedfdb7cd788e33ef669e4646b9567abd2789e03",
    "semantic_title": "multi-view frequency-attention alternative to cnn frontends for automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sankar23_interspeech.html": {
    "title": "Investigating the dynamics of hand and lips in French Cued Speech using attention mechanisms and CTC-based decoding",
    "volume": "main",
    "abstract": "Hard of hearing or profoundly deaf people make use of cued speech (CS) as a communication tool to understand spoken language. By delivering cues that are relevant to the phonetic information, CS offers a way to enhance lipreading. In literature, there have been several studies on the dynamics between the hand and the lips in the context of human production. This article proposes a way to investigate how a neural network learns this relation for a single speaker while performing a recognition task using attention mechanisms. Further, an analysis of the learnt dynamics is utilized to establish the relationship between the two modalities and extract automatic segments. For the purpose of this study, a new dataset has been recorded for French CS. Along with the release of this dataset, a benchmark will be reported for word-level recognition, a novelty in the automatic recognition of French CS",
    "checked": true,
    "id": "597358a260c80199e64470fa28534ff7e8139883",
    "semantic_title": "investigating the dynamics of hand and lips in french cued speech using attention mechanisms and ctc-based decoding",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23h_interspeech.html": {
    "title": "Hearing Loss Affects Emotion Perception in Older Adults: Evidence from a Prosody-Semantics Stroop Task",
    "volume": "main",
    "abstract": "Semantics and prosody are two cues for the perception of spoken emotion. In situations where cues conflict, older adults (OA) have difficulty inhibiting one channel and focusing on the other. OA with hearing loss may face more challenges. In this study, we examined the effects of aging and hearing loss on multi-channel emotion processing through a prosody-semantics Stroop task in three groups of participants, i.e., younger adults (YA) and OA with and without hearing loss. It was found that OA with hearing loss showed the most degraded performance in processing conflicting information. When information was incongruent in two channels, they judged emotions less accurately than the other two groups. Moreover, OA with hearing loss was the only group to show channel dominance, in that they performed lower accuracy in the prosodic channel. These findings suggest that hearing loss affects spoken emotional perception in conflict situations, independent of age-related changes",
    "checked": true,
    "id": "aa446c86165ba43fc6ca1a08aee081894f47c54f",
    "semantic_title": "hearing loss affects emotion perception in older adults: evidence from a prosody-semantics stroop task",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23b_interspeech.html": {
    "title": "Cochlear-implant Listeners Listening to Cochlear-implant Simulated Speech",
    "volume": "main",
    "abstract": "Channel vocoders with noise and sine-wave carriers are widely used to simulate modern multi-channel cochlear implants (CIs) in psychoacoustic experiments with normal hearing (NH) subjects. NH subjects perceive vocoded speech as impoverished and unnatural, but how CI listeners perceive vocoded sounds has not been systematically investigated. This letter reports that CI listeners could equally recognize both noise and sine-wave vocoded speech, albeit less well than NH listeners, and the recognition performance would not significantly increase beyond 8 channels. Nevertheless, they can easily discriminate up-to-80-channel vocoded speech from the original natural speech",
    "checked": true,
    "id": "6a8013a7a71323db97f69fa7033f16bd13b578c8",
    "semantic_title": "cochlear-implant listeners listening to cochlear-implant simulated speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/murton23_interspeech.html": {
    "title": "Validation of a Task-Independent Cepstral Peak Prominence Measure with Voice Activity Detection",
    "volume": "main",
    "abstract": "This study investigates the task-dependence of standard cepstral peak prominence (CPP) computation methods, and the advantages conferred by an open-source method of excluding unvoiced regions in CPP computation. We use Praat and a public dataset (Perceptual Voice Qualities Database, consisting of 295 speakers) to assess how well a voice-only CPP algorithm identifies voice disorders, identifies perceived dysphonia, and correlates with dysphonia severity. Results indicate that, compared to standard CPP computation, voice-only CPP is (1) less affected by unvoiced regions in the speech signal and (2) better reflects clinical outcomes (i.e., voice disorder diagnosis and dysphonia severity) for data sets that contain varying speech tasks. We expect voice-only CPP to be particularly useful for assessing speech that contains unknown or heterogeneous utterance types, as well as for speakers whose voice signal is affected by involvement of other speech subsystems (e.g., articulatory impairment)",
    "checked": true,
    "id": "9317e959c039eee400b74ca2f71d727a2784f587",
    "semantic_title": "validation of a task-independent cepstral peak prominence measure with voice activity detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23b_interspeech.html": {
    "title": "Score-balanced Loss for Multi-aspect Pronunciation Assessment",
    "volume": "main",
    "abstract": "With rapid technological growth, automatic pronunciation assessment has transitioned toward systems that evaluate pronunciation in various aspects, such as fluency and stress. However, despite the highly imbalanced score labels within each aspect, existing studies have rarely tackled the data imbalance problem. In this paper, we suggest a novel loss function, score-balanced loss, to address the problem caused by uneven data, such as bias toward the majority scores. As a re-weighting approach, we assign higher costs when the predicted score is of the minority class, thus, guiding the model to gain positive feedback for sparse score prediction. Specifically, we design two weighting factors by leveraging the concept of an effective number of samples and using the ranks of scores. We evaluate our method on the speechocean762 dataset, which has noticeably imbalanced scores for several aspects. Improved results particularly on such uneven aspects prove the effectiveness of our method",
    "checked": true,
    "id": "b0802785aad4c3f47c97f0dbeb92abf64144c1d0",
    "semantic_title": "score-balanced loss for multi-aspect pronunciation assessment",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tayebiarasteh23_interspeech.html": {
    "title": "Federated Learning for Secure Development of AI Models for Parkinson's Disease Detection Using Speech from Different Languages",
    "volume": "main",
    "abstract": "Parkinson's disease (PD) is a neurological disorder impacting a person's speech. Among automatic PD assessment methods, deep learning models have gained particular interest. Recently, the community has explored cross-pathology and cross-language models which can improve diagnostic accuracy even further. However, strict patient data privacy regulations largely prevent institutions from sharing patient speech data with each other. In this paper, we employ federated learning (FL) for PD detection using speech signals from 3 real-world language corpora of German, Spanish, and Czech, each from a separate institution. Our results indicate that the FL model outperforms all the local models in terms of diagnostic accuracy, while not performing very differently from the model based on centrally combined training sets, with the advantage of not requiring any data sharing among collaborators. This will simplify inter-institutional collaborations, resulting in enhancement of patient outcomes",
    "checked": true,
    "id": "b6df787f1ba25218bfa83108932390f8ccd6d8b6",
    "semantic_title": "federated learning for secure development of ai models for parkinson's disease detection using speech from different languages",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23c_interspeech.html": {
    "title": "F0inTFS: A lightweight periodicity enhancement strategy for cochlear implants",
    "volume": "main",
    "abstract": "Periodicity is one of the main cues for pitch-related speech perception but is poorly encoded in cochlear implants (CIs). Most efforts towards CI periodicity enhancement involve explicit fundamental frequency (F0) detection, which requires additional computational loads and may not be reliable in real settings. Here we propose a new strategy, namely F0inTFS, which encodes F0 information without explicit F0 detection. Our idea is inspired by the fact that temporal fine structures (TFS) at the low-frequency channels inherently contain strong F0-related periodicity cues. In F0inTFS, the TFS of the lowest-frequency channel is integrated into the temporal envelopes at all higher-frequency channels using a specifically designed algorithm. F0inTFS can be lightly implemented in the FFT-based framework of one clinically used strategy, i.e., the Advanced Combination Encoder (ACE) strategy. The benefits of F0inTFS are supported by a lexical tone perception experiment in simulated CI users",
    "checked": true,
    "id": "f98b81126887b9c9de8227a04a3a3dc21f4faf00",
    "semantic_title": "f0intfs: a lightweight periodicity enhancement strategy for cochlear implants",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/obrien23_interspeech.html": {
    "title": "Differentiating acoustic and physiological features in speech for hypoxia detection",
    "volume": "main",
    "abstract": "In order to stave off the effects of hypoxia, speech may become limited at elevated altitudes. This paper evaluates the role of speech on acoustic and physiological features used to detect hypoxia. Acoustic, cerebral blood oxygenation, and cardiac signals were recorded from participants who completed control and normobaric hypoxia experimental conditions. Acoustic and physiological features were extracted from (non-)speech segments via a voice activity detection method. Support Vector Machines were used to evaluate hypoxia classification using independent and combined features produced at sea-level and simulated 5 km altitudes. Models were built upon a 4-fold cross-validation design and evaluated on an independent dataset. Our results confirmed the importance of physiological features when detecting hypoxia. When combined, acoustic features boosted performance by 10% at 5 km in comparison to sea-level. Hypoxia detection may be improved by distinguishing respiration from non-speech",
    "checked": true,
    "id": "c58c1efe7192787557224d96d4891889602f08ce",
    "semantic_title": "differentiating acoustic and physiological features in speech for hypoxia detection",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23h_interspeech.html": {
    "title": "Mandarin Electrolaryngeal Speech Voice Conversion using Cross-domain Features",
    "volume": "main",
    "abstract": "Patients who have had their entire larynx removed, including the vocal folds, owing to throat cancer may experience difficulties in speaking. In such cases, electrolarynx devices are often prescribed to produce speech, which is commonly referred to as electrolaryngeal speech (EL speech). However, the quality and intelligibility of EL speech are poor. To address this problem, EL voice conversion (ELVC) is a method used to improve the intelligibility and quality of EL speech. In this paper, we propose a novel ELVC system that incorporates cross-domain features, specifically spectral features and self-supervised learning (SSL) embeddings. The experimental results show that applying cross-domain features can notably improve the conversion performance for the ELVC task compared with utilizing only traditional spectral features",
    "checked": true,
    "id": "3a0f860d161a940418ba66dc0d0e08e9a9c64aea",
    "semantic_title": "mandarin electrolaryngeal speech voice conversion using cross-domain features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chien23_interspeech.html": {
    "title": "Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion",
    "volume": "main",
    "abstract": "Electrolarynx is a commonly used assistive device to help patients with removed vocal cords regain their ability to speak. Although the electrolarynx can generate excitation signals like the vocal cords, the naturalness and intelligibility of electrolaryngeal (EL) speech are very different from those of natural (NL) speech. Many deep-learning-based models have been applied to electrolaryngeal speech voice conversion (ELVC) for converting EL speech to NL speech. In this study, we propose a multimodal voice conversion (VC) model that integrates acoustic and visual information into a unified network. We compared different pre-trained models as visual feature extractors and evaluated the effectiveness of these features in the ELVC task. The experimental results demonstrate that the proposed multimodal VC model outperforms single-modal models in both objective and subjective metrics, suggesting that the integration of visual information can significantly improve the quality of ELVC",
    "checked": true,
    "id": "247dcb6ead12bb6dbe2462666c7b33118b8f5b09",
    "semantic_title": "audio-visual mandarin electrolaryngeal speech voice conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/illner23_interspeech.html": {
    "title": "Which aspects of motor speech disorder are captured by Mel Frequency Cepstral Coefficients? Evidence from the change in STN-DBS conditions in Parkinson's disease",
    "volume": "main",
    "abstract": "One of the most popular speech parametrizations for dysarthria has been Mel Frequency Cepstral Coefficients (MFCCs). Although the MFCCs ability to capture vocal track characteristics is known, the reflected dysarthria aspects are primarily undisclosed. Thus, we investigated the relationship between key acoustic variables in Parkinson's disease (PD) and the MFCCs. 23 PD patients were recruited with ON and OFF conditions of Deep Brain Stimulation of the Subthalamic Nucleus (STN-DBS) and examined via a reading passage. The changes in dysarthria aspects were compared to changes in a global MFCC measure and individual MFCCs. A similarity was found in 2nd to 3rd MFCCs changes and voice quality. Changes in 4th to 9th MFCCs reflected articulation clarity. The global MFCC parameter outperformed individual MFCCs and acoustical measures in capturing STN-DBS conditions changes. The findings may assist in interpreting outcomes from clinical trials and improve the monitoring of disease progression",
    "checked": true,
    "id": "c138c7e2c6004417699aa899d93055eacc5ebcf4",
    "semantic_title": "which aspects of motor speech disorder are captured by mel frequency cepstral coefficients? evidence from the change in stn-dbs conditions in parkinson's disease",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/subramanian23_interspeech.html": {
    "title": "Detecting Manifest Huntington's Disease Using Vocal Data",
    "volume": "main",
    "abstract": "Huntington's disease (HD) is an autosomal-dominant neurodegenerative disorder that leads to the devastating loss of motor control - including severe speech impairment. Current models are insufficient to predict the onset or progression of manifest symptoms and early signs of the disease remain challenging to detect and monitor. Therefore, we propose a purely speech-based, non-invasive approach to discriminate Huntington's Disease patients who are exhibiting early signs of disease from those who are not. We study various features derived from speech and machine learning models to classify HD patients. Our results show that Random Forest classifiers leveraging language features perform very well with an unweighted accuracy of 0.95. In addition, we analyze the statistical significance of features, the importance of different questions asked to the patients, and other classification problems in Huntington's disease to provide a strong foundation for this field of research",
    "checked": true,
    "id": "35e015c0f3604e739a03cf0e21fc0255f52cd92d",
    "semantic_title": "detecting manifest huntington's disease using vocal data",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23q_interspeech.html": {
    "title": "Exploring multi-task learning and data augmentation in dementia detection with self-supervised pretrained models",
    "volume": "main",
    "abstract": "Detection of Alzheimer's Dementia (AD) is crucial for timely intervention to slow down disease progression. Using spontaneous speech to detect AD is a non-invasive, efficient and inexpensive approach. Recent innovations in self-supervised learning (SSL) have led to remarkable advances in speech processing. In this work, we investigate a set of SSL models using joint fine-tuning strategy and compare their performance with conventional classification model. Our work shows that fine-tuning the pretrained SSL models, in conjunction with multi-task learning and data augmentation, boosts the effectiveness of general-purpose speech representations in AD detection. The results surpass the baseline and are comparable to state-of-the-art performance on the popular ADReSS dataset. We also compare single- and multi-task training for AD classification, and analyze different augmentation methods to show how to achieve improved results",
    "checked": true,
    "id": "38bc595650d0d3e4cbb94c07a48f612e70210843",
    "semantic_title": "exploring multi-task learning and data augmentation in dementia detection with self-supervised pretrained models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23i_interspeech.html": {
    "title": "GL-SSD: Global and Local Speech Style Disentanglement by vector quantization for robust sentence boundary detection in speech stream",
    "volume": "main",
    "abstract": "Sentence boundary detection (SBD) in speech, aimed at segmenting the sentence units from the audio speech, plays a significant role in a broad range of tasks such as automatic speech recognition and speech translation. Previous studies have explored the solution based on basic acoustic features and high level semantic representation. Although widely studied, sentence boundary detection still remains a challenge when applied to different speech styles, including the global style and local style. To improve the robustness of SBD in the scene of different speech styles, we propose Global and Local Speech Style Disentanglement (GL-SSD) by vector quantization from the raw speech and incorporate the disentangled style representations into the semantic representation. Relevant experiments demonstrate the superior performance of the proposed method compared to other recent mainstream methods",
    "checked": true,
    "id": "5c5a4e505b21ad1489cab9a638b639140400c576",
    "semantic_title": "gl-ssd: global and local speech style disentanglement by vector quantization for robust sentence boundary detection in speech stream",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23c_interspeech.html": {
    "title": "Semantic VAD: Low-Latency Voice Activity Detection for Speech Interaction",
    "volume": "main",
    "abstract": "For speech interaction, voice activity detection (VAD) is often used as a front-end. However, traditional VAD algorithms usually need to wait for a continuous tail silence to reach a preset maximum duration before segmentation, resulting in a large latency that affects user experience. In this paper, we propose a novel semantic VAD for low-latency segmentation. Different from existing methods, a frame-level punctuation prediction task is added to the semantic VAD, and the artificial endpoint is included in the classification category in addition to the often-used speech presence and absence. To enhance the semantic information of the model, we also incorporate an automatic speech recognition (ASR) related semantic loss. Evaluations on an internal dataset show that the proposed method can reduce the average latency by 53.3% without significant deterioration of character error rate in the back-end ASR compared to the traditional VAD approach",
    "checked": true,
    "id": "76b9f90be351df9de896368fe04299e57b2e08b2",
    "semantic_title": "semantic vad: low-latency voice activity detection for speech interaction",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gudepu23_interspeech.html": {
    "title": "Dynamic Encoder RNN for Online Voice Activity Detection in Adverse Noise Conditions",
    "volume": "main",
    "abstract": "The majority of online Voice Activity Detection (VAD) models employ a Recurrent Neural Network (RNN) component to capture long context which helps to improve noise-robustness. These RNN components are static models which do not make efficient use of the model's predictions from previous frames. In this work, we introduce a new Dynamic Encoder RNN (DE-RNN) that encodes the target speech dynamically to facilitate distinguishing of target speech from noise. Experiments on different established baseline architectures by modifying their RNN component by the addition of DE-RNN, show improvement in both background noise and secondary competing speaker noise scenarios. We used publicly available datasets for experiments",
    "checked": true,
    "id": "c90e64899e7353bafc2ca26370f554423ae0d45e",
    "semantic_title": "dynamic encoder rnn for online voice activity detection in adverse noise conditions",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moussa23_interspeech.html": {
    "title": "Point to the Hidden: Exposing Speech Audio Splicing via Signal Pointer Nets",
    "volume": "main",
    "abstract": "Verifying the integrity of voice recording evidence for criminal investigations is an integral part of an audio forensic analyst's work. Here, one focus is on detecting deletion or insertion operations, so called audio splicing. While this is a rather easy approach to alter spoken statements, careful editing can yield quite convincing results. For difficult cases or big amounts of data, automated tools can support in detecting potential editing locations. To this end, several analytical and deep learning methods have been proposed by now. Still, few address unconstrained splicing scenarios as expected in practice. With SigPointer, we propose a pointer network framework for continuous input that uncovers splice locations naturally and more efficiently than existing works. Extensive experiments on forensically challenging data like strongly compressed and noisy signals quantify the benefit of the pointer mechanism with performance increases between about 6 to 10 percentage points",
    "checked": true,
    "id": "fecf9abb8ae7e1e01f94722f92dbbc3d15d380ad",
    "semantic_title": "point to the hidden: exposing speech audio splicing via signal pointer nets",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23k_interspeech.html": {
    "title": "Real-Time Causal Spectro-Temporal Voice Activity Detection Based on Convolutional Encoding and Residual Decoding",
    "volume": "main",
    "abstract": "Voice activity detection (VAD) is an essential front-end in many speech applications that aims at determining the presence or absence of speech signals in an audio frame. However, traditional VAD methods often suffer from poor performance or non-causality in low signal-to-noise ratio (SNR) environments. In this work, we therefore present a real-time causal VAD model, which mainly consists of a frequency-domain feature generation module, a convolutional-based encoding module and a residual block based decoding module. The exploitation of only current and past frames for feature extraction guarantees the causality. The effectiveness of the proposed model is verified on two datasets under various noise conditions. It is shown that the proposed method can achieve a comparable or even better performance than state-of-the-art non-causal models",
    "checked": true,
    "id": "d492a4a7e598cff631bdcded0489e165f656feec",
    "semantic_title": "real-time causal spectro-temporal voice activity detection based on convolutional encoding and residual decoding",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23c_interspeech.html": {
    "title": "SVVAD: Personal Voice Activity Detection for Speaker Verification",
    "volume": "main",
    "abstract": "Voice activity detection (VAD) improves the performance of speaker verification (SV) by preserving speech segments and attenuating the effects of non-speech. However, this scheme is not ideal: (1) it fails in noisy environments or multi-speaker conversations; (2) it is trained based on inaccurate non-SV sensitive labels. To address this, we propose a speaker verification-based voice activity detection (SVVAD) framework that can adapt the speech features according to which are most informative for SV. To achieve this, we introduce a label-free training method with triplet-like losses that completely avoids the performance degradation of SV due to incorrect labeling. Extensive experiments show that SVVAD significantly outperforms the baseline in terms of equal error rate (EER) under conditions where other speakers are mixed at different ratios. Moreover, the decision boundaries reveal the importance of the different parts of speech, which are largely consistent with human judgments",
    "checked": true,
    "id": "12f66a534e206fb72df7118a197a8ee9884e091a",
    "semantic_title": "svvad: personal voice activity detection for speaker verification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/farooq23_interspeech.html": {
    "title": "Learning Cross-lingual Mappings for Data Augmentation to Improve Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "Exploiting cross-lingual resources is an effective way to compensate for data scarcity of low resource languages. Recently, a novel multilingual model fusion technique has been proposed where a model is trained to learn cross-lingual acoustic-phonetic similarities as a mapping function. However, handcrafted lexicons have been used to train hybrid DNN-HMM ASR systems. To remove this dependency, we extend the concept of learnable cross-lingual mappings for end-to-end speech recognition. Furthermore, mapping models are employed to transliterate the source languages to the target language without using parallel data. Finally, the source audio and its transliteration is used for data augmentation to retrain the target language ASR. The results show that any source language ASR model can be used for a low-resource target language recognition followed by proposed mapping model. Furthermore, data augmentation results in a relative gain up to 5% over baseline monolingual model",
    "checked": true,
    "id": "5591536e72d24795803ea18e004ed369ec00a231",
    "semantic_title": "learning cross-lingual mappings for data augmentation to improve low-resource speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/olatunji23_interspeech.html": {
    "title": "AfriNames: Most ASR Models \"Butcher\" African Names",
    "volume": "main",
    "abstract": "Useful conversational agents must accurately capture named entities to minimize error for downstream tasks, for example, asking a voice assistant to play a track from a certain artist, initiating navigation to a specific location, or documenting a laboratory result for a patient. However, where named entities such as \"Ukachukwu\" (Igbo), \"Lakicia\" (Swahili), or \"Ingabire\" (Rwandan) are spoken, automatic speech recognition (ASR) models' performance degrades significantly, propagating errors to downstream systems. We model this problem as a distribution shift and demonstrate that such model bias can be mitigated through multilingual pre-training, intelligent data augmentation strategies to increase the representation of African-named entities, and fine-tuning multilingual ASR models on multiple African accents. The resulting fine-tuned models show an 81.5% relative WER improvement compared with the baseline on samples with African-named entities",
    "checked": true,
    "id": "682af4ad7c7d5489899c17a4fee4bfe55ebc702a",
    "semantic_title": "afrinames: most asr models \"butcher\" african names",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lonergan23_interspeech.html": {
    "title": "Towards Dialect-inclusive Recognition in a Low-resource Language: Are Balanced Corpora the Answer?",
    "volume": "main",
    "abstract": "ASR systems are generally built for the spoken 'standard', and their performance declines for non-standard dialects/varieties. This is a problem for a language like Irish, where there is no single spoken standard, but rather three major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a diagnostic to quantify the effect of the speaker's dialect on recognition performance, 12 ASR systems were trained, firstly using baseline dialect-balanced training corpora, and then using modified versions of the baseline corpora, where dialect-specific materials were either subtracted or added. Results indicate that dialect-balanced corpora do not yield a similar performance across the dialects: the Ul dialect consistently underperforms, whereas Mu yields lowest WERs. There is a close relationship between Co and Mu dialects, but one that is not symmetrical. These results will guide future corpus collection and system building strategies to optimise for cross-dialect performance equity",
    "checked": true,
    "id": "3e920149a0ca0fd52958cff095c7b24dbe5db2a5",
    "semantic_title": "towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/javed23_interspeech.html": {
    "title": "Svarah: Evaluating English ASR Systems on Indian Accents",
    "volume": "main",
    "abstract": "India is the second largest English-speaking country in the world with a speaker base of roughly 130 million. Thus, it is imperative that automatic speech recognition (ASR) systems for English should be evaluated on Indian accents. Unfortunately, Indian speakers find a very poor representation in existing English ASR benchmarks such as LibriSpeech, Switchboard, Speech Accent Archive, etc. In this work, we address this gap by creating Svarah, a benchmark that contains 9.6 hours of transcribed English audio from 117 speakers across 65 geographic locations throughout India, resulting in a diverse range of accents. Svarah comprises both read speech and spontaneous conversational data, covering various domains, such as history, culture, tourism, etc., ensuring a diverse vocabulary. We evaluate 6 open source ASR models and 2 commercial ASR systems on Svarah and show that there is clear scope for improvement on Indian accents",
    "checked": true,
    "id": "8d93cf8eae3db6121d8090ef3321d2ec805c059a",
    "semantic_title": "svarah: evaluating english asr systems on indian accents",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/talafha23_interspeech.html": {
    "title": "N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition",
    "volume": "main",
    "abstract": "Whisper, the recently developed multilingual weakly supervised model, is reported to perform well on multiple speech recognition benchmarks in both monolingual and multilingual settings. However, it is not clear how Whisper would fare under diverse conditions even on languages it was evaluated on such as Arabic. In this work, we address this gap by comprehensively evaluating Whisper on several varieties of Arabic speech for the ASR task. Our evaluation covers most publicly available Arabic speech data and is performed under n-shot (zero-, few-, and full) finetuning. We also investigate the robustness of Whisper under completely novel conditions such as in dialect-accented standard Arabic and in unseen dialects for which we develop evaluation data. Although Whisper zero-shot outperforms fully finetuned XLS-R models on all datasets, its performance deteriorates significantly in the zero-shot setting for five unseen dialects (i.e., Algeria, Jordan, Palestine, UAE, and Yemen)",
    "checked": true,
    "id": "9de1adba4949eb93e0aaea5f61f642d3a2b1835d",
    "semantic_title": "n-shot benchmarking of whisper on diverse arabic speech recognition",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/picheny23_interspeech.html": {
    "title": "The MALACH Corpus: Results with End-to-End Architectures and Pretraining",
    "volume": "main",
    "abstract": "The MALACH corpus contains approximately 375 hours of Holocaust survivor testimonies along with transcripts (for approximately half the data) and audio. It is an extremely difficult corpus for speech recognition, encompassing accented, emotional speech, in many cases from elderly survivors. Nevertheless, significant progress has been made on speech recognition on MALACH with WERs now typically hovering at a 20% level for hybrid speech recognition systems. The purpose of this paper is to examine if recent developments in end-to-end architectures and pretraining with self-supervision continue to drive down performance as they do on popular read corpora such as Librispeech. We also experiment with leveraging the large fraction of unlabeled corpus data by extracting pseudolabels produced from previously trained systems. It is found that the best system - a fine-tuned wav2vec2 system trained on labeled and pseudolabeled data - achieves a WER of 13.5%, a huge gain in performance",
    "checked": true,
    "id": "0a6547168872693a8f48ec147dded4062c98df57",
    "semantic_title": "the malach corpus: results with end-to-end architectures and pretraining",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23c_interspeech.html": {
    "title": "Unsupervised speech enhancement with deep dynamical generative speech and noise models",
    "volume": "main",
    "abstract": "This work builds on a previous work on unsupervised speech enhancement using a dynamical variational autoencoder (DVAE) as the clean speech model and non-negative matrix factorization (NMF) as the noise model. We propose to replace the NMF noise model with a deep dynamical generative model (DDGM) depending either on the DVAE latent variables, or on the noisy observations, or on both. This DDGM can be trained in three configurations: noise-agnostic, noise-dependent and noise adaptation after noise-dependent training. Experimental results show that the proposed method achieves competitive performance compared to state-of-the-art unsupervised speech enhancement methods, while the noise-dependent training configuration yields a much more time-efficient inference process",
    "checked": true,
    "id": "14ffea988aabdee4127ad57d35f293137b2c7e9e",
    "semantic_title": "unsupervised speech enhancement with deep dynamical generative speech and noise models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23f_interspeech.html": {
    "title": "Noise-Robust Bandwidth Expansion for 8K Speech Recordings",
    "volume": "main",
    "abstract": "Speech recordings in call centers are narrowband and mixed with various noises. Developing a bandwidth expansion (BWE) model is important to mitigate the automated speech recognition (ASR) performance gap between the low and high sampling rate speech data. To further address the in-the-wild noise in call center settings, we propose an Embedding-Polished Wave-U-Net (EP-WUN) that includes an additional speech quality classifier to handle the noise and bandwidth expansion of 8k audio simultaneously. Our framework shows improved speech quality metrics on a well-known BWE dataset (Valentini-Botinhao corpus) when comparing to the current state-of-the-art noise-robust BWE model with 33% fewer parameters. It also achieves an 11.71% word error rate reduction when evaluating on a real-world interactive voice response system from the E.SUN bank",
    "checked": true,
    "id": "8a68d7d095e275ac6c8582509d08c5f7dcff39e0",
    "semantic_title": "noise-robust bandwidth expansion for 8k speech recordings",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shuai23_interspeech.html": {
    "title": "mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra",
    "volume": "main",
    "abstract": "Speech super-resolution (SSR) aims to recover a high resolution (HR) speech from its corresponding low resolution (LR) counterpart. Recent SSR methods focus more on the reconstruction of the magnitude spectrogram, ignoring the importance of phase reconstruction, thereby limiting the recovery quality. To address this issue, we propose mdctGAN, a novel SSR framework based on modified discrete cosine transform (MDCT). By adversarial learning in the MDCT domain, our method reconstructs HR speeches in a phase-aware manner without vocoders or additional post-processing. Furthermore, by learning frequency consistent features with self-attentive mechanism, mdctGAN guarantees high quality speech reconstruction. For VCTK corpus dataset, the experiment results show that our model produces natural auditory quality with high MOS and PESQ scores. It also achieves the state-of-the-art log-spectral-distance (LSD) performance on 48 kHz target resolution from various input rates. Code is available from https://github.com/neoncloud/mdctGAN",
    "checked": true,
    "id": "65ca8615f2076fb6c115c8984fb7c4f6681149b6",
    "semantic_title": "mdctgan: taming transformer-based gan for speech super-resolution with modified dct spectra",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23b_interspeech.html": {
    "title": "Zoneformer: On-device Neural Beamformer For In-car Multi-zone Speech Separation, Enhancement and Echo Cancellation",
    "volume": "main",
    "abstract": "Despite the recent success of all-neural beamforming approaches for speech separation, deploying them onto low-powered devices is difficult due to their demanding computational requirements. To address this issue, we present a lightweight on-device Mel-subband neural beamformer for in-car multi-zone speech separation and introduce several effective methods to boost its performance. First, we propose a global full-band spectral and spatial embedding to assist the separation for each Mel-subband. Second, an explicit distortionless constraint is incorporated to control the non-linear distortion. Finally, teacher-student learning and quantization-aware training (QAT) are utilized to improve and accelerate the inference. Experimental results show that our proposed methods could achieve a significant word error rate (WER) reduction on real-recorded data and 0.39 real-time factor (RTF) on the device",
    "checked": true,
    "id": "d1534aa62c0897abadfb1dabd4547aefbca89880",
    "semantic_title": "zoneformer: on-device neural beamformer for in-car multi-zone speech separation, enhancement and echo cancellation",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23c_interspeech.html": {
    "title": "Low-complexity Broadband Beampattern Synthesis using Array Response Control",
    "volume": "main",
    "abstract": "Beampattern synthesis plays a critical role in fixed beamforming. A fast beampattern synthesis method is highly desired, especially for broadband beamformers requiring a large number of weight parameters for better performance. This paper proposes a low-complexity broadband beampattern synthesis method for time-domain beamformers. Introducing the null-forming scheme of adaptive beamformers, a virtual interference-plus-noise matrix is iteratively constructed to control the sidelobe pattern accurately. The proposed method reduces the computational complexity when compared with the existing algorithms based on the interior-point method (IPM), especially for large-scale microphone arrays. Simulation results demonstrate that the proposed method obtains equivalent beampatterns with much higher efficiency than the IPM-based method. In speech extraction experiments, the designed beamformers exhibit better suppression performance than the conventional fixed beamformer",
    "checked": true,
    "id": "20c0726052afa9cfbf143a95767fdd7a36ebe016",
    "semantic_title": "low-complexity broadband beampattern synthesis using array response control",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23d_interspeech.html": {
    "title": "A GAN Speech Inpainting Model for Audio Editing Software",
    "volume": "main",
    "abstract": "This paper proposes the generative adversarial networks (GAN) speech inpainting model consisting of the GAN magnitude inpainting network and the phase reconstruction algorithm. The GAN network with partial convolutions implements inpainting specific time-frequency (T-F) areas of spectrograms, and captures latent information of speech spectrograms and high-dimensional features using the proposed loss function, contributing to more real and speech-like results. The phase reconstruction algorithm adopts two strategies for different magnitudes, inpainting clear harmonics while reducing the buzzes in high frequency. The proposed model outperforms the conventional and the T-F mask-based deep inpainting baselines in inpainting performance of Short-Term Objective Intelligibility (STOI) and Perceptual Evaluation of Speech Quality (PESQ). Since it can inpaint specific T-F areas and improve the inpainting performance, the model implements the speech inpainting for audio editing software",
    "checked": true,
    "id": "05eb6d97161d0be4af23ca7f58c6ae74a342e9fd",
    "semantic_title": "a gan speech inpainting model for audio editing software",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23k_interspeech.html": {
    "title": "Deep Speech Synthesis from MRI-Based Articulatory Representations",
    "volume": "main",
    "abstract": "In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis",
    "checked": true,
    "id": "f53b6f5a85f2d74deb32022795b5dab0aa753cf4",
    "semantic_title": "deep speech synthesis from mri-based articulatory representations",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/siriwardena23_interspeech.html": {
    "title": "Learning to Compute the Articulatory Representations of Speech with the MIRRORNET",
    "volume": "main",
    "abstract": "Most organisms including humans function by coordinating and integrating sensory signals with motor actions to survive and accomplish desired tasks. Learning these complex sensorimotor mappings proceeds simultaneously and often in an unsupervised or semi-supervised fashion. An autoencoder architecture (MirrorNet) inspired by this sensorimotor learning paradigm is explored in this work to control an articulatory synthesizer, with minimal exposure to ground-truth articulatory data. The articulatory synthesizer takes as input a set of six vocal Tract Variables (TVs) and source features (voicing indicators and pitch) and is able to synthesize continuous speech for unseen speakers. We show that the MirrorNet, once initialized (with ~30 mins of articulatory data) and further trained in unsupervised fashion ('learning phase'), can learn meaningful articulatory representations with comparable accuracy to articulatory speech-inversion systems trained in a completely supervised fashion",
    "checked": true,
    "id": "cbbffa5b09b7dff89984e33e321d8904f88eb369",
    "semantic_title": "learning to compute the articulatory representations of speech with the mirrornet",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/strauch23_interspeech.html": {
    "title": "Generating high-resolution 3D real-time MRI of the vocal tract",
    "volume": "main",
    "abstract": "MRI recordings of the vocal tract allow researchers to obtain anatomical cross-sections in a non-invasive way, providing an important tool for speech production research. Acquiring MRI at equally high temporal and spatial resolution remains, however, challenging. We propose an image processing method for synthesising a real-time high spatial resolution 3D movie given real-time 2D MRI and static high spatial resolution 3D MRI data from the same speaker. We evaluate our method on a public dataset with 17 speakers, showing that a real-time 2D movie of the vocal tract during a speech task can be encoded by combinations of a small number of its frames. These combinations can be transferred to the domain of the high spatial resolution 3D data with static vocal tract articulations matched to frames of the 2D movie, synthesising a 3D movie of the speech task. Our algorithmic method provides a generic approach that can complement technical improvements of the acquisition process",
    "checked": true,
    "id": "afbbf3f978051706290d91d2453320c1aa60cc3c",
    "semantic_title": "generating high-resolution 3d real-time mri of the vocal tract",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bandekar23_interspeech.html": {
    "title": "Exploring a classification approach using quantised articulatory movements for acoustic to articulatory inversion",
    "volume": "main",
    "abstract": "Acoustic to articulatory inversion (AAI) is the task of predicting articulatory movements from speech acoustics. An AAI model is typically optimised with regression objectives on continuous articulatory targets. In this work, we explore an alternate approach of classifying bins of quantised articulatory movements. We extend it by utilising ordinal regression, along with a novel approach involving KL Divergence loss between a target Gaussian posterior and predicted. We train transformer AAI models with MFCC and TERA acoustic features, with various quantisation types (uniform vs nonuniform) and bins. Using 16 subjects' acousticarticulatory data, we evaluate the results with correlation coefficient (CC) and root mean squared error on unseen utterances from seen and unseen speakers. While the quantization type didn't alter the performance, we find that the highest CC (0.8838) is achieved with TERA using ordinal regression, also with the proposed KL divergence loss, which is found to be on par with the CC (0.8856) using regression baseline. Reducing the number of quantisation bins to 16 does not change the performance",
    "checked": true,
    "id": "d3bd34dd575706a4635fbd4efbbcb62865850a0f",
    "semantic_title": "exploring a classification approach using quantised articulatory movements for acoustic to articulatory inversion",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oota23b_interspeech.html": {
    "title": "MEG Encoding using Word Context Semantics in Listening Stories",
    "volume": "main",
    "abstract": "Brain encoding is the process of mapping stimuli to brain activity. There is a vast literature on linguistic brain encoding for functional MRI (fMRI) related to syntactic and semantic representations. Magnetoencephalography (MEG), with higher temporal resolution than fMRI, enables us to look more precisely at the timing of linguistic feature processing. Unlike MEG decoding, few studies on MEG encoding using natural stimuli exist. Existing ones on story listening focus on phoneme and simple word-based features, ignoring more abstract features such as context, syntactic and semantic aspects. Inspired by previous fMRI studies, we study MEG brain encoding using basic syntactic and semantic features, with various context lengths and directions (past vs. future), for a dataset of 8 subjects listening to stories. We find that BERT representations predict MEG significantly but not other syntactic features or word embeddings (e.g. GloVe), allowing us to encode MEG in a distributed way across auditory and language regions in time. In particular, past context is crucial in obtaining significant results",
    "checked": true,
    "id": "cfbb072b163ee402e6a9491c9543fe85d9bbedad",
    "semantic_title": "meg encoding using word context semantics in listening stories",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cantisani23_interspeech.html": {
    "title": "Investigating the cortical tracking of speech and music with sung speech",
    "volume": "main",
    "abstract": "The cortical tracking of speech and music has been primarily investigated separately. Here, we propose a novel paradigm involving sung speech to systematically compare the cortical encoding of sung speech with that of speech and music alone, offering a benchmark for using it in auditory research with ecologically-valid tasks. While this approach will ultimately lead to a variety of neural indices of speech and music processing at various levels of abstraction, the first step is to examine the envelope tracking of sung speech. EEG is recorded from subjects listening to a set of stimuli explicitly designed and built for the comparison: hummed melodies, speech monologues, and sung speech sharing the lyrics with the speech condition and the melody with the music condition. Preliminary analyses using encoding and decoding modeling show robust and consistent acoustic responses across conditions, with the only significant differences exclusively due to melody processing",
    "checked": true,
    "id": "2034b93c3a05dae84e377e0c3c08e9ed35d751d4",
    "semantic_title": "investigating the cortical tracking of speech and music with sung speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/keding23_interspeech.html": {
    "title": "Coherence Estimation Tracks Auditory Attention in Listeners with Hearing Impairment",
    "volume": "main",
    "abstract": "Coherence estimation between speech envelope and electroencephalography (EEG) is a proven method in neural speech tracking. This paper proposes an improved coherence estimation algorithm which utilises phase sensitive multitaper cross-spectral estimation. Estimated EEG coherence differences between attended and ignored speech envelopes for a hearing impaired (HI) population are evaluated and compared. Testing was made on 31 HI subjects and showed significant coherence differences for grand averages over the delta, theta, and alpha EEG bands. Significance of increased coherence for attended speech was stronger for the new method compared to the traditional method. The new method of estimating EEG coherence, improves statistical detection performance and enables more rigorous data-based hypothesis-testing results",
    "checked": true,
    "id": "86494ce6460e696335040ab485723fb5f7174280",
    "semantic_title": "coherence estimation tracks auditory attention in listeners with hearing impairment",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oota23_interspeech.html": {
    "title": "Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI Brain Activity?",
    "volume": "main",
    "abstract": "Self-supervised speech based models have been found to be successful in predicting brain recordings of subjects experiencing naturalistic story listening. Inspired by the recent progress on deep learning models for various speech-processing tasks, existing literature has leveraged pretrained speech Transformer models for brain encoding. However, there is no work on exploring the efficacy of task-specific finetuned Transformer representations for this task. Hence, in this paper, we explore transfer learning from representations finetuned for eight different tasks from Speech processing Universal PERformance Benchmark (SUPERB) for predicting brain responses. Encoding models based on task features are used to predict activity in different regions across the whole brain, and also in language and auditory brain regions. Our experiments on finetuning the Wav2Vec2.0 model for these eight tasks show that the model finetuned on automatic speech recognition (ASR) yields the best encoding performance for the whole brain, language and auditory regions",
    "checked": true,
    "id": "b54aee1384ecd6ff12ffc3d8f7ec3514ef7e1324",
    "semantic_title": "speech taskonomy: which speech tasks are the most predictive of fmri brain activity?",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qiu23_interspeech.html": {
    "title": "Exploring Auditory Attention Decoding using Speaker Features",
    "volume": "main",
    "abstract": "The auditory attention decoding (AAD) approach aims to determine the identity of the attended talker in a multi-talker scenario using neuro recordings. In the past few years, various AAD methods have been proposed, and most of them rely on speech envelope reconstruction, which unfortunately face challenges with shortened decoding windows. Inspired by the findings that voices with different acoustic features arouse diverse brain activities in a very short period, this paper proposes to use speaker voice features instead of speech envelope as a speaker indicator for conducting AAD in short-time situations. To achieve this, a novel dual-branch convolutional network (DBCNet) is proposed to estimate speaker features from EEG. Results show that the proposed method achieves higher decoding accuracy than existing methods for short decoding windows (approximately 75% for 0.3-s window and 82% for 1.0-s window)",
    "checked": true,
    "id": "c6169010b88e4bc0d7e7ea85a1bab25e0bf05cf8",
    "semantic_title": "exploring auditory attention decoding using speaker features",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/soman23_interspeech.html": {
    "title": "Enhancing the EEG Speech Match Mismatch Tasks With Word Boundaries",
    "volume": "main",
    "abstract": "Recent studies have shown that the underlying neural mechanisms of human speech comprehension can be analyzed using a match-mismatch classification of the speech stimulus and the neural response. However, such studies have been conducted for fixed-duration segments without accounting for the discrete processing of speech in the brain. In this work, we establish that word boundary information plays a significant role in sentence processing by relating EEG to its speech input. We process the speech and the EEG signals using a network of convolution layers. Then, a word boundary-based average pooling is performed on the representations, and the inter-word context is incorporated using a recurrent layer. The experiments show that the modelling accuracy can be significantly improved (match-mismatch classification accuracy) to 93% on a publicly available speech-EEG data set, while previous efforts achieved an accuracy of 65-75% for this task",
    "checked": true,
    "id": "d01817dc7560ef2eb5e1a83082dbd7e8f2434c53",
    "semantic_title": "enhancing the eeg speech match mismatch tasks with word boundaries",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23e_interspeech.html": {
    "title": "Similar Hierarchical Representation of Speech and Other Complex Sounds In the Brain and Deep Residual Networks: An MEG Study",
    "volume": "main",
    "abstract": "Listeners recognize a vast number of complex sounds, but vocal sounds, speech and song, are essential for communication. Recently, deep neural networks (DNNs) have achieved human-level accuracy in sound classification, but do they illuminate similar properties with biological brains? In this study, we compared DNNs to primary and secondary auditory cortex to understand the hierarchy of sound representations in the brain. Ten subjects listened to speech and other naturalistic sounds while their magnetoencephalography (MEG) signals were recorded. Widely-used DNNs were trained to classify the same sounds. Brain activity localized to secondary auditory areas decoded speech significantly more accurately than other non-human sounds. Secondary auditory selectivity best matched later, and more complex layers of DNNs. Our results are compatible with special coding for speech in the brain and suggest comparable hierarchical principles of DNNs and neural processing of sounds",
    "checked": true,
    "id": "f479268beffa17058595cc7e19bf5c49b54e8184",
    "semantic_title": "similar hierarchical representation of speech and other complex sounds in the brain and deep residual networks: an meg study",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/macintyre23_interspeech.html": {
    "title": "Effects of spectral degradation on the cortical tracking of the speech envelope",
    "volume": "main",
    "abstract": "During speech listening, recurring patterns of neural activity become temporally coupled to stimulus features, such as the speech envelope. This cortical tracking can be measured using electroencephalography (EEG). Quantifying speech-brain coupling (e.g., as a correlation coefficient) sheds light on the neurobiological processes underlying perception and holds promise as an objective measure, particularly for clinical populations such as cochlear implant (CI) users. How spectral degradation associated with CI stimulation affects cortical tracking, however, remains unclear. In this EEG study, we simulate CI listening using vocoded speech with and without current spread, a realistic complication of CI stimulation. We find no effect of either vocoding or current spread on cortical tracking, despite differences in subjective reports of speech comprehension and implicit behavioural measures. We conclude that, when speech is intelligible, cortical tracking is robust to spectral degradation",
    "checked": true,
    "id": "5393d643a629ee44b945c7d4895a5a4205d2b2bc",
    "semantic_title": "effects of spectral degradation on the cortical tracking of the speech envelope",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/calderondepalma23_interspeech.html": {
    "title": "Effects of spectral and temporal modulation degradation on intelligibility and cortical tracking of speech signals",
    "volume": "main",
    "abstract": "Understanding speech in challenging listening environments relies on diverse streams of information, including sensory signals, prior knowledge, and expectations. This is a challenge for patient populations who have compromised bottom-up sensory information. Neural entrainment evaluations can offer insights into the effects of signal degradation on speech processing. We collected electroencephalography from normal hearing listeners, in order to evaluate the effects of spectro-temporal information removal on speech intelligibility and cortical tracking. Our results showed a decrease in speech intelligibility with increased degradation and a decrease in encoding accuracy for the degraded conditions, compared to the clean control, but no differences in encoding accuracy between degradation conditions. We found significant differences between the weights of temporal response functions of clean and degraded speech conditions, which were specific to each type of degradation",
    "checked": true,
    "id": "549deaa17adeb4ff09ad0d4578362c04f652e80d",
    "semantic_title": "effects of spectral and temporal modulation degradation on intelligibility and cortical tracking of speech signals",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23da_interspeech.html": {
    "title": "Transfer Learning for Personality Perception via Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Holistic perception of affective attributes is an important human perceptual ability. However, this ability is far from being realized in current affective computing, as not all of the attributes are well studied and their interrelationships are poorly understood. In this work, we investigate the relationship between two affective attributes: personality and emotion, from a transfer learning perspective. Specifically, we transfer Transformer-based and wav2vec2-based emotion recognition models to perceive personality from speech across corpora. Compared with previous studies, our results show that transferring emotion recognition is effective for personality perception. Moreoever, this allows for better use and exploration of small personality corpora. We also provide novel findings on the relationship between personality and emotion that will aid future research on holistic affect recognition",
    "checked": true,
    "id": "306868007d7774efd670fb80ba754be48689173d",
    "semantic_title": "transfer learning for personality perception via speech emotion recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nagano23_interspeech.html": {
    "title": "A stimulus-organism-response model of willingness to buy from advertising speech using voice quality",
    "volume": "main",
    "abstract": "Speech can affect the behavior of the listener. Our previous study showed that the stimulus-organism-response theory using emotional states can explain a person's willingness to buy from advertising speech. In addition, there have been reports of the influence of voice quality in speech, which differs from other advertising stimuli, but few studies have been done on willingness to buy. In this study, we conducted an experiment to determine whether voice quality affects the willingness to buy from advertising speech. Participants listened to advertising speech and rated their willingness to buy the products advertised and their own emotions and voice quality. We found that a model constructed using voice quality as a mediator can better explain the willingness to buy from advertising speech. These results could help train salespeople in advertising speech",
    "checked": true,
    "id": "c4ea47870dc5c85f681e7c16c613bca6f9a0f493",
    "semantic_title": "a stimulus-organism-response model of willingness to buy from advertising speech using voice quality",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/doukhan23_interspeech.html": {
    "title": "Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition",
    "volume": "main",
    "abstract": "This paper presents a software allowing to describe voices using a continuous Voice Femininity Percentage (VFP). This system is intended for transgender speakers during their voice transition and for voice therapists supporting them in this process. A corpus of 41 French cis- and transgender speakers was recorded. A perceptual evaluation allowed 57 participants to estimate the VFP for each voice. Binary gender classification models were trained on external gender-balanced data and used on overlapping windows to obtain average gender prediction estimates, which were calibrated to predict VFP and obtained higher accuracy than F0 or vocal track length-based models. Training data speaking style and DNN architecture were shown to impact VFP estimation. Accuracy of the models was affected by speakers' age. This highlights the importance of style, age, and the conception of gender as binary or not, to build adequate statistical representations of cultural concepts",
    "checked": true,
    "id": "d3bea27ce090bb017ff1f28e1e79b88a8f38c9e2",
    "semantic_title": "voice passing : a non-binary voice gender prediction system for evaluating transgender voice transition",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yanagida23_interspeech.html": {
    "title": "Influence of Personal Traits on Impressions of One's Own Voice",
    "volume": "main",
    "abstract": "The aim of this paper is to identify personal traits, e.g., age group, gender, personality traits, and values, that influence the perception of one's own voice. Previous studies have shown that the perception of one's own voice is different from those of others. Although studies have also shown that the perception of one's own voice is associated with the listener's personal traits, only a limited personal trait was considered in previous studies. In this paper, a large-scale subjective experiment using a few hundred Japanese participants was conducted to evaluate the perceptual voice impressions of their own and others' voices. We next analyzed relationships between the obtained voice impressions and their personal traits. As a result, we found that the perception of one's own voice was affected by multiple personal traits, such reported in the previous studies, and habits of listening to their own voices, which were not considered in previous studies",
    "checked": true,
    "id": "7dcb4116a703abd374dc9d44bda629250527bf88",
    "semantic_title": "influence of personal traits on impressions of one's own voice",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kirkland23_interspeech.html": {
    "title": "Pardon my disfluency: The impact of disfluency effects on the perception of speaker competence and confidence",
    "volume": "main",
    "abstract": "Disfluencies are a hallmark of spontaneous speech and play an important role in conversation, yet have been shown to negatively impact judgments about speakers. We explored the role of disfluencies in the perception of competence, sincerity and confidence in public speaking contexts, using synthesized spontaneous speech. In one experiment, listeners rated 30-40-second clips which varied in terms of whether they contained filled pauses, as well as the number and types of repetition. Both the overall number of disfluencies and the repetition type had an impact on competence and confidence, and disfluent speech was also rated as less sincere. In the second experiment, the negative effects of repetition type on competence were attenuated when participants attributed disfluency to anxiety",
    "checked": true,
    "id": "b25e654614f62911c4f19de5c00526e7fab3ff9e",
    "semantic_title": "pardon my disfluency: the impact of disfluency effects on the perception of speaker competence and confidence",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gessinger23_interspeech.html": {
    "title": "Cross-linguistic Emotion Perception in Human and TTS Voices",
    "volume": "main",
    "abstract": "This study investigates how German listeners perceive changes in the emotional expression of German and American English human voices and Amazon Alexa text-to-speech (TTS) voices, respectively. Participants rated sentences containing emotionally neutral lexico-semantic information that were resynthesized to vary in prosodic emotional expressiveness. Starting from an emotionally neutral production, three levels of increasing 'happiness' were created. Results show that 'happiness' manipulations lead to higher ratings of emotional valence (i.e., more positive) and arousal (i.e., more excited) for German and English voices, with stronger effects for the German voices. In particular, changes in valence were perceived more prominently in German TTS compared to English TTS. Additionally, both TTS voices were rated lower than the respective human voices on scales that reflect anthropomorphism (e.g., human-likeness). We discuss these findings in the context of cross-linguistic emotion accounts",
    "checked": true,
    "id": "876801a18ee3be6813618a61daff382cbefeba18",
    "semantic_title": "cross-linguistic emotion perception in human and tts voices",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/duan23_interspeech.html": {
    "title": "Joint Learning Feature and Model Adaptation for Unsupervised Acoustic Modelling of Child Speech",
    "volume": "main",
    "abstract": "Due to the high acoustic variability of child speech and the lack of publicly available datasets, acoustic modeling for child speech is challenging. In this work, we address these challenges by leveraging the large amounts of resources for adult speech (well-trained acoustic models and transcribed speech dataset) and proposing a joint acoustic feature and model adaptation framework to minimize acoustic mismatch between adult and child speech. Empirical results on three tasks of speech recog- nition, pronunciation assessment, and fluency assessment show that our proposed approach consistently outperforms competi- tive baselines, achieving up to 31.18% phone error reduction on speech recognition and around 7% gains on speech evaluation tasks",
    "checked": true,
    "id": "dc785a052b450ad3279523793dbeefd328bb424a",
    "semantic_title": "joint learning feature and model adaptation for unsupervised acoustic modelling of child speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/molenaar23_interspeech.html": {
    "title": "Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics",
    "volume": "main",
    "abstract": "Automatic assessment of reading fluency using automatic speech recognition (ASR) holds great potential for early detection of reading difficulties and subsequent timely intervention. Precise assessment tools are required, especially for languages other than English. In this study, we evaluate six state-of-the-art ASR-based systems for automatically assessing Dutch oral reading accuracy using Kaldi and Whisper. Results show our most successful system reached substantial agreement with human evaluations (MCC = .63). The same system reached the highest correlation between forced decoding confidence scores and word correctness (r = .45). This system's language model (LM) consisted of manual orthographic transcriptions and reading prompts of the test data, which shows that including reading errors in the LM improves assessment performance. We discuss the implications for developing automatic assessment systems and identify possible avenues of future research",
    "checked": true,
    "id": "e6096fe633517be1560f0a05267b883b966bdd06",
    "semantic_title": "automatic assessment of oral reading accuracy for reading diagnostics",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bai23_interspeech.html": {
    "title": "An ASR-enabled Reading Tutor: Investigating Feedback to Optimize Interaction for Learning to Read",
    "volume": "main",
    "abstract": "An ASR-based Dutch Reading Tutor (RT) was developed and applied to further investigate the impact of different forms of feedback as opposed to no feedback on reading aloud by Dutch first graders. The total of 752 first-grade students of Dutch practiced with the RT during fluency exercises in which they had to read words twice and received automatic feedback (implicit or explicit) or no-feedback. The results show that lower reading accuracy at the first attempt was accompanied by a slowdown in reading speed at the second attempt, even in the no-feedback condition. This trade-off between reading accuracy and speed resulted in higher accuracy scores at the second attempt across the board, with the best results in the explicit feedback condition. The results also show that such an ASR-based RT can be employed as a research instrument to obtain detailed insights into reading development. In turn these can also contribute to optimizing the design of RTs",
    "checked": true,
    "id": "8d135cc94e26a4dc1e3de4cddafc6ea415a4a3f4",
    "semantic_title": "an asr-enabled reading tutor: investigating feedback to optimize interaction for learning to read",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jain23_interspeech.html": {
    "title": "Adaptation of Whisper models to child speech recognition",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non-finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning",
    "checked": true,
    "id": "230c8ec1bbd56f7af022c919708d452029d52a89",
    "semantic_title": "adaptation of whisper models to child speech recognition",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yin23b_interspeech.html": {
    "title": "Let's Give a Voice to Conversational Agents in Virtual Reality",
    "volume": "main",
    "abstract": "The dialogue experience with conversational agents can be greatly enhanced with multimodal and immersive interactions in virtual reality. In this work, we present an open-source architecture with the goal of simplifying the development of conversational agents operating in virtual environments. The architecture offers the possibility of plugging in conversational agents of different domains and adding custom or cloud-based Speech-To-Text and Text-To-Speech models to make the interaction voice-based. Using this architecture, we present two conversational prototypes operating in the digital health domain developed in Unity for both non-immersive displays and VR headsets. The architecture is publicly available on GitHub",
    "checked": true,
    "id": "6073c88db5ef76ba514b4a9b50609bf3c80f20e5",
    "semantic_title": "let's give a voice to conversational agents in virtual reality",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baali23b_interspeech.html": {
    "title": "FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator",
    "volume": "main",
    "abstract": "This paper presents FOOCTTS, an automatic pipeline for a football commentator that generates speech with background crowd noise. The application gets the text from the user, applies text pre-processing such as vowelization, followed by the commentator's speech synthesizer. Our pipeline included Arabic automatic speech recognition for data labeling, CTC segmentation, transcription vowelization to match speech, and fine-tuning the TTS. Our system is capable of generating speech with its acoustic environment within limited 15 minutes of football commentator recording. Our prototype is generalizable and can be easily applied to different domains and languages",
    "checked": true,
    "id": "4d32df1f8fbd2305ebc2dcce9a9d52349f140972",
    "semantic_title": "fooctts: generating arabic speech with acoustic environment for football commentator",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23x_interspeech.html": {
    "title": "Video Summarization Leveraging Multimodal Information for Presentations",
    "volume": "main",
    "abstract": "This demonstration introduces a video summarization system, leveraging multimodal information to efficiently extract essential contents from presentations. In contrast to existing methods focusing primarily on daily life videos and solely utilizing visual information, our system extracts multimodal information, including speech, text, and visual information from videos of presentations. Specifically, the proposed system extracts crucial slide texts from key-frames as queries to filter speech transcripts. By piecing together the video clips corresponding to the filtered speech transcripts, our system outputs the final video summarizations. The evaluation on ICCV 2017 videos demonstrates the effectiveness of the proposed system compared with the lead-3 baseline",
    "checked": true,
    "id": "866832f001a6a9417be3af36ff85beaa99e876b4",
    "semantic_title": "video summarization leveraging multimodal information for presentations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nathan23_interspeech.html": {
    "title": "What questions are my customers asking?: Towards Actionable Insights from Customer Questions in Contact Center Calls",
    "volume": "main",
    "abstract": "Call centers serve as a critical point of contact between businesses and customers. The communication between customers and agents in such calls typically involves asking questions and responding to them. On average, a 10-minute call includes 2-3 customer questions. Such questions provide insights into customer's asks as well as identify areas of improvement for the questions where agents are taking longer time to respond. This motivates a need to peek into each call flowing through the contact center and derive business insights over such questions. To facilitate such deeper analysis and business intelligence at scale, there is a need to efficiently identify and rank the group of questions being asked over millions of calls flowing through a contact center. In this paper, we present a system for question monitoring via question extraction, rewriting and grouping, which enables contact centers to discover questions from calls at scale. Our in-house system leverages natural language processing techniques to transform customer questions into a format that is easily understandable, facilitating streamlined analysis of the data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tripathi23_interspeech.html": {
    "title": "COnVoy: A Contact Center Operated Pipeline for Voice of Customer Discovery",
    "volume": "main",
    "abstract": "With ever increasing volume of contact centers globally, there is a massive flow of customer service interactions into contact center telephony systems. These interactions are a rich source of information that can be utilized to uncover valuable insights regarding customer preferences and pain-points. In this paper, we propose an unsupervised pipeline comprising of an ASR engine, a reason-utterance detector, a re-writing module, a topic modelling block and a topic-description generator to enable Voice of Customer (VoC) Discovery by systematically tracking the reasons for the conversations. The insights surfaced by our pipeline can help contact centers get a bird's eye view of what is happening in their business. This would not only help business leaders come up with strategic measures to provide high quality and quicker resolutions to customers but also drive better customer satisfaction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rastorgueva23_interspeech.html": {
    "title": "NeMo Forced Aligner and its application to word alignment for subtitle generation",
    "volume": "main",
    "abstract": "We present NeMo Forced Aligner (NFA): an efficient and accurate forced aligner which is part of the NeMo conversational AI open-source toolkit. NFA can produce token, word, and segment-level alignments, and can generate subtitle files for highlighting words or tokens as they are spoken. We present a demo which shows this functionality, and demonstrate that NFA has the best word alignment accuracy and speed of alignment generation compared with other aligners",
    "checked": true,
    "id": "8324ad27c428ae865cfcb18143334ca77770f222",
    "semantic_title": "nemo forced aligner and its application to word alignment for subtitle generation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pattnaik23_interspeech.html": {
    "title": "CauSE: Causal Search Engine for Understanding Contact-Center Conversations",
    "volume": "main",
    "abstract": "Contact centers sit on multitude of conversational data that contains helpful information which can assist businesses to deliver better outcomes like improving customer experience. However, finding such information manually is hard. Towards this end, we propose CauSE, a causal search engine for understanding contact center conversations that assist in finding relevant answers to a question. Using topic modelling, the engine identifies themes within conversational contexts to help reason for the given question. To address the challenge of multiple topics in a single context, we divide the context into Elementary Discourse Units (EDUs) and perform topic modelling on EDUs to better identify coherent themes as topics. Subsequently, we employ a novel contrastive ranking algorithm to surface meaningful topics, and LLM-prompting to obtain descriptions for the topics. Our evaluations of the resultant topics and proof of value exercises demonstrate the strength of the proposed engine",
    "checked": true,
    "id": "e1d633128d0094f98ddbaf81264105a129ed7361",
    "semantic_title": "cause: causal search engine for understanding contact-center conversations",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sachdeva23_interspeech.html": {
    "title": "Tailored Real-Time Call Summarization System for Contact Centers",
    "volume": "main",
    "abstract": "Contact centers are critical for delivering high-quality customer service to various businesses. Call summarization is a crucial task for contact center agents for compliance, to transfer contextual information to the next agent, or to serve as a reference for future interactions. Agents spend a substantial amount of time writing notes on or after a call, which reduces their productivity and adds to the cost per call. While there exist various pre-trained Large Language Models (LLM) for summarization, they often lack coverage of domain-specific information relevant to businesses. We propose a hybrid streaming notes generation system leveraging the generative capabilities of an LLM fine-tuned for contact center call summarization, but allowing businesses to focus notes generation around events of business interest. Our system reduces after-call work for agents by not only generating notes out-of-the-box but also allowing agents to edit them in real time due to its streaming nature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mandke23_interspeech.html": {
    "title": "Federated Learning Toolkit with Voice-based User Verification Demo",
    "volume": "main",
    "abstract": "Federated learning (FL) is a distributed training mechanism in which a machine learning model is trained with data that is local to a set of edge devices, e.g., PCs or mobile devices. In this demonstration, we show how we tackle the challenges of implementing a FL system through a combined effort of Qualcomm Technologies, Inc. and Microsoft. We demonstrate this system in two parts. In a technical demonstration for machine learning (ML) researchers we show FL in action on Snapdragon devices as well as training orchestration through Microsoft Florida. Secondly, federated user verification based on voice samples serves as a practically relevant example for the INTERSPEECH community. We feature enrollment, subsequent acceptance of the enrolled user and rejection of impostors using the model that was trained through FL",
    "checked": false,
    "id": "03d327d45f26c8813248f5d2b6fa664e60940f38",
    "semantic_title": "federated learning toolkit with voice-based user verification demo submitted to interspeech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dugan23_interspeech.html": {
    "title": "Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models",
    "volume": "main",
    "abstract": "Recent work in speech-to-speech translation (S2ST) has focused primarily on offline settings, where the full input utterance is available before any output is given. This, however, is not reasonable in many real-world scenarios. In latency-sensitive applications, rather than waiting for the full utterance, translations should be spoken as soon as the information in the input is present. In this work, we introduce a system for simultaneous S2ST targeting real-world use cases. Our system supports translation from 57 languages to English with tunable parameters for dynamically adjusting the latency of the output---including four policies for determining when to speak an output sequence. We show that these policies achieve offline-level accuracy with minimal increases in latency over a Greedy (wait-k) baseline. We open-source our evaluation code and interactive test script to aid future SimulS2ST research and application development",
    "checked": true,
    "id": "730085698da1cedf1aae07b6a2e086e93996a352",
    "semantic_title": "learning when to speak: latency and quality trade-offs for simultaneous speech-to-speech translation with offline models",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cho23b_interspeech.html": {
    "title": "Fast Enrollable Streaming Keyword Spotting System: Training and Inference using a Web Browser",
    "volume": "main",
    "abstract": "When a keyword spotting system is deployed on heavily personalized platforms such as digital humans, a few issues occur such as 1) a lack of training data when registering user-defined keywords, 2) a desire to reduce computation and minimize latency, and 3) the inability to immediately train and test the keyword-spotting model. We address the issues through 1) a keyword-spotting system based on a speech embedding model, 2) streamable system with duplicate computations removed, and 3) real-time inference in a web browser using WebAssembly",
    "checked": true,
    "id": "99e566167124e52197b469596d05ecfba27e7d13",
    "semantic_title": "fast enrollable streaming keyword spotting system: training and inference using a web browser",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/agrawal23b_interspeech.html": {
    "title": "Cross-lingual/Cross-channel Intent Detection in Contact-Center Conversations",
    "volume": "main",
    "abstract": "Contact centers play a critical role in providing quality customer service for various businesses. Identification of business-related intents in these conversations is valuable for compliance, quality assurance, and analytics. This is often done using keyword spotting. But, identifying the right key phrases for accurate intent detection is challenging, especially for noisy call transcripts generated by ASR systems. Moreover, the process has to be repeated across different languages & to adapt existing intent definitions from other communication channels to call transcripts. We present a novel Semantic Phrase Search Engine that enables cross-lingual/cross-channel discovery of intent key phrases. The platform facilitates automated expansion of intent key phrases to improve coverage, and in-context translation of phrases across languages/channels, while maintaining semantic integrity. Thereby, our system helps in improving quality as well as reducing cost of contact center operations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heo23_interspeech.html": {
    "title": "One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification",
    "volume": "main",
    "abstract": "The application of speech self-supervised learning (SSL) models has achieved remarkable performance in speaker verification (SV). However, there is a computational cost hurdle in employing them, which makes development and deployment difficult. Several studies have simply compressed SSL models through knowledge distillation (KD) without considering the target task. Consequently, these methods could not extract SV-tailored features. This paper suggests One-Step Knowledge Distillation and Fine-Tuning (OS-KDFT), which incorporates KD and fine-tuning (FT). We optimize a student model for SV during KD training to avert the distillation of inappropriate information for the SV. OS-KDFT could downsize Wav2Vec 2.0 based ECAPA-TDNN size by approximately 76.2%, and reduce the SSL model's inference time by 79% while presenting an EER of 0.98%. The proposed OS-KDFT is validated across VoxCeleb1 and VoxCeleb2 datasets and W2V2 and HuBERT SSL models. Experiments are available on our GitHub",
    "checked": true,
    "id": "a9c9afe450914f8d7b2221ef94c19b2175095a3c",
    "semantic_title": "one-step knowledge distillation and fine-tuning in using large pre-trained self-supervised learning models for speaker verification",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kawa23_interspeech.html": {
    "title": "Defense Against Adversarial Attacks on Audio DeepFake Detection",
    "volume": "main",
    "abstract": "Audio DeepFakes (DF) are artificially generated utterances created using deep learning, with the primary aim of fooling the listeners in a highly convincing manner. Their quality is sufficient to pose a severe threat in terms of security and privacy, including the reliability of news or defamation. Multiple neural network-based methods to detect generated speech have been proposed to prevent the threats. In this work, we cover the topic of adversarial attacks, which decrease the performance of detectors by adding superficial (difficult to spot by a human) changes to input data. Our contribution contains evaluating the robustness of 3 detection architectures against adversarial attacks in two scenarios (white-box and using transferability) and enhancing it later by using adversarial training performed by our novel adaptive training. Moreover, one of the investigated architectures is RawNet3, which, to the best of our knowledge, we adapted for the first time to DeepFake detection",
    "checked": true,
    "id": "3845e613d0a60f7d58ec63544f7a1e26a50ad01a",
    "semantic_title": "defense against adversarial attacks on audio deepfake detection",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rosello23_interspeech.html": {
    "title": "A conformer-based classifier for variable-length utterance processing in anti-spoofing",
    "volume": "main",
    "abstract": "The success achieved by conformers in Automatic Speech Recognition (ASR) leads us to their application in other domains, such as spoofing detection for automatic speaker verification (ASV), where the conformer self-attention mechanism might effectively model and detect the artifacts introduced in spoofed speech signals. Also, conformers can naturally handle the variable duration of speech utterances. However, as with transformers, the conformer performance may degrade when trained with limited data. To address this issue, we propose utilizing conformers in conjunction with self-supervised learning, specifically leveraging a pre-trained model called wav2vec 2.0, which is pre-trained using a substantial amount of bonafide data. Our experimental results demonstrate that our proposed method achieves one of the best results in the recent ASVspoof 2021 logical access (LA) and deep fake (DF) databases",
    "checked": true,
    "id": "d8de21412f934b16955874c342bd4e73c47ac9d8",
    "semantic_title": "a conformer-based classifier for variable-length utterance processing in anti-spoofing",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ia_interspeech.html": {
    "title": "Conformer-based Language Embedding with Self-Knowledge Distillation for Spoken Language Identification",
    "volume": "main",
    "abstract": "The utilization of Conformer-based architecture has been shown to be effective in improving the performance of spoken language identification (LID) in recent years due to Conformer's superior representational capacity. However, when performing language identification on short speech segments, a significant drop in performance is often observed. In this paper, we propose a novel method to alleviate this issue by introducing a self-knowledge distillation technique to Conformer-based LID architecture. We distill the predictive distribution between the original input and the input processed by a double-ended random masking module during the training stage for each sample. Experimental results demonstrate the effectiveness of the proposed method on two datasets: OLR21 with 16,000 Hz sampling rate and LRE22 with 8,000 Hz sampling rate. Moreover, the proposed method also enhances the performance of language identification on short-duration speech segments",
    "checked": true,
    "id": "dcf4c6d4210429bf1ee06403f0580cb6ef1353c5",
    "semantic_title": "conformer-based language embedding with self-knowledge distillation for spoken language identification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zuluagagomez23_interspeech.html": {
    "title": "CommonAccent: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice",
    "volume": "main",
    "abstract": "Despite the recent advancements in Automatic Speech Recognition (ASR), the recognition of accented speech still remains a dominant problem. In order to create more inclusive ASR systems, research has shown that the integration of accent information, as part of a larger ASR framework, can lead to the mitigation of accented speech errors. We address multilingual accent classification through the ECAPA-TDNN and Wav2Vec 2.0/XLSR architectures which have been proven to perform well on a variety of speech-related downstream tasks. We introduce a simple-to-follow recipe aligned to the SpeechBrain toolkit for accent classification based on Common Voice 7.0 (English) and Common Voice 11.0 (Italian, German, and Spanish). Furthermore, we establish new state-of-the-art for English accent classification with as high as 95% accuracy. We also study the internal categorization of the Wav2Vev 2.0 embeddings through t-SNE, noting that there is a level of clustering based on phonological similarity",
    "checked": true,
    "id": "5fb48cb6711edca5c19711ee9fb5e7e881b10091",
    "semantic_title": "commonaccent: exploring large acoustic pretrained models for accent classification based on common voice",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cumani23_interspeech.html": {
    "title": "From adaptive score normalization to adaptive data normalization for speaker verification systems",
    "volume": "main",
    "abstract": "Domain and trial-dependent mismatch between training and evaluation data can severely affect the performance of speaker verification systems, and are usually addressed either at embedding level, with methods that try matching the distribution of in-domain and out-of-domain data, or at score level by means of calibration and score normalization approaches. In this work we propose an alternative to score normalization that leverages the adaptive cohort selection of Adaptive S-norm (AS-norm), but performs normalization at embedding rather than at score level. Experimental results on SRE 2016 and SRE 2019 show that the proposed method is able to outperform other approaches in presence of severe mismatch, and achieves similar performance in scenarios where score normalization is less important. Furthermore, in contrast with AS-norm, our approach allows independently normalizing the enrollment and test segments, and has negligible computational cost at scoring time",
    "checked": true,
    "id": "762e841f4ada616bf14b0a2776cec59ca8d3b87b",
    "semantic_title": "from adaptive score normalization to adaptive data normalization for speaker verification systems",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ha_interspeech.html": {
    "title": "CAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking",
    "volume": "main",
    "abstract": "Time delay neural network (TDNN) has been proven to be efficient for speaker verification. One of its successful variants, ECAPA-TDNN, achieved state-of-the-art performance at the cost of much higher computational complexity and slower inference speed. This makes it inadequate for scenarios with demanding inference rate and limited computational resources. We are thus interested in finding an architecture that can achieve the performance of ECAPA-TDNN and the efficiency of vanilla TDNN. In this paper, we propose an efficient network based on context-aware masking, namely CAM++, which uses densely connected time delay neural network (D-TDNN) as backbone and adopts a novel multi-granularity pooling to capture contextual information at different levels. Extensive experiments on two public benchmarks, VoxCeleb and CN-Celeb, demonstrate that the proposed architecture outperforms other mainstream speaker verification systems with lower computational cost and faster inference speed",
    "checked": true,
    "id": "2bfd331a0cc40db3f1b89d9730f612a6460d6971",
    "semantic_title": "cam++: a fast and efficient network for speaker verification using context-aware masking",
    "citation_count": 11,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kakouros23_interspeech.html": {
    "title": "North Sámi Dialect Identification with Self-supervised Speech Models",
    "volume": "main",
    "abstract": "The North Sámi (NS) language encapsulates four primary dialectal variants that are related but that also have differences in their phonology, morphology, and vocabulary. The unique geopolitical location of NS speakers means that in many cases they are bilingual in Sámi as well as in the dominant state language: Norwegian, Swedish, or Finnish. This enables us to study the NS variants both with respect to the spoken state language and their acoustic characteristics. In this paper, we investigate an extensive set of acoustic features, including MFCCs and prosodic features, as well as state-of-the-art self-supervised representations, namely, XLS-R, WavLM, and HuBERT, for the automatic detection of the four NS variants. In addition, we examine how the majority state language is reflected in the dialects. Our results show that NS dialects are influenced by the state language and that the four dialects are separable, reaching high classification accuracy, especially with the XLS-R model",
    "checked": true,
    "id": "2c086f9d71b6e6c1ece434d3f72bd7a609416204",
    "semantic_title": "north sámi dialect identification with self-supervised speech models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23c_interspeech.html": {
    "title": "Encoder-decoder Multimodal Speaker Change Detection",
    "volume": "main",
    "abstract": "The task of speaker change detection (SCD), which detects points where speakers change in an input, is essential for several applications. Several studies solved the SCD task using audio inputs only and have shown limited performance. Recently, multimodal SCD (MMSCD) models, which utilise text modality in addition to audio, have shown improved performance. In this study, the proposed model are built upon two main proposals, a novel mechanism for modality fusion and the adoption of a encoder-decoder architecture. Different to previous MMSCD works that extract speaker embeddings from extremely short audio segments, aligned to a single word, we use a speaker embedding extracted from 1.5s. A transformer decoder layer further improves the performance of an encoder-only MMSCD model. The proposed model achieves state-of-the-art results among studies that report SCD performance and is also on par with recent work that combines SCD with automatic speech recognition via human transcription",
    "checked": true,
    "id": "dbed2629d3b58ecf889189101b41955af7db6876",
    "semantic_title": "encoder-decoder multimodal speaker change detection",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nam23_interspeech.html": {
    "title": "Disentangled Representation Learning for Multilingual Speaker Recognition",
    "volume": "main",
    "abstract": "The goal of this paper is to learn robust speaker representation for bilingual speaking scenario. The majority of the world's population speak at least two languages; however, most speaker recognition systems fail to recognise the same speaker when speaking in different languages. Popular speaker recognition evaluation sets do not consider the bilingual scenario, making it difficult to analyse the effect of bilingual speakers on speaker recognition performance. In this paper, we publish a large-scale evaluation set named VoxCeleb1-B derived from VoxCeleb that considers bilingual scenarios. We introduce an effective disentanglement learning strategy that combines adversarial and metric learning-based methods. This approach addresses the bilingual situation by disentangling language-related information from speaker representation while ensuring stable speaker representation learning. Our language-disentangled learning method only uses language pseudo-labels without manual information",
    "checked": true,
    "id": "80fc2ee9bbc80c5b479a262d06a3eada6d76d879",
    "semantic_title": "disentangled representation learning for multilingual speaker recognition",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jia23b_interspeech.html": {
    "title": "A Compact End-to-End Model with Local and Global Context for Spoken Language Identification",
    "volume": "main",
    "abstract": "We introduce TitaNet-LID, a compact end-to-end neural network for Spoken Language Identification (LID) that is based on the ContextNet architecture. TitaNet-LID employs 1D depth-wise separable convolutions and Squeeze-and-Excitation layers to effectively capture local and global context within an utterance. Despite its small size, TitaNet-LID achieves performance similar to state-of-the-art models on the VoxLingua107 dataset while being 10 times smaller. Furthermore, it can be easily adapted to new acoustic conditions and unseen languages through simple fine-tuning, achieving a state-of-the-art accuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can achieve a better trade-off between accuracy and speed. TitaNet-LID performs well even on short utterances less than 5s in length, indicating its robustness to input length",
    "checked": true,
    "id": "c1afe3805248362523fea9a79378a5715254168a",
    "semantic_title": "a compact end-to-end model with local and global context for spoken language identification",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sullivan23_interspeech.html": {
    "title": "On the Robustness of Arabic Speech Dialect Identification",
    "volume": "main",
    "abstract": "Arabic dialect identification (ADI) tools are an important part of the large-scale data collection pipelines necessary for training speech recognition models. As these pipelines require application of ADI tools to potentially out-of-domain data, we aim to investigate how vulnerable the tools may be to this domain shift. With self-supervised learning (SSL) models as a starting point, we evaluate transfer learning and direct classification from SSL features. We undertake our evaluation under rich conditions, with a goal to develop ADI systems from pretrained models and ultimately evaluate performance on newly collected data. In order to understand what factors contribute to model decisions, we carry out a careful human study of a subset of our data. Our analysis confirms that domain shift is a major challenge for ADI models. We also find that while self-training does alleviate this challenges, it may be insufficient for realistic conditions",
    "checked": true,
    "id": "2a63cdcfc054d7601085565466a0aa7d32a210ee",
    "semantic_title": "on the robustness of arabic speech dialect identification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23u_interspeech.html": {
    "title": "Adaptive Neural Network Quantization For Lightweight Speaker Verification",
    "volume": "main",
    "abstract": "Recently, speaker verification systems benefit from deep neural networks and the size of speaker embedding encoder increases with these sophisticated architectures. Nevertheless, mobile devices have inadequate memory for oversized embedding extractors, thus demanding compact networks. In this paper, we explore neural network quantization for model compression. Specifically, we first propose a novel uniform quantization method based on K-Means clustering. Then, to further improve the small model performance, mixed precision quantization is introduced. Besides, we implement a multi-stage fine-tuning (MSFT) recipe to boost the accuracy of mixed-precision model. In experiments, the performance degradation of 4 bit quantized ResNet34 is negligible. Our quantized models outperform former model compression methods in terms of size and accuracy. In addition, mixed-precision quantization with MSFT strategy further improves the model performance",
    "checked": true,
    "id": "13e3fcf23daa3c960b6ae1f12f5c0bd6670180cd",
    "semantic_title": "adaptive neural network quantization for lightweight speaker verification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/su23_interspeech.html": {
    "title": "Adversarial Diffusion Probability Model For Cross-domain Speaker Verification Integrating Contrastive Loss",
    "volume": "main",
    "abstract": "In speaker verification, performance degradation caused by domain mismatch has been a common problem as the test domain lies outside the training distribution. In this paper, we present a novel domain transfer network called Adversarial Diffusion Probabilistic Model (ADPM), to better alleviate this problem. More specifically, ADPM is used to transfer melspectrogram from the source domain into the target domain. To generate the melspectrogram, we propose to regard the diffusion model as the generator and a discriminator is employed for adversarial training. We also explore the contrastive learning objective to retain the context information of source domain. The generated and the original feature maps from the source domain are fed into the ResNet34 network jointly to construct cross-domain speaker verification. We evaluate the proposed techniques on VOiCES dataset, and our best model achieves a relative 8.94% Equal Error Rate (EER) drop compared to the previous adaption methods",
    "checked": true,
    "id": "1f6db463127319c26f576352f6ba417214324a19",
    "semantic_title": "adversarial diffusion probability model for cross-domain speaker verification integrating contrastive loss",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ito23_interspeech.html": {
    "title": "Spoofing Attacker Also Benefits from Self-Supervised Pretrained Model",
    "volume": "main",
    "abstract": "Large-scale pretrained models using self-supervised learning have reportedly improved the performance of speech anti-spoofing. However, the attacker side may also make use of such models. Also, since it is very expensive to train such models from scratch, pretrained models on the Internet are often used, but the attacker and defender may possibly use the same pretrained model. This paper investigates whether the improvement in anti-spoofing with pretrained models holds under the condition that the models are available to attackers. As the attacker, we train a model that enhances spoofed utterances so that the speaker embedding extractor based on the pretrained models cannot distinguish between bona fide and spoofed utterances. Experimental results show that the gains the anti-spoofing models obtained by using the pretrained models almost disappear if the attacker also makes use of the pretrained models",
    "checked": true,
    "id": "476541e465a12c11d4c9e8e4bd8dfed408b98bbf",
    "semantic_title": "spoofing attacker also benefits from self-supervised pretrained model",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vashishth23_interspeech.html": {
    "title": "Label Aware Speech Representation Learning For Language Identification",
    "volume": "main",
    "abstract": "The speech representation learning approaches, for non-semantic tasks such as language recognition, have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approach using raw data. In this paper, we propose a novel framework of combining the self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation learning (LASR), uses a triplet based objective function to incorporate the language labels along with the self-supervised loss function. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that LASR framework improves over the SOTA systems in terms of recognition performance. We also report an analysis of the robustness of the LASR approach to noisy/missing labels as well as for ASR task",
    "checked": true,
    "id": "b186ede7bdf82545240077ba455afbb4dc0edf27",
    "semantic_title": "label aware speech representation learning for language identification",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23c_interspeech.html": {
    "title": "Exploring the Impact of Back-End Network on Wav2vec 2.0 for Dialect Identification",
    "volume": "main",
    "abstract": "This paper explores the wav2vec 2.0 model for dialect identi-fication, focusing on the impact of the back-end network dur-ing fine-tuning. Prior research has typically used wav2vec 2.0 as a frame-level feature extractor, followed by a simple back-end consist-ing of a pooling layer and a fully connected layer. In contrast, we employ multi-scale aggregation and a graph neural net-work to design a more sophisticated back-end that implicitly exploit phoneme sequence information and significantly im-proves system performance. We evaluate our system on the dialect identification task of the Oriental Language Recogni-tion Challenge 2020 (AP20-OLR). Experimental results demonstrate that our system outperforms the state-of-the-art baseline by a relative reduction of 50% in average cost per-formance (Cavg). We also verify the effectiveness of our pro-posed back-end network, which results in a relative reduction of 54% in Cavg. Our findings highlight the importance of incorporating a more effective back-end network for improved dialect identification performance when using the wav2vec 2.0 model",
    "checked": true,
    "id": "a596308460f74edcde4b3b03add4c97fea096a9e",
    "semantic_title": "exploring the impact of back-end network on wav2vec 2.0 for dialect identification",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23_interspeech.html": {
    "title": "Improving Speaker Verification with Self-Pretrained Transformer Models",
    "volume": "main",
    "abstract": "Recently, fine-tuning large pre-trained Transformer models using downstream datasets has received a rising interest. Despite their success, it is still challenging to disentangle the benefits of large-scale datasets and Transformer structures from the limitations of the pre-training. In this paper, we introduce a hierarchical training approach, named self-pretraining, in which Transformer models are pretrained and finetuned on the same dataset. Three pre-trained models including HuBERT, Conformer and WavLM are evaluated on four different speaker verification datasets with varying sizes. Our experiments show that these self-pretrained models achieve competitive performance on downstream speaker verification tasks with only one-third of the data compared to Librispeech pretraining, such as VoxCeleb1 and CNCeleb1. Furthermore, when pre-training only on the VoxCeleb2-dev, the Conformer model outperforms the one pre-trained on 94k hours of data using the same fine-tuning settings",
    "checked": true,
    "id": "f60e9a09c3a7e3b81b19144503973c526d414064",
    "semantic_title": "improving speaker verification with self-pretrained transformer models",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ribeiro23_interspeech.html": {
    "title": "Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches",
    "volume": "main",
    "abstract": "Wake word detection exists in most intelligent homes and portable devices. It offers these devices the ability to \"wake up\" when summoned at a low cost of power and computing. This paper focuses on understanding alignment's role in developing a wake-word system that answers a generic phrase. We discuss three approaches. The first is alignment-based, where the model is trained with frame-wise cross-entropy. The second is alignment-free, where the model is trained with CTC. The third, proposed by us, is a hybrid solution in which the model is trained with a small set of aligned data and then tuned with a sizeable unaligned dataset. We compare the three approaches and evaluate the impact of the different aligned-to-unaligned ratios for hybrid training. Our results show that the alignment-free system performs better than the alignment-based for the target operating point, and with a small fraction of the data (20%), we can train a model that complies with our initial constraints",
    "checked": true,
    "id": "5facd18f29698735c621a0acf674499ab22ba992",
    "semantic_title": "handling the alignment for wake word detection: a comparison between alignment-based, alignment-free and hybrid approaches",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/linke23_interspeech.html": {
    "title": "What do self-supervised speech representations encode? An analysis of languages, varieties, speaking styles and speakers",
    "volume": "main",
    "abstract": "Automatic speech recognition systems based on self-supervised learning yield excellent performance for read, but not so for conversational speech. This paper contributes insights into how corpora from different languages and speaking styles are encoded in shared discrete speech representations (based on wav2vec2 XLSR). We analyze codebook entries of data from two languages from different language families (i.e., German and Hungarian), of data from different varieties from the same language (i.e., German and Austrian German) and of data from different speaking styles (read and conversational speech). We find that -- as expected -- the two languages are clearly separable. With respect to speaking style, conversational Austrian German has the highest similarity with a corpus of similar spontaneity from a different German variety, and speakers differ more among themselves when using different speaking styles than from other speakers of a different region when using the same speaking style",
    "checked": true,
    "id": "238a2da50587d155b0e8b4ebf261766a588fab55",
    "semantic_title": "what do self-supervised speech representations encode? an analysis of languages, varieties, speaking styles and speakers",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ca_interspeech.html": {
    "title": "A Compressed Synthetic Speech Detection Method with Compression Feature Embedding",
    "volume": "main",
    "abstract": "With the development of deep fake technology, synthetic speech is created easier by forgery techniques based on text-to-speech and voice conversion, which poses a challenge to automatic speaker verification systems. Existing methods demonstrate excellent performance on public databases, but most methods are weak in detecting compressed speech commonly used in social networks, such as MP3 and AAC. We believe that if the classifier has compressed information as a priori knowledge, it will help the classifier make a more accurate decision when detecting compressed speech. To solve this issue, a multi-branch residual network with a compression feature embedding module is proposed in this paper. The feature embedding module is used to integrate the authenticity feature and compression feature. Our method is evaluated on the ASVspoof database and experimental results show the effectiveness of the proposed method for detecting compressed speech",
    "checked": true,
    "id": "5d19153ada2dd94660f8291cf716919787d553b0",
    "semantic_title": "a compressed synthetic speech detection method with compression feature embedding",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23j_interspeech.html": {
    "title": "Outlier-aware Inlier Modeling and Multi-scale Scoring for Anomalous Sound Detection via Multitask Learning",
    "volume": "main",
    "abstract": "This paper proposes an approach for anomalous sound detection that incorporates outlier exposure and inlier modeling within a unified framework by multitask learning. While outlier exposure-based methods can extract features efficiently, it is not robust. Inlier modeling is good at generating robust features, but the features are not very effective. Recently, serial approaches are proposed to combine these two methods, but it still requires a separate training step for normal data modeling. To overcome these limitations, we use multitask learning to train a conformer-based encoder for outlier-aware inlier modeling. Moreover, our approach provides multi-scale scores for detecting anomalies. Experimental results on the MIMII and DCASE 2020 task 2 datasets show that our approach outperforms state-of-the-art single-model systems and achieves comparable results with top-ranked multi-system ensembles",
    "checked": true,
    "id": "94225a6422b10d5b51ed86cac852a853932c9515",
    "semantic_title": "outlier-aware inlier modeling and multi-scale scoring for anomalous sound detection via multitask learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23c_interspeech.html": {
    "title": "MOSLight: A Lightweight Data-Efficient System for Non-Intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "Automatically predicting the mean opinion score (MOS) of a synthesized speech without the reference signal with deep learning systems has been studied extensively recently and shown great results. However, previous best systems are mostly based on self-supervised learned (SSL) models consisting of up to hundreds of millions of parameters making them unsuitable for mobile or embedded applications. In this paper, we propose MOSLight, a non-SSL-based lightweight yet powerful system for MOS prediction. We argue that 2D convolutions are inefficient for audio feature processing and not ideal for tasks where training data are scarce. To build MOSLight, we utilized depthwise separable dilated 1D convolutions and incorporated multi-task learning and non-strict frame-level score clipping. We conducted experiments on the Voice Conversion Challenge 2018 (VCC2018) and BVCC. Results show MOSLight achieves great effectiveness despite being a lightweight model trained with limited training data",
    "checked": true,
    "id": "874f352be2adb965a4d5c10c07e38712ddf0bc65",
    "semantic_title": "moslight: a lightweight data-efficient system for non-intrusive speech quality assessment",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23c_interspeech.html": {
    "title": "A Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music Generation",
    "volume": "main",
    "abstract": "Recently, multi-instrument music generation has become a hot topic. Different from single-instrument generation, multi-instrument generation needs to consider inter-track harmony besides intra-track coherence. This is usually achieved by composing note segments from different instruments into a signal sequence. This composition could be on different scales, such as note, bar, or track. Most existing work focuses on a particular scale, leading to shortage in modeling music with diverse temporal and track dependencies. This paper proposes a multi-scale attentive Transformer model to improve the quality of multi-instrument generation. We first employ multiple Transformer encoders to learn multi-instrument representations of different scales and then design an attentive mechanism to fuse the multi-scale information. Experiments conducted on SOD and LMD datasets show that our model improves both quantitative and qualitative performance compared to models based on single-scale information. The source code and some generated samples can be found at https://github.com/HaRryqaq/MSAT",
    "checked": true,
    "id": "17267220d56415ebb5fb495531f3c9f52247250f",
    "semantic_title": "a multi-scale attentive transformer for multi-instrument symbolic music generation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23i_interspeech.html": {
    "title": "MTANet: Multi-band Time-frequency Attention Network for Singing Melody Extraction from Polyphonic Music",
    "volume": "main",
    "abstract": "Singing melody extraction is an important task in music information retrieval. In this paper, we propose a multi-band time-frequency attention network (MTANet) for singing melody extraction from polyphonic music, which can generate the feature representation to characterize the fundamental frequency (F0) component. Moreover, a band partition scheme is proposed to fit the position distribution of the F0 component. Further, three hourglass sub-networks are used to capture various multi-band features. Then, a feature fusion module (FFM) is employed to fuse the multi-band features. Visualization analysis shows that the multi-band feature extraction branch can generate the feature representation for characterizing the F0 component effectively. Experimental results show that the MTANet outperforms the existing state-of-the-art methods, while keeping with fewer network parameters. Visualized results intuitively show that the MTANet can reduce the octave and melody detection errors",
    "checked": true,
    "id": "e153ddb08bed89b7ff79e49b6ee502da74d6ebcd",
    "semantic_title": "mtanet: multi-band time-frequency attention network for singing melody extraction from polyphonic music",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chunhui23_interspeech.html": {
    "title": "Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on Generative Adversarial Network",
    "volume": "main",
    "abstract": "The over-smoothing problem in the middle- and high-frequency areas prevents the acoustic model from generating high-quality singing voices. In this paper, we propose XiaoiceSing2, which is a generative adversarial network consisting of a FastSpeech-based generator and a multi-band discriminator, to generate the full-band mel-spectrogram. Specifically, we improve the feed-forward Transformer (FFT) block by adding multiple residual convolutional blocks in parallel with the self-attention block to balance the local and global features. The multi-band discriminator contains three sub-discriminators responsible for low-, middle-, and high-frequency parts of the mel-spectrogram, respectively. Each sub-discriminator is composed of several segment discriminators (SD) and detail discriminators (DD) to distinguish the audio from different aspects. The experiment on our internal 48kHz singing voice dataset shows XiaoiceSing2 significantly improves the quality of the singing voice over XiaoiceSing",
    "checked": true,
    "id": "b86e89717524b7652535f6995e486d4349964c25",
    "semantic_title": "xiaoicesing 2: a high-fidelity singing voice synthesizer based on generative adversarial network",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/solanki23_interspeech.html": {
    "title": "Do Vocal Breath Sounds Encode Gender Cues for Automatic Gender Classification?",
    "volume": "main",
    "abstract": "The acoustic features of continuous speech, such as pitch (F0) and formant frequencies (F1, F2) have been utilized for gender classification. However, non-speech signals including vocal breath sounds have not been explored due to the absence of gender-specific acoustic features. This study investigates if vocal breath sounds carry gender information and if they can be used for automatic gender classification. The study examines the use of data-driven and knowledge-based features from breath sounds, classifier complexity, and the importance of breath signal segment location and duration. Results from experiments on 54 minutes of male and 52 minutes of female breath sounds demonstrate that classifiers with low-complexity and knowledge-based features (MFCC statistics) perform similarly to high-complexity classifiers with data-driven features. Breath segments of around 3 seconds are found to be the most suitable choice regardless of location, eliminating the need for breath cycle boundary marking",
    "checked": true,
    "id": "de4cc147e85a410d2ec46698e88b8434b9c19f4d",
    "semantic_title": "do vocal breath sounds encode gender cues for automatic gender classification?",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sugiura23_interspeech.html": {
    "title": "Automatic Exploration of Optimal Data Processing Operations for Sound Data Augmentation Using Improved Differentiable Automatic Data Augmentation",
    "volume": "main",
    "abstract": "Data augmentation is one of the methods used to robustly train machine learning models with a small dataset. This method randomly applies pre-defined data processing operations to input data, regardless of the characteristics of the input data. However, some data processing operations may be inappropriate for certain data. In this study, we propose a new method to automatically search for the best data processing operations for each sound file to be input into a sound classification neural network. The proposed method is an improvement on the previously proposed differentiable automatic data augmentation (DADA), which uses a differentiable neural network to select the optimal data processing operations. We evaluated our proposed method on an acoustic scene classification task on the ESC-50 dataset and demonstrated that the proposed method can train a more robust model compared to the original DADA-based data augmentation",
    "checked": true,
    "id": "26a9f394b2aca2832899ace632ab13e69bb9eb47",
    "semantic_title": "automatic exploration of optimal data processing operations for sound data augmentation using improved differentiable automatic data augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23b_interspeech.html": {
    "title": "A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis",
    "volume": "main",
    "abstract": "Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS) is a chronic breathing disorder caused by a blockage in the upper airways. Snoring is a prominent symptom of OSAHS, and previous studies have attempted to identify the obstruction site of the upper airways by snoring sounds. Despite some progress, the classification of the obstruction site remains challenging in real-world clinical settings due to the influence of sleep body position on upper airways. To address this challenge, this paper proposes a snore-based sleep body position recognition dataset (SSBPR) consisting of 7570 snoring recordings, which comprises six distinct labels for sleep body position: supine, supine but left lateral head, supine but right lateral head, left-side lying, right-side lying and prone. Experimental results show that snoring sounds exhibit certain acoustic features that enable their effective utilization for identifying body posture during sleep in real-world scenarios",
    "checked": true,
    "id": "da41539cf396cd15ff3834e6a8d24f96eff65673",
    "semantic_title": "a snoring sound dataset for body position recognition: collection, annotation, and analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23b_interspeech.html": {
    "title": "RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music",
    "volume": "main",
    "abstract": "Vocal pitch is an important high-level feature in music audio processing. However, extracting vocal pitch in polyphonic music is more challenging due to the presence of accompaniment. To eliminate the influence of the accompaniment, most previous methods adopt music source separation models to obtain clean vocals from polyphonic music before predicting vocal pitches. As a result, the performance of vocal pitch estimation is affected by the music source separation models. To address this issue and directly extract vocal pitches from polyphonic music, we propose a robust model named RMVPE. This model can extract effective hidden features and accurately predict vocal pitches from polyphonic music. The experimental results demonstrate the superiority of RMVPE in terms of raw pitch accuracy (RPA) and raw chroma accuracy (RCA). Additionally, experiments conducted with different types of noise show that RMVPE is robust across all signal-to-noise ratio (SNR) levels",
    "checked": true,
    "id": "32502c723e183f3fbd6af920b4bc495fed3b0cf9",
    "semantic_title": "rmvpe: a robust model for vocal pitch estimation in polyphonic music",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/manocha23_interspeech.html": {
    "title": "Spatialization Quality Metric for Binaural Speech",
    "volume": "main",
    "abstract": "In spatial-audio enabled systems, evaluating the quality of spatialization is an essential process. This paper proposes a new objective metric to measure the spatialization quality (SQ) between any pair of binaural signals while being agnostic to speech content and signal duration. We formulate SQ as a metric learning problem and compute deep-feature distance on embeddings learned using triplet loss and multi-task learning with direction-of-arrival and binaural speech synthesis as auxiliary tasks. We show the robustness of our model on localization in (un)seen contexts, monotonicity with increasing angular distance, content in-variance and retrieval performance. Experiments show that our metric correlates well with publicly available subjective ratings, and it yields improvements when used as a differentiable loss in a binaural speech enhancement system",
    "checked": true,
    "id": "90aa94d756fe8146c2727a02ecb500e8f8f3e84b",
    "semantic_title": "spatialization quality metric for binaural speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/roy23_interspeech.html": {
    "title": "AsthmaSCELNet: A Lightweight Supervised Contrastive Embedding Learning Framework for Asthma Classification Using Lung Sounds",
    "volume": "main",
    "abstract": "Asthma is one of the most prevalent respiratory disorders, which can be identified by different modalities such as speech, wheezing of lung sounds (LSs), spirometric measures, etc. In this paper, we propose AsthmaSCELNet, a lightweight supervised contrastive embedding learning framework, to classify asthmatic LSs by providing adequate classification margin across the embeddings of healthy and asthma LS, in contrast to vanilla supervised learning. Our proposed framework consists of three steps: pre-processing, melspectrogram extraction, and classification. The AsthmaSCELNet consists of two stages: embedding learning using a lightweight embedding extraction backbone module that extracts compact embedding from the melspectrogram, and classification by the learnt embeddings using multi-layer perceptrons. The proposed framework achieves an accuracy, sensitivity, and specificity of 98.54%, 98.27%, and 98.73% respectively, that outperforms existing methods based on LSs and other modalities",
    "checked": true,
    "id": "4ef24621edc89eba7f21377d300db7c1b6a82fc2",
    "semantic_title": "asthmascelnet: a lightweight supervised contrastive embedding learning framework for asthma classification using lung sounds",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bae23b_interspeech.html": {
    "title": "Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification",
    "volume": "main",
    "abstract": "Respiratory sound contains crucial information for the early diagnosis of fatal lung diseases. Since the COVID-19 pandemic, there has been a growing interest in contact-free medical care based on electronic stethoscopes. To this end, cutting-edge deep learning models have been developed to diagnose lung diseases; however, it is still challenging due to the scarcity of medical data. In this study, we demonstrate that the pretrained model on large-scale visual and audio datasets can be generalized to the respiratory sound classification task. In addition, we introduce a straightforward Patch-Mix augmentation, which randomly mixes patches between different samples, with Audio Spectrogram Transformer (AST). We further propose a novel and effective Patch-Mix Contrastive Learning to distinguish the mixed representations in the latent space. Our method achieves state-of-the-art performance on the ICBHI dataset, outperforming the prior leading score by an improvement of 4.08%",
    "checked": true,
    "id": "e5b258148159e96d759e28764675e549b71014dc",
    "semantic_title": "patch-mix contrastive learning with audio spectrogram transformer on respiratory sound classification",
    "citation_count": 3,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/richter23_interspeech.html": {
    "title": "Remote Assessment for ALS using Multimodal Dialog Agents: Data Quality, Feasibility and Task Compliance",
    "volume": "main",
    "abstract": "We investigate the feasibility, task compliance and audiovisual data quality of a multimodal dialog-based solution for remote assessment of Amyotrophic Lateral Sclerosis (ALS). 53 people with ALS and 52 healthy controls interacted with Tina, a cloud-based conversational agent, in performing speech tasks designed to probe various aspects of motor speech function while their audio and video was recorded. We rated a total of 250 recordings for audio/video quality and participant task compliance, along with the relative frequency of different issues observed. We observed excellent compliance (98%) and audio (95.2%) and visual quality rates (84.8%), resulting in an overall yield of 80.8% recordings that were both compliant and of high quality. Furthermore, recording quality and compliance were not affected by level of speech severity and did not differ significantly across end devices. These findings support the utility of dialog systems for remote monitoring of speech in ALS",
    "checked": true,
    "id": "d0a968746a3e47f82312f476d9bba9bbff236555",
    "semantic_title": "remote assessment for als using multimodal dialog agents: data quality, feasibility and task compliance",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yariv23_interspeech.html": {
    "title": "Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation",
    "volume": "main",
    "abstract": "In recent years, image generation has shown a great leap in performance, where diffusion models play a central role. Although generating high-quality images, such models are mainly conditioned on textual descriptions. This begs the question: how can we adopt such models to be conditioned on other modalities? In this paper, we propose a novel method utilizing latent diffusion models, trained for text-to-image-generation, to generate images, conditioned on audio recordings. Using a pre-trained audio encoding model, the proposed method encodes audio into a new token, which can be considered as an adaptation layer between the audio and text representations. Such a modeling paradigm requires a small number of trainable parameters, making the proposed approach appealing for lightweight optimization. Results suggest the proposed method is superior to the evaluated baseline methods, considering both objective and subjective metrics. Code and samples will be publicly available",
    "checked": true,
    "id": "3e3d3c96a409f4d84793ae2d625c66581b685329",
    "semantic_title": "adaptation of text-conditioned diffusion models for audio-to-image generation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/romero23_interspeech.html": {
    "title": "Obstructive sleep apnea screening with breathing sounds and respiratory effort: a multimodal deep learning approach",
    "volume": "main",
    "abstract": "Obstructive sleep apnea (OSA) is a chronic and prevalent condition with well-established comorbidities. Due to limited diagnostic resources and high cost, a significant OSA population lives undiagnosed, and accurate and low-cost methods to screen for OSA are needed. We propose a novel screening method based on breathing sounds recorded with a smartphone and respiratory effort. Whole night recordings are divided into 30-s segments, each of which is classified for the presence or absence of OSA events by a multimodal deep neural network. Data fusion techniques were investigated and evaluated based on the apnea-hypopnea index estimated from whole night recordings. Real-world recordings made during home sleep apnea testing from 103 participants were used to develop and evaluate the proposed system. The late fusion system achieved the best sensitivity and specificity when screening for severe OSA, at 0.93 and 0.92, respectively. This offers the prospect of inexpensive OSA screening at home",
    "checked": true,
    "id": "4966dc1a959be735d18095f04e8c89b884aed887",
    "semantic_title": "obstructive sleep apnea screening with breathing sounds and respiratory effort: a multimodal deep learning approach",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23f_interspeech.html": {
    "title": "Investigation of Music Emotion Recognition Based on Segmented Semi-Supervised Learning",
    "volume": "main",
    "abstract": "The production and annotation of music datasets requires very specialized background knowledge, which is difficult for most people to complete. Therefore, the number of annotated music samples is at a premium for Music Information Retrieval (MIR) tasks. Recently, segment-based methods for emotion-related tasks have been proposed, which train backbone networks on shorter segments instead of entire audio clips, thereby naturally augmenting training samples without requiring additional resources. However, when training at the segment level, segment labels are the major problem. The most commonly used method is that segment inherits the label of the clip containing it, but as we all know, music emotion is not constant during the whole clip. Doing so will introduce label noise and make the training overfit easily. To handle the noisy label issue, we propose a semi-supervised self-learning method and achieve better results than previous methods",
    "checked": true,
    "id": "049cdbb80b358a90a3d05247da483d43fd08bf6d",
    "semantic_title": "investigation of music emotion recognition based on segmented semi-supervised learning",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23c_interspeech.html": {
    "title": "The Effects of Input Type and Pronunciation Dictionary Usage in Transfer Learning for Low-Resource Text-to-Speech",
    "volume": "main",
    "abstract": "We compare phone labels and articulatory features as input for cross-lingual transfer learning in text-to-speech (TTS) for low-resource languages (LRLs). Experiments with FastSpeech 2 and the LRL West Frisian show that using articulatory features outperformed using phone labels in both intelligibility and naturalness. For LRLs without pronunciation dictionaries, we propose two novel approaches: a) using a massively multilingual model to convert grapheme-to-phone (G2P) in both training and synthesizing, and b) using a universal phone recognizer to create a makeshift dictionary. Results show that the G2P approach performs largely on par with using a ground-truth dictionary and the phone recognition approach, while performing generally worse, remains a viable option for LRLs less suitable for the G2P approach. Within each approach, using articulatory features as input outperforms using phone labels",
    "checked": true,
    "id": "702d836f47929d23af4f17a81983fcc60b6e24b2",
    "semantic_title": "the effects of input type and pronunciation dictionary usage in transfer learning for low-resource text-to-speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23d_interspeech.html": {
    "title": "Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages",
    "volume": "main",
    "abstract": "We train a MOS prediction model based on wav2vec 2.0 using the open-access data sets BVCC and SOMOS. Our test with neural TTS data in the low-resource language (LRL) West Frisian shows that pre-training on BVCC before fine-tuning on SOMOS leads to the best accuracy for both fine-tuned and zero-shot prediction. Further fine-tuning experiments show that using more than 30 percent of the total data does not lead to significant improvements. In addition, fine-tuning with data from a single listener shows promising system-level accuracy, supporting the viability of one-participant pilot tests. These findings can all assist the resource-conscious development of TTS for LRLs by progressing towards better zero-shot MOS prediction and informing the design of listening tests, especially in early-stage evaluation",
    "checked": true,
    "id": "bd10a1d72644f12c050d39584185a9a8f8ce5710",
    "semantic_title": "resource-efficient fine-tuning strategies for automatic mos prediction in text-to-speech for low-resource languages",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23b_interspeech.html": {
    "title": "Robust Feature Decoupling in Voice Conversion by Using Locality-Based Instance Normalization",
    "volume": "main",
    "abstract": "Extensive style transfer methods have shown that instance normalization (IN) is a simple yet effective way to remove style information. However, few studies have focused on whether these channel-wise feature statistics, such as mean and standard deviation (std) are consistent locally and globally, which ultimately leads to insufficient feature decoupling. In this paper, we first propose locality-based instance normalization (LoIN) to impose statistical feature consistency constraints on latent feature maps. LoIN performs normalization using local feature statistics which are calculated on randomly selected frames rather than the entire set of frames used in the training phase. In particular, LoIN is lightweight, less computationally intensive, and transferable to any IN-driven VC method. Experimental results show the superiority of LoIN in disentanglement and transfer performance and show improvement in both speaker similarity and content consistency",
    "checked": true,
    "id": "581b9a82683b8a7b9e727a885c167b23c6d9db3f",
    "semantic_title": "robust feature decoupling in voice conversion by using locality-based instance normalization",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jia23_interspeech.html": {
    "title": "Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network",
    "volume": "main",
    "abstract": "The goal of accent conversion (AC) is to convert the accent of speech into the target accent while preserving the content and speaker identity. AC enables a variety of applications, such as language learning, speech content creation, and data augmentation. Previous methods rely on reference utterances in the inference phase or are unable to preserve speaker identity. To address these issues, we propose a zero-shot reference-free accent conversion method, which is able to convert unseen speakers' utterances into a target accent. Pseudo Siamese Disentanglement Network (PSDN) is proposed to disentangle the accent from the content representation. Experimental results show that our model generates speech samples with much higher accentedness than the input and comparable naturalness, on two-way conversion including foreign-to-native and native-to-foreign",
    "checked": true,
    "id": "83a6aaacc2dde530284189dd9cddde8c0c472556",
    "semantic_title": "zero-shot accent conversion using pseudo siamese disentanglement network",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ekstedt23_interspeech.html": {
    "title": "Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis",
    "volume": "main",
    "abstract": "Turn-taking is a fundamental aspect of human communication where speakers convey their intention to either hold, or yield, their turn through prosodic cues. Using the recently proposed Voice Activity Projection model, we propose an automatic evaluation approach to measure these aspects for conversational speech synthesis. We investigate the ability of three commercial, and two open-source, Text-To-Speech (TTS) systems ability to generate turn-taking cues over simulated turns. By varying the stimuli, or controlling the prosody, we analyze the models performances. We show that while commercial TTS largely provide appropriate cues, they often produce ambiguous signals, and that further improvements are possible. TTS, trained on read or spontaneous speech, produce strong turn-hold but weak turn-yield cues. We argue that this approach, that focus on functional aspects of interaction, provides a useful addition to other important speech metrics, such as intelligibility and naturalness",
    "checked": true,
    "id": "73fc08c5e023f7f7702a94817301f72fd074e88a",
    "semantic_title": "automatic evaluation of turn-taking cues in conversational speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cong23_interspeech.html": {
    "title": "GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "Cross-lingual timbre and style generalizable text-to-speech (TTS) aims to synthesize speech with a specific reference tim- bre or style that is never trained in the target language. It encounters the following challenges: 1) timbre and pronun- ciation are correlated since multilingual speech of a specific speaker is usually hard to obtain; 2) style and pronunciation are mixed because the speech style contains language-agnostic and language-specific parts. To address these challenges, we pro- pose GenerTTS, which mainly includes the following works: 1) we elaborately design a HuBERT-based information bottleneck to disentangle timbre and pronunciation/style; 2) we minimize the mutual information between style and language to discard the language-specific information in the style embedding. The experiments indicate that GenerTTS outperforms baseline sys- tems in terms of style similarity and pronunciation accuracy, and enables cross-lingual timbre and style generalization",
    "checked": true,
    "id": "d4c76b8f559abd5921512361475be156ff9deba1",
    "semantic_title": "genertts: pronunciation disentanglement for timbre and style generalization in cross-lingual text-to-speech",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yasuda23_interspeech.html": {
    "title": "Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech Based on Tail Probabilities",
    "volume": "main",
    "abstract": "Subjective evaluations such as mean opinion scores (MOS) are essential for evaluations of synthetic speech including automatic speech quality assessment (SQA) models. In this paper, we evaluate the confidence intervals of MOS in a listening test and the number of required samples to achieve a certain confidence interval based on various tail probability evaluation methods. The tail probability is a probability representing the sample mean deviates greatly from the true mean. We use tail probability evaluations based on asymptotic and upper-bound-based approaches. In our experiments about toy data and actual listening test data, we show that achieving small confidence intervals requires huge sample volumes, and the MOS corpus for SQA has large confidence intervals due to limited sample volumes. We suggest adopting comparative scoring and online learning for more reliable subjective evaluations under limited budgets as the future direction",
    "checked": true,
    "id": "58c3e8545493fd94a0be85776d6cdb8df1adbabc",
    "semantic_title": "analysis of mean opinion scores in subjective evaluation of synthetic speech based on tail probabilities",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koizumi23_interspeech.html": {
    "title": "LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus",
    "volume": "main",
    "abstract": "This paper introduces a new speech dataset called \"LibriTTS-R\" designed for text-to-speech (TTS) use. It is derived by applying speech restoration to the LibriTTS corpus, which consists of 585 hours of speech data at 24 kHz sampling rate from 2,456 speakers and the corresponding texts. The constituent samples of LibriTTS-R are identical to those of LibriTTS, with only the sound quality improved. Experimental results show that the LibriTTS-R ground-truth samples showed significantly improved sound quality compared to those in LibriTTS. In addition, neural end-to-end TTS trained with LibriTTS-R achieved speech naturalness on par with that of the ground-truth samples. The corpus is freely available for download from http://www.openslr.org/141/",
    "checked": true,
    "id": "eca57337cee8de0822c428bb65c3b93d1f9830e3",
    "semantic_title": "libritts-r: a restored multi-speaker text-to-speech corpus",
    "citation_count": 7,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mitsui23_interspeech.html": {
    "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
    "volume": "main",
    "abstract": "Talking face generation has been extensively investigated owing to its wide applicability. The two primary frameworks used for talking face generation comprise a text-driven framework, which generates synchronized speech and talking faces from text, and a speech-driven framework, which generates talking faces from speech. To integrate these frameworks, this paper proposes a unified facial landmark generator (UniFLG). The proposed system exploits end-to-end text-to-speech not only for synthesizing speech but also for extracting a series of latent representations that are common to text and speech, and feeds it to a landmark decoder to generate facial landmarks. We demonstrate that our system achieves higher naturalness in both speech synthesis and facial landmark generation compared to the state-of-the-art text-driven method. We further demonstrate that our system can generate facial landmarks from speech of speakers without facial video data or even speech data",
    "checked": true,
    "id": "3058b966bb57236192ef35044acb86513b21452e",
    "semantic_title": "uniflg: unified facial landmark generator from text or speech",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thenguyen23_interspeech.html": {
    "title": "XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech",
    "volume": "main",
    "abstract": "We present XPhoneBERT, the first multilingual model pre-trained to learn phoneme representations for the downstream text-to-speech (TTS) task. Our XPhoneBERT has the same model architecture as BERT-base, trained using the RoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100 languages and locales. Experimental results show that employing XPhoneBERT as an input phoneme encoder significantly boosts the performance of a strong neural TTS model in terms of naturalness and prosody and also helps produce fairly high-quality speech with limited training data. We publicly release our pre-trained XPhoneBERT with the hope that it would facilitate future research and downstream TTS applications for multiple languages",
    "checked": true,
    "id": "5f82d3c927cc2ca51f52197e321ace7be5320c7b",
    "semantic_title": "xphonebert: a pre-trained multilingual model for phoneme representations for text-to-speech",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kulkarni23_interspeech.html": {
    "title": "ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus",
    "volume": "main",
    "abstract": "We present a Classical Arabic Text-to-Speech (ClArTTS) corpus to facilitate the development of end-to-end TTS systems for the Arabic language. The speech is extracted from a LibriVox audiobook, which is then processed, segmented, and manually transcribed and annotated. The ClArTTS corpus contains about 12 hours of speech from a single male speaker sampled at 40100 Hz. In this paper, we describe the process of corpus creation, details of corpus statistics, and a comparison with existing resources. Furthermore, we develop two TTS systems based on Grad-TTS and Glow-TTS and illustrate the performance of the resulting systems via subjective and objective evaluations. The ClArTTS corpus is publicly available at www.clartts.com for research purposes, along with the baseline TTS systems and an interactive demo",
    "checked": true,
    "id": "e1d6c1a21dbea01385e32c2fdff284b07d3a263a",
    "semantic_title": "clartts: an open-source classical arabic text-to-speech corpus",
    "citation_count": 2,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deja23_interspeech.html": {
    "title": "Diffusion-based accent modelling in speech synthesis",
    "volume": "main",
    "abstract": "In this work, we introduce a diffusion-based text-to-speech (TTS) system for accent modelling. TTS systems have become a natural part of our surroundings. Nevertheless, because of the complexity of accent modelling, recent state-of-the-art solutions mainly focus on the most common variants of each language. In this work, we propose to address this issue with a newly proposed diffusion generative model (DDGM). We first show how we can adapt DDGMs to the problem of accent modelling. We evaluate and compare this approach with a recent state-of-the-art solution, showing its superiority in modelling six different English accents. On top of our TTS system, we introduce a novel accent conversion method, where using the saliency map technique, we remove source accent-related features and replace them with the target ones through the diffusion process. We show that with this approach, we can perform accent conversion without a need for any additional speech information such as phonemes or text",
    "checked": true,
    "id": "0b586c3a55b0992e092e10794f16cf9327f81482",
    "semantic_title": "diffusion-based accent modelling in speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeshpanov23_interspeech.html": {
    "title": "Multilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration",
    "volume": "main",
    "abstract": "This work aims to build a multilingual text-to-speech (TTS) synthesis system for ten lower-resourced Turkic languages: Azerbaijani, Bashkir, Kazakh, Kyrgyz, Sakha, Tatar, Turkish, Turkmen, Uyghur, and Uzbek. We specifically target the zero-shot learning scenario, where a TTS model trained using the data of one language is applied to synthesise speech for other, unseen languages. An end-to-end TTS system based on the Tacotron 2 architecture was trained using only the available data of the Kazakh language. To generate speech for the other Turkic languages, we first mapped the letters of the Turkic alphabets onto the symbols of the International Phonetic Alphabet (IPA), which were then converted to the Kazakh alphabet letters. To demonstrate the feasibility of the proposed approach, we evaluated the multilingual Turkic TTS model subjectively and obtained promising results. To enable replication of the experiments, we make our code and dataset publicly available in our GitHub repository",
    "checked": true,
    "id": "22812ef9c089322ea6a26d1c0711a415b552eb95",
    "semantic_title": "multilingual text-to-speech synthesis for turkic languages using transliteration",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23h_interspeech.html": {
    "title": "CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation",
    "volume": "main",
    "abstract": "Conversion from graphemes to phonemes is an essential component in Text-To-Speech systems, and in Chinese, one main challenge is polyphone disambiguation-to determine the pronunciation of characters with multiple pronunciations. In this task, the benchmark dataset Chinese Polyphone disambiguation with Pinyin (CPP) suffers from two main limitations: Firstly, it contains some wrong labels in contrast to the newest official dictionary. Secondly, it is imbalanced and hence models learned from it show a learning bias towards frequently-used pronunciations and polyphones. In this paper, we refine CPP and release a new dataset named CVTE-poly, containing 845254 samples, nearly ten times the size of CPP and is more balanced. Besides, we propose a comprehensive measurement for polyphone disambiguation task, against the data imbalance problem. Experiments show that our simple but flexible baseline trained on CVTE-poly outperforms existing models, which demonstrate the benefit of our dataset",
    "checked": true,
    "id": "a1a6150e53ea3ed93cb337fea8cbae3f3b6ac07d",
    "semantic_title": "cvte-poly: a new benchmark for chinese polyphone disambiguation",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23k_interspeech.html": {
    "title": "Improving Bilingual TTS Using Language And Phonology Embedding With Embedding Strength Modulator",
    "volume": "main",
    "abstract": "In most cases, bilingual TTS needs to handle three types of input scripts: first language only, second language only, and second language embedded in the first language. In the latter two situations, it is a big challenge to accurately model the pronunciation and intonation of the second language in different contexts without mutual interference. This paper builds a Mandarin-English TTS system to acquire more standard spoken English speech from a monolingual Chinese speaker. We introduce phonology embedding to capture the English differences between different phonology with embedding masks. An embedding strength modulator is specially designed to capture the dynamic strength of language and phonology. Experiments show that our approach can produce significantly more natural and standard spoken English speech of the monolingual Chinese speaker. From analysis, we find that suitable phonology control contributes to better performance in different scenarios",
    "checked": true,
    "id": "5a87cd59278ace6df86789bde2fb50a0dec3082f",
    "semantic_title": "improving bilingual tts using language and phonology embedding with embedding strength modulator",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23f_interspeech.html": {
    "title": "High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units",
    "volume": "main",
    "abstract": "The goal of Automatic Voice Over (AVO) is to generate speech in sync with a silent video given its text script. Recent AVO frameworks built upon text-to-speech synthesis (TTS) have shown impressive results. However, the current AVO learning objective of acoustic feature reconstruction brings in indirect supervision for inter-modal alignment learning, thus limiting the synchronization performance and synthetic speech quality. To this end, we propose a novel AVO method leveraging the learning objective of self-supervised discrete speech unit prediction, which not only provides more direct supervision for the alignment learning, but also alleviates the mismatch between the text-video context and acoustic features. Experimental results show that our proposed method achieves remarkable lip-speech synchronization and high speech quality by outperforming baselines in both objective and subjective evaluations. Code and speech samples are publicly available",
    "checked": true,
    "id": "f544dd76a720e89835e6d5285d9393b84ee37699",
    "semantic_title": "high-quality automatic voice over with accurate alignment: supervision through self-supervised discrete speech units",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23_interspeech.html": {
    "title": "PronScribe: Highly Accurate Multimodal Phonemic Transcription From Speech and Text",
    "volume": "main",
    "abstract": "We present PronScribe, a novel method for phonemic transcription from speech and text input based on careful fine-tuning and adaptation of a massive, multilingual, multimodal speech-text pretrained model. We show that our model is capable of phonemically transcribing pronunciations of full utterances with accurate word boundaries in a variety of languages covering diverse phonological phenomena, achieving phoneme error rates in the vicinity of 1-2% which is comparable to human transcribers. We show that PronScribe can effectively learn this task from relatively little training data, making it attractive even in low-resource settings. It learns from text and speech simultaneously in a coherent way, and is better than previous models using speech, text or both. Additionally, the model's good transfer learning characteristics in multilingual settings can effectively boost performance for lower-resourced languages",
    "checked": true,
    "id": "8f94dc05a0a8406fe938e257c02f44b4a5b470bc",
    "semantic_title": "pronscribe: highly accurate multimodal phonemic transcription from speech and text",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/swiatkowski23b_interspeech.html": {
    "title": "Expressive Machine Dubbing Through Phrase-level Cross-lingual Prosody Transfer",
    "volume": "main",
    "abstract": "Speech generation for machine dubbing adds complexity to conventional Text-To-Speech solutions as the generated output is required to match the expressiveness, emotion and speaking rate of the source content. Capturing and transferring details and variations in prosody is a challenge. We introduce phrase-level cross-lingual prosody transfer for expressive multi-lingual machine dubbing. The proposed phrase-level prosody transfer delivers a significant 6.2% MUSHRA score increase over a baseline with utterance-level global prosody transfer, thereby closing the gap between the baseline and expressive human dubbing by 23.2%, while preserving intelligibility of the synthesised speech",
    "checked": true,
    "id": "b845efae88fcab7213081ebd17f8368198e91c5c",
    "semantic_title": "expressive machine dubbing through phrase-level cross-lingual prosody transfer",
    "citation_count": 1,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chiang23_interspeech.html": {
    "title": "Why We Should Report the Details in Subjective Evaluation of TTS More Rigorously",
    "volume": "main",
    "abstract": "This paper emphasizes the importance of reporting experiment details in subjective evaluations and demonstrates how such details can significantly impact evaluation results in the field of speech synthesis. Through an analysis of 80 papers presented at INTERSPEECH 2022, we find a lack of thorough reporting on critical details such as evaluator recruitment and filtering, instructions and payments, and the geographic and linguistic backgrounds of evaluators. To illustrate the effect of these details on evaluation outcomes, we conducted mean opinion score (MOS) tests on three well-known TTS systems under different evaluation settings and we obtain at least three distinct rankings of TTS models. We urge the community to report experiment details in subjective evaluations to improve the reliability and interpretability of experimental results",
    "checked": true,
    "id": "a9109eb5e0ce1cab8a512f31abfe44a289a779fc",
    "semantic_title": "why we should report the details in subjective evaluation of tts more rigorously",
    "citation_count": 4,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/perezzarazaga23_interspeech.html": {
    "title": "Speaker-independent neural formant synthesis",
    "volume": "main",
    "abstract": "We describe speaker-independent speech synthesis driven by a small set of phonetically meaningful speech parameters such as formant frequencies. The intention is to leverage deep-learning advances to provide a highly realistic signal generator that includes control affordances required for stimulus creation in the speech sciences. Our approach turns input speech parameters into predicted mel-spectrograms, which are rendered into waveforms by a pre-trained neural vocoder. Experiments with WaveNet and HiFi-GAN confirm that the method achieves our goals of accurate control over speech parameters combined with high perceptual audio quality. We also find that the small set of phonetically relevant speech parameters we use is sufficient to allow for speaker-independent synthesis (a.k.a. universal vocoding)",
    "checked": true,
    "id": "6232f49653591d7bfb45c06c994f3a6d15a3f964",
    "semantic_title": "speaker-independent neural formant synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saito23b_interspeech.html": {
    "title": "CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center",
    "volume": "main",
    "abstract": "We present CALLS, a Japanese speech corpus that considers phone calls in a customer center as a new domain of empathetic spoken dialogue. The existing STUDIES corpus covers only empathetic dialogue between a teacher and student in a school. To extend the application range of empathetic dialogue speech synthesis (EDSS), we designed our corpus to include the same female speaker as the STUDIES teacher, acting as an operator in simulated phone calls. We describe a corpus construction methodology and analyze the recorded speech. We also conduct EDSS experiments using the CALLS and STUDIES corpora to investigate the effect of domain differences. The results show that mixing the two corpora during training causes biased improvements in the quality of synthetic speech due to the different degrees of expressiveness. Our project page of the corpus is http://sython.org/Corpus/STUDIES-2",
    "checked": true,
    "id": "3d01daeb2a9ac37c537ae22bd77a1ab127c2e271",
    "semantic_title": "calls: japanese empathetic dialogue speech corpus of complaint handling and attentive listening in customer center",
    "citation_count": 0,
    "authors": []
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharoni23_interspeech.html": {
    "title": "SASPEECH: A Hebrew Single Speaker Dataset for Text To Speech and Voice Conversion",
    "volume": "main",
    "abstract": "We present SASPEECH, a 30-hour single speaker Hebrew corpus accompanied by a text-to-speech (TTS) benchmark. Our TTS benchmark was developed with other low resource languages in mind, allowing it to be adapted and potentially generalized. For the proposed method to work, one must have several hours of recordings and transcripts or have their language included in the Whisper model. SASPEECH is the first large-scale high-quality open dataset of its kind. Thus, it allows a discussion of challenges Hebrew presents when incorporated into generative models. For instance: bridging the gap between modern Hebrew lettering which lacks diacritics and correct pronunciation. We also tackle prominent issues shared by low resource languages and examine how to evaluate output quality without a benchmark. We believe our work will facilitate future generative Hebrew tools and low resource language research. The corpus is publicly accessible at https://www.openslr.org/134",
    "checked": true,
    "id": "e0e32badd874b0cdd608041a108e78fed80302e3",
    "semantic_title": "saspeech: a hebrew single speaker dataset for text to speech and voice conversion",
    "citation_count": 1,
    "authors": []
  }
}