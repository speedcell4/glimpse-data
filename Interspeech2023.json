{
  "https://www.isca-speech.org/archive/interspeech_2023/narayanan23_interspeech.html": {
    "title": "Bridging Speech Science and Technology — Now and Into the Future",
    "volume": "main",
    "abstract": "Speech research is remarkable in so many ways – in its essential human-centeredness, the rich interconnections between the science and technology, and its wide-ranging impact that is both fundamental and applied. Crucial advances in speech science research catalyze and leverage technological advances across the machine intelligence ecosystem, from sensing and imaging to signal processing and machine learning. Likewise, creation of speech-centric societal applications benefits from an understanding of how humans produce, process and use speech in communication. In these complementary endeavors, two intertwined lines of inquiry endure: illuminating the rich information tapestry and inherent variability in speech and creating trustworthy speech technologies This talk will highlight some advances and possibilities in this multifaceted speech research realm. The first is capturing and modeling the human vocal instrument during speaking and how related technological and clinical applications leverage this technology. The second focuses on speech-based informatics tools to support research and clinical translation related to human health and wellbeing. Finally, the talk will highlight the critical goal of designing trustworthy speech and spoken language machine intelligence tools that are inclusive, equitable, robust, safe, and secure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23m_interspeech.html": {
    "title": "Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks",
    "volume": "main",
    "abstract": "Given an audio clip and a reference face image, the goal of the talking head generation is to generate a high-fidelity talking head video. Although some audio-driven methods of generating talking head videos have made some achievements in the past, most of them only focused on lip and audio synchronization and lack the ability to reproduce the facial expressions of the target person. To this end, we propose a talking head generation model consisting of a Memory-Sharing Emotion Feature extractor (MSEF) and an Attention-Augmented Translator based on U-net (AATU). Firstly, MSEF can extract implicit emotional auxiliary features from audio to estimate more accurate emotional face landmarks. Secondly, AATU acts as a translator between the estimated landmarks and the photo-realistic video frames. Extensive qualitative and quantitative experiments have shown the superiority of the proposed method to the previous works. Codes will be made publicly available",
    "checked": true,
    "id": "450c82af807da7c70dd8d43096cb1a0c5c8c929e",
    "semantic_title": "emotional talking head generation based on memory-sharing and attention-augmented networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23n_interspeech.html": {
    "title": "Speech Synthesis with Self-Supervisedly Learnt Prosodic Representations",
    "volume": "main",
    "abstract": "This paper presents S4LPR, a Speech Synthesis model conditioned on Self-Supervisedly Learnt Prosodic Representations. Instead of using raw acoustic features, such as F0 and energy, as intermediate prosodic variables, three self-supervised speech models are designed for comparison and are pre-trained on large-scale unlabeled data to extract frame-level prosodic representations. In addition to vanilla wav2vec 2.0, the other two pre-trained models learn representations from LPC residuals or adopt a multi-task learning strategy to focus on the prosodic information in speech. Based on FastSpeech2 and PnGBERT, our acoustic model is built with the learned prosodic representations as intermediate variables. Experimental results demonstrate that the naturalness of speech synthesized using S4LPR is significantly better than the FastSpeech2 baseline",
    "checked": true,
    "id": "c0a927f459171b0a15114a7a812996563964e91a",
    "semantic_title": "speech synthesis with self-supervisedly learnt prosodic representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tang23_interspeech.html": {
    "title": "EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis",
    "volume": "main",
    "abstract": "There has been significant progress in emotional Text-To-Speech (TTS) synthesis technology in recent years. However, existing methods primarily focus on the synthesis of a limited number of emotion types and have achieved unsatisfactory performance in intensity control. To address these limitations, we propose EmoMix, which can generate emotional speech with specified intensity or a mixture of emotions. Specifically, EmoMix is a controllable emotional TTS model based on a diffusion probabilistic model and a pre-trained speech emotion recognition (SER) model used to extract emotion embedding. Mixed emotion synthesis is achieved by combining the noises predicted by diffusion model conditioned on different emotions during only one sampling process at the run-time. We further apply the Neutral and specific primary emotion mixed in varying degrees to control intensity. Experimental results validate the effectiveness of EmoMix for synthesizing mixed emotion and intensity control",
    "checked": true,
    "id": "5e635e749a90022f5b3704a8fb1c6b48645519cc",
    "semantic_title": "emomix: emotion mixing via diffusion models for emotional speech synthesis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23b_interspeech.html": {
    "title": "Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus",
    "volume": "main",
    "abstract": "We present a large-scale in-the-wild Japanese laughter corpus and a laughter synthesis method. Previous work on laughter synthesis lacks not only data but also proper ways to represent laughter. To solve these problems, we first propose an in-the-wild corpus comprising 3.5 hours of laughter, which is to our best knowledge the largest laughter corpus designed for laughter synthesis. We then propose pseudo phonetic tokens (PPTs) to represent laughter by a sequence of discrete tokens, which are obtained by training a clustering model on features extracted from laughter by a pretrained self-supervised model. Laughter can then be synthesized by feeding PPTs into a text-to-speech system. We further show PPTs can be used to train a language model for unconditional laughter generation. Results of comprehensive subjective and objective evaluations demonstrate that the proposed method significantly outperforms a baseline method, and can generate natural laughter unconditionally",
    "checked": true,
    "id": "20f7cc755d832c664ee5a84b46a30c27b92a6ac5",
    "semantic_title": "laughter synthesis using pseudo phonetic tokens with a large-scale in-the-wild laughter corpus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23u_interspeech.html": {
    "title": "Explicit Intensity Control for Accented Text-to-speech",
    "volume": "main",
    "abstract": "Accented text-to-speech (TTS) synthesis seeks to generate speech with an accent (L2) as a variant of the standard version (L1). How to control the intensity of accent is a very interesting research direction. Recent works design a speaker-adversarial loss to disentangle the speaker and accent information, and then adjust the loss weight to control the accent intensity. However, there is no direct correlation between the disentanglement factor and natural accent intensity. To this end, this paper proposes a new intuitive and explicit accent intensity control scheme for accented TTS. Specifically, we first extract the posterior probability from the L1 speech recognition model to quantify the phoneme accent intensity for accented speech, then design a FastSpeech2 based TTS model, named Ai-TTS, to take the accent intensity expression into account during speech generation. Experiments show that our method outperforms the baseline model in terms of accent rendering and intensity control",
    "checked": true,
    "id": "d354de4a4c182d294ffba7256e08bf6c4767d642",
    "semantic_title": "explicit intensity control for accented text-to-speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23o_interspeech.html": {
    "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "volume": "main",
    "abstract": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models",
    "checked": true,
    "id": "f3cff86b41ccbb4dd533e6d115b82f34d2d8a04c",
    "semantic_title": "comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/duquenne23_interspeech.html": {
    "title": "Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer",
    "volume": "main",
    "abstract": "Recent research has shown that independently trained encoders and decoders, combined through a shared fixed-size representation, can achieve competitive performance in speech-to-text translation. In this work, we show that this type of approach can be further improved with multilingual training. We observe significant improvements in zero-shot cross-modal speech translation, even outperforming a supervised approach based on XLSR for several languages",
    "checked": true,
    "id": "39b4255a439d2aa85c683935cd47314d098fecf3",
    "semantic_title": "modular speech-to-text translation for zero-shot cross-modal transfer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pal23_interspeech.html": {
    "title": "Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters",
    "volume": "main",
    "abstract": "To translate speech for automatic dubbing, machine translation needs to be isochronous, i.e. translated speech needs to be aligned with the source in terms of speech durations. We introduce target factors in a transformer model to predict durations jointly with target language phoneme sequences. We also introduce auxiliary counters to help the decoder to keep track of the timing information while generating target phonemes. We show that our model improves translation quality and isochrony compared to previous work where the translation model is instead trained to predict interleaved sequences of phonemes and durations",
    "checked": true,
    "id": "38872b9e3f937287012bac3c66ab5df0c084a726",
    "semantic_title": "improving isochronous machine translation with target factors and auxiliary counters",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23_interspeech.html": {
    "title": "StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation",
    "volume": "main",
    "abstract": "Direct speech-to-speech translation (S2ST) has gradually become popular as it has many advantages compared with cascade S2ST. However, current research mainly focuses on the accuracy of semantic translation and ignores the speech style transfer from a source language to a target language. The lack of high-fidelity expressive parallel data makes such style transfer challenging, especially in more practical zero-shot scenarios. To solve this problem, we first build a parallel corpus using a multi-lingual multi-speaker text-to-speech synthesis (TTS) system and then propose the StyleS2ST model with cross-lingual speech style transfer ability based on a style adaptor on a direct S2ST system framework. Enabling continuous style space modeling of an acoustic model through parallel corpus training and non-parallel TTS data augmentation, StyleS2ST captures cross-lingual acoustic feature mapping from the source to the target language. Experiments show that StyleS2ST achieves good style similarity and naturalness in both in-set and out-of-set zero-shot scenarios",
    "checked": true,
    "id": "9dd2e6b5076cb5e2325e6a3f40977228b473904b",
    "semantic_title": "styles2st: zero-shot style transfer for direct speech-to-speech translation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gaido23_interspeech.html": {
    "title": "Joint Speech Translation and Named Entity Recognition",
    "volume": "main",
    "abstract": "Modern automatic translation systems aim at supporting the users by providing contextual knowledge. In this framework, a critical task is the output enrichment with information regarding the mentioned entities. This is currently achieved by processing the generated translations with named entity recognition (NER) tools and retrieving their description from knowledge bases. In light of the recent promising results shown by direct speech translation (ST) models and the known weaknesses of cascades (error propagation and additional latency), in this paper we propose multitask models that jointly perform ST and NER, and compare them with a cascade baseline. Experimental results on three language pairs (en-es/fr/it) show that our models significantly outperform the cascade on the NER task (by 0.4-1.0 F1), without degradation in terms of translation quality, and with the same computational efficiency of a plain direct ST model",
    "checked": true,
    "id": "563f203f7efc841dacd6e7801256e8d6f2578509",
    "semantic_title": "joint speech translation and named entity recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sant23_interspeech.html": {
    "title": "Analysis of Acoustic information in End-to-End Spoken Language Translation",
    "volume": "main",
    "abstract": "End-to-End Transformer-based models are the most popular approach for Spoken Language Translation (SLT). While obtaining state-of-the-art results, we are still far from understanding how these models extract acoustic information from the data and how they are transformed into semantic representations. In this paper, we seek to provide a better understanding of the flow of acoustic information along speech-to-text translation models. By means of the Speaker Classification and Spectrogram Reconstruction tasks, this study (i) interprets the main role of the encoder with respect to the acoustic features, (ii) highlights the importance of the acoustic information throughout the model and its transfer between encoder and decoder, and (iii) reveals the significant effect of downsampling convolutional layers for learning acoustic features. (iv) Finally, we also observe the existence of a strong correlation between the semantic domain and the speakers' labels in MuST-C",
    "checked": true,
    "id": "89dc1dd817b0af8184fb55e2b819d0fbcbdfad17",
    "semantic_title": "analysis of acoustic information in end-to-end spoken language translation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23oa_interspeech.html": {
    "title": "LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) and speech translation (ST) can both use neural transducers as the model structure. It is thus possible to use a single transducer model to perform both tasks. In real-world applications, such joint ASR and ST models may need to be streaming and do not require source language identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a streaming language-agnostic multilingual speech recognition and translation model using neural transducers. Based on the transducer model structure, we propose four methods, a unified joint and prediction network for multilingual output, a clustered multilingual encoder, target language identification for encoder, and connectionist temporal classification regularization. Experimental results show that LAMASSU not only drastically reduces the model size but also reaches the performances of monolingual ASR and bilingual ST models",
    "checked": true,
    "id": "41fa401b3ed0b20168c9ad19471c248f1fe9b00c",
    "semantic_title": "lamassu: a streaming language-agnostic multilingual speech recognition and translation model using neural transducers",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23c_interspeech.html": {
    "title": "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available",
    "checked": true,
    "id": "e4f2d75856ce149b994f079ae50fd33ca47245d3",
    "semantic_title": "dphubert: joint distillation and pruning of self-supervised speech models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zaiem23_interspeech.html": {
    "title": "Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases",
    "checked": true,
    "id": "6801e61b2fb45b660fab6d287cae5e23cd6b76dd",
    "semantic_title": "automatic data augmentation for domain adapted fine-tuning of self-supervised speech representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23e_interspeech.html": {
    "title": "Dual Acoustic Linguistic Self-supervised Representation Learning for Cross-Domain Speech Recognition",
    "volume": "main",
    "abstract": "The integration of well-pre-trained acoustic and linguistic representations boosts the performance of speech-to-text cross-modality tasks. However, the potential of fine-tuning cross-modality integrated model on accented and noisy corpus is still under-explored. To address this gap, we propose an end-to-end acoustic and linguistic integrated representation learning model, namely Dual-w2v-BART. Our model incorporates acoustic representations from wav2vec2.0 and linguistic information from BART model by utilizing the cross-attention mechanism in the decoder, with paired speech-text dual inputs. To enhance model robustness on accent and noise, we propose a text-centric representation consistency component that helps to gain the similarity between different modality inputs while representing the same content. The results on accented and noisy speech recognition tasks demonstrate the effectiveness of the proposed model for reducing error rates compared to baseline and other competitive models",
    "checked": true,
    "id": "d5111d4769b60fa9940dda15146af3c7959c2cee",
    "semantic_title": "dual acoustic linguistic self-supervised representation learning for cross-domain speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baskar23_interspeech.html": {
    "title": "O-1: Self-training with Oracle and 1-best Hypothesis",
    "volume": "main",
    "abstract": "We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80% relative compared to EMBR which bridges the gap by 43% relative. O-1 achieves 13% to 25% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23d_interspeech.html": {
    "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets",
    "volume": "main",
    "abstract": "In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend",
    "checked": true,
    "id": "d2451c2cce0d44e3d390aa09059a8a0e5369c223",
    "semantic_title": "mt4ssl: boosting self-supervised speech representation learning by integrating multiple targets",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lamyeemui23_interspeech.html": {
    "title": "Comparing Self-Supervised Pre-Training and Semi-Supervised Training for Speech Recognition in Languages with Weak Language Models",
    "volume": "main",
    "abstract": "This paper investigates the potential of improving a hybrid automatic speech recognition model trained on 10 hours of transcribed data with 200 hours of untranscribed data in low-resource languages. First, we compare baseline methods of cross-lingual transfer with MFCC features and features extracted with the multilingual self-supervised model XLSR-53. Subsequently, we compare two approaches that can leverage the untranscribed data: semi-supervised training with LF-MMI and continued self-supervised pre-training of XLSR-53. Our results on well-resourced English broadcast data derived from MGB show that both methods achieve 18% and 27% relative improvements compared to the baseline, respectively. On the low-resource South African Soap Opera dataset, the relative improvement with semi-supervised training is only 3% due to the inherently weak language model. However, continued pre-training achieves 8.6% relative improvement because it does not rely on any external information",
    "checked": true,
    "id": "0e8e5d938af4fa0d114ae155421681e0755a649e",
    "semantic_title": "comparing self-supervised pre-training and semi-supervised training for speech recognition in languages with weak language models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23da_interspeech.html": {
    "title": "Chinese EFL Learners' Perception of English Prosodic Focus",
    "volume": "main",
    "abstract": "Focus in a sentence can be realized prosodically in speech communication. It has been found not easy for L2 learners to acquire. The present study examines Chinese learners' perception of English prosodic focus, specifically the effects of learners' English proficiency, intonation type, sentence length, and focus location on the perceptual accuracy of English prosodic focus by Chinese EFL learners. Results of two trials in the perception experiment reveal that focus location, intonation type, and English proficiency significantly impacted Chinese learners' perceptual accuracy of both single focus and dual focus in English. Focus in statements was perceived more accurately than that in questions for both single focus and dual focus. Focus located on sentence-final words in questions was perceived more accurately than that on non-final words in questions. Learners' English proficiency positively correlated to the accuracy of focus perception, especially for dual focus",
    "checked": true,
    "id": "e85507287ca0daa9bd73a0a82182c8212e6d0249",
    "semantic_title": "chinese efl learners' perception of english prosodic focus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sostarics23_interspeech.html": {
    "title": "Pitch Accent Variation and the Interpretation of Rising and Falling Intonation in American English",
    "volume": "main",
    "abstract": "This study tests the division of labor in the meaning conveyed by pitch accents and edge tones in English intonation. In three perception studies, we investigate where the locus of the contrast between an assertive vs inquisitive interpretation resides. By doing so, we also gain insight into the role of potentially meaningful within- and between-category variation in the phonetic implementation of discrete intonational tunes. We find that the pitch accent does not contribute to assertive interpretation. Rather, the distinction between assertive and inquisitive interpretation is cued primarily by the final F0 of the pitch contour regardless of the pitch accent, but that increased overall pitch prominence may trigger a salient focus interpretation that interferes with judging assertiveness",
    "checked": true,
    "id": "d47244483903120338e389ac8db57617477d73d5",
    "semantic_title": "pitch accent variation and the interpretation of rising and falling intonation in american english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kuang23_interspeech.html": {
    "title": "Tonal coarticulation as a cue for upcoming prosodic boundary",
    "volume": "main",
    "abstract": "It has been established that the lack of tonal coarticulation or pitch reset is a salient cue for the beginning of a large prosodic domain, however, it is yet unclear whether tonal coarticulation can be an informative cue for the end of a prosodic domain. We examined this question with two continuous speech corpora of Mandarin, and both expert and crowd-sourced perceptual annotations were used. The FPCA model of the holistic tonal contours shows that the carry-over effect of the preceding tone is significantly affected by the strength of the following boundaries. Stronger carry-over effects are associated with the end of larger prosodic boundaries. Moreover, machine learning classification shows that the fine-grained tonal coarticulation patterns are salient cues for predicting larger prosodic boundaries. This result is further validated by crowd-sourced boundary perceptual ratings from human listeners. This study has important implications for the understanding of prosodic phrasing",
    "checked": true,
    "id": "84c4d2832913419041c3d8d6b63c6e7cd8abc176",
    "semantic_title": "tonal coarticulation as a cue for upcoming prosodic boundary",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/repp23_interspeech.html": {
    "title": "Alignment of Beat Gestures and Prosodic Prominence in German",
    "volume": "main",
    "abstract": "We present evidence on the alignment of beat gestures and prosodic prominence from a video corpus consisting of six German educational videos for students from six presenters. Our analysis of 120 beat gestures (with a substantial variety of hand shapes) shows that beat gestures almost always align with prosodically prominent syllables, i.e., syllables carrying a pitch accent. Specifically, the stroke always starts before, or - more often - on, a pitch-accented syllable; the apex mostly falls on the accented syllable (74%) but may also occur in subsequent syllables. The degree of prosodic prominence of the accented syllable (in terms of DIMA-prominence levels) is predictive for the position of the apex, which occurs within rather than after the accented syllable more often for higher degrees of prominence. These findings provide new insights into the alignment of prominence-lending features of prosody and gesture, thereby broadening the empirical landscape for beat gestures",
    "checked": true,
    "id": "5293c0b253dc03cbff04cd19fb28c865a0fcba14",
    "semantic_title": "alignment of beat gestures and prosodic prominence in german",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/white23_interspeech.html": {
    "title": "Creak Prevalence and Prosodic Context in Australian English",
    "volume": "main",
    "abstract": "Creaky voice has been found to mark phrase-finality in many varieties of English, as well as in other languages. The present study aims to investigate whether this is also true for Australian English (AusE), a variety that is understudied in creaky voice research. Using automatic creak detection methods, the need for manual annotation of creak is reduced, and we are able to analyse a large dataset of Australian teenagers' speech. As in other varieties, creak is found to be a marker of finality in AusE. Additionally, we find that males use higher rates of creaky voice than females, challenging the widely held assumption that creak is a feature of female speech",
    "checked": true,
    "id": "6ba65b1ad194c04f21ea2b05088ccb1d92aa432d",
    "semantic_title": "creak prevalence and prosodic context in australian english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bodur23_interspeech.html": {
    "title": "Speech reduction: position within French prosodic structure",
    "volume": "main",
    "abstract": "Variation in the speech signal is a characteristic of spoken language, emerging partially as a result of interactions between various linguistic levels. One example of variation is phonetic reduction, where words are produced with missing or underspecified phonetic forms. Using a French conversational corpus, this paper focuses on the relationship between reduction and prosodic structure to see whether certain positions favor the occurrence of reduction. We annotated and observed the distribution of reduced sequences within specific prosodic domains (Intonational and Accentual Phrases). Preliminary analyses revealed that the detected reductions occur mostly mid- IP and very rarely at IP-final. However, this pattern may vary among speakers, as speakers have different patterns in terms of the number of reductions produced and their positions. It is also usually the case that the reduced sequences occurring mid-IP, coincide with the AP level boundaries, extending from one AP to another",
    "checked": true,
    "id": "a21240a74af5b918c91e05a0ebae2292b35ff2f5",
    "semantic_title": "speech reduction: position within french prosodic structure",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23d_interspeech.html": {
    "title": "Transvelar Nasal Coupling Contributing to Speaker Characteristics in Non-nasal Vowels",
    "volume": "main",
    "abstract": "Nasal-cavity structure is stable in speech and varied across speakers, which potentially gives rise to speaker characteristics. Many studies have reported the acoustic contribution of the nasal cavity for nasal and nasalized sounds with velopharyngeal port opening. However, nasal-cavity resonance does emerge in non-nasal vowels through transvelar nasal coupling, which results in non-negligible modifications to non-nasal vowel spectra. In this study, nasal and oral output sounds were separately recorded during non-nasal utterances, and spectral analysis was conducted. The results indicate clear inter-speaker variability in two spectral measures below 2 kHz: frequency location of double-peaked first nasal-cavity resonance and inconsistent distribution of minor dips above the first resonance. It was also observed that nostril outputs modulate oral output signals to lower the first formant frequency of naturally produced non-low vowels, which also exhibited varied degrees across speakers",
    "checked": true,
    "id": "d6c08360cc77b37e108194d8af38d40d7b98de26",
    "semantic_title": "transvelar nasal coupling contributing to speaker characteristics in non-nasal vowels",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/otani23_interspeech.html": {
    "title": "Speech Synthesis from Articulatory Movements Recorded by Real-time MRI",
    "volume": "main",
    "abstract": "Previous speech synthesis models from articulatory movements recorded using real-time MRI (rtMRI) only predicted vocal tract shape parameters and required additional pitch information to generate a speech waveform. This study proposes a two-stage deep learning model composed of CNN-BiLSTM that predicts a mel-spectrogram from a rtMRI video and a HiFi-GAN vocoder that synthesizes a speech waveform. We evaluated our model on two databases: the ATR 503 sentences rtMRI database and the USC-TIMIT database. The experimental results on the ATR 503 sentences rtMRI database show that the PESQ score and the RMSE of F0 are 1.64 and 26.7 Hz. This demonstrates that all acoustic parameters, including fundamental frequency, can be estimated from the rtMRI videos. In the experiment on the USC-TIMIT database, we obtained a good PESQ score and RMSE for F0. However, the synthesized speech is unclear, indicating that the quality of the datasets affects the intelligibility of the synthesized speech",
    "checked": true,
    "id": "977f2f760d0eb99d73182fd8334c1986989c5b07",
    "semantic_title": "speech synthesis from articulatory movements recorded by real-time mri",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23b_interspeech.html": {
    "title": "The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN",
    "volume": "main",
    "abstract": "Phonetic convergence describes the automatic and unconscious speech adaptation of two interlocutors in a conversation. This paper proposes a Siamese recurrent neural network (RNN) architecture to measure the convergence of the holistic spectral characteristics of speech sounds in an L2-L2 interaction. We extend an alternating reading task (the ART) dataset by adding 20 native Slovak L2 English speakers. We train and test the Siamese RNN model to measure phonetic convergence of L2 English speech from three different native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10 dyads). Our results indicate that the Siamese RNN model effectively captures the dynamics of phonetic convergence and the speaker's imitation ability. Moreover, this text-independent model is scalable and capable of handling L1-induced speaker variability",
    "checked": true,
    "id": "2b9b85a451ee43149db92918bda991886295374b",
    "semantic_title": "the art of conversation: measuring phonetic convergence and deliberate imitation in l2-speech with a siamese rnn",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mahshie23_interspeech.html": {
    "title": "Did you see that? Exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants",
    "volume": "main",
    "abstract": "This project aimed to explore the potential role of vision in speech contrast production and auditory perception development in children with cochlear implants (CWCI). Ten CWCI between 43 and 61 months of age, with at least 2 years of CI experience, served as participants. Employing an auditory imitation task, children's ability to auditorily perceive contrasts that are more or less visible was examined both at baseline and one year after the initial assessment. The children's ability to produce these contrasts was also examined through a picture-naming task. The CWCI tended to produce features in both visibility conditions with greater accuracy than they perceived, both at baseline and at 1 year. Production and perception accuracy increased after one year of CI usage, with the mean perceptual gain for the more visible contrasts exceeding that of the less visible contrasts. The implications of the role of vision in contrast development are discussed",
    "checked": true,
    "id": "81ec5fa89978dde59b20c0c580c79f9a37cd3d3e",
    "semantic_title": "did you see that? exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vanbemmel23_interspeech.html": {
    "title": "Automatic assessments of dysarthric speech: the usability of acoustic-phonetic features",
    "volume": "main",
    "abstract": "Individuals with dysarthria suffer from difficulties in speech production and consequent reductions in speech intelligibility, which is an important concept for diagnosing and assessing effectiveness of speech therapy. In the current study, we investigate which acoustic-phonetic features are most relevant and important in automatically assessing intelligibility and in classifying speech as healthy or dysarthric. After feature selection, we applied a stepwise linear regression to predict intelligibility ratings and a Linear Discriminant Analysis to classify healthy and dysarthric speech. We observed a very strong correlation between actual and predicted intelligibility ratings in the regression analysis. We also observed a high classification accuracy of 98.06% by using 17 features and a comparable, high accuracy of 96.11% with only two features. These results indicate the usefulness of the acoustic-phonetic features in automatic assessments of dysarthric speech",
    "checked": true,
    "id": "8174cc3c98bb4fe0af86fd75c3dc000a54a39489",
    "semantic_title": "automatic assessments of dysarthric speech: the usability of acoustic-phonetic features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/venkatathirumalakumar23_interspeech.html": {
    "title": "Classification of Multi-class Vowels and Fricatives From Patients Having Amyotrophic Lateral Sclerosis with Varied Levels of Dysarthria Severity",
    "volume": "main",
    "abstract": "Dysarthria due to Amyotrophic Lateral Sclerosis (ALS) progressively distorts the acoustic space affecting the discriminability of different vowels and fricatives. However, the extent to which this happens with increasing severity is not thoroughly investigated. In this work, we perform automatic 4-class vowel (/a/, /i/, /o/, /u/) and 3-class fricative (/s/, /sh/, /f/) classification at varied severity levels and compare the performances with those from manual classification (through listening tests). Experiments with speech data from 119 ALS and 40 healthy subjects suggest that the manual and automatic classification accuracies reduce with an increase in dysarthria severity reaching 59.22% and 61.67% for vowels and 41.78% and 38.00% for fricatives, respectively, at the most severe cases. While manual classification is better than automatic one for all severity levels except the highest severity case for vowels, the difference between the two gradually reduces with an increase in severity",
    "checked": true,
    "id": "5eb48172191d12d829d3d4646b9d600cfb3751c5",
    "semantic_title": "classification of multi-class vowels and fricatives from patients having amyotrophic lateral sclerosis with varied levels of dysarthria severity",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qi23b_interspeech.html": {
    "title": "Parameter-efficient Dysarthric Speech Recognition Using Adapter Fusion and Householder Transformation",
    "volume": "main",
    "abstract": "In dysarthric speech recognition, data scarcity and the vast diversity between dysarthric speakers pose significant challenges. While finetuning has been a popular solution, it can lead to overfitting and low parameter efficiency. Adapter modules offer a better solution, with their small size and easy applicability. Additionally, Adapter Fusion can facilitate knowledge transfer from multiple learned adapters, but may employ more parameters. In this work, we apply Adapter Fusion for target speaker adaptation and speech recognition, achieving acceptable accuracy with significantly fewer speaker-specific trainable parameters than classical finetuning methods. We further improve the parameter efficiency of the fusion layer by reducing the size of query and key layers and using Householder transformation to reparameterize the value linear layer. Our proposed fusion layer achieves comparable recognition results to the original method with only one third of the parameters",
    "checked": true,
    "id": "c8c29732ed33b019854bbeb56e7ad8c707c6f270",
    "semantic_title": "parameter-efficient dysarthric speech recognition using adapter fusion and householder transformation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hermann23_interspeech.html": {
    "title": "Few-shot Dysarthric Speech Recognition with Text-to-Speech Data Augmentation",
    "volume": "main",
    "abstract": "Speakers with dysarthria could particularly benefit from assistive speech technology, but are underserved by current automatic speech recognition (ASR) systems. The differences of dysarthric speech pose challenges, while recording large amounts of training data can be exhausting for patients. In this paper, we synthesise dysarthric speech with a FastSpeech 2-based multi-speaker text-to-speech (TTS) system for ASR data augmentation. We evaluate its few-shot capability by generating dysarthric speech with as few as 5 words from an unseen target speaker and then using it to train speaker-dependent ASR systems. The results indicated that, while the TTS output is not yet of sufficient quality, this could allow easy development of personalised acoustic models for new dysarthric speakers and domains in the future",
    "checked": true,
    "id": "4692b33dca3e8dd24bef1ce0e2ba1a8f2acc2e44",
    "semantic_title": "few-shot dysarthric speech recognition with text-to-speech data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yee23_interspeech.html": {
    "title": "Latent Phrase Matching for Dysarthric Speech",
    "volume": "main",
    "abstract": "Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies has emphasized interest in designing and improving personalized speech models for atypical speech. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases",
    "checked": true,
    "id": "d757a58200254625c3326a32a1da6fa8eaa2eff3",
    "semantic_title": "latent phrase matching for dysarthric speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeo23_interspeech.html": {
    "title": "Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification",
    "volume": "main",
    "abstract": "This paper proposes an improved Goodness of Pronunciation (GoP) that utilizes Uncertainty Quantification (UQ) for automatic speech intelligibility assessment for dysarthric speech. Current GoP methods rely heavily on neural network-driven overconfident predictions, which is unsuitable for assessing dysarthric speech due to its significant acoustic differences from healthy speech. To alleviate the problem, UQ techniques were used on GoP by 1) normalizing the phoneme prediction (entropy, margin, maxlogit, logit-margin) and 2) modifying the scoring function (scaling, prior normalization). As a result, prior-normalized maxlogit GoP achieves the best performance, with a relative increase of 5.66%, 3.91%, and 23.65% compared to the baseline GoP for English, Korean, and Tamil, respectively. Furthermore, phoneme analysis is conducted to identify which phoneme scores significantly correlate with intelligibility scores in each language",
    "checked": true,
    "id": "4879abd5687f59bc4123ebe5c0de84f94f6d8583",
    "semantic_title": "speech intelligibility assessment of dysarthric speech by using goodness of pronunciation with uncertainty quantification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23c_interspeech.html": {
    "title": "CQNV: A Combination of Coarsely Quantized Bitstream and Neural Vocoder for Low Rate Speech Coding",
    "volume": "main",
    "abstract": "Recently, speech codecs based on neural networks have proven to perform better than traditional methods. However, redundancy in traditional parameter quantization is visible within the codec architecture of combining the traditional codec with the neural vocoder. In this paper, we propose a novel framework named CQNV, which combines the coarsely quantized parameters of a traditional parametric codec to reduce the bitrate with a neural vocoder to improve the quality of the decoded speech. Furthermore, we introduce a parameters processing module into the neural vocoder to enhance the application of the bitstream of traditional speech coding parameters to the neural vocoder, further improving the reconstructed speech's quality. In the experiments, both subjective and objective evaluations demonstrate the effectiveness of the proposed CQNV framework. Specifically, our proposed method can achieve higher quality reconstructed speech at 1.1 kbps than Lyra and Encodec at 3 kbps",
    "checked": true,
    "id": "79ccc49646b09821403ae4f0d76dace82b19db22",
    "semantic_title": "cqnv: a combination of coarsely quantized bitstream and neural vocoder for low rate speech coding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kamo23_interspeech.html": {
    "title": "Target Speech Extraction with Conditional Diffusion Model",
    "volume": "main",
    "abstract": "Diffusion model-based speech enhancement has received increased attention since it can generate very natural enhanced signals and generalizes well to unseen conditions. Diffusion models have been explored for several sub-tasks of speech enhancement, such as speech denoising, dereverberation, and source separation. In this paper, we investigate their use for target speech extraction (TSE), which consists of estimating the clean speech signal of a target speaker in a mixture of multi-talkers. TSE is realized by conditioning the extraction process on a clue identifying the target speaker. We show we can realize TSE using a conditional diffusion model conditioned on the clue. Besides, we introduce ensemble inference to reduce potential extraction errors caused by the diffusion process. In experiments on Libri2mix corpus, we show that the proposed diffusion model-based TSE combined with ensemble inference outperforms a comparable TSE system trained discriminatively",
    "checked": true,
    "id": "cb8a0a610ddfd79794603b4729c501a662d461f0",
    "semantic_title": "target speech extraction with conditional diffusion model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cohen23_interspeech.html": {
    "title": "Towards Fully Quantized Neural Networks For Speech Enhancement",
    "volume": "main",
    "abstract": "Deep learning models have shown state-of-the-art results in speech enhancement. However, deploying such models on an eight-bit integer-only device is challenging. In this work, we analyze the gaps in deploying a vanilla quantization-aware training method for speech enhancement, revealing two significant observations. First, quantization mainly affects signals with a high input Signal-to-Noise Ratio (SNR). Second, quantizing the model's input and output shows major performance degradation. Based on our analysis, we propose Fully Quantized Speech Enhancement (FQSE), a new quantization-aware training method that closes these gaps and enables eight-bit integer-only quantization. FQSE introduces data augmentation to mitigate the quantization effect on high SNR. Additionally, we add an input splitter and a residual quantization block to the model to overcome the error of the input-output quantization. We show that FQSE closes the performance gaps induced by eight-bit quantization",
    "checked": true,
    "id": "e85c275be3f166e3208ed6a981c9e90ce4498f93",
    "semantic_title": "towards fully quantized neural networks for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23p_interspeech.html": {
    "title": "Complex Image Generation SwinTransformer Network for Audio Denoising",
    "volume": "main",
    "abstract": "Achieving high-performance audio denoising is still a challenging task in real-world applications. Existing time-frequency methods often ignore the quality of generated frequency domain images. This paper converts the audio denoising problem into an image generation task. We first develop a complex image generation SwinTransformer network to capture more information from the complex Fourier domain. We then impose structure similarity and detailed loss functions to generate high-quality images and develop an SDR loss to minimize the difference between denoised and clean audios. Extensive experiments on two benchmark datasets demonstrate that our proposed model is better than state-of-the-art methods",
    "checked": true,
    "id": "34e95df66dc9419ea92343fce9b10f0ccf24b687",
    "semantic_title": "complex image generation swintransformer network for audio denoising",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/blau23_interspeech.html": {
    "title": "Using Text Injection to Improve Recognition of Personal Identifiers in Speech",
    "volume": "main",
    "abstract": "Accurate recognition of specific categories, such as persons' names, dates or other identifiers is critical in many Automatic Speech Recognition (ASR) applications. As these categories represent personal information, ethical use of this data including collection, transcription, training and evaluation demands special care. One way of ensuring the security and privacy of individuals is to redact or eliminate Personally Identifiable Information (PII) from collection altogether. However, this results in ASR models that tend to have lower recognition accuracy of these categories. We use text-injection to improve the recognition of PII categories by including fake textual substitutes of PII categories in the training data using a text injection method. We demonstrate substantial improvement to Recall of Names and Dates in medical notes while improving overall WER. For alphanumeric digit sequences we show improvements to Character Error Rate and Sentence Accuracy",
    "checked": true,
    "id": "645e9909180d729a069b71f7c52750b534e28a83",
    "semantic_title": "using text injection to improve recognition of personal identifiers in speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grosz23_interspeech.html": {
    "title": "Investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a Finnish model",
    "volume": "main",
    "abstract": "Self-supervised speech models, such as the wav2vec2, have become extremely popular in the past few years. Their main appeal is that after their pre-training on a large amount of audio, they require only a small amount of supervised, finetuning data to achieve outstanding results. Despite their immense success, very little is understood about the pre-trained models and how finetuning changes them. In this work, we take the first steps towards a better understanding of wav2vec2 systems using model interpretation tools such as visualization and latent embedding clustering. Through our analysis, we gain new insights into the abilities of the pre-trained networks and the effect that finetuning has on them. We demonstrate that the clusters learned by the pre-trained model are just as important a factor as the supervised training data distribution in determining the accuracy of the finetuned system, which could aid us in selecting the most suitable pre-trained model for the supervised data",
    "checked": true,
    "id": "2ae665744fc5662c89168f14f3153144936cdd37",
    "semantic_title": "investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a finnish model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lehecka23_interspeech.html": {
    "title": "Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech",
    "volume": "main",
    "abstract": "This paper is a step forward in our effort to make vast oral history archives more accessible to the public and researchers by breaking down the decoding barriers between the knowledge encoded in the spoken testimonies and users who want to search for the information of their interest. We present new Transformer-based monolingual models suitable for speech recognition of oral history archives in English, German, and Czech. Our experiments show that although the all-purpose speech recognition systems have recently made tremendous progress, the transcription of oral history archives is still a challenging task for them; our tailored models significantly outperformed larger public multilingual models and scored new state-of-the-art results on all tested datasets. Due to the 2-phase fine-tuning process, our models are robust and can be used for oral history archives of various domains. We publicly release our models within a public speech recognition service",
    "checked": true,
    "id": "d26d8f4c79120558aa4f8131df408af091aff001",
    "semantic_title": "transformer-based speech recognition models for oral history archives in english, german, and czech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23_interspeech.html": {
    "title": "Iteratively Improving Speech Recognition and Voice Conversion",
    "volume": "main",
    "abstract": "Many existing works on voice conversion (VC) tasks use automatic speech recognition (ASR) models for ensuring linguistic consistency between source and converted samples. However, for the low-data resource domains, training a high-quality ASR remains to be a challenging task. In this work, we propose a novel iterative way of improving both the ASR and VC models. We first train an ASR model which is used to ensure content preservation while training a VC model. In the next iteration, the VC model is used as a data augmentation method to further fine-tune the ASR model and generalize it to diverse speakers. By iteratively leveraging the improved ASR model to train VC model and vice-versa, we experimentally show improvement in both the models. Our proposed framework outperforms the ASR and one-shot VC baseline models on English singing and Hindi speech domains in subjective and objective evaluations in low-data resource settings",
    "checked": true,
    "id": "e9f0e92d7a18cc78b71146d67358fcfd1a6bdc4d",
    "semantic_title": "iteratively improving speech recognition and voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fatehi23_interspeech.html": {
    "title": "LABERT: A Combination of Local Aggregation and Self-Supervised Speech Representation Learning for Detecting Informative Hidden Units in Low-Resource ASR Systems",
    "volume": "main",
    "abstract": "With advances in deep learning methodologies, Automatic Speech Recognition (ASR) systems have seen impressive results. However, ASR in Low-Resource Environments (LREs) are challenged by a lack of training data for the specific target domain. We propose that data sampling criteria for choosing more informative speech samples can be critical to addressing the problem of training data bottleneck. Our proposed Local Aggregation BERT (LABERT) method for self-supervised speech representation learning fuses an active learning model with an adapted local aggregation metric. Active learning is used to pick informative speech units, whereas the aggregation metric forces the model to move similar data together in the latent space while separating dissimilar instances to detect hidden units in LRE tasks. We evaluate LABERT with two LRE datasets: I-CUBE and UASpeech to explore the performance of our model in the LRE ASR problems",
    "checked": true,
    "id": "046609639dcd941f5438bde0eb73481f60411735",
    "semantic_title": "labert: a combination of local aggregation and self-supervised speech representation learning for detecting informative hidden units in low-resource asr systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xue23_interspeech.html": {
    "title": "TranUSR: Phoneme-to-word Transcoder Based Unified Speech Representation Learning for Cross-lingual Speech Recognition",
    "volume": "main",
    "abstract": "UniSpeech has achieved superior performance in cross-lingual automatic speech recognition (ASR) by explicitly aligning latent representations to phoneme units using multi-task self-supervised learning. While the learned representations transfer well from high-resource to low-resource languages, predicting words directly from these phonetic representations in downstream ASR is challenging. In this paper, we propose TranUSR, a two-stage model comprising a pre-trained UniData2vec and a phoneme-to-word Transcoder. Different from UniSpeech, UniData2vec replaces the quantized discrete representations with continuous and contextual representations from a teacher model for phonetically-aware pre-training. Then, Transcoder learns to translate phonemes to words with the aid of extra texts, enabling direct word generation. Experiments on Common Voice show that UniData2vec reduces PER by 5.3% compared to UniSpeech, while Transcoder yields a 14.4% WER reduction compared to grapheme fine-tuning",
    "checked": true,
    "id": "c03ec12dd7bd78123cd4b7959d6eacd0d4b2ae7b",
    "semantic_title": "tranusr: phoneme-to-word transcoder based unified speech representation learning for cross-lingual speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23e_interspeech.html": {
    "title": "Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR",
    "volume": "main",
    "abstract": "ASR systems in real applications must be adapted on the fly to correctly recognize task-specific contextual terms, such as contacts, application names and media entities. However, it is challenging to achieve scalability, large in-domain quality gains, and minimal out-of-domain quality regressions simultaneously. In this work, we introduce an effective neural biasing architecture called Dual-Mode NAM. Dual-Mode NAM embeds a top-k search process in its attention mechanism in a trainable fashion to perform an accurate top-k phrase selection before injecting the corresponding word-piece context into the acoustic encoder. We further propose a controllable mechanism to enable the ASR system to be able to trade off its in-domain and out-of-domain quality at inference time. When evaluated on a large-scale biasing benchmark, the combined techniques improve a previously proposed method with an average in-domain and out-of-domain WER reduction by up to 53.3% and 12.0% relative respectively",
    "checked": true,
    "id": "7e39237918eb9b5c0b3baa66b174a268c684a975",
    "semantic_title": "dual-mode nam: effective top-k context injection for end-to-end asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23g_interspeech.html": {
    "title": "GhostRNN: Reducing State Redundancy in RNN with Cheap Operations",
    "volume": "main",
    "abstract": "Recurrent neural network (RNNs) that are capable of modeling long-distance dependencies are widely used in various speech tasks, eg., keyword spotting (KWS) and speech enhancement (SE). Due to the limitation of power and memory in low-resource devices, efficient RNN models are urgently required for real-world applications. In this paper, we propose an efficient RNN architecture, GhostRNN, which reduces hidden state redundancy with cheap operations. In particular, we observe that partial dimensions of hidden states are similar to the others in trained RNN models, suggesting that redundancy exists in specific RNNs. To reduce the redundancy and hence computational cost, we propose to first generate a few intrinsic states, and then apply cheap operations to produce ghost states based on the intrinsic states. Experiments on KWS and SE tasks demonstrate that the proposed GhostRNN significantly reduces the memory usage (~40%) and computation cost while keeping performance similar. Codes will be available at https://gitee.com/mindspore/models/tree/master/research/audio/ghostrnn",
    "checked": true,
    "id": "2ca7637abf82a22f345829ff6a6e065c67a3925f",
    "semantic_title": "ghostrnn: reducing state redundancy in rnn with cheap operations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23da_interspeech.html": {
    "title": "Task-Agnostic Structured Pruning of Speech Representation Models",
    "volume": "main",
    "abstract": "Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have been shown to significantly improve many speech tasks. However, their large memory and strong computational requirements hinder their industrial applicability. Structured pruning is a hardware-friendly model compression technique but usually results in a larger loss of accuracy. In this paper, we propose a fine-grained attention head pruning method to compensate for the performance degradation. In addition, we also introduce the straight through estimator into the L0 regularization to further accelerate the pruned model. Experiments on the SUPERB benchmark show that our model can achieve comparable performance to the dense model in multiple tasks and outperforms the Wav2vec 2.0 base model on average, with 72% fewer parameters and 2 times faster inference speed",
    "checked": true,
    "id": "99aa56c8136a83756c4b8d901941c5bb2b2dcdac",
    "semantic_title": "task-agnostic structured pruning of speech representation models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kanda23_interspeech.html": {
    "title": "Factual Consistency Oriented Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents a novel optimization framework for automatic speech recognition (ASR) with the aim of reducing hallucinations produced by an ASR model. The proposed framework optimizes the ASR model to maximize an expected factual consistency score between ASR hypotheses and ground-truth transcriptions, where the factual consistency score is computed by a separately trained estimator. Experimental results using the AMI meeting corpus and the VoxPopuli corpus show that the ASR model trained with the proposed framework generates ASR hypotheses that have significantly higher consistency scores with ground-truth transcriptions while maintaining the word error rates close to those of cross entropy-trained ASR models. Furthermore, it is shown that training the ASR models with the proposed framework improves the speech summarization quality as measured by the factual consistency of meeting conversation summaries generated by a large language model",
    "checked": true,
    "id": "979a68d069fe7b41105085e9c6182da5058665b6",
    "semantic_title": "factual consistency oriented speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fathullah23_interspeech.html": {
    "title": "Multi-Head State Space Model for Speech Recognition",
    "volume": "main",
    "abstract": "State space models (SSMs) have recently shown promising results on small-scale sequence and language modelling tasks, rivalling and outperforming many attention-based approaches. In this paper, we propose a multi-head state space (MH-SSM) architecture equipped with special gating mechanisms, where parallel heads are taught to learn local and global temporal dynamics on sequence data. As a drop-in replacement for multi-head attention in transformer encoders, this new model significantly outperforms the transformer transducer on the LibriSpeech speech recognition corpus. Furthermore, we augment the transformer block with MH-SSMs layers, referred to as the Stateformer, achieving state-of-the-art performance on the LibriSpeech task, with word error rates of 1.76%/4.37% on the development and 1.91%/4.36% on the test sets without using an external language model",
    "checked": true,
    "id": "067aaf0d1cde4ee21063be137559f2fe50125570",
    "semantic_title": "multi-head state space model for speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23_interspeech.html": {
    "title": "Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search",
    "volume": "main",
    "abstract": "Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance",
    "checked": true,
    "id": "9c607471820fe74572e142fc6e9ce432716048c8",
    "semantic_title": "cascaded multi-task adaptive learning based on neural architecture search",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martin23_interspeech.html": {
    "title": "Probing Self-supervised Speech Models for Phonetic and Phonemic Information: A Case Study in Aspiration",
    "volume": "main",
    "abstract": "Textless self-supervised speech models have grown in capabilities in recent years, but the nature of the linguistic information they encode has not yet been thoroughly examined. We evaluate the extent to which these models' learned representations align with basic representational distinctions made by humans, focusing on a set of phonetic (low-level) and phonemic (more abstract) contrasts instantiated in word-initial stops. We find that robust representations of both phonetic and phonemic distinctions emerge in early layers of these models' architectures, and are preserved in the principal components of deeper layer representations. Our analyses suggest two sources for this success: some can only be explained by the optimization of the models on speech data, while some can be attributed to these models' high-dimensional architectures. Our findings show that speech-trained HuBERT derives a low-noise and low-dimensional subspace corresponding to abstract phonological distinctions",
    "checked": true,
    "id": "7373477778b12d453191b09d46e2f77f4295ca52",
    "semantic_title": "probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/harding23_interspeech.html": {
    "title": "Selective Biasing with Trie-based Contextual Adapters for Personalised Speech Recognition using Neural Transducers",
    "volume": "main",
    "abstract": "Neural transducer ASR models achieve state of the art accuracy on many tasks, however rare word recognition poses a particular challenge as models often fail to recognise words that occur rarely, or not at all, in the training data. Methods of contextual biasing, where models are dynamically adapted to bias their outputs towards a given list of relevant words and phrases, have been shown to be effective at alleviating this issue. While such methods are effective at improving rare word recognition, over-biasing can lead to degradation on common words. In this work we propose several extensions to a recently proposed trie-based method of contextual biasing. We show how performance of the method can be improved in terms of rare word recognition, especially in the case of very large catalogues, by introducing a simple normalisation term, how the method can be trained as an adapter module, and how selective biasing can be applied to practically eliminate over-biasing on common words",
    "checked": true,
    "id": "c261ebb5eec148522963b9c6bdd958e463ebcc2c",
    "semantic_title": "selective biasing with trie-based contextual adapters for personalised speech recognition using neural transducers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23b_interspeech.html": {
    "title": "Robust Prototype Learning for Anomalous Sound Detection",
    "volume": "main",
    "abstract": "In this paper, we present a robust prototype learning framework for anomalous sound detection (ASD), where prototypical loss is exploited to measure the similarity between samples and prototypes. We show that existing generative and discriminative based ASD methods can be unified into this framework from the perspective of prototypical learning. For ASD in recent DCASE challenges, extensions related to imbalanced learning are proposed to improve the robustness of prototypes learned from source and target domains. Specifically, balanced sampling and multiple-prototype expansion (MPE) strategies are proposed to address imbalances across attributes of source and target domains. Furthermore, a novel negative-prototype expansion (NPE) method is used to construct pseudo-anomalies to learn a more compact and effective embedding space for normal sounds. Evaluation on the DCASE2022 Task2 development dataset demonstrates the validity of the proposed prototype learning framework",
    "checked": true,
    "id": "d6a1b187a270900853a4a89c0dcc6ae3c0a2830e",
    "semantic_title": "robust prototype learning for anomalous sound detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kushwaha23_interspeech.html": {
    "title": "A multimodal prototypical approach for unsupervised sound classification",
    "volume": "main",
    "abstract": "In the context of environmental sound classification, the adaptability of systems is key: which sound classes are interesting depends on the context and the user's needs. Recent advances in text-to-audio retrieval allow for zero-shot audio classification, but performance compared to supervised models remains limited. This work proposes a multimodal prototypical approach that exploits local audio-text embeddings to provide more relevant answers to audio queries, augmenting the adaptability of sound detection in the wild. We do this by first using text to query a nearby community of audio embeddings that best characterize each query sound, and select the group's centroids as our prototypes. Second, we compare unseen audio to these prototypes for classification. We perform multiple ablation studies to understand the impact of the embedding models and prompts. Our unsupervised approach improves upon the zero-shot state-of-the-art in three sound recognition benchmarks by an average of 12%",
    "checked": true,
    "id": "fb082d89b1f8906509991f738961e1cbe21d7435",
    "semantic_title": "a multimodal prototypical approach for unsupervised sound classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wen23_interspeech.html": {
    "title": "Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms",
    "volume": "main",
    "abstract": "Robust audio anti-spoofing has been increasingly challeng- ing due to the recent advancements on deepfake techniques. While spectrograms have demonstrated their capability for anti- spoofing, complementary information presented in multi-order spectral patterns have not been well explored, which limits their effectiveness for varying spoofing attacks. Therefore, we propose a novel deep learning method with a spectral fusion- reconstruction strategy, namely S2pecNet, to utilise multi-order spectral patterns for robust audio anti-spoofing representations. Specifically, spectral patterns up to second-order are fused in a coarse-to-fine manner and two branches are designed for the fine-level fusion from the spectral and temporal contexts. A reconstruction from the fused representation to the input spec- trograms further reduces the potential fused information loss. Our method achieved the state-of-the-art performance with an EER of 0.77% on a widely used dataset - ASVspoof2019 LA Challenge",
    "checked": true,
    "id": "9f8e5fa471adab4938b3145fcb41b79c22ec2f0b",
    "semantic_title": "robust audio anti-spoofing with fusion-reconstruction learning on multi-order spectrograms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23c_interspeech.html": {
    "title": "Adapting Language-Audio Models as Few-Shot Audio Learners",
    "volume": "main",
    "abstract": "Contrastive language-audio pretraining (CLAP) has become a new paradigm to learn audio concepts with audio-text pairs. CLAP models have shown unprecedented performance as zero-shot classifiers on downstream tasks. To further adapt CLAP with domain-specific knowledge, a popular method is to finetune its audio encoder with available labelled examples. However, this is challenging in low-shot scenarios, as the amount of annotations is limited compared to the model size. In this work, we introduce a Training-efficient (Treff) adapter to rapidly learn with a small set of examples while maintaining the capacity for zero-shot classification. First, we propose a cross-attention linear model (CALM) to map a set of labelled examples and test audio to test labels. Second, we find initialising CALM as a cosine measurement improves our Treff adapter even without training. The Treff adapter beats metric-based methods in few-shot settings and yields competitive results to fully-supervised methods",
    "checked": true,
    "id": "237032b6256087766e6d366a47227aef980fd2b7",
    "semantic_title": "adapting language-audio models as few-shot audio learners",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23l_interspeech.html": {
    "title": "TFECN: Time-Frequency Enhanced ConvNet for Audio Classification",
    "volume": "main",
    "abstract": "Recently, transformer-based models have shown leading performance in audio classification, gradually replacing the dominant ConvNet in the past. However, some research has shown that certain characteristics and designs in transformers can be applied to other architectures and make them achieve similar performance as transformers. In this paper, we introduce TFECN, a pure ConvNet that combines the design in transformers and has time-frequency enhanced convolution with large kernels. It can provide a global receptive field on the frequency dimension as well as avoid the influence of the convolution's shift-equivariance on the recognition of not shift-invariant patterns along the frequency axis. Furthermore, to use ImageNet-pretrained weights, we propose a method for transferring weights between kernels of different sizes. On the commonly used datasets AudioSet, FSD50K, and ESC50, our TFECN outperforms the models trained in the same way",
    "checked": true,
    "id": "8cb1a3348c7004f0d3c4aa666edd09532da0b4db",
    "semantic_title": "tfecn: time-frequency enhanced convnet for audio classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23b_interspeech.html": {
    "title": "Resolution Consistency Training on Time-Frequency Domain for Semi-Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "The fact that unlabeled data can be used for supervised learning is of considerable relevance concerning polyphonic sound event detection (PSED) because of the high costs of frame-wise labeling. While semi-supervised learning (SSL) for image tasks has been extensively developed, SSL for PSED has not been substantially explored due to data augmentation limitations. In this paper, we propose a novel SSL strategy for PSED called resolution consistency training (ResCT), combining unsupervised terms with the mean teacher using different resolutions of a spectrogram for data augmentation. The proposed method regularizes the consistency between the model predictions for different resolutions by controlling the sampling rate and window size. Experimental results show that ResCT outperforms other SSL methods on various evaluation metrics: event-f1 score, intersection-f1 score, and PSDSs. Finally, we report on some ablation studies for the weak and strong augmentation policies",
    "checked": true,
    "id": "24624c92335e516840fe5f362d34ab272c66f78b",
    "semantic_title": "resolution consistency training on time-frequency domain for semi-supervised sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23n_interspeech.html": {
    "title": "Fine-tuning Audio Spectrogram Transformer with Task-aware Adapters for Sound Event Detection",
    "volume": "main",
    "abstract": "In this paper, we present a task-aware fine-tuning method to transfer Patchout faSt Spectrogram Transformer (PaSST) model to sound event detection (SED) task. Pretrained PaSST has shown significant performance on audio tagging (AT) and SED tasks, but it is not optimal to fine-tune the model from a single layer as the local and semantic information have not been well exploited. To address this, we first introduce task-aware adapters including SED-adapter and AT-adapter to fine-tune PaSST for SED and AT task respectively, and then propose task-aware fine-tuning to combine local information from shallower layer with semantic information from deeper layer, based on task-aware adapters. Besides, we propose the self-distillated mean teacher (SdMT) to train a robust student model with soft pseudo labels from teacher. Experiments are conducted on DCASE2022 task4 development set, the EB-F1 of 64.85% and PSDS1 of 0.5548 are achieved which outperform previous state-of-the-art systems",
    "checked": true,
    "id": "5035bf01ae576ab415d822284e629eb3734c6708",
    "semantic_title": "fine-tuning audio spectrogram transformer with task-aware adapters for sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23b_interspeech.html": {
    "title": "Small Footprint Multi-channel Network for Keyword Spotting with Centroid Based Awareness",
    "volume": "main",
    "abstract": "Spoken Keyword Spotting (KWS) in noisy far-field environments is challenging for small-footprint models, given the restrictions on computational resources (e.g., model size, running memory). This is even more intricate when handling noises from multiple microphones. To address this, we present a new multi-channel model that uses a CNN-based network with a linear mixing unit to achieve local-global dependency representations. Our method enhances noise-robustness while ensuring more efficient computation. Besides, we propose an end-to-end centroid-based awareness module that provides class similarity awareness at the bottleneck level to correct ambiguous cases during prediction. We conducted experiments using real noisy far-field data from the MISP challenge 2021 and achieved SOTA results compared to existing small-footprint KWS models. Our best score of 0.126 is highly competitive against larger models like 3D-ResNet, which is 0.122, but ours is much smaller at 473K compared to 13M",
    "checked": true,
    "id": "ca0781a2185f29b95cb0d7b627911a3e4162c975",
    "semantic_title": "small footprint multi-channel network for keyword spotting with centroid based awareness",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23b_interspeech.html": {
    "title": "Few-shot Class-incremental Audio Classification Using Adaptively-refined Prototypes",
    "volume": "main",
    "abstract": "New classes of sounds constantly emerge with a few samples, making it challenging for models to adapt to dynamic acoustic environments. This challenge motivates us to address the new problem of few-shot class-incremental audio classification. This study aims to enable a model to continuously recognize new classes of sounds with a few training samples of new classes while remembering the learned ones. To this end, we propose a method to generate discriminative prototypes and use them to expand the model's classifier for recognizing sounds of new and learned classes. The model is first trained with a random episodic training strategy, and then its backbone is used to generate the prototypes. A dynamic relation projection module refines the prototypes to enhance their discriminability. Results on two datasets (derived from the corpora of Nsynth and FSD-MIX-CLIPS) show that the proposed method exceeds three state-of-the-art methods in average accuracy and performance dropping rate",
    "checked": true,
    "id": "178677fe7e568509249e657635287bf6285ba00e",
    "semantic_title": "few-shot class-incremental audio classification using adaptively-refined prototypes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vali23_interspeech.html": {
    "title": "Interpretable Latent Space Using Space-Filling Curves for Phonetic Analysis in Voice Conversion",
    "volume": "main",
    "abstract": "Vector quantized variational autoencoders (VQ-VAE) are well-known deep generative models, which map input data to a latent space that is used for data generation. Such latent spaces are unstructured and can thus be difficult to interpret. Some earlier approaches have introduced a structure to the latent space through supervised learning by defining data labels as latent variables. In contrast, we propose an unsupervised technique incorporating space-filling curves into vector quantization (VQ), which yields an arranged form of latent vectors such that adjacent elements in the VQ codebook refer to similar content. We applied this technique to the latent codebook vectors of a VQ-VAE, which encode the phonetic information of a speech signal in a voice conversion task. Our experiments show there is a clear arrangement in latent vectors representing speech phones, which clarifies what phone each latent vector corresponds to and facilitates other detailed interpretations of latent vectors",
    "checked": true,
    "id": "4f3e14e1d39ca360c76239dde618ea44500ed98a",
    "semantic_title": "interpretable latent space using space-filling curves for phonetic analysis in voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tulchinskii23_interspeech.html": {
    "title": "Topological Data Analysis for Speech Processing",
    "volume": "main",
    "abstract": "We apply topological data analysis (TDA) to speech classification problems and to the introspection of a pretrained speech model, HuBERT. To this end, we introduce a number of topological and algebraic features derived from Transformer attention maps and embeddings. We show that a simple linear classifier built on top of such features outperforms a fine-tuned classification head. We achieve an improvement of about 9% accuracy and 5% ERR on two common datasets; on CREMA-D, the proposed feature set reaches a new state of the art performance with accuracy 80.155. We also show that topological features are able to reveal functional roles of speech Transformer heads; e.g., we find the heads capable to distinguish between pairs of sample sources (natural/synthetic) or voices without any downstream fine-tuning. Our results demonstrate that TDA is a promising new approach for speech analysis, especially for tasks that require structural prediction",
    "checked": true,
    "id": "12c37eaa9ab37d66e1b014fd79b62bf544522065",
    "semantic_title": "topological data analysis for speech processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jang23_interspeech.html": {
    "title": "Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation",
    "volume": "main",
    "abstract": "Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark",
    "checked": true,
    "id": "d65dd690635d6a220360b2193e6d020da24330c2",
    "semantic_title": "recycle-and-distill: universal compression strategy for transformer-based speech ssl models with attention map reusing and masking distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koppelmann23_interspeech.html": {
    "title": "Personalized Acoustic Scene Classification in Ultra-low Power Embedded Devices Using Privacy-preserving Data Augmentation",
    "volume": "main",
    "abstract": "In this work we present an adaptation method for personalized acoustic scene classification in ultra-low power embedded devices (EDs). The computational limitation of EDs and a large variety of acoustic scenes may lead to poor performance of the embedded classifier in specific real-world user environments. We propose a semi-supervised scheme that estimates the audio feature distribution at ED level and then samples this statistical model to generate artificial data points which emulate user-specific audio features. Then, a second, cloud-based classifier assigns pseudo labels to samples, which are merged with existing labeled data for retraining the embedded classifier. The proposed method leads to significant performance improvements on user-specific data sets and does neither require a persistent connection to a cloud service nor the transmission of raw audio or audio features. It thus results in low data rates, high utility, and privacy-preservation",
    "checked": true,
    "id": "0e85062bba41b628de2d96cac8d77d03191e6a5c",
    "semantic_title": "personalized acoustic scene classification in ultra-low power embedded devices using privacy-preserving data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23_interspeech.html": {
    "title": "Background Domain Switch: A Novel Data Augmentation Technique for Robust Sound Event Detection",
    "volume": "main",
    "abstract": "Data augmentation is a key component to achieve robust and generalizable performance in sound event detection (SED). A well trained SED model should be able to resist the interference of non-target audio events and maintain a robust recognition rate under unknown and possibly mismatched testing conditions. In this study, we propose a novel background domain switch (BDS) data augmentation technique for SED. BDS utilizes a trained SED model on-the-fly to detect backgrounds in audio clips, and switches them among the data points to increase sample variability. This approach can be easily combined with other types of data augmentation techniques. We evaluate the effectiveness of BDS by applying it to several state-of-the-art SED frameworks, and used both publicly available datasets as well as a synthesized mismatched test set. Experiment results systematically show that BDS obtains significant performance improvements from all evaluation aspects. The code is available at: https://github.com/boschresearch/soundseebackgrounddomainswitch",
    "checked": true,
    "id": "cfcf644b1805c24fdbc9e6cddfac41383a2f0d8f",
    "semantic_title": "background domain switch: a novel data augmentation technique for robust sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hou23_interspeech.html": {
    "title": "Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning",
    "volume": "main",
    "abstract": "Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans' listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR",
    "checked": true,
    "id": "e74522543b25c3eca86f7dd62793dbf8d527e8be",
    "semantic_title": "joint prediction of audio event and annoyance rating in an urban soundscape by hierarchical graph representation learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23fa_interspeech.html": {
    "title": "Anomalous Sound Detection Using Self-Attention-Based Frequency Pattern Analysis of Machine Sounds",
    "volume": "main",
    "abstract": "Different machines can exhibit diverse frequency patterns in their emitted sound. This feature has been recently explored in anomaly sound detection and reached state-of-the-art performance. However, existing methods rely on the manual or empirical determination of the frequency filter by observing the effective frequency range in the training data, which may be impractical for general application. This paper proposes an anomalous sound detection method using self-attention-based frequency pattern analysis and spectral-temporal information fusion. Our experiments demonstrate that the self-attention module automatically and adaptively analyses the effective frequencies of a machine sound and enhances that information in the spectral feature representation. With spectral-temporal information fusion, the obtained audio feature eventually improves the anomaly detection performance on the DCASE 2020 Challenge Task 2 dataset",
    "checked": true,
    "id": "5656e4b77179dbc012460460bf92334f45b9235a",
    "semantic_title": "anomalous sound detection using self-attention-based frequency pattern analysis of machine sounds",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23c_interspeech.html": {
    "title": "Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions",
    "volume": "main",
    "abstract": "Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained crossmodal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical crossmodal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets",
    "checked": true,
    "id": "29dc27e49654fc7cd0a9001bbf44a57d78bb74ca",
    "semantic_title": "improving audio-text retrieval via hierarchical cross-modal interaction and auxiliary captions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bn23_interspeech.html": {
    "title": "Differential Privacy enabled Dementia Classification: An Exploration of the Privacy-Accuracy Trade-off in Speech Signal Data",
    "volume": "main",
    "abstract": "Early detection of dementia is critical for effective symptom management. Recent studies have aimed to develop machine learning (ML) models to identify dementia onset and severity using language and speech features. However, existing methods can lead to serious privacy concerns due to sensitive data collected from a vulnerable population. In this work, we aim to establish the privacy-accuracy tradeoff benchmark for dementia classification models using audio and speech features. Specifically, we explore the effects of differential privacy (DP) on the training phase of the audio model. We then compare the classification accuracy of DP and non-DP models using a publicly available dataset. The resultant comparison provides useful insights to make informed decisions about the need for balancing privacy and accuracy tradeoff for dementia classification tasks. Our findings have implications for real-world deployment of ML models to support early detection and effective management of dementia",
    "checked": true,
    "id": "7304c74495f8c4630a852f614e8a613a3b0e410b",
    "semantic_title": "differential privacy enabled dementia classification: an exploration of the privacy-accuracy trade-off in speech signal data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ka_interspeech.html": {
    "title": "Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech",
    "volume": "main",
    "abstract": "Effective speech emotional representations play a key role in Speech Emotion Recognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional speech samples are more difficult and expensive to acquire compared with Neutral style speech, which causes one issue that most related works unfortunately neglect: imbalanced datasets. Models might overfit to the majority Neutral class and fail to produce robust and effective emotional representations. In this paper, we propose an Emotion Extractor to address this issue. We use augmentation approaches to train the model and enable it to extract effective and generalizable emotional representations from imbalanced datasets. Our empirical results show that (1) for the SER task, the proposed Emotion Extractor surpasses the state-of-the-art baseline on three imbalanced datasets; (2) the produced representations from our Emotion Extractor benefit the TTS model, and enable it to synthesize more expressive speech",
    "checked": true,
    "id": "044e47bc8995d603122963d92b84aa6dabea1a53",
    "semantic_title": "learning emotional representations from imbalanced speech data for speech emotion recognition and emotional text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/behera23_interspeech.html": {
    "title": "Towards Multi-Lingual Audio Question Answering",
    "volume": "main",
    "abstract": "Audio Question Answering (AQA) is a multi-modal translation task where a system analyzes an audio signal and a natural language question to generate a desirable natural language answer. AQA has been primarily studied through the lens of the English language. However, addressing AQA in other languages, in the same manner, would require a considerable amount of resources. This paper proposes scalable solutions to multi-lingual audio question answering on both data and modeling fronts. We propose mClothoAQA, a translation-based multi-lingual AQA dataset in eight languages. The dataset consists of 1991 audio files and nearly 0.3 million question-answer pairs. Finally, we introduce a multi-lingual AQA model and demonstrate its strong performance in eight languages. The dataset and code can be accessed at https://github.com/swarupbehera/mAQA",
    "checked": true,
    "id": "431ec63f76d5d64c18e3ab92008cca98975a1678",
    "semantic_title": "towards multi-lingual audio question answering",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/aldarmaki23_interspeech.html": {
    "title": "Diacritic Recognition Performance in Arabic ASR",
    "volume": "main",
    "abstract": "In Arabic text, most vowels are encoded in the form of diacritics that are often omitted, so most speech corpora and ASR models are undiacritized. Text-based diacritization has previously been used to preprocess the input or post-processs ASR hypotheses. It is generally believed that input diacritization degrades ASR quality, but no systematic evaluation of ASR diacritization performance has been conducted to date. We experimentally clarify whether input diacritiztation indeed degrades ASR quality and compare ASR diacritization with text-based diacritization. We fine-tune pre-trained ASR models on transcribed speech with different diacritization conditions: manual, automatic, and no diacritization. We isolate diacritic recognition performance from the overall ASR performance using coverage and precision metrics. We find that ASR diacritization significantly outperforms text-based diacritization, particularly when the ASR model is fine-tuned with manually diacritized transcripts",
    "checked": true,
    "id": "cd31445ba366d10bd11e2a7b2da5c8281c8f1564",
    "semantic_title": "diacritic recognition performance in arabic asr",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kolehmainen23_interspeech.html": {
    "title": "Personalization for BERT-based Discriminative Speech Recognition Rescoring",
    "volume": "main",
    "abstract": "Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/krishnan23_interspeech.html": {
    "title": "On the N-gram Approximation of Pre-trained Language Models",
    "volume": "main",
    "abstract": "Large pre-trained language models (PLMs) have shown remarkable performance across various natural language understanding (NLU) tasks, particularly in low-resource settings. Nevertheless, their potential in Automatic Speech Recognition (ASR) remains largely unexplored. This study investigates the potential usage of PLMs for language modelling in ASR. We compare the application of large-scale text sampling and probability conversion for approximating GPT-2 into an n-gram model. Furthermore, we introduce a vocabulary-restricted decoding method for random sampling, and evaluate the effects of domain difficulty and data size on the usability of generated text. Our findings across eight domain-specific corpora support the use of sampling-based approximation and show that interpolating with a large sampled corpus improves test perplexity over a baseline trigram by 15%. Our vocabulary-restricted decoding method pushes this improvement further by 5% in domain-specific settings",
    "checked": true,
    "id": "821d918fd7c316a1d48579979fa7bde2f7a63c50",
    "semantic_title": "on the n-gram approximation of pre-trained language models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23g_interspeech.html": {
    "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
    "volume": "main",
    "abstract": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers' true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction",
    "checked": true,
    "id": "b3c7d778e5bdafa8b2cd62b996d0e6dfc670effc",
    "semantic_title": "record deduplication for entity distribution modeling in asr transcripts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/agrawal23_interspeech.html": {
    "title": "Learning When to Trust Which Teacher for Weakly Supervised ASR",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) training can utilize multiple experts as teacher models, each trained on a specific domain or accent. Teacher models may be opaque in nature since their architecture may be not be known or their training cadence is different from that of the student ASR model. Still, the student models are updated incrementally using the pseudo-labels generated independently by the expert teachers. In this paper, we exploit supervision from multiple domain experts in training student ASR models. This training strategy is especially useful in scenarios where few or no human transcriptions are available. To that end, we propose a Smart-Weighter mechanism that selects an appropriate expert based on the input audio, and then trains the student model in an unsupervised setting. We show the efficacy of our approach using LibriSpeech and LibriLight benchmarks and find an improvement of 4 to 25% over baselines that uniformly weight all the experts, use a single expert model, or combine experts using ROVER",
    "checked": true,
    "id": "124c16b1243ef8f727df9db70d3c2f1b47ec31c1",
    "semantic_title": "learning when to trust which teacher for weakly supervised asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23f_interspeech.html": {
    "title": "Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer",
    "volume": "main",
    "abstract": "Domain adaptation using text-only corpus is challenging in end-to-end(E2E) speech recognition. Adaptation by synthesizing audio from text through TTS is resource-consuming. We present a method to learn Unified Speech-Text Representation in Conformer Transducer(USTR-CT) to enable fast domain adaptation using the text-only corpus. Different from the previous textogram method, an extra text encoder is introduced in our work to learn text representation and is removed during inference, so there is no modification for online deployment. To improve the efficiency of adaptation, single-step and multistep adaptations are also explored. The experiments on adapting LibriSpeech to SPGISpeech show the proposed method reduces the word error rate(WER) by relatively 44% on the target domain, which is better than those of TTS method and textogram method. Also, it is shown the proposed method can be combined with internal language model estimation(ILME) to further improve the performance",
    "checked": true,
    "id": "f035b38c98d79dd43dda10b919604b1d46cb63df",
    "semantic_title": "text-only domain adaptation using unified speech-text representation in transducer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23e_interspeech.html": {
    "title": "Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model",
    "volume": "main",
    "abstract": "In this paper, we show that representations capturing syllabic units emerge when training a self-supervised speech model with a visually-grounded training objective. We demonstrate that a nearly identical model architecture (HuBErT) trained with a masked language modeling loss does not exhibit this same ability, suggesting that the visual grounding objective is responsible for the emergence of this phenomenon. We propose the use of a minimum cut algorithm to automatically predict syllable boundaries in speech, followed by a 2-stage clustering method to group identical syllables together. We show that our model not only outperforms a state-of-the-art syllabic segmentation method on the language it was trained on (English), but also generalizes in a zero-shot fashion to Estonian. Finally, we show that the same model is capable of zero-shot generalization for a word segmentation task on 4 other languages from the Zerospeech Challenge, in some cases beating the previous state-of-the-art",
    "checked": true,
    "id": "47ba6504f14a3d16f25b1b9afaf5531e41671faf",
    "semantic_title": "syllable discovery and cross-lingual generalization in a visually grounded, self-supervised speech model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23d_interspeech.html": {
    "title": "Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization",
    "volume": "main",
    "abstract": "We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. we design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available here",
    "checked": true,
    "id": "10e8dc07ea256c6a88d7043cf135417402ed38f4",
    "semantic_title": "prompting the hidden talent of web-scale speech models for zero-shot task generalization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moore23_interspeech.html": {
    "title": "Progress and Prospects for Spoken Language Technology: Results from Five Sexennial Surveys",
    "volume": "main",
    "abstract": "Every six years (since 1997), a survey has been conducted at the IEEE workshop on Automatic Speech Recognition and Understanding (ASRU). The aim has been to gain an insight into the research community's perspective on the 'progress and prospects' for spoken language technology. These surveys have been based on a set of 'statements' describing possible scenarios, and respondents are asked to estimate the year (in the future or in the past) when each given scenario might be realised. A number of the statements have appeared in multiple surveys, hence it has been possible to track changes in opinion over time. This paper presents the combined results from five such surveys, the most recent of which was conducted at ASRU-2021. The latest results reveal a dramatic increase in optimism",
    "checked": true,
    "id": "9098d7a04eb5d8a1d64e57e808d240a426a16be1",
    "semantic_title": "progress and prospects for spoken language technology: results from five sexennial surveys",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sanabria23_interspeech.html": {
    "title": "Acoustic Word Embeddings for Untranscribed Target Languages with Continued Pretraining and Learned Pooling",
    "volume": "main",
    "abstract": "Acoustic word embeddings are typically created by training a pooling function using pairs of word-like units. For unsupervised systems, these are mined using k-nearest neighbor (KNN) search, which is slow. Recently, mean-pooled representations from a pre-trained self-supervised English model were suggested as a promising alternative, but their performance on target languages was not fully competitive. Here, we explore improvements to both approaches: we use continued pre-training to adapt the self-supervised model to the target language, and we use a multilingual phone recognizer (MPR) to mine phone n-gram pairs for training the pooling function. Evaluating on four languages, we show that both methods outperform a recent approach on word discrimination. Moreover, the MPR method is orders of magnitude faster than KNN, and is highly data efficient. We also show a small improvement from performing learned pooling on top of the continued pre-trained representations",
    "checked": true,
    "id": "2c82c551b151835cec78df7a2ab86f2a58d0a682",
    "semantic_title": "acoustic word embeddings for untranscribed target languages with continued pretraining and learned pooling",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23d_interspeech.html": {
    "title": "CASA-ASR: Context-Aware Speaker-Attributed ASR",
    "volume": "main",
    "abstract": "Recently, speaker-attributed automatic speech recognition (SAASR) has attracted a wide attention, which aims at answering the question \"who spoke what\". Different from modular systems, end-to-end (E2E) SA-ASR minimizes the speaker-dependent recognition errors directly and shows a promising applicability. In this paper, we propose a context-aware SAASR (CASA-ASR) model by enhancing the contextual modeling ability of E2E SA-ASR. Specifically, in CASA-ASR, a contextual text encoder is involved to aggregate the semantic information of the whole utterance, and a context-dependent scorer is employed to model the speaker discriminability by contrasting with speakers in the context. In addition, a two-pass decoding strategy is further proposed to fully leverage the contextual modeling ability resulting in a better recognition performance. Experimental results on AliMeeting corpus show that the proposed CASA-ASR model outperforms the original E2E SAASR system with a relative improvement of 11.76% in terms of speaker-dependent character error rate",
    "checked": true,
    "id": "eaf765c07764a802c5200a9abd739a921349caab",
    "semantic_title": "casa-asr: context-aware speaker-attributed asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/takahashi23_interspeech.html": {
    "title": "Unsupervised Learning of Discrete Latent Representations with Data-Adaptive Dimensionality from Continuous Speech Streams",
    "volume": "main",
    "abstract": "This work presents a novel deep generative model for unsupervised learning of sparse binary feature representations with data-adaptive dimensionality directly from continuous speech streams. Sharing the critical assumption of unbounded latent dimensionality with previously proposed Bayesian non-parametric approaches, our proposed model can capture the much richer, non-Markovian dependencies between its latent representations. The present work focuses on an investigation of our proposed model's performance in learning linguistically meaningful representations under challenging, realistic scenarios. We train our model with highly speaker-imbalanced datasets and evaluate it on the ABX phone discriminability test. Our model achieves a promising, competitive performance to the state-of-the-art model, despite its huge disadvantage: limited or no access to speaker information during training",
    "checked": true,
    "id": "1355151151f18fb3e98e65b57549ea2e87d5cf80",
    "semantic_title": "unsupervised learning of discrete latent representations with data-adaptive dimensionality from continuous speech streams",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23n_interspeech.html": {
    "title": "AD-TUNING: An Adaptive CHILD-TUNING Approach to Efficient Hyperparameter Optimization of Child Networks for Speech Processing Tasks in the SUPERB Benchmark",
    "volume": "main",
    "abstract": "In this paper, we propose AD-TUNING, an adaptive CHILD-TUNING approach for hyperparameter tuning of child networks. To address the issue of selecting an optimal hyperparameter set P, which often varies for different tasks in CHILD-TUNING, we first analyze the distribution of parameter importance to ascertain the range of P. Next, we propose a simple yet efficient early-stop algorithm to select the appropriate child network from different sizes for various speech tasks. When evaluated on seven speech processing tasks in the SUPERB benchmark, our proposed framework only requires fine-tuning less than 0.1%~10% of pre-trained model parameters for each task to achieve state-of-the-art results in most of the tasks. For instance, the DER of the speaker diarization task is 9.22% relatively lower than the previously reported best results. Other benchmark results are also very competitive. Our code is available at https://github.com/liyunlongaaa/AD-TUNING",
    "checked": true,
    "id": "8d3bbe7ded99577cf07844e63a58c51aaebe25e5",
    "semantic_title": "ad-tuning: an adaptive child-tuning approach to efficient hyperparameter optimization of child networks for speech processing tasks in the superb benchmark",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wong23_interspeech.html": {
    "title": "Distilling knowledge from Gaussian process teacher to neural network student",
    "volume": "main",
    "abstract": "Neural Networks (NN) and Gaussian Processes (GP) are different modelling approaches. The former stores characteristics of the training data in its many parameters, and then performs inference by parsing inputs through these parameters. The latter instead performs inference by computing a similarity between the test and training inputs, and then predicts test outputs that are correlated with the reference training outputs of similar inputs. These models may be combined to leverage upon their diversity. However, both combination and the matrix computations for GP inference are expensive. This paper investigates whether a NN student is able to effectively learn from the information distilled from a GP or ensemble teacher. It is computationally cheaper to infer using this student. Experiments on the speechocean762 spoken language assessment dataset suggest that learning is effective",
    "checked": true,
    "id": "480c5f9471a8822eacd04f2545b01f23539313f7",
    "semantic_title": "distilling knowledge from gaussian process teacher to neural network student",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhati23_interspeech.html": {
    "title": "Segmental SpeechCLIP: Utilizing Pretrained Image-text Models for Audio-Visual Learning",
    "volume": "main",
    "abstract": "Visually grounded models learn from paired images and their spoken captions. Recently, there have been attempts to utilize the visually grounded models trained from images and their corresponding text captions, such as CLIP, to improve speech-based visually grounded models' performance. However, the majority of these models only utilize the pretrained image encoder. Cascaded SpeechCLIP attempted to generate localized word-level information and utilize both the pretrained image and text encoders. Despite using both, they noticed a substantial drop in retrieval performance. Here, we propose to use a hierarchical segmental audio encoder that can generate a sequence of word-like units from audio. We use the pretrained CLIP text encoder on top of these word-like units representations and show significant improvements over the cascaded variant of SpeechCLIP",
    "checked": true,
    "id": "1617d389b7947161f2943e2d30afeb1856052b14",
    "semantic_title": "segmental speechclip: utilizing pretrained image-text models for audio-visual learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jacobs23_interspeech.html": {
    "title": "Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili",
    "volume": "main",
    "abstract": "We consider hate speech detection through keyword spotting on radio broadcasts. One approach is to build an automatic speech recognition (ASR) system for the target low-resource language. We compare this to using acoustic word embedding (AWE) models that map speech segments to a space where matching words have similar vectors. We specifically use a multilingual AWE model trained on labelled data from well-resourced languages to spot keywords in data in the unseen target language. In contrast to ASR, the AWE approach only requires a few keyword exemplars. In controlled experiments on Wolof and Swahili where training and test data are from the same domain, an ASR model trained on just five minutes of data outperforms the AWE approach. But in an in-the-wild test on Swahili radio broadcasts with actual hate speech keywords, the AWE model (using one minute of template data) is more robust, giving similar performance to an ASR system trained on 30 hours of labelled data",
    "checked": true,
    "id": "ce1c8b4655157435f87f9b4c5eb3589e64d3f0da",
    "semantic_title": "towards hate speech detection in low-resource languages: comparing asr to acoustic word embeddings on wolof and swahili",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vandermerwe23_interspeech.html": {
    "title": "Mitigating Catastrophic Forgetting for Few-Shot Spoken Word Classification Through Meta-Learning",
    "volume": "main",
    "abstract": "We consider the problem of few-shot spoken word classification in a setting where a model is incrementally introduced to new word classes. This would occur in a user-defined keyword system where new words can be added as the system is used. In such a continual learning scenario, a model might start to misclassify earlier words as newer classes are added, i.e. catastrophic forgetting. To address this, we propose an extension to model-agnostic meta-learning (MAML). In our new approach, each inner learning loop—where a model \"learns how to learn\" new classes—ends with a single gradient update using stored templates from all the classes that the model has already seen (one template per class). We compare this method to OML (another extension of MAML) in few-shot isolated-word classification experiments on Google Commands and FACC. Our method consistently outperforms OML in experiments where the number of shots and the final number of classes are varied",
    "checked": true,
    "id": "af849704754c3cb8ea621cca465c4c13372c9148",
    "semantic_title": "mitigating catastrophic forgetting for few-shot spoken word classification through meta-learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/polacek23_interspeech.html": {
    "title": "Online Punctuation Restoration using ELECTRA Model for streaming ASR Systems",
    "volume": "main",
    "abstract": "In this work, we propose a lightweight online approach to automatic punctuation restoration (APR), which can be utilized in speech transcription systems for, e.g., live captioning TV or radio streams. It uses only text input without prosodic features and a fine-tuned ELECTRA-Small model with a two-layer classification head. It allows for restoring question marks, commas, and periods with a very short inference time and a low latency of just three words. Our APR scheme is first tuned and compared to other architectures on a set of manual TV news transcripts. The resulting system is then compared to another real-time APR module utilizing a recurrent network and a combination of text and acoustic features. The test data we use contains automatic transcripts of radio talks and TV debates; we are also publishing this data. The results show that our APR module performs better than the above-mentioned system and yields on the two test sets an average F1 of 71.2% and 69.4%, respectively",
    "checked": true,
    "id": "01372fd6f9c099ea351050a99e547ebe2a36f746",
    "semantic_title": "online punctuation restoration using electra model for streaming asr systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23s_interspeech.html": {
    "title": "Language Agnostic Data-Driven Inverse Text Normalization",
    "volume": "main",
    "abstract": "The rise of automatic speech recognition (ASR) models has created an urgent need for converting spoken language into written text to provide better user experiences. This has drawn the attention of researchers, particularly for real-time on-device ASR deployment, towards the inverse text normalization (ITN) problem. While data-driven ITN methods have shown great promise in recent studies, the lack of labeled spoken-written datasets is hindering the development for non-English data-driven ITN. To bridge this gap, we propose a language-agnostic data-driven ITN framework that leverages data augmentation and neural machine translation specifically designed for real-time miniature models and low-resource languages. Additionally, we have developed an evaluation method for language-agnostic ITN models when only English data is available. Our empirical evaluation attests to the efficacy of this language-agnostic ITN modeling with data augmentation approach for multiple non-English languages",
    "checked": true,
    "id": "68f9e81413ea8bfbc10e88191983a7f4dc7b1b30",
    "semantic_title": "language agnostic data-driven inverse text normalization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23j_interspeech.html": {
    "title": "How to Estimate Model Transferability of Pre-Trained Speech Models?",
    "volume": "main",
    "abstract": "In this work, we introduce a ''score-based assessment'' framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks. We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations. Our framework efficiently computes transferability scores without actual fine-tuning of candidate models or layers by making a temporal independent hypothesis. We evaluate some popular supervised speech models (and self-supervised speech models in cross-layer and cross-model settings using public data. Experimental results show a high Spearman's rank correlation and low p-value between our estimation framework and fine-tuning ground truth. Our proposed transferability framework requires less computational time and resources, making it a resource-saving and time-efficient approach for tuning speech foundation models",
    "checked": true,
    "id": "81a6b9cba431287c46fc29148ccbf6e01bf52d30",
    "semantic_title": "how to estimate model transferability of pre-trained speech models?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ihori23_interspeech.html": {
    "title": "Transcribing Speech as Spoken and Written Dual Text Using an Autoregressive Model",
    "volume": "main",
    "abstract": "This paper proposes a novel method to jointly generate spoken and written text from input speech for expanding use cases of speech-based applications. The spoken text generated using speech-to-spoken text systems, i.e., speech recognition systems, has disfluencies and no punctuation marks. Thus, spoken text is often converted into written text using a spoken text-to-written text system. However, this cascading is unsuitable for overall optimization and computationally expensive. Although a speech-to-written-text system that directly outputs the written text from the speech is also developed, it cannot output the spoken text. To efficiently produce both spoken and written text from speech, our key advance is to handle a joint text of spoken and written texts in an autoregressive model. This enables us to correctly generate both spoken and written texts by considering their dependencies via a single decoding process. Our experiments demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "1ddf41cf13aeda48ce9a79df0eb08e8255bc233f",
    "semantic_title": "transcribing speech as spoken and written dual text using an autoregressive model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuksel23_interspeech.html": {
    "title": "NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning",
    "volume": "main",
    "abstract": "This paper introduces NoRefER, a novel referenceless quality metric for automatic speech recognition (ASR) systems. Traditional reference-based metrics for evaluating ASR systems require costly ground-truth transcripts. NoRefER overcomes this limitation by fine-tuning a multilingual language model for pair-wise ranking ASR hypotheses using contrastive learning with Siamese network architecture. The self-supervised NoRefER exploits the known quality relationships between hypotheses from multiple compression levels of an ASR for learning to rank intra-sample hypotheses by quality, which is essential for model comparisons. The semi-supervised version also uses a referenced dataset to improve its inter-sample quality ranking, which is crucial for selecting potentially erroneous samples. The results indicate that NoRefER correlates highly with reference-based metrics and their intra-sample ranks, indicating a high potential for referenceless ASR evaluation or a/b testing",
    "checked": true,
    "id": "b99733670ef7c2553944208a16a68968cbb85946",
    "semantic_title": "norefer: a referenceless quality metric for automatic speech recognition via semi-supervised language model fine-tuning with contrastive learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23c_interspeech.html": {
    "title": "Scaling Laws for Discriminative Speech Recognition Rescoring Models",
    "volume": "main",
    "abstract": "Recent studies have found that model performance has a smooth power-law relationship, or scaling laws, with training data and model size, for a wide range of problems. These scaling laws allow one to choose nearly optimal data and model sizes. We study whether this scaling property is also applicable to second-pass rescoring, which is an important component of speech recognition systems. We focus on RescoreBERT as the rescoring model, which uses a pre-trained Transformer-based architecture fined tuned with an ASR discriminative loss. Using such a rescoring model, we show that the word error rate (WER) follows a scaling law for over two orders of magnitude as training data and model size increase. In addition, it is found that a pre-trained model would require less data than a randomly initialized model of the same size, representing effective data transferred from pre-training step. This effective data transferred is found to also follow a scaling law with the data and model size",
    "checked": true,
    "id": "a20c6ce80872dbc6a8e6403b2a366973061a9f89",
    "semantic_title": "scaling laws for discriminative speech recognition rescoring models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23w_interspeech.html": {
    "title": "Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition",
    "volume": "main",
    "abstract": "Energy-based language models (ELMs) parameterize an unnormalized distribution for natural sentences and are radically different from popular autoregressive language models (ALMs). As an important application, ELMs have been successfully used as a means for calculating sentence scores in speech recognition, but they all use less-modern CNN or LSTM networks. The recent progress in Transformer networks and large pretrained models such as BERT and GPT2 opens new possibility to further advancing ELMs. In this paper, we explore different architectures of energy functions and different training methods to investigate the capabilities of ELMs in rescoring for speech recognition, all using large pretrained models as backbones. Extensive experiments are conducted on two datasets, AISHELL-1 and WenetSpeech. The results show that the best ELM achieves competitive results with the finetuned GPT2 and performs significantly better than the finetuned BERT. Further analysis show that the ELM obtains better confidence estimate performance than the finetuned GPT2",
    "checked": true,
    "id": "1f99d057382445a7c83f15693e287f56d6305185",
    "semantic_title": "exploring energy-based language models with different architectures and training methods for speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23d_interspeech.html": {
    "title": "Memory Augmented Lookup Dictionary Based Language Modeling for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Recent studies have shown that using an external Language Model (LM) benefits the end-to-end Automatic Speech Recognition (ASR). However, predicting tokens that appear less frequently in the training set is still quite challenging. The long-tail prediction problems have been widely studied in many applications, but only been addressed by a few studies for ASR and LMs. In this paper, we propose a new memory augmented lookup dictionary based Transformer architecture for LM. The newly introduced lookup dictionary incorporates rich contextual information in training set, which is vital to correctly predict long-tail tokens. With intensive experiments on Chinese and English data sets, our proposed method is proved to outperform the baseline Transformer LM by a great margin on both word/character error rate and tail tokens error rate. This is achieved without impact on the decoding efficiency",
    "checked": true,
    "id": "b0c037b4037597f6e023cf300ea864bb9b7aa6dc",
    "semantic_title": "memory augmented lookup dictionary based language modeling for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/iwamoto23_interspeech.html": {
    "title": "Memory Network-Based End-To-End Neural ES-KMeans for Improved Word Segmentation",
    "volume": "main",
    "abstract": "Unsupervised word learning from unlabeled speech is a fundamental problem in zero-resource speech processing, which enables dialogue agents to learn new words directly from spoken utterances. The embedded segmental K-means (ES-KMeans) is a representative unsupervised word segmentation method. However, it has a heterogeneous structure consisting of word boundary search based on Dynamic Programming, segment embedding, and K-Means clustering, which prevents unified optimization. This paper proposes an end-to-end neural network version of the ES-KMeans model. We apply the memory network to hold a dictionary of word embeddings and realize the word boundary search and the clustering respectively as forward and backward propagations. Moreover, we replace the fixed embedding function of the original method with a learnable neural network. Experimental results using the ZeroSpeech Challenge 2020 package show the proposed approach provides superior performance to the state-of-the-art methods",
    "checked": true,
    "id": "88768f2934aa59d668a3e18a4e9c77694702230c",
    "semantic_title": "memory network-based end-to-end neural es-kmeans for improved word segmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23b_interspeech.html": {
    "title": "Retraining-free Customized ASR for Enharmonic Words Based on a Named-Entity-Aware Model and Phoneme Similarity Estimation",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (E2E-ASR) has the potential to improve performance, but a specific issue that needs to be addressed is the difficulty it has in handling enharmonic words: named entities (NEs) with the same pronunciation and part of speech that are spelled differently. This often occurs with Japanese personal names that have the same pronunciation but different Kanji characters. Since such NE words tend to be important keywords, ASR easily loses user trust if it misrecognizes them. To solve these problems, this paper proposes a novel retraining-free customized method for E2E-ASRs based on a named-entity-aware E2E-ASR model and phoneme similarity estimation. Experimental results show that the proposed method improves the target NE character error rate by 35.7% on average relative to the conventional E2E-ASR model when selecting personal names as a target NE",
    "checked": true,
    "id": "2c9beae437c30cd16bea013c10568e0a428feb88",
    "semantic_title": "retraining-free customized asr for enharmonic words based on a named-entity-aware model and phoneme similarity estimation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23c_interspeech.html": {
    "title": "Lightweight and Efficient Spoken Language Identification of Long-form Audio",
    "volume": "main",
    "abstract": "State-of-the-art Spoken Language Identification (SLI) systems usually focus on tackling short audio clips, and thus their performance degrade drastically when applied to long-form audio, such as podcast, which poses peculiar challenges to existing SLI approaches due to its long duration and diverse content that frequently involves multiple speakers as well as various languages, topics, and speech styles. In this paper, we propose the first system to tackle SLI for long-form audio using podcast data by training a lightweight, multi-class feedforward neural classifier using speaker embeddings as input. We demonstrate that our approach can make inference on long audio input efficiently; furthermore, our system can handle long audio files with multiple speakers and can be further extended into utterance-level inference and code-switching detection, which is currently not covered by any existing SLI system",
    "checked": true,
    "id": "1ff7218b81c3d54eb4c17e70def2d0675de3bd91",
    "semantic_title": "lightweight and efficient spoken language identification of long-form audio",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mishra23_interspeech.html": {
    "title": "End to End Spoken Language Diarization with Wav2vec Embeddings",
    "volume": "main",
    "abstract": "The performance of the available end-to-end (E2E) spoken language diarization (LD) systems is biased toward primary language. This is due to the unavailability of sufficient secondary language data. Because in code-switched (CS) utterances, the duration of the primary language is significant over the secondary language. Hence, to resolve the issue, this work initially uses wav2vec (W2V) pre-trained embedding in place of x-vector and can reduce the primary language bias and provides a relative improvement of 30.7% in terms of Jaccard error rate (JER) over the baseline x-vector based E2E (X-E2E) framework. Further, the performance of LD is improved by fine-tuning the W2V embedding extractor and modifying the temporal aggregation strategy from statistical pooling to attention pooling. The Final performance achieved in terms of JER is 22.5, which provides a relative improvement of 38.8% and 62.6% over the standalone W2V fine-tuned and the baseline X-E2E framework, respectively",
    "checked": true,
    "id": "fc2236674339b0c280b16d452c811164f8fd09bb",
    "semantic_title": "end to end spoken language diarization with wav2vec embeddings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nieto23_interspeech.html": {
    "title": "Efficient Spoken Language Recognition via Multilabel Classification",
    "volume": "main",
    "abstract": "Spoken language recognition (SLR) is the task of automatically identifying the language present in a speech signal. Existing SLR models are either too computationally expensive or too large to run effectively on devices with limited resources. For real-world deployment, a model should also gracefully handle unseen languages outside of the target language set, yet prior work has focused on closed-set classification where all input languages are known a-priori. In this paper we address these two limitations: we explore efficient model architectures for SLR based on convolutional networks, and propose a multilabel training strategy to handle non-target languages at inference time. Using the VoxLingua107 dataset, we show that our models obtain competitive results while being orders of magnitude smaller and faster than current state-of-the-art methods, and that our multilabel strategy is more robust to unseen non-target languages compared to multiclass classification",
    "checked": true,
    "id": "bdddcef337decd84c51d34091e52cc0e6f51204d",
    "semantic_title": "efficient spoken language recognition via multilabel classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matejka23_interspeech.html": {
    "title": "Description and Analysis of ABC Submission to NIST LRE 2022",
    "volume": "main",
    "abstract": "This paper summarizes our efforts in the NIST Language Recognition Evaluations 2022 resulting in systems providing competitive performance. We provide both the description and analysis of the systems. We describe what data we have used to train our models, and we follow with embedding extractors and backend classifiers. After covering the architecture, we concentrate on post-evaluation analysis. We compare different topologies of DNN, different backend classifiers, and the impact of the data used to train them. We also report results with XLS-R pre-trained models. We present the performance of the systems in the Fixed condition, where participants are required to use only predefined data sets, and also in the Open condition allowing to use any data to train the systems",
    "checked": true,
    "id": "a570f70209140ef4d62fb3e8c68bf7465e549710",
    "semantic_title": "description and analysis of abc submission to nist lre 2022",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alumae23_interspeech.html": {
    "title": "Exploring the Impact of Pretrained Models and Web-Scraped Data for the 2022 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "This paper describes Vocapia-TalTech team systems developed for the 2022 NIST Language Recognition Evaluation (LRE22) which focused on spoken language identication of African languages, including low-resource languages. In both fixed and open conditions, our primary systems were fused from multiple individual systems using logistic regression. In the fixed condition, we largely relied on wav2vec2.0 conformer models pretrained on the provided training data. In the open condition, we used external pretrained wav2vec2.0 models, phonotactic models and features derived from a multilingual speech recognition system, and also augmented the provided target language development data with additional data scraped from the web. On the LRE22 evaluation data, our final fixed and open condition systems obtained excellent results, with primary metric Cact values of 0.111 and 0.067, respectively. A post-evaluation study shows that both pretrained models as well as additional data are important for accurate models",
    "checked": true,
    "id": "dfb3b676ea6cda7b184911db7fc60fd9a466ed57",
    "semantic_title": "exploring the impact of pretrained models and web-scraped data for the 2022 nist language recognition evaluation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/villalba23_interspeech.html": {
    "title": "Advances in Language Recognition in Low Resource African Languages: The JHU-MIT Submission for NIST LRE22",
    "volume": "main",
    "abstract": "We present the effort of JHU-CLSP/HLTCOE and MIT Lincoln labs for NIST Language Recognition Evaluation (LRE) 2022. LRE22 consisted of a language detection task, i.e., determining whether a given target language was spoken in a speech segment. LRE22 focused on telephone and broadcast narrowband speech in African languages. Since LRE17, there has been large progress in neural embeddings, combined or not, with self-supervised models like Wav2Vec2. Therefore, one of our goals was to investigate these new models, i.e., ECAPA-TDNN, Res2Net or Wav2Vec2+ECAPA-TDNN, in the LRE scenario. In the fixed training condition, LRE22 target languages were only included in a small development set. Hence, we focused on tuning our models to exploit the limited data. For the open condition, we built a massive training set including African data, which improved Cprimary by 50% w.r.t. fixed. Wav2Vec2 embeddings were the best, outperforming ECAPA and Res2Net by 11 and 3%, respectively",
    "checked": true,
    "id": "51aca07d500c44ebde896b8df3b0388dd3ade489",
    "semantic_title": "advances in language recognition in low resource african languages: the jhu-mit submission for nist lre22",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23d_interspeech.html": {
    "title": "DeePMOS: Deep Posterior Mean-Opinion-Score of Speech",
    "volume": "main",
    "abstract": "We propose a deep neural network (DNN) based method that provides a posterior distribution of mean-opinion-score (MOS) for an input speech signal. The DNN outputs parameters of the posterior, mainly the posterior's mean and variance. The proposed method is referred to as deep posterior MOS (DeePMOS). The relevant training data is inherently limited in size (limited number of labeled samples) and noisy due to the subjective nature of human listeners. For robust training of DeePMOS, we use a combination of maximum-likelihood learning, stochastic gradient noise, and a student-teacher learning setup. Using the mean of the posterior as a point estimate, we evaluate standard performance measures of the proposed DeePMOS. The results show comparable performance with existing DNN-based methods that only provide point estimates of the MOS. Then we provide an ablation study showing the importance of various components in DeePMOS",
    "checked": true,
    "id": "8a69d5a2b10234ac6cf64e2a3dafe3b6a7a22e84",
    "semantic_title": "deepmos: deep posterior mean-opinion-score of speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dasare23_interspeech.html": {
    "title": "The Role of Formant and Excitation Source Features in Perceived Naturalness of Low Resource Tribal Language TTS: An Empirical Study",
    "volume": "main",
    "abstract": "Text-to-speech synthesis is a prominent area in the speechprocessing domain that has significant use in reading digital content in a given language. In the proposed work, we worked on two tribal languages of India viz., Lambani and Soliga, which are zero-resource languages. The study began with a dataset collection for both tribal languages. Secondly, a Text-To-Speech (TTS) system was built separately based on the transfer learning approach. To validate the voice quality of TTS-generated speech, subjective as well as objective evaluations were performed. As a part of objective analysis, the voice source and vocal tract filter properties of the synthetic speech have been explored. The extensive study on various aspects of speech, such as LP residual, F0 contour, and formants (F1 & sF2) has shown interesting results that can correlate to the subjective listening test results. The link to the original and synthetic speech can be found online",
    "checked": true,
    "id": "7828e3abbedaecbd890ccf7475783cfa3f397c71",
    "semantic_title": "the role of formant and excitation source features in perceived naturalness of low resource tribal language tts: an empirical study",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23_interspeech.html": {
    "title": "A no-reference speech quality assessment method based on neural network with densely connected convolutional architecture",
    "volume": "main",
    "abstract": "Most speech quality assessment methods require a perfect reference signal to evaluate the damaged speech's quality. However, it is challenging to obtain clean reference signals due to various types and levels of noise in reality. Meanwhile, no-reference speech quality assessment is less accurate than full-reference method. To address these issues, we propose a novel no-reference speech quality assessment model that improves evaluation accuracy with lower complexity. The model is primarily composed of three densely connected convolutional (DCC) modules and a bidirectional long short-term memory (BLSTM) module. Experiment results demonstrate that our method outperforms the baselines, achieving state-of-the-art on the no-reference speech quality assessment task. When using PESQ as optimization targets, the MSE, PLCC and SRCC reach 0.0389, 0.9695 and 0.9715, whereas when using STOI, these metrics reach 0.0019, 0.9608, and 0.9630, respectively",
    "checked": true,
    "id": "8e9a4eb13c54dbadee82df8389a683ef27f9c994",
    "semantic_title": "a no-reference speech quality assessment method based on neural network with densely connected convolutional architecture",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ta23_interspeech.html": {
    "title": "Probing Speech Quality Information in ASR Systems",
    "volume": "main",
    "abstract": "This paper investigates how intermediate speech representations in a state-of-the-art automatic speech recognition (ASR) system encode multi-dimensional speech quality, including MOS, Noisiness, Coloration, Discontinuity, and Loudness. We found that speech quality information is encoded in the ASR encoder layers at various levels but is still much richer than the Mel-spectrogram, an input widely used in previous works. This discovery inspires us to develop the Attentive Conformer with ASR pretraining, a novel deep learning model that enables the utilization of rich information from pretrained ASR models and the ability to focus on specific layers. Experiments on the NISQA speech quality assessment dataset demonstrate that the proposed model achieves state-of-the-art performance with significantly less training data",
    "checked": true,
    "id": "ffa5ed4bd8625dc803fe3de4e0c0f72752ff6df2",
    "semantic_title": "probing speech quality information in asr systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23d_interspeech.html": {
    "title": "Preference-based training framework for automatic speech quality assessment using deep neural network",
    "volume": "main",
    "abstract": "One objective of Speech Quality Assessment (SQA) is to estimate the ranks of synthetic speech systems. However, recent SQA models are typically trained using low-precision direct scores such as mean opinion scores (MOS) as the training objective, which is not straightforward to estimate ranking. Although it is effective for predicting quality scores of individual sentences, this approach does not account for speech and system preferences when ranking multiple systems. We propose a training framework of SQA models that can be trained with only preference scores derived from pairs of MOS to improve ranking prediction. Our experiment reveals conditions where our framework works the best in terms of pair generation, aggregation functions to derive system score from utterance preferences, and threshold functions to determine preference from a pair of MOS. Our results demonstrate that our proposed method significantly outperforms the baseline model in Spearman's Rank Correlation Coefficient",
    "checked": true,
    "id": "3c54bd226841a3994ab5239ffdc4b9031081d961",
    "semantic_title": "preference-based training framework for automatic speech quality assessment using deep neural network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/phatthiyaphaibun23_interspeech.html": {
    "title": "Crowdsourced Data Validation for ASR Training",
    "volume": "main",
    "abstract": "Many ASR engines are based on crowdsourced speech corpora, such as Common Voice. Although crowdsourced data is inexpensive, the utterances obtained from crowdsourcing can be noisy because of uncontrollable factors such as accents, environments, etc. Another issue with the Common Voice corpus is the lack of validators to cover a vast collection of crowdsourced utterances. This issue presents a significant challenge to speech data validation. To mitigate this bottleneck, we propose a machine-learning classifier that predicts the correctness of the data, which can act as either the validator itself or a prescreen for the validator. Our system achieves more than 95% F1-score in the three Common Voice languages, including Thai, Japanese, and Turkish, and performs even better when we have only one human judge involved in the decision. Furthermore, we also found that the data obtained from our method outperformed the current crowdsourcing validation method when used to train the ASR model",
    "checked": true,
    "id": "7dae6964be1b4dd1e254c7f2c2af88945b26c643",
    "semantic_title": "crowdsourced data validation for asr training",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huo23b_interspeech.html": {
    "title": "Re-investigating the Efficient Transfer Learning of Speech Foundation Model using Feature Fusion Methods",
    "volume": "main",
    "abstract": "Speech foundation models, pre-trained on large amounts of unsupervised or supervised audio data, have demonstrated an impressive ability to transfer their learning to specific domains for speech recognition. Parameter-efficient fine-tuning methods offer an efficient paradigm where a small set of parameters are updated to adapt the foundation model to new tasks. However, it is unclear how the intermediate features of the foundation model behave, and how to utilize them in a more efficient way. In this paper, we compare the performance of three speech foundation models for speech recognition. We re-investigate how features from different layers behave and propose a simple and effective feature fusion method for efficient transfer learning. Experimental results demonstrate that the proposed method uses 31.7% fewer trainable encoder parameters, 13.4% less computational memory cost than compared method, and does not compromise quality on the target task",
    "checked": true,
    "id": "d3ccf04f65bfa037227ebf1637e0b1bc9654ff59",
    "semantic_title": "re-investigating the efficient transfer learning of speech foundation model using feature fusion methods",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qi23_interspeech.html": {
    "title": "Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training",
    "volume": "main",
    "abstract": "Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (WAPAT). WAPAT use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, WAPAT utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of WAPAT on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-WAPAT outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art",
    "checked": true,
    "id": "1628c06f7b63c10c0aa78317aa9ca6c8da68198f",
    "semantic_title": "robust automatic speech recognition via wavaugment guided phoneme adversarial training",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23b_interspeech.html": {
    "title": "InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the baseline models",
    "checked": true,
    "id": "a86370b202b4856f32ba9db9ed10cb2ba4aca8e6",
    "semantic_title": "interformer: interactive local and global features fusion for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23_interspeech.html": {
    "title": "Transductive Feature Space Regularization for Few-shot Bioacoustic Event Detection",
    "volume": "main",
    "abstract": "In few-shot bioacoustic event detection, besides interested target events, background noises and various uninterested sound events lead to complex decision boundaries, which require regularized feature distributions in feature space. Due to the low label availability of uncertain noise events, existing few-shot learning methods with entropy-based regularizers suffer from overfitting during optimization. In this paper, we propose a transductive inference model with a prior knowledge based regularizer (PKR) to overcome the overfitting problem. We use a task-adaptive feature extractor to reconstruct a regularized feature space. A PKR is proposed to minimize the divergence between the original and reconstructed feature space. The development set of DCASE 2022 Task 5 is adopted as the experimental dataset. With the increasing iterations, the proposed model performs with long-lasting results around 55.43 F-measure, and well solves the overfitting problem in transductive inference",
    "checked": true,
    "id": "2a4b8260e63debd06db059f02b14fa36b603e7e7",
    "semantic_title": "transductive feature space regularization for few-shot bioacoustic event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23e_interspeech.html": {
    "title": "Incorporating L2 Phonemes Using Articulatory Features for Robust Speech Recognition",
    "volume": "main",
    "abstract": "The limited availability of non-native speech datasets presents a major challenge in automatic speech recognition (ASR) to narrow the performance gap between native and non-native speakers. To address this, the focus of this study is on the efficient incorporation of the L2 phonemes, which in this work refer to Korean phonemes, through articulatory feature analysis. This not only enables accurate modeling of pronunciation variants but also allows for the utilization of both native Korean and English speech datasets. We employ the lattice-free maximum mutual information (LF-MMI) objective in an end-to-end manner, to train the acoustic model to align and predict one of multiple pronunciation candidates. Experimental results show that the proposed method improves ASR accuracy for Korean L2 speech by training solely on L1 speech data. Furthermore, fine-tuning on L2 speech improves recognition accuracy for both L1 and L2 speech without performance trade-offs",
    "checked": true,
    "id": "64abb0112d24dc62105e302cf98af445651b86ab",
    "semantic_title": "incorporating l2 phonemes using articulatory features for robust speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/parcollet23_interspeech.html": {
    "title": "On the (In)Efficiency of Acoustic Feature Extractors for Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Speech representations learned with self-supervised learning (SSL) have the potential to significantly improve the performance of a number of audio applications, especially when availability of labeled data from the deployment domain is limited. Despite their successes, SSL training methods are compute- and memory-heavy, and require large investments in computing infrastructure, thus putting it out of the reach of most institutions. Therefore, building efficient model architectures is essential for the wide-scale adoption of SSL in speech technologies. CNN-based Acoustic Feature Extractors (AFE), which are widely used as encoders of acoustic waveforms, remain one of the main efficiency bottlenecks. This work proposes replacing CNN-based AFEs with more efficient ones and demonstrates that SSL pre-training time and memory consumption can be reduced by a factor of two to three over existing methods while preserving performances in speech-, command-, and speaker-recognition tasks",
    "checked": true,
    "id": "67377c8759a5a21ed27a8f6eee585dde1c3bd6a7",
    "semantic_title": "on the (in)efficiency of acoustic feature extractors for self-supervised speech representation learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tenbosch23_interspeech.html": {
    "title": "Phonemic competition in end-to-end ASR models",
    "volume": "main",
    "abstract": "Advanced end-to-end ASR systems encode speech signals by means of a multi-layer network architecture. In Wav2vec2.0, for example, a CNN is used as feature encoder on top of which transformer layers are used to map the high-dimensional CNN representations to the elements of some lexicon. Compared to the previous generation of 'modular' ASR systems it is much more difficult to interpret the processing and representations in an end-to-end system from a phonetic point of view. We built a Wav2vec2.0-based end-to-end system for producing broad phonetic transcriptions of Dutch. In this paper we investigate to what extent the CNN features and the representations on several transformer layers of a pre-trained and fine-tuned model reflect widely-shared phonetic knowledge. For that purpose we analyze distances between phones and the phonetic features of the most-activated phones in the output of an MLP classifier operating on the representations in several layers",
    "checked": true,
    "id": "294e96df613afb8a84c37ffabd183ba049cedd99",
    "semantic_title": "phonemic competition in end-to-end asr models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hughes23_interspeech.html": {
    "title": "Automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics",
    "volume": "main",
    "abstract": "Automatic Speaker Recognition (ASR) involves a complex range of processes to extract, model, and compare speaker-specific information from a pair of voice samples. Using heavily controlled recordings, this paper explores the impact of specific vocal conditions (i.e. vocal setting, disguise, accent guises) on ASR performance. When vocal conditions are matched, ASR performance is generally excellent (whisper is an exception). When conditions are mismatched, as in most forensic cases, we see an increase in discrimination and calibration error in some cases. The most problematic mismatches are those involving whisper and supralaryngeal vocal settings; these produce the greatest phonetic changes to speech. Mismatches involving high pitch also produce poor performance, although this appears to be driven by speaker-specific differences in articulatory implementation. We discuss the implications of the findings for the use of ASR in forensic casework and the interpretability of system output",
    "checked": true,
    "id": "199229d3865f23548de42544004502ffee6f843c",
    "semantic_title": "automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geiger23_interspeech.html": {
    "title": "Exploring Graph Theory Methods For the Analysis of Pronunciation Variation in Spontaneous Speech",
    "volume": "main",
    "abstract": "Given the development of automatic speech recognition based techniques for creating phonetic annotations of large speech corpora, there has been a growing interest in investigating the frequencies of occurrence of phonological and reduction processes. Given that most studies have analyzed these processes separately, they did not provide insights about their co-occurrences. This paper contributes with introducing graph theory methods for the analysis of pronunciation variation in a large corpus of Austrian German conversational speech. More specifically, we investigate how reduction processes that are typical for spontaneous German in general co-occur with phonological processes typical for the Austrian German variety. Whereas our concrete findings are of special interest to scientists investigating variation in German, the approach presented opens new possibilities to analyze pronunciation variation in large corpora of different speaking styles in any language",
    "checked": true,
    "id": "338f9ef7acb8d1b25b8eda135ec9e8fb710f8085",
    "semantic_title": "exploring graph theory methods for the analysis of pronunciation variation in spontaneous speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nuttall23_interspeech.html": {
    "title": "Automatic Speaker Recognition performance with matched and mismatched female bilingual speech data",
    "volume": "main",
    "abstract": "Validation of forensic voice comparison methods requires testing using speech samples that are representative of forensic casework conditions. Increasingly, around the world, forensic voice comparison casework is being undertaken using automatic speaker recognition (ASR) systems. However, multilingualism remains a key issue in applying automatic systems to forensic casework. This research aims to consider the effect of language on ASR performance, testing developers' claims of 'language independency'. Specifically, we examine the extent to which language mismatch either between the known and questioned samples, or between the evidential samples and the calibration data, affects overall system performance and the resulting strength of evidence (i.e., likelihood ratios for individual comparisons). Results indicate that mixed language trials produce more errors than single language trials which makes drawing evidential conclusions based on bilingual data challenging",
    "checked": true,
    "id": "1e58d3ff954b2690435fcb91d712ced6875d06a4",
    "semantic_title": "automatic speaker recognition performance with matched and mismatched female bilingual speech data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23x_interspeech.html": {
    "title": "FACTSpeech: Speaking a Foreign Language Pronunciation Using Only Your Native Characters",
    "volume": "main",
    "abstract": "Recent text-to-speech models have been requested to synthesize natural speech from language-mixed sentences because they are commonly used in real-world applications. However, most models do not consider transliterated words as input. When generating speech from transliterated text, it is not always natural to pronounce transliterated words as they are written, such as in the case of song titles. To address this issue, we introduce FACTSpeech, a system that can synthesize natural speech from transliterated text while allowing users to control the pronunciation between native and literal languages. Specifically, we propose a new language shift embedding to control the pronunciation of input text between native or literal pronunciation. Moreover, we leverage conditional instance normalization to improve pronunciation while preserving the speaker identity. The experimental results show that FACTSpeech generates native speech even from the sentences of transliterated form",
    "checked": true,
    "id": "8b0feedfec68ce2e39a5d9604b3b16fdfd3fcf07",
    "semantic_title": "factspeech: speaking a foreign language pronunciation using only your native characters",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23e_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model",
    "volume": "main",
    "abstract": "Phrase break prediction is a crucial task for improving the prosody naturalness of a text-to-speech (TTS) system. However, most proposed phrase break prediction models are monolingual, trained exclusively on a large amount of labeled data. In this paper, we address this issue for low-resource languages with limited labeled data using cross-lingual transfer. We investigate the effectiveness of zero-shot and few-shot cross-lingual transfer for phrase break prediction using a pre-trained multilingual language model. We use manually collected datasets in four Indo-European languages: one high-resource language and three with limited resources. Our findings demonstrate that cross-lingual transfer learning can be a particularly effective approach, especially in the few-shot setting, for improving performance in low-resource languages. This suggests that cross-lingual transfer can be inexpensive and effective for developing TTS front-end in resource-poor languages",
    "checked": true,
    "id": "07ecfeb52ff0768c3067cac5309d8150701d5906",
    "semantic_title": "cross-lingual transfer learning for phrase break prediction with multilingual language model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23d_interspeech.html": {
    "title": "DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "Although high-fidelity speech can be obtained for intralingual speech synthesis, cross-lingual text-to-speech (CTTS) is still far from satisfactory as it is difficult to accurately retain the speaker timbres (i.e. speaker similarity) and eliminate the accents from their first language (i.e. nativeness). In this paper, we demonstrated that vector-quantized (VQ) acoustic feature contains less speaker information than mel-spectrogram. Based on this finding, we propose a novel dual speaker embedding TTS (DSE-TTS) framework for CTTS with authentic speaking style. Here, one embedding is fed to the acoustic model to learn the linguistic speaking style, while the other one is integrated into the vocoder to mimic the target speaker's timbre. Experiments show that by combining both embeddings, DSE-TTS significantly outperforms the state-of-the-art SANE-TTS in cross-lingual synthesis, especially in terms of nativeness",
    "checked": true,
    "id": "45a04efc208faa3eb97c60b96dd07bbbb853f69c",
    "semantic_title": "dse-tts: dual speaker embedding for cross-lingual text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/markopoulos23_interspeech.html": {
    "title": "Generating Multilingual Gender-Ambiguous Text-to-Speech Voices",
    "volume": "main",
    "abstract": "The gender of any voice user interface is a key element of its perceived identity. Recently there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices",
    "checked": true,
    "id": "be0afc3192c002797d767930ce271fccab713471",
    "semantic_title": "generating multilingual gender-ambiguous text-to-speech voices",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/badlani23_interspeech.html": {
    "title": "RAD-MMM: Multilingual Multiaccented Multispeaker Text To Speech",
    "volume": "main",
    "abstract": "We create a multilingual speech synthesis system that can generate speech with a native accent in any seen language while retaining the characteristics of an individual's voice. It is expensive to obtain bilingual training data for a speaker and the lack of such data results in strong correlations that entangle speaker, language, and accent, resulting in poor transfer capabilities. To overcome this, we present RADMMM, a speech synthesis model based on RADTTS with explicit control over accent, language, speaker, and fine-grained F0 and energy features. Our proposed model does not rely on bilingual training data. We demonstrate an ability to control synthesized accent for any speaker in an open-source dataset comprising of 7 languages, with one native speaker per language. Human subjective evaluation demonstrates that, when compared to controlled baselines, our model better retains a speaker's voice and target accent, while synthesizing fluent speech in all target languages and accents in our dataset",
    "checked": true,
    "id": "3beba85b45c7ee6e567a8e44fbaa31a6867cd17b",
    "semantic_title": "rad-mmm: multilingual multiaccented multispeaker text to speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/comini23_interspeech.html": {
    "title": "Multilingual context-based pronunciation learning for Text-to-Speech",
    "volume": "main",
    "abstract": "Phonetic information and linguistic knowledge are an essential component of a Text-to-speech (TTS) front-end. Given a language, a lexicon can be collected offline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in order to predict the pronunciation for out-of-vocabulary (OOV) words. Additionally, post-lexical phonology, often defined in the form of rule-based systems, is used to correct pronunciation within or between words. In this work we showcase a multilingual unified front-end system that addresses any pronunciation related task, typically handled by separate modules. We evaluate the proposed model on G2P conversion and other language-specific challenges, such as homograph and polyphones disambiguation, post-lexical rules and implicit diacritization. We find that the multilingual model is competitive across languages and tasks, however, some trade-offs exists when compared to equivalent monolingual solutions",
    "checked": true,
    "id": "0d2776b80f93b96be8f4542ffdeb3a6a31f9b770",
    "semantic_title": "multilingual context-based pronunciation learning for text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23c_interspeech.html": {
    "title": "Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chou23_interspeech.html": {
    "title": "The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/malik23_interspeech.html": {
    "title": "A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alsenani23_interspeech.html": {
    "title": "Privacy Risks in Speech Emotion Recognition: A Systematic Study on Gender Inference Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tavernor23_interspeech.html": {
    "title": "Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ding23_interspeech.html": {
    "title": "Stable Speech Emotion Recognition with Head-k-Pooling Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gibson23b_interspeech.html": {
    "title": "A Personalised Speech Communication Application for Dysarthric Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23l_interspeech.html": {
    "title": "Video Multimodal Emotion Recognition System for Real World Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rohmatillah23_interspeech.html": {
    "title": "Promoting Mental Self-Disclosure in a Spoken Dialogue System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bujnowski23_interspeech.html": {
    "title": "Select language, modality or put on a mask!\" Experiments with Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/valentine23_interspeech.html": {
    "title": "My Vowels Matter: Formant Automation Tools for Diverse Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chongwhite23_interspeech.html": {
    "title": "NEMA: An Ecologically Valid Tool for Assessing Hearing Devices, Advanced Algorithms, and Communication in Diverse Listening Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ramanarayanan23_interspeech.html": {
    "title": "When Words Speak Just as Loudly as Actions: Virtual Agent Based Remote Health Assessment Integrating What Patients Say with What They Do",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/motepalli23_interspeech.html": {
    "title": "Stuttering Detection Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zusag23b_interspeech.html": {
    "title": "Providing Interpretable Insights for Neurological Speech and Cognitive Disorders from Interactive Serious Games",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/solinsky23_interspeech.html": {
    "title": "Automated Neural Nursing Assistant (ANNA): An Over-The-Phone System for Cognitive Monitoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gupta23b_interspeech.html": {
    "title": "5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raza23_interspeech.html": {
    "title": "Towards Two-point Neuron-inspired Energy-efficient Multimodal Open Master Hearing Aid",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23b_interspeech.html": {
    "title": "FC-MTLF: A Fine- and Coarse-grained Multi-Task Learning Framework for Cross-Lingual Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23c_interspeech.html": {
    "title": "C²A-SLU: Cross and Contrastive Attention for Improving ASR Robustness in Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/weld23_interspeech.html": {
    "title": "Tri-level Joint Natural Language Understanding for Multi-turn Conversational Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/laperriere23_interspeech.html": {
    "title": "Semantic Enrichment Towards Efficient Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kashiwagi23b_interspeech.html": {
    "title": "Tensor decomposition for minimization of E2E SLU model toward on-device processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mao23_interspeech.html": {
    "title": "DiffSLU: Knowledge Distillation Based Diffusion Model for Cross-Lingual Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arora23_interspeech.html": {
    "title": "Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23e_interspeech.html": {
    "title": "Contrastive Learning Based ASR Robust Knowledge Selection For Spoken Dialogue System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23f_interspeech.html": {
    "title": "Unsupervised Dialogue Topic Segmentation in Hyperdimensional Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cappellazzo23_interspeech.html": {
    "title": "An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23h_interspeech.html": {
    "title": "Enhancing New Intent Discovery via Robust Neighbor-based Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schwarz23_interspeech.html": {
    "title": "Personalized Predictive ASR for Latency Reduction in Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ray23_interspeech.html": {
    "title": "Compositional Generalization in Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ha_interspeech.html": {
    "title": "Sampling bias in NLU models: Impact and Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23d_interspeech.html": {
    "title": "5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23e_interspeech.html": {
    "title": "Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ga_interspeech.html": {
    "title": "WhiSLU: End-to-End Spoken Language Understanding with Whisper",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wen23b_interspeech.html": {
    "title": "Biophysically-inspired single-channel speech enhancement in the time domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jalal23_interspeech.html": {
    "title": "On-Device Speaker Anonymization of Acoustic Embeddings for ASR based on Flexible Location Gradient Reversal Layer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23b_interspeech.html": {
    "title": "How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23c_interspeech.html": {
    "title": "CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23e_interspeech.html": {
    "title": "A Two-stage Progressive Neural Network for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23_interspeech.html": {
    "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23r_interspeech.html": {
    "title": "Real-Time Personalised Speech Enhancement Transformers with Dynamic Cross-attended Speaker Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mamun23_interspeech.html": {
    "title": "CFTNet: Complex-valued Frequency Transformation Network for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23h_interspeech.html": {
    "title": "Feature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23c_interspeech.html": {
    "title": "Multi-mode Neural Speech Coding Based on Deep Generative Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bae23_interspeech.html": {
    "title": "Streaming Dual-Path Transformer for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kadkhodaeielyaderani23_interspeech.html": {
    "title": "Sequence-to-Sequence Multi-Modal Speech In-Painting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23q_interspeech.html": {
    "title": "Hybrid AHS: A Hybrid of Kalman Filter and Deep Learning for Acoustic Howling Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ho23_interspeech.html": {
    "title": "Differentially Private Adapters for Parameter Efficient Acoustic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23b_interspeech.html": {
    "title": "Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/uezu23_interspeech.html": {
    "title": "Consonant-emphasis Method Incorporating Robust Consonant-section Detection to Improve Intelligibility of Bone-conducted speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sato23_interspeech.html": {
    "title": "Downstream Task Agnostic Speech Enhancement with Self-Supervised Representation Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/byun23_interspeech.html": {
    "title": "Perceptual Improvement of Deep Neural Network (DNN) Speech Coder Using Parametric and Non-parametric Density Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23j_interspeech.html": {
    "title": "DeFT-AN RT: Real-time Multichannel Speech Enhancement using Dense Frequency-Time Attentive Network and Non-overlapping Synthesis Window",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23b_interspeech.html": {
    "title": "A More Accurate Internal Language Model Score Estimation for the Hybrid Autoregressive Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23_interspeech.html": {
    "title": "Attention Gate Between Capsules in Fully Capsule-Network Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rakib23_interspeech.html": {
    "title": "OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23g_interspeech.html": {
    "title": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23l_interspeech.html": {
    "title": "General-purpose Adversarial Training for Enhanced Automatic Speech Recognition Model Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23_interspeech.html": {
    "title": "Joint Instance Reconstruction and Feature Subspace Alignment for Cross-Domain Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moriya23_interspeech.html": {
    "title": "Knowledge Distillation for Neural Transducer-based Target-Speaker ASR: Exploiting Parallel Mixture/Single-Talker Speech Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23i_interspeech.html": {
    "title": "Random Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muthuchamyselvaraj23_interspeech.html": {
    "title": "Adapter Incremental Continual Learning of Efficient Audio Spectrogram Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23_interspeech.html": {
    "title": "Rethinking Speech Recognition with A Multimodal Perspective via Acoustic and Semantic Cooperative Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23b_interspeech.html": {
    "title": "Improving Code-Switching and Name Entity Recognition in ASR with Speech Editing based Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23h_interspeech.html": {
    "title": "Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lv23_interspeech.html": {
    "title": "DCCRN-KWS: An Audio Bias Based Model for Noise Robust Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23b_interspeech.html": {
    "title": "OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bleeker23_interspeech.html": {
    "title": "Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vandereeckt23_interspeech.html": {
    "title": "Rehearsal-Free Online Continual Learning for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23_interspeech.html": {
    "title": "Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23_interspeech.html": {
    "title": "Disentangling the Contribution of Non-native Speech in Automated Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ryu23_interspeech.html": {
    "title": "A Joint Model for Pronunciation Assessment and Mispronunciation Detection and Diagnosis with Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23d_interspeech.html": {
    "title": "Assessing Intelligibility in Non-native Speech: Comparing Measures Obtained at Different Levels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23_interspeech.html": {
    "title": "End-to-End Word-Level Pronunciation Assessment with MASK Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chao23_interspeech.html": {
    "title": "A Hierarchical Context-aware Modeling Approach for Multi-aspect and Multi-granular Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23j_interspeech.html": {
    "title": "Automatic Prediction of Language Learners' Listenability Using Speech and Text Features Extracted from Listening Drills",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shekar23b_interspeech.html": {
    "title": "Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-level Goodness of Pronunciation Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23f_interspeech.html": {
    "title": "Adapting an Unadaptable ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23c_interspeech.html": {
    "title": "Addressing Cold Start Problem for End-to-end Automatic Speech Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ribeiro23b_interspeech.html": {
    "title": "Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/richter23b_interspeech.html": {
    "title": "Orthography-based Pronunciation Scoring for Better CAPT Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23r_interspeech.html": {
    "title": "Zero-Shot Automatic Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huu23_interspeech.html": {
    "title": "Mispronunciation detection and diagnosis model for tonal language, applied to Vietnamese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dignum23_interspeech.html": {
    "title": "Beyond the AI hype: Balancing Innovation and Social Responsibility",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/stemmer23_interspeech.html": {
    "title": "Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matsuda23_interspeech.html": {
    "title": "Detection of Laughter and Screaming Using the Attention and CTC Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhattacharya23_interspeech.html": {
    "title": "Capturing Formality in Speech Across Domains and Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23e_interspeech.html": {
    "title": "Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feindt23_interspeech.html": {
    "title": "Cues to next-speaker projection in conversational Swedish: Evidence from reaction times",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/buker23_interspeech.html": {
    "title": "Multiple Instance Learning for Inference of Child Attachment From Paralinguistic Aspects of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eskimez23_interspeech.html": {
    "title": "Real-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23g_interspeech.html": {
    "title": "TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23s_interspeech.html": {
    "title": "MFT-CRN:Multi-scale Fourier Transform for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/guo23_interspeech.html": {
    "title": "Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/taherian23_interspeech.html": {
    "title": "Multi-input Multi-output Complex Spectral Mapping for Speaker Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oberhag23_interspeech.html": {
    "title": "Short-term Extrapolation of Speech Signals Using Recursive Neural Networks in the STFT Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pandey23_interspeech.html": {
    "title": "Listener sensitivity to deviating obstruents in WaveNet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23d_interspeech.html": {
    "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/camp23_interspeech.html": {
    "title": "MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23r_interspeech.html": {
    "title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/melnikleroy23_interspeech.html": {
    "title": "Can Better Perception Become a Disadvantage? Synthetic Speech Perception in Congenitally Blind Users",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cooper23_interspeech.html": {
    "title": "Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/he23_interspeech.html": {
    "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rajaa23_interspeech.html": {
    "title": "Improving End-to-End SLU performance with Prosodic Attention and Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23n_interspeech.html": {
    "title": "Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23c_interspeech.html": {
    "title": "Cross-Modal Semantic Alignment before Fusion for Two-Pass End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sunder23_interspeech.html": {
    "title": "ConvKT: Conversation-Level Knowledge Transfer for Context Aware End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23_interspeech.html": {
    "title": "GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23b_interspeech.html": {
    "title": "Obstructive Sleep Apnea Detection using Pre-trained Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23f_interspeech.html": {
    "title": "EEG-based Auditory Attention Detection with Spatiotemporal Graph and Graph Convolutional Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/beeson23_interspeech.html": {
    "title": "Silent Speech Recognition with Articulator Positions Estimated from Tongue Ultrasound and Lip Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23r_interspeech.html": {
    "title": "Auditory Attention Detection in Real-Life Scenarios Using Common Spatial Patterns from EEG",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23g_interspeech.html": {
    "title": "Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/csapo23_interspeech.html": {
    "title": "Towards Ultrasound Tongue Image prediction from EEG during speech production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/toth23_interspeech.html": {
    "title": "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/scheck23_interspeech.html": {
    "title": "STE-GAN: Speech-to-Electromyography Signal Conversion using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/salomons23_interspeech.html": {
    "title": "Spanish Phone Confusion Analysis for EMG-Based Silent Speech Interfaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23l_interspeech.html": {
    "title": "Hybrid Silent Speech Interface Through Fusion of Electroencephalography and Electromyography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarkar23_interspeech.html": {
    "title": "Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cai23b_interspeech.html": {
    "title": "Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data Using Contrastive Learning with Varying Pre-Training Domains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23_interspeech.html": {
    "title": "Background-aware Modeling for Weakly Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/srivastava23_interspeech.html": {
    "title": "How to (Virtually) Train Your Speaker Localizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghosh23b_interspeech.html": {
    "title": "MMER: Multimodal Multi-task Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/khandelwal23_interspeech.html": {
    "title": "A Multi-Task Learning Framework for Sound Event Detection using High-level Acoustic Characteristics of Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/neumann23b_interspeech.html": {
    "title": "A Multimodal Investigation of Speech, Text, Cognitive and Facial Video Features for Characterizing Depression With and Without Medication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/addlesee23_interspeech.html": {
    "title": "Understanding Disrupted Sentences Using Underspecified Abstract Meaning Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/field23_interspeech.html": {
    "title": "Developing Speech Processing Pipelines for Police Accountability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szekely23_interspeech.html": {
    "title": "Prosody-controllable Gender-ambiguous Speech Synthesis: A Tool for Investigating Implicit Bias in Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rouas23_interspeech.html": {
    "title": "Affective attributes of French caregivers' professional speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/casanova23_interspeech.html": {
    "title": "ASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23_interspeech.html": {
    "title": "Personality-aware Training based Speaker Adaptation for End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ito23b_interspeech.html": {
    "title": "Target Vocabulary Recognition Based on Multi-Task Learning with Decomposed Teacher Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shen23_interspeech.html": {
    "title": "Wave to Syntax: Probing spoken language models for syntax",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naowarat23_interspeech.html": {
    "title": "Effective Training of Attention-based Contextual Biasing Adapters with Synthetic Audio for Personalised ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23b_interspeech.html": {
    "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/haque23_interspeech.html": {
    "title": "SlothSpeech: Denial-of-service Attack Against Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23d_interspeech.html": {
    "title": "CLRL-Tuning: A Novel Continual Learning Approach for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23_interspeech.html": {
    "title": "Exploring Sources of Racial Bias in Automatic Speech Recognition through the Lens of Rhythmic Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23e_interspeech.html": {
    "title": "Can Contextual Biasing Remain Effective with Whisper and GPT-2?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niizumi23_interspeech.html": {
    "title": "Masked Modeling Duo for Speech: Specializing General-Purpose Audio Representation to Speech using Denoising Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23c_interspeech.html": {
    "title": "Improving RNN Transducer Acoustic Models for English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23_interspeech.html": {
    "title": "MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23h_interspeech.html": {
    "title": "Improving Chinese Mandarin Speech Recognition Using Semantic Graph Embedding Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23o_interspeech.html": {
    "title": "Adapting Multi-Lingual ASR Models for Handling Multiple Talkers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23c_interspeech.html": {
    "title": "Adapter-tuning with Effective Token-dependent Representation Shift for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23c_interspeech.html": {
    "title": "Model-Internal Slot-triggered Biasing for Domain Expansion in Neural Transducer ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yao23b_interspeech.html": {
    "title": "Delay-penalized CTC Implemented Based on Finite State Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23f_interspeech.html": {
    "title": "Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23u_interspeech.html": {
    "title": "Knowledge Distillation Approach for Efficient Internal Language Model Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/adhikary23_interspeech.html": {
    "title": "Language Model Personalization for Improved Touchscreen Typing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23b_interspeech.html": {
    "title": "Blank Collapse: Compressing CTC Emission for the Faster Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peyser23_interspeech.html": {
    "title": "Improving Joint Speech-Text Representations Without Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/flynn23_interspeech.html": {
    "title": "Leveraging Cross-Utterance Context For ASR Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23_interspeech.html": {
    "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tsunoo23_interspeech.html": {
    "title": "Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23e_interspeech.html": {
    "title": "A Neural Time Alignment Module for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23i_interspeech.html": {
    "title": "Accelerating Transducers through Adjacent Token Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23_interspeech.html": {
    "title": "Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23sa_interspeech.html": {
    "title": "Language-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23h_interspeech.html": {
    "title": "Embedding Articulatory Constraints for Low-resource Speech Recognition Based on Large Pre-trained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23b_interspeech.html": {
    "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/antonova23_interspeech.html": {
    "title": "SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bijwadia23_interspeech.html": {
    "title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gitman23_interspeech.html": {
    "title": "Confidence-based Ensembles of End-to-End Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chi23_interspeech.html": {
    "title": "Unsupervised Code-switched Text Generation from Parallel Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23b_interspeech.html": {
    "title": "A Binary Keyword Spotting System with Error-Diffusion Based Feature Binarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23b_interspeech.html": {
    "title": "Language-universal Phonetic Encoder for Low-resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23g_interspeech.html": {
    "title": "A Lexical-aware Non-autoregressive Transformer-based ASR Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vanvuren23_interspeech.html": {
    "title": "Improving Under-Resourced Code-Switched Speech Recognition: Large Pre-trained Models or Architectural Interventions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bellegarda23_interspeech.html": {
    "title": "Pragmatic Pertinence: A Learnable Confidence Metric to Assess the Subjective Quality of LM-Generated Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ea_interspeech.html": {
    "title": "ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharma23_interspeech.html": {
    "title": "BASS: Block-wise Adaptation for Speech Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shekar23_interspeech.html": {
    "title": "Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data: Fearless Steps Apollo-11 Audio Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yan23_interspeech.html": {
    "title": "Combining language corpora in a Japanese electromagnetic articulography database for acoustic-to-articulatory inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23g_interspeech.html": {
    "title": "A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chivriga23_interspeech.html": {
    "title": "Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/banerjee23_interspeech.html": {
    "title": "Enc-Dec RNN Acoustic Word Embeddings learned via Pairwise Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kotey23_interspeech.html": {
    "title": "Query Based Acoustic Summarization for Podcasts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23f_interspeech.html": {
    "title": "Spot Keywords From Very Noisy and Mixed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nayem23_interspeech.html": {
    "title": "Knowledge Distillation on Joint Task End-to-End Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23d_interspeech.html": {
    "title": "Investigating Pre-trained Audio Encoders in the Low-Resource Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23g_interspeech.html": {
    "title": "Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23m_interspeech.html": {
    "title": "Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/papadimitriou23_interspeech.html": {
    "title": "Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gonzalezmachorro23_interspeech.html": {
    "title": "Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rathod23_interspeech.html": {
    "title": "Whisper Features for Dysarthric Severity-Level Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tang23b_interspeech.html": {
    "title": "A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yue23_interspeech.html": {
    "title": "Dysarthric Speech Recognition, Detection and Classification using Raw Phase and Magnitude Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bayerl23_interspeech.html": {
    "title": "A Stutter Seldom Comes Alone – Cross-Corpus Stuttering Detection as a Multi-label Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhattacharjee23_interspeech.html": {
    "title": "Transfer Learning to Aid Dysarthria Severity Classification for Patients with Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23qa_interspeech.html": {
    "title": "DuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach with Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hedeshy23_interspeech.html": {
    "title": "CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baali23_interspeech.html": {
    "title": "Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kouzelis23_interspeech.html": {
    "title": "Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/novotny23_interspeech.html": {
    "title": "Glottal source analysis of voice deficits in basal ganglia dysfunction: evidence from de novo Parkinson's disease and Huntington's disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mun23b_interspeech.html": {
    "title": "An Analysis of Glottal Features of Chronic Kidney Disease Speech and Its Application to CKD Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/belagali23_interspeech.html": {
    "title": "Weakly supervised glottis segmentation in high-speed videoendoscopy using bounding box labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23k_interspeech.html": {
    "title": "An Efficient and Noise-Robust Audiovisual Encoder for Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23b_interspeech.html": {
    "title": "A Novel Self-training Approach for Low-resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23g_interspeech.html": {
    "title": "FunASR: A Fundamental End-to-End Speech Recognition Toolkit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23_interspeech.html": {
    "title": "Streaming Audio-Visual Speech Recognition with Alignment Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fernandezlopez23_interspeech.html": {
    "title": "SparseVSR: Lightweight and Noise Robust Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23c_interspeech.html": {
    "title": "Multimodal Speech Recognition for Language-Guided Embodied Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nishu23_interspeech.html": {
    "title": "Matching Latent Encoding for Audio-Text based Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/p23_interspeech.html": {
    "title": "Self-Paced Pattern Augmentation for Spoken Term Detection in Zero-Resource",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23y_interspeech.html": {
    "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/michieli23_interspeech.html": {
    "title": "Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23j_interspeech.html": {
    "title": "Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23t_interspeech.html": {
    "title": "Robust Keyword Spotting for Noisy Environments by Leveraging Speech Enhancement and Speech Presence Probability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23m_interspeech.html": {
    "title": "Enhancing the Unified Streaming and Non-streaming Model with Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23c_interspeech.html": {
    "title": "ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23_interspeech.html": {
    "title": "Improved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huybrechts23_interspeech.html": {
    "title": "DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23_interspeech.html": {
    "title": "Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23d_interspeech.html": {
    "title": "Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martel23_interspeech.html": {
    "title": "Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saijo23_interspeech.html": {
    "title": "Remixing-based Unsupervised Source Separation from Scratch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/okamoto23_interspeech.html": {
    "title": "CAPTDURE: Captioned Sound Dataset of Single Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/munakata23_interspeech.html": {
    "title": "Recursive Sound Source Separation with Deep Learning-based Beamforming for Unknown Number of Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mosner23_interspeech.html": {
    "title": "Multi-Channel Speech Separation with Cross-Attention and Beamforming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eom23_interspeech.html": {
    "title": "Background-Sound Controllable Voice Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/escobargrisales23_interspeech.html": {
    "title": "An Automatic Multimodal Approach to Analyze Linguistic and Acoustic Cues on Parkinson's Disease Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23_interspeech.html": {
    "title": "Personalization for Robust Voice Pathology Detection in Sound Waves",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23d_interspeech.html": {
    "title": "Integrated and Enhanced Pipeline System to Support Spoken Language Analytics for Screening Neurocognitive Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niu23b_interspeech.html": {
    "title": "Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23d_interspeech.html": {
    "title": "FTA-net: A Frequency and Time Attention Network for Speech Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fara23_interspeech.html": {
    "title": "Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23y_interspeech.html": {
    "title": "Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/campbell23_interspeech.html": {
    "title": "Classifying depression symptom severity: Assessment of speech representations in personalized and generalized machine learning models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghaffarzadegan23_interspeech.html": {
    "title": "Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pereztoro23_interspeech.html": {
    "title": "Automatic Assessment of Alzheimer's across Three Languages Using Speech and Language Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geng23_interspeech.html": {
    "title": "On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/svihlik23_interspeech.html": {
    "title": "Relationship between LTAS-based spectral moments and acoustic parameters of hypokinetic dysarthria in Parkinson's disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alvarado23_interspeech.html": {
    "title": "Respiratory distress estimation in human-robot interaction scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/reynerfuentes23_interspeech.html": {
    "title": "Prediction of the Gender-based Violence Victim Condition using Speech: What do Machine Learning Models rely on?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/charola23_interspeech.html": {
    "title": "Whisper Encoder features for Infant Cry Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jurov23_interspeech.html": {
    "title": "A neural architecture for selective attention to speech features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huo23_interspeech.html": {
    "title": "Quantifying Informational Masking due to Masker Intelligibility in Same-talker Speech-in-speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cuervo23_interspeech.html": {
    "title": "On the Benefits of Self-supervised Learned Speech Representations for Predicting Human Phonetic Misperceptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schulz23_interspeech.html": {
    "title": "Predicting Perceptual Centers Located at Vowel Onset in German Speech Using Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cooke23_interspeech.html": {
    "title": "Exploring the mutual intelligibility breakdown caused by sculpting speech from a competing speech signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kitahara23_interspeech.html": {
    "title": "Perception of Incomplete Voicing Neutralization of Obstruents in Tohoku Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pohnlein23_interspeech.html": {
    "title": "The emergence of obstruent-intrinsic f0 and VOT as cues to the fortis/lenis contrast in West Central Bavarian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/havard23_interspeech.html": {
    "title": "〈'〉 in Tsimane': a Preliminary Investigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hoffmann23_interspeech.html": {
    "title": "Segmental features of Brazilian (Santa Catarina) Hunsrik",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ratko23_interspeech.html": {
    "title": "Opening or Closing? An Electroglottographic Analysis of Voiceless Coda Consonants in Australian English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zebe23_interspeech.html": {
    "title": "Increasing aspiration of word-medial fortis plosives in Swiss Standard German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shao23_interspeech.html": {
    "title": "Lexical Stress and Velar Palatalization in Italian: A spatio-temporal Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23i_interspeech.html": {
    "title": "Speaker Embeddings as Individuality Proxy for Voice Stress Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23j_interspeech.html": {
    "title": "From Interval to Ordinal: A HMM based Approach for Emotion Label Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23l_interspeech.html": {
    "title": "Turbo your multi-modal classification with contrastive learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ioannides23_interspeech.html": {
    "title": "Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23y_interspeech.html": {
    "title": "SOT: Self-supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bansal23_interspeech.html": {
    "title": "On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23b_interspeech.html": {
    "title": "Speaking State Decoder with Transition Detection for Next Speaker Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kitagishi23_interspeech.html": {
    "title": "What are differences? Comparing DNN and Human by Their Performance and Characteristics in Speaker Age Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arts23_interspeech.html": {
    "title": "Effects of perceived gender on the perceived social function of laughter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/purohit23_interspeech.html": {
    "title": "Implicit phonetic information modeling for speech emotion recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/leem23_interspeech.html": {
    "title": "Computation and Memory Efficient Noise Adaptation of Wav2Vec2.0 for Noisy Speech Emotion Recognition with Skip Connection Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23b_interspeech.html": {
    "title": "Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naini23_interspeech.html": {
    "title": "Preference Learning Labels by Anchoring on Consecutive Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chetiaphukan23_interspeech.html": {
    "title": "Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23_interspeech.html": {
    "title": "Learning Local to Global Feature Aggregation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23q_interspeech.html": {
    "title": "Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pham23b_interspeech.html": {
    "title": "Vietnam-Celeb: a large-scale dataset for Vietnamese speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23v_interspeech.html": {
    "title": "What Can an Accent Identifier Learn? Probing Phonetic and Prosodic Information in a Wav2vec2-based Accent Identification Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23c_interspeech.html": {
    "title": "The 2022 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarni23_interspeech.html": {
    "title": "Description and analysis of the KPT system for NIST Language Recognition Evaluation 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yip23_interspeech.html": {
    "title": "ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yao23_interspeech.html": {
    "title": "Branch-ECAPA-TDNN: A Parallel Branch Architecture to Capture Local and Global Features for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23d_interspeech.html": {
    "title": "Speaker Verification Across Ages: Investigating Deep Speaker Embedding Sensitivity to Age Mismatch in Enrollment and Test Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dey23_interspeech.html": {
    "title": "Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/radhakrishnan23_interspeech.html": {
    "title": "A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tamayoflorez23_interspeech.html": {
    "title": "HABLA: A Dataset of Latin American Spanish Accents for Voice Anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23aa_interspeech.html": {
    "title": "Self-supervised Learning Representation based Accent Recognition with Persistent Accent Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23g_interspeech.html": {
    "title": "Extremely Low Bit Quantization for Mobile Speaker Verification Systems Under 1MB Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/das23_interspeech.html": {
    "title": "Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bredin23_interspeech.html": {
    "title": "pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23t_interspeech.html": {
    "title": "Model Compression for DNN-based Speaker Verification Using Weight Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vachhani23_interspeech.html": {
    "title": "Multi-resolution Approach to Identification of Spoken Languages and To Improve Overall Language Diarization System Using Whisper Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23_interspeech.html": {
    "title": "Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23b_interspeech.html": {
    "title": "Dynamic Fully-Connected Layer for Large-Scale Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schroter23b_interspeech.html": {
    "title": "DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burkhardt23_interspeech.html": {
    "title": "Nkululeko: Machine Learning Experiments on Speaker Characteristics Without Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lemaguer23_interspeech.html": {
    "title": "Sp1NY: A Quick and Flexible Speech Visualisation Tool in Python",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/corkey23_interspeech.html": {
    "title": "Intonation Control for Neural Text-to-Speech Synthesis with Polynomial Models of F0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szekely23b_interspeech.html": {
    "title": "So-to-Speak: An Exploratory Platform for Investigating the Interplay between Style and Prosody in TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arai23_interspeech.html": {
    "title": "Comparing /b/ and /d/ with a Single Physical Model of the Human Vocal Tract to Visualize Droplets Produced while Speaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ekstedt23b_interspeech.html": {
    "title": "Show & Tell: Voice Activity Projection and Turn-taking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cordourier23_interspeech.html": {
    "title": "Real Time Detection of Soft Voice for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tanna23_interspeech.html": {
    "title": "Data Augmentation for Diverse Voice Conversion in Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gogate23_interspeech.html": {
    "title": "Application for Real-time Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23d_interspeech.html": {
    "title": "Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23_interspeech.html": {
    "title": "Streaming Parrotron for on-device speech-to-speech conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shaheen23_interspeech.html": {
    "title": "Exploiting Emotion Information in Speaker Embeddings for Expressive Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/okamoto23b_interspeech.html": {
    "title": "E2E-S2S-VC: End-To-End Sequence-To-Sequence Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23f_interspeech.html": {
    "title": "DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baas23_interspeech.html": {
    "title": "Voice Conversion With Just Nearest Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tanaka23_interspeech.html": {
    "title": "CFVC: Conditional Filtering for Controllable Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ning23_interspeech.html": {
    "title": "DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23_interspeech.html": {
    "title": "Attention-based Interactive Disentangling Network for Instance-level Emotional Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23p_interspeech.html": {
    "title": "ALO-VC: Any-to-any Low-latency One-shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/minixhofer23_interspeech.html": {
    "title": "Evaluating and reducing the distance between synthetic and real speech distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/quamer23_interspeech.html": {
    "title": "Decoupling Segmental and Prosodic Cues of Non-native Speech through Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kanagawa23_interspeech.html": {
    "title": "VC-T: Streaming Voice Conversion Based on Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghosh23_interspeech.html": {
    "title": "Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion-Preserving Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23r_interspeech.html": {
    "title": "ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23e_interspeech.html": {
    "title": "Reverberation-Controllable Voice Conversion Using Reverberation Time Estimator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23d_interspeech.html": {
    "title": "Cross-utterance Conditioned Coherent Speech Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23o_interspeech.html": {
    "title": "MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23y_interspeech.html": {
    "title": "CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ca_interspeech.html": {
    "title": "Improving Zero-shot Cross-domain Slot Filling via Transformer-based Slot Semantics Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shin23_interspeech.html": {
    "title": "Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23c_interspeech.html": {
    "title": "Boosting Punctuation Restoration with Data Generation and Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23e_interspeech.html": {
    "title": "J-ToneNet: A Transformer-based Encoding Network for Improving Tone Classification in Continuous Speech via F0 Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/avila23_interspeech.html": {
    "title": "Towards Cross-Language Prosody Transfer for Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kesiraju23_interspeech.html": {
    "title": "Strategies for Improving Low Resource Speech to Text Translation Relying on Pre-trained ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koudounas23_interspeech.html": {
    "title": "ITALIC: An Italian Intent Classification Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rugayan23_interspeech.html": {
    "title": "Perceptual and Task-Oriented Assessment of a Semantic Metric for ASR Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23s_interspeech.html": {
    "title": "How ChatGPT is Robust for Spoken Language Understanding?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ye23b_interspeech.html": {
    "title": "GigaST: A 10,000-hour Pseudo Speech Translation Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fan23b_interspeech.html": {
    "title": "Boosting Chinese ASR Error Correction with Dynamic Error Scaling Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fallgren23_interspeech.html": {
    "title": "Crowdsource-based Validation of the Audio Cocktail as a Sound Browsing Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23z_interspeech.html": {
    "title": "PunCantonese: A Benchmark Corpus for Low-Resource Cantonese Punctuation Restoration from Speech Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kato23_interspeech.html": {
    "title": "Speech-to-Face Conversion Using Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nishikawa23_interspeech.html": {
    "title": "Inter-connection: Effective Connection between Pre-trained Encoder and Decoder for Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/radfar23_interspeech.html": {
    "title": "Conmer: Streaming Conformer Without Self-attention for Interactive Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23e_interspeech.html": {
    "title": "Intra-ensemble: A New Method for Combining Intermediate Outputs in Transformer-based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23b_interspeech.html": {
    "title": "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mai23_interspeech.html": {
    "title": "HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/carvalho23_interspeech.html": {
    "title": "Memory-augmented conformer for improved end-to-end long-form ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23_interspeech.html": {
    "title": "Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23o_interspeech.html": {
    "title": "An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23x_interspeech.html": {
    "title": "A Study on Visualization of Voiceprint Feature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yakovlev23_interspeech.html": {
    "title": "VoxTube: a multilingual speaker recognition dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23p_interspeech.html": {
    "title": "Visualizing Data Augmentation in Deep Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ba_interspeech.html": {
    "title": "Fast and Efficient Multilingual Self-Supervised Pre-training for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23z_interspeech.html": {
    "title": "UniSplice: Universal Cross-Lingual Data Splicing for Low-Resource ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/glocker23_interspeech.html": {
    "title": "Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23_interspeech.html": {
    "title": "Phonetic-assisted Multi-Target Units Modeling for Improving Conformer-Transducer ASR system",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rouditchenko23_interspeech.html": {
    "title": "Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ea_interspeech.html": {
    "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23b_interspeech.html": {
    "title": "Emotional Voice Conversion with Semi-Supervised Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23d_interspeech.html": {
    "title": "Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23_interspeech.html": {
    "title": "S2CD: Self-heuristic Speaker Content Disentanglement for Any-to-Any Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23g_interspeech.html": {
    "title": "Flow-VAE VC: End-to-End Flow Framework with Contrastive Loss for Zero-shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23s_interspeech.html": {
    "title": "Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23b_interspeech.html": {
    "title": "End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/braun23_interspeech.html": {
    "title": "Classifying Dementia in the Presence of Depression: A Cross-Corpus Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23b_interspeech.html": {
    "title": "Exploiting Cross-Domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wagner23_interspeech.html": {
    "title": "Multi-class Detection of Pathological Speech with Latent Features: How does it perform on unseen data?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kothare23_interspeech.html": {
    "title": "Responsiveness, Sensitivity and Clinical Utility of Timing-Related Speech Biomarkers for Remote Monitoring of ALS Disease Progression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geng23b_interspeech.html": {
    "title": "Use of Speech Impairment Severity for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mosuily23_interspeech.html": {
    "title": "MMLung: Moving Closer to Practical Lung Health Estimation using Smartphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23i_interspeech.html": {
    "title": "Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23pa_interspeech.html": {
    "title": "Non-uniform Speaker Disentanglement For Depression Detection From Raw Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/demir23_interspeech.html": {
    "title": "PoCaPNet: A Novel Approach for Surgical Phase Recognition Using Speech and X-Ray Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/neumann23_interspeech.html": {
    "title": "Combining Multiple Multimodal Speech Features into an Interpretable Index Score for Capturing Disease Progression in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mallolragolta23_interspeech.html": {
    "title": "The MASCFLICHT Corpus: Face Mask Type and Coverage Area Recognition from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/botelho23_interspeech.html": {
    "title": "Towards Reference Speech Characterization for Health Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/riosurrego23_interspeech.html": {
    "title": "Automatic Classification of Hypokinetic and Hyperkinetic Dysarthria based on GMM-Supervectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dineley23_interspeech.html": {
    "title": "Towards robust paralinguistic assessment for real-world mobile health (mHealth) monitoring: an initial study of reverberation effects on speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simmatis23_interspeech.html": {
    "title": "Multimodal Assessment of Bulbar Amyotrophic Lateral Sclerosis (ALS) Using a Novel Remote Speech Assessment App",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martinez23_interspeech.html": {
    "title": "On the Use of High Frequency Information for Voice Pathology Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/favaro23_interspeech.html": {
    "title": "Do Phonatory Features Display Robustness to Characterize Parkinsonian Speech Across Corpora?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kadiri23_interspeech.html": {
    "title": "Severity Classification of Parkinson's Disease from Speech using Single Frequency Filtering-based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simek23_interspeech.html": {
    "title": "Comparison of acoustic measures of dysphonia in Parkinson's disease and Huntington's disease: Effect of sex and speaking task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gomezzaragoza23_interspeech.html": {
    "title": "Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23c_interspeech.html": {
    "title": "LanSER: Language-Model Supported Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23_interspeech.html": {
    "title": "Fine-tuned RoBERTa Model with a CNN-LSTM Network for Conversational Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/stanley23_interspeech.html": {
    "title": "Emotion Label Encoding Using Word Embeddings for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ia_interspeech.html": {
    "title": "Discrimination of the Different Intents Carried by the Same Text Through Integrating Multimodal Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23f_interspeech.html": {
    "title": "Meta-domain Adversarial Contrastive Learning for Alleviating Individual Bias in Self-sentiment Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23b_interspeech.html": {
    "title": "SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23f_interspeech.html": {
    "title": "PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23b_interspeech.html": {
    "title": "Exploring the Interactions Between Target Positive and Negative Information for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/andreev23_interspeech.html": {
    "title": "Iterative autoregression: a novel trick to improve your low-latency speech enhancement model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ku23_interspeech.html": {
    "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/frenkel23_interspeech.html": {
    "title": "Domain Adaptation for Speech Enhancement in a Large Domain Gap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zadorozhnyy23_interspeech.html": {
    "title": "SCP-GAN: Self-Correcting Discriminator Optimization for Training Consistency Preserving Metric GAN on Speech Enhancement Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23c_interspeech.html": {
    "title": "A Mask Free Neural Network for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23p_interspeech.html": {
    "title": "A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pandey23b_interspeech.html": {
    "title": "A Simple RNN Model for Lightweight, Low-compute and Low-latency Multichannel Speech Enhancement in the Time Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23b_interspeech.html": {
    "title": "High Fidelity Speech Enhancement with Band-split RNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23b_interspeech.html": {
    "title": "Focus on the Sound around You: Monaural Target Speaker Extraction via Distance and Speaker Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kovalyov23_interspeech.html": {
    "title": "DFSNet: A Steerable Neural Beamformer Invariant to Microphone Array Configuration for Real-Time, Low-Latency Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23o_interspeech.html": {
    "title": "Speaker-Aware Anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/araki23_interspeech.html": {
    "title": "Impact of Residual Noise and Artifacts in Speech Enhancement Errors on Intelligibility of Human and Machine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sach23_interspeech.html": {
    "title": "EffCRN: An Efficient Convolutional Recurrent Network for High-Performance Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23e_interspeech.html": {
    "title": "HAD-ANC: A Hybrid System Comprising an Adaptive Filter and Deep Neural Networks for Active Noise Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chu23_interspeech.html": {
    "title": "MSAF: A Multiple Self-Attention Field Method for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23t_interspeech.html": {
    "title": "Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wan23_interspeech.html": {
    "title": "ABC-KD: Attention-Based-Compression Knowledge Distillation for Deep Learning-Based Noise Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/diener23_interspeech.html": {
    "title": "PLCMOS – A Data-driven Non-intrusive Metric for The Evaluation of Packet Loss Concealment Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wagner23b_interspeech.html": {
    "title": "Effects of Meter, Genre and Experience on Pausing, Lengthening and Prosodic Phrasing in German Poetry Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szalay23_interspeech.html": {
    "title": "Comparing first spectral moment of Australian English /s/ between straight and gay voices using three analysis window sizes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/taguchi23_interspeech.html": {
    "title": "Universal Automatic Phonetic Transcription into the International Phonetic Alphabet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gerlach23_interspeech.html": {
    "title": "Voice Twins: Discovering Extremely Similar-sounding, Unrelated Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hedegard23_interspeech.html": {
    "title": "Filling the population statistics gap: Swiss German reference data on F0 and speech tempo for forensic contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hutin23_interspeech.html": {
    "title": "Investigating the Syntax-Discourse Interface in the Phonetic Implementation of Discourse Markers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/essery23_interspeech.html": {
    "title": "Evaluation of a Forensic Automatic Speaker Recognition System with Emotional Speech Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahn23_interspeech.html": {
    "title": "An Outlier Analysis of Vowel Formants from a Corpus Phonetics Pipeline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qu23_interspeech.html": {
    "title": "The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/blaylock23_interspeech.html": {
    "title": "Beatboxing Kick Drum Kinematics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23b_interspeech.html": {
    "title": "Effects of hearing loss and amplification on Mandarin consonant perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/adams23_interspeech.html": {
    "title": "An Acoustic Analysis of Fricative Variation in Three Accents of English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bros23_interspeech.html": {
    "title": "Acoustic cues to stress perception in Spanish – a mismatch negativity study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sabev23_interspeech.html": {
    "title": "Bulgarian Unstressed Vowel Reduction: Received Views vs Corpus Findings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jain23b_interspeech.html": {
    "title": "An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23b_interspeech.html": {
    "title": "Identifying Stable Sections for Formant Frequency Extraction of French Nasal Vowels Based on Difference Thresholds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/audibert23_interspeech.html": {
    "title": "Evaluation of delexicalization methods for research on emotional speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kejriwal23b_interspeech.html": {
    "title": "Relationship between auditory and semantic entrainment using Deep Neural Networks (DNN)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kejriwal23_interspeech.html": {
    "title": "Unsupervised Auditory and Semantic Entrainment Models with Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nielsen23_interspeech.html": {
    "title": "Parsing dialog turns with prosodic features in English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muromachi23_interspeech.html": {
    "title": "Estimation of Listening Response Timing by Generative Model and Parameter Control of Response Substantialness Using Dynamic-Prompt-Tune",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chowdhury23_interspeech.html": {
    "title": "Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/buddi23_interspeech.html": {
    "title": "Efficient Multimodal Neural Networks for Trigger-less Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ostrand23_interspeech.html": {
    "title": "Rapid Lexical Alignment to a Conversational Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kurata23_interspeech.html": {
    "title": "Multimodal Turn-Taking Model Using Visual Cues for End-of-Utterance Prediction in Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hojo23_interspeech.html": {
    "title": "Audio-Visual Praise Estimation for Conversational Video based on Synchronization-Guided Multimodal Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sakuma23_interspeech.html": {
    "title": "Improving the response timing estimation for spoken dialogue systems by reducing the effect of speech recognition delay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23c_interspeech.html": {
    "title": "Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23la_interspeech.html": {
    "title": "A Multiple-Teacher Pruning Based Self-Distillation (MT-PSD) Approach to Model Compression for Audio-Visual Wake Word Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/spiesberger23_interspeech.html": {
    "title": "Abusive Speech Detection in Indic Languages Using Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ingle23_interspeech.html": {
    "title": "Listening To Silences In Contact Center Conversations Using Textual Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeen23_interspeech.html": {
    "title": "I Learned Error, I Can Fix It! : A Detector-Corrector Structure for ASR Error Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/garnier23b_interspeech.html": {
    "title": "Verbal and nonverbal feedback signals in response to increasing levels of miscommunication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/amiriparian23_interspeech.html": {
    "title": "Speech-Based Classification of Defensive Communication: A Novel Dataset and Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wallbridge23_interspeech.html": {
    "title": "Quantifying the perceptual value of lexical and non-lexical channels in speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tsubokura23_interspeech.html": {
    "title": "Relationships Between Gender, Personality Traits and Features of Multi-Modal Data to Responses to Spoken Dialog Systems Breakdown",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23e_interspeech.html": {
    "title": "Speaker-aware Cross-modal Fusion Architecture for Conversational Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liao23_interspeech.html": {
    "title": "Blind Estimation of Room Impulse Response from Monaural Reverberant Speech with Segmental Generative Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ren23_interspeech.html": {
    "title": "Emotion-Aware Audio-Driven Face Animation via Contrastive Feature Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shimonishi23_interspeech.html": {
    "title": "Anomalous Sound Detection Based on Sound Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fahed23_interspeech.html": {
    "title": "Random Forest Classification of Breathing Phases from Audio Signals Recorded using Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahn23b_interspeech.html": {
    "title": "GRAVO: Learning to Generate Relevant Audio from Visual Features with Noisy Online Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhai23_interspeech.html": {
    "title": "Wav2ToBI: a new approach to automatic ToBI transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23b_interspeech.html": {
    "title": "Joint-Former: Jointly Regularized and Locally Down-sampled Conformer for Semi-supervised Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/goel23_interspeech.html": {
    "title": "Towards Attention-based Contrastive Learning for Audio Spoof Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23d_interspeech.html": {
    "title": "Masked Audio Modeling with CLAP and Multi-Objective Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rusci23_interspeech.html": {
    "title": "Few-Shot Open-Set Learning for On-Device Customization of KeyWord Spotting Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/azeemi23_interspeech.html": {
    "title": "Self-Supervised Dataset Pruning for Efficient Training in Audio Anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23b_interspeech.html": {
    "title": "Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mariotte23_interspeech.html": {
    "title": "Multi-microphone Automatic Speech Segmentation in Meetings Based on Circular Harmonics Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23h_interspeech.html": {
    "title": "Advanced RawNet2 with Attention-based Channel Masking for Synthetic Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martinezsevilla23_interspeech.html": {
    "title": "Insights into end-to-end audio-to-score transcription with real recordings: A case study with saxophone works",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23d_interspeech.html": {
    "title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23b_interspeech.html": {
    "title": "Synthetic Voice Spoofing Detection based on Feature Pyramid Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23c_interspeech.html": {
    "title": "Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kerpicci23_interspeech.html": {
    "title": "Application of Knowledge Distillation to Multi-Task Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23g_interspeech.html": {
    "title": "DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/almudevar23_interspeech.html": {
    "title": "Variational Classifier for Unsupervised Anomalous Sound Detection under Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23c_interspeech.html": {
    "title": "FlexiAST: Flexibility is What AST Needs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23c_interspeech.html": {
    "title": "MCR-Data2vec 2.0: Improving Self-supervised Speech Pre-training via Model-level Consistency Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23l_interspeech.html": {
    "title": "Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ditthapron23_interspeech.html": {
    "title": "Masking Kernel for Learning Energy-Efficient Representations for Speaker Recognition and Mobile Health",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiang23_interspeech.html": {
    "title": "eSTImate: A Real-time Speech Transmission Index Estimator With Speech Enhancement Auxiliary Task Using Self-Attention Feature Pyramid Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23n_interspeech.html": {
    "title": "Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23b_interspeech.html": {
    "title": "Privacy-preserving Representation Learning for Speech Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panariello23_interspeech.html": {
    "title": "Vocoder drift in x-vector–based speaker anonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panariello23b_interspeech.html": {
    "title": "Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zaiem23b_interspeech.html": {
    "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23d_interspeech.html": {
    "title": "An extension of disentanglement metrics and its application to voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/abdullah23_interspeech.html": {
    "title": "An Information-Theoretic Analysis of Self-supervised Discrete Representations of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ashihara23_interspeech.html": {
    "title": "SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sasou23_interspeech.html": {
    "title": "Comparison of GIF- and SSL-based Features in Pathological-voice Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23c_interspeech.html": {
    "title": "What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/masumura23_interspeech.html": {
    "title": "End-to-End Joint Target and Non-Target Speakers ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23c_interspeech.html": {
    "title": "Improving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/makishima23_interspeech.html": {
    "title": "Joint Autoregressive Modeling of End-to-End Multi-Talker Overlapped Speech Recognition and Utterance-level Timestamp Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23_interspeech.html": {
    "title": "Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23_interspeech.html": {
    "title": "Multi-pass Training and Cross-information Fusion for Low-resource End-to-end Accented Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bataev23_interspeech.html": {
    "title": "Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23_interspeech.html": {
    "title": "Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23b_interspeech.html": {
    "title": "Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matsuura23_interspeech.html": {
    "title": "Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deshmukh23_interspeech.html": {
    "title": "Audio Retrieval with WavText5K and CLAP Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cappellazzo23b_interspeech.html": {
    "title": "Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chien23b_interspeech.html": {
    "title": "Contrastive Disentangled Learning for Memory-Augmented Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deseyssel23_interspeech.html": {
    "title": "ProsAudit, a prosodic benchmark for self-supervised speech models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23j_interspeech.html": {
    "title": "Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hallap23_interspeech.html": {
    "title": "Evaluating context-invariance in unsupervised speech representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23_interspeech.html": {
    "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23_interspeech.html": {
    "title": "Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23d_interspeech.html": {
    "title": "Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/paterson23_interspeech.html": {
    "title": "A Pipeline to Evaluate the Effects of Noise on Machine Learning Detection of Laryngeal Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ga_interspeech.html": {
    "title": "ReCLR: Reference-Enhanced Contrastive Learning of Audio Representation for Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/egaslopez23_interspeech.html": {
    "title": "Automated Multiple Sclerosis Screening Based on Encoded Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/melistas23_interspeech.html": {
    "title": "Cross-Lingual Features for Alzheimer's Dementia Detection from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zusag23_interspeech.html": {
    "title": "Careful Whisper - leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thienpondt23_interspeech.html": {
    "title": "Behavioral Analysis of Pathological Speaker Embeddings of Patients During Oncological Treatment of Oral Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23b_interspeech.html": {
    "title": "Adversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hsieh23_interspeech.html": {
    "title": "Adapter-Based Extension of Multi-Speaker Text-To-Speech Model for New Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sivaguru23_interspeech.html": {
    "title": "SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23k_interspeech.html": {
    "title": "UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dang23b_interspeech.html": {
    "title": "LightVoc: An Upsampling-Free GAN Vocoder Based On Conformer And Inverse Short-time Fourier Transform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saito23_interspeech.html": {
    "title": "ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23f_interspeech.html": {
    "title": "Human Transcription Quality Improvement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simantiraki23_interspeech.html": {
    "title": "The effect of masking noise on listeners' spectral tilt preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tranngoc23_interspeech.html": {
    "title": "The Effect of Whistled Vowels on Whistled Word Categorization for Naive Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bharati23_interspeech.html": {
    "title": "Automatic Deep Neural Network-Based Segmental Pronunciation Error Detection of L2 English Speech (L1 Bengali)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hao23_interspeech.html": {
    "title": "The effect of stress on Mandarin tonal perception in continuous speech for Spanish-speaking learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elmerich23_interspeech.html": {
    "title": "Combining acoustic and aerodynamic data collection: A perceptual evaluation of acoustic distortions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elie23b_interspeech.html": {
    "title": "Estimating virtual targets for lingual stop consonants using general Tau theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gibson23_interspeech.html": {
    "title": "Using Random Forests to classify language as a function of syllable timing in two groups: children with cochlear implants and with normal hearing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23w_interspeech.html": {
    "title": "An Improved End-to-End Audio-Visual Speech Recognition Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wesoek23_interspeech.html": {
    "title": "What influences the foreign accent strength? Phonological and grammatical errors in the perception of accentedness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huttner23_interspeech.html": {
    "title": "Investigating the Perception Production Link through Perceptual Adaptation and Phonetic Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23f_interspeech.html": {
    "title": "Emotion Prompting for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chin23_interspeech.html": {
    "title": "Speech-in-Speech Recognition is Modulated by Familiarity to Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23m_interspeech.html": {
    "title": "BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/miodonska23_interspeech.html": {
    "title": "Are retroflex-to-dental sibilant substitutions in Polish children's speech an example of a covert contrast? A preliminary acoustic study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23i_interspeech.html": {
    "title": "Reversible Neural Networks for Memory-Efficient Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23f_interspeech.html": {
    "title": "ECAPA++: Fine-grained Deep Embedding Learning for TDNN Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23w_interspeech.html": {
    "title": "TO-Rawnet: Improving RawNet with TCN and Orthogonal Regularization for Fake Audio Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zuo23_interspeech.html": {
    "title": "Fooling Speaker Identification Systems with Adversarial Background Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23r_interspeech.html": {
    "title": "Mutual Information-based Embedding Decoupling for Generalizable Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23c_interspeech.html": {
    "title": "Target Active Speaker Detection with Audio-visual Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/broughton23_interspeech.html": {
    "title": "Improving End-to-End Neural Diarization Using Conversational Summary Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zang23_interspeech.html": {
    "title": "Phase perturbation improves channel robustness for speech spoofing countermeasures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bousquet23_interspeech.html": {
    "title": "Improving training datasets for resource-constrained speaker recognition neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lertpetchpun23_interspeech.html": {
    "title": "Instance-based Temporal Normalization for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/novoselov23_interspeech.html": {
    "title": "On the robustness of wav2vec 2.0 based speaker recognition systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23i_interspeech.html": {
    "title": "P-vectors: A Parallel-coupled TDNN/Transformer Network for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lei23_interspeech.html": {
    "title": "Group GMM-ResNet for Detection of Synthetic Speech Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fang23_interspeech.html": {
    "title": "Robust Training for Speaker Verification against Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jeoung23_interspeech.html": {
    "title": "Self-Distillation into Self-Attention Heads for Improving Transformer-based End-to-End Neural Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23m_interspeech.html": {
    "title": "Build a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benamor23_interspeech.html": {
    "title": "Describing the phonetics in the underlying speech attributes for deep and interpretable speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23v_interspeech.html": {
    "title": "Range-Based Equal Error Rate for Spoof Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tabassum23_interspeech.html": {
    "title": "Exploring the English Accent-independent Features for Speech Emotion Recognition using Filter and Wrapper-based Methods for Feature Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/plaquet23_interspeech.html": {
    "title": "Powerset multi-class cross entropy loss for neural speaker diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23_interspeech.html": {
    "title": "A Method of Audio-Visual Person Verification by Mining Connections between Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fish23_interspeech.html": {
    "title": "A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23_interspeech.html": {
    "title": "Modeling Dependent Structure for Utterances in ASR Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/verma23_interspeech.html": {
    "title": "ASR for Low Resource and Multilingual Noisy Code-Mixed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23b_interspeech.html": {
    "title": "Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mateju23_interspeech.html": {
    "title": "Combining Multilingual Resources and Models to Develop State-of-the-Art E2E ASR for Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23o_interspeech.html": {
    "title": "Two Stage Contextual Word Filtering for Context Bias in Unified Streaming and Non-streaming Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pham23_interspeech.html": {
    "title": "Towards continually learning new languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23e_interspeech.html": {
    "title": "N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23g_interspeech.html": {
    "title": "SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gulzar23_interspeech.html": {
    "title": "miniStreamer: Enhancing Small Conformer with Chunked-Context Masking for Streaming ASR Applications on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23p_interspeech.html": {
    "title": "CoMFLP: Correlation Measure Based Fast Search on ASR Layer Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23h_interspeech.html": {
    "title": "Exploration on HuBERT with Multiple Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23s_interspeech.html": {
    "title": "Quantization-aware and Tensor-compressed Training of Transformers for Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naowarat23b_interspeech.html": {
    "title": "Word-level Confidence Estimation for CTC Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kulshreshtha23_interspeech.html": {
    "title": "Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23_interspeech.html": {
    "title": "Unsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23_interspeech.html": {
    "title": "4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yen23_interspeech.html": {
    "title": "Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fan23_interspeech.html": {
    "title": "Language-specific Boundary Learning for Improving Mandarin-English Code-switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23c_interspeech.html": {
    "title": "Mixture-of-Expert Conformer for Streaming Multilingual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23x_interspeech.html": {
    "title": "Lossless 4-bit Quantization of Architecture Compressed Conformer ASR Systems on the 300-hr Switchboard Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23c_interspeech.html": {
    "title": "Compressed MoE ASR Model Based on Knowledge Distillation and Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deng23b_interspeech.html": {
    "title": "Factorised Speaker-environment Adaptive Training of Conformer Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23aa_interspeech.html": {
    "title": "Text Only Domain Adaptation with Phoneme Guided Data Splicing for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cahyawijaya23_interspeech.html": {
    "title": "Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23fa_interspeech.html": {
    "title": "Modular Domain Adaptation for Conformer-Based Streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhatia23_interspeech.html": {
    "title": "Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23f_interspeech.html": {
    "title": "SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mori23_interspeech.html": {
    "title": "A Generative Framework for Conversational Laughter: Its 'Language Model' and Laughter Sound Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ba_interspeech.html": {
    "title": "Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lameris23_interspeech.html": {
    "title": "Beyond Style: Synthesizing Speech with Pragmatic Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/abbas23_interspeech.html": {
    "title": "eCat: An End-to-End Model for Multi-Speaker TTS & Many-to-Many Fine-Grained Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deb23_interspeech.html": {
    "title": "BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kashiwagi23_interspeech.html": {
    "title": "Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jakubiak23_interspeech.html": {
    "title": "Whistle-to-text: Automatic recognition of the Silbo Gomero whistled language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23c_interspeech.html": {
    "title": "A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nortje23_interspeech.html": {
    "title": "Visually grounded few-shot word acquisition with fewer shots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23_interspeech.html": {
    "title": "JAMFN: Joint Attention Multi-Scale Fusion Network for Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23z_interspeech.html": {
    "title": "Prompt Guided Copy Mechanism for Conversational Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/faustini23_interspeech.html": {
    "title": "Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23c_interspeech.html": {
    "title": "On Monotonic Aggregation for Open-domain QA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nguyen23b_interspeech.html": {
    "title": "Question-Context Alignment and Answer-Context Dependencies for Effective Answer Sentence Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23v_interspeech.html": {
    "title": "Multi-Scale Attention for Audio Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23f_interspeech.html": {
    "title": "Enhancing Visual Question Answering via Deconstructing Questions and Explicating Answers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23c_interspeech.html": {
    "title": "SEF-Net: Speaker Embedding Free Target Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rose23_interspeech.html": {
    "title": "Cascaded encoders for fine-tuning ASR models on overlapped speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/erdogan23_interspeech.html": {
    "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23b_interspeech.html": {
    "title": "Unified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahmadikalkhorani23_interspeech.html": {
    "title": "Time-domain Transformer-based Audiovisual Speaker Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/delcroix23_interspeech.html": {
    "title": "Multi-Stream Extension of Variational Bayesian HMM Clustering (MS-VBx) for Combined End-to-End and Vector Clustering-based Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niu23_interspeech.html": {
    "title": "Unsupervised Adaptation with Quality-Aware Masking to Improve Target-Speaker Voice Activity Detection for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23e_interspeech.html": {
    "title": "BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23e_interspeech.html": {
    "title": "Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gaultier23_interspeech.html": {
    "title": "Joint compensation of multi-talker noise and reverberation for speech enhancement with cochlear implants using one or more microphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yousefi23_interspeech.html": {
    "title": "Speaker Diarization for ASR Output with T-vectors: A Sequence Classification Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raj23_interspeech.html": {
    "title": "GPU-accelerated Guided Source Separation for Meeting Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23c_interspeech.html": {
    "title": "Overlap Aware Continuous Speech Separation without Permutation Invariant Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23w_interspeech.html": {
    "title": "Weakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23j_interspeech.html": {
    "title": "Directional Speech Recognition for Speaker Disambiguation and Cross-talk Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/berger23_interspeech.html": {
    "title": "Mixture Encoder for Joint Speech Separation and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hejna23_interspeech.html": {
    "title": "Aberystwyth English Pre-aspiration in Apparent Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23c_interspeech.html": {
    "title": "Speech Entrainment in Chinese Story-Style Talk Shows: The Interaction Between Gender and Role",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/steiner23_interspeech.html": {
    "title": "Sociodemographic and Attitudinal Effects on Dialect Speakers' Articulation of the Standard Language: Evidence from German-Speaking Switzerland",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burridge23_interspeech.html": {
    "title": "Vowel Normalisation in Latent Space for Sociolinguistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23n_interspeech.html": {
    "title": "Attention-based Encoder-Decoder Network for End-to-End Neural Speaker Diarization with Target Speaker Attractor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lahiri23_interspeech.html": {
    "title": "Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baghel23_interspeech.html": {
    "title": "The DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/paturi23_interspeech.html": {
    "title": "Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pirlogeanu23_interspeech.html": {
    "title": "The SpeeD--ZevoTech submission at DISPLACE 2023",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23g_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with Absolute Speaker Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23d_interspeech.html": {
    "title": "A Context-Constrained Sentence Modeling for Deception Detection in Real Interrogation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23c_interspeech.html": {
    "title": "MetricAug: A Distortion Metric-Lead Augmentation Strategy for Training Noise-Robust Speech Emotion Recognizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ludusan23_interspeech.html": {
    "title": "The co-use of laughter and head gestures across speech styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23d_interspeech.html": {
    "title": "EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23b_interspeech.html": {
    "title": "Pre-Finetuning for Few-Shot Emotional Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23_interspeech.html": {
    "title": "Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lavania23_interspeech.html": {
    "title": "Utility-Preserving Privacy-Enabled Speech Embeddings for Emotion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burdisso23_interspeech.html": {
    "title": "Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/branco23_interspeech.html": {
    "title": "Laughter in task-based settings: whom we talk to affects how, when, and how often we laugh",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fang23b_interspeech.html": {
    "title": "Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deoliveira23_interspeech.html": {
    "title": "Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23d_interspeech.html": {
    "title": "Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thakran23_interspeech.html": {
    "title": "Investigating Acoustic Cues for Multilingual Abuse Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23c_interspeech.html": {
    "title": "A novel frequency warping scale for speech emotion recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23m_interspeech.html": {
    "title": "Multi-Scale Temporal Transformer For Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grageda23_interspeech.html": {
    "title": "Distant Speech Emotion Recognition in an Indoor Human-robot Interaction Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tao23b_interspeech.html": {
    "title": "A Study on Prosodic Entrainment in Relation to Therapist Empathy in Counseling Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/minematsu23_interspeech.html": {
    "title": "A Unified Framework to Improve Learners' Skills of Perception and Production Based on Speech Shadowing and Overlapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nicholls23_interspeech.html": {
    "title": "Speak & Improve: L2 English Speaking Practice Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nicolao23_interspeech.html": {
    "title": "Measuring prosody in child speech using SoapBox Fluency API",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nissen23_interspeech.html": {
    "title": "Teaching Non-native Sound Contrasts using Visual Biofeedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/walsh23_interspeech.html": {
    "title": "Large-Scale Automatic Audiobook Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elkheir23_interspeech.html": {
    "title": "QVoice: Arabic Speech Pronunciation Learning Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/svec23_interspeech.html": {
    "title": "Asking Questions: an Innovative Way to Interact with Oral History Archives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhat23_interspeech.html": {
    "title": "DisfluencyFixer: A tool to enhance Language Learning through Speech To Speech Disfluency Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/prakash23_interspeech.html": {
    "title": "Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elshahawy23_interspeech.html": {
    "title": "MyVoice: Arabic Speech Resource Collaboration Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hromada23_interspeech.html": {
    "title": "Personal Primer Prototype 1: Invitation to Make Your Own Embooked Speech-Based Educational Artifact",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deng23_interspeech.html": {
    "title": "Time-frequency Domain Filter-and-sum Network for Multi-channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23h_interspeech.html": {
    "title": "Audio-Visual Fusion using Multiscale Temporal Convolutional Attention for Time-Domain Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ca_interspeech.html": {
    "title": "An Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/phokhinanan23_interspeech.html": {
    "title": "Binaural Sound Localization in Noisy Environments Using Frequency-Based Audio Vision Transformer (FAViT)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23i_interspeech.html": {
    "title": "Contrastive Learning based Deep Latent Masking for Music Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23k_interspeech.html": {
    "title": "Speaker Extraction with Detection of Presence and Absence of Target Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23k_interspeech.html": {
    "title": "PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarabia23_interspeech.html": {
    "title": "Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23q_interspeech.html": {
    "title": "Image-driven Audio-visual Universal Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fras23_interspeech.html": {
    "title": "Joint Blind Source Separation and Dereverberation for Automatic Speech Recognition using Delayed-Subsource MNMF with Localization Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23na_interspeech.html": {
    "title": "SDNet: Stream-attention and Dual-feature Learning Network for Ad-hoc Array Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baek23_interspeech.html": {
    "title": "Deeply Supervised Curriculum Learning for Deep Neural Network-based Sound Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fujimura23_interspeech.html": {
    "title": "Multi-channel separation of dynamic speech and sound events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ja_interspeech.html": {
    "title": "Rethinking the Visual Cues in Audio-Visual Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dang23_interspeech.html": {
    "title": "Using Semi-supervised Learning for Monaural Time-domain Speech Separation with a Self-supervised Learning-based SI-SNR Estimator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23d_interspeech.html": {
    "title": "Investigation of Training Mute-Expressive End-to-End Speech Separation Networks for an Unknown Number of Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cho23_interspeech.html": {
    "title": "SR-SRP: Super-Resolution based SRP-PHAT for Sound Source Localization and Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23g_interspeech.html": {
    "title": "Dual-Memory Multi-Modal Learning for Continual Spoken Keyword Spotting with Confidence Selection and Diversity Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23j_interspeech.html": {
    "title": "FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23g_interspeech.html": {
    "title": "A Neural State-Space Modeling Approach to Efficient Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23c_interspeech.html": {
    "title": "Locate and Beamform: Two-dimensional Locating All-neural Beamformer for Multi-channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23f_interspeech.html": {
    "title": "Monaural Speech Separation Method Based on Recurrent Attention with Parallel Branches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23m_interspeech.html": {
    "title": "Ontology-aware Learning and Evaluation for Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23c_interspeech.html": {
    "title": "Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lay23_interspeech.html": {
    "title": "Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muller23_interspeech.html": {
    "title": "Complex-valued neural networks for voice anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ristea23_interspeech.html": {
    "title": "DeepVQE: Real Time Deep Voice Quality Enhancement for Joint Acoustic Echo Cancellation, Noise Suppression and Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sawata23_interspeech.html": {
    "title": "Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23h_interspeech.html": {
    "title": "HD-DEMUCS: General Speech Restoration with Heterogeneous Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23e_interspeech.html": {
    "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yin23_interspeech.html": {
    "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23x_interspeech.html": {
    "title": "Detection of Cross-Dataset Fake Audio Based on Prosodic and Pronunciation Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dowerah23_interspeech.html": {
    "title": "Self-supervised learning with Diffusion-based multichannel speech enhancement for speaker verification under noisy conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nespoli23_interspeech.html": {
    "title": "Two-Stage Voice Anonymization for Enhanced Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23h_interspeech.html": {
    "title": "Personalized Dereverberation of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/binhthien23_interspeech.html": {
    "title": "Weighted Von Mises Distribution-based Loss Function for Real-time STFT Phase Reconstruction Using DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schroter23_interspeech.html": {
    "title": "Deep Multi-Frame Filtering for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiong23_interspeech.html": {
    "title": "Aligning Speech Enhancement for Improving Downstream Classification Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23b_interspeech.html": {
    "title": "DNN-based Parameter Estimation for MVDR Beamforming and Post-filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23b_interspeech.html": {
    "title": "FRA-RIR: Fast Random Approximation of the Image-source Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23b_interspeech.html": {
    "title": "Rethinking Complex-Valued Deep Neural Networks for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/le23_interspeech.html": {
    "title": "Harmonic enhancement using learnable comb filter for light-weight full-band speech enhancement model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23e_interspeech.html": {
    "title": "How Does Pretraining Improve Discourse-Aware Translation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23t_interspeech.html": {
    "title": "PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tseng23_interspeech.html": {
    "title": "Model-assisted Lexical Tone Evaluation of three-year-old Chinese-speaking Children by also Considering Segment Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23b_interspeech.html": {
    "title": "Sentence Embedder Guided Utterance Encoder (SEGUE) for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23u_interspeech.html": {
    "title": "Joint Time and Frequency Transformer for Chinese Opera Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23_interspeech.html": {
    "title": "AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arvan23_interspeech.html": {
    "title": "Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pouranbenveyseh23_interspeech.html": {
    "title": "Combining Heterogeneous Structures for Event Causality Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/biswas23_interspeech.html": {
    "title": "An Efficient Approach for the Automated Segmentation and Transcription of the People's Speech Sorpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23g_interspeech.html": {
    "title": "Diverse Feature Mapping and Fusion via Multitask Learning for Multilingual Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bahar23_interspeech.html": {
    "title": "Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23e_interspeech.html": {
    "title": "Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23j_interspeech.html": {
    "title": "Efficient Adaptation of Spoken Language Understanding based on End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23d_interspeech.html": {
    "title": "PhonMatchNet: Phoneme-Guided Zero-Shot Keyword Spotting for User-Defined Keywords",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23_interspeech.html": {
    "title": "Mix before Align: Towards Zero-shot Cross-lingual Sentiment Analysis via Soft-Mix and Multi-View Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/papi23_interspeech.html": {
    "title": "AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/polak23_interspeech.html": {
    "title": "Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sikasote23_interspeech.html": {
    "title": "Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mun23_interspeech.html": {
    "title": "Towards Single Integrated Spoofing-aware Speaker Verification Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ba_interspeech.html": {
    "title": "Pseudo-Siamese Network based Timbre-reserved Black-box Adversarial Attack in Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23v_interspeech.html": {
    "title": "Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23v_interspeech.html": {
    "title": "Robust Audio Anti-spoofing Countermeasure with Joint Training of Front-end and Back-end Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kawa23b_interspeech.html": {
    "title": "Improved DeepFake Detection Using Whisper Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23c_interspeech.html": {
    "title": "DoubleDeceiver: Deceiving the Speaker Verification System Protected by Spoofing Countermeasures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panchapagesan23_interspeech.html": {
    "title": "On Training a Neural Residual Acoustic Echo Suppressor for Improved ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lemercier23_interspeech.html": {
    "title": "Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23b_interspeech.html": {
    "title": "UnSE: Unsupervised Speech Enhancement Using Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23k_interspeech.html": {
    "title": "MC-SpEx: Towards Effective Speaker Extraction with Multi-Scale Interfusion and Conditional Speaker Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bartolewska23_interspeech.html": {
    "title": "Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23q_interspeech.html": {
    "title": "Gesper: A Restoration-Enhancement Framework for General Speech Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ryumina23_interspeech.html": {
    "title": "Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pudo23_interspeech.html": {
    "title": "MOCKS 1.0: Multilingual Open Custom Keyword Spotting Testset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eisenstein23_interspeech.html": {
    "title": "MD3: The Multi-Dialect Dataset of Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/anwar23_interspeech.html": {
    "title": "MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/suwanbandit23_interspeech.html": {
    "title": "Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23d_interspeech.html": {
    "title": "HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bekal23_interspeech.html": {
    "title": "A Metric-Driven Approach to Conformer Layer Pruning for Efficient ASR Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gurunathshivakumar23_interspeech.html": {
    "title": "Distillation Strategies for Discriminative Speech Recognition Rescoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pouthier23_interspeech.html": {
    "title": "Another Point of View on Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23e_interspeech.html": {
    "title": "RASR2: The RWTH ASR Toolkit for Generic Sequence-to-sequence Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/filimonov23_interspeech.html": {
    "title": "Streaming Speech-to-Confusion Network Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23d_interspeech.html": {
    "title": "Accurate and Structured Pruning for Efficient Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chua23_interspeech.html": {
    "title": "MERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gupta23_interspeech.html": {
    "title": "Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shahin23_interspeech.html": {
    "title": "Improving wav2vec2-based Spoken Language Identification by Learning Phonological Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/praveen23_interspeech.html": {
    "title": "Language Identification Networks for Multilingual Everyday Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/styles23_interspeech.html": {
    "title": "Investigating model performance in language identification: beyond simple error statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kodali23_interspeech.html": {
    "title": "Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kathan23_interspeech.html": {
    "title": "The effect of clinical intervention on the speech of individuals with PTSD: features and recognition performances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/triantafyllopoulos23_interspeech.html": {
    "title": "Analysis and automatic prediction of exertion from speech: Contrasting objective and subjective measures collected while running",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tao23_interspeech.html": {
    "title": "The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eni23_interspeech.html": {
    "title": "Comparing Hand-Crafted Features to Spectrograms for Autism Severity Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mijnders23_interspeech.html": {
    "title": "Acoustic characteristics of depression in older adults' speech: the role of covariates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23b_interspeech.html": {
    "title": "Dual Transformer Decoder based Features Fusion Network for Automated Audio Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pellegrini23_interspeech.html": {
    "title": "Adapting a ConvNeXt Model to Audio Classification on AudioSet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23w_interspeech.html": {
    "title": "Few-shot Class-incremental Audio Classification Using Stochastic Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23d_interspeech.html": {
    "title": "Enhance Temporal Relations in Audio Captioning with Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23s_interspeech.html": {
    "title": "First Language Effects on Second Language Perception: Evidence from English Low-vowel Nasal Sequences Perceived by L1 Mandarin Chinese Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/maity23_interspeech.html": {
    "title": "Motor Control Similarity Between Speakers Saying \"A Souk\" Using Inverse Atlas Tongue Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23t_interspeech.html": {
    "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoshinaga23_interspeech.html": {
    "title": "A Relationship Between Vocal Fold Vibration and Droplet Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/garnier23_interspeech.html": {
    "title": "Audio, Visual and Audiovisual intelligibility of vowels produced in noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elie23_interspeech.html": {
    "title": "Optimal control of speech with context-dependent articulatory targets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23d_interspeech.html": {
    "title": "Computational modeling of auditory brainstem responses derived from modified speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ma_interspeech.html": {
    "title": "Leveraging Label Information for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23c_interspeech.html": {
    "title": "Improving End-to-End Modeling For Mandarin-English Code-Switching Using Lightweight Switch-Routing Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ea_interspeech.html": {
    "title": "Frequency Patterns of Individual Speaker Characteristics at Higher and Lower Spectral Ranges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gosselkeberthelsen23_interspeech.html": {
    "title": "Adaptation to predictive prosodic cues in non-native standard dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/archerboyd23_interspeech.html": {
    "title": "Head movements in two- and four-person interactive conversational tasks in noisy and moderately reverberant conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23d_interspeech.html": {
    "title": "Second language identification of Vietnamese tones by native Mandarin learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fagniart23_interspeech.html": {
    "title": "Nasal vowel production and grammatical processing in French-speaking children with cochlear implants and normal-hearing peers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23f_interspeech.html": {
    "title": "Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23b_interspeech.html": {
    "title": "L2-Mandarin regional accent variability during Mandarin tone-word training facilitates English listeners' subsequent tone categorizations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ueda23_interspeech.html": {
    "title": "HumanDiffusion: diffusion model using perceptual gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kachel23_interspeech.html": {
    "title": "Queer Events, Relationships, and Sports: Does Topic Influence Speakers' Acoustic Expression of Sexual Orientation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gunason23_interspeech.html": {
    "title": "Epoch-Based Spectrum Estimation for Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mehta23_interspeech.html": {
    "title": "OverFlow: Putting flows on top of neural transducers for better TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mehrish23_interspeech.html": {
    "title": "ADAPTERMIX: Exploring the Efficacy of Mixture of Adapters for Low-Resource TTS Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23c_interspeech.html": {
    "title": "Prior-free Guided TTS: An Improved and Efficient Diffusion-based Text-Guided Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/iashchenko23_interspeech.html": {
    "title": "UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23_interspeech.html": {
    "title": "Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/guan23_interspeech.html": {
    "title": "Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kogel23_interspeech.html": {
    "title": "Towards Robust FastSpeech 2 by Modelling Residual Multimodality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23c_interspeech.html": {
    "title": "Real time spectrogram inversion on mobile phone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23_interspeech.html": {
    "title": "Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wells23_interspeech.html": {
    "title": "A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/krug23_interspeech.html": {
    "title": "Self-Supervised Solution to the Control Problem of Articulatory Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23f_interspeech.html": {
    "title": "Hierarchical Timbre-Cadence Speaker Encoder for Zero-shot Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23_interspeech.html": {
    "title": "ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/du23_interspeech.html": {
    "title": "Improving WaveRNN with Heuristic Dynamic Blending for Fast and High-Quality GPU Vocoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23_interspeech.html": {
    "title": "Intelligible Lip-to-Speech Synthesis with Speech Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23p_interspeech.html": {
    "title": "Parameter-Efficient Learning for Text-to-Speech Accent Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/khan23_interspeech.html": {
    "title": "Controlling formant frequencies with neural text-to-speech for the manipulation of perceived speaker age",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jang23b_interspeech.html": {
    "title": "FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kaneko23_interspeech.html": {
    "title": "iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23_interspeech.html": {
    "title": "VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luong23_interspeech.html": {
    "title": "Controlling Multi-Class Human Vocalization Generation via a Simple Segment-based Labeling Scheme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhogale23_interspeech.html": {
    "title": "Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23_interspeech.html": {
    "title": "Domain Adaptive Self-supervised Training of Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/olivier23_interspeech.html": {
    "title": "There is more than one kind of robustness: Fooling Whisper with adversarial examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heggan23_interspeech.html": {
    "title": "MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23l_interspeech.html": {
    "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23l_interspeech.html": {
    "title": "Blank-regularized CTC for Frame Skipping in Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jayakumar23_interspeech.html": {
    "title": "The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/unni23_interspeech.html": {
    "title": "Improving RNN-Transducers with Acoustic LookAhead",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/markl23_interspeech.html": {
    "title": "Everyone has an accent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/maison23_interspeech.html": {
    "title": "Some Voices are Too Common: Building Fair Speech Recognition Systems Using the CommonVoice Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23u_interspeech.html": {
    "title": "Information Magnitude Based Dynamic Sub-sampling for Speech-to-text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grice23_interspeech.html": {
    "title": "What's in a Rise? The Relevance of Intonation for Attention Orienting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23i_interspeech.html": {
    "title": "HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23e_interspeech.html": {
    "title": "VISinger2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23c_interspeech.html": {
    "title": "EdenTTS: A Simple and Efficient Parallel Text-to-speech Architecture with Collaborative Duration-alignment Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23c_interspeech.html": {
    "title": "Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/montesinos23_interspeech.html": {
    "title": "Speech inpainting: Context-based speech synthesis guided by video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23d_interspeech.html": {
    "title": "STEN-TTS: Improving Zero-shot Cross-Lingual Transfer for Multi-Lingual TTS with Style-Enhanced Normalization Diffusion Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kano23_interspeech.html": {
    "title": "Average Token Delay: A Latency Metric for Simultaneous Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qian23_interspeech.html": {
    "title": "Automatic Speech Recognition Transformer with Global Contextual Information Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23c_interspeech.html": {
    "title": "Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/praveen23b_interspeech.html": {
    "title": "Prefix Search Decoding for RNN Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bain23_interspeech.html": {
    "title": "WhisperX: Time-Accurate Speech Transcription of Long-Form Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nigmatulina23_interspeech.html": {
    "title": "Implementing Contextual Biasing in GPU Decoder for Online ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chung23_interspeech.html": {
    "title": "MF-PAM: Accurate Pitch Estimation through Periodicity Analysis and Multi-level Feature Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/attia23_interspeech.html": {
    "title": "Enhancing Speech Articulation Analysis Using A Geometric Transformation of the X-ray Microbeam Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jouaiti23_interspeech.html": {
    "title": "Matching Acoustic and Perceptual Measures of Phonation Assessment in Disordered Speech - A Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23_interspeech.html": {
    "title": "Improved Contextualized Speech Representations for Tonal Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chandrasekar23_interspeech.html": {
    "title": "A Study on the Importance of Formant Transitions for Stop-Consonant Classification in VCV Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eren23_interspeech.html": {
    "title": "FusedF0: Improving DNN-based F0 Estimation by Fusion of Summary-Correlograms and Raw Waveform Representations of Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kyung23_interspeech.html": {
    "title": "Improving Joint Speech and Emotion Recognition Using Global Style Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nagase23_interspeech.html": {
    "title": "Speech Emotion Recognition by Estimating Emotional Label Sequences with Phoneme Class Attribute",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23_interspeech.html": {
    "title": "Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/prisayad23_interspeech.html": {
    "title": "Dual Memory Fusion for Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kondratenko23_interspeech.html": {
    "title": "Hybrid Dataset for Speech Emotion Recognition in Russian Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hsu23_interspeech.html": {
    "title": "Speech Emotion Recognition using Decomposed Speech via Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23b_interspeech.html": {
    "title": "Prospective Validation of Motor-Based Intervention with Automated Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23_interspeech.html": {
    "title": "Classifying Rhoticity of /ɹ/ in Speech Sound Disorder using Age-and-Sex Normalized Formants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23c_interspeech.html": {
    "title": "Acoustic-to-Articulatory Speech Inversion Features for Mispronunciation Detection of /ɹ/ in Child Speech Sound Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/piton23_interspeech.html": {
    "title": "Using Commercial ASR Solutions to Assess Reading Skills in Children: A Case Report",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gebauer23_interspeech.html": {
    "title": "Exploiting Diversity of Automatic Transcripts from Distinct Speech Recognition Techniques for Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rumberg23_interspeech.html": {
    "title": "Uncertainty Estimation for Connectionist Temporal Classification Based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lavechin23_interspeech.html": {
    "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23c_interspeech.html": {
    "title": "Data augmentation for children ASR and child-adult speaker classification using voice conversion methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shetty23_interspeech.html": {
    "title": "Developmental Articulatory and Acoustic Features for Six to Ten Year Old Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23u_interspeech.html": {
    "title": "Automatically Predicting Perceived Conversation Quality in a Pediatric Sample Enriched for Autism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/johnson23_interspeech.html": {
    "title": "An Equitable Framework for Automatically Assessing Children's Oral Narrative Language Abilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cao23_interspeech.html": {
    "title": "An Analysis of Goodness of Pronunciation for Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sy23_interspeech.html": {
    "title": "Measuring Language Development From Child-centered Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hung23_interspeech.html": {
    "title": "Speaking Clearly, Understanding Better: Predicting the L2 Narrative Comprehension of Chinese Bilingual Kindergarten Children Based on Speech Intelligibility Using a Machine Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/charuau23_interspeech.html": {
    "title": "Speech Breathing Behavior During Pauses in Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23e_interspeech.html": {
    "title": "Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ariasvergara23_interspeech.html": {
    "title": "Measuring Phonological Precision in Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23_interspeech.html": {
    "title": "A Study on Using Duration and Formant Features in Automatic Detection of Speech Sound Disorder in Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baumann23_interspeech.html": {
    "title": "Influence of Utterance and Speaker Characteristics on the Classification of Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23g_interspeech.html": {
    "title": "Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mcneill23_interspeech.html": {
    "title": "An Autoregressive Conversational Dynamics Model for Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hori23_interspeech.html": {
    "title": "Style-transfer based Speech and Audio-visual Scene understanding for Robot Action Sequence Acquisition from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/soltau23_interspeech.html": {
    "title": "Speech Aware Dialog System Technology Challenge (DSTC11)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cai23_interspeech.html": {
    "title": "Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23k_interspeech.html": {
    "title": "Tracking Must Go On : Dialogue State Tracking with Verified Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ja_interspeech.html": {
    "title": "Ordered and Binary Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kataria23_interspeech.html": {
    "title": "Self-FiLM: Conditioning GANs with self-supervised representations for bandwidth extension based speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heo23b_interspeech.html": {
    "title": "Curriculum Learning for Self-supervised Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23aa_interspeech.html": {
    "title": "Introducing Self-Supervised Phonetic Information for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cordlandwehr23_interspeech.html": {
    "title": "A Teacher-Student Approach for Extracting Informative Speaker Embeddings From Speech Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lepage23_interspeech.html": {
    "title": "Experimenting with Additive Margins for Contrastive Self-Supervised Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hope23_interspeech.html": {
    "title": "Nonbinary American English speakers encode gender in vowel acoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharp23_interspeech.html": {
    "title": "Coarticulation of Sibe Vowels and Dorsal Fricatives in Spontaneous Speech: An Acoustic Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/brown23_interspeech.html": {
    "title": "Using speech synthesis to explain automatic speaker recognition: a new application of synthetic speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23i_interspeech.html": {
    "title": "Same F0, Different Tones: A Multidimensional Investigation of Zhangzhou Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/english23_interspeech.html": {
    "title": "Discovering Phonetic Feature Event Patterns in Transformer Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ra_interspeech.html": {
    "title": "A System for Generating Voice Source Signals that Implements the Transformed LF-model Parameter Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/siriwardena23b_interspeech.html": {
    "title": "Speaker-independent Speech Inversion for Estimation of Nasalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23e_interspeech.html": {
    "title": "Effects of Tonal Coarticulation and Prosodic Positions on Tonal Contours of Low Rising Tones: In the Case of Xiamen Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/issa23_interspeech.html": {
    "title": "Durational and Non-durational Correlates of Lexical and Derived Geminates in Arabic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rao23_interspeech.html": {
    "title": "Mapping Phonemes to Acoustic Symbols and Codes Using Synchrony in Speech Modulation Vectors Estimated by the Travellingwave Filter Bank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ge23_interspeech.html": {
    "title": "Rhythmic Characteristics of L2 German Speech by Advanced Chinese Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kelterer23_interspeech.html": {
    "title": "(Dis)agreement and Preference Structure are Reflected in Matching Along Distinct Acoustic-prosodic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/christodoulidou23_interspeech.html": {
    "title": "Vowel reduction by Greek-speaking children: The effect of stress and word length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lennes23_interspeech.html": {
    "title": "Pitch distributions in a very large corpus of spontaneous Finnish speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kudera23_interspeech.html": {
    "title": "Speech Enhancement Patterns in Human-Robot Interaction: A Cross-Linguistic Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html": {
    "title": "Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ga_interspeech.html": {
    "title": "Dual Audio Encoders Based Mandarin Prosodic Boundary Prediction by Using Multi-Granularity Prosodic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23i_interspeech.html": {
    "title": "NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23n_interspeech.html": {
    "title": "MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pavankalyan23_interspeech.html": {
    "title": "Narrator or Character: Voice Modulation in an Expressive Multi-speaker TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23b_interspeech.html": {
    "title": "CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oh23_interspeech.html": {
    "title": "Semi-supervised Learning for Continuous Emotional Intensity Controllable Speech Synthesis with Disentangled Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nguyen23_interspeech.html": {
    "title": "Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23fa_interspeech.html": {
    "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kunesova23_interspeech.html": {
    "title": "Neural Speech Synthesis with Enriched Phrase Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/swiatkowski23_interspeech.html": {
    "title": "Cross-lingual Prosody Transfer for Expressive Machine Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elmers23_interspeech.html": {
    "title": "Synthesis after a couple PINTs: Investigating the Role of Pause-Internal Phonetic Particles in Speech Synthesis and Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geneva23_interspeech.html": {
    "title": "Accentor: An Explicit Lexical Stress Model for TTS Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shechtman23_interspeech.html": {
    "title": "A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23j_interspeech.html": {
    "title": "Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23c_interspeech.html": {
    "title": "Prosody Modeling with 3D Visual Information for Expressive Video Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23f_interspeech.html": {
    "title": "LightClone: Speaker-guided Parallel Subnet Selection for Few-shot Voice Cloning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhong23_interspeech.html": {
    "title": "EE-TTS: Emphatic Expressive TTS with Linguistic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ogun23_interspeech.html": {
    "title": "Stochastic Pitch Prediction Improves the Diversity and Naturalness of Speech in Glow-TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23_interspeech.html": {
    "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23t_interspeech.html": {
    "title": "PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tian23b_interspeech.html": {
    "title": "Creating Personalized Synthetic Voices from Post-Glossectomy Speech with Guided Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vaessen23_interspeech.html": {
    "title": "Towards Multi-task Learning of Speech and Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23f_interspeech.html": {
    "title": "Regarding Topology and Variant Frame Rates for Differentiable WFST-based End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23b_interspeech.html": {
    "title": "2-bit Conformer quantization for automatic speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23_interspeech.html": {
    "title": "Time-Domain Speech Enhancement for Robust Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yifan23_interspeech.html": {
    "title": "Multi-channel multi-speaker transformer for speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ye23_interspeech.html": {
    "title": "Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/miwa23_interspeech.html": {
    "title": "Dialect Speech Recognition Modeling using Corpus of Japanese Dialects and Self-Supervised Learning-based Model XLSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23d_interspeech.html": {
    "title": "Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raissi23_interspeech.html": {
    "title": "Competitive and Resource Efficient Factored Hybrid HMM Systems are Simpler Than You Think",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23d_interspeech.html": {
    "title": "MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kreyssig23_interspeech.html": {
    "title": "Biased Self-supervised Learning for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23q_interspeech.html": {
    "title": "A Unified Recognition and Correction Model under Noisy and Accent Speech Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23h_interspeech.html": {
    "title": "wav2vec 2.0 ASR for Cantonese-Speaking Older Adults in a Clinical Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/an23_interspeech.html": {
    "title": "BAT: Boundary aware transducer for memory-efficient and low-latency ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tian23_interspeech.html": {
    "title": "Bayes Risk Transducer: Transducer with Controllable Alignment Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alastruey23_interspeech.html": {
    "title": "Multi-View Frequency-Attention Alternative to CNN Frontends for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sankar23_interspeech.html": {
    "title": "Investigating the dynamics of hand and lips in French Cued Speech using attention mechanisms and CTC-based decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23h_interspeech.html": {
    "title": "Hearing Loss Affects Emotion Perception in Older Adults: Evidence from a Prosody-Semantics Stroop Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23b_interspeech.html": {
    "title": "Cochlear-implant Listeners Listening to Cochlear-implant Simulated Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/murton23_interspeech.html": {
    "title": "Validation of a Task-Independent Cepstral Peak Prominence Measure with Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23b_interspeech.html": {
    "title": "Score-balanced Loss for Multi-aspect Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tayebiarasteh23_interspeech.html": {
    "title": "Federated Learning for Secure Development of AI Models for Parkinson's Disease Detection Using Speech from Different Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23c_interspeech.html": {
    "title": "F0inTFS: A lightweight periodicity enhancement strategy for cochlear implants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/obrien23_interspeech.html": {
    "title": "Differentiating acoustic and physiological features in speech for hypoxia detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23h_interspeech.html": {
    "title": "Mandarin Electrolaryngeal Speech Voice Conversion using Cross-domain Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chien23_interspeech.html": {
    "title": "Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/illner23_interspeech.html": {
    "title": "Which aspects of motor speech disorder are captured by Mel Frequency Cepstral Coefficients? Evidence from the change in STN-DBS conditions in Parkinson's disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/subramanian23_interspeech.html": {
    "title": "Detecting Manifest Huntington's Disease Using Vocal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23q_interspeech.html": {
    "title": "Exploring multi-task learning and data augmentation in dementia detection with self-supervised pretrained models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23i_interspeech.html": {
    "title": "GL-SSD: Global and Local Speech Style Disentanglement by vector quantization for robust sentence boundary detection in speech stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23c_interspeech.html": {
    "title": "Semantic VAD: Low-Latency Voice Activity Detection for Speech Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gudepu23_interspeech.html": {
    "title": "Dynamic Encoder RNN for Online Voice Activity Detection in Adverse Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moussa23_interspeech.html": {
    "title": "Point to the Hidden: Exposing Speech Audio Splicing via Signal Pointer Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23k_interspeech.html": {
    "title": "Real-Time Causal Spectro-Temporal Voice Activity Detection Based on Convolutional Encoding and Residual Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23c_interspeech.html": {
    "title": "SVVAD: Personal Voice Activity Detection for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/farooq23_interspeech.html": {
    "title": "Learning Cross-lingual Mappings for Data Augmentation to Improve Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/olatunji23_interspeech.html": {
    "title": "AfriNames: Most ASR Models \"Butcher\" African Names",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lonergan23_interspeech.html": {
    "title": "Towards Dialect-inclusive Recognition in a Low-resource Language: Are Balanced Corpora the Answer?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/javed23_interspeech.html": {
    "title": "Svarah: Evaluating English ASR Systems on Indian Accents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/talafha23_interspeech.html": {
    "title": "N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/picheny23_interspeech.html": {
    "title": "The MALACH Corpus: Results with End-to-End Architectures and Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23c_interspeech.html": {
    "title": "Unsupervised speech enhancement with deep dynamical generative speech and noise models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23f_interspeech.html": {
    "title": "Noise-Robust Bandwidth Expansion for 8K Speech Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shuai23_interspeech.html": {
    "title": "mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23b_interspeech.html": {
    "title": "Zoneformer: On-device Neural Beamformer For In-car Multi-zone Speech Separation, Enhancement and Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23c_interspeech.html": {
    "title": "Low-complexity Broadband Beampattern Synthesis using Array Response Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23d_interspeech.html": {
    "title": "A GAN Speech Inpainting Model for Audio Editing Software",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23k_interspeech.html": {
    "title": "Deep Speech Synthesis from MRI-Based Articulatory Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/siriwardena23_interspeech.html": {
    "title": "Learning to Compute the Articulatory Representations of Speech with the MIRRORNET",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/strauch23_interspeech.html": {
    "title": "Generating high-resolution 3D real-time MRI of the vocal tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bandekar23_interspeech.html": {
    "title": "Exploring a classification approach using quantised articulatory movements for acoustic to articulatory inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oota23b_interspeech.html": {
    "title": "MEG Encoding using Word Context Semantics in Listening Stories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cantisani23_interspeech.html": {
    "title": "Investigating the cortical tracking of speech and music with sung speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/keding23_interspeech.html": {
    "title": "Coherence Estimation Tracks Auditory Attention in Listeners with Hearing Impairment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oota23_interspeech.html": {
    "title": "Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI Brain Activity?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qiu23_interspeech.html": {
    "title": "Exploring Auditory Attention Decoding using Speaker Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/soman23_interspeech.html": {
    "title": "Enhancing the EEG Speech Match Mismatch Tasks With Word Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23e_interspeech.html": {
    "title": "Similar Hierarchical Representation of Speech and Other Complex Sounds In the Brain and Deep Residual Networks: An MEG Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/macintyre23_interspeech.html": {
    "title": "Effects of spectral degradation on the cortical tracking of the speech envelope",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/calderondepalma23_interspeech.html": {
    "title": "Effects of spectral and temporal modulation degradation on intelligibility and cortical tracking of speech signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23da_interspeech.html": {
    "title": "Transfer Learning for Personality Perception via Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nagano23_interspeech.html": {
    "title": "A stimulus-organism-response model of willingness to buy from advertising speech using voice quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/doukhan23_interspeech.html": {
    "title": "Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yanagida23_interspeech.html": {
    "title": "Influence of Personal Traits on Impressions of One's Own Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kirkland23_interspeech.html": {
    "title": "Pardon my disfluency: The impact of disfluency effects on the perception of speaker competence and confidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gessinger23_interspeech.html": {
    "title": "Cross-linguistic Emotion Perception in Human and TTS Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/duan23_interspeech.html": {
    "title": "Joint Learning Feature and Model Adaptation for Unsupervised Acoustic Modelling of Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/molenaar23_interspeech.html": {
    "title": "Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bai23_interspeech.html": {
    "title": "An ASR-enabled Reading Tutor: Investigating Feedback to Optimize Interaction for Learning to Read",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jain23_interspeech.html": {
    "title": "Adaptation of Whisper models to child speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yin23b_interspeech.html": {
    "title": "Let's Give a Voice to Conversational Agents in Virtual Reality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baali23b_interspeech.html": {
    "title": "FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23x_interspeech.html": {
    "title": "Video Summarization Leveraging Multimodal Information for Presentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nathan23_interspeech.html": {
    "title": "What questions are my customers asking?: Towards Actionable Insights from Customer Questions in Contact Center Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tripathi23_interspeech.html": {
    "title": "COnVoy: A Contact Center Operated Pipeline for Voice of Customer Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rastorgueva23_interspeech.html": {
    "title": "NeMo Forced Aligner and its application to word alignment for subtitle generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pattnaik23_interspeech.html": {
    "title": "CauSE: Causal Search Engine for Understanding Contact-Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sachdeva23_interspeech.html": {
    "title": "Tailored Real-Time Call Summarization System for Contact Centers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mandke23_interspeech.html": {
    "title": "Federated Learning Toolkit with Voice-based User Verification Demo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dugan23_interspeech.html": {
    "title": "Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cho23b_interspeech.html": {
    "title": "Fast Enrollable Streaming Keyword Spotting System: Training and Inference using a Web Browser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/agrawal23b_interspeech.html": {
    "title": "Cross-lingual/Cross-channel Intent Detection in Contact-Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heo23_interspeech.html": {
    "title": "One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kawa23_interspeech.html": {
    "title": "Defense Against Adversarial Attacks on Audio DeepFake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rosello23_interspeech.html": {
    "title": "A conformer-based classifier for variable-length utterance processing in anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ia_interspeech.html": {
    "title": "Conformer-based Language Embedding with Self-Knowledge Distillation for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zuluagagomez23_interspeech.html": {
    "title": "CommonAccent: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cumani23_interspeech.html": {
    "title": "From adaptive score normalization to adaptive data normalization for speaker verification systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ha_interspeech.html": {
    "title": "CAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kakouros23_interspeech.html": {
    "title": "North Sámi Dialect Identification with Self-supervised Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23c_interspeech.html": {
    "title": "Encoder-decoder Multimodal Speaker Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nam23_interspeech.html": {
    "title": "Disentangled Representation Learning for Multilingual Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jia23b_interspeech.html": {
    "title": "A Compact End-to-End Model with Local and Global Context for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sullivan23_interspeech.html": {
    "title": "On the Robustness of Arabic Speech Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23u_interspeech.html": {
    "title": "Adaptive Neural Network Quantization For Lightweight Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/su23_interspeech.html": {
    "title": "Adversarial Diffusion Probability Model For Cross-domain Speaker Verification Integrating Contrastive Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23f_interspeech.html": {
    "title": "Chinese Dialect Recognition Based on Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ito23_interspeech.html": {
    "title": "Spoofing Attacker Also Benefits from Self-Supervised Pretrained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vashishth23_interspeech.html": {
    "title": "Label Aware Speech Representation Learning For Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23c_interspeech.html": {
    "title": "Exploring the Impact of Back-End Network on Wav2vec 2.0 for Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23_interspeech.html": {
    "title": "Improving Speaker Verification with Self-Pretrained Transformer Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ribeiro23_interspeech.html": {
    "title": "Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/linke23_interspeech.html": {
    "title": "What do self-supervised speech representations encode? An analysis of languages, varieties, speaking styles and speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ca_interspeech.html": {
    "title": "A Compressed Synthetic Speech Detection Method with Compression Feature Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23j_interspeech.html": {
    "title": "Outlier-aware Inlier Modeling and Multi-scale Scoring for Anomalous Sound Detection via Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23c_interspeech.html": {
    "title": "MOSLight: A Lightweight Data-Efficient System for Non-Intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23c_interspeech.html": {
    "title": "A Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23i_interspeech.html": {
    "title": "MTANet: Multi-band Time-frequency Attention Network for Singing Melody Extraction from Polyphonic Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chunhui23_interspeech.html": {
    "title": "Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on Generative Adversarial Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/solanki23_interspeech.html": {
    "title": "Do Vocal Breath Sounds Encode Gender Cues for Automatic Gender Classification?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sugiura23_interspeech.html": {
    "title": "Automatic Exploration of Optimal Data Processing Operations for Sound Data Augmentation Using Improved Differentiable Automatic Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23b_interspeech.html": {
    "title": "A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23b_interspeech.html": {
    "title": "RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/manocha23_interspeech.html": {
    "title": "Spatialization Quality Metric for Binaural Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/roy23_interspeech.html": {
    "title": "AsthmaSCELNet: A Lightweight Supervised Contrastive Embedding Learning Framework for Asthma Classification Using Lung Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bae23b_interspeech.html": {
    "title": "Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/richter23_interspeech.html": {
    "title": "Remote Assessment for ALS using Multimodal Dialog Agents: Data Quality, Feasibility and Task Compliance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yariv23_interspeech.html": {
    "title": "Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/romero23_interspeech.html": {
    "title": "Obstructive sleep apnea screening with breathing sounds and respiratory effort: a multimodal deep learning approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23f_interspeech.html": {
    "title": "Investigation of Music Emotion Recognition Based on Segmented Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23c_interspeech.html": {
    "title": "The Effects of Input Type and Pronunciation Dictionary Usage in Transfer Learning for Low-Resource Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23d_interspeech.html": {
    "title": "Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23b_interspeech.html": {
    "title": "Robust Feature Decoupling in Voice Conversion by Using Locality-Based Instance Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jia23_interspeech.html": {
    "title": "Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ekstedt23_interspeech.html": {
    "title": "Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cong23_interspeech.html": {
    "title": "GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yasuda23_interspeech.html": {
    "title": "Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech Based on Tail Probabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koizumi23_interspeech.html": {
    "title": "LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mitsui23_interspeech.html": {
    "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thenguyen23_interspeech.html": {
    "title": "XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kulkarni23_interspeech.html": {
    "title": "ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deja23_interspeech.html": {
    "title": "Diffusion-based accent modelling in speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeshpanov23_interspeech.html": {
    "title": "Multilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23h_interspeech.html": {
    "title": "CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23k_interspeech.html": {
    "title": "Improving Bilingual TTS Using Language And Phonology Embedding With Embedding Strength Modulator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23f_interspeech.html": {
    "title": "High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23_interspeech.html": {
    "title": "PronScribe: Highly Accurate Multimodal Phonemic Transcription From Speech and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/swiatkowski23b_interspeech.html": {
    "title": "Expressive Machine Dubbing Through Phrase-level Cross-lingual Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chiang23_interspeech.html": {
    "title": "Why We Should Report the Details in Subjective Evaluation of TTS More Rigorously",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/perezzarazaga23_interspeech.html": {
    "title": "Speaker-independent neural formant synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saito23b_interspeech.html": {
    "title": "CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharoni23_interspeech.html": {
    "title": "SASPEECH: A Hebrew Single Speaker Dataset for Text To Speech and Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}