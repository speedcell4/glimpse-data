{
  "https://www.isca-speech.org/archive/interspeech_2023/narayanan23_interspeech.html": {
    "title": "Bridging Speech Science and Technology — Now and Into the Future",
    "volume": "main",
    "abstract": "Speech research is remarkable in so many ways – in its essential human-centeredness, the rich interconnections between the science and technology, and its wide-ranging impact that is both fundamental and applied. Crucial advances in speech science research catalyze and leverage technological advances across the machine intelligence ecosystem, from sensing and imaging to signal processing and machine learning. Likewise, creation of speech-centric societal applications benefits from an understanding of how humans produce, process and use speech in communication. In these complementary endeavors, two intertwined lines of inquiry endure: illuminating the rich information tapestry and inherent variability in speech and creating trustworthy speech technologies This talk will highlight some advances and possibilities in this multifaceted speech research realm. The first is capturing and modeling the human vocal instrument during speaking and how related technological and clinical applications leverage this technology. The second focuses on speech-based informatics tools to support research and clinical translation related to human health and wellbeing. Finally, the talk will highlight the critical goal of designing trustworthy speech and spoken language machine intelligence tools that are inclusive, equitable, robust, safe, and secure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23m_interspeech.html": {
    "title": "Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks",
    "volume": "main",
    "abstract": "Given an audio clip and a reference face image, the goal of the talking head generation is to generate a high-fidelity talking head video. Although some audio-driven methods of generating talking head videos have made some achievements in the past, most of them only focused on lip and audio synchronization and lack the ability to reproduce the facial expressions of the target person. To this end, we propose a talking head generation model consisting of a Memory-Sharing Emotion Feature extractor (MSEF) and an Attention-Augmented Translator based on U-net (AATU). Firstly, MSEF can extract implicit emotional auxiliary features from audio to estimate more accurate emotional face landmarks. Secondly, AATU acts as a translator between the estimated landmarks and the photo-realistic video frames. Extensive qualitative and quantitative experiments have shown the superiority of the proposed method to the previous works. Codes will be made publicly available",
    "checked": true,
    "id": "450c82af807da7c70dd8d43096cb1a0c5c8c929e",
    "semantic_title": "emotional talking head generation based on memory-sharing and attention-augmented networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23n_interspeech.html": {
    "title": "Speech Synthesis with Self-Supervisedly Learnt Prosodic Representations",
    "volume": "main",
    "abstract": "This paper presents S4LPR, a Speech Synthesis model conditioned on Self-Supervisedly Learnt Prosodic Representations. Instead of using raw acoustic features, such as F0 and energy, as intermediate prosodic variables, three self-supervised speech models are designed for comparison and are pre-trained on large-scale unlabeled data to extract frame-level prosodic representations. In addition to vanilla wav2vec 2.0, the other two pre-trained models learn representations from LPC residuals or adopt a multi-task learning strategy to focus on the prosodic information in speech. Based on FastSpeech2 and PnGBERT, our acoustic model is built with the learned prosodic representations as intermediate variables. Experimental results demonstrate that the naturalness of speech synthesized using S4LPR is significantly better than the FastSpeech2 baseline",
    "checked": true,
    "id": "c0a927f459171b0a15114a7a812996563964e91a",
    "semantic_title": "speech synthesis with self-supervisedly learnt prosodic representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tang23_interspeech.html": {
    "title": "EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis",
    "volume": "main",
    "abstract": "There has been significant progress in emotional Text-To-Speech (TTS) synthesis technology in recent years. However, existing methods primarily focus on the synthesis of a limited number of emotion types and have achieved unsatisfactory performance in intensity control. To address these limitations, we propose EmoMix, which can generate emotional speech with specified intensity or a mixture of emotions. Specifically, EmoMix is a controllable emotional TTS model based on a diffusion probabilistic model and a pre-trained speech emotion recognition (SER) model used to extract emotion embedding. Mixed emotion synthesis is achieved by combining the noises predicted by diffusion model conditioned on different emotions during only one sampling process at the run-time. We further apply the Neutral and specific primary emotion mixed in varying degrees to control intensity. Experimental results validate the effectiveness of EmoMix for synthesizing mixed emotion and intensity control",
    "checked": true,
    "id": "5e635e749a90022f5b3704a8fb1c6b48645519cc",
    "semantic_title": "emomix: emotion mixing via diffusion models for emotional speech synthesis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23b_interspeech.html": {
    "title": "Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus",
    "volume": "main",
    "abstract": "We present a large-scale in-the-wild Japanese laughter corpus and a laughter synthesis method. Previous work on laughter synthesis lacks not only data but also proper ways to represent laughter. To solve these problems, we first propose an in-the-wild corpus comprising 3.5 hours of laughter, which is to our best knowledge the largest laughter corpus designed for laughter synthesis. We then propose pseudo phonetic tokens (PPTs) to represent laughter by a sequence of discrete tokens, which are obtained by training a clustering model on features extracted from laughter by a pretrained self-supervised model. Laughter can then be synthesized by feeding PPTs into a text-to-speech system. We further show PPTs can be used to train a language model for unconditional laughter generation. Results of comprehensive subjective and objective evaluations demonstrate that the proposed method significantly outperforms a baseline method, and can generate natural laughter unconditionally",
    "checked": true,
    "id": "20f7cc755d832c664ee5a84b46a30c27b92a6ac5",
    "semantic_title": "laughter synthesis using pseudo phonetic tokens with a large-scale in-the-wild laughter corpus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23u_interspeech.html": {
    "title": "Explicit Intensity Control for Accented Text-to-speech",
    "volume": "main",
    "abstract": "Accented text-to-speech (TTS) synthesis seeks to generate speech with an accent (L2) as a variant of the standard version (L1). How to control the intensity of accent is a very interesting research direction. Recent works design a speaker-adversarial loss to disentangle the speaker and accent information, and then adjust the loss weight to control the accent intensity. However, there is no direct correlation between the disentanglement factor and natural accent intensity. To this end, this paper proposes a new intuitive and explicit accent intensity control scheme for accented TTS. Specifically, we first extract the posterior probability from the L1 speech recognition model to quantify the phoneme accent intensity for accented speech, then design a FastSpeech2 based TTS model, named Ai-TTS, to take the accent intensity expression into account during speech generation. Experiments show that our method outperforms the baseline model in terms of accent rendering and intensity control",
    "checked": true,
    "id": "d354de4a4c182d294ffba7256e08bf6c4767d642",
    "semantic_title": "explicit intensity control for accented text-to-speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23o_interspeech.html": {
    "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "volume": "main",
    "abstract": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models",
    "checked": true,
    "id": "f3cff86b41ccbb4dd533e6d115b82f34d2d8a04c",
    "semantic_title": "comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/duquenne23_interspeech.html": {
    "title": "Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer",
    "volume": "main",
    "abstract": "Recent research has shown that independently trained encoders and decoders, combined through a shared fixed-size representation, can achieve competitive performance in speech-to-text translation. In this work, we show that this type of approach can be further improved with multilingual training. We observe significant improvements in zero-shot cross-modal speech translation, even outperforming a supervised approach based on XLSR for several languages",
    "checked": true,
    "id": "39b4255a439d2aa85c683935cd47314d098fecf3",
    "semantic_title": "modular speech-to-text translation for zero-shot cross-modal transfer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pal23_interspeech.html": {
    "title": "Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters",
    "volume": "main",
    "abstract": "To translate speech for automatic dubbing, machine translation needs to be isochronous, i.e. translated speech needs to be aligned with the source in terms of speech durations. We introduce target factors in a transformer model to predict durations jointly with target language phoneme sequences. We also introduce auxiliary counters to help the decoder to keep track of the timing information while generating target phonemes. We show that our model improves translation quality and isochrony compared to previous work where the translation model is instead trained to predict interleaved sequences of phonemes and durations",
    "checked": true,
    "id": "38872b9e3f937287012bac3c66ab5df0c084a726",
    "semantic_title": "improving isochronous machine translation with target factors and auxiliary counters",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23_interspeech.html": {
    "title": "StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation",
    "volume": "main",
    "abstract": "Direct speech-to-speech translation (S2ST) has gradually become popular as it has many advantages compared with cascade S2ST. However, current research mainly focuses on the accuracy of semantic translation and ignores the speech style transfer from a source language to a target language. The lack of high-fidelity expressive parallel data makes such style transfer challenging, especially in more practical zero-shot scenarios. To solve this problem, we first build a parallel corpus using a multi-lingual multi-speaker text-to-speech synthesis (TTS) system and then propose the StyleS2ST model with cross-lingual speech style transfer ability based on a style adaptor on a direct S2ST system framework. Enabling continuous style space modeling of an acoustic model through parallel corpus training and non-parallel TTS data augmentation, StyleS2ST captures cross-lingual acoustic feature mapping from the source to the target language. Experiments show that StyleS2ST achieves good style similarity and naturalness in both in-set and out-of-set zero-shot scenarios",
    "checked": true,
    "id": "9dd2e6b5076cb5e2325e6a3f40977228b473904b",
    "semantic_title": "styles2st: zero-shot style transfer for direct speech-to-speech translation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gaido23_interspeech.html": {
    "title": "Joint Speech Translation and Named Entity Recognition",
    "volume": "main",
    "abstract": "Modern automatic translation systems aim at supporting the users by providing contextual knowledge. In this framework, a critical task is the output enrichment with information regarding the mentioned entities. This is currently achieved by processing the generated translations with named entity recognition (NER) tools and retrieving their description from knowledge bases. In light of the recent promising results shown by direct speech translation (ST) models and the known weaknesses of cascades (error propagation and additional latency), in this paper we propose multitask models that jointly perform ST and NER, and compare them with a cascade baseline. Experimental results on three language pairs (en-es/fr/it) show that our models significantly outperform the cascade on the NER task (by 0.4-1.0 F1), without degradation in terms of translation quality, and with the same computational efficiency of a plain direct ST model",
    "checked": true,
    "id": "563f203f7efc841dacd6e7801256e8d6f2578509",
    "semantic_title": "joint speech translation and named entity recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sant23_interspeech.html": {
    "title": "Analysis of Acoustic information in End-to-End Spoken Language Translation",
    "volume": "main",
    "abstract": "End-to-End Transformer-based models are the most popular approach for Spoken Language Translation (SLT). While obtaining state-of-the-art results, we are still far from understanding how these models extract acoustic information from the data and how they are transformed into semantic representations. In this paper, we seek to provide a better understanding of the flow of acoustic information along speech-to-text translation models. By means of the Speaker Classification and Spectrogram Reconstruction tasks, this study (i) interprets the main role of the encoder with respect to the acoustic features, (ii) highlights the importance of the acoustic information throughout the model and its transfer between encoder and decoder, and (iii) reveals the significant effect of downsampling convolutional layers for learning acoustic features. (iv) Finally, we also observe the existence of a strong correlation between the semantic domain and the speakers' labels in MuST-C",
    "checked": true,
    "id": "89dc1dd817b0af8184fb55e2b819d0fbcbdfad17",
    "semantic_title": "analysis of acoustic information in end-to-end spoken language translation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23oa_interspeech.html": {
    "title": "LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) and speech translation (ST) can both use neural transducers as the model structure. It is thus possible to use a single transducer model to perform both tasks. In real-world applications, such joint ASR and ST models may need to be streaming and do not require source language identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a streaming language-agnostic multilingual speech recognition and translation model using neural transducers. Based on the transducer model structure, we propose four methods, a unified joint and prediction network for multilingual output, a clustered multilingual encoder, target language identification for encoder, and connectionist temporal classification regularization. Experimental results show that LAMASSU not only drastically reduces the model size but also reaches the performances of monolingual ASR and bilingual ST models",
    "checked": true,
    "id": "41fa401b3ed0b20168c9ad19471c248f1fe9b00c",
    "semantic_title": "lamassu: a streaming language-agnostic multilingual speech recognition and translation model using neural transducers",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23c_interspeech.html": {
    "title": "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available",
    "checked": true,
    "id": "e4f2d75856ce149b994f079ae50fd33ca47245d3",
    "semantic_title": "dphubert: joint distillation and pruning of self-supervised speech models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zaiem23_interspeech.html": {
    "title": "Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases",
    "checked": true,
    "id": "6801e61b2fb45b660fab6d287cae5e23cd6b76dd",
    "semantic_title": "automatic data augmentation for domain adapted fine-tuning of self-supervised speech representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23e_interspeech.html": {
    "title": "Dual Acoustic Linguistic Self-supervised Representation Learning for Cross-Domain Speech Recognition",
    "volume": "main",
    "abstract": "The integration of well-pre-trained acoustic and linguistic representations boosts the performance of speech-to-text cross-modality tasks. However, the potential of fine-tuning cross-modality integrated model on accented and noisy corpus is still under-explored. To address this gap, we propose an end-to-end acoustic and linguistic integrated representation learning model, namely Dual-w2v-BART. Our model incorporates acoustic representations from wav2vec2.0 and linguistic information from BART model by utilizing the cross-attention mechanism in the decoder, with paired speech-text dual inputs. To enhance model robustness on accent and noise, we propose a text-centric representation consistency component that helps to gain the similarity between different modality inputs while representing the same content. The results on accented and noisy speech recognition tasks demonstrate the effectiveness of the proposed model for reducing error rates compared to baseline and other competitive models",
    "checked": true,
    "id": "d5111d4769b60fa9940dda15146af3c7959c2cee",
    "semantic_title": "dual acoustic linguistic self-supervised representation learning for cross-domain speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baskar23_interspeech.html": {
    "title": "O-1: Self-training with Oracle and 1-best Hypothesis",
    "volume": "main",
    "abstract": "We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80% relative compared to EMBR which bridges the gap by 43% relative. O-1 achieves 13% to 25% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23d_interspeech.html": {
    "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets",
    "volume": "main",
    "abstract": "In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend",
    "checked": true,
    "id": "d2451c2cce0d44e3d390aa09059a8a0e5369c223",
    "semantic_title": "mt4ssl: boosting self-supervised speech representation learning by integrating multiple targets",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lamyeemui23_interspeech.html": {
    "title": "Comparing Self-Supervised Pre-Training and Semi-Supervised Training for Speech Recognition in Languages with Weak Language Models",
    "volume": "main",
    "abstract": "This paper investigates the potential of improving a hybrid automatic speech recognition model trained on 10 hours of transcribed data with 200 hours of untranscribed data in low-resource languages. First, we compare baseline methods of cross-lingual transfer with MFCC features and features extracted with the multilingual self-supervised model XLSR-53. Subsequently, we compare two approaches that can leverage the untranscribed data: semi-supervised training with LF-MMI and continued self-supervised pre-training of XLSR-53. Our results on well-resourced English broadcast data derived from MGB show that both methods achieve 18% and 27% relative improvements compared to the baseline, respectively. On the low-resource South African Soap Opera dataset, the relative improvement with semi-supervised training is only 3% due to the inherently weak language model. However, continued pre-training achieves 8.6% relative improvement because it does not rely on any external information",
    "checked": true,
    "id": "0e8e5d938af4fa0d114ae155421681e0755a649e",
    "semantic_title": "comparing self-supervised pre-training and semi-supervised training for speech recognition in languages with weak language models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23da_interspeech.html": {
    "title": "Chinese EFL Learners' Perception of English Prosodic Focus",
    "volume": "main",
    "abstract": "Focus in a sentence can be realized prosodically in speech communication. It has been found not easy for L2 learners to acquire. The present study examines Chinese learners' perception of English prosodic focus, specifically the effects of learners' English proficiency, intonation type, sentence length, and focus location on the perceptual accuracy of English prosodic focus by Chinese EFL learners. Results of two trials in the perception experiment reveal that focus location, intonation type, and English proficiency significantly impacted Chinese learners' perceptual accuracy of both single focus and dual focus in English. Focus in statements was perceived more accurately than that in questions for both single focus and dual focus. Focus located on sentence-final words in questions was perceived more accurately than that on non-final words in questions. Learners' English proficiency positively correlated to the accuracy of focus perception, especially for dual focus",
    "checked": true,
    "id": "e85507287ca0daa9bd73a0a82182c8212e6d0249",
    "semantic_title": "chinese efl learners' perception of english prosodic focus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sostarics23_interspeech.html": {
    "title": "Pitch Accent Variation and the Interpretation of Rising and Falling Intonation in American English",
    "volume": "main",
    "abstract": "This study tests the division of labor in the meaning conveyed by pitch accents and edge tones in English intonation. In three perception studies, we investigate where the locus of the contrast between an assertive vs inquisitive interpretation resides. By doing so, we also gain insight into the role of potentially meaningful within- and between-category variation in the phonetic implementation of discrete intonational tunes. We find that the pitch accent does not contribute to assertive interpretation. Rather, the distinction between assertive and inquisitive interpretation is cued primarily by the final F0 of the pitch contour regardless of the pitch accent, but that increased overall pitch prominence may trigger a salient focus interpretation that interferes with judging assertiveness",
    "checked": true,
    "id": "d47244483903120338e389ac8db57617477d73d5",
    "semantic_title": "pitch accent variation and the interpretation of rising and falling intonation in american english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kuang23_interspeech.html": {
    "title": "Tonal coarticulation as a cue for upcoming prosodic boundary",
    "volume": "main",
    "abstract": "It has been established that the lack of tonal coarticulation or pitch reset is a salient cue for the beginning of a large prosodic domain, however, it is yet unclear whether tonal coarticulation can be an informative cue for the end of a prosodic domain. We examined this question with two continuous speech corpora of Mandarin, and both expert and crowd-sourced perceptual annotations were used. The FPCA model of the holistic tonal contours shows that the carry-over effect of the preceding tone is significantly affected by the strength of the following boundaries. Stronger carry-over effects are associated with the end of larger prosodic boundaries. Moreover, machine learning classification shows that the fine-grained tonal coarticulation patterns are salient cues for predicting larger prosodic boundaries. This result is further validated by crowd-sourced boundary perceptual ratings from human listeners. This study has important implications for the understanding of prosodic phrasing",
    "checked": true,
    "id": "84c4d2832913419041c3d8d6b63c6e7cd8abc176",
    "semantic_title": "tonal coarticulation as a cue for upcoming prosodic boundary",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/repp23_interspeech.html": {
    "title": "Alignment of Beat Gestures and Prosodic Prominence in German",
    "volume": "main",
    "abstract": "We present evidence on the alignment of beat gestures and prosodic prominence from a video corpus consisting of six German educational videos for students from six presenters. Our analysis of 120 beat gestures (with a substantial variety of hand shapes) shows that beat gestures almost always align with prosodically prominent syllables, i.e., syllables carrying a pitch accent. Specifically, the stroke always starts before, or - more often - on, a pitch-accented syllable; the apex mostly falls on the accented syllable (74%) but may also occur in subsequent syllables. The degree of prosodic prominence of the accented syllable (in terms of DIMA-prominence levels) is predictive for the position of the apex, which occurs within rather than after the accented syllable more often for higher degrees of prominence. These findings provide new insights into the alignment of prominence-lending features of prosody and gesture, thereby broadening the empirical landscape for beat gestures",
    "checked": true,
    "id": "5293c0b253dc03cbff04cd19fb28c865a0fcba14",
    "semantic_title": "alignment of beat gestures and prosodic prominence in german",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/white23_interspeech.html": {
    "title": "Creak Prevalence and Prosodic Context in Australian English",
    "volume": "main",
    "abstract": "Creaky voice has been found to mark phrase-finality in many varieties of English, as well as in other languages. The present study aims to investigate whether this is also true for Australian English (AusE), a variety that is understudied in creaky voice research. Using automatic creak detection methods, the need for manual annotation of creak is reduced, and we are able to analyse a large dataset of Australian teenagers' speech. As in other varieties, creak is found to be a marker of finality in AusE. Additionally, we find that males use higher rates of creaky voice than females, challenging the widely held assumption that creak is a feature of female speech",
    "checked": true,
    "id": "6ba65b1ad194c04f21ea2b05088ccb1d92aa432d",
    "semantic_title": "creak prevalence and prosodic context in australian english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bodur23_interspeech.html": {
    "title": "Speech reduction: position within French prosodic structure",
    "volume": "main",
    "abstract": "Variation in the speech signal is a characteristic of spoken language, emerging partially as a result of interactions between various linguistic levels. One example of variation is phonetic reduction, where words are produced with missing or underspecified phonetic forms. Using a French conversational corpus, this paper focuses on the relationship between reduction and prosodic structure to see whether certain positions favor the occurrence of reduction. We annotated and observed the distribution of reduced sequences within specific prosodic domains (Intonational and Accentual Phrases). Preliminary analyses revealed that the detected reductions occur mostly mid- IP and very rarely at IP-final. However, this pattern may vary among speakers, as speakers have different patterns in terms of the number of reductions produced and their positions. It is also usually the case that the reduced sequences occurring mid-IP, coincide with the AP level boundaries, extending from one AP to another",
    "checked": true,
    "id": "a21240a74af5b918c91e05a0ebae2292b35ff2f5",
    "semantic_title": "speech reduction: position within french prosodic structure",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23d_interspeech.html": {
    "title": "Transvelar Nasal Coupling Contributing to Speaker Characteristics in Non-nasal Vowels",
    "volume": "main",
    "abstract": "Nasal-cavity structure is stable in speech and varied across speakers, which potentially gives rise to speaker characteristics. Many studies have reported the acoustic contribution of the nasal cavity for nasal and nasalized sounds with velopharyngeal port opening. However, nasal-cavity resonance does emerge in non-nasal vowels through transvelar nasal coupling, which results in non-negligible modifications to non-nasal vowel spectra. In this study, nasal and oral output sounds were separately recorded during non-nasal utterances, and spectral analysis was conducted. The results indicate clear inter-speaker variability in two spectral measures below 2 kHz: frequency location of double-peaked first nasal-cavity resonance and inconsistent distribution of minor dips above the first resonance. It was also observed that nostril outputs modulate oral output signals to lower the first formant frequency of naturally produced non-low vowels, which also exhibited varied degrees across speakers",
    "checked": true,
    "id": "d6c08360cc77b37e108194d8af38d40d7b98de26",
    "semantic_title": "transvelar nasal coupling contributing to speaker characteristics in non-nasal vowels",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/otani23_interspeech.html": {
    "title": "Speech Synthesis from Articulatory Movements Recorded by Real-time MRI",
    "volume": "main",
    "abstract": "Previous speech synthesis models from articulatory movements recorded using real-time MRI (rtMRI) only predicted vocal tract shape parameters and required additional pitch information to generate a speech waveform. This study proposes a two-stage deep learning model composed of CNN-BiLSTM that predicts a mel-spectrogram from a rtMRI video and a HiFi-GAN vocoder that synthesizes a speech waveform. We evaluated our model on two databases: the ATR 503 sentences rtMRI database and the USC-TIMIT database. The experimental results on the ATR 503 sentences rtMRI database show that the PESQ score and the RMSE of F0 are 1.64 and 26.7 Hz. This demonstrates that all acoustic parameters, including fundamental frequency, can be estimated from the rtMRI videos. In the experiment on the USC-TIMIT database, we obtained a good PESQ score and RMSE for F0. However, the synthesized speech is unclear, indicating that the quality of the datasets affects the intelligibility of the synthesized speech",
    "checked": true,
    "id": "977f2f760d0eb99d73182fd8334c1986989c5b07",
    "semantic_title": "speech synthesis from articulatory movements recorded by real-time mri",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23b_interspeech.html": {
    "title": "The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN",
    "volume": "main",
    "abstract": "Phonetic convergence describes the automatic and unconscious speech adaptation of two interlocutors in a conversation. This paper proposes a Siamese recurrent neural network (RNN) architecture to measure the convergence of the holistic spectral characteristics of speech sounds in an L2-L2 interaction. We extend an alternating reading task (the ART) dataset by adding 20 native Slovak L2 English speakers. We train and test the Siamese RNN model to measure phonetic convergence of L2 English speech from three different native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10 dyads). Our results indicate that the Siamese RNN model effectively captures the dynamics of phonetic convergence and the speaker's imitation ability. Moreover, this text-independent model is scalable and capable of handling L1-induced speaker variability",
    "checked": true,
    "id": "2b9b85a451ee43149db92918bda991886295374b",
    "semantic_title": "the art of conversation: measuring phonetic convergence and deliberate imitation in l2-speech with a siamese rnn",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mahshie23_interspeech.html": {
    "title": "Did you see that? Exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants",
    "volume": "main",
    "abstract": "This project aimed to explore the potential role of vision in speech contrast production and auditory perception development in children with cochlear implants (CWCI). Ten CWCI between 43 and 61 months of age, with at least 2 years of CI experience, served as participants. Employing an auditory imitation task, children's ability to auditorily perceive contrasts that are more or less visible was examined both at baseline and one year after the initial assessment. The children's ability to produce these contrasts was also examined through a picture-naming task. The CWCI tended to produce features in both visibility conditions with greater accuracy than they perceived, both at baseline and at 1 year. Production and perception accuracy increased after one year of CI usage, with the mean perceptual gain for the more visible contrasts exceeding that of the less visible contrasts. The implications of the role of vision in contrast development are discussed",
    "checked": true,
    "id": "81ec5fa89978dde59b20c0c580c79f9a37cd3d3e",
    "semantic_title": "did you see that? exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vanbemmel23_interspeech.html": {
    "title": "Automatic assessments of dysarthric speech: the usability of acoustic-phonetic features",
    "volume": "main",
    "abstract": "Individuals with dysarthria suffer from difficulties in speech production and consequent reductions in speech intelligibility, which is an important concept for diagnosing and assessing effectiveness of speech therapy. In the current study, we investigate which acoustic-phonetic features are most relevant and important in automatically assessing intelligibility and in classifying speech as healthy or dysarthric. After feature selection, we applied a stepwise linear regression to predict intelligibility ratings and a Linear Discriminant Analysis to classify healthy and dysarthric speech. We observed a very strong correlation between actual and predicted intelligibility ratings in the regression analysis. We also observed a high classification accuracy of 98.06% by using 17 features and a comparable, high accuracy of 96.11% with only two features. These results indicate the usefulness of the acoustic-phonetic features in automatic assessments of dysarthric speech",
    "checked": true,
    "id": "8174cc3c98bb4fe0af86fd75c3dc000a54a39489",
    "semantic_title": "automatic assessments of dysarthric speech: the usability of acoustic-phonetic features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/venkatathirumalakumar23_interspeech.html": {
    "title": "Classification of Multi-class Vowels and Fricatives From Patients Having Amyotrophic Lateral Sclerosis with Varied Levels of Dysarthria Severity",
    "volume": "main",
    "abstract": "Dysarthria due to Amyotrophic Lateral Sclerosis (ALS) progressively distorts the acoustic space affecting the discriminability of different vowels and fricatives. However, the extent to which this happens with increasing severity is not thoroughly investigated. In this work, we perform automatic 4-class vowel (/a/, /i/, /o/, /u/) and 3-class fricative (/s/, /sh/, /f/) classification at varied severity levels and compare the performances with those from manual classification (through listening tests). Experiments with speech data from 119 ALS and 40 healthy subjects suggest that the manual and automatic classification accuracies reduce with an increase in dysarthria severity reaching 59.22% and 61.67% for vowels and 41.78% and 38.00% for fricatives, respectively, at the most severe cases. While manual classification is better than automatic one for all severity levels except the highest severity case for vowels, the difference between the two gradually reduces with an increase in severity",
    "checked": true,
    "id": "5eb48172191d12d829d3d4646b9d600cfb3751c5",
    "semantic_title": "classification of multi-class vowels and fricatives from patients having amyotrophic lateral sclerosis with varied levels of dysarthria severity",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qi23b_interspeech.html": {
    "title": "Parameter-efficient Dysarthric Speech Recognition Using Adapter Fusion and Householder Transformation",
    "volume": "main",
    "abstract": "In dysarthric speech recognition, data scarcity and the vast diversity between dysarthric speakers pose significant challenges. While finetuning has been a popular solution, it can lead to overfitting and low parameter efficiency. Adapter modules offer a better solution, with their small size and easy applicability. Additionally, Adapter Fusion can facilitate knowledge transfer from multiple learned adapters, but may employ more parameters. In this work, we apply Adapter Fusion for target speaker adaptation and speech recognition, achieving acceptable accuracy with significantly fewer speaker-specific trainable parameters than classical finetuning methods. We further improve the parameter efficiency of the fusion layer by reducing the size of query and key layers and using Householder transformation to reparameterize the value linear layer. Our proposed fusion layer achieves comparable recognition results to the original method with only one third of the parameters",
    "checked": true,
    "id": "c8c29732ed33b019854bbeb56e7ad8c707c6f270",
    "semantic_title": "parameter-efficient dysarthric speech recognition using adapter fusion and householder transformation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hermann23_interspeech.html": {
    "title": "Few-shot Dysarthric Speech Recognition with Text-to-Speech Data Augmentation",
    "volume": "main",
    "abstract": "Speakers with dysarthria could particularly benefit from assistive speech technology, but are underserved by current automatic speech recognition (ASR) systems. The differences of dysarthric speech pose challenges, while recording large amounts of training data can be exhausting for patients. In this paper, we synthesise dysarthric speech with a FastSpeech 2-based multi-speaker text-to-speech (TTS) system for ASR data augmentation. We evaluate its few-shot capability by generating dysarthric speech with as few as 5 words from an unseen target speaker and then using it to train speaker-dependent ASR systems. The results indicated that, while the TTS output is not yet of sufficient quality, this could allow easy development of personalised acoustic models for new dysarthric speakers and domains in the future",
    "checked": true,
    "id": "4692b33dca3e8dd24bef1ce0e2ba1a8f2acc2e44",
    "semantic_title": "few-shot dysarthric speech recognition with text-to-speech data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yee23_interspeech.html": {
    "title": "Latent Phrase Matching for Dysarthric Speech",
    "volume": "main",
    "abstract": "Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies has emphasized interest in designing and improving personalized speech models for atypical speech. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases",
    "checked": true,
    "id": "d757a58200254625c3326a32a1da6fa8eaa2eff3",
    "semantic_title": "latent phrase matching for dysarthric speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeo23_interspeech.html": {
    "title": "Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification",
    "volume": "main",
    "abstract": "This paper proposes an improved Goodness of Pronunciation (GoP) that utilizes Uncertainty Quantification (UQ) for automatic speech intelligibility assessment for dysarthric speech. Current GoP methods rely heavily on neural network-driven overconfident predictions, which is unsuitable for assessing dysarthric speech due to its significant acoustic differences from healthy speech. To alleviate the problem, UQ techniques were used on GoP by 1) normalizing the phoneme prediction (entropy, margin, maxlogit, logit-margin) and 2) modifying the scoring function (scaling, prior normalization). As a result, prior-normalized maxlogit GoP achieves the best performance, with a relative increase of 5.66%, 3.91%, and 23.65% compared to the baseline GoP for English, Korean, and Tamil, respectively. Furthermore, phoneme analysis is conducted to identify which phoneme scores significantly correlate with intelligibility scores in each language",
    "checked": true,
    "id": "4879abd5687f59bc4123ebe5c0de84f94f6d8583",
    "semantic_title": "speech intelligibility assessment of dysarthric speech by using goodness of pronunciation with uncertainty quantification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23c_interspeech.html": {
    "title": "CQNV: A Combination of Coarsely Quantized Bitstream and Neural Vocoder for Low Rate Speech Coding",
    "volume": "main",
    "abstract": "Recently, speech codecs based on neural networks have proven to perform better than traditional methods. However, redundancy in traditional parameter quantization is visible within the codec architecture of combining the traditional codec with the neural vocoder. In this paper, we propose a novel framework named CQNV, which combines the coarsely quantized parameters of a traditional parametric codec to reduce the bitrate with a neural vocoder to improve the quality of the decoded speech. Furthermore, we introduce a parameters processing module into the neural vocoder to enhance the application of the bitstream of traditional speech coding parameters to the neural vocoder, further improving the reconstructed speech's quality. In the experiments, both subjective and objective evaluations demonstrate the effectiveness of the proposed CQNV framework. Specifically, our proposed method can achieve higher quality reconstructed speech at 1.1 kbps than Lyra and Encodec at 3 kbps",
    "checked": true,
    "id": "79ccc49646b09821403ae4f0d76dace82b19db22",
    "semantic_title": "cqnv: a combination of coarsely quantized bitstream and neural vocoder for low rate speech coding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kamo23_interspeech.html": {
    "title": "Target Speech Extraction with Conditional Diffusion Model",
    "volume": "main",
    "abstract": "Diffusion model-based speech enhancement has received increased attention since it can generate very natural enhanced signals and generalizes well to unseen conditions. Diffusion models have been explored for several sub-tasks of speech enhancement, such as speech denoising, dereverberation, and source separation. In this paper, we investigate their use for target speech extraction (TSE), which consists of estimating the clean speech signal of a target speaker in a mixture of multi-talkers. TSE is realized by conditioning the extraction process on a clue identifying the target speaker. We show we can realize TSE using a conditional diffusion model conditioned on the clue. Besides, we introduce ensemble inference to reduce potential extraction errors caused by the diffusion process. In experiments on Libri2mix corpus, we show that the proposed diffusion model-based TSE combined with ensemble inference outperforms a comparable TSE system trained discriminatively",
    "checked": true,
    "id": "cb8a0a610ddfd79794603b4729c501a662d461f0",
    "semantic_title": "target speech extraction with conditional diffusion model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cohen23_interspeech.html": {
    "title": "Towards Fully Quantized Neural Networks For Speech Enhancement",
    "volume": "main",
    "abstract": "Deep learning models have shown state-of-the-art results in speech enhancement. However, deploying such models on an eight-bit integer-only device is challenging. In this work, we analyze the gaps in deploying a vanilla quantization-aware training method for speech enhancement, revealing two significant observations. First, quantization mainly affects signals with a high input Signal-to-Noise Ratio (SNR). Second, quantizing the model's input and output shows major performance degradation. Based on our analysis, we propose Fully Quantized Speech Enhancement (FQSE), a new quantization-aware training method that closes these gaps and enables eight-bit integer-only quantization. FQSE introduces data augmentation to mitigate the quantization effect on high SNR. Additionally, we add an input splitter and a residual quantization block to the model to overcome the error of the input-output quantization. We show that FQSE closes the performance gaps induced by eight-bit quantization",
    "checked": true,
    "id": "e85c275be3f166e3208ed6a981c9e90ce4498f93",
    "semantic_title": "towards fully quantized neural networks for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23p_interspeech.html": {
    "title": "Complex Image Generation SwinTransformer Network for Audio Denoising",
    "volume": "main",
    "abstract": "Achieving high-performance audio denoising is still a challenging task in real-world applications. Existing time-frequency methods often ignore the quality of generated frequency domain images. This paper converts the audio denoising problem into an image generation task. We first develop a complex image generation SwinTransformer network to capture more information from the complex Fourier domain. We then impose structure similarity and detailed loss functions to generate high-quality images and develop an SDR loss to minimize the difference between denoised and clean audios. Extensive experiments on two benchmark datasets demonstrate that our proposed model is better than state-of-the-art methods",
    "checked": true,
    "id": "34e95df66dc9419ea92343fce9b10f0ccf24b687",
    "semantic_title": "complex image generation swintransformer network for audio denoising",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/blau23_interspeech.html": {
    "title": "Using Text Injection to Improve Recognition of Personal Identifiers in Speech",
    "volume": "main",
    "abstract": "Accurate recognition of specific categories, such as persons' names, dates or other identifiers is critical in many Automatic Speech Recognition (ASR) applications. As these categories represent personal information, ethical use of this data including collection, transcription, training and evaluation demands special care. One way of ensuring the security and privacy of individuals is to redact or eliminate Personally Identifiable Information (PII) from collection altogether. However, this results in ASR models that tend to have lower recognition accuracy of these categories. We use text-injection to improve the recognition of PII categories by including fake textual substitutes of PII categories in the training data using a text injection method. We demonstrate substantial improvement to Recall of Names and Dates in medical notes while improving overall WER. For alphanumeric digit sequences we show improvements to Character Error Rate and Sentence Accuracy",
    "checked": true,
    "id": "645e9909180d729a069b71f7c52750b534e28a83",
    "semantic_title": "using text injection to improve recognition of personal identifiers in speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grosz23_interspeech.html": {
    "title": "Investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a Finnish model",
    "volume": "main",
    "abstract": "Self-supervised speech models, such as the wav2vec2, have become extremely popular in the past few years. Their main appeal is that after their pre-training on a large amount of audio, they require only a small amount of supervised, finetuning data to achieve outstanding results. Despite their immense success, very little is understood about the pre-trained models and how finetuning changes them. In this work, we take the first steps towards a better understanding of wav2vec2 systems using model interpretation tools such as visualization and latent embedding clustering. Through our analysis, we gain new insights into the abilities of the pre-trained networks and the effect that finetuning has on them. We demonstrate that the clusters learned by the pre-trained model are just as important a factor as the supervised training data distribution in determining the accuracy of the finetuned system, which could aid us in selecting the most suitable pre-trained model for the supervised data",
    "checked": true,
    "id": "2ae665744fc5662c89168f14f3153144936cdd37",
    "semantic_title": "investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a finnish model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lehecka23_interspeech.html": {
    "title": "Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech",
    "volume": "main",
    "abstract": "This paper is a step forward in our effort to make vast oral history archives more accessible to the public and researchers by breaking down the decoding barriers between the knowledge encoded in the spoken testimonies and users who want to search for the information of their interest. We present new Transformer-based monolingual models suitable for speech recognition of oral history archives in English, German, and Czech. Our experiments show that although the all-purpose speech recognition systems have recently made tremendous progress, the transcription of oral history archives is still a challenging task for them; our tailored models significantly outperformed larger public multilingual models and scored new state-of-the-art results on all tested datasets. Due to the 2-phase fine-tuning process, our models are robust and can be used for oral history archives of various domains. We publicly release our models within a public speech recognition service",
    "checked": true,
    "id": "d26d8f4c79120558aa4f8131df408af091aff001",
    "semantic_title": "transformer-based speech recognition models for oral history archives in english, german, and czech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23_interspeech.html": {
    "title": "Iteratively Improving Speech Recognition and Voice Conversion",
    "volume": "main",
    "abstract": "Many existing works on voice conversion (VC) tasks use automatic speech recognition (ASR) models for ensuring linguistic consistency between source and converted samples. However, for the low-data resource domains, training a high-quality ASR remains to be a challenging task. In this work, we propose a novel iterative way of improving both the ASR and VC models. We first train an ASR model which is used to ensure content preservation while training a VC model. In the next iteration, the VC model is used as a data augmentation method to further fine-tune the ASR model and generalize it to diverse speakers. By iteratively leveraging the improved ASR model to train VC model and vice-versa, we experimentally show improvement in both the models. Our proposed framework outperforms the ASR and one-shot VC baseline models on English singing and Hindi speech domains in subjective and objective evaluations in low-data resource settings",
    "checked": true,
    "id": "e9f0e92d7a18cc78b71146d67358fcfd1a6bdc4d",
    "semantic_title": "iteratively improving speech recognition and voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fatehi23_interspeech.html": {
    "title": "LABERT: A Combination of Local Aggregation and Self-Supervised Speech Representation Learning for Detecting Informative Hidden Units in Low-Resource ASR Systems",
    "volume": "main",
    "abstract": "With advances in deep learning methodologies, Automatic Speech Recognition (ASR) systems have seen impressive results. However, ASR in Low-Resource Environments (LREs) are challenged by a lack of training data for the specific target domain. We propose that data sampling criteria for choosing more informative speech samples can be critical to addressing the problem of training data bottleneck. Our proposed Local Aggregation BERT (LABERT) method for self-supervised speech representation learning fuses an active learning model with an adapted local aggregation metric. Active learning is used to pick informative speech units, whereas the aggregation metric forces the model to move similar data together in the latent space while separating dissimilar instances to detect hidden units in LRE tasks. We evaluate LABERT with two LRE datasets: I-CUBE and UASpeech to explore the performance of our model in the LRE ASR problems",
    "checked": true,
    "id": "046609639dcd941f5438bde0eb73481f60411735",
    "semantic_title": "labert: a combination of local aggregation and self-supervised speech representation learning for detecting informative hidden units in low-resource asr systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xue23_interspeech.html": {
    "title": "TranUSR: Phoneme-to-word Transcoder Based Unified Speech Representation Learning for Cross-lingual Speech Recognition",
    "volume": "main",
    "abstract": "UniSpeech has achieved superior performance in cross-lingual automatic speech recognition (ASR) by explicitly aligning latent representations to phoneme units using multi-task self-supervised learning. While the learned representations transfer well from high-resource to low-resource languages, predicting words directly from these phonetic representations in downstream ASR is challenging. In this paper, we propose TranUSR, a two-stage model comprising a pre-trained UniData2vec and a phoneme-to-word Transcoder. Different from UniSpeech, UniData2vec replaces the quantized discrete representations with continuous and contextual representations from a teacher model for phonetically-aware pre-training. Then, Transcoder learns to translate phonemes to words with the aid of extra texts, enabling direct word generation. Experiments on Common Voice show that UniData2vec reduces PER by 5.3% compared to UniSpeech, while Transcoder yields a 14.4% WER reduction compared to grapheme fine-tuning",
    "checked": true,
    "id": "c03ec12dd7bd78123cd4b7959d6eacd0d4b2ae7b",
    "semantic_title": "tranusr: phoneme-to-word transcoder based unified speech representation learning for cross-lingual speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23e_interspeech.html": {
    "title": "Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR",
    "volume": "main",
    "abstract": "ASR systems in real applications must be adapted on the fly to correctly recognize task-specific contextual terms, such as contacts, application names and media entities. However, it is challenging to achieve scalability, large in-domain quality gains, and minimal out-of-domain quality regressions simultaneously. In this work, we introduce an effective neural biasing architecture called Dual-Mode NAM. Dual-Mode NAM embeds a top-k search process in its attention mechanism in a trainable fashion to perform an accurate top-k phrase selection before injecting the corresponding word-piece context into the acoustic encoder. We further propose a controllable mechanism to enable the ASR system to be able to trade off its in-domain and out-of-domain quality at inference time. When evaluated on a large-scale biasing benchmark, the combined techniques improve a previously proposed method with an average in-domain and out-of-domain WER reduction by up to 53.3% and 12.0% relative respectively",
    "checked": true,
    "id": "7e39237918eb9b5c0b3baa66b174a268c684a975",
    "semantic_title": "dual-mode nam: effective top-k context injection for end-to-end asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23g_interspeech.html": {
    "title": "GhostRNN: Reducing State Redundancy in RNN with Cheap Operations",
    "volume": "main",
    "abstract": "Recurrent neural network (RNNs) that are capable of modeling long-distance dependencies are widely used in various speech tasks, eg., keyword spotting (KWS) and speech enhancement (SE). Due to the limitation of power and memory in low-resource devices, efficient RNN models are urgently required for real-world applications. In this paper, we propose an efficient RNN architecture, GhostRNN, which reduces hidden state redundancy with cheap operations. In particular, we observe that partial dimensions of hidden states are similar to the others in trained RNN models, suggesting that redundancy exists in specific RNNs. To reduce the redundancy and hence computational cost, we propose to first generate a few intrinsic states, and then apply cheap operations to produce ghost states based on the intrinsic states. Experiments on KWS and SE tasks demonstrate that the proposed GhostRNN significantly reduces the memory usage (~40%) and computation cost while keeping performance similar. Codes will be available at https://gitee.com/mindspore/models/tree/master/research/audio/ghostrnn",
    "checked": true,
    "id": "2ca7637abf82a22f345829ff6a6e065c67a3925f",
    "semantic_title": "ghostrnn: reducing state redundancy in rnn with cheap operations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23da_interspeech.html": {
    "title": "Task-Agnostic Structured Pruning of Speech Representation Models",
    "volume": "main",
    "abstract": "Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have been shown to significantly improve many speech tasks. However, their large memory and strong computational requirements hinder their industrial applicability. Structured pruning is a hardware-friendly model compression technique but usually results in a larger loss of accuracy. In this paper, we propose a fine-grained attention head pruning method to compensate for the performance degradation. In addition, we also introduce the straight through estimator into the L0 regularization to further accelerate the pruned model. Experiments on the SUPERB benchmark show that our model can achieve comparable performance to the dense model in multiple tasks and outperforms the Wav2vec 2.0 base model on average, with 72% fewer parameters and 2 times faster inference speed",
    "checked": true,
    "id": "99aa56c8136a83756c4b8d901941c5bb2b2dcdac",
    "semantic_title": "task-agnostic structured pruning of speech representation models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kanda23_interspeech.html": {
    "title": "Factual Consistency Oriented Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents a novel optimization framework for automatic speech recognition (ASR) with the aim of reducing hallucinations produced by an ASR model. The proposed framework optimizes the ASR model to maximize an expected factual consistency score between ASR hypotheses and ground-truth transcriptions, where the factual consistency score is computed by a separately trained estimator. Experimental results using the AMI meeting corpus and the VoxPopuli corpus show that the ASR model trained with the proposed framework generates ASR hypotheses that have significantly higher consistency scores with ground-truth transcriptions while maintaining the word error rates close to those of cross entropy-trained ASR models. Furthermore, it is shown that training the ASR models with the proposed framework improves the speech summarization quality as measured by the factual consistency of meeting conversation summaries generated by a large language model",
    "checked": true,
    "id": "979a68d069fe7b41105085e9c6182da5058665b6",
    "semantic_title": "factual consistency oriented speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fathullah23_interspeech.html": {
    "title": "Multi-Head State Space Model for Speech Recognition",
    "volume": "main",
    "abstract": "State space models (SSMs) have recently shown promising results on small-scale sequence and language modelling tasks, rivalling and outperforming many attention-based approaches. In this paper, we propose a multi-head state space (MH-SSM) architecture equipped with special gating mechanisms, where parallel heads are taught to learn local and global temporal dynamics on sequence data. As a drop-in replacement for multi-head attention in transformer encoders, this new model significantly outperforms the transformer transducer on the LibriSpeech speech recognition corpus. Furthermore, we augment the transformer block with MH-SSMs layers, referred to as the Stateformer, achieving state-of-the-art performance on the LibriSpeech task, with word error rates of 1.76%/4.37% on the development and 1.91%/4.36% on the test sets without using an external language model",
    "checked": true,
    "id": "067aaf0d1cde4ee21063be137559f2fe50125570",
    "semantic_title": "multi-head state space model for speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23_interspeech.html": {
    "title": "Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search",
    "volume": "main",
    "abstract": "Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance",
    "checked": true,
    "id": "9c607471820fe74572e142fc6e9ce432716048c8",
    "semantic_title": "cascaded multi-task adaptive learning based on neural architecture search",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martin23_interspeech.html": {
    "title": "Probing Self-supervised Speech Models for Phonetic and Phonemic Information: A Case Study in Aspiration",
    "volume": "main",
    "abstract": "Textless self-supervised speech models have grown in capabilities in recent years, but the nature of the linguistic information they encode has not yet been thoroughly examined. We evaluate the extent to which these models' learned representations align with basic representational distinctions made by humans, focusing on a set of phonetic (low-level) and phonemic (more abstract) contrasts instantiated in word-initial stops. We find that robust representations of both phonetic and phonemic distinctions emerge in early layers of these models' architectures, and are preserved in the principal components of deeper layer representations. Our analyses suggest two sources for this success: some can only be explained by the optimization of the models on speech data, while some can be attributed to these models' high-dimensional architectures. Our findings show that speech-trained HuBERT derives a low-noise and low-dimensional subspace corresponding to abstract phonological distinctions",
    "checked": true,
    "id": "7373477778b12d453191b09d46e2f77f4295ca52",
    "semantic_title": "probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/harding23_interspeech.html": {
    "title": "Selective Biasing with Trie-based Contextual Adapters for Personalised Speech Recognition using Neural Transducers",
    "volume": "main",
    "abstract": "Neural transducer ASR models achieve state of the art accuracy on many tasks, however rare word recognition poses a particular challenge as models often fail to recognise words that occur rarely, or not at all, in the training data. Methods of contextual biasing, where models are dynamically adapted to bias their outputs towards a given list of relevant words and phrases, have been shown to be effective at alleviating this issue. While such methods are effective at improving rare word recognition, over-biasing can lead to degradation on common words. In this work we propose several extensions to a recently proposed trie-based method of contextual biasing. We show how performance of the method can be improved in terms of rare word recognition, especially in the case of very large catalogues, by introducing a simple normalisation term, how the method can be trained as an adapter module, and how selective biasing can be applied to practically eliminate over-biasing on common words",
    "checked": true,
    "id": "c261ebb5eec148522963b9c6bdd958e463ebcc2c",
    "semantic_title": "selective biasing with trie-based contextual adapters for personalised speech recognition using neural transducers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23b_interspeech.html": {
    "title": "Robust Prototype Learning for Anomalous Sound Detection",
    "volume": "main",
    "abstract": "In this paper, we present a robust prototype learning framework for anomalous sound detection (ASD), where prototypical loss is exploited to measure the similarity between samples and prototypes. We show that existing generative and discriminative based ASD methods can be unified into this framework from the perspective of prototypical learning. For ASD in recent DCASE challenges, extensions related to imbalanced learning are proposed to improve the robustness of prototypes learned from source and target domains. Specifically, balanced sampling and multiple-prototype expansion (MPE) strategies are proposed to address imbalances across attributes of source and target domains. Furthermore, a novel negative-prototype expansion (NPE) method is used to construct pseudo-anomalies to learn a more compact and effective embedding space for normal sounds. Evaluation on the DCASE2022 Task2 development dataset demonstrates the validity of the proposed prototype learning framework",
    "checked": true,
    "id": "d6a1b187a270900853a4a89c0dcc6ae3c0a2830e",
    "semantic_title": "robust prototype learning for anomalous sound detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kushwaha23_interspeech.html": {
    "title": "A multimodal prototypical approach for unsupervised sound classification",
    "volume": "main",
    "abstract": "In the context of environmental sound classification, the adaptability of systems is key: which sound classes are interesting depends on the context and the user's needs. Recent advances in text-to-audio retrieval allow for zero-shot audio classification, but performance compared to supervised models remains limited. This work proposes a multimodal prototypical approach that exploits local audio-text embeddings to provide more relevant answers to audio queries, augmenting the adaptability of sound detection in the wild. We do this by first using text to query a nearby community of audio embeddings that best characterize each query sound, and select the group's centroids as our prototypes. Second, we compare unseen audio to these prototypes for classification. We perform multiple ablation studies to understand the impact of the embedding models and prompts. Our unsupervised approach improves upon the zero-shot state-of-the-art in three sound recognition benchmarks by an average of 12%",
    "checked": true,
    "id": "fb082d89b1f8906509991f738961e1cbe21d7435",
    "semantic_title": "a multimodal prototypical approach for unsupervised sound classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wen23_interspeech.html": {
    "title": "Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms",
    "volume": "main",
    "abstract": "Robust audio anti-spoofing has been increasingly challeng- ing due to the recent advancements on deepfake techniques. While spectrograms have demonstrated their capability for anti- spoofing, complementary information presented in multi-order spectral patterns have not been well explored, which limits their effectiveness for varying spoofing attacks. Therefore, we propose a novel deep learning method with a spectral fusion- reconstruction strategy, namely S2pecNet, to utilise multi-order spectral patterns for robust audio anti-spoofing representations. Specifically, spectral patterns up to second-order are fused in a coarse-to-fine manner and two branches are designed for the fine-level fusion from the spectral and temporal contexts. A reconstruction from the fused representation to the input spec- trograms further reduces the potential fused information loss. Our method achieved the state-of-the-art performance with an EER of 0.77% on a widely used dataset - ASVspoof2019 LA Challenge",
    "checked": true,
    "id": "9f8e5fa471adab4938b3145fcb41b79c22ec2f0b",
    "semantic_title": "robust audio anti-spoofing with fusion-reconstruction learning on multi-order spectrograms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23c_interspeech.html": {
    "title": "Adapting Language-Audio Models as Few-Shot Audio Learners",
    "volume": "main",
    "abstract": "Contrastive language-audio pretraining (CLAP) has become a new paradigm to learn audio concepts with audio-text pairs. CLAP models have shown unprecedented performance as zero-shot classifiers on downstream tasks. To further adapt CLAP with domain-specific knowledge, a popular method is to finetune its audio encoder with available labelled examples. However, this is challenging in low-shot scenarios, as the amount of annotations is limited compared to the model size. In this work, we introduce a Training-efficient (Treff) adapter to rapidly learn with a small set of examples while maintaining the capacity for zero-shot classification. First, we propose a cross-attention linear model (CALM) to map a set of labelled examples and test audio to test labels. Second, we find initialising CALM as a cosine measurement improves our Treff adapter even without training. The Treff adapter beats metric-based methods in few-shot settings and yields competitive results to fully-supervised methods",
    "checked": true,
    "id": "237032b6256087766e6d366a47227aef980fd2b7",
    "semantic_title": "adapting language-audio models as few-shot audio learners",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23l_interspeech.html": {
    "title": "TFECN: Time-Frequency Enhanced ConvNet for Audio Classification",
    "volume": "main",
    "abstract": "Recently, transformer-based models have shown leading performance in audio classification, gradually replacing the dominant ConvNet in the past. However, some research has shown that certain characteristics and designs in transformers can be applied to other architectures and make them achieve similar performance as transformers. In this paper, we introduce TFECN, a pure ConvNet that combines the design in transformers and has time-frequency enhanced convolution with large kernels. It can provide a global receptive field on the frequency dimension as well as avoid the influence of the convolution's shift-equivariance on the recognition of not shift-invariant patterns along the frequency axis. Furthermore, to use ImageNet-pretrained weights, we propose a method for transferring weights between kernels of different sizes. On the commonly used datasets AudioSet, FSD50K, and ESC50, our TFECN outperforms the models trained in the same way",
    "checked": true,
    "id": "8cb1a3348c7004f0d3c4aa666edd09532da0b4db",
    "semantic_title": "tfecn: time-frequency enhanced convnet for audio classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23b_interspeech.html": {
    "title": "Resolution Consistency Training on Time-Frequency Domain for Semi-Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "The fact that unlabeled data can be used for supervised learning is of considerable relevance concerning polyphonic sound event detection (PSED) because of the high costs of frame-wise labeling. While semi-supervised learning (SSL) for image tasks has been extensively developed, SSL for PSED has not been substantially explored due to data augmentation limitations. In this paper, we propose a novel SSL strategy for PSED called resolution consistency training (ResCT), combining unsupervised terms with the mean teacher using different resolutions of a spectrogram for data augmentation. The proposed method regularizes the consistency between the model predictions for different resolutions by controlling the sampling rate and window size. Experimental results show that ResCT outperforms other SSL methods on various evaluation metrics: event-f1 score, intersection-f1 score, and PSDSs. Finally, we report on some ablation studies for the weak and strong augmentation policies",
    "checked": true,
    "id": "24624c92335e516840fe5f362d34ab272c66f78b",
    "semantic_title": "resolution consistency training on time-frequency domain for semi-supervised sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23n_interspeech.html": {
    "title": "Fine-tuning Audio Spectrogram Transformer with Task-aware Adapters for Sound Event Detection",
    "volume": "main",
    "abstract": "In this paper, we present a task-aware fine-tuning method to transfer Patchout faSt Spectrogram Transformer (PaSST) model to sound event detection (SED) task. Pretrained PaSST has shown significant performance on audio tagging (AT) and SED tasks, but it is not optimal to fine-tune the model from a single layer as the local and semantic information have not been well exploited. To address this, we first introduce task-aware adapters including SED-adapter and AT-adapter to fine-tune PaSST for SED and AT task respectively, and then propose task-aware fine-tuning to combine local information from shallower layer with semantic information from deeper layer, based on task-aware adapters. Besides, we propose the self-distillated mean teacher (SdMT) to train a robust student model with soft pseudo labels from teacher. Experiments are conducted on DCASE2022 task4 development set, the EB-F1 of 64.85% and PSDS1 of 0.5548 are achieved which outperform previous state-of-the-art systems",
    "checked": true,
    "id": "5035bf01ae576ab415d822284e629eb3734c6708",
    "semantic_title": "fine-tuning audio spectrogram transformer with task-aware adapters for sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23b_interspeech.html": {
    "title": "Small Footprint Multi-channel Network for Keyword Spotting with Centroid Based Awareness",
    "volume": "main",
    "abstract": "Spoken Keyword Spotting (KWS) in noisy far-field environments is challenging for small-footprint models, given the restrictions on computational resources (e.g., model size, running memory). This is even more intricate when handling noises from multiple microphones. To address this, we present a new multi-channel model that uses a CNN-based network with a linear mixing unit to achieve local-global dependency representations. Our method enhances noise-robustness while ensuring more efficient computation. Besides, we propose an end-to-end centroid-based awareness module that provides class similarity awareness at the bottleneck level to correct ambiguous cases during prediction. We conducted experiments using real noisy far-field data from the MISP challenge 2021 and achieved SOTA results compared to existing small-footprint KWS models. Our best score of 0.126 is highly competitive against larger models like 3D-ResNet, which is 0.122, but ours is much smaller at 473K compared to 13M",
    "checked": true,
    "id": "ca0781a2185f29b95cb0d7b627911a3e4162c975",
    "semantic_title": "small footprint multi-channel network for keyword spotting with centroid based awareness",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23b_interspeech.html": {
    "title": "Few-shot Class-incremental Audio Classification Using Adaptively-refined Prototypes",
    "volume": "main",
    "abstract": "New classes of sounds constantly emerge with a few samples, making it challenging for models to adapt to dynamic acoustic environments. This challenge motivates us to address the new problem of few-shot class-incremental audio classification. This study aims to enable a model to continuously recognize new classes of sounds with a few training samples of new classes while remembering the learned ones. To this end, we propose a method to generate discriminative prototypes and use them to expand the model's classifier for recognizing sounds of new and learned classes. The model is first trained with a random episodic training strategy, and then its backbone is used to generate the prototypes. A dynamic relation projection module refines the prototypes to enhance their discriminability. Results on two datasets (derived from the corpora of Nsynth and FSD-MIX-CLIPS) show that the proposed method exceeds three state-of-the-art methods in average accuracy and performance dropping rate",
    "checked": true,
    "id": "178677fe7e568509249e657635287bf6285ba00e",
    "semantic_title": "few-shot class-incremental audio classification using adaptively-refined prototypes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vali23_interspeech.html": {
    "title": "Interpretable Latent Space Using Space-Filling Curves for Phonetic Analysis in Voice Conversion",
    "volume": "main",
    "abstract": "Vector quantized variational autoencoders (VQ-VAE) are well-known deep generative models, which map input data to a latent space that is used for data generation. Such latent spaces are unstructured and can thus be difficult to interpret. Some earlier approaches have introduced a structure to the latent space through supervised learning by defining data labels as latent variables. In contrast, we propose an unsupervised technique incorporating space-filling curves into vector quantization (VQ), which yields an arranged form of latent vectors such that adjacent elements in the VQ codebook refer to similar content. We applied this technique to the latent codebook vectors of a VQ-VAE, which encode the phonetic information of a speech signal in a voice conversion task. Our experiments show there is a clear arrangement in latent vectors representing speech phones, which clarifies what phone each latent vector corresponds to and facilitates other detailed interpretations of latent vectors",
    "checked": true,
    "id": "4f3e14e1d39ca360c76239dde618ea44500ed98a",
    "semantic_title": "interpretable latent space using space-filling curves for phonetic analysis in voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tulchinskii23_interspeech.html": {
    "title": "Topological Data Analysis for Speech Processing",
    "volume": "main",
    "abstract": "We apply topological data analysis (TDA) to speech classification problems and to the introspection of a pretrained speech model, HuBERT. To this end, we introduce a number of topological and algebraic features derived from Transformer attention maps and embeddings. We show that a simple linear classifier built on top of such features outperforms a fine-tuned classification head. We achieve an improvement of about 9% accuracy and 5% ERR on two common datasets; on CREMA-D, the proposed feature set reaches a new state of the art performance with accuracy 80.155. We also show that topological features are able to reveal functional roles of speech Transformer heads; e.g., we find the heads capable to distinguish between pairs of sample sources (natural/synthetic) or voices without any downstream fine-tuning. Our results demonstrate that TDA is a promising new approach for speech analysis, especially for tasks that require structural prediction",
    "checked": true,
    "id": "12c37eaa9ab37d66e1b014fd79b62bf544522065",
    "semantic_title": "topological data analysis for speech processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jang23_interspeech.html": {
    "title": "Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation",
    "volume": "main",
    "abstract": "Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark",
    "checked": true,
    "id": "d65dd690635d6a220360b2193e6d020da24330c2",
    "semantic_title": "recycle-and-distill: universal compression strategy for transformer-based speech ssl models with attention map reusing and masking distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koppelmann23_interspeech.html": {
    "title": "Personalized Acoustic Scene Classification in Ultra-low Power Embedded Devices Using Privacy-preserving Data Augmentation",
    "volume": "main",
    "abstract": "In this work we present an adaptation method for personalized acoustic scene classification in ultra-low power embedded devices (EDs). The computational limitation of EDs and a large variety of acoustic scenes may lead to poor performance of the embedded classifier in specific real-world user environments. We propose a semi-supervised scheme that estimates the audio feature distribution at ED level and then samples this statistical model to generate artificial data points which emulate user-specific audio features. Then, a second, cloud-based classifier assigns pseudo labels to samples, which are merged with existing labeled data for retraining the embedded classifier. The proposed method leads to significant performance improvements on user-specific data sets and does neither require a persistent connection to a cloud service nor the transmission of raw audio or audio features. It thus results in low data rates, high utility, and privacy-preservation",
    "checked": true,
    "id": "0e85062bba41b628de2d96cac8d77d03191e6a5c",
    "semantic_title": "personalized acoustic scene classification in ultra-low power embedded devices using privacy-preserving data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23_interspeech.html": {
    "title": "Background Domain Switch: A Novel Data Augmentation Technique for Robust Sound Event Detection",
    "volume": "main",
    "abstract": "Data augmentation is a key component to achieve robust and generalizable performance in sound event detection (SED). A well trained SED model should be able to resist the interference of non-target audio events and maintain a robust recognition rate under unknown and possibly mismatched testing conditions. In this study, we propose a novel background domain switch (BDS) data augmentation technique for SED. BDS utilizes a trained SED model on-the-fly to detect backgrounds in audio clips, and switches them among the data points to increase sample variability. This approach can be easily combined with other types of data augmentation techniques. We evaluate the effectiveness of BDS by applying it to several state-of-the-art SED frameworks, and used both publicly available datasets as well as a synthesized mismatched test set. Experiment results systematically show that BDS obtains significant performance improvements from all evaluation aspects. The code is available at: https://github.com/boschresearch/soundseebackgrounddomainswitch",
    "checked": true,
    "id": "cfcf644b1805c24fdbc9e6cddfac41383a2f0d8f",
    "semantic_title": "background domain switch: a novel data augmentation technique for robust sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hou23_interspeech.html": {
    "title": "Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning",
    "volume": "main",
    "abstract": "Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans' listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR",
    "checked": true,
    "id": "e74522543b25c3eca86f7dd62793dbf8d527e8be",
    "semantic_title": "joint prediction of audio event and annoyance rating in an urban soundscape by hierarchical graph representation learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23fa_interspeech.html": {
    "title": "Anomalous Sound Detection Using Self-Attention-Based Frequency Pattern Analysis of Machine Sounds",
    "volume": "main",
    "abstract": "Different machines can exhibit diverse frequency patterns in their emitted sound. This feature has been recently explored in anomaly sound detection and reached state-of-the-art performance. However, existing methods rely on the manual or empirical determination of the frequency filter by observing the effective frequency range in the training data, which may be impractical for general application. This paper proposes an anomalous sound detection method using self-attention-based frequency pattern analysis and spectral-temporal information fusion. Our experiments demonstrate that the self-attention module automatically and adaptively analyses the effective frequencies of a machine sound and enhances that information in the spectral feature representation. With spectral-temporal information fusion, the obtained audio feature eventually improves the anomaly detection performance on the DCASE 2020 Challenge Task 2 dataset",
    "checked": true,
    "id": "5656e4b77179dbc012460460bf92334f45b9235a",
    "semantic_title": "anomalous sound detection using self-attention-based frequency pattern analysis of machine sounds",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23c_interspeech.html": {
    "title": "Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions",
    "volume": "main",
    "abstract": "Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained crossmodal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical crossmodal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets",
    "checked": true,
    "id": "29dc27e49654fc7cd0a9001bbf44a57d78bb74ca",
    "semantic_title": "improving audio-text retrieval via hierarchical cross-modal interaction and auxiliary captions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bn23_interspeech.html": {
    "title": "Differential Privacy enabled Dementia Classification: An Exploration of the Privacy-Accuracy Trade-off in Speech Signal Data",
    "volume": "main",
    "abstract": "Early detection of dementia is critical for effective symptom management. Recent studies have aimed to develop machine learning (ML) models to identify dementia onset and severity using language and speech features. However, existing methods can lead to serious privacy concerns due to sensitive data collected from a vulnerable population. In this work, we aim to establish the privacy-accuracy tradeoff benchmark for dementia classification models using audio and speech features. Specifically, we explore the effects of differential privacy (DP) on the training phase of the audio model. We then compare the classification accuracy of DP and non-DP models using a publicly available dataset. The resultant comparison provides useful insights to make informed decisions about the need for balancing privacy and accuracy tradeoff for dementia classification tasks. Our findings have implications for real-world deployment of ML models to support early detection and effective management of dementia",
    "checked": true,
    "id": "7304c74495f8c4630a852f614e8a613a3b0e410b",
    "semantic_title": "differential privacy enabled dementia classification: an exploration of the privacy-accuracy trade-off in speech signal data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ka_interspeech.html": {
    "title": "Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech",
    "volume": "main",
    "abstract": "Effective speech emotional representations play a key role in Speech Emotion Recognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional speech samples are more difficult and expensive to acquire compared with Neutral style speech, which causes one issue that most related works unfortunately neglect: imbalanced datasets. Models might overfit to the majority Neutral class and fail to produce robust and effective emotional representations. In this paper, we propose an Emotion Extractor to address this issue. We use augmentation approaches to train the model and enable it to extract effective and generalizable emotional representations from imbalanced datasets. Our empirical results show that (1) for the SER task, the proposed Emotion Extractor surpasses the state-of-the-art baseline on three imbalanced datasets; (2) the produced representations from our Emotion Extractor benefit the TTS model, and enable it to synthesize more expressive speech",
    "checked": true,
    "id": "044e47bc8995d603122963d92b84aa6dabea1a53",
    "semantic_title": "learning emotional representations from imbalanced speech data for speech emotion recognition and emotional text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/behera23_interspeech.html": {
    "title": "Towards Multi-Lingual Audio Question Answering",
    "volume": "main",
    "abstract": "Audio Question Answering (AQA) is a multi-modal translation task where a system analyzes an audio signal and a natural language question to generate a desirable natural language answer. AQA has been primarily studied through the lens of the English language. However, addressing AQA in other languages, in the same manner, would require a considerable amount of resources. This paper proposes scalable solutions to multi-lingual audio question answering on both data and modeling fronts. We propose mClothoAQA, a translation-based multi-lingual AQA dataset in eight languages. The dataset consists of 1991 audio files and nearly 0.3 million question-answer pairs. Finally, we introduce a multi-lingual AQA model and demonstrate its strong performance in eight languages. The dataset and code can be accessed at https://github.com/swarupbehera/mAQA",
    "checked": true,
    "id": "431ec63f76d5d64c18e3ab92008cca98975a1678",
    "semantic_title": "towards multi-lingual audio question answering",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/aldarmaki23_interspeech.html": {
    "title": "Diacritic Recognition Performance in Arabic ASR",
    "volume": "main",
    "abstract": "In Arabic text, most vowels are encoded in the form of diacritics that are often omitted, so most speech corpora and ASR models are undiacritized. Text-based diacritization has previously been used to preprocess the input or post-processs ASR hypotheses. It is generally believed that input diacritization degrades ASR quality, but no systematic evaluation of ASR diacritization performance has been conducted to date. We experimentally clarify whether input diacritiztation indeed degrades ASR quality and compare ASR diacritization with text-based diacritization. We fine-tune pre-trained ASR models on transcribed speech with different diacritization conditions: manual, automatic, and no diacritization. We isolate diacritic recognition performance from the overall ASR performance using coverage and precision metrics. We find that ASR diacritization significantly outperforms text-based diacritization, particularly when the ASR model is fine-tuned with manually diacritized transcripts",
    "checked": true,
    "id": "cd31445ba366d10bd11e2a7b2da5c8281c8f1564",
    "semantic_title": "diacritic recognition performance in arabic asr",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kolehmainen23_interspeech.html": {
    "title": "Personalization for BERT-based Discriminative Speech Recognition Rescoring",
    "volume": "main",
    "abstract": "Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/krishnan23_interspeech.html": {
    "title": "On the N-gram Approximation of Pre-trained Language Models",
    "volume": "main",
    "abstract": "Large pre-trained language models (PLMs) have shown remarkable performance across various natural language understanding (NLU) tasks, particularly in low-resource settings. Nevertheless, their potential in Automatic Speech Recognition (ASR) remains largely unexplored. This study investigates the potential usage of PLMs for language modelling in ASR. We compare the application of large-scale text sampling and probability conversion for approximating GPT-2 into an n-gram model. Furthermore, we introduce a vocabulary-restricted decoding method for random sampling, and evaluate the effects of domain difficulty and data size on the usability of generated text. Our findings across eight domain-specific corpora support the use of sampling-based approximation and show that interpolating with a large sampled corpus improves test perplexity over a baseline trigram by 15%. Our vocabulary-restricted decoding method pushes this improvement further by 5% in domain-specific settings",
    "checked": true,
    "id": "821d918fd7c316a1d48579979fa7bde2f7a63c50",
    "semantic_title": "on the n-gram approximation of pre-trained language models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23g_interspeech.html": {
    "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
    "volume": "main",
    "abstract": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers' true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction",
    "checked": true,
    "id": "b3c7d778e5bdafa8b2cd62b996d0e6dfc670effc",
    "semantic_title": "record deduplication for entity distribution modeling in asr transcripts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/agrawal23_interspeech.html": {
    "title": "Learning When to Trust Which Teacher for Weakly Supervised ASR",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) training can utilize multiple experts as teacher models, each trained on a specific domain or accent. Teacher models may be opaque in nature since their architecture may be not be known or their training cadence is different from that of the student ASR model. Still, the student models are updated incrementally using the pseudo-labels generated independently by the expert teachers. In this paper, we exploit supervision from multiple domain experts in training student ASR models. This training strategy is especially useful in scenarios where few or no human transcriptions are available. To that end, we propose a Smart-Weighter mechanism that selects an appropriate expert based on the input audio, and then trains the student model in an unsupervised setting. We show the efficacy of our approach using LibriSpeech and LibriLight benchmarks and find an improvement of 4 to 25% over baselines that uniformly weight all the experts, use a single expert model, or combine experts using ROVER",
    "checked": true,
    "id": "124c16b1243ef8f727df9db70d3c2f1b47ec31c1",
    "semantic_title": "learning when to trust which teacher for weakly supervised asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23f_interspeech.html": {
    "title": "Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer",
    "volume": "main",
    "abstract": "Domain adaptation using text-only corpus is challenging in end-to-end(E2E) speech recognition. Adaptation by synthesizing audio from text through TTS is resource-consuming. We present a method to learn Unified Speech-Text Representation in Conformer Transducer(USTR-CT) to enable fast domain adaptation using the text-only corpus. Different from the previous textogram method, an extra text encoder is introduced in our work to learn text representation and is removed during inference, so there is no modification for online deployment. To improve the efficiency of adaptation, single-step and multistep adaptations are also explored. The experiments on adapting LibriSpeech to SPGISpeech show the proposed method reduces the word error rate(WER) by relatively 44% on the target domain, which is better than those of TTS method and textogram method. Also, it is shown the proposed method can be combined with internal language model estimation(ILME) to further improve the performance",
    "checked": true,
    "id": "f035b38c98d79dd43dda10b919604b1d46cb63df",
    "semantic_title": "text-only domain adaptation using unified speech-text representation in transducer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23e_interspeech.html": {
    "title": "Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model",
    "volume": "main",
    "abstract": "In this paper, we show that representations capturing syllabic units emerge when training a self-supervised speech model with a visually-grounded training objective. We demonstrate that a nearly identical model architecture (HuBErT) trained with a masked language modeling loss does not exhibit this same ability, suggesting that the visual grounding objective is responsible for the emergence of this phenomenon. We propose the use of a minimum cut algorithm to automatically predict syllable boundaries in speech, followed by a 2-stage clustering method to group identical syllables together. We show that our model not only outperforms a state-of-the-art syllabic segmentation method on the language it was trained on (English), but also generalizes in a zero-shot fashion to Estonian. Finally, we show that the same model is capable of zero-shot generalization for a word segmentation task on 4 other languages from the Zerospeech Challenge, in some cases beating the previous state-of-the-art",
    "checked": true,
    "id": "47ba6504f14a3d16f25b1b9afaf5531e41671faf",
    "semantic_title": "syllable discovery and cross-lingual generalization in a visually grounded, self-supervised speech model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23d_interspeech.html": {
    "title": "Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization",
    "volume": "main",
    "abstract": "We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. we design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available here",
    "checked": true,
    "id": "10e8dc07ea256c6a88d7043cf135417402ed38f4",
    "semantic_title": "prompting the hidden talent of web-scale speech models for zero-shot task generalization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moore23_interspeech.html": {
    "title": "Progress and Prospects for Spoken Language Technology: Results from Five Sexennial Surveys",
    "volume": "main",
    "abstract": "Every six years (since 1997), a survey has been conducted at the IEEE workshop on Automatic Speech Recognition and Understanding (ASRU). The aim has been to gain an insight into the research community's perspective on the 'progress and prospects' for spoken language technology. These surveys have been based on a set of 'statements' describing possible scenarios, and respondents are asked to estimate the year (in the future or in the past) when each given scenario might be realised. A number of the statements have appeared in multiple surveys, hence it has been possible to track changes in opinion over time. This paper presents the combined results from five such surveys, the most recent of which was conducted at ASRU-2021. The latest results reveal a dramatic increase in optimism",
    "checked": true,
    "id": "9098d7a04eb5d8a1d64e57e808d240a426a16be1",
    "semantic_title": "progress and prospects for spoken language technology: results from five sexennial surveys",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sanabria23_interspeech.html": {
    "title": "Acoustic Word Embeddings for Untranscribed Target Languages with Continued Pretraining and Learned Pooling",
    "volume": "main",
    "abstract": "Acoustic word embeddings are typically created by training a pooling function using pairs of word-like units. For unsupervised systems, these are mined using k-nearest neighbor (KNN) search, which is slow. Recently, mean-pooled representations from a pre-trained self-supervised English model were suggested as a promising alternative, but their performance on target languages was not fully competitive. Here, we explore improvements to both approaches: we use continued pre-training to adapt the self-supervised model to the target language, and we use a multilingual phone recognizer (MPR) to mine phone n-gram pairs for training the pooling function. Evaluating on four languages, we show that both methods outperform a recent approach on word discrimination. Moreover, the MPR method is orders of magnitude faster than KNN, and is highly data efficient. We also show a small improvement from performing learned pooling on top of the continued pre-trained representations",
    "checked": true,
    "id": "2c82c551b151835cec78df7a2ab86f2a58d0a682",
    "semantic_title": "acoustic word embeddings for untranscribed target languages with continued pretraining and learned pooling",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23d_interspeech.html": {
    "title": "CASA-ASR: Context-Aware Speaker-Attributed ASR",
    "volume": "main",
    "abstract": "Recently, speaker-attributed automatic speech recognition (SAASR) has attracted a wide attention, which aims at answering the question \"who spoke what\". Different from modular systems, end-to-end (E2E) SA-ASR minimizes the speaker-dependent recognition errors directly and shows a promising applicability. In this paper, we propose a context-aware SAASR (CASA-ASR) model by enhancing the contextual modeling ability of E2E SA-ASR. Specifically, in CASA-ASR, a contextual text encoder is involved to aggregate the semantic information of the whole utterance, and a context-dependent scorer is employed to model the speaker discriminability by contrasting with speakers in the context. In addition, a two-pass decoding strategy is further proposed to fully leverage the contextual modeling ability resulting in a better recognition performance. Experimental results on AliMeeting corpus show that the proposed CASA-ASR model outperforms the original E2E SAASR system with a relative improvement of 11.76% in terms of speaker-dependent character error rate",
    "checked": true,
    "id": "eaf765c07764a802c5200a9abd739a921349caab",
    "semantic_title": "casa-asr: context-aware speaker-attributed asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/takahashi23_interspeech.html": {
    "title": "Unsupervised Learning of Discrete Latent Representations with Data-Adaptive Dimensionality from Continuous Speech Streams",
    "volume": "main",
    "abstract": "This work presents a novel deep generative model for unsupervised learning of sparse binary feature representations with data-adaptive dimensionality directly from continuous speech streams. Sharing the critical assumption of unbounded latent dimensionality with previously proposed Bayesian non-parametric approaches, our proposed model can capture the much richer, non-Markovian dependencies between its latent representations. The present work focuses on an investigation of our proposed model's performance in learning linguistically meaningful representations under challenging, realistic scenarios. We train our model with highly speaker-imbalanced datasets and evaluate it on the ABX phone discriminability test. Our model achieves a promising, competitive performance to the state-of-the-art model, despite its huge disadvantage: limited or no access to speaker information during training",
    "checked": true,
    "id": "1355151151f18fb3e98e65b57549ea2e87d5cf80",
    "semantic_title": "unsupervised learning of discrete latent representations with data-adaptive dimensionality from continuous speech streams",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23n_interspeech.html": {
    "title": "AD-TUNING: An Adaptive CHILD-TUNING Approach to Efficient Hyperparameter Optimization of Child Networks for Speech Processing Tasks in the SUPERB Benchmark",
    "volume": "main",
    "abstract": "In this paper, we propose AD-TUNING, an adaptive CHILD-TUNING approach for hyperparameter tuning of child networks. To address the issue of selecting an optimal hyperparameter set P, which often varies for different tasks in CHILD-TUNING, we first analyze the distribution of parameter importance to ascertain the range of P. Next, we propose a simple yet efficient early-stop algorithm to select the appropriate child network from different sizes for various speech tasks. When evaluated on seven speech processing tasks in the SUPERB benchmark, our proposed framework only requires fine-tuning less than 0.1%~10% of pre-trained model parameters for each task to achieve state-of-the-art results in most of the tasks. For instance, the DER of the speaker diarization task is 9.22% relatively lower than the previously reported best results. Other benchmark results are also very competitive. Our code is available at https://github.com/liyunlongaaa/AD-TUNING",
    "checked": true,
    "id": "8d3bbe7ded99577cf07844e63a58c51aaebe25e5",
    "semantic_title": "ad-tuning: an adaptive child-tuning approach to efficient hyperparameter optimization of child networks for speech processing tasks in the superb benchmark",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wong23_interspeech.html": {
    "title": "Distilling knowledge from Gaussian process teacher to neural network student",
    "volume": "main",
    "abstract": "Neural Networks (NN) and Gaussian Processes (GP) are different modelling approaches. The former stores characteristics of the training data in its many parameters, and then performs inference by parsing inputs through these parameters. The latter instead performs inference by computing a similarity between the test and training inputs, and then predicts test outputs that are correlated with the reference training outputs of similar inputs. These models may be combined to leverage upon their diversity. However, both combination and the matrix computations for GP inference are expensive. This paper investigates whether a NN student is able to effectively learn from the information distilled from a GP or ensemble teacher. It is computationally cheaper to infer using this student. Experiments on the speechocean762 spoken language assessment dataset suggest that learning is effective",
    "checked": true,
    "id": "480c5f9471a8822eacd04f2545b01f23539313f7",
    "semantic_title": "distilling knowledge from gaussian process teacher to neural network student",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhati23_interspeech.html": {
    "title": "Segmental SpeechCLIP: Utilizing Pretrained Image-text Models for Audio-Visual Learning",
    "volume": "main",
    "abstract": "Visually grounded models learn from paired images and their spoken captions. Recently, there have been attempts to utilize the visually grounded models trained from images and their corresponding text captions, such as CLIP, to improve speech-based visually grounded models' performance. However, the majority of these models only utilize the pretrained image encoder. Cascaded SpeechCLIP attempted to generate localized word-level information and utilize both the pretrained image and text encoders. Despite using both, they noticed a substantial drop in retrieval performance. Here, we propose to use a hierarchical segmental audio encoder that can generate a sequence of word-like units from audio. We use the pretrained CLIP text encoder on top of these word-like units representations and show significant improvements over the cascaded variant of SpeechCLIP",
    "checked": true,
    "id": "1617d389b7947161f2943e2d30afeb1856052b14",
    "semantic_title": "segmental speechclip: utilizing pretrained image-text models for audio-visual learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jacobs23_interspeech.html": {
    "title": "Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili",
    "volume": "main",
    "abstract": "We consider hate speech detection through keyword spotting on radio broadcasts. One approach is to build an automatic speech recognition (ASR) system for the target low-resource language. We compare this to using acoustic word embedding (AWE) models that map speech segments to a space where matching words have similar vectors. We specifically use a multilingual AWE model trained on labelled data from well-resourced languages to spot keywords in data in the unseen target language. In contrast to ASR, the AWE approach only requires a few keyword exemplars. In controlled experiments on Wolof and Swahili where training and test data are from the same domain, an ASR model trained on just five minutes of data outperforms the AWE approach. But in an in-the-wild test on Swahili radio broadcasts with actual hate speech keywords, the AWE model (using one minute of template data) is more robust, giving similar performance to an ASR system trained on 30 hours of labelled data",
    "checked": true,
    "id": "ce1c8b4655157435f87f9b4c5eb3589e64d3f0da",
    "semantic_title": "towards hate speech detection in low-resource languages: comparing asr to acoustic word embeddings on wolof and swahili",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vandermerwe23_interspeech.html": {
    "title": "Mitigating Catastrophic Forgetting for Few-Shot Spoken Word Classification Through Meta-Learning",
    "volume": "main",
    "abstract": "We consider the problem of few-shot spoken word classification in a setting where a model is incrementally introduced to new word classes. This would occur in a user-defined keyword system where new words can be added as the system is used. In such a continual learning scenario, a model might start to misclassify earlier words as newer classes are added, i.e. catastrophic forgetting. To address this, we propose an extension to model-agnostic meta-learning (MAML). In our new approach, each inner learning loop—where a model \"learns how to learn\" new classes—ends with a single gradient update using stored templates from all the classes that the model has already seen (one template per class). We compare this method to OML (another extension of MAML) in few-shot isolated-word classification experiments on Google Commands and FACC. Our method consistently outperforms OML in experiments where the number of shots and the final number of classes are varied",
    "checked": true,
    "id": "af849704754c3cb8ea621cca465c4c13372c9148",
    "semantic_title": "mitigating catastrophic forgetting for few-shot spoken word classification through meta-learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/polacek23_interspeech.html": {
    "title": "Online Punctuation Restoration using ELECTRA Model for streaming ASR Systems",
    "volume": "main",
    "abstract": "In this work, we propose a lightweight online approach to automatic punctuation restoration (APR), which can be utilized in speech transcription systems for, e.g., live captioning TV or radio streams. It uses only text input without prosodic features and a fine-tuned ELECTRA-Small model with a two-layer classification head. It allows for restoring question marks, commas, and periods with a very short inference time and a low latency of just three words. Our APR scheme is first tuned and compared to other architectures on a set of manual TV news transcripts. The resulting system is then compared to another real-time APR module utilizing a recurrent network and a combination of text and acoustic features. The test data we use contains automatic transcripts of radio talks and TV debates; we are also publishing this data. The results show that our APR module performs better than the above-mentioned system and yields on the two test sets an average F1 of 71.2% and 69.4%, respectively",
    "checked": true,
    "id": "01372fd6f9c099ea351050a99e547ebe2a36f746",
    "semantic_title": "online punctuation restoration using electra model for streaming asr systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23s_interspeech.html": {
    "title": "Language Agnostic Data-Driven Inverse Text Normalization",
    "volume": "main",
    "abstract": "The rise of automatic speech recognition (ASR) models has created an urgent need for converting spoken language into written text to provide better user experiences. This has drawn the attention of researchers, particularly for real-time on-device ASR deployment, towards the inverse text normalization (ITN) problem. While data-driven ITN methods have shown great promise in recent studies, the lack of labeled spoken-written datasets is hindering the development for non-English data-driven ITN. To bridge this gap, we propose a language-agnostic data-driven ITN framework that leverages data augmentation and neural machine translation specifically designed for real-time miniature models and low-resource languages. Additionally, we have developed an evaluation method for language-agnostic ITN models when only English data is available. Our empirical evaluation attests to the efficacy of this language-agnostic ITN modeling with data augmentation approach for multiple non-English languages",
    "checked": true,
    "id": "68f9e81413ea8bfbc10e88191983a7f4dc7b1b30",
    "semantic_title": "language agnostic data-driven inverse text normalization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23j_interspeech.html": {
    "title": "How to Estimate Model Transferability of Pre-Trained Speech Models?",
    "volume": "main",
    "abstract": "In this work, we introduce a ''score-based assessment'' framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks. We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations. Our framework efficiently computes transferability scores without actual fine-tuning of candidate models or layers by making a temporal independent hypothesis. We evaluate some popular supervised speech models (and self-supervised speech models in cross-layer and cross-model settings using public data. Experimental results show a high Spearman's rank correlation and low p-value between our estimation framework and fine-tuning ground truth. Our proposed transferability framework requires less computational time and resources, making it a resource-saving and time-efficient approach for tuning speech foundation models",
    "checked": true,
    "id": "81a6b9cba431287c46fc29148ccbf6e01bf52d30",
    "semantic_title": "how to estimate model transferability of pre-trained speech models?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ihori23_interspeech.html": {
    "title": "Transcribing Speech as Spoken and Written Dual Text Using an Autoregressive Model",
    "volume": "main",
    "abstract": "This paper proposes a novel method to jointly generate spoken and written text from input speech for expanding use cases of speech-based applications. The spoken text generated using speech-to-spoken text systems, i.e., speech recognition systems, has disfluencies and no punctuation marks. Thus, spoken text is often converted into written text using a spoken text-to-written text system. However, this cascading is unsuitable for overall optimization and computationally expensive. Although a speech-to-written-text system that directly outputs the written text from the speech is also developed, it cannot output the spoken text. To efficiently produce both spoken and written text from speech, our key advance is to handle a joint text of spoken and written texts in an autoregressive model. This enables us to correctly generate both spoken and written texts by considering their dependencies via a single decoding process. Our experiments demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "1ddf41cf13aeda48ce9a79df0eb08e8255bc233f",
    "semantic_title": "transcribing speech as spoken and written dual text using an autoregressive model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuksel23_interspeech.html": {
    "title": "NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning",
    "volume": "main",
    "abstract": "This paper introduces NoRefER, a novel referenceless quality metric for automatic speech recognition (ASR) systems. Traditional reference-based metrics for evaluating ASR systems require costly ground-truth transcripts. NoRefER overcomes this limitation by fine-tuning a multilingual language model for pair-wise ranking ASR hypotheses using contrastive learning with Siamese network architecture. The self-supervised NoRefER exploits the known quality relationships between hypotheses from multiple compression levels of an ASR for learning to rank intra-sample hypotheses by quality, which is essential for model comparisons. The semi-supervised version also uses a referenced dataset to improve its inter-sample quality ranking, which is crucial for selecting potentially erroneous samples. The results indicate that NoRefER correlates highly with reference-based metrics and their intra-sample ranks, indicating a high potential for referenceless ASR evaluation or a/b testing",
    "checked": true,
    "id": "b99733670ef7c2553944208a16a68968cbb85946",
    "semantic_title": "norefer: a referenceless quality metric for automatic speech recognition via semi-supervised language model fine-tuning with contrastive learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23c_interspeech.html": {
    "title": "Scaling Laws for Discriminative Speech Recognition Rescoring Models",
    "volume": "main",
    "abstract": "Recent studies have found that model performance has a smooth power-law relationship, or scaling laws, with training data and model size, for a wide range of problems. These scaling laws allow one to choose nearly optimal data and model sizes. We study whether this scaling property is also applicable to second-pass rescoring, which is an important component of speech recognition systems. We focus on RescoreBERT as the rescoring model, which uses a pre-trained Transformer-based architecture fined tuned with an ASR discriminative loss. Using such a rescoring model, we show that the word error rate (WER) follows a scaling law for over two orders of magnitude as training data and model size increase. In addition, it is found that a pre-trained model would require less data than a randomly initialized model of the same size, representing effective data transferred from pre-training step. This effective data transferred is found to also follow a scaling law with the data and model size",
    "checked": true,
    "id": "a20c6ce80872dbc6a8e6403b2a366973061a9f89",
    "semantic_title": "scaling laws for discriminative speech recognition rescoring models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23w_interspeech.html": {
    "title": "Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition",
    "volume": "main",
    "abstract": "Energy-based language models (ELMs) parameterize an unnormalized distribution for natural sentences and are radically different from popular autoregressive language models (ALMs). As an important application, ELMs have been successfully used as a means for calculating sentence scores in speech recognition, but they all use less-modern CNN or LSTM networks. The recent progress in Transformer networks and large pretrained models such as BERT and GPT2 opens new possibility to further advancing ELMs. In this paper, we explore different architectures of energy functions and different training methods to investigate the capabilities of ELMs in rescoring for speech recognition, all using large pretrained models as backbones. Extensive experiments are conducted on two datasets, AISHELL-1 and WenetSpeech. The results show that the best ELM achieves competitive results with the finetuned GPT2 and performs significantly better than the finetuned BERT. Further analysis show that the ELM obtains better confidence estimate performance than the finetuned GPT2",
    "checked": true,
    "id": "1f99d057382445a7c83f15693e287f56d6305185",
    "semantic_title": "exploring energy-based language models with different architectures and training methods for speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23d_interspeech.html": {
    "title": "Memory Augmented Lookup Dictionary Based Language Modeling for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Recent studies have shown that using an external Language Model (LM) benefits the end-to-end Automatic Speech Recognition (ASR). However, predicting tokens that appear less frequently in the training set is still quite challenging. The long-tail prediction problems have been widely studied in many applications, but only been addressed by a few studies for ASR and LMs. In this paper, we propose a new memory augmented lookup dictionary based Transformer architecture for LM. The newly introduced lookup dictionary incorporates rich contextual information in training set, which is vital to correctly predict long-tail tokens. With intensive experiments on Chinese and English data sets, our proposed method is proved to outperform the baseline Transformer LM by a great margin on both word/character error rate and tail tokens error rate. This is achieved without impact on the decoding efficiency",
    "checked": true,
    "id": "b0c037b4037597f6e023cf300ea864bb9b7aa6dc",
    "semantic_title": "memory augmented lookup dictionary based language modeling for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/iwamoto23_interspeech.html": {
    "title": "Memory Network-Based End-To-End Neural ES-KMeans for Improved Word Segmentation",
    "volume": "main",
    "abstract": "Unsupervised word learning from unlabeled speech is a fundamental problem in zero-resource speech processing, which enables dialogue agents to learn new words directly from spoken utterances. The embedded segmental K-means (ES-KMeans) is a representative unsupervised word segmentation method. However, it has a heterogeneous structure consisting of word boundary search based on Dynamic Programming, segment embedding, and K-Means clustering, which prevents unified optimization. This paper proposes an end-to-end neural network version of the ES-KMeans model. We apply the memory network to hold a dictionary of word embeddings and realize the word boundary search and the clustering respectively as forward and backward propagations. Moreover, we replace the fixed embedding function of the original method with a learnable neural network. Experimental results using the ZeroSpeech Challenge 2020 package show the proposed approach provides superior performance to the state-of-the-art methods",
    "checked": true,
    "id": "88768f2934aa59d668a3e18a4e9c77694702230c",
    "semantic_title": "memory network-based end-to-end neural es-kmeans for improved word segmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23b_interspeech.html": {
    "title": "Retraining-free Customized ASR for Enharmonic Words Based on a Named-Entity-Aware Model and Phoneme Similarity Estimation",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (E2E-ASR) has the potential to improve performance, but a specific issue that needs to be addressed is the difficulty it has in handling enharmonic words: named entities (NEs) with the same pronunciation and part of speech that are spelled differently. This often occurs with Japanese personal names that have the same pronunciation but different Kanji characters. Since such NE words tend to be important keywords, ASR easily loses user trust if it misrecognizes them. To solve these problems, this paper proposes a novel retraining-free customized method for E2E-ASRs based on a named-entity-aware E2E-ASR model and phoneme similarity estimation. Experimental results show that the proposed method improves the target NE character error rate by 35.7% on average relative to the conventional E2E-ASR model when selecting personal names as a target NE",
    "checked": true,
    "id": "2c9beae437c30cd16bea013c10568e0a428feb88",
    "semantic_title": "retraining-free customized asr for enharmonic words based on a named-entity-aware model and phoneme similarity estimation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23c_interspeech.html": {
    "title": "Lightweight and Efficient Spoken Language Identification of Long-form Audio",
    "volume": "main",
    "abstract": "State-of-the-art Spoken Language Identification (SLI) systems usually focus on tackling short audio clips, and thus their performance degrade drastically when applied to long-form audio, such as podcast, which poses peculiar challenges to existing SLI approaches due to its long duration and diverse content that frequently involves multiple speakers as well as various languages, topics, and speech styles. In this paper, we propose the first system to tackle SLI for long-form audio using podcast data by training a lightweight, multi-class feedforward neural classifier using speaker embeddings as input. We demonstrate that our approach can make inference on long audio input efficiently; furthermore, our system can handle long audio files with multiple speakers and can be further extended into utterance-level inference and code-switching detection, which is currently not covered by any existing SLI system",
    "checked": true,
    "id": "1ff7218b81c3d54eb4c17e70def2d0675de3bd91",
    "semantic_title": "lightweight and efficient spoken language identification of long-form audio",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mishra23_interspeech.html": {
    "title": "End to End Spoken Language Diarization with Wav2vec Embeddings",
    "volume": "main",
    "abstract": "The performance of the available end-to-end (E2E) spoken language diarization (LD) systems is biased toward primary language. This is due to the unavailability of sufficient secondary language data. Because in code-switched (CS) utterances, the duration of the primary language is significant over the secondary language. Hence, to resolve the issue, this work initially uses wav2vec (W2V) pre-trained embedding in place of x-vector and can reduce the primary language bias and provides a relative improvement of 30.7% in terms of Jaccard error rate (JER) over the baseline x-vector based E2E (X-E2E) framework. Further, the performance of LD is improved by fine-tuning the W2V embedding extractor and modifying the temporal aggregation strategy from statistical pooling to attention pooling. The Final performance achieved in terms of JER is 22.5, which provides a relative improvement of 38.8% and 62.6% over the standalone W2V fine-tuned and the baseline X-E2E framework, respectively",
    "checked": true,
    "id": "fc2236674339b0c280b16d452c811164f8fd09bb",
    "semantic_title": "end to end spoken language diarization with wav2vec embeddings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nieto23_interspeech.html": {
    "title": "Efficient Spoken Language Recognition via Multilabel Classification",
    "volume": "main",
    "abstract": "Spoken language recognition (SLR) is the task of automatically identifying the language present in a speech signal. Existing SLR models are either too computationally expensive or too large to run effectively on devices with limited resources. For real-world deployment, a model should also gracefully handle unseen languages outside of the target language set, yet prior work has focused on closed-set classification where all input languages are known a-priori. In this paper we address these two limitations: we explore efficient model architectures for SLR based on convolutional networks, and propose a multilabel training strategy to handle non-target languages at inference time. Using the VoxLingua107 dataset, we show that our models obtain competitive results while being orders of magnitude smaller and faster than current state-of-the-art methods, and that our multilabel strategy is more robust to unseen non-target languages compared to multiclass classification",
    "checked": true,
    "id": "bdddcef337decd84c51d34091e52cc0e6f51204d",
    "semantic_title": "efficient spoken language recognition via multilabel classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matejka23_interspeech.html": {
    "title": "Description and Analysis of ABC Submission to NIST LRE 2022",
    "volume": "main",
    "abstract": "This paper summarizes our efforts in the NIST Language Recognition Evaluations 2022 resulting in systems providing competitive performance. We provide both the description and analysis of the systems. We describe what data we have used to train our models, and we follow with embedding extractors and backend classifiers. After covering the architecture, we concentrate on post-evaluation analysis. We compare different topologies of DNN, different backend classifiers, and the impact of the data used to train them. We also report results with XLS-R pre-trained models. We present the performance of the systems in the Fixed condition, where participants are required to use only predefined data sets, and also in the Open condition allowing to use any data to train the systems",
    "checked": true,
    "id": "a570f70209140ef4d62fb3e8c68bf7465e549710",
    "semantic_title": "description and analysis of abc submission to nist lre 2022",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alumae23_interspeech.html": {
    "title": "Exploring the Impact of Pretrained Models and Web-Scraped Data for the 2022 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "This paper describes Vocapia-TalTech team systems developed for the 2022 NIST Language Recognition Evaluation (LRE22) which focused on spoken language identication of African languages, including low-resource languages. In both fixed and open conditions, our primary systems were fused from multiple individual systems using logistic regression. In the fixed condition, we largely relied on wav2vec2.0 conformer models pretrained on the provided training data. In the open condition, we used external pretrained wav2vec2.0 models, phonotactic models and features derived from a multilingual speech recognition system, and also augmented the provided target language development data with additional data scraped from the web. On the LRE22 evaluation data, our final fixed and open condition systems obtained excellent results, with primary metric Cact values of 0.111 and 0.067, respectively. A post-evaluation study shows that both pretrained models as well as additional data are important for accurate models",
    "checked": true,
    "id": "dfb3b676ea6cda7b184911db7fc60fd9a466ed57",
    "semantic_title": "exploring the impact of pretrained models and web-scraped data for the 2022 nist language recognition evaluation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/villalba23_interspeech.html": {
    "title": "Advances in Language Recognition in Low Resource African Languages: The JHU-MIT Submission for NIST LRE22",
    "volume": "main",
    "abstract": "We present the effort of JHU-CLSP/HLTCOE and MIT Lincoln labs for NIST Language Recognition Evaluation (LRE) 2022. LRE22 consisted of a language detection task, i.e., determining whether a given target language was spoken in a speech segment. LRE22 focused on telephone and broadcast narrowband speech in African languages. Since LRE17, there has been large progress in neural embeddings, combined or not, with self-supervised models like Wav2Vec2. Therefore, one of our goals was to investigate these new models, i.e., ECAPA-TDNN, Res2Net or Wav2Vec2+ECAPA-TDNN, in the LRE scenario. In the fixed training condition, LRE22 target languages were only included in a small development set. Hence, we focused on tuning our models to exploit the limited data. For the open condition, we built a massive training set including African data, which improved Cprimary by 50% w.r.t. fixed. Wav2Vec2 embeddings were the best, outperforming ECAPA and Res2Net by 11 and 3%, respectively",
    "checked": true,
    "id": "51aca07d500c44ebde896b8df3b0388dd3ade489",
    "semantic_title": "advances in language recognition in low resource african languages: the jhu-mit submission for nist lre22",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23d_interspeech.html": {
    "title": "DeePMOS: Deep Posterior Mean-Opinion-Score of Speech",
    "volume": "main",
    "abstract": "We propose a deep neural network (DNN) based method that provides a posterior distribution of mean-opinion-score (MOS) for an input speech signal. The DNN outputs parameters of the posterior, mainly the posterior's mean and variance. The proposed method is referred to as deep posterior MOS (DeePMOS). The relevant training data is inherently limited in size (limited number of labeled samples) and noisy due to the subjective nature of human listeners. For robust training of DeePMOS, we use a combination of maximum-likelihood learning, stochastic gradient noise, and a student-teacher learning setup. Using the mean of the posterior as a point estimate, we evaluate standard performance measures of the proposed DeePMOS. The results show comparable performance with existing DNN-based methods that only provide point estimates of the MOS. Then we provide an ablation study showing the importance of various components in DeePMOS",
    "checked": true,
    "id": "8a69d5a2b10234ac6cf64e2a3dafe3b6a7a22e84",
    "semantic_title": "deepmos: deep posterior mean-opinion-score of speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dasare23_interspeech.html": {
    "title": "The Role of Formant and Excitation Source Features in Perceived Naturalness of Low Resource Tribal Language TTS: An Empirical Study",
    "volume": "main",
    "abstract": "Text-to-speech synthesis is a prominent area in the speechprocessing domain that has significant use in reading digital content in a given language. In the proposed work, we worked on two tribal languages of India viz., Lambani and Soliga, which are zero-resource languages. The study began with a dataset collection for both tribal languages. Secondly, a Text-To-Speech (TTS) system was built separately based on the transfer learning approach. To validate the voice quality of TTS-generated speech, subjective as well as objective evaluations were performed. As a part of objective analysis, the voice source and vocal tract filter properties of the synthetic speech have been explored. The extensive study on various aspects of speech, such as LP residual, F0 contour, and formants (F1 & sF2) has shown interesting results that can correlate to the subjective listening test results. The link to the original and synthetic speech can be found online",
    "checked": true,
    "id": "7828e3abbedaecbd890ccf7475783cfa3f397c71",
    "semantic_title": "the role of formant and excitation source features in perceived naturalness of low resource tribal language tts: an empirical study",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23_interspeech.html": {
    "title": "A no-reference speech quality assessment method based on neural network with densely connected convolutional architecture",
    "volume": "main",
    "abstract": "Most speech quality assessment methods require a perfect reference signal to evaluate the damaged speech's quality. However, it is challenging to obtain clean reference signals due to various types and levels of noise in reality. Meanwhile, no-reference speech quality assessment is less accurate than full-reference method. To address these issues, we propose a novel no-reference speech quality assessment model that improves evaluation accuracy with lower complexity. The model is primarily composed of three densely connected convolutional (DCC) modules and a bidirectional long short-term memory (BLSTM) module. Experiment results demonstrate that our method outperforms the baselines, achieving state-of-the-art on the no-reference speech quality assessment task. When using PESQ as optimization targets, the MSE, PLCC and SRCC reach 0.0389, 0.9695 and 0.9715, whereas when using STOI, these metrics reach 0.0019, 0.9608, and 0.9630, respectively",
    "checked": true,
    "id": "8e9a4eb13c54dbadee82df8389a683ef27f9c994",
    "semantic_title": "a no-reference speech quality assessment method based on neural network with densely connected convolutional architecture",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ta23_interspeech.html": {
    "title": "Probing Speech Quality Information in ASR Systems",
    "volume": "main",
    "abstract": "This paper investigates how intermediate speech representations in a state-of-the-art automatic speech recognition (ASR) system encode multi-dimensional speech quality, including MOS, Noisiness, Coloration, Discontinuity, and Loudness. We found that speech quality information is encoded in the ASR encoder layers at various levels but is still much richer than the Mel-spectrogram, an input widely used in previous works. This discovery inspires us to develop the Attentive Conformer with ASR pretraining, a novel deep learning model that enables the utilization of rich information from pretrained ASR models and the ability to focus on specific layers. Experiments on the NISQA speech quality assessment dataset demonstrate that the proposed model achieves state-of-the-art performance with significantly less training data",
    "checked": true,
    "id": "ffa5ed4bd8625dc803fe3de4e0c0f72752ff6df2",
    "semantic_title": "probing speech quality information in asr systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23d_interspeech.html": {
    "title": "Preference-based training framework for automatic speech quality assessment using deep neural network",
    "volume": "main",
    "abstract": "One objective of Speech Quality Assessment (SQA) is to estimate the ranks of synthetic speech systems. However, recent SQA models are typically trained using low-precision direct scores such as mean opinion scores (MOS) as the training objective, which is not straightforward to estimate ranking. Although it is effective for predicting quality scores of individual sentences, this approach does not account for speech and system preferences when ranking multiple systems. We propose a training framework of SQA models that can be trained with only preference scores derived from pairs of MOS to improve ranking prediction. Our experiment reveals conditions where our framework works the best in terms of pair generation, aggregation functions to derive system score from utterance preferences, and threshold functions to determine preference from a pair of MOS. Our results demonstrate that our proposed method significantly outperforms the baseline model in Spearman's Rank Correlation Coefficient",
    "checked": true,
    "id": "3c54bd226841a3994ab5239ffdc4b9031081d961",
    "semantic_title": "preference-based training framework for automatic speech quality assessment using deep neural network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/phatthiyaphaibun23_interspeech.html": {
    "title": "Crowdsourced Data Validation for ASR Training",
    "volume": "main",
    "abstract": "Many ASR engines are based on crowdsourced speech corpora, such as Common Voice. Although crowdsourced data is inexpensive, the utterances obtained from crowdsourcing can be noisy because of uncontrollable factors such as accents, environments, etc. Another issue with the Common Voice corpus is the lack of validators to cover a vast collection of crowdsourced utterances. This issue presents a significant challenge to speech data validation. To mitigate this bottleneck, we propose a machine-learning classifier that predicts the correctness of the data, which can act as either the validator itself or a prescreen for the validator. Our system achieves more than 95% F1-score in the three Common Voice languages, including Thai, Japanese, and Turkish, and performs even better when we have only one human judge involved in the decision. Furthermore, we also found that the data obtained from our method outperformed the current crowdsourcing validation method when used to train the ASR model",
    "checked": true,
    "id": "7dae6964be1b4dd1e254c7f2c2af88945b26c643",
    "semantic_title": "crowdsourced data validation for asr training",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huo23b_interspeech.html": {
    "title": "Re-investigating the Efficient Transfer Learning of Speech Foundation Model using Feature Fusion Methods",
    "volume": "main",
    "abstract": "Speech foundation models, pre-trained on large amounts of unsupervised or supervised audio data, have demonstrated an impressive ability to transfer their learning to specific domains for speech recognition. Parameter-efficient fine-tuning methods offer an efficient paradigm where a small set of parameters are updated to adapt the foundation model to new tasks. However, it is unclear how the intermediate features of the foundation model behave, and how to utilize them in a more efficient way. In this paper, we compare the performance of three speech foundation models for speech recognition. We re-investigate how features from different layers behave and propose a simple and effective feature fusion method for efficient transfer learning. Experimental results demonstrate that the proposed method uses 31.7% fewer trainable encoder parameters, 13.4% less computational memory cost than compared method, and does not compromise quality on the target task",
    "checked": true,
    "id": "d3ccf04f65bfa037227ebf1637e0b1bc9654ff59",
    "semantic_title": "re-investigating the efficient transfer learning of speech foundation model using feature fusion methods",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qi23_interspeech.html": {
    "title": "Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training",
    "volume": "main",
    "abstract": "Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (WAPAT). WAPAT use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, WAPAT utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of WAPAT on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-WAPAT outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art",
    "checked": true,
    "id": "1628c06f7b63c10c0aa78317aa9ca6c8da68198f",
    "semantic_title": "robust automatic speech recognition via wavaugment guided phoneme adversarial training",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23b_interspeech.html": {
    "title": "InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the baseline models",
    "checked": true,
    "id": "a86370b202b4856f32ba9db9ed10cb2ba4aca8e6",
    "semantic_title": "interformer: interactive local and global features fusion for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23_interspeech.html": {
    "title": "Transductive Feature Space Regularization for Few-shot Bioacoustic Event Detection",
    "volume": "main",
    "abstract": "In few-shot bioacoustic event detection, besides interested target events, background noises and various uninterested sound events lead to complex decision boundaries, which require regularized feature distributions in feature space. Due to the low label availability of uncertain noise events, existing few-shot learning methods with entropy-based regularizers suffer from overfitting during optimization. In this paper, we propose a transductive inference model with a prior knowledge based regularizer (PKR) to overcome the overfitting problem. We use a task-adaptive feature extractor to reconstruct a regularized feature space. A PKR is proposed to minimize the divergence between the original and reconstructed feature space. The development set of DCASE 2022 Task 5 is adopted as the experimental dataset. With the increasing iterations, the proposed model performs with long-lasting results around 55.43 F-measure, and well solves the overfitting problem in transductive inference",
    "checked": true,
    "id": "2a4b8260e63debd06db059f02b14fa36b603e7e7",
    "semantic_title": "transductive feature space regularization for few-shot bioacoustic event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23e_interspeech.html": {
    "title": "Incorporating L2 Phonemes Using Articulatory Features for Robust Speech Recognition",
    "volume": "main",
    "abstract": "The limited availability of non-native speech datasets presents a major challenge in automatic speech recognition (ASR) to narrow the performance gap between native and non-native speakers. To address this, the focus of this study is on the efficient incorporation of the L2 phonemes, which in this work refer to Korean phonemes, through articulatory feature analysis. This not only enables accurate modeling of pronunciation variants but also allows for the utilization of both native Korean and English speech datasets. We employ the lattice-free maximum mutual information (LF-MMI) objective in an end-to-end manner, to train the acoustic model to align and predict one of multiple pronunciation candidates. Experimental results show that the proposed method improves ASR accuracy for Korean L2 speech by training solely on L1 speech data. Furthermore, fine-tuning on L2 speech improves recognition accuracy for both L1 and L2 speech without performance trade-offs",
    "checked": true,
    "id": "64abb0112d24dc62105e302cf98af445651b86ab",
    "semantic_title": "incorporating l2 phonemes using articulatory features for robust speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/parcollet23_interspeech.html": {
    "title": "On the (In)Efficiency of Acoustic Feature Extractors for Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Speech representations learned with self-supervised learning (SSL) have the potential to significantly improve the performance of a number of audio applications, especially when availability of labeled data from the deployment domain is limited. Despite their successes, SSL training methods are compute- and memory-heavy, and require large investments in computing infrastructure, thus putting it out of the reach of most institutions. Therefore, building efficient model architectures is essential for the wide-scale adoption of SSL in speech technologies. CNN-based Acoustic Feature Extractors (AFE), which are widely used as encoders of acoustic waveforms, remain one of the main efficiency bottlenecks. This work proposes replacing CNN-based AFEs with more efficient ones and demonstrates that SSL pre-training time and memory consumption can be reduced by a factor of two to three over existing methods while preserving performances in speech-, command-, and speaker-recognition tasks",
    "checked": true,
    "id": "67377c8759a5a21ed27a8f6eee585dde1c3bd6a7",
    "semantic_title": "on the (in)efficiency of acoustic feature extractors for self-supervised speech representation learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tenbosch23_interspeech.html": {
    "title": "Phonemic competition in end-to-end ASR models",
    "volume": "main",
    "abstract": "Advanced end-to-end ASR systems encode speech signals by means of a multi-layer network architecture. In Wav2vec2.0, for example, a CNN is used as feature encoder on top of which transformer layers are used to map the high-dimensional CNN representations to the elements of some lexicon. Compared to the previous generation of 'modular' ASR systems it is much more difficult to interpret the processing and representations in an end-to-end system from a phonetic point of view. We built a Wav2vec2.0-based end-to-end system for producing broad phonetic transcriptions of Dutch. In this paper we investigate to what extent the CNN features and the representations on several transformer layers of a pre-trained and fine-tuned model reflect widely-shared phonetic knowledge. For that purpose we analyze distances between phones and the phonetic features of the most-activated phones in the output of an MLP classifier operating on the representations in several layers",
    "checked": true,
    "id": "294e96df613afb8a84c37ffabd183ba049cedd99",
    "semantic_title": "phonemic competition in end-to-end asr models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hughes23_interspeech.html": {
    "title": "Automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics",
    "volume": "main",
    "abstract": "Automatic Speaker Recognition (ASR) involves a complex range of processes to extract, model, and compare speaker-specific information from a pair of voice samples. Using heavily controlled recordings, this paper explores the impact of specific vocal conditions (i.e. vocal setting, disguise, accent guises) on ASR performance. When vocal conditions are matched, ASR performance is generally excellent (whisper is an exception). When conditions are mismatched, as in most forensic cases, we see an increase in discrimination and calibration error in some cases. The most problematic mismatches are those involving whisper and supralaryngeal vocal settings; these produce the greatest phonetic changes to speech. Mismatches involving high pitch also produce poor performance, although this appears to be driven by speaker-specific differences in articulatory implementation. We discuss the implications of the findings for the use of ASR in forensic casework and the interpretability of system output",
    "checked": true,
    "id": "199229d3865f23548de42544004502ffee6f843c",
    "semantic_title": "automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geiger23_interspeech.html": {
    "title": "Exploring Graph Theory Methods For the Analysis of Pronunciation Variation in Spontaneous Speech",
    "volume": "main",
    "abstract": "Given the development of automatic speech recognition based techniques for creating phonetic annotations of large speech corpora, there has been a growing interest in investigating the frequencies of occurrence of phonological and reduction processes. Given that most studies have analyzed these processes separately, they did not provide insights about their co-occurrences. This paper contributes with introducing graph theory methods for the analysis of pronunciation variation in a large corpus of Austrian German conversational speech. More specifically, we investigate how reduction processes that are typical for spontaneous German in general co-occur with phonological processes typical for the Austrian German variety. Whereas our concrete findings are of special interest to scientists investigating variation in German, the approach presented opens new possibilities to analyze pronunciation variation in large corpora of different speaking styles in any language",
    "checked": true,
    "id": "338f9ef7acb8d1b25b8eda135ec9e8fb710f8085",
    "semantic_title": "exploring graph theory methods for the analysis of pronunciation variation in spontaneous speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nuttall23_interspeech.html": {
    "title": "Automatic Speaker Recognition performance with matched and mismatched female bilingual speech data",
    "volume": "main",
    "abstract": "Validation of forensic voice comparison methods requires testing using speech samples that are representative of forensic casework conditions. Increasingly, around the world, forensic voice comparison casework is being undertaken using automatic speaker recognition (ASR) systems. However, multilingualism remains a key issue in applying automatic systems to forensic casework. This research aims to consider the effect of language on ASR performance, testing developers' claims of 'language independency'. Specifically, we examine the extent to which language mismatch either between the known and questioned samples, or between the evidential samples and the calibration data, affects overall system performance and the resulting strength of evidence (i.e., likelihood ratios for individual comparisons). Results indicate that mixed language trials produce more errors than single language trials which makes drawing evidential conclusions based on bilingual data challenging",
    "checked": true,
    "id": "1e58d3ff954b2690435fcb91d712ced6875d06a4",
    "semantic_title": "automatic speaker recognition performance with matched and mismatched female bilingual speech data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23x_interspeech.html": {
    "title": "FACTSpeech: Speaking a Foreign Language Pronunciation Using Only Your Native Characters",
    "volume": "main",
    "abstract": "Recent text-to-speech models have been requested to synthesize natural speech from language-mixed sentences because they are commonly used in real-world applications. However, most models do not consider transliterated words as input. When generating speech from transliterated text, it is not always natural to pronounce transliterated words as they are written, such as in the case of song titles. To address this issue, we introduce FACTSpeech, a system that can synthesize natural speech from transliterated text while allowing users to control the pronunciation between native and literal languages. Specifically, we propose a new language shift embedding to control the pronunciation of input text between native or literal pronunciation. Moreover, we leverage conditional instance normalization to improve pronunciation while preserving the speaker identity. The experimental results show that FACTSpeech generates native speech even from the sentences of transliterated form",
    "checked": true,
    "id": "8b0feedfec68ce2e39a5d9604b3b16fdfd3fcf07",
    "semantic_title": "factspeech: speaking a foreign language pronunciation using only your native characters",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23e_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model",
    "volume": "main",
    "abstract": "Phrase break prediction is a crucial task for improving the prosody naturalness of a text-to-speech (TTS) system. However, most proposed phrase break prediction models are monolingual, trained exclusively on a large amount of labeled data. In this paper, we address this issue for low-resource languages with limited labeled data using cross-lingual transfer. We investigate the effectiveness of zero-shot and few-shot cross-lingual transfer for phrase break prediction using a pre-trained multilingual language model. We use manually collected datasets in four Indo-European languages: one high-resource language and three with limited resources. Our findings demonstrate that cross-lingual transfer learning can be a particularly effective approach, especially in the few-shot setting, for improving performance in low-resource languages. This suggests that cross-lingual transfer can be inexpensive and effective for developing TTS front-end in resource-poor languages",
    "checked": true,
    "id": "07ecfeb52ff0768c3067cac5309d8150701d5906",
    "semantic_title": "cross-lingual transfer learning for phrase break prediction with multilingual language model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23d_interspeech.html": {
    "title": "DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "Although high-fidelity speech can be obtained for intralingual speech synthesis, cross-lingual text-to-speech (CTTS) is still far from satisfactory as it is difficult to accurately retain the speaker timbres (i.e. speaker similarity) and eliminate the accents from their first language (i.e. nativeness). In this paper, we demonstrated that vector-quantized (VQ) acoustic feature contains less speaker information than mel-spectrogram. Based on this finding, we propose a novel dual speaker embedding TTS (DSE-TTS) framework for CTTS with authentic speaking style. Here, one embedding is fed to the acoustic model to learn the linguistic speaking style, while the other one is integrated into the vocoder to mimic the target speaker's timbre. Experiments show that by combining both embeddings, DSE-TTS significantly outperforms the state-of-the-art SANE-TTS in cross-lingual synthesis, especially in terms of nativeness",
    "checked": true,
    "id": "45a04efc208faa3eb97c60b96dd07bbbb853f69c",
    "semantic_title": "dse-tts: dual speaker embedding for cross-lingual text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/markopoulos23_interspeech.html": {
    "title": "Generating Multilingual Gender-Ambiguous Text-to-Speech Voices",
    "volume": "main",
    "abstract": "The gender of any voice user interface is a key element of its perceived identity. Recently there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices",
    "checked": true,
    "id": "be0afc3192c002797d767930ce271fccab713471",
    "semantic_title": "generating multilingual gender-ambiguous text-to-speech voices",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/badlani23_interspeech.html": {
    "title": "RAD-MMM: Multilingual Multiaccented Multispeaker Text To Speech",
    "volume": "main",
    "abstract": "We create a multilingual speech synthesis system that can generate speech with a native accent in any seen language while retaining the characteristics of an individual's voice. It is expensive to obtain bilingual training data for a speaker and the lack of such data results in strong correlations that entangle speaker, language, and accent, resulting in poor transfer capabilities. To overcome this, we present RADMMM, a speech synthesis model based on RADTTS with explicit control over accent, language, speaker, and fine-grained F0 and energy features. Our proposed model does not rely on bilingual training data. We demonstrate an ability to control synthesized accent for any speaker in an open-source dataset comprising of 7 languages, with one native speaker per language. Human subjective evaluation demonstrates that, when compared to controlled baselines, our model better retains a speaker's voice and target accent, while synthesizing fluent speech in all target languages and accents in our dataset",
    "checked": true,
    "id": "3beba85b45c7ee6e567a8e44fbaa31a6867cd17b",
    "semantic_title": "rad-mmm: multilingual multiaccented multispeaker text to speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/comini23_interspeech.html": {
    "title": "Multilingual context-based pronunciation learning for Text-to-Speech",
    "volume": "main",
    "abstract": "Phonetic information and linguistic knowledge are an essential component of a Text-to-speech (TTS) front-end. Given a language, a lexicon can be collected offline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in order to predict the pronunciation for out-of-vocabulary (OOV) words. Additionally, post-lexical phonology, often defined in the form of rule-based systems, is used to correct pronunciation within or between words. In this work we showcase a multilingual unified front-end system that addresses any pronunciation related task, typically handled by separate modules. We evaluate the proposed model on G2P conversion and other language-specific challenges, such as homograph and polyphones disambiguation, post-lexical rules and implicit diacritization. We find that the multilingual model is competitive across languages and tasks, however, some trade-offs exists when compared to equivalent monolingual solutions",
    "checked": true,
    "id": "0d2776b80f93b96be8f4542ffdeb3a6a31f9b770",
    "semantic_title": "multilingual context-based pronunciation learning for text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23c_interspeech.html": {
    "title": "Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition",
    "volume": "main",
    "abstract": "There are individual differences in expressive behaviors driven by cultural norms and personality. This between-person variation can result in reduced emotion recognition performance. Therefore, personalization is an important step in improving the generalization and robustness of speech emotion recognition. In this paper, to achieve unsupervised personalized emotion recognition, we first pre-train an encoder with learnable speaker embeddings in a self-supervised manner to learn robust speech representations conditioned on speakers. Second, we propose an unsupervised method to compensate for the label distribution shifts by finding similar speakers and leveraging their label distributions from the training set. Extensive experimental results on the MSP-Podcast corpus indicate that our method consistently outperforms strong personalization baselines and achieves state-of-the-art performance for valence estimation",
    "checked": true,
    "id": "a8e465a05d423f6201b4cd5631f04d00261e41b8",
    "semantic_title": "personalized adaptation with pre-trained speech encoders for continuous emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chou23_interspeech.html": {
    "title": "The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers",
    "volume": "main",
    "abstract": "The uncertainty in modeling emotions makes speech emotion recognition (SER) systems less reliable. An intuitive way to increase trust in SER is to reject predictions with low confidence. This approach assumes that an SER system is well calibrated, where highly confident predictions are often right and low confident predictions are often wrong. Hence, it is desirable to calibrate the confidence of SER classifiers. We evaluate the reliability of SER systems by exploring the relationship between confidence and accuracy, using the expected calibration error (ECE) metric. We develop a multi-label variant of the post-hoc temperature scaling (TS) method to calibrate SER systems, while preserving their accuracy. The best method combines an emotion co-occurrence weight penalty function, a class-balanced objective function, and the proposed multi-label TS calibration method. The experiments show the effectiveness of our developed multi-label calibration method in terms of accuracy and ECE",
    "checked": true,
    "id": "dfb438c044685e25a970c622d5c282c79b4831c1",
    "semantic_title": "the importance of calibration: rethinking confidence and performance of speech multi-label emotion classifiers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/malik23_interspeech.html": {
    "title": "A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model",
    "volume": "main",
    "abstract": "In this paper, we propose to utilise diffusion models for data augmentation in speech emotion recognition (SER). In particular, we present an effective approach to utilise improved denoising diffusion probabilistic models (IDDPM) to generate synthetic emotional data. We condition the IDDPM with the textual embedding from bidirectional encoder representations from transformers (BERT) to generate high-quality synthetic emotional samples in different speakers' voiceswe uploaded the synthetic samples for reviewers to listen.. We implement a series of experiments and show that better quality synthetic data helps improve SER performance. We compare results with generative adversarial networks (GANs) and show that the proposed model generates better-quality synthetic samples that can considerably improve the performance of SER when augmented with synthetic data",
    "checked": true,
    "id": "21da715d3a9bee5e91206347560a898587303ce0",
    "semantic_title": "a preliminary study on augmenting speech emotion recognition using a diffusion model",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alsenani23_interspeech.html": {
    "title": "Privacy Risks in Speech Emotion Recognition: A Systematic Study on Gender Inference Attack",
    "volume": "main",
    "abstract": "Increasingly more applications now use deep networks to analyse speaker's affective states. An undesirable side effect is that models trained to perform one task (e.g, emotion from speech) can be attacked to infer other, possibly privacy-sensitive attributes (e.g., gender) of the speaker. The amount of information an attacker can infer through such attacks is called leakage, and this article presents the first systematic study of the interplay between gender leakage and the main characteristics of the attacker model (family, architecture and training condition). To this end, we define various attack scenarios, and perform extensive experiments to analyse privacy risks in Speech Emotion Recognition (SER). Results show that SER models can leak a speaker's gender with an accuracy of 51% to 95% (upper bound) depending on the attack condition. Furthermore, our results provide fresh insights on how to limit the effectiveness of possible attacks and, thereby, to ensure privacy preservation",
    "checked": true,
    "id": "9982df071491764bee55ec7ea6c2f896934785f7",
    "semantic_title": "privacy risks in speech emotion recognition: a systematic study on gender inference attack",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tavernor23_interspeech.html": {
    "title": "Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion conveys abundant information that can improve the user experience of various automated systems, in addition to communicating information important for managing well-being. Human speech conveys emotion, but speech emotion recognition models do not perform well in unseen environments. This limits the ubiquitous use of speech emotion recognition models. In this paper, we investigate how a model can be adapted to unseen environments without forgetting previously learned environments. We show that memory-based methods maintain performance on previously seen environments while still being able to adapt to new environments. These methods enable continual training of speech emotion recognition models following deployment while retaining previous knowledge, working towards a more general, adaptable, acoustic model",
    "checked": true,
    "id": "f734a8f825d3c0b2982f343d95d622a4e3eecaa1",
    "semantic_title": "episodic memory for domain-adaptable, robust speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ding23_interspeech.html": {
    "title": "Stable Speech Emotion Recognition with Head-k-Pooling Loss",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) aims to detect the emotion of the speaker involved in a given utterance. Most existing SER methods focus on local speech features by stacking convolutions and training all segments of an utterance with an utterance-level label. Two deficiencies exist in these methods: i) learning only local speech features may be insufficient for SER due to the ambiguity of emotions; ii) consistent supervision of each segment may lead to label error propagation, as the true emotions of some segments may not match the utterance label. To solve the two issues, we first devise a global-local fusion network to model both long- and short-range relations in speech. Second, we tailor a novel head-k-pooling loss for SER tasks, which dynamically assigns labels for each segment and selectively performs loss calculation across segments. We test our method on the IEMOCAP and the newly collected ST-EMO dataset, and the results show its superiority and stability",
    "checked": true,
    "id": "b8f958dcae03dcb7a3f27277a1a10ab784a49e0f",
    "semantic_title": "stable speech emotion recognition with head-k-pooling loss",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gibson23b_interspeech.html": {
    "title": "A Personalised Speech Communication Application for Dysarthric Speakers",
    "volume": "main",
    "abstract": "Individuals with impaired speech are often understood only by those familiar with their speech e.g. a care-giver or close family member. These impaired speakers are therefore highly dependent upon those familiar listeners for their spoken communication needs. These needs vary from basic expressions of hunger or thirst to much more advanced requirements like being understood at a work meeting. A significant subset of individuals with impaired speech also have reduced motor function which limits their mobility or dexterity. For this subset of individuals, the ability to communicate via the medium of speech is crucial. This paper describes a personalised speech communication application targeted towards English language speakers with impaired speech. This application enables the user to hold conversations with other humans, dictate text to a machine and participate in meetings via closed captioning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23l_interspeech.html": {
    "title": "Video Multimodal Emotion Recognition System for Real World Applications",
    "volume": "main",
    "abstract": "This paper proposes a system capable of recognizing a speaker's utterance-level emotion through multimodal cues in a video. The system seamlessly integrates multiple AI models to first extract and pre-process multimodal information from the raw video input. Next, an end-to-end MER model sequentially predicts the speaker's emotions at the utterance level. Additionally, users can interactively demonstrate the system through the implemented interface",
    "checked": true,
    "id": "74d9bea6a26e928b042f009bc277bb3fe04190bb",
    "semantic_title": "video multimodal emotion recognition system for real world applications",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rohmatillah23_interspeech.html": {
    "title": "Promoting Mental Self-Disclosure in a Spoken Dialogue System",
    "volume": "main",
    "abstract": "This paper proposes a mental health spoken dialogue to relax mental illness for university students by acting as an active listener to promote self-disclosure. The proposed system is designed for Mandarin with the specific accent and lexicon in Taiwan which is known as one of the underrepresented spoken languages. To achieve the objective, this work considers three key factors which are high quality speech components including automatic speech recognition and text-to-speech models, and the personalized responses while keeping the trustworthiness and seamless integration among dialogue system components",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bujnowski23_interspeech.html": {
    "title": "Select language, modality or put on a mask!\" Experiments with Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "We propose a system designed for multimodal emotion recognition. Our research focuses on showing the impact of various signals in the emotion recognition process. Apart from reporting the average results of our models, we would like to encourage individual engagement of conference participants and explore how a unique emotional scene recorded on the spot can be interpreted by the models - for individual modalities as well as their combinations. Our models work for English, German and Korean. We show the comparison of emotion recognition accuracy for these 3 languages, including the influence of each modality. Our second experiment explores emotion recognition for people wearing face masks. We show that the use of face masks affects not only the video signal but also audio and text. To our knowledge, no other study shows the effects of wearing a mask for three modalities. Unlike other studies where masks are added artificially, we use real recordings with actors in masks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/valentine23_interspeech.html": {
    "title": "My Vowels Matter: Formant Automation Tools for Diverse Child Speech",
    "volume": "main",
    "abstract": "Tools to automate formant measurement in vowels have been developed recently, but they have not been tested on pediatric speech samples. Critically, child speech includes unique acoustic challenges including high fundamental frequencies, wide formant bandwidths, more variable formant values, and increased subglottal coupling relative to adult speech. More importantly, these tools have not been tested on the diverse linguistic variations spoken by children. This study compares three tools for automatic formant estimation: Voweltine, Fast Track, and SpeechMark. The tools are tested on vowel productions from a young child with a speech sound disorder from a Black-identifying family. Benefits and tradeoffs of each automation tool are discussed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chongwhite23_interspeech.html": {
    "title": "NEMA: An Ecologically Valid Tool for Assessing Hearing Devices, Advanced Algorithms, and Communication in Diverse Listening Environments",
    "volume": "main",
    "abstract": "Ecological Momentary Assessment (EMA) is valuable research method for evaluating the real-world performance of novel computational algorithms and device technologies, addressing the shortcomings of objective metrics and laboratory assessments. Our customisable, cloud-connected smartphone app, NEMA, gathers repeated self-reports and related acoustic features in users' natural environments, providing personalised insights on how specific technologies impact daily activities. NEMA has proven effective in assessing the real-world performance of novel hearing aid algorithms and features, while also improving our understanding of the challenges faced by those with hearing loss which drive new developments. This paper outlines NEMA's innovative features designed to facilitate efficient data collection and presents findings from a recent clinical trial where NEMA played a key role in providing real-world evidence of user benefits for a medical device seeking FDA approval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ramanarayanan23_interspeech.html": {
    "title": "When Words Speak Just as Loudly as Actions: Virtual Agent Based Remote Health Assessment Integrating What Patients Say with What They Do",
    "volume": "main",
    "abstract": "We present a unified multimodal dialog platform for the remote assessment and monitoring of patients' neurological and mental health. Tina, a virtual agent, guides participants through an immersive interaction wherein objective speech, facial, linguistic and cognitive biomarkers can be automatically computed from participant speech and video in near real time. Furthermore, Tina encourages participants to describe, in their own words, their most bothersome problems and what makes them better or worse, through the Patient Report of Problems (PROP) instrument. The PROP captures unfiltered verbatim replies of patients, in contrast with traditional patient reported outcomes that typically rely on categorical assessments. We argue that combining these patient reports (i.e., what they say) with objective biomarkers (i.e., how they say it and what they do) can greatly enhance the quality of telemedicine and improve the efficacy of siteless trials and digital therapeutic interventions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/motepalli23_interspeech.html": {
    "title": "Stuttering Detection Application",
    "volume": "main",
    "abstract": "Stuttering is a prevalent speech disorder that affects millions of people worldwide. In this Show and Tell presentation, we demonstrate a novel platform that takes speech samples in English and Kannada to detect and analyze stuttering in patients. The user-friendly interface includes demographic details and speech samples, generating comprehensive reports for different stuttering disfluencies. The platform has four different user types, providing full read-only access for admins and full write access for super admins. Our platform provides valuable assistance for speech-language pathologists to evaluate speech samples. The proposed platform supports both live and recorded speech samples and presents a flexible approach to stuttering detection and analysis. Our research demonstrates the potential of technology to improve speech-language pathology for stuttering. Used F-score as a metric for evaluating the models for the stutter detection task",
    "checked": false,
    "id": "7da56460260e74076d524527eb89a981747864e7",
    "semantic_title": "drowsiness detection application",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zusag23b_interspeech.html": {
    "title": "Providing Interpretable Insights for Neurological Speech and Cognitive Disorders from Interactive Serious Games",
    "volume": "main",
    "abstract": "We propose an automated pipeline for robustly identifying neurological disorders from interactive therapeutic exercises, which are gathered via the mobile therapy app myReha. The app captures speech and cognitive parameters from over 30.000 tasks in various scenarios. Users get immediate and highly accurate feedback for pronunciation and coherency for language tasks, while voice recordings are fed to a feature extraction pipeline in the backend. These features are then used to construct speech characteristics, which are highly indicative of different neurological disorders, such as acquired aphasia after stroke. The data is visually presented in a web application nyra.insights, which allows medical professionals to quickly derive recommendations for treatment and closely monitor outcomes. During the Show and Tell session, users can experiment with the interactive myReha app and experience the real-time speech analysis capabilities via the nyra.insights web platform",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/solinsky23_interspeech.html": {
    "title": "Automated Neural Nursing Assistant (ANNA): An Over-The-Phone System for Cognitive Monitoring",
    "volume": "main",
    "abstract": "ANNA is a telephony-based cognitive assessment tool designed to aid nurses in caring for patients who require close monitoring for the development of confusion or neurological impairment. Of particular concern is the treatment of Immune Effector Cell-Associated Neurotoxicity Syndrome (ICANS), a condition which occurs quite frequently as an adverse outcome of Chimeric Antigen Receptor-T (CAR-T) cancer immunotherapy. ANNA employs both traditional verbal tests for cognitive impairment and novel linguistic methods which identify abnormalities in the patient's speech during ordinary conversation. To collect ordinary speech it uses a lightweight instance of the Facebook's Large Language Model BlenderBot to engage the patient in a partially unscripted conversation. ANNA is designed with easy employment by healthcare providers in mind, being sufficiently lightweight to run on consumer-grade hardware and needing access only to a patient's phone number to interact with them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gupta23b_interspeech.html": {
    "title": "5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids",
    "volume": "main",
    "abstract": "Over twenty percent of the world's population suffers from some form of hearing loss, making it one of the most significant public health challenges. Current hearing aids commonly amplify noises while failing to improve speech comprehension in crowded social settings. In this demonstration, we showcase a proof-of-concept implementation of the world's first 5G and Internet of Things (IoT) enabled multi-modal hearing aid (MM HA) prototype. This integrates an innovative 5G cloud-radio access network (C-RAN) and IoT based transceiver model for real-time audio-visual speech enhancement (AVSE). Specifically, we demonstrate a transceiver model for Cloud-based AVSE which satisfies high data rate and low latency requirements for future MM HAs. The innovative 5G-IoT transceiver application is shown to satisfy HA latency limitations while transmitting raw noisy AV data from an MM HA prototype device to the cloud for deep learning-based real-time AVSE processing and obtaining a clean audio signal",
    "checked": false,
    "id": "672c2b5508b040435a90193b8a2e127b42bf6bea",
    "semantic_title": "live demonstration: cloud-based audio-visual speech enhancement in multimodal hearing-aids",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raza23_interspeech.html": {
    "title": "Towards Two-point Neuron-inspired Energy-efficient Multimodal Open Master Hearing Aid",
    "volume": "main",
    "abstract": "Here we demonstrate a two-point neuron-inspired audio-visual (AV) open Master Hearing Aid (OpenMHA) framework for on-chip energy-efficient speech enhancement (SE). The developed system is compared against state-of-the-art cepstrum-based audio-only (A-only) SE and conventional point-neuron inspired deep neural net (DNN) driven multimodal (MM) SE. Pilot experiments demonstrate that the proposed system outperforms audio-only SE in terms of speech quality and intelligibility and performs comparably to point neuron-inspired DNN with significantly reduced energy consumption at any time --- both during training and inferencing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23b_interspeech.html": {
    "title": "FC-MTLF: A Fine- and Coarse-grained Multi-Task Learning Framework for Cross-Lingual Spoken Language Understanding",
    "volume": "main",
    "abstract": "Currently, zero-shot cross-lingual spoken language understanding (SLU) attracts increasing attention. Most of existing methods construct a mixed-language context via the code-switching approach. However, due to the different syntactic structures of each language, code-switching might fail to perform well and result in the loss of semantics. To address this issue, we propose a novel framework termed FC-MTLF, which applies a multi-task learning by introducing an auxiliary multilingual neural machine translation (NMT) task to compensate for the shortcomings of code-switching. In addition, we also adopt the curriculum learning strategy to further improve the performance. Experimental results show that our framework achieves the new state-of-the-art performance on the MultiATIS++ dataset. Further analysis verifies that our FC-MTLF can effectively transfer knowledge from source languages to target languages",
    "checked": true,
    "id": "118a703723c066895be4477f8d7bfb62e370ac2f",
    "semantic_title": "fc-mtlf: a fine- and coarse-grained multi-task learning framework for cross-lingual spoken language understanding",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23c_interspeech.html": {
    "title": "C²A-SLU: Cross and Contrastive Attention for Improving ASR Robustness in Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) is a critical task in task-oriented dialogue systems. However, automatic speech recognition (ASR) errors often impair the understanding performance. Despite many previous models have obtained promising results for improving ASR robustness in SLU, most of them treat clean manual transcripts and ASR transcripts equally during the fine-tuning stage. To tackle this issue, in this paper, we propose a novel method termed C²A-SLU. Specifically speaking, we add calculated cross attention to the original hidden states and apply contrastive attention to compare the input transcript with clean manual transcripts to distill the contrastive information, which can better capture distinctive features of ASR transcripts. Experiments on three datasets show that C²A-SLU surpasses existing models and achieves a new state-of-the-art performance, with a relative improvement of 3.4% in terms of accuracy over the previous best model on SLURP dataset",
    "checked": true,
    "id": "55b95df9297c2eb3dbf32b2cbc65c402ad67a856",
    "semantic_title": "c²a-slu: cross and contrastive attention for improving asr robustness in spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/weld23_interspeech.html": {
    "title": "Tri-level Joint Natural Language Understanding for Multi-turn Conversational Datasets",
    "volume": "main",
    "abstract": "Natural language understanding typically maps single utterances to a dual level semantic frame, sentence level intent and slot labels at the word level. The best performing models force explicit interaction between intent detection and slot filling. We present a novel tri-level joint natural language understanding approach, adding domain, and explicitly exchange semantic information between all levels. This approach enables the use of multi-turn datasets which are a more natural conversational environment than single utterance. We evaluate our model on two multi-turn datasets for which we are the first to conduct joint slot-filling and intent detection. Our model outperforms state-of-the-art joint models in slot filling and intent detection on multi-turn data sets. We provide an analysis of explicit interaction locations between the layers. We conclude that including domain information improves model performance",
    "checked": true,
    "id": "bbebba31b33f265c2e9b475080051b1a8246f089",
    "semantic_title": "tri-level joint natural language understanding for multi-turn conversational datasets",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/laperriere23_interspeech.html": {
    "title": "Semantic Enrichment Towards Efficient Speech Representations",
    "volume": "main",
    "abstract": "Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR",
    "checked": true,
    "id": "6b0aff42f33d169a0ac5668e23f5f99f695da170",
    "semantic_title": "semantic enrichment towards efficient speech representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kashiwagi23b_interspeech.html": {
    "title": "Tensor decomposition for minimization of E2E SLU model toward on-device processing",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters",
    "checked": true,
    "id": "0534fd0ed04acaa60f820b730bf3c4816767fa43",
    "semantic_title": "tensor decomposition for minimization of e2e slu model toward on-device processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mao23_interspeech.html": {
    "title": "DiffSLU: Knowledge Distillation Based Diffusion Model for Cross-Lingual Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) has achieved great success in high-resource languages, but it still remains challenging in the low-resource languages due to the scarcity of labeled training data. Hence, there is an increasing interest in zero-shot cross-lingual SLU. SLU typically has two subtasks, including intent detection and slot filling. Slots and intent in the same utterance are correlated, thus it is beneficial to achieve mutual guidance between them. In this paper, we propose a novel cross-lingual SLU framework termed DiffSLU, which leverages powerful diffusion model to enhance the mutual guidance. In addition, we also utilize knowledge distillation to facilitate knowledge transfer. Experimental results demonstrate that our DiffSLU can improve the performance compared with the strong baselines and achieves the new state-of-the-art performance on MultiATIS++ dataset, obtaining a relative improvement of 3.1% over the previous best model in overall accuracy",
    "checked": true,
    "id": "9e46a2f7d0193fee8b3d702bfeb9692ed6660e14",
    "semantic_title": "diffslu: knowledge distillation based diffusion model for cross-lingual spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arora23_interspeech.html": {
    "title": "Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding",
    "volume": "main",
    "abstract": "There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances",
    "checked": true,
    "id": "0c7018db4a00df1792a7b3de3cb0b48aa19ca041",
    "semantic_title": "integrating pretrained asr and lm to perform sequence generation for spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23e_interspeech.html": {
    "title": "Contrastive Learning Based ASR Robust Knowledge Selection For Spoken Dialogue System",
    "volume": "main",
    "abstract": "The construction of knowledge-based, task-oriented systems for spoken conversations is a challenging task. Given the spoken dialogue history information, a knowledge selection model selects the appropriate knowledge snippet from an unstructured knowledge base. However, the performance of this model is sensitive to automatic speech recognizer (ASR) recognition errors. To address this problem, we propose a method called CLKS, which develops a knowledge selection model that is robust to ASR recognition errors. This approach involves: 1) To leverage a wide range of information from various ASR outputs, we employ the self-attention mechanism to aggregate the representation of the N-best hypotheses of the dialogue history. 2) We use the written dialogue representation to guide the aggregated spoken dialogue representation to select the correct knowledge candidate through contrastive learning. Experimental results on the DSTC10 dataset demonstrate the effectiveness of our method",
    "checked": true,
    "id": "519f73c93cf31cce7d760f146235e8f0fd2c179c",
    "semantic_title": "contrastive learning based asr robust knowledge selection for spoken dialogue system",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23f_interspeech.html": {
    "title": "Unsupervised Dialogue Topic Segmentation in Hyperdimensional Space",
    "volume": "main",
    "abstract": "We present HyperSeg, a hyperdimensional computing (HDC) approach to unsupervised dialogue topic segmentation. HDC is a class of vector symbolic architectures that leverages the probabilistic orthogonality of randomly drawn vectors at extremely high dimensions (typically over 10,000). HDC generates rich token representations through its low-cost initialization of many unrelated vectors. This is especially beneficial in topic segmentation, which often operates as a resource-constrained pre-processing step for downstream transcript understanding tasks. HyperSeg outperforms the current state-of-the-art in 4 out of 5 segmentation benchmarks -- even when baselines are given partial access to the ground truth -- and is 10 times faster on average. We show that HyperSeg also improves downstream summarization accuracy. With HyperSeg, we demonstrate the viability of HDC in a major language task. We open-source HyperSeg to provide a strong baseline for unsupervised topic segmentation",
    "checked": true,
    "id": "073c91fc6082111154d4525b48eca8e54477f71f",
    "semantic_title": "unsupervised dialogue topic segmentation in hyperdimensional space",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cappellazzo23_interspeech.html": {
    "title": "An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Continual learning refers to a dynamical framework in which a model receives a stream of non-stationary data over time and must adapt to new data while preserving previously acquired knowledge. Unluckily, neural networks fail to meet these two desiderata, incurring the so-called catastrophic forgetting phenomenon. Whereas a vast array of strategies have been proposed to attenuate forgetting in the computer vision domain, for speech-related tasks, on the other hand, there is a dearth of works. In this paper, we consider the joint use of rehearsal and knowledge distillation (KD) approaches for spoken language understanding under a class-incremental learning scenario. We report on multiple KD combinations at different levels in the network, showing that combining feature-level and predictions-level KDs leads to the best results. Finally, we provide an ablation study on the effect of the size of the rehearsal memory that corroborates the efficacy of our approach for low-resource devices",
    "checked": true,
    "id": "2802090f23e01cc1341a767daf825c6256341a0f",
    "semantic_title": "an investigation of the combination of rehearsal and knowledge distillation in continual learning for spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23h_interspeech.html": {
    "title": "Enhancing New Intent Discovery via Robust Neighbor-based Contrastive Learning",
    "volume": "main",
    "abstract": "New intent discovery (NID) has become a hot topic for dialogue system, which aims to discover the Out-Of-Domain intents from conversation corpus and classify these utterances correctly. Existing methods usually focus on learning compact representations of utterances, and leverage the clustering algorithm to generate new intents. Inspired by the recent progress of contrastive learning, in this work, we propose a novel neighbor-based contrastive learning (NCL) to obtain discriminative representations for utterances. Specifically, to enhance the robustness of NCL, on the one hand, we pick out diverse samples as positive pairs by considering both the anchor neighborhood and nearby neighborhood. On the other hand, we also devise a boundary distance constraint to avoid introducing noisy samples when extending the positives via neighbors. Extensive experiments are conducted on three public NID datasets and the results demonstrate the competitiveness and effectiveness of our proposed approach",
    "checked": true,
    "id": "b789782866e0ba2078353dc5a3331a589263b9f5",
    "semantic_title": "enhancing new intent discovery via robust neighbor-based contrastive learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schwarz23_interspeech.html": {
    "title": "Personalized Predictive ASR for Latency Reduction in Voice Assistants",
    "volume": "main",
    "abstract": "Streaming Automatic Speech Recognition (ASR) in voice assistants can utilize prefetching to partially hide the latency of response generation. Prefetching involves passing a preliminary ASR hypothesis to downstream systems in order to prefetch and cache a response. If the final ASR hypothesis after endpoint detection matches the preliminary one, the cached response can be delivered to the user, thus saving latency. In this paper, we extend this idea by introducing predictive automatic speech recognition, where we predict the full utterance from a partially observed utterance, and prefetch the response based on the predicted utterance. We introduce two personalization approaches and investigate the tradeoff between potential latency gains from successful predictions and the cost increase from failed predictions. We evaluate our methods on an internal voice assistant dataset as well as the public SLURP dataset",
    "checked": true,
    "id": "0fed61ee16a5fe55a74a7a1791f869c1ac2b67c8",
    "semantic_title": "personalized predictive asr for latency reduction in voice assistants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ray23_interspeech.html": {
    "title": "Compositional Generalization in Spoken Language Understanding",
    "volume": "main",
    "abstract": "State-of-the-art spoken language understanding (SLU) models have shown tremendous success in benchmark SLU datasets, yet they still fail in many practical scenario due to the lack of model compositionality when trained on limited training data. In this paper, we study two types of compositionality: novel slot combination, and length generalization. We first conduct in-depth analysis, and find that state-of-the-art SLU models often learn spurious slot correlations during training, which leads to poor performance in both compositional cases. To mitigate these limitations, we create the first compositional splits of benchmark SLU datasets and we propose the first compositional SLU model, including compositional loss and paired training that tackle each compositional case respectively. On both benchmark and compositional splits in ATIS and SNIPS, we show that our compositional SLU model significantly outperforms (up to 5% F1 score) state-of-the-art BERT SLU model",
    "checked": true,
    "id": "0543cd2268bb70b7557a620019022f37350b92b7",
    "semantic_title": "compositional generalization in spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ha_interspeech.html": {
    "title": "Sampling bias in NLU models: Impact and Mitigation",
    "volume": "main",
    "abstract": "Natural Language Understanding (NLU) systems such as chatbots or virtual assistants have seen a significant rise in popularity in recent times, thanks to availability of large volumes of user data. However, typical user data collected for training such models may suffer from sampling biases due to a variety of factors. In this paper, we study the impact of bias in the training data for intent classification task, a core component of NLU systems. We experiment with three kinds of data bias settings: (i) random down-sampling, (ii) class-dependent bias, and (iii) class-independent bias injection. For each setting, we report the loss in model performance and survey strategies to mitigate the loss from two families of methods: (i) semi-supervised learning (SSL), and (ii) synthetic data generation. Overall, we find that while both methods perform well with random down-sampling, synthetic data generation out-performs SSL when only biased training data is available",
    "checked": true,
    "id": "fd60853c71938007a8564d53b25f4a15e746a4d5",
    "semantic_title": "sampling bias in nlu models: impact and mitigation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23d_interspeech.html": {
    "title": "5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair",
    "volume": "main",
    "abstract": "Providing voice assistants the ability to navigate multi-turn conversations is a challenging problem. Handling multi-turn interactions requires the system to understand various conversational use-cases, such as steering, intent carryover, disfluencies, entity carryover, and repair. The complexity of this problem is compounded by the fact that these use-cases mix with each other, often appearing simultaneously in natural language. This work proposes a non-autoregressive query rewriting architecture that can handle not only the five aforementioned tasks, but also complex compositions of these use-cases. We show that our proposed model has competitive single task performance compared to the baseline approach, and even outperforms a fine-tuned T5 model in use-case compositions, despite being 15 times smaller in parameters and 25 times faster in latency",
    "checked": true,
    "id": "f7b21c1a591129d926d8ddb6b1babe2b0e140b50",
    "semantic_title": "5ider: unified query rewriting for steering, intent carryover, disfluencies, entity carryover and repair",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23e_interspeech.html": {
    "title": "Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation",
    "volume": "main",
    "abstract": "The aim of emotion prediction in conversation (EPC) is to predict the future emotional state of a speaker based on context information, which is essential for conducting a friendly human-computer conversation. Most EPC works only investigated context information by merging a speaker's multiple utterances into a single utterance per turn and focused on conversations in a dual-speaker scenario, which ignored the information in multi-utterance turn and a more complex and natural scenario of multi-speaker conversations. This paper introduces a context information modeling approach that considers potential emotional interactive information within a speaker's multi-utterance turn, which dominates his/her future emotions. Moreover, our approach advances emotion prediction in both dual- and multi-speaker conversations. Experimental results show that such an approach significantly enhances context information modeling and renders a higher accuracy in EPC than reported in the literature",
    "checked": true,
    "id": "e45ee1978b572b07ffb578c0dcd855510015d56c",
    "semantic_title": "emotion awareness in multi-utterance turn for improving emotion prediction in multi-speaker conversation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ga_interspeech.html": {
    "title": "WhiSLU: End-to-End Spoken Language Understanding with Whisper",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) systems commonly use cascading structures. However, these systems are prone to error propagation, information loss, high costs, and latency, leading researchers to explore end-to-end (E2E) SLU as a hot topic. However, E2E SLU faces the challenge of insufficient data, resulting in most previous work relying on pretrained acoustic models. Nevertheless, pre-training task and SLU task solution spaces are often substantially different, making it difficult for E2E SLU models to surpass cascading models. To address this, we propose using OpenAI's Whisper model for SLU tasks. We employ the Sequence-level Multitask Learning (SML) paradigm, which encodes multiple ASR-related tasks into a sequence for learning. Our method significantly outperforms the E2E baseline by a large margin (with a 10% improvement in EM score) and even outperforms cascading models, achieving a 77% EM score on the STOP dataset, demonstrating its effectiveness",
    "checked": true,
    "id": "005d3c8d253c940f5b709258df5975dde7f17259",
    "semantic_title": "whislu: end-to-end spoken language understanding with whisper",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wen23b_interspeech.html": {
    "title": "Biophysically-inspired single-channel speech enhancement in the time domain",
    "volume": "main",
    "abstract": "Most state-of-the-art speech enhancement (SE) methods utilize time-frequency (T-F) features or waveforms as input features and have poor generalizability at negative signal-to-noise ratios (SNR). To overcome these issues, we propose a novel network that integrates biophysical properties of the human auditory system known to perform even at negative SNRs. We generated biophysical features using CoNNear, a neural network auditory model, which were fed into a SOTA speech enhancement model AECNN. The model was trained on the INTERSPEECH 2021 DNS Challenge dataset and evaluated on mismatched noise conditions at various SNRs. The experimental results revealed that the bio-inspired approaches outperformed T-F and waveform features under positive SNRs and demonstrated stronger robustness to unseen noise at negative SNRs. We conclude that incorporating human-like features can extend the operating range of SE systems to more negative SNRs",
    "checked": true,
    "id": "b1a4342950e0081fb86220105d660e36eb59a6fa",
    "semantic_title": "biophysically-inspired single-channel speech enhancement in the time domain",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jalal23_interspeech.html": {
    "title": "On-Device Speaker Anonymization of Acoustic Embeddings for ASR based on Flexible Location Gradient Reversal Layer",
    "volume": "main",
    "abstract": "Smart devices serviced by large-scale AI models necessitates user data transfer to the cloud for inference. For speech applications, this means transferring private user information, e.g., speaker identity. Our paper proposes a privacy-enhancing framework that targets speaker identity anonymization while preserving speech recognition accuracy for our downstream task Automatic Speech Recognition (ASR). The proposed framework attaches flexible gradient reversal based speaker adversarial layers to target layers within an ASR model, where speaker adversarial training anonymizes acoustic embeddings generated by the targeted layers to remove speaker identityy. We propose on-device deployment by execution of initial layers of the ASR model, and transmitting anonymized embeddings to the cloud, where the rest of the model is executed while preserving privacy. The results show that our method efficiently reduces speaker recognition relative accuracy by 33%, and improves ASR performance by achieving 6.2% relative Word Error Rate (WER) reduction",
    "checked": false,
    "id": "a9563078af069997af352dfe6e9205404d787f5c",
    "semantic_title": "on-device speaker anonymization of acoustic embeddings for asr based onflexible location gradient reversal layer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23b_interspeech.html": {
    "title": "How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning",
    "volume": "main",
    "abstract": "Shortcut learning, or 'Clever Hans effect' refers to situations where a learning agent (e.g., deep neural networks) learns spurious correlations present in data, resulting in biased models. We focus on finding shortcuts in deep learning based spoofing countermeasures (CMs) that predict whether a given utterance is spoofed or not. While prior work has addressed specific data artifacts, such as silence, no general normative framework has been explored for analyzing shortcut learning in CMs. In this study, we propose a generic approach to identifying shortcuts by introducing systematic interventions on the training and test sides, including the boundary cases of 'near-perfect' and 'worse than coin flip' (label flip). By using three different models, ranging from classic to state-of-the-art, we demonstrate the presence of shortcut learning in five simulated conditions. We also analyze the results using a regression model to understand how biases affect the class-conditional score statistics",
    "checked": true,
    "id": "d499af874ef3bd4c7de351b42d479efad8c79a7c",
    "semantic_title": "how to construct perfect and worse-than-coin-flip spoofing countermeasures: a word of warning on shortcut learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23c_interspeech.html": {
    "title": "CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram",
    "volume": "main",
    "abstract": "In this work, we present CleanUNet 2, a speech denoising model that combines the advantages of waveform denoiser and spectrogram denoiser and achieves the best of both worlds. CleanUNet 2 uses a two-stage framework inspired by popular speech synthesis methods that consist of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further boosts its performance by taking predicted spectrograms from a spectrogram denoiser as the input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations",
    "checked": true,
    "id": "8926261bf0181a30d92efb0990e924ce2cb5f552",
    "semantic_title": "cleanunet 2: a hybrid speech denoising model on waveform and spectrogram",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23e_interspeech.html": {
    "title": "A Two-stage Progressive Neural Network for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Recent studies in deep learning based acoustic echo cancellation proves the benefits of introducing a linear echo cancellation module. However, the convergence problem and potential target speech distortion impose an additional learning burden for the neural network. In this paper, we propose a two-stage progressive neural network consisting of a coarse-stage and a fine-stage module. For the coarse-stage, a light-weighted network module is designed to suppress partial echo and potential noise, where a voice activity detection path is used to enhance the learned features. For the fine-stage, a larger network is employed to deal with the more complex echo path and restore the near-end speech. We have conducted extensive experiments to verify the proposed method, and the results show that the proposed two-stage method provides a superior performance to other state-of-the-art methods",
    "checked": true,
    "id": "ff45fad1ca0be6c76264aa11a4c4e11eed8c718b",
    "semantic_title": "a two-stage progressive neural network for acoustic echo cancellation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23_interspeech.html": {
    "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
    "volume": "main",
    "abstract": "Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, underutilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beamsearch Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps",
    "checked": true,
    "id": "223569e02810238817ea7fbb12386410a04bcb7f",
    "semantic_title": "an intra-brnn and gb-rvq based end-to-end neural audio codec",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23r_interspeech.html": {
    "title": "Real-Time Personalised Speech Enhancement Transformers with Dynamic Cross-attended Speaker Representations",
    "volume": "main",
    "abstract": "Personalised speech enhancement (PSE) extracts only the speech of a target user and removes everything else from corrupted input audio. This can greatly improve on-device streaming audio processing, such as voice calls and speech recognition, which has strict requirements on model size and latency. To focus the PSE system on the target speaker, it is conditioned on a recording of the user's voice. This recording is usually summarised as a single static vector. However, a static vector cannot reflect all the target user's voice characteristics. Thus, we propose using the full recording. To condition on such a variable-length sequence, we propose fully Transformer-based PSE models with a cross-attention mechanism which generates target speaker representations dynamically. To better reflect the on-device scenario, we carefully design and publish a new PSE dataset. On the dataset, our proposed model significantly surpasses strong baselines while halving the model size and reducing latency",
    "checked": true,
    "id": "f0e33ad6855ba4f977b73f897484d8d46faee9c1",
    "semantic_title": "real-time personalised speech enhancement transformers with dynamic cross-attended speaker representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mamun23_interspeech.html": {
    "title": "CFTNet: Complex-valued Frequency Transformation Network for Speech Enhancement",
    "volume": "main",
    "abstract": "It is widely known that the presence of multi-speaker babble noise greatly degrades speech intelligibility. However, suppressing noise without creating artifacts in human speech is challenging in environments with a low signal-to-noise ratio (SNR), and even more so if noise is speechlike such as babble noise. Deep learning-based systems either enhance the magnitude response and reuse distorted phases or enhance the complex spectrogram. Frequency transformation block (FTB) has emerged as a useful architecture to implicitly capture harmonic correlation which is especially important for people with hearing loss (hearing aid/ cochlear implant users). This study proposes a complex-valued frequency transformation network (CFTNet) for speech enhancement, which leverages both a complex-valued U-Net and FTB to capture sufficient low-level contextual information. The proposed system learns a complex transformation matrix to accurately recover speech in the time-frequency domain from a noisy spectrogram. Experimental results demonstrate that the proposed system can achieve significant improvements in both seen and unseen noise over state-of-art networks. Furthermore, the proposed CFTNet can suppress highly nonstationary noise without creating musical artifacts commonly observed in conventional enhancement methods",
    "checked": true,
    "id": "202b65cb3c783aea57daecbd350b643bbc9acca9",
    "semantic_title": "cftnet: complex-valued frequency transformation network for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23h_interspeech.html": {
    "title": "Feature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement",
    "volume": "main",
    "abstract": "Large, pre-trained representation models trained using self-supervised learning have gained popularity in various fields of machine learning because they are able to extract high-quality salient features from input data. As such, they have been frequently used as base networks for various pattern classification tasks such as speech recognition. However, not much research has been conducted on applying these types of models to the field of speech signal generation. In this paper, we investigate the feasibility of using pre-trained speech representation models for a downstream speech enhancement task. To alleviate mismatches between the input features of the pre-trained model and the target enhancement model, we adopt a novel feature normalization technique to smoothly link these modules together. Our proposed method enables significant improvements in speech quality compared to baselines when combined with various types of pre-trained speech models",
    "checked": true,
    "id": "a323e0e959f8d8d3df94b2f05c2be3e9d1cfae68",
    "semantic_title": "feature normalization for fine-tuning self-supervised models in speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23c_interspeech.html": {
    "title": "Multi-mode Neural Speech Coding Based on Deep Generative Networks",
    "volume": "main",
    "abstract": "The wideband or super wideband speech is one of the most prominent features in real-time communication services, with higher resolution spectrum. However, it requires higher computing expenses. In this paper, we introduce the Penguins codec, based on a multi-mode neural speech coding structure that combines sub-band speech processing and applies different strategies from the low band to the high band. Especially, it refers to deep generative networks with perceptual constraint loss functions and knowledge distillations to reconstruct wideband components and bandwidth extension to generate artificial super wideband components. The method results in high-quality speech at very low bitrates. Several subjective and objective experiments, including ablation studies, were organized, and the results proved the merit of the proposed scheme when compared with traditional coding schemes and state-of-the-art neural coding methods",
    "checked": true,
    "id": "dbc492a0c1f2981be9d7a447730e52c72a5444a5",
    "semantic_title": "multi-mode neural speech coding based on deep generative networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bae23_interspeech.html": {
    "title": "Streaming Dual-Path Transformer for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement employing a dual-path transformer (DPT) with a dilated DenseNet-based encoder and decoder has shown state-of-the-art performance. By applying attention in both time and frequency paths, the DPT learns the long-term dependency of speech and the relationship between frequency components. However, the batch processing of the DPT, which performs attention on all past and future frames, makes it impractical for real-time applications. To satisfy the real-time requirement, we propose a streaming dual-path transformer (stDPT) with zero look-ahead structure. In the training phase, we apply masking techniques to control the context length, and in the inference phase, caching methods are utilized to preserve sequential information. Extensive experiments have been conducted to show the performance based on different context lengths, and the results verify that the proposed method outperforms the current state-of-the-art speech enhancement models based on real-time processing",
    "checked": true,
    "id": "4e2f2070a4ac85aec9ba32c6864868b3d4632560",
    "semantic_title": "streaming dual-path transformer for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kadkhodaeielyaderani23_interspeech.html": {
    "title": "Sequence-to-Sequence Multi-Modal Speech In-Painting",
    "volume": "main",
    "abstract": "Speech in-painting is the task of regenerating missing audio contents using reliable context information. Despite various recent studies in multi-modal perception of audio in-painting, there is still a need for an effective infusion of visual and auditory information in speech in-painting. In this paper, we introduce a novel sequence-to-sequence model that leverages the visual information to in-paint audio signals via an encoder-decoder architecture. The encoder plays the role of a lip-reader for facial recordings and the decoder takes both encoder outputs as well as the distorted audio spectrograms to restore the original speech. Our model outperforms an audio-only speech in-painting model and has comparable results with a recent multi-modal speech in-painter in terms of speech quality and intelligibility metrics for distortions of 300 ms to 1500 ms duration, which proves the effectiveness of the introduced multi-modality in speech in-painting",
    "checked": true,
    "id": "0ef005eaa93b572ff419bfe37f55b9d4358c9f63",
    "semantic_title": "sequence-to-sequence multi-modal speech in-painting",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23q_interspeech.html": {
    "title": "Hybrid AHS: A Hybrid of Kalman Filter and Deep Learning for Acoustic Howling Suppression",
    "volume": "main",
    "abstract": "Deep learning has been recently introduced for efficient acoustic howling suppression (AHS). However, the recurrent nature of howling creates a mismatch between offline training and streaming inference, limiting the quality of enhanced speech. To address this limitation, we propose a hybrid method that combines a Kalman filter with a self-attentive recurrent neural network (SARNN) to leverage their respective advantages for robust AHS. During offline training, a pre-processed signal obtained from the Kalman filter and an ideal microphone signal generated via teacher-forced training strategy are used to train the deep neural network (DNN). During streaming inference, the DNN's parameters are fixed while its output serves as a reference signal for updating the Kalman filter. Evaluation in both offline and streaming inference scenarios using simulated and real-recorded data shows that the proposed method efficiently suppresses howling and consistently outperforms baselines",
    "checked": true,
    "id": "ef989cfd7f93b3a8d61bfd546ee467f7548ac47e",
    "semantic_title": "hybrid ahs: a hybrid of kalman filter and deep learning for acoustic howling suppression",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ho23_interspeech.html": {
    "title": "Differentially Private Adapters for Parameter Efficient Acoustic Modeling",
    "volume": "main",
    "abstract": "In this work, we devise a parameter-efficient solution to bring differential privacy (DP) guarantees into adaptation of a cross-lingual speech classifier. We investigate a new frozen pretrained adaptation framework for DP-preserving speech modeling without full model fine-tuning. First, we introduce a noisy teacher-student ensemble into a conventional adaptation scheme leveraging a frozen pre-trained acoustic model and attain superior performance than DP-based stochastic gradient descent (DPSGD). Next, we insert residual adapters (RA) between layers of the frozen pre-trained acoustic model. The RAs reduce training cost and time significantly with a negligible performance drop. Evaluated on the open-access Multilingual Spoken Words (MLSW) dataset, our solution reduces the number of trainable parameters by 97.5% using the RAs with only a 4% performance drop with respect to fine-tuning the cross-lingual speech classifier while preserving DP guarantees",
    "checked": true,
    "id": "fad5ed6c02d2f6a2be6c474d14c70d9608777c5d",
    "semantic_title": "differentially private adapters for parameter efficient acoustic modeling",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23b_interspeech.html": {
    "title": "Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation",
    "volume": "main",
    "abstract": "Audio-visual speech enhancement (AV-SE) aims to enhance degraded speech along with extra visual information such as lip videos, and has been shown to be more effective than audio-only speech enhancement. This paper proposes further incorporating ultrasound tongue images to improve lip-based AV-SE systems' performance. Knowledge distillation is employed at the training stage to address the challenge of acquiring ultrasound tongue images during inference, enabling an audio-lip speech enhancement student model to learn from a pre-trained audio-lip-tongue speech enhancement teacher model. Experimental results demonstrate significant improvements in the quality and intelligibility of the speech enhanced by the proposed method compared to the traditional audio-lip speech enhancement baselines. Further analysis using phone error rates (PER) of automatic speech recognition (ASR) shows that palatal and velar consonants benefit most from the introduction of ultrasound tongue images",
    "checked": true,
    "id": "e9fe7cbfd3a4dca8d0ab9d37e03741c89e98fc7c",
    "semantic_title": "incorporating ultrasound tongue images for audio-visual speech enhancement through knowledge distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/uezu23_interspeech.html": {
    "title": "Consonant-emphasis Method Incorporating Robust Consonant-section Detection to Improve Intelligibility of Bone-conducted speech",
    "volume": "main",
    "abstract": "A consonant-emphasis (CE) method was proposed to improve the word intelligibility of presented speech by using bone-conducted (BC) headphones. However, the consonant-section detection (CSD) performance of this method is not robust against certain consonants. Therefore, a CE method with robust CSD is necessary for presented BC speech. We focused on improving the word intelligibility of presented BC speech in noisy environments and propose a CE method with robust CSD that combines the detection processes of voiced and unvoiced consonant sections. The evaluation of CSD procedures showed that more robust CSD procedure outperformed those of the conventional CE method as well as voiced CSD only and unvoiced CSD only. Word-intelligibility tests were also conducted on presented BC speech in noisy environments to compare the proposed and conventional methods, and the proposed method significantly improved word intelligibility over these conventional methods at a noise level of 75 dB",
    "checked": true,
    "id": "4589bb8446d21caa7d3e06e1e4f4a8778b1a3266",
    "semantic_title": "consonant-emphasis method incorporating robust consonant-section detection to improve intelligibility of bone-conducted speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sato23_interspeech.html": {
    "title": "Downstream Task Agnostic Speech Enhancement with Self-Supervised Representation Loss",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) is the latest breakthrough in speech processing, especially for label-scarce downstream tasks by leveraging massive unlabeled audio data. The noise robustness of the SSL is one of the important challenges to expanding its application. We can use speech enhancement (SE) to tackle this issue. However, the mismatch between the SE model and SSL models potentially limits its effect. In this work, we propose a new SE training criterion that minimizes the distance between clean and enhanced signals in the feature representation of the SSL model to alleviate the mismatch. We expect that the loss in the SSL domain could guide SE training to preserve or enhance various levels of characteristics of the speech signals that may be required for high-level downstream tasks. Experiments show that our proposal improves the performance of an SE and SSL pipeline on five downstream tasks with noisy input while maintaining the SE performance",
    "checked": true,
    "id": "649fd45481be158627e602252ce2d6c6c6a8713e",
    "semantic_title": "downstream task agnostic speech enhancement with self-supervised representation loss",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/byun23_interspeech.html": {
    "title": "Perceptual Improvement of Deep Neural Network (DNN) Speech Coder Using Parametric and Non-parametric Density Models",
    "volume": "main",
    "abstract": "This paper proposes a method to improve the perceptual quality of an end-to-end neural speech coder using density models for bottleneck samples. Two parametric and non-parametric approaches are explored for modeling the bottleneck sample density. The first approach utilizes a sub-network to generate mean-scale hyperpriors for bottleneck samples, while the second approach models the bottleneck samples using a separate sub-network without any side information. The whole network, including the sub-network, is trained using PAM-based perceptual losses in different timescales to shape quantization noise below the masking threshold. The proposed method achieves a frame-dependent entropy model that enhances arithmetic coding efficiency while emphasizing perceptually relevant audio cues. Experimental results show that the proposed density model combined with PAM-based losses improves perceptual quality compared to conventional speech coders in both objective and subjective tests",
    "checked": true,
    "id": "91c932d16623a7c8904af0aec728064a059843b6",
    "semantic_title": "perceptual improvement of deep neural network (dnn) speech coder using parametric and non-parametric density models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23j_interspeech.html": {
    "title": "DeFT-AN RT: Real-time Multichannel Speech Enhancement using Dense Frequency-Time Attentive Network and Non-overlapping Synthesis Window",
    "volume": "main",
    "abstract": "In real-time speech enhancement models based on the short-time Fourier transform (STFT), algorithmic latency induced by the STFT window size can induce perceptible delays, leading to reduced immersion in real-time applications. This study proposes an efficient real-time enhancement model based on dense frequency-time attentive network (DeFT-AN). The vanilla DeFT-AN consists of cascaded dense blocks and time-frequency transformers, which allow for a smooth transition between time frames through a temporal attention mechanism. To inherit this advantage and reduce algorithmic latency, we develop the lightweight and causal version of DeFT-AN with dual-window size processing that utilizes synthesis windows shorter than analysis windows. The benefit of DeFT-AN in identifying temporal context enables the use of non-overlapping synthesis windows, and experimental results show that the model can achieve the highest performance with the lowest algorithmic latency among STFT-based models",
    "checked": true,
    "id": "fc3e2cb1b0b3a850621bd8d293d9ab62abb5abaf",
    "semantic_title": "deft-an rt: real-time multichannel speech enhancement using dense frequency-time attentive network and non-overlapping synthesis window",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23b_interspeech.html": {
    "title": "A More Accurate Internal Language Model Score Estimation for the Hybrid Autoregressive Transducer",
    "volume": "main",
    "abstract": "We present a novel constrained learning method for hybrid autoregressive transducer (HAT) models that results in more validated language model (LM) adaptation. LM adaptation in HAT is justified only when the transducer logits and the sum of speech and text logits in the label estimation sub-networks are approximately the same. The mean squared error (MSE) between the two logits was added to the HAT loss to encourage the HAT models to satisfy the required condition. The proposed method exhibited significantly lower and more stable internal language model perplexities than those of HAT. Consequently, it attained lower word error rates (WERs) compared to HAT in various model architecture settings and in both cases with and without LM adaptation. In the television content task, the proposed method achieved a relative reduction in WERs of up to 28.60% compared to HAT. In most cases, the accuracy of pre-trained HAT models also improved upon training with the additional MSE loss",
    "checked": true,
    "id": "c1f4fd83d318525db351c7fac8e4dc7863662c87",
    "semantic_title": "a more accurate internal language model score estimation for the hybrid autoregressive transducer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23_interspeech.html": {
    "title": "Attention Gate Between Capsules in Fully Capsule-Network Speech Recognition",
    "volume": "main",
    "abstract": "We present a novel capsule network-based speech recognition model that effectively utilizes the full context of past time capsules. The input capsule sequences are recurrently used by filtering unnecessary contextual information using multi-head attention, which uses previous time output vectors as keys and values, and current time output vectors as queries. We applied the attention gate to the sequential dynamic routing (SDR), an all-capsule speech recognition model. The proposed method attained higher accuracy than the existing SDR with two attention heads on all test sets of the TIMIT and Wall Street Journal (WSJ) corpora while maintaining the same algorithmic delay. For the WSJ corpus, 10.75% of a relative word error rate (WER) reduction was achieved when the required delay was set to 525 ms. In addition, the model showed a 1.76x reduction in delay while maintaining the WERs. The proposed method results in an increase of approximately 0.1% in the number of parameters",
    "checked": true,
    "id": "97bb673f61e3b5370f22ff5527680e3e48f91201",
    "semantic_title": "attention gate between capsules in fully capsule-network speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rakib23_interspeech.html": {
    "title": "OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking",
    "volume": "main",
    "abstract": "We present OOD-Speech, the first out-of-distribution (OOD) benchmarking dataset for Bengali automatic speech recognition (ASR). Bengali, being one of the most spoken languages, portrays large diversity in dialects and prosodic features, which demands ASR frameworks robust toward distribution shifts. For example, Islamic religious sermons in Bengali are delivered with a tonality significantly different from regular speech. To reflect this, our work comprises a large train set and a diverse test set. Our train dataset is collected via massively online crowd-sourcing campaigns resulting in 1178.30 hours being voluntarily contributed and curated from 22,645 native Bengali speakers from South Asia. Our test dataset comprises 22.67 hours of speech collected and manually annotated from 17 different sources, e.g., TV drama, Audiobooks, Online classes, Islamic sermons, etc. OOD-Speech is jointly the largest publicly available speech dataset & the first OOD ASR benchmarking dataset for Bengali",
    "checked": true,
    "id": "0079e59b1e2a4251ef0762defca108991aaf0744",
    "semantic_title": "ood-speech: a large bengali speech recognition dataset for out-of-distribution benchmarking",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23g_interspeech.html": {
    "title": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark",
    "volume": "main",
    "abstract": "Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research",
    "checked": true,
    "id": "090b284b2f8fc93ac3e7a92fc9f91bf4965ba75c",
    "semantic_title": "ml-superb: multilingual speech universal performance benchmark",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23l_interspeech.html": {
    "title": "General-purpose Adversarial Training for Enhanced Automatic Speech Recognition Model Generalization",
    "volume": "main",
    "abstract": "We present a new adversarial training method called General-purpose adversarial training (GPAT) that enhances the performance of automatic speech recognition models. In GPAT, we propose the followings: (1) a plausible adversarial examples converter (PAC); (2) a distribution matching regularization term (DM reg.). Compared to previous studies that directly compute gradients with respect to the input, PAC incorporates non-linearity to achieve performance improvement while eliminating the need for extra forward passes. Furthermore, unlike previous studies that use fixed norms, GPAT can generate similar yet diverse samples through DM reg. We demonstrate that the GPAT elevates the performance of various models on the LibriSpeech dataset. Specifically, by applying GPAT to the conformer model, we achieved 5.3% average relative improvements. With respect to the wav2vec 2.0 experiments, our method yielded a 2.0%/4.4% word error rate on the LibriSpeech test sets without a language model",
    "checked": true,
    "id": "818da1cd8dae525aec31b499e468003d0741a964",
    "semantic_title": "general-purpose adversarial training for enhanced automatic speech recognition model generalization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23_interspeech.html": {
    "title": "Joint Instance Reconstruction and Feature Subspace Alignment for Cross-Domain Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition is a popular research branch of speech signal processing. Many previous studies have proven that the generalization ability of the emotion recognition model across domains can be improved by using transfer learning methods. To solve the cross-domain speech emotion recognition problem, this paper proposes a novel transfer learning method, which simultaneously performs the instance reconstruction and subspace alignment. Firstly, we conduct the instance transferring based on coupled projection, which utilizes a weighting reconstruction strategy to exploit the intrinsic information of cross-domain samples and improve the contribution of essential features through an adaptive weighting matrix. Then, we conduct the feature transferring through a novel co-regularized term, which can make the source and target subspace be well aligned. Finally, extensive experiments indicate that our method is superior to several state-of-the-art methods",
    "checked": true,
    "id": "0e22929ae837d48dad4a47352754dc0b51c6e313",
    "semantic_title": "joint instance reconstruction and feature subspace alignment for cross-domain speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moriya23_interspeech.html": {
    "title": "Knowledge Distillation for Neural Transducer-based Target-Speaker ASR: Exploiting Parallel Mixture/Single-Talker Speech Data",
    "volume": "main",
    "abstract": "Neural transducer (RNNT)-based target-speaker speech recognition (TS-RNNT) directly transcribes a target speaker's voice from a multi-talker mixture. It is a promising approach for streaming applications because it does not incur the extra computation costs of a target speech extraction frontend, which is a critical barrier to quick response. TS-RNNT is trained end-to-end given the input speech (i.e., mixtures and enrollment speech) and reference transcriptions. The training mixtures are generally simulated by mixing single-talker signals, but conventional TS-RNNT training does not utilize single-speaker signals. This paper proposes using knowledge distillation (KD) to exploit the parallel mixture/single-talker speech data. Our proposed KD scheme uses an RNNT system pretrained with the target single-talker speech input to generate pseudo labels for the TS-RNNT training. Experimental results show that TS-RNNT systems trained with the proposed KD scheme outperform a baseline TS-RNNT",
    "checked": true,
    "id": "000ca94ed121225aa997088def719c747c4fb797",
    "semantic_title": "knowledge distillation for neural transducer-based target-speaker asr: exploiting parallel mixture/single-talker speech data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23i_interspeech.html": {
    "title": "Random Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition",
    "volume": "main",
    "abstract": "One of limitations in end-to-end automatic speech recognition (ASR) framework is its performance would be compromised if train-test utterance lengths are mismatched. In this paper, we propose an on-the-fly random utterance concatenation (RUC) based data augmentation method to alleviate train-test utterance length mismatch issue for short-video ASR task. Specifically, we are motivated by observations that our human-transcribed training utterances tend to be much shorter for short-video spontaneous speech (∼3 seconds on average), while our test utterance generated from voice activity detection front-end is much longer (∼10 seconds on average). Such a mismatch can lead to suboptimal performance. Empirically, it's observed the proposed RUC method significantly improves long utterance recognition without performance drop on short one. Overall, it achieves 5.72% word error rate reduction on average for 15 languages and improved robustness to various utterance length",
    "checked": true,
    "id": "92f621eec89837c3b3f353690881a9d409b07e67",
    "semantic_title": "random utterance concatenation based data augmentation for improving short-video speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muthuchamyselvaraj23_interspeech.html": {
    "title": "Adapter Incremental Continual Learning of Efficient Audio Spectrogram Transformers",
    "volume": "main",
    "abstract": "Efficient tuning of neural networks for continual learning with minimal computational resources remains a challenge. In this paper, we propose continual learning of audio classifiers with parameter and compute efficient Audio Spectrogram Transformers (AST). To reduce the trainable parameters without performance degradation we propose AST with Convolutional Adapter, which has less than 5% of trainable parameters of full fine-tuning. To reduce the computational complexity of self-attention, we introduce a novel Frequency-Time factorized Attention (FTA) method that achieves competitive performance with only a factor of the computations. Finally, we formulate our method called Adapter Incremental Continual Learning (AI-CL), as a combination of the parameter-efficient Convolutional Adapter and the compute-efficient FTA. Experiments on ESC-50, SpeechCommandsV2, and Audio-Visual Event benchmarks show that our proposed method efficiently learns new tasks and prevents catastrophic forgetting",
    "checked": true,
    "id": "81018236b57da31875a453866b884e3b3dda71e7",
    "semantic_title": "adapter incremental continual learning of efficient audio spectrogram transformers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23_interspeech.html": {
    "title": "Rethinking Speech Recognition with A Multimodal Perspective via Acoustic and Semantic Cooperative Decoding",
    "volume": "main",
    "abstract": "Attention-based encoder-decoder (AED) models have shown impressive performance in ASR. However, most existing AED methods neglect to simultaneously leverage both acoustic and semantic features in decoder, which is crucial for generating more accurate and informative semantic states. In this paper, we propose an Acoustic and Semantic Cooperative Decoder (ASCD) for ASR. In particular, unlike vanilla decoders that process acoustic and semantic features in two separate stages, ASCD integrates them cooperatively. To prevent information leakage during training, we design a Causal Multimodal Mask. Moreover, a variant Semi-ASCD is proposed to balance accuracy and computational cost. Our proposal is evaluated on the publicly available AISHELL-1 and aidatatang_200zh datasets using Transformer, Conformer, and Branchformer as encoders, respectively. The experimental results show that ASCD significantly improves the performance by leveraging both the acoustic and semantic information cooperatively",
    "checked": true,
    "id": "0be940a9578fe944175394ec8e0042cec258f7d2",
    "semantic_title": "rethinking speech recognition with a multimodal perspective via acoustic and semantic cooperative decoding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23b_interspeech.html": {
    "title": "Improving Code-Switching and Name Entity Recognition in ASR with Speech Editing based Data Augmentation",
    "volume": "main",
    "abstract": "Recently, end-to-end (E2E) automatic speech recognition (ASR) models have made great strides and exhibit excellent performance in general speech recognition. However, there remain several challenging scenarios that E2E models are not competent in, such as code-switching and named entity recognition (NER). Data augmentation is a common and effective practice for these two scenarios. However, the current data augmentation methods mainly rely on audio splicing and text-to-speech (TTS) models, which might result in discontinuous, unrealistic, and less diversified speech. To mitigate these potential issues, we propose a novel data augmentation method by applying the text-based speech editing model. The augmented speech from speech editing systems is more coherent and diversified, also more akin to real speech. The experimental results on code-switching and NER tasks show that our proposed method can significantly outperform the audio splicing and neural TTS based data augmentation systems",
    "checked": true,
    "id": "85fe4894afe95e326814d8b7fa41b308a0054255",
    "semantic_title": "improving code-switching and name entity recognition in asr with speech editing based data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23h_interspeech.html": {
    "title": "Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts",
    "volume": "main",
    "abstract": "This paper presents a novel algorithm for building an automatic speech recognition (ASR) model with imperfect training data. Imperfectly transcribed speech is a prevalent issue in human-annotated speech corpora, which degrades the performance of ASR models. To address this problem, we propose Bypass Temporal Classification (BTC) as an expansion of the Connectionist Temporal Classification (CTC) criterion. BTC explicitly encodes the uncertainties associated with transcripts during training. This is accomplished by enhancing the flexibility of the training graph, which is implemented as a weighted finite-state transducer (WFST) composition. The proposed algorithm improves the robustness and accuracy of ASR systems, particularly when working with imprecisely transcribed speech corpora. Our implementation will be open-sourced",
    "checked": true,
    "id": "b620d46668d62930e41393168434118bb9a2bfcb",
    "semantic_title": "bypass temporal classification: weakly supervised automatic speech recognition with imperfect transcripts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lv23_interspeech.html": {
    "title": "DCCRN-KWS: An Audio Bias Based Model for Noise Robust Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "Real-world complex acoustic environments especially the ones with a low signal-to-noise ratio (SNR) will bring tremendous challenges to a keyword spotting (KWS) system. Inspired by the recent advances of neural speech enhancement and context bias in speech recognition, we propose a robust audio context bias based DCCRN-KWS model to address this challenge. We form the whole architecture as a multi-task learning framework for both denoising and keyword spotting, where the DCCRN encoder is connected with the KWS model. Helped with the denoising task, we further introduce an audio context bias module to leverage the real keyword samples and bias the network to better discriminate keywords in noisy conditions. Feature merge and complex context linear modules are also introduced to strengthen such discrimination and to effectively leverage contextual information respectively. Experiments on an internal challenging dataset and the HIMIYA public dataset show that DCCRN-KWS is superior in performance, while the ablation study demonstrates the good design of the whole model",
    "checked": true,
    "id": "4f9f170be7b22dc9ad5f220e8f1ea414a9399bbe",
    "semantic_title": "dccrn-kws: an audio bias based model for noise robust small-footprint keyword spotting",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23b_interspeech.html": {
    "title": "OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models have shown great promise over Supervised Learning (SL) ones in low-resource settings. However, the advantages of SSL are gradually weakened when the amount of labeled data increases in many industrial applications. To further improve the ASR performance when abundant labels are available, we first explore the potential of combining SL and SSL ASR models via analyzing their complementarity in recognition accuracy and optimization property. Then, we propose a novel Optimal Transport based Fusion (OTF) method for SL and SSL models without incurring extra computation cost in inference. Specifically, optimal transport is adopted to softly align the layer-wise weights to unify the two different networks into a single one. Experimental results on the public 1k-hour English LibriSpeech dataset and our in-house 2.6k-hour Chinese dataset show that OTF largely outperforms the individual models with lower error rates",
    "checked": true,
    "id": "3820ec258664fab9e279db8ca7d31d375e6e530b",
    "semantic_title": "otf: optimal transport based fusion of supervised and self-supervised learning models for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bleeker23_interspeech.html": {
    "title": "Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents an extension to train end-to-end Context-Aware Transformer Transducer (CATT) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases in the context list during training, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications",
    "checked": true,
    "id": "96a2c0877721ab1915dd9baa38286d2d950691d6",
    "semantic_title": "approximate nearest neighbour phrase mining for contextual speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vandereeckt23_interspeech.html": {
    "title": "Rehearsal-Free Online Continual Learning for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Fine-tuning an Automatic Speech Recognition (ASR) model to new domains results in degradation on original domains, referred to as Catastrophic Forgetting (CF). Continual Learning (CL) attempts to train ASR models without suffering from CF. While in ASR, offline CL is usually considered, online CL is a more realistic but also more challenging scenario where the model, unlike in offline CL, does not know when a task boundary occurs. Rehearsal-based methods, which store previously seen utterances in a memory, are often considered for online CL, in ASR and other research domains. However, recent research has shown that weight averaging is an effective method for offline CL in ASR. Based on this result, we propose, in this paper, a rehearsal-free method applicable for online CL. Our method outperforms all baselines, including rehearsal-based methods, in two experiments. Our method is a next step towards general CL for ASR, which should enable CL in all scenarios with few if any constraints",
    "checked": true,
    "id": "2febc2e47c944378865796121466056156d195ae",
    "semantic_title": "rehearsal-free online continual learning for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23_interspeech.html": {
    "title": "Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring",
    "volume": "main",
    "abstract": "Speech fluency/disfluency can be evaluated by analyzing a range of phonetic and prosodic features. Deep neural networks are commonly trained to map fluency-related features into the human scores. However, the effectiveness of deep learning-based models is constrained by the limited amount of labeled training samples. To address this, we introduce a self-supervised learning (SSL) approach that takes into account phonetic and prosody awareness for fluency scoring. Specifically, we first pre-train the model using a reconstruction loss function, by masking phones and their durations jointly on a large amount of unlabeled speech and text prompts. We then fine-tune the pre-trained model using human-annotated scoring data. Our experimental results, conducted on datasets such as Speechocean762 and our non-native datasets, show that our proposed method outperforms the baseline systems in terms of Pearson correlation coefficients (PCC). Moreover, we also conduct an ablation study to better understand the contribution of phonetic and prosody factors during the pre-training stage",
    "checked": true,
    "id": "cf8e7ecad1de2403baff4e0b035fc7daa623bfda",
    "semantic_title": "phonetic and prosody-aware self-supervised learning approach for non-native fluency scoring",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23_interspeech.html": {
    "title": "Disentangling the Contribution of Non-native Speech in Automated Pronunciation Assessment",
    "volume": "main",
    "abstract": "This study explores the impact of using non-native speech data in acoustic model training for pronunciation assessment systems. The goal is to determine how introducing non-native data in acoustic model training can influence alignment accuracy and assessment performance. Acoustic models are trained using different combinations of native and non-native speech data, and the Goodness of Pronunciation (GOP) metric is used to evaluate performance. Results show that models trained with manually labeled non-native data yield the highest assessment performance and alignment accuracy. Models trained with mixed non-native and native data perform best when considering the GOP distribution on both non-native and native speech. Additionally, models trained with native data are more robust to alignment variations. These findings highlight the importance of carefully selecting and incorporating non-native data in acoustic model training for pronunciation assessment systems",
    "checked": true,
    "id": "d107c6611d5c705d0c2f1924b8b03166fadf4f5b",
    "semantic_title": "disentangling the contribution of non-native speech in automated pronunciation assessment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ryu23_interspeech.html": {
    "title": "A Joint Model for Pronunciation Assessment and Mispronunciation Detection and Diagnosis with Multi-task Learning",
    "volume": "main",
    "abstract": "Empirical studies report a strong correlation between pronunciation proficiency scores and phonetic errors in non-native speech assessments of human evaluators. However, the existing system of computer-assisted pronunciation training (CAPT) regards automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD) as independent and focuses on individual performance improvement. Motivated by the correlation between two tasks, we propose a novel architecture that jointly tackles APA and MDD using CTC and cross-entropy criteria with a multi-task learning scheme to benefit both tasks. To leverage additional knowledge transfer, Wav2Vec2-robust finetuned on TIMIT is used for the joint optimization. The integrated model significantly outperforms single-task learning, with a mean of 0.057 PCC increase for APA and 0.004 F1 increase for MDD on Speechocean762, which reveals that proficiency scores and phonetic errors are correlated for both human and model assessments",
    "checked": true,
    "id": "64a86c00541526821c4f8cf650691d1ae36879dd",
    "semantic_title": "a joint model for pronunciation assessment and mispronunciation detection and diagnosis with multi-task learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23d_interspeech.html": {
    "title": "Assessing Intelligibility in Non-native Speech: Comparing Measures Obtained at Different Levels",
    "volume": "main",
    "abstract": "Speech intelligibility (SI) is essential in communication and second language learning. In this study, non-native SI was measured through Visual Analogue Scale (VAS) scores and Orthographic Transcriptions (OTs) of read aloud sentences. Seven measures automatically derived from the OTs at word and subword levels were studied. The reliability of the intelligibility measures and the correlations between VAS scores and OT-based measures were also explored. Despite the different speaker language backgrounds, the recruited raters exhibited high scoring reliability. The correlations between VAS scores and OT-based measures were weak, corroborating previous assumptions that they refer to two related but distinct notions, comprehensibility (VAS) and intelligibility (OT). OT-based measures are reliable and valid indicators of SI. The results are discussed in relation to previous studies and avenues for future research are proposed",
    "checked": true,
    "id": "8de6875bdf6b1090d4e3f74ca1aee1f5ba47fd07",
    "semantic_title": "assessing intelligibility in non-native speech: comparing measures obtained at different levels",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23_interspeech.html": {
    "title": "End-to-End Word-Level Pronunciation Assessment with MASK Pre-training",
    "volume": "main",
    "abstract": "Pronunciation assessment is a major challenge in the computer-aided pronunciation training system, especially at the word (phoneme)-level. To obtain word (phoneme)-level scores, current methods usually rely on aligning components to obtain acoustic features of each word (phoneme), which limits the performance of assessment to the accuracy of alignments. Therefore, to address this problem, we propose a simple yet effective method, namely Masked pre-training for Pronunciation Assessment (MPA). Specifically, by incorporating a mask-predict strategy, our MPA supports end-to-end training without leveraging any aligning components and can solve misalignment issues to a large extent during prediction. Furthermore, we design two evaluation strategies to enable our model to conduct assessments in both unsupervised and supervised settings. Experimental results on SpeechOcean762 dataset demonstrate that MPA could achieve better performance than previous methods, without any explicit alignment. In spite of this, MPA still has some limitations, such as requiring more inference time and reference text. They expect to be addressed in future work",
    "checked": true,
    "id": "9c90d47bc4e4cdc53e409d810ed96882865f9c15",
    "semantic_title": "end-to-end word-level pronunciation assessment with mask pre-training",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chao23_interspeech.html": {
    "title": "A Hierarchical Context-aware Modeling Approach for Multi-aspect and Multi-granular Pronunciation Assessment",
    "volume": "main",
    "abstract": "Automatic Pronunciation Assessment (APA) plays a vital role in Computer-assisted Pronunciation Training (CAPT) when evaluating a second language (L2) learner's speaking proficiency. However, an apparent downside of most de facto methods is that they parallelize the modeling process throughout different speech granularities without accounting for the hierarchical and local contextual relationships among them. In light of this, a novel hierarchical approach is proposed in this paper for multi-aspect and multi-granular APA. Specifically, we first introduce the notion of sup-phonemes to explore more subtle semantic traits of L2 speakers. Second, a depth-wise separable convolution layer is exploited to better encapsulate the local context cues at the sub-word level. Finally, we use a score-restraint attention pooling mechanism to predict the sentence-level scores and optimize the component models with a multitask learning (MTL) framework. Extensive experiments carried out on a publicly-available benchmark dataset, viz. speechocean762, demonstrate the efficacy of our approach in relation to some cutting-edge baselines",
    "checked": true,
    "id": "6fc89eb8f2f8db50aff10ccdd2f2f5480e98e75e",
    "semantic_title": "a hierarchical context-aware modeling approach for multi-aspect and multi-granular pronunciation assessment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23j_interspeech.html": {
    "title": "Automatic Prediction of Language Learners' Listenability Using Speech and Text Features Extracted from Listening Drills",
    "volume": "main",
    "abstract": "When language learners are listening to L2 speech, they experience listening disfluencies or breakdowns not rarely. Although listening disfluencies are mental phenomena, previous studies showed that they can be measured acoustically by asking the learners to shadow the L2 speech, where inarticulate productions in shadowing are reasonably attributed to listening disfluencies. In this paper, we model the measured listening disfluencies by BLSTM and attempt to predict which words in new listening drills are difficult to perceive correctly. Taking some studies in psycholinguistics and applied linguistics into account, which revealed what kind of factors influence human perception of spoken words, speech and text features are extracted from listening drills and used for prediction. Experiments show that our model shows a better performance than other models previously proposed and that learners' factors are very effective for prediction because learners are developing through training",
    "checked": true,
    "id": "921014073a118aa8383b3101e2c4bde5fa0911c1",
    "semantic_title": "automatic prediction of language learners' listenability using speech and text features extracted from listening drills",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shekar23b_interspeech.html": {
    "title": "Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-level Goodness of Pronunciation Transformer",
    "volume": "main",
    "abstract": "Automatic pronunciation assessment (APA) plays an important role in providing feedback for self-directed language learners in computer-assisted pronunciation training (CAPT). Several mispronunciation detection and diagnosis (MDD) systems have achieved promising performance based on end-to-end phoneme recognition. However, assessing the intelligibility of second language (L2) remains a challenging problem. One issue is the lack of large-scale labeled speech data from non-native speakers. Additionally, relying only on one aspect (e.g., accuracy) at a phonetic level may not provide a sufficient assessment of pronunciation quality and L2 intelligibility. It is possible to leverage segmental/phonetic-level features such as goodness of pronunciation (GOP), however, feature granularity may cause a discrepancy in prosodic-level (suprasegmental) pronunciation assessment. In this study, Wav2vec 2.0-based MDD and Goodness Of Pronunciation feature-based Transformer are employed to characterize L2 intelligibility. Here, an L2 speech dataset, with human-annotated prosodic (suprasegmental) labels, is used for multi-granular and multi-aspect pronunciation assessment and identification of factors important for intelligibility in L2 English speech. The study provides a transformative comparative assessment of automated pronunciation scores versus the relationship between suprasegmental features and listener perceptions, which taken collectively can help support the development of instantaneous assessment tools and solutions for L2 learners",
    "checked": true,
    "id": "5028fa83522d5df0e7080f537b6b740bfc8f3718",
    "semantic_title": "assessment of non-native speech intelligibility using wav2vec2-based mispronunciation detection and multi-level goodness of pronunciation transformer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23f_interspeech.html": {
    "title": "Adapting an Unadaptable ASR System",
    "volume": "main",
    "abstract": "As speech recognition model sizes and training data requirements grow, it is increasingly common for systems to only be available via APIs from online service providers rather than having direct access to models themselves. In this scenario it is challenging to adapt systems to a specific target domain. To address this problem we consider the recently released OpenAI Whisper ASR as an example of a large-scale ASR system to assess adaptation methods. An error correction based approach is adopted, as this does not require access to the model, but can be trained from either 1-best or N-best outputs that are normally available via the ASR API. LibriSpeech is used as the primary target domain for adaptation. The generalization ability of the system in two distinct dimensions are then evaluated. First, whether the form of correction model is portable to other speech recognition domains, and secondly whether it can be used for ASR models having a different architecture",
    "checked": true,
    "id": "76f816ad49600bc0d4b5e2f0a8d6d935c994ad37",
    "semantic_title": "adapting an unadaptable asr system",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23c_interspeech.html": {
    "title": "Addressing Cold Start Problem for End-to-end Automatic Speech Scoring",
    "volume": "main",
    "abstract": "Integrating automatic speech scoring/assessment systems has become a critical aspect of second-language speaking education. With self-supervised learning advancements, end-to-end speech scoring approaches have exhibited promising results. However, this study highlights the significant decrease in the performance of speech scoring systems in new question contexts, thereby identifying this as a cold start problem in terms of items. With the finding of cold-start phenomena, this paper seeks to alleviate the problem by following methods: 1) prompt embeddings, 2) question context embeddings using BERT or CLIP models, and 3) choice of the pretrained acoustic model. Experiments are conducted on TOEIC speaking test datasets collected from English-as-a-second-language (ESL) learners rated by professional TOEIC speaking evaluators. The results demonstrate that the proposed framework not only exhibits robustness in a cold-start environment but also outperforms the baselines for known content",
    "checked": true,
    "id": "1da84c2b98eddd89b4c0248d425f315ccfedccbf",
    "semantic_title": "addressing cold start problem for end-to-end automatic speech scoring",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ribeiro23b_interspeech.html": {
    "title": "Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings",
    "volume": "main",
    "abstract": "The Grapheme-to-Phoneme (G2P) task aims to convert orthographic input into a discrete phonetic representation. G2P conversion is beneficial to various speech processing applications, such as text-to-speech and speech recognition. However, these tend to rely on manually-annotated pronunciation dictionaries, which are often time-consuming and costly to acquire. In this paper, we propose a method to improve the G2P conversion task by learning pronunciation examples from audio recordings. Our approach bootstraps a G2P with a small set of annotated examples. The G2P model is used to train a multilingual phone recognition system, which then decodes speech recordings with a phonetic representation. Given hypothesized phoneme labels, we learn pronunciation dictionaries for out-of-vocabulary words, and we use those to re-train the G2P system. Results indicate that our approach consistently improves the phone error rate of G2P systems across languages and amount of available data",
    "checked": true,
    "id": "a67dc585d180ec3a646d01799293dc34e3e05c18",
    "semantic_title": "improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/richter23b_interspeech.html": {
    "title": "Orthography-based Pronunciation Scoring for Better CAPT Feedback",
    "volume": "main",
    "abstract": "We establish the viability of a streamlined architecture for pedagogically appropriate computer assisted pronunciation training (CAPT), to give second language learners automatic feedback about their mispronunciations. This takes advantage of end-to-end speech recognition models to detect mispronunciation in audio segments that correspond directly to orthographic letters, in contrast to standard mispronunciation detection using phone representations. Results in a classification task show the potential for similar sensitivity to non-nativelike phonetic errors in grapheme-aligned segments as in phone-aligned segments. Advantages of this approach over phone-based pronunciation scoring can include providing naturally comprehensible (orthographic, not phonemic) feedback to learners, being inherently open-vocabulary in the target language, and evaluating pronunciations with reference to a full range of target-language acoustic variants rather than a prespecified canonical phone sequence",
    "checked": true,
    "id": "156d0b46faf275d83cb82e802611d7cde4dc0007",
    "semantic_title": "orthography-based pronunciation scoring for better capt feedback",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23r_interspeech.html": {
    "title": "Zero-Shot Automatic Pronunciation Assessment",
    "volume": "main",
    "abstract": "Automatic Pronunciation Assessment (APA) is vital for computer-assisted language learning. Prior methods rely on annotated speech-text data to train Automatic Speech Recognition (ASR) models or speech-score data to train regression models. In this work, we propose a novel zero-shot APA method based on the pre-trained acoustic model, HuBERT. Our method involves encoding speech input and corrupting them via a masking module. We then employ the Transformer encoder and apply k-means clustering to obtain token sequences. Finally, a scoring module is designed to measure the number of wrongly recovered tokens. Experimental results on speechocean762 demonstrate that the proposed method achieves comparable performance to supervised regression baselines and outperforms non-regression baselines in terms of Pearson Correlation Coefficient (PCC). Additionally, we analyze how masking strategies affect the performance of APA",
    "checked": true,
    "id": "0540a9c3710d68c880463e4f55ddeccd7f4cac35",
    "semantic_title": "zero-shot automatic pronunciation assessment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huu23_interspeech.html": {
    "title": "Mispronunciation detection and diagnosis model for tonal language, applied to Vietnamese",
    "volume": "main",
    "abstract": "A tonal language is a language in which the meaning of words is not only determined by the sounds of the consonants and vowels, but also by the pitch or tone used to pronounce them. Mispronunciation Detection and Diagnosis (MD&D) of tonal languages is challenging since tone presentation is difficult to be detected correctly. There has been relatively little research conducted on tonal languages, with most focusing on Mandarin. Furthermore, there are no publicly available datasets and source codes for the task. This work constructs and publishes a Vietnamese dataset for experimenting with MD&D, as well as proposes an end-to-end model that utilizes pitch analysis to detect and diagnose mispronunciations for tonal languages, especially focusing on Vietnamese. Experiments show that the proposed model achieved a relative improvement in phone error rate of 7.1% and detection accuracy of 7.4% compared to a state-of-the-art baseline",
    "checked": true,
    "id": "64cde42cf0e4b3dcda9ab7a0a4ecb552aa96ee00",
    "semantic_title": "mispronunciation detection and diagnosis model for tonal language, applied to vietnamese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dignum23_interspeech.html": {
    "title": "Beyond the AI hype: Balancing Innovation and Social Responsibility",
    "volume": "main",
    "abstract": "AI can extend human capabilities but requires addressing challenges in education, jobs, and biases. Taking a responsible approach involves understanding AI's nature, design choices, societal role, and ethical considerations. Recent AI developments, including foundational models, transformer models, generative models, and large language models (LLMs), raise questions about whether they are changing the paradigm of AI, and about the responsibility of those that are developing and deploying AI systems. In all these developments, is vital to understand that AI is not an autonomous entity but rather dependent on human responsibility and decision-making In this talk, I will further discuss the need for a responsible approach to AI that emphasize trust, cooperation, and the common good. Taking responsibility involves regulation, governance, and awareness. Ethics and dilemmas are ongoing considerations, but require understanding that trade-offs must be made and that decision processes are always contextual. Taking responsibility requires designing AI systems with values in mind, implementing regulations, governance, monitoring, agreements, and norms. Rather than viewing regulation as a constraint, it should be seen as a stepping stone for innovation, ensuring public acceptance, driving transformation, and promoting business differentiation. Responsible Artificial Intelligence (AI) is not an option but the only possible way to go forward in AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/stemmer23_interspeech.html": {
    "title": "Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach",
    "volume": "main",
    "abstract": "Speech emotion recognition for natural human-to-human conversations has many useful applications, including generating comprehensive meeting transcripts or detecting communication problems. We investigate the detection of emotional hotspots, i.e., regions of increased speaker involvement in technical meetings. As there is a scarcity of annotated, not-acted corpora, and to avoid introducing unwanted biases to our models, we follow a cross-corpus approach where models are trained on data from domains unrelated to the test data. In this work we propose a model ensemble trained on spontaneous phone conversations, political discussions and acted emotions. Evaluation is performed on the natural ICSI and AMI meeting corpora, where we used existing hotspot annotations for ICSI and created labels for the AMI corpus. A semi-supervised fine-tuning procedure is introduced to adapt the model. We show that an equal error rate of below 21% can be achieved using the proposed cross-corpus approach",
    "checked": true,
    "id": "c69cd5ba401a20261bbb9dd301c12e58aef7b3d7",
    "semantic_title": "detection of emotional hotspots in meetings using a cross-corpus approach",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matsuda23_interspeech.html": {
    "title": "Detection of Laughter and Screaming Using the Attention and CTC Models",
    "volume": "main",
    "abstract": "This study aimed to detect social signals, such as laughter and screams, in real environments. Social signals influence human-to-human communication. To effectively apply these signals in various systems, computer systems must appropriately detect social signals. In this study, social signal detection (SSD) experiments were conducted to demonstrate which of three feature sets, i.e., a spectral feature set, prosodic feature set, and spectral and prosodic feature set, was best for detecting laughter and screaming. The results showed that using both the spectral and prosodic feature sets yielded the best performance, with 81.83% accuracy for laughter and 81.68% accuracy for screams. Moreover, the detection model comparison results revealed that the bidirectional long short-term memory (BiLSTM)-connectionist temporal classification (CTC) yielded the best laughter detection performance, while attention-CTC was best for scream detection. These results suggest that CTC is effective for SSD",
    "checked": true,
    "id": "42671c0d5013b123fd5dc43e9d7e45d8fb8b6a7a",
    "semantic_title": "detection of laughter and screaming using the attention and ctc models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhattacharya23_interspeech.html": {
    "title": "Capturing Formality in Speech Across Domains and Languages",
    "volume": "main",
    "abstract": "The linguistic notion of formality is one dimension of stylistic variation in human communication. A universal characteristic of language production, formality has surface-level realizations in written and spoken language. In this work, we explore ways of measuring the formality of such realizations in multilingual speech corpora across a wide range of domains. We compare measures of formality, contrasting textual and acoustic-prosodic metrics. We believe that a combination of these should correlate well with downstream applications. Our findings include: an indication that certain prosodic variables might play a stronger role than others; no correlation between prosodic and textual measures; limited evidence for anticipated inter-domain trends, but some evidence of consistency of measures between languages. We conclude that non-lexical indicators of formality in speech may be more subtle than our initial expectations, motivating further work on reliably encoding spoken formality",
    "checked": true,
    "id": "36ca450788efeee276b1be7c1ababe05a32bc8e2",
    "semantic_title": "capturing formality in speech across domains and languages",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23e_interspeech.html": {
    "title": "Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio",
    "volume": "main",
    "abstract": "To perform automatic family audio analysis, past studies have collected recordings using phone, video, or audio-only recording device like LENA, investigated supervised learning methods, and used or fine-tuned general-purpose embeddings learned from large pretrained models. In this study, we advance the audio component of a new infant wearable multi-modal device called LittleBeats (LB) by learning family audio representation via wav2vec 2.0 (W2V2) pretraining. We show given a limited number of labeled LB home recordings, W2V2 pretrained using 1k-hour of unlabeled home recordings outperforms oracle W2V2 pretrained on 52k-hour unlabeled audio in terms of parent/infant speaker diarization (SD) and vocalization classifications (VC) at home. Extra relevant external unlabeled and labeled data further benefit W2V2 pretraining and fine-tuning. With SpecAug and environmental speech corruptions, we obtain 12% relative gain on SD and moderate boost on VC. Code and model weights are available",
    "checked": true,
    "id": "62a233ec8d3f5e9e64b725c99892cf7d42070839",
    "semantic_title": "towards robust family-infant audio analysis based on unsupervised pretraining of wav2vec 2.0 on large-scale unlabeled family audio",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feindt23_interspeech.html": {
    "title": "Cues to next-speaker projection in conversational Swedish: Evidence from reaction times",
    "volume": "main",
    "abstract": "We present first results of a study investigating the salience and typicality of prosodic markers in Swedish at turn ends for turn-yielding and turn-keeping purposes. We performed an experiment where participants (N=32) were presented with conversational chunks and, after the audio ended, were asked to determine which of two speakers would speak next by clicking a picture on a screen. Audio stimuli were manipulated by (i) raising and (ii) lowering fo over the last 500 ms of a turn, (iii) speeding up or (iv) slowing down duration over the last 500 ms, and (v) raising and (vi) lowering the last pitch peak. Out of all manipulations, increasing the speech rate was found to be the most disruptive p<.005). Higher speech rate led to longer reaction times in turn-keeping, which were shorter in turn-yielding. Other manipulations did not significantly alter reaction times. Results may be complemented with eye movement data, to elucidate cognitive mechanisms underlying turn-taking behavior",
    "checked": true,
    "id": "810fc956a506312d3690678525013e9be3bb7875",
    "semantic_title": "cues to next-speaker projection in conversational swedish: evidence from reaction times",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/buker23_interspeech.html": {
    "title": "Multiple Instance Learning for Inference of Child Attachment From Paralinguistic Aspects of Speech",
    "volume": "main",
    "abstract": "Attachment is a psychological construct that accounts for the way children perceive their relationship with their caregivers. Depending on the attachment condition, a child can either be secure or insecure. Identifying as many insecure children as possible is important to mitigate the negative consequences of insecure attachment in adult life. For this reason, this article proposes an attachment recognition approach that, compared to other approaches, increases the Recall, the percentage of insecure children identified as such. The approach is based on Multiple Instance Learning, a body of methodologies dealing with data represented as \"bags\" of feature vectors. This is suitable for speech recordings because these are typically represented as vector sequences. The experiments involved 104 participants of age 5 to 9. The results show that insecure children can be identified with Recall up to 63.3% (accuracy up to 75%), an improvement with respect to most existing models",
    "checked": true,
    "id": "20b135d33966d60a5fb042cee8cf34b7087b9822",
    "semantic_title": "multiple instance learning for inference of child attachment from paralinguistic aspects of speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eskimez23_interspeech.html": {
    "title": "Real-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Personalized speech enhancement (PSE) is a real-time SE approach utilizing a speaker embedding of a target person to remove background noise, reverberation, and interfering voices. To deploy a PSE model for full duplex communications, the model must be combined with acoustic echo cancellation (AEC), although such a combination has been less explored. This paper proposes a series of methods that are applicable to various model architectures to develop efficient causal models that can handle the tasks of PSE, AEC, and joint PSE-AEC. We present extensive evaluation results using both simulated data and real recordings, covering various acoustic conditions and evaluation metrics. The results show the effectiveness of the proposed methods for two different model architectures. Our best joint PSE-AEC model comes close to the expert models optimized for individual tasks of PSE and AEC in their respective scenarios and significantly outperforms the expert models for the combined PSE-AEC task",
    "checked": true,
    "id": "fbfb5091ebbad691125d22ab76d2785bb0385b85",
    "semantic_title": "real-time joint personalized speech enhancement and acoustic echo cancellation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23g_interspeech.html": {
    "title": "TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective",
    "volume": "main",
    "abstract": "Despite the promising performance of existing frame-wise all-neural beamformers in the speech enhancement field, it remains unclear what the underlying mechanism exists. In this paper, we revisit the beamforming behavior from the beam-space dictionary perspective and formulate it into the learning and mixing of different beam-space components. Based on that, we propose an all-neural beamformer called TaylorBM to simulate Taylor's series expansion operation in which the 0th-order term serves as a spatial filter to conduct the beam mixing, and several high-order terms are tasked with residual noise cancellation for post-processing. The whole system is devised to work in an end-to-end manner. Experiments are conducted on the spatialized LibriSpeech corpus and results show that the proposed approach outperforms existing advanced baselines in terms of evaluation metrics",
    "checked": true,
    "id": "74cf3364ccbfb60dbceffe5ba2e61429c6ceda45",
    "semantic_title": "taylorbeamixer: learning taylor-inspired all-neural multi-channel speech enhancement from beam-space dictionary perspective",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23s_interspeech.html": {
    "title": "MFT-CRN:Multi-scale Fourier Transform for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Convolutional recurrent networks (CRN) that combine a convolutional encoder-decoder (CED) structure with a recurrent structure have shown promising results in monaural speech enhancement. However, the commonly used short-time Fourier transform fails to balance the needs of frequency and time resolution effectively, which is crucial for accurate speech estimation. To address this issue, we propose MFT-CRN, a multi-scale short-time Fourier transform fusion model. We process the input speech signal through short-time Fourier transforms with different window functions, and add them layer by layer in the encoder and decoder of the network to achieve feature fusion with different window functions, effectively balancing frequency and temporal resolution. Comprehensive experiments on the WSJ0 dataset show that MFT-CRN significantly outperforms the method using only a single window function in terms of short-time intelligibility and perceptual evaluation of speech quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/guo23_interspeech.html": {
    "title": "Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement",
    "volume": "main",
    "abstract": "The goal of this study is to implement diffusion models for speech enhancement (SE). The first step is to emphasize the theoretical foundation of variance-preserving (VP)-based interpolation diffusion under continuous conditions. Subsequently, we present a more concise framework that encapsulates both the VP- and variance-exploding (VE)-based interpolation diffusion methods. We demonstrate that these two methods are special cases of the proposed framework. Additionally, we provide a practical example of VP-based interpolation diffusion for the SE task. To improve performance and ease model training, we analyze the common difficulties encountered in diffusion models and suggest amenable hyper-parameters. Finally, we evaluate our model against several methods using a public benchmark to showcase the effectiveness of our approach",
    "checked": true,
    "id": "84e6f4d2d30e289ae0131dba82b696836f1d0724",
    "semantic_title": "variance-preserving-based interpolation diffusion models for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/taherian23_interspeech.html": {
    "title": "Multi-input Multi-output Complex Spectral Mapping for Speaker Separation",
    "volume": "main",
    "abstract": "Current deep learning based multi-channel speaker separation methods produce a monaural estimate of speaker signals captured by a reference microphone. This work presents a new multi-channel complex spectral mapping approach that simultaneously estimates the real and imaginary spectrograms of all speakers at all microphones. The proposed multi-input multi-output (MIMO) separation model uses a location-based training (LBT) criterion to resolve the permutation ambiguity in talker-independent speaker separation across microphones. Experimental results show that the proposed MIMO separation model outperforms a multi-input single-output (MISO) speaker separation model with monaural estimates. We also combine the MIMO separation model with a beamformer and a MISO speech enhancement model to further improve separation performance. The proposed approach achieves the state-of-the-art speaker separation on the open LibriCSS dataset",
    "checked": true,
    "id": "fce2ec66047942842e5e92ac767f17360929c1c4",
    "semantic_title": "multi-input multi-output complex spectral mapping for speaker separation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oberhag23_interspeech.html": {
    "title": "Short-term Extrapolation of Speech Signals Using Recursive Neural Networks in the STFT Domain",
    "volume": "main",
    "abstract": "This paper investigates several approaches for the short-term extrapolation of speech signals. The signal extrapolation methods are embedded into a nested two-stage spectral analysis-synthesis system for single-channel noise reduction in hearing aids. They predict additional signal samples in the low-frequency sub-bands of the first analysis stage and may compensate the additional algorithmic latency of the second, higher-resolution analysis stage in these bands. We thus achieve a higher spectral resolution in frequency bands below 3 kHz without increasing the algorithmic latency of the overall system. In the context of noise reduction, especially female voices benefit from the increased spectral resolution in the lower sub-bands of the first stage. We show that among the investigated approaches, both recursive neural-network-based extrapolation methods provide benefits in conjunction with a noise reduction algorithm and outperform our baseline linear extrapolation method",
    "checked": true,
    "id": "e33774540f41f85d22584c5e401ebddb1a419622",
    "semantic_title": "short-term extrapolation of speech signals using recursive neural networks in the stft domain",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pandey23_interspeech.html": {
    "title": "Listener sensitivity to deviating obstruents in WaveNet",
    "volume": "main",
    "abstract": "This paper investigates the perceptual significance of the deviation in obstruents previously observed in WaveNet vocoders. The study involved presenting stimuli of varying lengths to 128 participants, who were asked to identify whether each stimulus was produced by a human or a machine. The participants' responses were captured using a 2-alternative forced choice task. The study found that while the length of the stimuli did not reliably affect participants' accuracy in the task, the concentration of obstruents did have a significant effect. Participants were consistently more accurate in identifying WaveNet stimuli as machine when the phrases were obstruent-rich. These findings show that the deviation in obstruents reported in WaveNet voices is perceivable by human listeners. The test protocol may be of wider utility in TTS",
    "checked": true,
    "id": "4a1bc694b2f7eb7b15bb885e7d6b8b124cfee36a",
    "semantic_title": "listener sensitivity to deviating obstruents in wavenet",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23d_interspeech.html": {
    "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
    "volume": "main",
    "abstract": "We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech",
    "checked": true,
    "id": "f70d0bb5685b8a119fcaa91c7e9b29c10cf403b6",
    "semantic_title": "how generative spoken language modeling encodes noisy speech: investigation from phonetics to syntactics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/camp23_interspeech.html": {
    "title": "MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors",
    "volume": "main",
    "abstract": "The quality of synthetic speech is typically evaluated using subjective listening tests. An underlying assumption is that these tests are reliable, i.e., running the test multiple times gives consistent results. A common approach to study reliability is a replication study. Existing studies focus primarily on Mean Opinion Score (MOS), and few consider the error bounds from the original test. In contrast, we present a replication study of both MOS and AB preference tests to answer two questions: (1) which of the two test types is more reliable for system comparison, and (2) for both test types, how reliable are the results with respect to their estimated standard error? We find that while AB tests are more reliable for system comparison, standard errors are underestimated for both test types. We show that these underestimates are partially due to broken independence assumptions, and suggest alternate methods of standard error estimation that account for dependencies among ratings",
    "checked": true,
    "id": "504bdd978507d86c3f27989479c7efe24b272e59",
    "semantic_title": "mos vs. ab: evaluating text-to-speech systems reliably using clustered standard errors",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23r_interspeech.html": {
    "title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting",
    "volume": "main",
    "abstract": "Automatic Mean Opinion Score (MOS) prediction is crucial to evaluate the perceptual quality of the synthetic speech. While recent approaches using pre-trained self-supervised learning (SSL) models have shown promising results, they only partly address the data scarcity issue for the feature extractor. This leaves the data scarcity issue for the decoder unresolved and leading to suboptimal performance. To address this challenge, we propose a retrieval-augmented MOS prediction method, dubbed RAMP, to enhance the decoder's ability against the data scarcity issue. A fusing network is also proposed to dynamically adjust the retrieval scope for each instance and the fusion weights based on the predictive confidence. Experimental results show that our proposed method outperforms the existing methods in multiple scenarios",
    "checked": true,
    "id": "6f6393908a5dd3059182da815876e1d5a7a7acfe",
    "semantic_title": "ramp: retrieval-augmented mos prediction via confidence-based dynamic weighting",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/melnikleroy23_interspeech.html": {
    "title": "Can Better Perception Become a Disadvantage? Synthetic Speech Perception in Congenitally Blind Users",
    "volume": "main",
    "abstract": "Modern Text-To-Speech systems are rarely tested on non-standard user groups, such as people with impairments. Nevertheless, evidence suggests that some of these groups might perceive synthetic speech differently (better or worse) than regular users. The current study investigated for the first time how synthetic speech is perceived by blind vs. sighted users. For this purpose, we used a speeded AX discrimination task and tested how sighted and blind listeners perceive synthetic speech of different qualities. Results show that blind participants had significantly better discrimination on this task, and both groups performed worse when the perceptual differences in the synthetic speech were smaller. This suggests that blind participants were indeed more sensitive to the acoustic characteristics of synthetic speech compared to their sighted peers. We discuss implications for speech perception and the development of modern speech technologies",
    "checked": true,
    "id": "0ea5ab42fe6c3d3cfc2409436bf8ea935b3a288e",
    "semantic_title": "can better perception become a disadvantage? synthetic speech perception in congenitally blind users",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cooper23_interspeech.html": {
    "title": "Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech",
    "volume": "main",
    "abstract": "Mean Opinion Score (MOS) is a popular measure for evaluating synthesized speech. However, the scores obtained in MOS tests are heavily dependent upon many contextual factors. One such factor is the overall range of quality of the samples presented in the test -- listeners tend to try to use the entire range of scoring options available to them regardless of this, a phenomenon which is known as range-equalizing bias. In this paper, we systematically investigate the effects of range-equalizing bias on MOS tests for synthesized speech by conducting a series of listening tests in which we progressively \"zoom in\" on a smaller number of systems in the higher-quality range. This allows us to better understand and quantify the effects of range-equalizing bias in MOS tests",
    "checked": true,
    "id": "0eaa565c3745308f67993e8fa8e2d9c402e009d4",
    "semantic_title": "investigating range-equalizing bias in mean opinion score ratings of synthesized speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/he23_interspeech.html": {
    "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU",
    "checked": true,
    "id": "daf9e24adbba3d1aead91cbac26502d3043db069",
    "semantic_title": "can chatgpt detect intent? evaluating large language models for spoken language understanding",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rajaa23_interspeech.html": {
    "title": "Improving End-to-End SLU performance with Prosodic Attention and Distillation",
    "volume": "main",
    "abstract": "Most End-to-End SLU methods depend on the pretrained ASR or language model features for intent prediction. However, other essential information in speech, such as prosody, is often ignored. Recent research has shown improved results in classifying dialogue acts by incorporating prosodic information. The margins of improvement in these methods are minimal as the neural models ignore prosodic features. In this work, we propose prosody-attention, which uses the prosodic features differently to generate attention maps across time frames of the utterance. Then we propose prosody-distillation to explicitly learn the prosodic information in the acoustic encoder rather than concatenating the implicit prosodic features. Both the proposed methods improve the baseline results, and the prosody-distillation method gives an intent classification accuracy improvement of 8% and 2% on SLURP and STOP datasets over the prosody baseline",
    "checked": true,
    "id": "2735ddc5618b54d5ce717adf3752921a4de511ce",
    "semantic_title": "improving end-to-end slu performance with prosodic attention and distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23n_interspeech.html": {
    "title": "Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "3c5470149173a6343346694d8da09a09c33c7d49",
    "semantic_title": "modality confidence aware training for robust end-to-end spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23c_interspeech.html": {
    "title": "Cross-Modal Semantic Alignment before Fusion for Two-Pass End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "The deliberation-based two-pass model that combines both semantic and acoustic information can effectively improve the performance of end-to-end (E2E) spoken language understanding (SLU). However, existing two-pass models usually simply fuse speech embedding and text embedding without taking into account the inherent distinctions between these two modalities. We propose a novel approach named Cross-modal Semantic Alignment before Fusion (CSAF), which adopt contrastive loss aligning speech and text embeddings before fusing them. We introduce a shared semantic memory transformer to project the embeddings from two modalities into a common semantic space, and a multi-modal gated network to generate the fused embeddings. We conduct experiments on the FSC Challenge test set and SLURP dataset. The results demonstrate that our method can significantly promote intent classification accuracy, achieving an absolute improvement of 3.1% over previous works in the FSC Challenge Utterance Set",
    "checked": true,
    "id": "426965f0e3b3dc24a55821915523b6032b2ad01a",
    "semantic_title": "cross-modal semantic alignment before fusion for two-pass end-to-end spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sunder23_interspeech.html": {
    "title": "ConvKT: Conversation-Level Knowledge Transfer for Context Aware End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "Dialog history enhances downstream classification performance in both speech and text based dialog systems. However, there still exists a gap in dialog history integration in a fully end-to-end (E2E) spoken dialog system (SDS) versus a textual dialog system. Text-based dialog systems use large language models (LLMs) to encode long-range dependencies by attending to the entire conversation as a contiguous token sequence. This is not possible in an E2E SDS, as speech sequences can be intractably long. We propose a convolution subsampling approach to make the speech sequence of a conversation tractable and use a conformer to attend to the speech-based conversation in a fine-grained manner. This model is further enhanced via a conversation-level knowledge transfer from a LLM using a token-level alignment strategy. Finetuning the E2E model pretrained this way gives significant gains, of up to 8%, over strong non-contextual baselines in the E2E dialog act classification task on two datasets",
    "checked": true,
    "id": "0d388b5c4a617792d62655af6b8ee1f71dab09f4",
    "semantic_title": "convkt: conversation-level knowledge transfer for context aware end-to-end spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23_interspeech.html": {
    "title": "GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering",
    "volume": "main",
    "abstract": "Spoken question answering (SQA) aims to identify the correct answer to the given the question from a spoken passage. Most conventional SQA frameworks combine an automatic speech recognition (ASR) module and a text question answering (TQA) module in a cascaded manner, which might suffer from error propagation and high latency. To tackle these issues, several end-to-end SQA frameworks based on Textless NLP are proposed. However, existing end-to-end models still fail to outperform the cascade models with the similar number of parameters. In this paper, to improve textless SQA, we propose GhostT5, which generates more features from the remaining features with very cheap operations for stronger performance. Experiment results and further analysis show that our GhostT5 achieves the new state-of-the-art performance on NMSQA dataset and surpasses cascaded SQA models. More encouragingly, GhostT5 surpasses the previous best end-to-end SQA model with less than half of the parameters",
    "checked": true,
    "id": "dc0436318a08f4df8f9653f164a830f245caca8b",
    "semantic_title": "ghostt5: generate more features with cheap operations to improve textless spoken question answering",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23b_interspeech.html": {
    "title": "Obstructive Sleep Apnea Detection using Pre-trained Speech Representations",
    "volume": "main",
    "abstract": "Obstructive sleep apnea (OSA) is a condition commonly affecting middle-aged men that can disturb sleep, cause daytime tiredness, and increase the risk of heart disease. Speech can serve as a valuable biomarker for identifying and predicting the severity of OSA due to its connection with changes in throat structure. This study proposes a new deep-learning-based method for detecting OSA by analyzing speech recordings of participants in sitting and lying positions. The method utilizes a Siamese structure that employs a pre-trained XLSR model to encode ten utterances for each position, reducing the amount of necessary training data and enabling comparison of throat structure changes between the two positions through voice analysis. The study also explores the use of patient characteristic features. Results show this approach achieves an F1 value of 0.725 on our in-house dataset, proving the feasibility of end-to-end speech OSA detection with foundation models",
    "checked": true,
    "id": "0dc21b1086d042d696930e17b984b4327e4f12d0",
    "semantic_title": "obstructive sleep apnea detection using pre-trained speech representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23f_interspeech.html": {
    "title": "EEG-based Auditory Attention Detection with Spatiotemporal Graph and Graph Convolutional Network",
    "volume": "main",
    "abstract": "The ability to detect auditory attention from electroencephalography (EEG) offers many possibilities for brain-computer interface (BCI) applications, such as hearing assistive devices. However, effective feature representation for EEG signals remains a challenge due to the complex spatial and temporal dynamics of EEG signals. To overcome this challenge, we introduce a Spatiotemporal Graph Convolutional Network (ST-GCN), which combines a temporal attention mechanism and a graph convolutional module. The temporal attention mechanism captures the temporal dynamics of EEG segments, while the graph convolutional module learns the spatial pattern of multi-channel EEG signals. We evaluate the performance of our proposed ST-GCN on two publicly available datasets and demonstrate significant improvements over existing state-of-the-art models. These findings suggest that the ST-GCN model has the potential to advance auditory attention detection in real-life BCI applications",
    "checked": true,
    "id": "7d5bcf3e0a7274326628c5f406942e7ae859a671",
    "semantic_title": "eeg-based auditory attention detection with spatiotemporal graph and graph convolutional network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/beeson23_interspeech.html": {
    "title": "Silent Speech Recognition with Articulator Positions Estimated from Tongue Ultrasound and Lip Video",
    "volume": "main",
    "abstract": "We present a multi-speaker silent speech recognition system trained on articulator features derived from the Tongue and Lips corpus, a multi-speaker corpus of ultrasound tongue imaging and lip video data. We extracted articulator features using the pose estimation software DeepLabCut, then trained recognition models with these point-tracking features using Kaldi. We trained with voiced utterances, then tested performance on both voiced and silent utterances. Our multi-speaker SSR improved WER by 23.06% when compared to a previous similar multi-speaker SSR system which used image-based instead of point-tracking features. We also found great improvements (up to 15.45% decrease in WER) in recognition of silent speech using fMLLR adaptation compared to raw features. Finally, we investigated differences in articulator trajectories between voiced and silent speech and found that speakers tend to miss articulatory targets that are present in voiced speech when speaking silently",
    "checked": true,
    "id": "e1c80da2f7cf1fa7d3db4f8220bc7df0a8ff7041",
    "semantic_title": "silent speech recognition with articulator positions estimated from tongue ultrasound and lip video",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23r_interspeech.html": {
    "title": "Auditory Attention Detection in Real-Life Scenarios Using Common Spatial Patterns from EEG",
    "volume": "main",
    "abstract": "Auditory attention detection (AAD) methods based on electroencephalography (EEG) could be used in neuro-steered hearing devices to help hearing-loss people improve their hearing ability. However, previous studies have mostly obtained EEG data in laboratory settings which limits the practical application of neuro-steered hearing devices. In this study, we employ a common spatial pattern (CSP) algorithm to perform AAD using EEG signals collected by a wireless mobile EEG system, from real-life scenarios when people are walking and sitting. The results show that the CSP method can achieve AAD accuracy between 81.3% and 87.5% when using different decision windows (1 s- 30 s), which is better than previous methods based on linear mapping methods and convolutional neural networks (CNN). This proves that the CSP algorithm can decode people's attention efficiently even outside the laboratory. Analysis of EEG frequency bands shows that the δ and β bands have high activity in attention tasks",
    "checked": true,
    "id": "cbf24317cd7e287298ff2c4acce166adc2b4e35a",
    "semantic_title": "auditory attention detection in real-life scenarios using common spatial patterns from eeg",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23g_interspeech.html": {
    "title": "Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG",
    "volume": "main",
    "abstract": "Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech",
    "checked": true,
    "id": "465e3528f56edc662cbdde43fe9a02758411e5f1",
    "semantic_title": "diff-e: diffusion-based learning for decoding imagined speech eeg",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/csapo23_interspeech.html": {
    "title": "Towards Ultrasound Tongue Image prediction from EEG during speech production",
    "volume": "main",
    "abstract": "Previous initial research has already been carried out to propose speech-based BCI using brain signals (e.g. non-invasive EEG and invasive sEEG / ECoG), but there is a lack of combined methods that investigate non-invasive brain, articulation, and speech signals together and analyze the cognitive processes in the brain, the kinematics of the articulatory movement and the resulting speech signal. In this paper, we describe our multimodal (electroencephalography, ultrasound tongue imaging, and speech) analysis and synthesis experiments, as a feasibility study. We extend the analysis of brain signals recorded during speech production with ultrasound-based articulation data. From the brain signal measured with EEG, we predict ultrasound images of the tongue with a fully connected deep neural network. The results show that there is a weak but noticeable relationship between EEG and ultrasound tongue images, i.e. the network can differentiate articulated speech and neutral tongue position",
    "checked": true,
    "id": "a6987b9e00e23ef1b96d586404097bb3f7a1cc02",
    "semantic_title": "towards ultrasound tongue image prediction from eeg during speech production",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/toth23_interspeech.html": {
    "title": "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks",
    "volume": "main",
    "abstract": "Thanks to the latest deep learning algorithms, silent speech interfaces (SSI) are now able to synthesize intelligible speech from articulatory movement data under certain conditions. However, the resulting models are rather speaker-specific, making a quick switch between users troublesome. Even for the same speaker, these models perform poorly cross-session, i.e. after dismounting and re-mounting the recording equipment. To aid quick speaker and session adaptation of ultrasound tongue imaging-based SSI models, we extend our deep networks with a spatial transformer network (STN) module, capable of performing an affine transformation on the input images. Although the STN part takes up only about 10% of the network, our experiments show that adapting just the STN module might allow to reduce MSE by 88% on the average, compared to retraining the whole network. The improvement is even larger (around 92%) when adapting the network to different recording sessions from the same speaker",
    "checked": true,
    "id": "9cf0034b9b1f4060b3dd171eb7b4289846e12315",
    "semantic_title": "adaptation of tongue ultrasound-based silent speech interfaces using spatial transformer networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/scheck23_interspeech.html": {
    "title": "STE-GAN: Speech-to-Electromyography Signal Conversion using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "With Speech-to-Electromyography Generative Adversarial Network (STE-GAN), we propose a model which can synthesize Electromyography (EMG) signals from acoustic speech. We condition the generator network on representations of the spoken content obtained from a voice conversion model. Given these representations, the generator outputs an EMG signal corresponding to the articulated content of the acoustic speech in the setting of a specific EMG recording session. In comparison to previous work, STE-GAN directly generates EMG signals from acoustic speech. As it uses more speaker-independent content representations as input, it can synthesize EMG signals from speech of speakers who were unseen during training",
    "checked": true,
    "id": "432875ec218e05e1ab77c1b4266d1754b5239945",
    "semantic_title": "ste-gan: speech-to-electromyography signal conversion using generative adversarial networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/salomons23_interspeech.html": {
    "title": "Spanish Phone Confusion Analysis for EMG-Based Silent Speech Interfaces",
    "volume": "main",
    "abstract": "This paper describes a set of phone classification experiments based on electromyography (EMG) signals and a subsequent phone confusion analysis, as part of a project that aims to restore speech for Spanish laryngectomees by developing a Silent Speech Interface (SSI). Understanding the relationship between speech and the muscles used for speaking is essential to learn the possibilities and limitations of such EMG-based SSIs, before advancing to a complex task such as direct EMG-to-speech conversion. When considering only information from the muscles of the face and neck, important information from the tongue and vocal cords is missing. This is reflected in the results, which show confusion between pairs of phones that only differ in the position of the tongue or the voicing feature",
    "checked": true,
    "id": "bac5c07481ad7b769b72370b64f692b7c5ca046c",
    "semantic_title": "spanish phone confusion analysis for emg-based silent speech interfaces",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23l_interspeech.html": {
    "title": "Hybrid Silent Speech Interface Through Fusion of Electroencephalography and Electromyography",
    "volume": "main",
    "abstract": "Silent Speech Interface (SSI) can enable interaction in a new and natural way based on no-audible biosignals generated by the human body. Electroencephalography (EEG) or surface electromyography (sEMG) generated during speech production can be utilized to decode silent speech. However, obtaining complementary information from EEG and sEMG is still challenging. This paper presents a hybrid SSI based on the converter between bimodal electrophysiological signals and audio signals. EEG and sEMG are fused through two sequence-to-sequence models, and multi-task losses are applied to achieve complementarity between speech intention and muscle activity in silent speech. The feasibility of the proposed fusion method is validated in the silent speech dataset, and an average objective character error rate (CER) of 7.22% among eight speakers is obtained. The experimental results show that our bimodal-based hybrid SSI facilitates the conversion of electrophysiological signals to audio",
    "checked": true,
    "id": "474caa51703da1b8d2bb0d89833efbe54d205714",
    "semantic_title": "hybrid silent speech interface through fusion of electroencephalography and electromyography",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarkar23_interspeech.html": {
    "title": "Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) models use only the intrinsic structure of a given signal, independent of its acoustic domain, to extract essential information from the input to an embedding space. This implies that the utility of such representations is not limited to modeling human speech alone. Building on this understanding, this paper explores the cross-transferability of SSL neural representations learned from human speech to analyze bio-acoustic signals. We conduct a caller discrimination analysis and a caller detection study on Marmoset vocalizations using eleven SSL models pre-trained with various pretext tasks. The results show that the embedding spaces carry meaningful caller information and can successfully distinguish the individual identities of Marmoset callers without fine-tuning. This demonstrates that representations pre-trained on human speech can be effectively applied to the bio-acoustics domain, providing valuable insights for future investigations in this field",
    "checked": true,
    "id": "f904b9ca82972a983b661a7d7855f52352d84dd3",
    "semantic_title": "can self-supervised neural representations pre-trained on human speech distinguish animal callers?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cai23b_interspeech.html": {
    "title": "Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data Using Contrastive Learning with Varying Pre-Training Domains",
    "volume": "main",
    "abstract": "Rapid discovery of new diseases, such as COVID-19 can enable a timely epidemic response, preventing the large-scale spread and protecting public health. However, limited research efforts have been taken on this problem. In this paper, we propose a contrastive learning-based modeling approach for COVID-19 coughing and breathing pattern discovery from non-COVID coughs. To validate our models, extensive experiments have been conducted using four large audio datasets and one image dataset. We further explore the effects of different factors, such as domain relevance and augmentation order on the pre-trained models. Our results show that the proposed model can effectively distinguish COVID-19 coughing and breathing from unlabeled data and labeled non-COVID coughs with an accuracy of up to 0.81 and 0.86, respectively. Findings from this work will guide future research to detect an outbreak of a new disease early",
    "checked": true,
    "id": "0f28b2ef2d098bf8c194811afb0bb1280d48394d",
    "semantic_title": "discovering covid-19 coughing and breathing patterns from unlabeled data using contrastive learning with varying pre-training domains",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23_interspeech.html": {
    "title": "Background-aware Modeling for Weakly Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "Nowadays, a common framework for weakly supervised sound event detection (WSSED) is multiple instance learning (MIL). However, MIL directly optimizes the clip-level classification results, so it tends to localize the most distinct part rather than the entire sound event, making the indiscriminating parts of sound events mistakenly identified as background sounds. In this paper, we focus on adding background awareness for WSSED by proposing a learning structure called BA-WSSED. Our BA-WSSED first introduces a pseudo separator with softmax activation and two aggregators to purify and aggregate the event feature and the background feature, respectively. Then, with the help of the proposed background-aware staggered (BAS) loss, both the event classifier and the background classifier are learned to generate staggered classification scores for discerning and suppressing background sounds. Experiments show that our BA-WSSED significantly improves the performance of the general MIL-based WSSED method on multiple datasets and can be employed on various baseline models",
    "checked": true,
    "id": "65ae26f921b0abebaaba29af2bcba82591a068e9",
    "semantic_title": "background-aware modeling for weakly supervised sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/srivastava23_interspeech.html": {
    "title": "How to (Virtually) Train Your Speaker Localizer",
    "volume": "main",
    "abstract": "Learning-based methods have become ubiquitous in speaker localization. Existing systems rely on simulated training sets for the lack of sufficiently large, diverse and annotated real datasets. Most room acoustics simulators used for this purpose rely on the image source method (ISM) because of its computational efficiency. This paper argues that carefully extending the ISM to incorporate more realistic surface, source and microphone responses into training sets can significantly boost the real-world performance of speaker localization systems. It is shown that increasing the training-set realism of a state-of-the-art direction-of-arrival estimator yields consistent improvements across three different real test sets featuring human speakers in a variety of rooms and various microphone arrays. An ablation study further reveals that every added layer of realism contributes positively to these improvements",
    "checked": true,
    "id": "50ec9df4395264e474850f4ed9ffd9573c4ad23b",
    "semantic_title": "how to (virtually) train your speaker localizer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghosh23b_interspeech.html": {
    "title": "MMER: Multimodal Multi-task Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose MMER, a novel Multimodal Multi-task learning approach for Speech Emotion Recognition. MMER leverages a novel multimodal network based on early-fusion and cross-modal self-attention between text and acoustic modalities and solves three novel auxiliary tasks for learning emotion recognition from spoken utterances. In practice, MMER outperforms all our baselines and achieves state-of-the-art performance on the IEMOCAP benchmark. Additionally, we conduct extensive ablation studies and results analysis to prove the effectiveness of our proposed approach",
    "checked": true,
    "id": "10ab7df49d5a61cf656d97092590af3ed4defd4f",
    "semantic_title": "mmer: multimodal multi-task learning for speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/khandelwal23_interspeech.html": {
    "title": "A Multi-Task Learning Framework for Sound Event Detection using High-level Acoustic Characteristics of Sounds",
    "volume": "main",
    "abstract": "Sound event detection (SED) entails identifying the type of sound and estimating its temporal boundaries from acoustic signals. These events are uniquely characterized by their spatio-temporal features, which are determined by the way they are produced. In this study, we leverage some distinctive high-level acoustic characteristics of various sound events to assist the SED model training, without requiring additional labeled data. Specifically, we use the DCASE Task 4 2022 dataset and categorize the 10 classes into four subcategories based on their high-level acoustic characteristics. We then introduce a novel multi-task learning framework that jointly trains the SED and high-level acoustic characteristics classification tasks, using shared layers and weighted loss. Our method significantly improves the performance of the SED system, achieving a 36.3% improvement in terms of the polyphonic sound event detection score compared to the baseline on the DCASE 2022 Task 4 validation set",
    "checked": true,
    "id": "3296c89f3338e5ff53097366dacfe44768450c4e",
    "semantic_title": "a multi-task learning framework for sound event detection using high-level acoustic characteristics of sounds",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/neumann23b_interspeech.html": {
    "title": "A Multimodal Investigation of Speech, Text, Cognitive and Facial Video Features for Characterizing Depression With and Without Medication",
    "volume": "main",
    "abstract": "Clinical depression is one of the most common mental disorders and technology for remote assessment of depression, including monitoring of treatment responses, is gaining more and more importance. Using a cloud-based multimodal dialog platform, we conducted a crowdsourced study to investigate the effect of depression severity and antidepressant use on various acoustic, linguistic, cognitive, and orofacial features. Our findings show that multiple features from all tested modalities show statistically significant differences between subjects with no or minimal depression and subjects with more severe depression symptoms. Moreover, certain acoustic and visual features show significant differences between subjects with moderately severe or severe symptoms who take antidepressants and those who do not take any. Machine learning experiments show that subjects with and without medication can be better discriminated from each other at higher severity levels",
    "checked": true,
    "id": "649703d09c09fd1d06a9c1f802302ba4bac533ce",
    "semantic_title": "a multimodal investigation of speech, text, cognitive and facial video features for characterizing depression with and without medication",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/addlesee23_interspeech.html": {
    "title": "Understanding Disrupted Sentences Using Underspecified Abstract Meaning Representation",
    "volume": "main",
    "abstract": "Voice assistant accessibility is generally overlooked as today's spoken dialogue systems are trained on huge corpora to help them understand the 'average' user. This raises frustrating barriers for certain user groups as their speech shifts from the average. People with dementia pause more frequently mid-sentence for example, and people with hearing impairments may mispronounce words learned post-diagnosis. We explore whether semantic parsing can improve accessibility for people with non-standard speech, and consequently become more robust to external disruptions like dogs barking, sirens passing, or doors slamming mid-utterance. We generate corpora of disrupted sentences paired with their underspecified Abstract Meaning Representation (AMR) graphs, and use these to train pipelines to understand and repair disruptions. Our best disruption recovery pipeline lost only 1.6% graph similarity f-score when compared to a model given the full original sentence",
    "checked": true,
    "id": "269a77ef89c0a88e6aac7a1bb0455bcbc39a7efa",
    "semantic_title": "understanding disrupted sentences using underspecified abstract meaning representation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/field23_interspeech.html": {
    "title": "Developing Speech Processing Pipelines for Police Accountability",
    "volume": "main",
    "abstract": "Police body-worn cameras have the potential to improve accountability and transparency in policing. Yet in practice, they result in millions of hours of footage that is never reviewed. We investigate the potential of large pre-trained speech models for facilitating reviews, focusing on ASR and officer speech detection in footage from traffic stops. Our proposed pipeline includes training data alignment and filtering, fine-tuning with resource constraints, and combining officer speech detection with ASR for a fully automated approach. We find that (1) fine-tuning strongly improves ASR performance on officer speech (WER=12-13%), (2) ASR on officer speech is much more accurate than on community member speech (WER=43.55-49.07%), (3) domain-specific tasks like officer speech detection and diarization remain challenging. Our work offers practical applications for reviewing body camera footage and general guidance for adapting pre-trained speech models to noisy multi-speaker domains",
    "checked": true,
    "id": "38c2e4e54a50ea5027c7a06ab325c22ece7b6c40",
    "semantic_title": "developing speech processing pipelines for police accountability",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szekely23_interspeech.html": {
    "title": "Prosody-controllable Gender-ambiguous Speech Synthesis: A Tool for Investigating Implicit Bias in Speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rouas23_interspeech.html": {
    "title": "Affective attributes of French caregivers' professional speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/casanova23_interspeech.html": {
    "title": "ASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23_interspeech.html": {
    "title": "Personality-aware Training based Speaker Adaptation for End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ito23b_interspeech.html": {
    "title": "Target Vocabulary Recognition Based on Multi-Task Learning with Decomposed Teacher Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shen23_interspeech.html": {
    "title": "Wave to Syntax: Probing spoken language models for syntax",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naowarat23_interspeech.html": {
    "title": "Effective Training of Attention-based Contextual Biasing Adapters with Synthetic Audio for Personalised ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23b_interspeech.html": {
    "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/haque23_interspeech.html": {
    "title": "SlothSpeech: Denial-of-service Attack Against Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23d_interspeech.html": {
    "title": "CLRL-Tuning: A Novel Continual Learning Approach for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23_interspeech.html": {
    "title": "Exploring Sources of Racial Bias in Automatic Speech Recognition through the Lens of Rhythmic Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23e_interspeech.html": {
    "title": "Can Contextual Biasing Remain Effective with Whisper and GPT-2?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niizumi23_interspeech.html": {
    "title": "Masked Modeling Duo for Speech: Specializing General-Purpose Audio Representation to Speech using Denoising Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23c_interspeech.html": {
    "title": "Improving RNN Transducer Acoustic Models for English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23_interspeech.html": {
    "title": "MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23h_interspeech.html": {
    "title": "Improving Chinese Mandarin Speech Recognition Using Semantic Graph Embedding Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23o_interspeech.html": {
    "title": "Adapting Multi-Lingual ASR Models for Handling Multiple Talkers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23c_interspeech.html": {
    "title": "Adapter-tuning with Effective Token-dependent Representation Shift for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23c_interspeech.html": {
    "title": "Model-Internal Slot-triggered Biasing for Domain Expansion in Neural Transducer ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yao23b_interspeech.html": {
    "title": "Delay-penalized CTC Implemented Based on Finite State Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23f_interspeech.html": {
    "title": "Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23u_interspeech.html": {
    "title": "Knowledge Distillation Approach for Efficient Internal Language Model Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/adhikary23_interspeech.html": {
    "title": "Language Model Personalization for Improved Touchscreen Typing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23b_interspeech.html": {
    "title": "Blank Collapse: Compressing CTC Emission for the Faster Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peyser23_interspeech.html": {
    "title": "Improving Joint Speech-Text Representations Without Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/flynn23_interspeech.html": {
    "title": "Leveraging Cross-Utterance Context For ASR Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23_interspeech.html": {
    "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tsunoo23_interspeech.html": {
    "title": "Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23e_interspeech.html": {
    "title": "A Neural Time Alignment Module for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23i_interspeech.html": {
    "title": "Accelerating Transducers through Adjacent Token Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23_interspeech.html": {
    "title": "Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23sa_interspeech.html": {
    "title": "Language-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23h_interspeech.html": {
    "title": "Embedding Articulatory Constraints for Low-resource Speech Recognition Based on Large Pre-trained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23b_interspeech.html": {
    "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/antonova23_interspeech.html": {
    "title": "SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bijwadia23_interspeech.html": {
    "title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gitman23_interspeech.html": {
    "title": "Confidence-based Ensembles of End-to-End Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chi23_interspeech.html": {
    "title": "Unsupervised Code-switched Text Generation from Parallel Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23b_interspeech.html": {
    "title": "A Binary Keyword Spotting System with Error-Diffusion Based Feature Binarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23b_interspeech.html": {
    "title": "Language-universal Phonetic Encoder for Low-resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23g_interspeech.html": {
    "title": "A Lexical-aware Non-autoregressive Transformer-based ASR Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vanvuren23_interspeech.html": {
    "title": "Improving Under-Resourced Code-Switched Speech Recognition: Large Pre-trained Models or Architectural Interventions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bellegarda23_interspeech.html": {
    "title": "Pragmatic Pertinence: A Learnable Confidence Metric to Assess the Subjective Quality of LM-Generated Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ea_interspeech.html": {
    "title": "ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharma23_interspeech.html": {
    "title": "BASS: Block-wise Adaptation for Speech Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shekar23_interspeech.html": {
    "title": "Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data: Fearless Steps Apollo-11 Audio Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yan23_interspeech.html": {
    "title": "Combining language corpora in a Japanese electromagnetic articulography database for acoustic-to-articulatory inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23g_interspeech.html": {
    "title": "A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chivriga23_interspeech.html": {
    "title": "Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/banerjee23_interspeech.html": {
    "title": "Enc-Dec RNN Acoustic Word Embeddings learned via Pairwise Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kotey23_interspeech.html": {
    "title": "Query Based Acoustic Summarization for Podcasts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23f_interspeech.html": {
    "title": "Spot Keywords From Very Noisy and Mixed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nayem23_interspeech.html": {
    "title": "Knowledge Distillation on Joint Task End-to-End Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23d_interspeech.html": {
    "title": "Investigating Pre-trained Audio Encoders in the Low-Resource Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23g_interspeech.html": {
    "title": "Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23m_interspeech.html": {
    "title": "Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/papadimitriou23_interspeech.html": {
    "title": "Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gonzalezmachorro23_interspeech.html": {
    "title": "Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rathod23_interspeech.html": {
    "title": "Whisper Features for Dysarthric Severity-Level Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tang23b_interspeech.html": {
    "title": "A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yue23_interspeech.html": {
    "title": "Dysarthric Speech Recognition, Detection and Classification using Raw Phase and Magnitude Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bayerl23_interspeech.html": {
    "title": "A Stutter Seldom Comes Alone – Cross-Corpus Stuttering Detection as a Multi-label Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhattacharjee23_interspeech.html": {
    "title": "Transfer Learning to Aid Dysarthria Severity Classification for Patients with Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23qa_interspeech.html": {
    "title": "DuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach with Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hedeshy23_interspeech.html": {
    "title": "CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baali23_interspeech.html": {
    "title": "Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kouzelis23_interspeech.html": {
    "title": "Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/novotny23_interspeech.html": {
    "title": "Glottal source analysis of voice deficits in basal ganglia dysfunction: evidence from de novo Parkinson's disease and Huntington's disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mun23b_interspeech.html": {
    "title": "An Analysis of Glottal Features of Chronic Kidney Disease Speech and Its Application to CKD Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/belagali23_interspeech.html": {
    "title": "Weakly supervised glottis segmentation in high-speed videoendoscopy using bounding box labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23k_interspeech.html": {
    "title": "An Efficient and Noise-Robust Audiovisual Encoder for Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23b_interspeech.html": {
    "title": "A Novel Self-training Approach for Low-resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23g_interspeech.html": {
    "title": "FunASR: A Fundamental End-to-End Speech Recognition Toolkit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23_interspeech.html": {
    "title": "Streaming Audio-Visual Speech Recognition with Alignment Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fernandezlopez23_interspeech.html": {
    "title": "SparseVSR: Lightweight and Noise Robust Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23c_interspeech.html": {
    "title": "Multimodal Speech Recognition for Language-Guided Embodied Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nishu23_interspeech.html": {
    "title": "Matching Latent Encoding for Audio-Text based Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/p23_interspeech.html": {
    "title": "Self-Paced Pattern Augmentation for Spoken Term Detection in Zero-Resource",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23y_interspeech.html": {
    "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/michieli23_interspeech.html": {
    "title": "Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23j_interspeech.html": {
    "title": "Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23t_interspeech.html": {
    "title": "Robust Keyword Spotting for Noisy Environments by Leveraging Speech Enhancement and Speech Presence Probability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23m_interspeech.html": {
    "title": "Enhancing the Unified Streaming and Non-streaming Model with Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23c_interspeech.html": {
    "title": "ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23_interspeech.html": {
    "title": "Improved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huybrechts23_interspeech.html": {
    "title": "DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23_interspeech.html": {
    "title": "Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23d_interspeech.html": {
    "title": "Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martel23_interspeech.html": {
    "title": "Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saijo23_interspeech.html": {
    "title": "Remixing-based Unsupervised Source Separation from Scratch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/okamoto23_interspeech.html": {
    "title": "CAPTDURE: Captioned Sound Dataset of Single Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/munakata23_interspeech.html": {
    "title": "Recursive Sound Source Separation with Deep Learning-based Beamforming for Unknown Number of Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mosner23_interspeech.html": {
    "title": "Multi-Channel Speech Separation with Cross-Attention and Beamforming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eom23_interspeech.html": {
    "title": "Background-Sound Controllable Voice Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/escobargrisales23_interspeech.html": {
    "title": "An Automatic Multimodal Approach to Analyze Linguistic and Acoustic Cues on Parkinson's Disease Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23_interspeech.html": {
    "title": "Personalization for Robust Voice Pathology Detection in Sound Waves",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23d_interspeech.html": {
    "title": "Integrated and Enhanced Pipeline System to Support Spoken Language Analytics for Screening Neurocognitive Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niu23b_interspeech.html": {
    "title": "Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23d_interspeech.html": {
    "title": "FTA-net: A Frequency and Time Attention Network for Speech Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fara23_interspeech.html": {
    "title": "Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23y_interspeech.html": {
    "title": "Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/campbell23_interspeech.html": {
    "title": "Classifying depression symptom severity: Assessment of speech representations in personalized and generalized machine learning models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghaffarzadegan23_interspeech.html": {
    "title": "Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pereztoro23_interspeech.html": {
    "title": "Automatic Assessment of Alzheimer's across Three Languages Using Speech and Language Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geng23_interspeech.html": {
    "title": "On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/svihlik23_interspeech.html": {
    "title": "Relationship between LTAS-based spectral moments and acoustic parameters of hypokinetic dysarthria in Parkinson's disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alvarado23_interspeech.html": {
    "title": "Respiratory distress estimation in human-robot interaction scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/reynerfuentes23_interspeech.html": {
    "title": "Prediction of the Gender-based Violence Victim Condition using Speech: What do Machine Learning Models rely on?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/charola23_interspeech.html": {
    "title": "Whisper Encoder features for Infant Cry Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jurov23_interspeech.html": {
    "title": "A neural architecture for selective attention to speech features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huo23_interspeech.html": {
    "title": "Quantifying Informational Masking due to Masker Intelligibility in Same-talker Speech-in-speech Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cuervo23_interspeech.html": {
    "title": "On the Benefits of Self-supervised Learned Speech Representations for Predicting Human Phonetic Misperceptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schulz23_interspeech.html": {
    "title": "Predicting Perceptual Centers Located at Vowel Onset in German Speech Using Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cooke23_interspeech.html": {
    "title": "Exploring the mutual intelligibility breakdown caused by sculpting speech from a competing speech signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kitahara23_interspeech.html": {
    "title": "Perception of Incomplete Voicing Neutralization of Obstruents in Tohoku Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pohnlein23_interspeech.html": {
    "title": "The emergence of obstruent-intrinsic f0 and VOT as cues to the fortis/lenis contrast in West Central Bavarian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/havard23_interspeech.html": {
    "title": "〈'〉 in Tsimane': a Preliminary Investigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hoffmann23_interspeech.html": {
    "title": "Segmental features of Brazilian (Santa Catarina) Hunsrik",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ratko23_interspeech.html": {
    "title": "Opening or Closing? An Electroglottographic Analysis of Voiceless Coda Consonants in Australian English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zebe23_interspeech.html": {
    "title": "Increasing aspiration of word-medial fortis plosives in Swiss Standard German",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shao23_interspeech.html": {
    "title": "Lexical Stress and Velar Palatalization in Italian: A spatio-temporal Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23i_interspeech.html": {
    "title": "Speaker Embeddings as Individuality Proxy for Voice Stress Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23j_interspeech.html": {
    "title": "From Interval to Ordinal: A HMM based Approach for Emotion Label Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23l_interspeech.html": {
    "title": "Turbo your multi-modal classification with contrastive learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ioannides23_interspeech.html": {
    "title": "Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23y_interspeech.html": {
    "title": "SOT: Self-supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bansal23_interspeech.html": {
    "title": "On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23b_interspeech.html": {
    "title": "Speaking State Decoder with Transition Detection for Next Speaker Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kitagishi23_interspeech.html": {
    "title": "What are differences? Comparing DNN and Human by Their Performance and Characteristics in Speaker Age Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arts23_interspeech.html": {
    "title": "Effects of perceived gender on the perceived social function of laughter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/purohit23_interspeech.html": {
    "title": "Implicit phonetic information modeling for speech emotion recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/leem23_interspeech.html": {
    "title": "Computation and Memory Efficient Noise Adaptation of Wav2Vec2.0 for Noisy Speech Emotion Recognition with Skip Connection Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23b_interspeech.html": {
    "title": "Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naini23_interspeech.html": {
    "title": "Preference Learning Labels by Anchoring on Consecutive Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chetiaphukan23_interspeech.html": {
    "title": "Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23_interspeech.html": {
    "title": "Learning Local to Global Feature Aggregation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23q_interspeech.html": {
    "title": "Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pham23b_interspeech.html": {
    "title": "Vietnam-Celeb: a large-scale dataset for Vietnamese speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23v_interspeech.html": {
    "title": "What Can an Accent Identifier Learn? Probing Phonetic and Prosodic Information in a Wav2vec2-based Accent Identification Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23c_interspeech.html": {
    "title": "The 2022 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarni23_interspeech.html": {
    "title": "Description and analysis of the KPT system for NIST Language Recognition Evaluation 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yip23_interspeech.html": {
    "title": "ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yao23_interspeech.html": {
    "title": "Branch-ECAPA-TDNN: A Parallel Branch Architecture to Capture Local and Global Features for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23d_interspeech.html": {
    "title": "Speaker Verification Across Ages: Investigating Deep Speaker Embedding Sensitivity to Age Mismatch in Enrollment and Test Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dey23_interspeech.html": {
    "title": "Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/radhakrishnan23_interspeech.html": {
    "title": "A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tamayoflorez23_interspeech.html": {
    "title": "HABLA: A Dataset of Latin American Spanish Accents for Voice Anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23aa_interspeech.html": {
    "title": "Self-supervised Learning Representation based Accent Recognition with Persistent Accent Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23g_interspeech.html": {
    "title": "Extremely Low Bit Quantization for Mobile Speaker Verification Systems Under 1MB Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/das23_interspeech.html": {
    "title": "Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bredin23_interspeech.html": {
    "title": "pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23t_interspeech.html": {
    "title": "Model Compression for DNN-based Speaker Verification Using Weight Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vachhani23_interspeech.html": {
    "title": "Multi-resolution Approach to Identification of Spoken Languages and To Improve Overall Language Diarization System Using Whisper Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23_interspeech.html": {
    "title": "Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23b_interspeech.html": {
    "title": "Dynamic Fully-Connected Layer for Large-Scale Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schroter23b_interspeech.html": {
    "title": "DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burkhardt23_interspeech.html": {
    "title": "Nkululeko: Machine Learning Experiments on Speaker Characteristics Without Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lemaguer23_interspeech.html": {
    "title": "Sp1NY: A Quick and Flexible Speech Visualisation Tool in Python",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/corkey23_interspeech.html": {
    "title": "Intonation Control for Neural Text-to-Speech Synthesis with Polynomial Models of F0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szekely23b_interspeech.html": {
    "title": "So-to-Speak: An Exploratory Platform for Investigating the Interplay between Style and Prosody in TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arai23_interspeech.html": {
    "title": "Comparing /b/ and /d/ with a Single Physical Model of the Human Vocal Tract to Visualize Droplets Produced while Speaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ekstedt23b_interspeech.html": {
    "title": "Show & Tell: Voice Activity Projection and Turn-taking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cordourier23_interspeech.html": {
    "title": "Real Time Detection of Soft Voice for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tanna23_interspeech.html": {
    "title": "Data Augmentation for Diverse Voice Conversion in Noisy Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gogate23_interspeech.html": {
    "title": "Application for Real-time Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23d_interspeech.html": {
    "title": "Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23_interspeech.html": {
    "title": "Streaming Parrotron for on-device speech-to-speech conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shaheen23_interspeech.html": {
    "title": "Exploiting Emotion Information in Speaker Embeddings for Expressive Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/okamoto23b_interspeech.html": {
    "title": "E2E-S2S-VC: End-To-End Sequence-To-Sequence Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23f_interspeech.html": {
    "title": "DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baas23_interspeech.html": {
    "title": "Voice Conversion With Just Nearest Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tanaka23_interspeech.html": {
    "title": "CFVC: Conditional Filtering for Controllable Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ning23_interspeech.html": {
    "title": "DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23_interspeech.html": {
    "title": "Attention-based Interactive Disentangling Network for Instance-level Emotional Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23p_interspeech.html": {
    "title": "ALO-VC: Any-to-any Low-latency One-shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/minixhofer23_interspeech.html": {
    "title": "Evaluating and reducing the distance between synthetic and real speech distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/quamer23_interspeech.html": {
    "title": "Decoupling Segmental and Prosodic Cues of Non-native Speech through Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kanagawa23_interspeech.html": {
    "title": "VC-T: Streaming Voice Conversion Based on Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghosh23_interspeech.html": {
    "title": "Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion-Preserving Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23r_interspeech.html": {
    "title": "ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23e_interspeech.html": {
    "title": "Reverberation-Controllable Voice Conversion Using Reverberation Time Estimator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23d_interspeech.html": {
    "title": "Cross-utterance Conditioned Coherent Speech Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23o_interspeech.html": {
    "title": "MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23y_interspeech.html": {
    "title": "CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ca_interspeech.html": {
    "title": "Improving Zero-shot Cross-domain Slot Filling via Transformer-based Slot Semantics Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shin23_interspeech.html": {
    "title": "Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23c_interspeech.html": {
    "title": "Boosting Punctuation Restoration with Data Generation and Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23e_interspeech.html": {
    "title": "J-ToneNet: A Transformer-based Encoding Network for Improving Tone Classification in Continuous Speech via F0 Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/avila23_interspeech.html": {
    "title": "Towards Cross-Language Prosody Transfer for Dialog",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kesiraju23_interspeech.html": {
    "title": "Strategies for Improving Low Resource Speech to Text Translation Relying on Pre-trained ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koudounas23_interspeech.html": {
    "title": "ITALIC: An Italian Intent Classification Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rugayan23_interspeech.html": {
    "title": "Perceptual and Task-Oriented Assessment of a Semantic Metric for ASR Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23s_interspeech.html": {
    "title": "How ChatGPT is Robust for Spoken Language Understanding?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ye23b_interspeech.html": {
    "title": "GigaST: A 10,000-hour Pseudo Speech Translation Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fan23b_interspeech.html": {
    "title": "Boosting Chinese ASR Error Correction with Dynamic Error Scaling Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fallgren23_interspeech.html": {
    "title": "Crowdsource-based Validation of the Audio Cocktail as a Sound Browsing Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23z_interspeech.html": {
    "title": "PunCantonese: A Benchmark Corpus for Low-Resource Cantonese Punctuation Restoration from Speech Transcripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kato23_interspeech.html": {
    "title": "Speech-to-Face Conversion Using Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nishikawa23_interspeech.html": {
    "title": "Inter-connection: Effective Connection between Pre-trained Encoder and Decoder for Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/radfar23_interspeech.html": {
    "title": "Conmer: Streaming Conformer Without Self-attention for Interactive Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23e_interspeech.html": {
    "title": "Intra-ensemble: A New Method for Combining Intermediate Outputs in Transformer-based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23b_interspeech.html": {
    "title": "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mai23_interspeech.html": {
    "title": "HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/carvalho23_interspeech.html": {
    "title": "Memory-augmented conformer for improved end-to-end long-form ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23_interspeech.html": {
    "title": "Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23o_interspeech.html": {
    "title": "An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23x_interspeech.html": {
    "title": "A Study on Visualization of Voiceprint Feature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yakovlev23_interspeech.html": {
    "title": "VoxTube: a multilingual speaker recognition dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23p_interspeech.html": {
    "title": "Visualizing Data Augmentation in Deep Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ba_interspeech.html": {
    "title": "Fast and Efficient Multilingual Self-Supervised Pre-training for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23z_interspeech.html": {
    "title": "UniSplice: Universal Cross-Lingual Data Splicing for Low-Resource ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/glocker23_interspeech.html": {
    "title": "Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23_interspeech.html": {
    "title": "Phonetic-assisted Multi-Target Units Modeling for Improving Conformer-Transducer ASR system",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rouditchenko23_interspeech.html": {
    "title": "Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ea_interspeech.html": {
    "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23b_interspeech.html": {
    "title": "Emotional Voice Conversion with Semi-Supervised Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23d_interspeech.html": {
    "title": "Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23_interspeech.html": {
    "title": "S2CD: Self-heuristic Speaker Content Disentanglement for Any-to-Any Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23g_interspeech.html": {
    "title": "Flow-VAE VC: End-to-End Flow Framework with Contrastive Loss for Zero-shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23s_interspeech.html": {
    "title": "Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23b_interspeech.html": {
    "title": "End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/braun23_interspeech.html": {
    "title": "Classifying Dementia in the Presence of Depression: A Cross-Corpus Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23b_interspeech.html": {
    "title": "Exploiting Cross-Domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wagner23_interspeech.html": {
    "title": "Multi-class Detection of Pathological Speech with Latent Features: How does it perform on unseen data?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kothare23_interspeech.html": {
    "title": "Responsiveness, Sensitivity and Clinical Utility of Timing-Related Speech Biomarkers for Remote Monitoring of ALS Disease Progression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geng23b_interspeech.html": {
    "title": "Use of Speech Impairment Severity for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mosuily23_interspeech.html": {
    "title": "MMLung: Moving Closer to Practical Lung Health Estimation using Smartphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23i_interspeech.html": {
    "title": "Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23pa_interspeech.html": {
    "title": "Non-uniform Speaker Disentanglement For Depression Detection From Raw Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/demir23_interspeech.html": {
    "title": "PoCaPNet: A Novel Approach for Surgical Phase Recognition Using Speech and X-Ray Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/neumann23_interspeech.html": {
    "title": "Combining Multiple Multimodal Speech Features into an Interpretable Index Score for Capturing Disease Progression in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mallolragolta23_interspeech.html": {
    "title": "The MASCFLICHT Corpus: Face Mask Type and Coverage Area Recognition from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/botelho23_interspeech.html": {
    "title": "Towards Reference Speech Characterization for Health Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/riosurrego23_interspeech.html": {
    "title": "Automatic Classification of Hypokinetic and Hyperkinetic Dysarthria based on GMM-Supervectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dineley23_interspeech.html": {
    "title": "Towards robust paralinguistic assessment for real-world mobile health (mHealth) monitoring: an initial study of reverberation effects on speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simmatis23_interspeech.html": {
    "title": "Multimodal Assessment of Bulbar Amyotrophic Lateral Sclerosis (ALS) Using a Novel Remote Speech Assessment App",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martinez23_interspeech.html": {
    "title": "On the Use of High Frequency Information for Voice Pathology Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/favaro23_interspeech.html": {
    "title": "Do Phonatory Features Display Robustness to Characterize Parkinsonian Speech Across Corpora?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kadiri23_interspeech.html": {
    "title": "Severity Classification of Parkinson's Disease from Speech using Single Frequency Filtering-based Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simek23_interspeech.html": {
    "title": "Comparison of acoustic measures of dysphonia in Parkinson's disease and Huntington's disease: Effect of sex and speaking task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gomezzaragoza23_interspeech.html": {
    "title": "Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23c_interspeech.html": {
    "title": "LanSER: Language-Model Supported Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23_interspeech.html": {
    "title": "Fine-tuned RoBERTa Model with a CNN-LSTM Network for Conversational Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/stanley23_interspeech.html": {
    "title": "Emotion Label Encoding Using Word Embeddings for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ia_interspeech.html": {
    "title": "Discrimination of the Different Intents Carried by the Same Text Through Integrating Multimodal Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23f_interspeech.html": {
    "title": "Meta-domain Adversarial Contrastive Learning for Alleviating Individual Bias in Self-sentiment Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23b_interspeech.html": {
    "title": "SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23f_interspeech.html": {
    "title": "PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23b_interspeech.html": {
    "title": "Exploring the Interactions Between Target Positive and Negative Information for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/andreev23_interspeech.html": {
    "title": "Iterative autoregression: a novel trick to improve your low-latency speech enhancement model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ku23_interspeech.html": {
    "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/frenkel23_interspeech.html": {
    "title": "Domain Adaptation for Speech Enhancement in a Large Domain Gap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zadorozhnyy23_interspeech.html": {
    "title": "SCP-GAN: Self-Correcting Discriminator Optimization for Training Consistency Preserving Metric GAN on Speech Enhancement Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23c_interspeech.html": {
    "title": "A Mask Free Neural Network for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23p_interspeech.html": {
    "title": "A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pandey23b_interspeech.html": {
    "title": "A Simple RNN Model for Lightweight, Low-compute and Low-latency Multichannel Speech Enhancement in the Time Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23b_interspeech.html": {
    "title": "High Fidelity Speech Enhancement with Band-split RNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23b_interspeech.html": {
    "title": "Focus on the Sound around You: Monaural Target Speaker Extraction via Distance and Speaker Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kovalyov23_interspeech.html": {
    "title": "DFSNet: A Steerable Neural Beamformer Invariant to Microphone Array Configuration for Real-Time, Low-Latency Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23o_interspeech.html": {
    "title": "Speaker-Aware Anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/araki23_interspeech.html": {
    "title": "Impact of Residual Noise and Artifacts in Speech Enhancement Errors on Intelligibility of Human and Machine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sach23_interspeech.html": {
    "title": "EffCRN: An Efficient Convolutional Recurrent Network for High-Performance Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23e_interspeech.html": {
    "title": "HAD-ANC: A Hybrid System Comprising an Adaptive Filter and Deep Neural Networks for Active Noise Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chu23_interspeech.html": {
    "title": "MSAF: A Multiple Self-Attention Field Method for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23t_interspeech.html": {
    "title": "Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wan23_interspeech.html": {
    "title": "ABC-KD: Attention-Based-Compression Knowledge Distillation for Deep Learning-Based Noise Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/diener23_interspeech.html": {
    "title": "PLCMOS – A Data-driven Non-intrusive Metric for The Evaluation of Packet Loss Concealment Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wagner23b_interspeech.html": {
    "title": "Effects of Meter, Genre and Experience on Pausing, Lengthening and Prosodic Phrasing in German Poetry Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szalay23_interspeech.html": {
    "title": "Comparing first spectral moment of Australian English /s/ between straight and gay voices using three analysis window sizes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/taguchi23_interspeech.html": {
    "title": "Universal Automatic Phonetic Transcription into the International Phonetic Alphabet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gerlach23_interspeech.html": {
    "title": "Voice Twins: Discovering Extremely Similar-sounding, Unrelated Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hedegard23_interspeech.html": {
    "title": "Filling the population statistics gap: Swiss German reference data on F0 and speech tempo for forensic contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hutin23_interspeech.html": {
    "title": "Investigating the Syntax-Discourse Interface in the Phonetic Implementation of Discourse Markers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/essery23_interspeech.html": {
    "title": "Evaluation of a Forensic Automatic Speaker Recognition System with Emotional Speech Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahn23_interspeech.html": {
    "title": "An Outlier Analysis of Vowel Formants from a Corpus Phonetics Pipeline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qu23_interspeech.html": {
    "title": "The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/blaylock23_interspeech.html": {
    "title": "Beatboxing Kick Drum Kinematics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23b_interspeech.html": {
    "title": "Effects of hearing loss and amplification on Mandarin consonant perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/adams23_interspeech.html": {
    "title": "An Acoustic Analysis of Fricative Variation in Three Accents of English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bros23_interspeech.html": {
    "title": "Acoustic cues to stress perception in Spanish – a mismatch negativity study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sabev23_interspeech.html": {
    "title": "Bulgarian Unstressed Vowel Reduction: Received Views vs Corpus Findings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jain23b_interspeech.html": {
    "title": "An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23b_interspeech.html": {
    "title": "Identifying Stable Sections for Formant Frequency Extraction of French Nasal Vowels Based on Difference Thresholds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/audibert23_interspeech.html": {
    "title": "Evaluation of delexicalization methods for research on emotional speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kejriwal23b_interspeech.html": {
    "title": "Relationship between auditory and semantic entrainment using Deep Neural Networks (DNN)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kejriwal23_interspeech.html": {
    "title": "Unsupervised Auditory and Semantic Entrainment Models with Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nielsen23_interspeech.html": {
    "title": "Parsing dialog turns with prosodic features in English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muromachi23_interspeech.html": {
    "title": "Estimation of Listening Response Timing by Generative Model and Parameter Control of Response Substantialness Using Dynamic-Prompt-Tune",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chowdhury23_interspeech.html": {
    "title": "Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/buddi23_interspeech.html": {
    "title": "Efficient Multimodal Neural Networks for Trigger-less Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ostrand23_interspeech.html": {
    "title": "Rapid Lexical Alignment to a Conversational Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kurata23_interspeech.html": {
    "title": "Multimodal Turn-Taking Model Using Visual Cues for End-of-Utterance Prediction in Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hojo23_interspeech.html": {
    "title": "Audio-Visual Praise Estimation for Conversational Video based on Synchronization-Guided Multimodal Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sakuma23_interspeech.html": {
    "title": "Improving the response timing estimation for spoken dialogue systems by reducing the effect of speech recognition delay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23c_interspeech.html": {
    "title": "Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23la_interspeech.html": {
    "title": "A Multiple-Teacher Pruning Based Self-Distillation (MT-PSD) Approach to Model Compression for Audio-Visual Wake Word Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/spiesberger23_interspeech.html": {
    "title": "Abusive Speech Detection in Indic Languages Using Acoustic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ingle23_interspeech.html": {
    "title": "Listening To Silences In Contact Center Conversations Using Textual Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeen23_interspeech.html": {
    "title": "I Learned Error, I Can Fix It! : A Detector-Corrector Structure for ASR Error Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/garnier23b_interspeech.html": {
    "title": "Verbal and nonverbal feedback signals in response to increasing levels of miscommunication",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/amiriparian23_interspeech.html": {
    "title": "Speech-Based Classification of Defensive Communication: A Novel Dataset and Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wallbridge23_interspeech.html": {
    "title": "Quantifying the perceptual value of lexical and non-lexical channels in speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tsubokura23_interspeech.html": {
    "title": "Relationships Between Gender, Personality Traits and Features of Multi-Modal Data to Responses to Spoken Dialog Systems Breakdown",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23e_interspeech.html": {
    "title": "Speaker-aware Cross-modal Fusion Architecture for Conversational Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liao23_interspeech.html": {
    "title": "Blind Estimation of Room Impulse Response from Monaural Reverberant Speech with Segmental Generative Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ren23_interspeech.html": {
    "title": "Emotion-Aware Audio-Driven Face Animation via Contrastive Feature Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shimonishi23_interspeech.html": {
    "title": "Anomalous Sound Detection Based on Sound Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fahed23_interspeech.html": {
    "title": "Random Forest Classification of Breathing Phases from Audio Signals Recorded using Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahn23b_interspeech.html": {
    "title": "GRAVO: Learning to Generate Relevant Audio from Visual Features with Noisy Online Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhai23_interspeech.html": {
    "title": "Wav2ToBI: a new approach to automatic ToBI transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23b_interspeech.html": {
    "title": "Joint-Former: Jointly Regularized and Locally Down-sampled Conformer for Semi-supervised Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/goel23_interspeech.html": {
    "title": "Towards Attention-based Contrastive Learning for Audio Spoof Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23d_interspeech.html": {
    "title": "Masked Audio Modeling with CLAP and Multi-Objective Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rusci23_interspeech.html": {
    "title": "Few-Shot Open-Set Learning for On-Device Customization of KeyWord Spotting Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/azeemi23_interspeech.html": {
    "title": "Self-Supervised Dataset Pruning for Efficient Training in Audio Anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23b_interspeech.html": {
    "title": "Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mariotte23_interspeech.html": {
    "title": "Multi-microphone Automatic Speech Segmentation in Meetings Based on Circular Harmonics Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23h_interspeech.html": {
    "title": "Advanced RawNet2 with Attention-based Channel Masking for Synthetic Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martinezsevilla23_interspeech.html": {
    "title": "Insights into end-to-end audio-to-score transcription with real recordings: A case study with saxophone works",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23d_interspeech.html": {
    "title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23b_interspeech.html": {
    "title": "Synthetic Voice Spoofing Detection based on Feature Pyramid Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23c_interspeech.html": {
    "title": "Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kerpicci23_interspeech.html": {
    "title": "Application of Knowledge Distillation to Multi-Task Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23g_interspeech.html": {
    "title": "DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/almudevar23_interspeech.html": {
    "title": "Variational Classifier for Unsupervised Anomalous Sound Detection under Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23c_interspeech.html": {
    "title": "FlexiAST: Flexibility is What AST Needs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23c_interspeech.html": {
    "title": "MCR-Data2vec 2.0: Improving Self-supervised Speech Pre-training via Model-level Consistency Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23l_interspeech.html": {
    "title": "Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ditthapron23_interspeech.html": {
    "title": "Masking Kernel for Learning Energy-Efficient Representations for Speaker Recognition and Mobile Health",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiang23_interspeech.html": {
    "title": "eSTImate: A Real-time Speech Transmission Index Estimator With Speech Enhancement Auxiliary Task Using Self-Attention Feature Pyramid Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23n_interspeech.html": {
    "title": "Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23b_interspeech.html": {
    "title": "Privacy-preserving Representation Learning for Speech Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panariello23_interspeech.html": {
    "title": "Vocoder drift in x-vector–based speaker anonymization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panariello23b_interspeech.html": {
    "title": "Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zaiem23b_interspeech.html": {
    "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23d_interspeech.html": {
    "title": "An extension of disentanglement metrics and its application to voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/abdullah23_interspeech.html": {
    "title": "An Information-Theoretic Analysis of Self-supervised Discrete Representations of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ashihara23_interspeech.html": {
    "title": "SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sasou23_interspeech.html": {
    "title": "Comparison of GIF- and SSL-based Features in Pathological-voice Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23c_interspeech.html": {
    "title": "What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/masumura23_interspeech.html": {
    "title": "End-to-End Joint Target and Non-Target Speakers ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23c_interspeech.html": {
    "title": "Improving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/makishima23_interspeech.html": {
    "title": "Joint Autoregressive Modeling of End-to-End Multi-Talker Overlapped Speech Recognition and Utterance-level Timestamp Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23_interspeech.html": {
    "title": "Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23_interspeech.html": {
    "title": "Multi-pass Training and Cross-information Fusion for Low-resource End-to-end Accented Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bataev23_interspeech.html": {
    "title": "Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23_interspeech.html": {
    "title": "Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23b_interspeech.html": {
    "title": "Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matsuura23_interspeech.html": {
    "title": "Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deshmukh23_interspeech.html": {
    "title": "Audio Retrieval with WavText5K and CLAP Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cappellazzo23b_interspeech.html": {
    "title": "Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chien23b_interspeech.html": {
    "title": "Contrastive Disentangled Learning for Memory-Augmented Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deseyssel23_interspeech.html": {
    "title": "ProsAudit, a prosodic benchmark for self-supervised speech models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23j_interspeech.html": {
    "title": "Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hallap23_interspeech.html": {
    "title": "Evaluating context-invariance in unsupervised speech representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23_interspeech.html": {
    "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23_interspeech.html": {
    "title": "Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23d_interspeech.html": {
    "title": "Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/paterson23_interspeech.html": {
    "title": "A Pipeline to Evaluate the Effects of Noise on Machine Learning Detection of Laryngeal Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ga_interspeech.html": {
    "title": "ReCLR: Reference-Enhanced Contrastive Learning of Audio Representation for Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/egaslopez23_interspeech.html": {
    "title": "Automated Multiple Sclerosis Screening Based on Encoded Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/melistas23_interspeech.html": {
    "title": "Cross-Lingual Features for Alzheimer's Dementia Detection from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zusag23_interspeech.html": {
    "title": "Careful Whisper - leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thienpondt23_interspeech.html": {
    "title": "Behavioral Analysis of Pathological Speaker Embeddings of Patients During Oncological Treatment of Oral Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23b_interspeech.html": {
    "title": "Adversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hsieh23_interspeech.html": {
    "title": "Adapter-Based Extension of Multi-Speaker Text-To-Speech Model for New Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sivaguru23_interspeech.html": {
    "title": "SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23k_interspeech.html": {
    "title": "UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dang23b_interspeech.html": {
    "title": "LightVoc: An Upsampling-Free GAN Vocoder Based On Conformer And Inverse Short-time Fourier Transform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saito23_interspeech.html": {
    "title": "ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23f_interspeech.html": {
    "title": "Human Transcription Quality Improvement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simantiraki23_interspeech.html": {
    "title": "The effect of masking noise on listeners' spectral tilt preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tranngoc23_interspeech.html": {
    "title": "The Effect of Whistled Vowels on Whistled Word Categorization for Naive Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bharati23_interspeech.html": {
    "title": "Automatic Deep Neural Network-Based Segmental Pronunciation Error Detection of L2 English Speech (L1 Bengali)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hao23_interspeech.html": {
    "title": "The effect of stress on Mandarin tonal perception in continuous speech for Spanish-speaking learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elmerich23_interspeech.html": {
    "title": "Combining acoustic and aerodynamic data collection: A perceptual evaluation of acoustic distortions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elie23b_interspeech.html": {
    "title": "Estimating virtual targets for lingual stop consonants using general Tau theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gibson23_interspeech.html": {
    "title": "Using Random Forests to classify language as a function of syllable timing in two groups: children with cochlear implants and with normal hearing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23w_interspeech.html": {
    "title": "An Improved End-to-End Audio-Visual Speech Recognition Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wesoek23_interspeech.html": {
    "title": "What influences the foreign accent strength? Phonological and grammatical errors in the perception of accentedness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huttner23_interspeech.html": {
    "title": "Investigating the Perception Production Link through Perceptual Adaptation and Phonetic Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23f_interspeech.html": {
    "title": "Emotion Prompting for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chin23_interspeech.html": {
    "title": "Speech-in-Speech Recognition is Modulated by Familiarity to Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23m_interspeech.html": {
    "title": "BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/miodonska23_interspeech.html": {
    "title": "Are retroflex-to-dental sibilant substitutions in Polish children's speech an example of a covert contrast? A preliminary acoustic study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23i_interspeech.html": {
    "title": "Reversible Neural Networks for Memory-Efficient Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23f_interspeech.html": {
    "title": "ECAPA++: Fine-grained Deep Embedding Learning for TDNN Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23w_interspeech.html": {
    "title": "TO-Rawnet: Improving RawNet with TCN and Orthogonal Regularization for Fake Audio Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zuo23_interspeech.html": {
    "title": "Fooling Speaker Identification Systems with Adversarial Background Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23r_interspeech.html": {
    "title": "Mutual Information-based Embedding Decoupling for Generalizable Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23c_interspeech.html": {
    "title": "Target Active Speaker Detection with Audio-visual Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/broughton23_interspeech.html": {
    "title": "Improving End-to-End Neural Diarization Using Conversational Summary Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zang23_interspeech.html": {
    "title": "Phase perturbation improves channel robustness for speech spoofing countermeasures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bousquet23_interspeech.html": {
    "title": "Improving training datasets for resource-constrained speaker recognition neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lertpetchpun23_interspeech.html": {
    "title": "Instance-based Temporal Normalization for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/novoselov23_interspeech.html": {
    "title": "On the robustness of wav2vec 2.0 based speaker recognition systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23i_interspeech.html": {
    "title": "P-vectors: A Parallel-coupled TDNN/Transformer Network for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lei23_interspeech.html": {
    "title": "Group GMM-ResNet for Detection of Synthetic Speech Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fang23_interspeech.html": {
    "title": "Robust Training for Speaker Verification against Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jeoung23_interspeech.html": {
    "title": "Self-Distillation into Self-Attention Heads for Improving Transformer-based End-to-End Neural Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23m_interspeech.html": {
    "title": "Build a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benamor23_interspeech.html": {
    "title": "Describing the phonetics in the underlying speech attributes for deep and interpretable speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23v_interspeech.html": {
    "title": "Range-Based Equal Error Rate for Spoof Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tabassum23_interspeech.html": {
    "title": "Exploring the English Accent-independent Features for Speech Emotion Recognition using Filter and Wrapper-based Methods for Feature Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/plaquet23_interspeech.html": {
    "title": "Powerset multi-class cross entropy loss for neural speaker diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23_interspeech.html": {
    "title": "A Method of Audio-Visual Person Verification by Mining Connections between Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fish23_interspeech.html": {
    "title": "A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23_interspeech.html": {
    "title": "Modeling Dependent Structure for Utterances in ASR Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/verma23_interspeech.html": {
    "title": "ASR for Low Resource and Multilingual Noisy Code-Mixed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23b_interspeech.html": {
    "title": "Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mateju23_interspeech.html": {
    "title": "Combining Multilingual Resources and Models to Develop State-of-the-Art E2E ASR for Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23o_interspeech.html": {
    "title": "Two Stage Contextual Word Filtering for Context Bias in Unified Streaming and Non-streaming Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pham23_interspeech.html": {
    "title": "Towards continually learning new languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23e_interspeech.html": {
    "title": "N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23g_interspeech.html": {
    "title": "SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gulzar23_interspeech.html": {
    "title": "miniStreamer: Enhancing Small Conformer with Chunked-Context Masking for Streaming ASR Applications on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23p_interspeech.html": {
    "title": "CoMFLP: Correlation Measure Based Fast Search on ASR Layer Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23h_interspeech.html": {
    "title": "Exploration on HuBERT with Multiple Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23s_interspeech.html": {
    "title": "Quantization-aware and Tensor-compressed Training of Transformers for Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naowarat23b_interspeech.html": {
    "title": "Word-level Confidence Estimation for CTC Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kulshreshtha23_interspeech.html": {
    "title": "Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23_interspeech.html": {
    "title": "Unsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23_interspeech.html": {
    "title": "4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yen23_interspeech.html": {
    "title": "Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fan23_interspeech.html": {
    "title": "Language-specific Boundary Learning for Improving Mandarin-English Code-switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23c_interspeech.html": {
    "title": "Mixture-of-Expert Conformer for Streaming Multilingual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23x_interspeech.html": {
    "title": "Lossless 4-bit Quantization of Architecture Compressed Conformer ASR Systems on the 300-hr Switchboard Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23c_interspeech.html": {
    "title": "Compressed MoE ASR Model Based on Knowledge Distillation and Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deng23b_interspeech.html": {
    "title": "Factorised Speaker-environment Adaptive Training of Conformer Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23aa_interspeech.html": {
    "title": "Text Only Domain Adaptation with Phoneme Guided Data Splicing for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cahyawijaya23_interspeech.html": {
    "title": "Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23fa_interspeech.html": {
    "title": "Modular Domain Adaptation for Conformer-Based Streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhatia23_interspeech.html": {
    "title": "Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23f_interspeech.html": {
    "title": "SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mori23_interspeech.html": {
    "title": "A Generative Framework for Conversational Laughter: Its 'Language Model' and Laughter Sound Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ba_interspeech.html": {
    "title": "Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lameris23_interspeech.html": {
    "title": "Beyond Style: Synthesizing Speech with Pragmatic Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/abbas23_interspeech.html": {
    "title": "eCat: An End-to-End Model for Multi-Speaker TTS & Many-to-Many Fine-Grained Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deb23_interspeech.html": {
    "title": "BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kashiwagi23_interspeech.html": {
    "title": "Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jakubiak23_interspeech.html": {
    "title": "Whistle-to-text: Automatic recognition of the Silbo Gomero whistled language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23c_interspeech.html": {
    "title": "A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nortje23_interspeech.html": {
    "title": "Visually grounded few-shot word acquisition with fewer shots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23_interspeech.html": {
    "title": "JAMFN: Joint Attention Multi-Scale Fusion Network for Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23z_interspeech.html": {
    "title": "Prompt Guided Copy Mechanism for Conversational Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/faustini23_interspeech.html": {
    "title": "Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23c_interspeech.html": {
    "title": "On Monotonic Aggregation for Open-domain QA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nguyen23b_interspeech.html": {
    "title": "Question-Context Alignment and Answer-Context Dependencies for Effective Answer Sentence Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23v_interspeech.html": {
    "title": "Multi-Scale Attention for Audio Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23f_interspeech.html": {
    "title": "Enhancing Visual Question Answering via Deconstructing Questions and Explicating Answers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23c_interspeech.html": {
    "title": "SEF-Net: Speaker Embedding Free Target Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rose23_interspeech.html": {
    "title": "Cascaded encoders for fine-tuning ASR models on overlapped speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/erdogan23_interspeech.html": {
    "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23b_interspeech.html": {
    "title": "Unified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahmadikalkhorani23_interspeech.html": {
    "title": "Time-domain Transformer-based Audiovisual Speaker Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/delcroix23_interspeech.html": {
    "title": "Multi-Stream Extension of Variational Bayesian HMM Clustering (MS-VBx) for Combined End-to-End and Vector Clustering-based Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niu23_interspeech.html": {
    "title": "Unsupervised Adaptation with Quality-Aware Masking to Improve Target-Speaker Voice Activity Detection for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23e_interspeech.html": {
    "title": "BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23e_interspeech.html": {
    "title": "Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gaultier23_interspeech.html": {
    "title": "Joint compensation of multi-talker noise and reverberation for speech enhancement with cochlear implants using one or more microphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yousefi23_interspeech.html": {
    "title": "Speaker Diarization for ASR Output with T-vectors: A Sequence Classification Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raj23_interspeech.html": {
    "title": "GPU-accelerated Guided Source Separation for Meeting Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23c_interspeech.html": {
    "title": "Overlap Aware Continuous Speech Separation without Permutation Invariant Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23w_interspeech.html": {
    "title": "Weakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23j_interspeech.html": {
    "title": "Directional Speech Recognition for Speaker Disambiguation and Cross-talk Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/berger23_interspeech.html": {
    "title": "Mixture Encoder for Joint Speech Separation and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hejna23_interspeech.html": {
    "title": "Aberystwyth English Pre-aspiration in Apparent Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23c_interspeech.html": {
    "title": "Speech Entrainment in Chinese Story-Style Talk Shows: The Interaction Between Gender and Role",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/steiner23_interspeech.html": {
    "title": "Sociodemographic and Attitudinal Effects on Dialect Speakers' Articulation of the Standard Language: Evidence from German-Speaking Switzerland",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burridge23_interspeech.html": {
    "title": "Vowel Normalisation in Latent Space for Sociolinguistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23n_interspeech.html": {
    "title": "Attention-based Encoder-Decoder Network for End-to-End Neural Speaker Diarization with Target Speaker Attractor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lahiri23_interspeech.html": {
    "title": "Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baghel23_interspeech.html": {
    "title": "The DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/paturi23_interspeech.html": {
    "title": "Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pirlogeanu23_interspeech.html": {
    "title": "The SpeeD--ZevoTech submission at DISPLACE 2023",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23g_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with Absolute Speaker Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23d_interspeech.html": {
    "title": "A Context-Constrained Sentence Modeling for Deception Detection in Real Interrogation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23c_interspeech.html": {
    "title": "MetricAug: A Distortion Metric-Lead Augmentation Strategy for Training Noise-Robust Speech Emotion Recognizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ludusan23_interspeech.html": {
    "title": "The co-use of laughter and head gestures across speech styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23d_interspeech.html": {
    "title": "EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23b_interspeech.html": {
    "title": "Pre-Finetuning for Few-Shot Emotional Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23_interspeech.html": {
    "title": "Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lavania23_interspeech.html": {
    "title": "Utility-Preserving Privacy-Enabled Speech Embeddings for Emotion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burdisso23_interspeech.html": {
    "title": "Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/branco23_interspeech.html": {
    "title": "Laughter in task-based settings: whom we talk to affects how, when, and how often we laugh",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fang23b_interspeech.html": {
    "title": "Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deoliveira23_interspeech.html": {
    "title": "Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23d_interspeech.html": {
    "title": "Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thakran23_interspeech.html": {
    "title": "Investigating Acoustic Cues for Multilingual Abuse Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23c_interspeech.html": {
    "title": "A novel frequency warping scale for speech emotion recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23m_interspeech.html": {
    "title": "Multi-Scale Temporal Transformer For Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grageda23_interspeech.html": {
    "title": "Distant Speech Emotion Recognition in an Indoor Human-robot Interaction Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tao23b_interspeech.html": {
    "title": "A Study on Prosodic Entrainment in Relation to Therapist Empathy in Counseling Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/minematsu23_interspeech.html": {
    "title": "A Unified Framework to Improve Learners' Skills of Perception and Production Based on Speech Shadowing and Overlapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nicholls23_interspeech.html": {
    "title": "Speak & Improve: L2 English Speaking Practice Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nicolao23_interspeech.html": {
    "title": "Measuring prosody in child speech using SoapBox Fluency API",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nissen23_interspeech.html": {
    "title": "Teaching Non-native Sound Contrasts using Visual Biofeedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/walsh23_interspeech.html": {
    "title": "Large-Scale Automatic Audiobook Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elkheir23_interspeech.html": {
    "title": "QVoice: Arabic Speech Pronunciation Learning Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/svec23_interspeech.html": {
    "title": "Asking Questions: an Innovative Way to Interact with Oral History Archives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhat23_interspeech.html": {
    "title": "DisfluencyFixer: A tool to enhance Language Learning through Speech To Speech Disfluency Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/prakash23_interspeech.html": {
    "title": "Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elshahawy23_interspeech.html": {
    "title": "MyVoice: Arabic Speech Resource Collaboration Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hromada23_interspeech.html": {
    "title": "Personal Primer Prototype 1: Invitation to Make Your Own Embooked Speech-Based Educational Artifact",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deng23_interspeech.html": {
    "title": "Time-frequency Domain Filter-and-sum Network for Multi-channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23h_interspeech.html": {
    "title": "Audio-Visual Fusion using Multiscale Temporal Convolutional Attention for Time-Domain Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ca_interspeech.html": {
    "title": "An Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/phokhinanan23_interspeech.html": {
    "title": "Binaural Sound Localization in Noisy Environments Using Frequency-Based Audio Vision Transformer (FAViT)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23i_interspeech.html": {
    "title": "Contrastive Learning based Deep Latent Masking for Music Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23k_interspeech.html": {
    "title": "Speaker Extraction with Detection of Presence and Absence of Target Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23k_interspeech.html": {
    "title": "PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarabia23_interspeech.html": {
    "title": "Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23q_interspeech.html": {
    "title": "Image-driven Audio-visual Universal Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fras23_interspeech.html": {
    "title": "Joint Blind Source Separation and Dereverberation for Automatic Speech Recognition using Delayed-Subsource MNMF with Localization Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23na_interspeech.html": {
    "title": "SDNet: Stream-attention and Dual-feature Learning Network for Ad-hoc Array Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baek23_interspeech.html": {
    "title": "Deeply Supervised Curriculum Learning for Deep Neural Network-based Sound Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fujimura23_interspeech.html": {
    "title": "Multi-channel separation of dynamic speech and sound events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ja_interspeech.html": {
    "title": "Rethinking the Visual Cues in Audio-Visual Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dang23_interspeech.html": {
    "title": "Using Semi-supervised Learning for Monaural Time-domain Speech Separation with a Self-supervised Learning-based SI-SNR Estimator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23d_interspeech.html": {
    "title": "Investigation of Training Mute-Expressive End-to-End Speech Separation Networks for an Unknown Number of Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cho23_interspeech.html": {
    "title": "SR-SRP: Super-Resolution based SRP-PHAT for Sound Source Localization and Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23g_interspeech.html": {
    "title": "Dual-Memory Multi-Modal Learning for Continual Spoken Keyword Spotting with Confidence Selection and Diversity Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23j_interspeech.html": {
    "title": "FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23g_interspeech.html": {
    "title": "A Neural State-Space Modeling Approach to Efficient Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23c_interspeech.html": {
    "title": "Locate and Beamform: Two-dimensional Locating All-neural Beamformer for Multi-channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23f_interspeech.html": {
    "title": "Monaural Speech Separation Method Based on Recurrent Attention with Parallel Branches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23m_interspeech.html": {
    "title": "Ontology-aware Learning and Evaluation for Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23c_interspeech.html": {
    "title": "Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lay23_interspeech.html": {
    "title": "Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muller23_interspeech.html": {
    "title": "Complex-valued neural networks for voice anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ristea23_interspeech.html": {
    "title": "DeepVQE: Real Time Deep Voice Quality Enhancement for Joint Acoustic Echo Cancellation, Noise Suppression and Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sawata23_interspeech.html": {
    "title": "Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23h_interspeech.html": {
    "title": "HD-DEMUCS: General Speech Restoration with Heterogeneous Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23e_interspeech.html": {
    "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yin23_interspeech.html": {
    "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23x_interspeech.html": {
    "title": "Detection of Cross-Dataset Fake Audio Based on Prosodic and Pronunciation Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dowerah23_interspeech.html": {
    "title": "Self-supervised learning with Diffusion-based multichannel speech enhancement for speaker verification under noisy conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nespoli23_interspeech.html": {
    "title": "Two-Stage Voice Anonymization for Enhanced Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23h_interspeech.html": {
    "title": "Personalized Dereverberation of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/binhthien23_interspeech.html": {
    "title": "Weighted Von Mises Distribution-based Loss Function for Real-time STFT Phase Reconstruction Using DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schroter23_interspeech.html": {
    "title": "Deep Multi-Frame Filtering for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiong23_interspeech.html": {
    "title": "Aligning Speech Enhancement for Improving Downstream Classification Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23b_interspeech.html": {
    "title": "DNN-based Parameter Estimation for MVDR Beamforming and Post-filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23b_interspeech.html": {
    "title": "FRA-RIR: Fast Random Approximation of the Image-source Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23b_interspeech.html": {
    "title": "Rethinking Complex-Valued Deep Neural Networks for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/le23_interspeech.html": {
    "title": "Harmonic enhancement using learnable comb filter for light-weight full-band speech enhancement model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23e_interspeech.html": {
    "title": "How Does Pretraining Improve Discourse-Aware Translation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23t_interspeech.html": {
    "title": "PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tseng23_interspeech.html": {
    "title": "Model-assisted Lexical Tone Evaluation of three-year-old Chinese-speaking Children by also Considering Segment Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23b_interspeech.html": {
    "title": "Sentence Embedder Guided Utterance Encoder (SEGUE) for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23u_interspeech.html": {
    "title": "Joint Time and Frequency Transformer for Chinese Opera Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23_interspeech.html": {
    "title": "AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arvan23_interspeech.html": {
    "title": "Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pouranbenveyseh23_interspeech.html": {
    "title": "Combining Heterogeneous Structures for Event Causality Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/biswas23_interspeech.html": {
    "title": "An Efficient Approach for the Automated Segmentation and Transcription of the People's Speech Sorpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23g_interspeech.html": {
    "title": "Diverse Feature Mapping and Fusion via Multitask Learning for Multilingual Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bahar23_interspeech.html": {
    "title": "Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23e_interspeech.html": {
    "title": "Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23j_interspeech.html": {
    "title": "Efficient Adaptation of Spoken Language Understanding based on End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23d_interspeech.html": {
    "title": "PhonMatchNet: Phoneme-Guided Zero-Shot Keyword Spotting for User-Defined Keywords",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23_interspeech.html": {
    "title": "Mix before Align: Towards Zero-shot Cross-lingual Sentiment Analysis via Soft-Mix and Multi-View Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/papi23_interspeech.html": {
    "title": "AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/polak23_interspeech.html": {
    "title": "Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sikasote23_interspeech.html": {
    "title": "Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mun23_interspeech.html": {
    "title": "Towards Single Integrated Spoofing-aware Speaker Verification Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ba_interspeech.html": {
    "title": "Pseudo-Siamese Network based Timbre-reserved Black-box Adversarial Attack in Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23v_interspeech.html": {
    "title": "Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23v_interspeech.html": {
    "title": "Robust Audio Anti-spoofing Countermeasure with Joint Training of Front-end and Back-end Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kawa23b_interspeech.html": {
    "title": "Improved DeepFake Detection Using Whisper Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23c_interspeech.html": {
    "title": "DoubleDeceiver: Deceiving the Speaker Verification System Protected by Spoofing Countermeasures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panchapagesan23_interspeech.html": {
    "title": "On Training a Neural Residual Acoustic Echo Suppressor for Improved ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lemercier23_interspeech.html": {
    "title": "Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23b_interspeech.html": {
    "title": "UnSE: Unsupervised Speech Enhancement Using Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23k_interspeech.html": {
    "title": "MC-SpEx: Towards Effective Speaker Extraction with Multi-Scale Interfusion and Conditional Speaker Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bartolewska23_interspeech.html": {
    "title": "Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23q_interspeech.html": {
    "title": "Gesper: A Restoration-Enhancement Framework for General Speech Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ryumina23_interspeech.html": {
    "title": "Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pudo23_interspeech.html": {
    "title": "MOCKS 1.0: Multilingual Open Custom Keyword Spotting Testset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eisenstein23_interspeech.html": {
    "title": "MD3: The Multi-Dialect Dataset of Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/anwar23_interspeech.html": {
    "title": "MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/suwanbandit23_interspeech.html": {
    "title": "Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23d_interspeech.html": {
    "title": "HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bekal23_interspeech.html": {
    "title": "A Metric-Driven Approach to Conformer Layer Pruning for Efficient ASR Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gurunathshivakumar23_interspeech.html": {
    "title": "Distillation Strategies for Discriminative Speech Recognition Rescoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pouthier23_interspeech.html": {
    "title": "Another Point of View on Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23e_interspeech.html": {
    "title": "RASR2: The RWTH ASR Toolkit for Generic Sequence-to-sequence Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/filimonov23_interspeech.html": {
    "title": "Streaming Speech-to-Confusion Network Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23d_interspeech.html": {
    "title": "Accurate and Structured Pruning for Efficient Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chua23_interspeech.html": {
    "title": "MERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gupta23_interspeech.html": {
    "title": "Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shahin23_interspeech.html": {
    "title": "Improving wav2vec2-based Spoken Language Identification by Learning Phonological Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/praveen23_interspeech.html": {
    "title": "Language Identification Networks for Multilingual Everyday Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/styles23_interspeech.html": {
    "title": "Investigating model performance in language identification: beyond simple error statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kodali23_interspeech.html": {
    "title": "Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kathan23_interspeech.html": {
    "title": "The effect of clinical intervention on the speech of individuals with PTSD: features and recognition performances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/triantafyllopoulos23_interspeech.html": {
    "title": "Analysis and automatic prediction of exertion from speech: Contrasting objective and subjective measures collected while running",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tao23_interspeech.html": {
    "title": "The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eni23_interspeech.html": {
    "title": "Comparing Hand-Crafted Features to Spectrograms for Autism Severity Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mijnders23_interspeech.html": {
    "title": "Acoustic characteristics of depression in older adults' speech: the role of covariates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23b_interspeech.html": {
    "title": "Dual Transformer Decoder based Features Fusion Network for Automated Audio Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pellegrini23_interspeech.html": {
    "title": "Adapting a ConvNeXt Model to Audio Classification on AudioSet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23w_interspeech.html": {
    "title": "Few-shot Class-incremental Audio Classification Using Stochastic Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23d_interspeech.html": {
    "title": "Enhance Temporal Relations in Audio Captioning with Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23s_interspeech.html": {
    "title": "First Language Effects on Second Language Perception: Evidence from English Low-vowel Nasal Sequences Perceived by L1 Mandarin Chinese Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/maity23_interspeech.html": {
    "title": "Motor Control Similarity Between Speakers Saying \"A Souk\" Using Inverse Atlas Tongue Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23t_interspeech.html": {
    "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoshinaga23_interspeech.html": {
    "title": "A Relationship Between Vocal Fold Vibration and Droplet Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/garnier23_interspeech.html": {
    "title": "Audio, Visual and Audiovisual intelligibility of vowels produced in noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elie23_interspeech.html": {
    "title": "Optimal control of speech with context-dependent articulatory targets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23d_interspeech.html": {
    "title": "Computational modeling of auditory brainstem responses derived from modified speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ma_interspeech.html": {
    "title": "Leveraging Label Information for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23c_interspeech.html": {
    "title": "Improving End-to-End Modeling For Mandarin-English Code-Switching Using Lightweight Switch-Routing Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ea_interspeech.html": {
    "title": "Frequency Patterns of Individual Speaker Characteristics at Higher and Lower Spectral Ranges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gosselkeberthelsen23_interspeech.html": {
    "title": "Adaptation to predictive prosodic cues in non-native standard dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/archerboyd23_interspeech.html": {
    "title": "Head movements in two- and four-person interactive conversational tasks in noisy and moderately reverberant conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23d_interspeech.html": {
    "title": "Second language identification of Vietnamese tones by native Mandarin learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fagniart23_interspeech.html": {
    "title": "Nasal vowel production and grammatical processing in French-speaking children with cochlear implants and normal-hearing peers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23f_interspeech.html": {
    "title": "Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23b_interspeech.html": {
    "title": "L2-Mandarin regional accent variability during Mandarin tone-word training facilitates English listeners' subsequent tone categorizations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ueda23_interspeech.html": {
    "title": "HumanDiffusion: diffusion model using perceptual gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kachel23_interspeech.html": {
    "title": "Queer Events, Relationships, and Sports: Does Topic Influence Speakers' Acoustic Expression of Sexual Orientation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gunason23_interspeech.html": {
    "title": "Epoch-Based Spectrum Estimation for Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mehta23_interspeech.html": {
    "title": "OverFlow: Putting flows on top of neural transducers for better TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mehrish23_interspeech.html": {
    "title": "ADAPTERMIX: Exploring the Efficacy of Mixture of Adapters for Low-Resource TTS Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23c_interspeech.html": {
    "title": "Prior-free Guided TTS: An Improved and Efficient Diffusion-based Text-Guided Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/iashchenko23_interspeech.html": {
    "title": "UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23_interspeech.html": {
    "title": "Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/guan23_interspeech.html": {
    "title": "Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kogel23_interspeech.html": {
    "title": "Towards Robust FastSpeech 2 by Modelling Residual Multimodality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23c_interspeech.html": {
    "title": "Real time spectrogram inversion on mobile phone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23_interspeech.html": {
    "title": "Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wells23_interspeech.html": {
    "title": "A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/krug23_interspeech.html": {
    "title": "Self-Supervised Solution to the Control Problem of Articulatory Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23f_interspeech.html": {
    "title": "Hierarchical Timbre-Cadence Speaker Encoder for Zero-shot Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23_interspeech.html": {
    "title": "ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/du23_interspeech.html": {
    "title": "Improving WaveRNN with Heuristic Dynamic Blending for Fast and High-Quality GPU Vocoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23_interspeech.html": {
    "title": "Intelligible Lip-to-Speech Synthesis with Speech Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23p_interspeech.html": {
    "title": "Parameter-Efficient Learning for Text-to-Speech Accent Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/khan23_interspeech.html": {
    "title": "Controlling formant frequencies with neural text-to-speech for the manipulation of perceived speaker age",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jang23b_interspeech.html": {
    "title": "FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kaneko23_interspeech.html": {
    "title": "iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23_interspeech.html": {
    "title": "VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luong23_interspeech.html": {
    "title": "Controlling Multi-Class Human Vocalization Generation via a Simple Segment-based Labeling Scheme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhogale23_interspeech.html": {
    "title": "Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23_interspeech.html": {
    "title": "Domain Adaptive Self-supervised Training of Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/olivier23_interspeech.html": {
    "title": "There is more than one kind of robustness: Fooling Whisper with adversarial examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heggan23_interspeech.html": {
    "title": "MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23l_interspeech.html": {
    "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23l_interspeech.html": {
    "title": "Blank-regularized CTC for Frame Skipping in Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jayakumar23_interspeech.html": {
    "title": "The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/unni23_interspeech.html": {
    "title": "Improving RNN-Transducers with Acoustic LookAhead",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/markl23_interspeech.html": {
    "title": "Everyone has an accent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/maison23_interspeech.html": {
    "title": "Some Voices are Too Common: Building Fair Speech Recognition Systems Using the CommonVoice Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23u_interspeech.html": {
    "title": "Information Magnitude Based Dynamic Sub-sampling for Speech-to-text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grice23_interspeech.html": {
    "title": "What's in a Rise? The Relevance of Intonation for Attention Orienting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23i_interspeech.html": {
    "title": "HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23e_interspeech.html": {
    "title": "VISinger2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23c_interspeech.html": {
    "title": "EdenTTS: A Simple and Efficient Parallel Text-to-speech Architecture with Collaborative Duration-alignment Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23c_interspeech.html": {
    "title": "Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/montesinos23_interspeech.html": {
    "title": "Speech inpainting: Context-based speech synthesis guided by video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23d_interspeech.html": {
    "title": "STEN-TTS: Improving Zero-shot Cross-Lingual Transfer for Multi-Lingual TTS with Style-Enhanced Normalization Diffusion Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kano23_interspeech.html": {
    "title": "Average Token Delay: A Latency Metric for Simultaneous Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qian23_interspeech.html": {
    "title": "Automatic Speech Recognition Transformer with Global Contextual Information Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23c_interspeech.html": {
    "title": "Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/praveen23b_interspeech.html": {
    "title": "Prefix Search Decoding for RNN Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bain23_interspeech.html": {
    "title": "WhisperX: Time-Accurate Speech Transcription of Long-Form Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nigmatulina23_interspeech.html": {
    "title": "Implementing Contextual Biasing in GPU Decoder for Online ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chung23_interspeech.html": {
    "title": "MF-PAM: Accurate Pitch Estimation through Periodicity Analysis and Multi-level Feature Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/attia23_interspeech.html": {
    "title": "Enhancing Speech Articulation Analysis Using A Geometric Transformation of the X-ray Microbeam Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jouaiti23_interspeech.html": {
    "title": "Matching Acoustic and Perceptual Measures of Phonation Assessment in Disordered Speech - A Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23_interspeech.html": {
    "title": "Improved Contextualized Speech Representations for Tonal Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chandrasekar23_interspeech.html": {
    "title": "A Study on the Importance of Formant Transitions for Stop-Consonant Classification in VCV Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eren23_interspeech.html": {
    "title": "FusedF0: Improving DNN-based F0 Estimation by Fusion of Summary-Correlograms and Raw Waveform Representations of Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kyung23_interspeech.html": {
    "title": "Improving Joint Speech and Emotion Recognition Using Global Style Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nagase23_interspeech.html": {
    "title": "Speech Emotion Recognition by Estimating Emotional Label Sequences with Phoneme Class Attribute",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23_interspeech.html": {
    "title": "Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/prisayad23_interspeech.html": {
    "title": "Dual Memory Fusion for Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kondratenko23_interspeech.html": {
    "title": "Hybrid Dataset for Speech Emotion Recognition in Russian Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hsu23_interspeech.html": {
    "title": "Speech Emotion Recognition using Decomposed Speech via Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23b_interspeech.html": {
    "title": "Prospective Validation of Motor-Based Intervention with Automated Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23_interspeech.html": {
    "title": "Classifying Rhoticity of /ɹ/ in Speech Sound Disorder using Age-and-Sex Normalized Formants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23c_interspeech.html": {
    "title": "Acoustic-to-Articulatory Speech Inversion Features for Mispronunciation Detection of /ɹ/ in Child Speech Sound Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/piton23_interspeech.html": {
    "title": "Using Commercial ASR Solutions to Assess Reading Skills in Children: A Case Report",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gebauer23_interspeech.html": {
    "title": "Exploiting Diversity of Automatic Transcripts from Distinct Speech Recognition Techniques for Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rumberg23_interspeech.html": {
    "title": "Uncertainty Estimation for Connectionist Temporal Classification Based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lavechin23_interspeech.html": {
    "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23c_interspeech.html": {
    "title": "Data augmentation for children ASR and child-adult speaker classification using voice conversion methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shetty23_interspeech.html": {
    "title": "Developmental Articulatory and Acoustic Features for Six to Ten Year Old Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23u_interspeech.html": {
    "title": "Automatically Predicting Perceived Conversation Quality in a Pediatric Sample Enriched for Autism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/johnson23_interspeech.html": {
    "title": "An Equitable Framework for Automatically Assessing Children's Oral Narrative Language Abilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cao23_interspeech.html": {
    "title": "An Analysis of Goodness of Pronunciation for Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sy23_interspeech.html": {
    "title": "Measuring Language Development From Child-centered Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hung23_interspeech.html": {
    "title": "Speaking Clearly, Understanding Better: Predicting the L2 Narrative Comprehension of Chinese Bilingual Kindergarten Children Based on Speech Intelligibility Using a Machine Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/charuau23_interspeech.html": {
    "title": "Speech Breathing Behavior During Pauses in Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23e_interspeech.html": {
    "title": "Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ariasvergara23_interspeech.html": {
    "title": "Measuring Phonological Precision in Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23_interspeech.html": {
    "title": "A Study on Using Duration and Formant Features in Automatic Detection of Speech Sound Disorder in Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baumann23_interspeech.html": {
    "title": "Influence of Utterance and Speaker Characteristics on the Classification of Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23g_interspeech.html": {
    "title": "Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mcneill23_interspeech.html": {
    "title": "An Autoregressive Conversational Dynamics Model for Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hori23_interspeech.html": {
    "title": "Style-transfer based Speech and Audio-visual Scene understanding for Robot Action Sequence Acquisition from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/soltau23_interspeech.html": {
    "title": "Speech Aware Dialog System Technology Challenge (DSTC11)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cai23_interspeech.html": {
    "title": "Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23k_interspeech.html": {
    "title": "Tracking Must Go On : Dialogue State Tracking with Verified Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ja_interspeech.html": {
    "title": "Ordered and Binary Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kataria23_interspeech.html": {
    "title": "Self-FiLM: Conditioning GANs with self-supervised representations for bandwidth extension based speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heo23b_interspeech.html": {
    "title": "Curriculum Learning for Self-supervised Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23aa_interspeech.html": {
    "title": "Introducing Self-Supervised Phonetic Information for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cordlandwehr23_interspeech.html": {
    "title": "A Teacher-Student Approach for Extracting Informative Speaker Embeddings From Speech Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lepage23_interspeech.html": {
    "title": "Experimenting with Additive Margins for Contrastive Self-Supervised Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hope23_interspeech.html": {
    "title": "Nonbinary American English speakers encode gender in vowel acoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharp23_interspeech.html": {
    "title": "Coarticulation of Sibe Vowels and Dorsal Fricatives in Spontaneous Speech: An Acoustic Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/brown23_interspeech.html": {
    "title": "Using speech synthesis to explain automatic speaker recognition: a new application of synthetic speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23i_interspeech.html": {
    "title": "Same F0, Different Tones: A Multidimensional Investigation of Zhangzhou Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/english23_interspeech.html": {
    "title": "Discovering Phonetic Feature Event Patterns in Transformer Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ra_interspeech.html": {
    "title": "A System for Generating Voice Source Signals that Implements the Transformed LF-model Parameter Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/siriwardena23b_interspeech.html": {
    "title": "Speaker-independent Speech Inversion for Estimation of Nasalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23e_interspeech.html": {
    "title": "Effects of Tonal Coarticulation and Prosodic Positions on Tonal Contours of Low Rising Tones: In the Case of Xiamen Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/issa23_interspeech.html": {
    "title": "Durational and Non-durational Correlates of Lexical and Derived Geminates in Arabic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rao23_interspeech.html": {
    "title": "Mapping Phonemes to Acoustic Symbols and Codes Using Synchrony in Speech Modulation Vectors Estimated by the Travellingwave Filter Bank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ge23_interspeech.html": {
    "title": "Rhythmic Characteristics of L2 German Speech by Advanced Chinese Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kelterer23_interspeech.html": {
    "title": "(Dis)agreement and Preference Structure are Reflected in Matching Along Distinct Acoustic-prosodic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/christodoulidou23_interspeech.html": {
    "title": "Vowel reduction by Greek-speaking children: The effect of stress and word length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lennes23_interspeech.html": {
    "title": "Pitch distributions in a very large corpus of spontaneous Finnish speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kudera23_interspeech.html": {
    "title": "Speech Enhancement Patterns in Human-Robot Interaction: A Cross-Linguistic Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html": {
    "title": "Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ga_interspeech.html": {
    "title": "Dual Audio Encoders Based Mandarin Prosodic Boundary Prediction by Using Multi-Granularity Prosodic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23i_interspeech.html": {
    "title": "NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23n_interspeech.html": {
    "title": "MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pavankalyan23_interspeech.html": {
    "title": "Narrator or Character: Voice Modulation in an Expressive Multi-speaker TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23b_interspeech.html": {
    "title": "CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oh23_interspeech.html": {
    "title": "Semi-supervised Learning for Continuous Emotional Intensity Controllable Speech Synthesis with Disentangled Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nguyen23_interspeech.html": {
    "title": "Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23fa_interspeech.html": {
    "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kunesova23_interspeech.html": {
    "title": "Neural Speech Synthesis with Enriched Phrase Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/swiatkowski23_interspeech.html": {
    "title": "Cross-lingual Prosody Transfer for Expressive Machine Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elmers23_interspeech.html": {
    "title": "Synthesis after a couple PINTs: Investigating the Role of Pause-Internal Phonetic Particles in Speech Synthesis and Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geneva23_interspeech.html": {
    "title": "Accentor: An Explicit Lexical Stress Model for TTS Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shechtman23_interspeech.html": {
    "title": "A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23j_interspeech.html": {
    "title": "Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23c_interspeech.html": {
    "title": "Prosody Modeling with 3D Visual Information for Expressive Video Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23f_interspeech.html": {
    "title": "LightClone: Speaker-guided Parallel Subnet Selection for Few-shot Voice Cloning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhong23_interspeech.html": {
    "title": "EE-TTS: Emphatic Expressive TTS with Linguistic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ogun23_interspeech.html": {
    "title": "Stochastic Pitch Prediction Improves the Diversity and Naturalness of Speech in Glow-TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23_interspeech.html": {
    "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23t_interspeech.html": {
    "title": "PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tian23b_interspeech.html": {
    "title": "Creating Personalized Synthetic Voices from Post-Glossectomy Speech with Guided Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vaessen23_interspeech.html": {
    "title": "Towards Multi-task Learning of Speech and Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23f_interspeech.html": {
    "title": "Regarding Topology and Variant Frame Rates for Differentiable WFST-based End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23b_interspeech.html": {
    "title": "2-bit Conformer quantization for automatic speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23_interspeech.html": {
    "title": "Time-Domain Speech Enhancement for Robust Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yifan23_interspeech.html": {
    "title": "Multi-channel multi-speaker transformer for speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ye23_interspeech.html": {
    "title": "Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/miwa23_interspeech.html": {
    "title": "Dialect Speech Recognition Modeling using Corpus of Japanese Dialects and Self-Supervised Learning-based Model XLSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23d_interspeech.html": {
    "title": "Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raissi23_interspeech.html": {
    "title": "Competitive and Resource Efficient Factored Hybrid HMM Systems are Simpler Than You Think",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23d_interspeech.html": {
    "title": "MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kreyssig23_interspeech.html": {
    "title": "Biased Self-supervised Learning for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23q_interspeech.html": {
    "title": "A Unified Recognition and Correction Model under Noisy and Accent Speech Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23h_interspeech.html": {
    "title": "wav2vec 2.0 ASR for Cantonese-Speaking Older Adults in a Clinical Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/an23_interspeech.html": {
    "title": "BAT: Boundary aware transducer for memory-efficient and low-latency ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tian23_interspeech.html": {
    "title": "Bayes Risk Transducer: Transducer with Controllable Alignment Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alastruey23_interspeech.html": {
    "title": "Multi-View Frequency-Attention Alternative to CNN Frontends for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sankar23_interspeech.html": {
    "title": "Investigating the dynamics of hand and lips in French Cued Speech using attention mechanisms and CTC-based decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23h_interspeech.html": {
    "title": "Hearing Loss Affects Emotion Perception in Older Adults: Evidence from a Prosody-Semantics Stroop Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23b_interspeech.html": {
    "title": "Cochlear-implant Listeners Listening to Cochlear-implant Simulated Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/murton23_interspeech.html": {
    "title": "Validation of a Task-Independent Cepstral Peak Prominence Measure with Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23b_interspeech.html": {
    "title": "Score-balanced Loss for Multi-aspect Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tayebiarasteh23_interspeech.html": {
    "title": "Federated Learning for Secure Development of AI Models for Parkinson's Disease Detection Using Speech from Different Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23c_interspeech.html": {
    "title": "F0inTFS: A lightweight periodicity enhancement strategy for cochlear implants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/obrien23_interspeech.html": {
    "title": "Differentiating acoustic and physiological features in speech for hypoxia detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23h_interspeech.html": {
    "title": "Mandarin Electrolaryngeal Speech Voice Conversion using Cross-domain Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chien23_interspeech.html": {
    "title": "Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/illner23_interspeech.html": {
    "title": "Which aspects of motor speech disorder are captured by Mel Frequency Cepstral Coefficients? Evidence from the change in STN-DBS conditions in Parkinson's disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/subramanian23_interspeech.html": {
    "title": "Detecting Manifest Huntington's Disease Using Vocal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23q_interspeech.html": {
    "title": "Exploring multi-task learning and data augmentation in dementia detection with self-supervised pretrained models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23i_interspeech.html": {
    "title": "GL-SSD: Global and Local Speech Style Disentanglement by vector quantization for robust sentence boundary detection in speech stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23c_interspeech.html": {
    "title": "Semantic VAD: Low-Latency Voice Activity Detection for Speech Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gudepu23_interspeech.html": {
    "title": "Dynamic Encoder RNN for Online Voice Activity Detection in Adverse Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moussa23_interspeech.html": {
    "title": "Point to the Hidden: Exposing Speech Audio Splicing via Signal Pointer Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23k_interspeech.html": {
    "title": "Real-Time Causal Spectro-Temporal Voice Activity Detection Based on Convolutional Encoding and Residual Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23c_interspeech.html": {
    "title": "SVVAD: Personal Voice Activity Detection for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/farooq23_interspeech.html": {
    "title": "Learning Cross-lingual Mappings for Data Augmentation to Improve Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/olatunji23_interspeech.html": {
    "title": "AfriNames: Most ASR Models \"Butcher\" African Names",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lonergan23_interspeech.html": {
    "title": "Towards Dialect-inclusive Recognition in a Low-resource Language: Are Balanced Corpora the Answer?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/javed23_interspeech.html": {
    "title": "Svarah: Evaluating English ASR Systems on Indian Accents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/talafha23_interspeech.html": {
    "title": "N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/picheny23_interspeech.html": {
    "title": "The MALACH Corpus: Results with End-to-End Architectures and Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23c_interspeech.html": {
    "title": "Unsupervised speech enhancement with deep dynamical generative speech and noise models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23f_interspeech.html": {
    "title": "Noise-Robust Bandwidth Expansion for 8K Speech Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shuai23_interspeech.html": {
    "title": "mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23b_interspeech.html": {
    "title": "Zoneformer: On-device Neural Beamformer For In-car Multi-zone Speech Separation, Enhancement and Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23c_interspeech.html": {
    "title": "Low-complexity Broadband Beampattern Synthesis using Array Response Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23d_interspeech.html": {
    "title": "A GAN Speech Inpainting Model for Audio Editing Software",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23k_interspeech.html": {
    "title": "Deep Speech Synthesis from MRI-Based Articulatory Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/siriwardena23_interspeech.html": {
    "title": "Learning to Compute the Articulatory Representations of Speech with the MIRRORNET",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/strauch23_interspeech.html": {
    "title": "Generating high-resolution 3D real-time MRI of the vocal tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bandekar23_interspeech.html": {
    "title": "Exploring a classification approach using quantised articulatory movements for acoustic to articulatory inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oota23b_interspeech.html": {
    "title": "MEG Encoding using Word Context Semantics in Listening Stories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cantisani23_interspeech.html": {
    "title": "Investigating the cortical tracking of speech and music with sung speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/keding23_interspeech.html": {
    "title": "Coherence Estimation Tracks Auditory Attention in Listeners with Hearing Impairment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oota23_interspeech.html": {
    "title": "Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI Brain Activity?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qiu23_interspeech.html": {
    "title": "Exploring Auditory Attention Decoding using Speaker Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/soman23_interspeech.html": {
    "title": "Enhancing the EEG Speech Match Mismatch Tasks With Word Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23e_interspeech.html": {
    "title": "Similar Hierarchical Representation of Speech and Other Complex Sounds In the Brain and Deep Residual Networks: An MEG Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/macintyre23_interspeech.html": {
    "title": "Effects of spectral degradation on the cortical tracking of the speech envelope",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/calderondepalma23_interspeech.html": {
    "title": "Effects of spectral and temporal modulation degradation on intelligibility and cortical tracking of speech signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23da_interspeech.html": {
    "title": "Transfer Learning for Personality Perception via Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nagano23_interspeech.html": {
    "title": "A stimulus-organism-response model of willingness to buy from advertising speech using voice quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/doukhan23_interspeech.html": {
    "title": "Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yanagida23_interspeech.html": {
    "title": "Influence of Personal Traits on Impressions of One's Own Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kirkland23_interspeech.html": {
    "title": "Pardon my disfluency: The impact of disfluency effects on the perception of speaker competence and confidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gessinger23_interspeech.html": {
    "title": "Cross-linguistic Emotion Perception in Human and TTS Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/duan23_interspeech.html": {
    "title": "Joint Learning Feature and Model Adaptation for Unsupervised Acoustic Modelling of Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/molenaar23_interspeech.html": {
    "title": "Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bai23_interspeech.html": {
    "title": "An ASR-enabled Reading Tutor: Investigating Feedback to Optimize Interaction for Learning to Read",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jain23_interspeech.html": {
    "title": "Adaptation of Whisper models to child speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yin23b_interspeech.html": {
    "title": "Let's Give a Voice to Conversational Agents in Virtual Reality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baali23b_interspeech.html": {
    "title": "FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23x_interspeech.html": {
    "title": "Video Summarization Leveraging Multimodal Information for Presentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nathan23_interspeech.html": {
    "title": "What questions are my customers asking?: Towards Actionable Insights from Customer Questions in Contact Center Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tripathi23_interspeech.html": {
    "title": "COnVoy: A Contact Center Operated Pipeline for Voice of Customer Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rastorgueva23_interspeech.html": {
    "title": "NeMo Forced Aligner and its application to word alignment for subtitle generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pattnaik23_interspeech.html": {
    "title": "CauSE: Causal Search Engine for Understanding Contact-Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sachdeva23_interspeech.html": {
    "title": "Tailored Real-Time Call Summarization System for Contact Centers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mandke23_interspeech.html": {
    "title": "Federated Learning Toolkit with Voice-based User Verification Demo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dugan23_interspeech.html": {
    "title": "Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cho23b_interspeech.html": {
    "title": "Fast Enrollable Streaming Keyword Spotting System: Training and Inference using a Web Browser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/agrawal23b_interspeech.html": {
    "title": "Cross-lingual/Cross-channel Intent Detection in Contact-Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heo23_interspeech.html": {
    "title": "One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kawa23_interspeech.html": {
    "title": "Defense Against Adversarial Attacks on Audio DeepFake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rosello23_interspeech.html": {
    "title": "A conformer-based classifier for variable-length utterance processing in anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ia_interspeech.html": {
    "title": "Conformer-based Language Embedding with Self-Knowledge Distillation for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zuluagagomez23_interspeech.html": {
    "title": "CommonAccent: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cumani23_interspeech.html": {
    "title": "From adaptive score normalization to adaptive data normalization for speaker verification systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ha_interspeech.html": {
    "title": "CAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kakouros23_interspeech.html": {
    "title": "North Sámi Dialect Identification with Self-supervised Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23c_interspeech.html": {
    "title": "Encoder-decoder Multimodal Speaker Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nam23_interspeech.html": {
    "title": "Disentangled Representation Learning for Multilingual Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jia23b_interspeech.html": {
    "title": "A Compact End-to-End Model with Local and Global Context for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sullivan23_interspeech.html": {
    "title": "On the Robustness of Arabic Speech Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23u_interspeech.html": {
    "title": "Adaptive Neural Network Quantization For Lightweight Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/su23_interspeech.html": {
    "title": "Adversarial Diffusion Probability Model For Cross-domain Speaker Verification Integrating Contrastive Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23f_interspeech.html": {
    "title": "Chinese Dialect Recognition Based on Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ito23_interspeech.html": {
    "title": "Spoofing Attacker Also Benefits from Self-Supervised Pretrained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vashishth23_interspeech.html": {
    "title": "Label Aware Speech Representation Learning For Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23c_interspeech.html": {
    "title": "Exploring the Impact of Back-End Network on Wav2vec 2.0 for Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23_interspeech.html": {
    "title": "Improving Speaker Verification with Self-Pretrained Transformer Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ribeiro23_interspeech.html": {
    "title": "Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/linke23_interspeech.html": {
    "title": "What do self-supervised speech representations encode? An analysis of languages, varieties, speaking styles and speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ca_interspeech.html": {
    "title": "A Compressed Synthetic Speech Detection Method with Compression Feature Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23j_interspeech.html": {
    "title": "Outlier-aware Inlier Modeling and Multi-scale Scoring for Anomalous Sound Detection via Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23c_interspeech.html": {
    "title": "MOSLight: A Lightweight Data-Efficient System for Non-Intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23c_interspeech.html": {
    "title": "A Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23i_interspeech.html": {
    "title": "MTANet: Multi-band Time-frequency Attention Network for Singing Melody Extraction from Polyphonic Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chunhui23_interspeech.html": {
    "title": "Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on Generative Adversarial Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/solanki23_interspeech.html": {
    "title": "Do Vocal Breath Sounds Encode Gender Cues for Automatic Gender Classification?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sugiura23_interspeech.html": {
    "title": "Automatic Exploration of Optimal Data Processing Operations for Sound Data Augmentation Using Improved Differentiable Automatic Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23b_interspeech.html": {
    "title": "A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23b_interspeech.html": {
    "title": "RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/manocha23_interspeech.html": {
    "title": "Spatialization Quality Metric for Binaural Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/roy23_interspeech.html": {
    "title": "AsthmaSCELNet: A Lightweight Supervised Contrastive Embedding Learning Framework for Asthma Classification Using Lung Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bae23b_interspeech.html": {
    "title": "Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/richter23_interspeech.html": {
    "title": "Remote Assessment for ALS using Multimodal Dialog Agents: Data Quality, Feasibility and Task Compliance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yariv23_interspeech.html": {
    "title": "Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/romero23_interspeech.html": {
    "title": "Obstructive sleep apnea screening with breathing sounds and respiratory effort: a multimodal deep learning approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23f_interspeech.html": {
    "title": "Investigation of Music Emotion Recognition Based on Segmented Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23c_interspeech.html": {
    "title": "The Effects of Input Type and Pronunciation Dictionary Usage in Transfer Learning for Low-Resource Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23d_interspeech.html": {
    "title": "Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23b_interspeech.html": {
    "title": "Robust Feature Decoupling in Voice Conversion by Using Locality-Based Instance Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jia23_interspeech.html": {
    "title": "Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ekstedt23_interspeech.html": {
    "title": "Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cong23_interspeech.html": {
    "title": "GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yasuda23_interspeech.html": {
    "title": "Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech Based on Tail Probabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koizumi23_interspeech.html": {
    "title": "LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mitsui23_interspeech.html": {
    "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thenguyen23_interspeech.html": {
    "title": "XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kulkarni23_interspeech.html": {
    "title": "ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deja23_interspeech.html": {
    "title": "Diffusion-based accent modelling in speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeshpanov23_interspeech.html": {
    "title": "Multilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23h_interspeech.html": {
    "title": "CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23k_interspeech.html": {
    "title": "Improving Bilingual TTS Using Language And Phonology Embedding With Embedding Strength Modulator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23f_interspeech.html": {
    "title": "High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23_interspeech.html": {
    "title": "PronScribe: Highly Accurate Multimodal Phonemic Transcription From Speech and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/swiatkowski23b_interspeech.html": {
    "title": "Expressive Machine Dubbing Through Phrase-level Cross-lingual Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chiang23_interspeech.html": {
    "title": "Why We Should Report the Details in Subjective Evaluation of TTS More Rigorously",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/perezzarazaga23_interspeech.html": {
    "title": "Speaker-independent neural formant synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saito23b_interspeech.html": {
    "title": "CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharoni23_interspeech.html": {
    "title": "SASPEECH: A Hebrew Single Speaker Dataset for Text To Speech and Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}