{
  "https://www.isca-speech.org/archive/interspeech_2023/narayanan23_interspeech.html": {
    "title": "Bridging Speech Science and Technology — Now and Into the Future",
    "volume": "main",
    "abstract": "Speech research is remarkable in so many ways – in its essential human-centeredness, the rich interconnections between the science and technology, and its wide-ranging impact that is both fundamental and applied. Crucial advances in speech science research catalyze and leverage technological advances across the machine intelligence ecosystem, from sensing and imaging to signal processing and machine learning. Likewise, creation of speech-centric societal applications benefits from an understanding of how humans produce, process and use speech in communication. In these complementary endeavors, two intertwined lines of inquiry endure: illuminating the rich information tapestry and inherent variability in speech and creating trustworthy speech technologies This talk will highlight some advances and possibilities in this multifaceted speech research realm. The first is capturing and modeling the human vocal instrument during speaking and how related technological and clinical applications leverage this technology. The second focuses on speech-based informatics tools to support research and clinical translation related to human health and wellbeing. Finally, the talk will highlight the critical goal of designing trustworthy speech and spoken language machine intelligence tools that are inclusive, equitable, robust, safe, and secure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23m_interspeech.html": {
    "title": "Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks",
    "volume": "main",
    "abstract": "Given an audio clip and a reference face image, the goal of the talking head generation is to generate a high-fidelity talking head video. Although some audio-driven methods of generating talking head videos have made some achievements in the past, most of them only focused on lip and audio synchronization and lack the ability to reproduce the facial expressions of the target person. To this end, we propose a talking head generation model consisting of a Memory-Sharing Emotion Feature extractor (MSEF) and an Attention-Augmented Translator based on U-net (AATU). Firstly, MSEF can extract implicit emotional auxiliary features from audio to estimate more accurate emotional face landmarks. Secondly, AATU acts as a translator between the estimated landmarks and the photo-realistic video frames. Extensive qualitative and quantitative experiments have shown the superiority of the proposed method to the previous works. Codes will be made publicly available",
    "checked": true,
    "id": "450c82af807da7c70dd8d43096cb1a0c5c8c929e",
    "semantic_title": "emotional talking head generation based on memory-sharing and attention-augmented networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23n_interspeech.html": {
    "title": "Speech Synthesis with Self-Supervisedly Learnt Prosodic Representations",
    "volume": "main",
    "abstract": "This paper presents S4LPR, a Speech Synthesis model conditioned on Self-Supervisedly Learnt Prosodic Representations. Instead of using raw acoustic features, such as F0 and energy, as intermediate prosodic variables, three self-supervised speech models are designed for comparison and are pre-trained on large-scale unlabeled data to extract frame-level prosodic representations. In addition to vanilla wav2vec 2.0, the other two pre-trained models learn representations from LPC residuals or adopt a multi-task learning strategy to focus on the prosodic information in speech. Based on FastSpeech2 and PnGBERT, our acoustic model is built with the learned prosodic representations as intermediate variables. Experimental results demonstrate that the naturalness of speech synthesized using S4LPR is significantly better than the FastSpeech2 baseline",
    "checked": true,
    "id": "c0a927f459171b0a15114a7a812996563964e91a",
    "semantic_title": "speech synthesis with self-supervisedly learnt prosodic representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tang23_interspeech.html": {
    "title": "EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis",
    "volume": "main",
    "abstract": "There has been significant progress in emotional Text-To-Speech (TTS) synthesis technology in recent years. However, existing methods primarily focus on the synthesis of a limited number of emotion types and have achieved unsatisfactory performance in intensity control. To address these limitations, we propose EmoMix, which can generate emotional speech with specified intensity or a mixture of emotions. Specifically, EmoMix is a controllable emotional TTS model based on a diffusion probabilistic model and a pre-trained speech emotion recognition (SER) model used to extract emotion embedding. Mixed emotion synthesis is achieved by combining the noises predicted by diffusion model conditioned on different emotions during only one sampling process at the run-time. We further apply the Neutral and specific primary emotion mixed in varying degrees to control intensity. Experimental results validate the effectiveness of EmoMix for synthesizing mixed emotion and intensity control",
    "checked": true,
    "id": "5e635e749a90022f5b3704a8fb1c6b48645519cc",
    "semantic_title": "emomix: emotion mixing via diffusion models for emotional speech synthesis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23b_interspeech.html": {
    "title": "Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus",
    "volume": "main",
    "abstract": "We present a large-scale in-the-wild Japanese laughter corpus and a laughter synthesis method. Previous work on laughter synthesis lacks not only data but also proper ways to represent laughter. To solve these problems, we first propose an in-the-wild corpus comprising 3.5 hours of laughter, which is to our best knowledge the largest laughter corpus designed for laughter synthesis. We then propose pseudo phonetic tokens (PPTs) to represent laughter by a sequence of discrete tokens, which are obtained by training a clustering model on features extracted from laughter by a pretrained self-supervised model. Laughter can then be synthesized by feeding PPTs into a text-to-speech system. We further show PPTs can be used to train a language model for unconditional laughter generation. Results of comprehensive subjective and objective evaluations demonstrate that the proposed method significantly outperforms a baseline method, and can generate natural laughter unconditionally",
    "checked": true,
    "id": "20f7cc755d832c664ee5a84b46a30c27b92a6ac5",
    "semantic_title": "laughter synthesis using pseudo phonetic tokens with a large-scale in-the-wild laughter corpus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23u_interspeech.html": {
    "title": "Explicit Intensity Control for Accented Text-to-speech",
    "volume": "main",
    "abstract": "Accented text-to-speech (TTS) synthesis seeks to generate speech with an accent (L2) as a variant of the standard version (L1). How to control the intensity of accent is a very interesting research direction. Recent works design a speaker-adversarial loss to disentangle the speaker and accent information, and then adjust the loss weight to control the accent intensity. However, there is no direct correlation between the disentanglement factor and natural accent intensity. To this end, this paper proposes a new intuitive and explicit accent intensity control scheme for accented TTS. Specifically, we first extract the posterior probability from the L1 speech recognition model to quantify the phoneme accent intensity for accented speech, then design a FastSpeech2 based TTS model, named Ai-TTS, to take the accent intensity expression into account during speech generation. Experiments show that our method outperforms the baseline model in terms of accent rendering and intensity control",
    "checked": true,
    "id": "d354de4a4c182d294ffba7256e08bf6c4767d642",
    "semantic_title": "explicit intensity control for accented text-to-speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23o_interspeech.html": {
    "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "volume": "main",
    "abstract": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models",
    "checked": true,
    "id": "f3cff86b41ccbb4dd533e6d115b82f34d2d8a04c",
    "semantic_title": "comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/duquenne23_interspeech.html": {
    "title": "Modular Speech-to-Text Translation for Zero-Shot Cross-Modal Transfer",
    "volume": "main",
    "abstract": "Recent research has shown that independently trained encoders and decoders, combined through a shared fixed-size representation, can achieve competitive performance in speech-to-text translation. In this work, we show that this type of approach can be further improved with multilingual training. We observe significant improvements in zero-shot cross-modal speech translation, even outperforming a supervised approach based on XLSR for several languages",
    "checked": true,
    "id": "39b4255a439d2aa85c683935cd47314d098fecf3",
    "semantic_title": "modular speech-to-text translation for zero-shot cross-modal transfer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pal23_interspeech.html": {
    "title": "Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters",
    "volume": "main",
    "abstract": "To translate speech for automatic dubbing, machine translation needs to be isochronous, i.e. translated speech needs to be aligned with the source in terms of speech durations. We introduce target factors in a transformer model to predict durations jointly with target language phoneme sequences. We also introduce auxiliary counters to help the decoder to keep track of the timing information while generating target phonemes. We show that our model improves translation quality and isochrony compared to previous work where the translation model is instead trained to predict interleaved sequences of phonemes and durations",
    "checked": true,
    "id": "38872b9e3f937287012bac3c66ab5df0c084a726",
    "semantic_title": "improving isochronous machine translation with target factors and auxiliary counters",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23_interspeech.html": {
    "title": "StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation",
    "volume": "main",
    "abstract": "Direct speech-to-speech translation (S2ST) has gradually become popular as it has many advantages compared with cascade S2ST. However, current research mainly focuses on the accuracy of semantic translation and ignores the speech style transfer from a source language to a target language. The lack of high-fidelity expressive parallel data makes such style transfer challenging, especially in more practical zero-shot scenarios. To solve this problem, we first build a parallel corpus using a multi-lingual multi-speaker text-to-speech synthesis (TTS) system and then propose the StyleS2ST model with cross-lingual speech style transfer ability based on a style adaptor on a direct S2ST system framework. Enabling continuous style space modeling of an acoustic model through parallel corpus training and non-parallel TTS data augmentation, StyleS2ST captures cross-lingual acoustic feature mapping from the source to the target language. Experiments show that StyleS2ST achieves good style similarity and naturalness in both in-set and out-of-set zero-shot scenarios",
    "checked": true,
    "id": "9dd2e6b5076cb5e2325e6a3f40977228b473904b",
    "semantic_title": "styles2st: zero-shot style transfer for direct speech-to-speech translation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gaido23_interspeech.html": {
    "title": "Joint Speech Translation and Named Entity Recognition",
    "volume": "main",
    "abstract": "Modern automatic translation systems aim at supporting the users by providing contextual knowledge. In this framework, a critical task is the output enrichment with information regarding the mentioned entities. This is currently achieved by processing the generated translations with named entity recognition (NER) tools and retrieving their description from knowledge bases. In light of the recent promising results shown by direct speech translation (ST) models and the known weaknesses of cascades (error propagation and additional latency), in this paper we propose multitask models that jointly perform ST and NER, and compare them with a cascade baseline. Experimental results on three language pairs (en-es/fr/it) show that our models significantly outperform the cascade on the NER task (by 0.4-1.0 F1), without degradation in terms of translation quality, and with the same computational efficiency of a plain direct ST model",
    "checked": true,
    "id": "563f203f7efc841dacd6e7801256e8d6f2578509",
    "semantic_title": "joint speech translation and named entity recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sant23_interspeech.html": {
    "title": "Analysis of Acoustic information in End-to-End Spoken Language Translation",
    "volume": "main",
    "abstract": "End-to-End Transformer-based models are the most popular approach for Spoken Language Translation (SLT). While obtaining state-of-the-art results, we are still far from understanding how these models extract acoustic information from the data and how they are transformed into semantic representations. In this paper, we seek to provide a better understanding of the flow of acoustic information along speech-to-text translation models. By means of the Speaker Classification and Spectrogram Reconstruction tasks, this study (i) interprets the main role of the encoder with respect to the acoustic features, (ii) highlights the importance of the acoustic information throughout the model and its transfer between encoder and decoder, and (iii) reveals the significant effect of downsampling convolutional layers for learning acoustic features. (iv) Finally, we also observe the existence of a strong correlation between the semantic domain and the speakers' labels in MuST-C",
    "checked": true,
    "id": "89dc1dd817b0af8184fb55e2b819d0fbcbdfad17",
    "semantic_title": "analysis of acoustic information in end-to-end spoken language translation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23oa_interspeech.html": {
    "title": "LAMASSU: A Streaming Language-Agnostic Multilingual Speech Recognition and Translation Model Using Neural Transducers",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) and speech translation (ST) can both use neural transducers as the model structure. It is thus possible to use a single transducer model to perform both tasks. In real-world applications, such joint ASR and ST models may need to be streaming and do not require source language identification (i.e. language-agnostic). In this paper, we propose LAMASSU, a streaming language-agnostic multilingual speech recognition and translation model using neural transducers. Based on the transducer model structure, we propose four methods, a unified joint and prediction network for multilingual output, a clustered multilingual encoder, target language identification for encoder, and connectionist temporal classification regularization. Experimental results show that LAMASSU not only drastically reduces the model size but also reaches the performances of monolingual ASR and bilingual ST models",
    "checked": true,
    "id": "41fa401b3ed0b20168c9ad19471c248f1fe9b00c",
    "semantic_title": "lamassu: a streaming language-agnostic multilingual speech recognition and translation model using neural transducers",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23c_interspeech.html": {
    "title": "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available",
    "checked": true,
    "id": "e4f2d75856ce149b994f079ae50fd33ca47245d3",
    "semantic_title": "dphubert: joint distillation and pruning of self-supervised speech models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zaiem23_interspeech.html": {
    "title": "Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases",
    "checked": true,
    "id": "6801e61b2fb45b660fab6d287cae5e23cd6b76dd",
    "semantic_title": "automatic data augmentation for domain adapted fine-tuning of self-supervised speech representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23e_interspeech.html": {
    "title": "Dual Acoustic Linguistic Self-supervised Representation Learning for Cross-Domain Speech Recognition",
    "volume": "main",
    "abstract": "The integration of well-pre-trained acoustic and linguistic representations boosts the performance of speech-to-text cross-modality tasks. However, the potential of fine-tuning cross-modality integrated model on accented and noisy corpus is still under-explored. To address this gap, we propose an end-to-end acoustic and linguistic integrated representation learning model, namely Dual-w2v-BART. Our model incorporates acoustic representations from wav2vec2.0 and linguistic information from BART model by utilizing the cross-attention mechanism in the decoder, with paired speech-text dual inputs. To enhance model robustness on accent and noise, we propose a text-centric representation consistency component that helps to gain the similarity between different modality inputs while representing the same content. The results on accented and noisy speech recognition tasks demonstrate the effectiveness of the proposed model for reducing error rates compared to baseline and other competitive models",
    "checked": true,
    "id": "d5111d4769b60fa9940dda15146af3c7959c2cee",
    "semantic_title": "dual acoustic linguistic self-supervised representation learning for cross-domain speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baskar23_interspeech.html": {
    "title": "O-1: Self-training with Oracle and 1-best Hypothesis",
    "volume": "main",
    "abstract": "We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80% relative compared to EMBR which bridges the gap by 43% relative. O-1 achieves 13% to 25% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23d_interspeech.html": {
    "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets",
    "volume": "main",
    "abstract": "In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend",
    "checked": true,
    "id": "d2451c2cce0d44e3d390aa09059a8a0e5369c223",
    "semantic_title": "mt4ssl: boosting self-supervised speech representation learning by integrating multiple targets",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lamyeemui23_interspeech.html": {
    "title": "Comparing Self-Supervised Pre-Training and Semi-Supervised Training for Speech Recognition in Languages with Weak Language Models",
    "volume": "main",
    "abstract": "This paper investigates the potential of improving a hybrid automatic speech recognition model trained on 10 hours of transcribed data with 200 hours of untranscribed data in low-resource languages. First, we compare baseline methods of cross-lingual transfer with MFCC features and features extracted with the multilingual self-supervised model XLSR-53. Subsequently, we compare two approaches that can leverage the untranscribed data: semi-supervised training with LF-MMI and continued self-supervised pre-training of XLSR-53. Our results on well-resourced English broadcast data derived from MGB show that both methods achieve 18% and 27% relative improvements compared to the baseline, respectively. On the low-resource South African Soap Opera dataset, the relative improvement with semi-supervised training is only 3% due to the inherently weak language model. However, continued pre-training achieves 8.6% relative improvement because it does not rely on any external information",
    "checked": true,
    "id": "0e8e5d938af4fa0d114ae155421681e0755a649e",
    "semantic_title": "comparing self-supervised pre-training and semi-supervised training for speech recognition in languages with weak language models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23da_interspeech.html": {
    "title": "Chinese EFL Learners' Perception of English Prosodic Focus",
    "volume": "main",
    "abstract": "Focus in a sentence can be realized prosodically in speech communication. It has been found not easy for L2 learners to acquire. The present study examines Chinese learners' perception of English prosodic focus, specifically the effects of learners' English proficiency, intonation type, sentence length, and focus location on the perceptual accuracy of English prosodic focus by Chinese EFL learners. Results of two trials in the perception experiment reveal that focus location, intonation type, and English proficiency significantly impacted Chinese learners' perceptual accuracy of both single focus and dual focus in English. Focus in statements was perceived more accurately than that in questions for both single focus and dual focus. Focus located on sentence-final words in questions was perceived more accurately than that on non-final words in questions. Learners' English proficiency positively correlated to the accuracy of focus perception, especially for dual focus",
    "checked": true,
    "id": "e85507287ca0daa9bd73a0a82182c8212e6d0249",
    "semantic_title": "chinese efl learners' perception of english prosodic focus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sostarics23_interspeech.html": {
    "title": "Pitch Accent Variation and the Interpretation of Rising and Falling Intonation in American English",
    "volume": "main",
    "abstract": "This study tests the division of labor in the meaning conveyed by pitch accents and edge tones in English intonation. In three perception studies, we investigate where the locus of the contrast between an assertive vs inquisitive interpretation resides. By doing so, we also gain insight into the role of potentially meaningful within- and between-category variation in the phonetic implementation of discrete intonational tunes. We find that the pitch accent does not contribute to assertive interpretation. Rather, the distinction between assertive and inquisitive interpretation is cued primarily by the final F0 of the pitch contour regardless of the pitch accent, but that increased overall pitch prominence may trigger a salient focus interpretation that interferes with judging assertiveness",
    "checked": true,
    "id": "d47244483903120338e389ac8db57617477d73d5",
    "semantic_title": "pitch accent variation and the interpretation of rising and falling intonation in american english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kuang23_interspeech.html": {
    "title": "Tonal coarticulation as a cue for upcoming prosodic boundary",
    "volume": "main",
    "abstract": "It has been established that the lack of tonal coarticulation or pitch reset is a salient cue for the beginning of a large prosodic domain, however, it is yet unclear whether tonal coarticulation can be an informative cue for the end of a prosodic domain. We examined this question with two continuous speech corpora of Mandarin, and both expert and crowd-sourced perceptual annotations were used. The FPCA model of the holistic tonal contours shows that the carry-over effect of the preceding tone is significantly affected by the strength of the following boundaries. Stronger carry-over effects are associated with the end of larger prosodic boundaries. Moreover, machine learning classification shows that the fine-grained tonal coarticulation patterns are salient cues for predicting larger prosodic boundaries. This result is further validated by crowd-sourced boundary perceptual ratings from human listeners. This study has important implications for the understanding of prosodic phrasing",
    "checked": true,
    "id": "84c4d2832913419041c3d8d6b63c6e7cd8abc176",
    "semantic_title": "tonal coarticulation as a cue for upcoming prosodic boundary",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/repp23_interspeech.html": {
    "title": "Alignment of Beat Gestures and Prosodic Prominence in German",
    "volume": "main",
    "abstract": "We present evidence on the alignment of beat gestures and prosodic prominence from a video corpus consisting of six German educational videos for students from six presenters. Our analysis of 120 beat gestures (with a substantial variety of hand shapes) shows that beat gestures almost always align with prosodically prominent syllables, i.e., syllables carrying a pitch accent. Specifically, the stroke always starts before, or - more often - on, a pitch-accented syllable; the apex mostly falls on the accented syllable (74%) but may also occur in subsequent syllables. The degree of prosodic prominence of the accented syllable (in terms of DIMA-prominence levels) is predictive for the position of the apex, which occurs within rather than after the accented syllable more often for higher degrees of prominence. These findings provide new insights into the alignment of prominence-lending features of prosody and gesture, thereby broadening the empirical landscape for beat gestures",
    "checked": true,
    "id": "5293c0b253dc03cbff04cd19fb28c865a0fcba14",
    "semantic_title": "alignment of beat gestures and prosodic prominence in german",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/white23_interspeech.html": {
    "title": "Creak Prevalence and Prosodic Context in Australian English",
    "volume": "main",
    "abstract": "Creaky voice has been found to mark phrase-finality in many varieties of English, as well as in other languages. The present study aims to investigate whether this is also true for Australian English (AusE), a variety that is understudied in creaky voice research. Using automatic creak detection methods, the need for manual annotation of creak is reduced, and we are able to analyse a large dataset of Australian teenagers' speech. As in other varieties, creak is found to be a marker of finality in AusE. Additionally, we find that males use higher rates of creaky voice than females, challenging the widely held assumption that creak is a feature of female speech",
    "checked": true,
    "id": "6ba65b1ad194c04f21ea2b05088ccb1d92aa432d",
    "semantic_title": "creak prevalence and prosodic context in australian english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bodur23_interspeech.html": {
    "title": "Speech reduction: position within French prosodic structure",
    "volume": "main",
    "abstract": "Variation in the speech signal is a characteristic of spoken language, emerging partially as a result of interactions between various linguistic levels. One example of variation is phonetic reduction, where words are produced with missing or underspecified phonetic forms. Using a French conversational corpus, this paper focuses on the relationship between reduction and prosodic structure to see whether certain positions favor the occurrence of reduction. We annotated and observed the distribution of reduced sequences within specific prosodic domains (Intonational and Accentual Phrases). Preliminary analyses revealed that the detected reductions occur mostly mid- IP and very rarely at IP-final. However, this pattern may vary among speakers, as speakers have different patterns in terms of the number of reductions produced and their positions. It is also usually the case that the reduced sequences occurring mid-IP, coincide with the AP level boundaries, extending from one AP to another",
    "checked": true,
    "id": "a21240a74af5b918c91e05a0ebae2292b35ff2f5",
    "semantic_title": "speech reduction: position within french prosodic structure",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23d_interspeech.html": {
    "title": "Transvelar Nasal Coupling Contributing to Speaker Characteristics in Non-nasal Vowels",
    "volume": "main",
    "abstract": "Nasal-cavity structure is stable in speech and varied across speakers, which potentially gives rise to speaker characteristics. Many studies have reported the acoustic contribution of the nasal cavity for nasal and nasalized sounds with velopharyngeal port opening. However, nasal-cavity resonance does emerge in non-nasal vowels through transvelar nasal coupling, which results in non-negligible modifications to non-nasal vowel spectra. In this study, nasal and oral output sounds were separately recorded during non-nasal utterances, and spectral analysis was conducted. The results indicate clear inter-speaker variability in two spectral measures below 2 kHz: frequency location of double-peaked first nasal-cavity resonance and inconsistent distribution of minor dips above the first resonance. It was also observed that nostril outputs modulate oral output signals to lower the first formant frequency of naturally produced non-low vowels, which also exhibited varied degrees across speakers",
    "checked": true,
    "id": "d6c08360cc77b37e108194d8af38d40d7b98de26",
    "semantic_title": "transvelar nasal coupling contributing to speaker characteristics in non-nasal vowels",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/otani23_interspeech.html": {
    "title": "Speech Synthesis from Articulatory Movements Recorded by Real-time MRI",
    "volume": "main",
    "abstract": "Previous speech synthesis models from articulatory movements recorded using real-time MRI (rtMRI) only predicted vocal tract shape parameters and required additional pitch information to generate a speech waveform. This study proposes a two-stage deep learning model composed of CNN-BiLSTM that predicts a mel-spectrogram from a rtMRI video and a HiFi-GAN vocoder that synthesizes a speech waveform. We evaluated our model on two databases: the ATR 503 sentences rtMRI database and the USC-TIMIT database. The experimental results on the ATR 503 sentences rtMRI database show that the PESQ score and the RMSE of F0 are 1.64 and 26.7 Hz. This demonstrates that all acoustic parameters, including fundamental frequency, can be estimated from the rtMRI videos. In the experiment on the USC-TIMIT database, we obtained a good PESQ score and RMSE for F0. However, the synthesized speech is unclear, indicating that the quality of the datasets affects the intelligibility of the synthesized speech",
    "checked": true,
    "id": "977f2f760d0eb99d73182fd8334c1986989c5b07",
    "semantic_title": "speech synthesis from articulatory movements recorded by real-time mri",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23b_interspeech.html": {
    "title": "The ART of Conversation: Measuring Phonetic Convergence and Deliberate Imitation in L2-Speech with a Siamese RNN",
    "volume": "main",
    "abstract": "Phonetic convergence describes the automatic and unconscious speech adaptation of two interlocutors in a conversation. This paper proposes a Siamese recurrent neural network (RNN) architecture to measure the convergence of the holistic spectral characteristics of speech sounds in an L2-L2 interaction. We extend an alternating reading task (the ART) dataset by adding 20 native Slovak L2 English speakers. We train and test the Siamese RNN model to measure phonetic convergence of L2 English speech from three different native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10 dyads). Our results indicate that the Siamese RNN model effectively captures the dynamics of phonetic convergence and the speaker's imitation ability. Moreover, this text-independent model is scalable and capable of handling L1-induced speaker variability",
    "checked": true,
    "id": "2b9b85a451ee43149db92918bda991886295374b",
    "semantic_title": "the art of conversation: measuring phonetic convergence and deliberate imitation in l2-speech with a siamese rnn",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mahshie23_interspeech.html": {
    "title": "Did you see that? Exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants",
    "volume": "main",
    "abstract": "This project aimed to explore the potential role of vision in speech contrast production and auditory perception development in children with cochlear implants (CWCI). Ten CWCI between 43 and 61 months of age, with at least 2 years of CI experience, served as participants. Employing an auditory imitation task, children's ability to auditorily perceive contrasts that are more or less visible was examined both at baseline and one year after the initial assessment. The children's ability to produce these contrasts was also examined through a picture-naming task. The CWCI tended to produce features in both visibility conditions with greater accuracy than they perceived, both at baseline and at 1 year. Production and perception accuracy increased after one year of CI usage, with the mean perceptual gain for the more visible contrasts exceeding that of the less visible contrasts. The implications of the role of vision in contrast development are discussed",
    "checked": true,
    "id": "81ec5fa89978dde59b20c0c580c79f9a37cd3d3e",
    "semantic_title": "did you see that? exploring the role of vision in the development of consonant feature contrasts in children with cochlear implants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vanbemmel23_interspeech.html": {
    "title": "Automatic assessments of dysarthric speech: the usability of acoustic-phonetic features",
    "volume": "main",
    "abstract": "Individuals with dysarthria suffer from difficulties in speech production and consequent reductions in speech intelligibility, which is an important concept for diagnosing and assessing effectiveness of speech therapy. In the current study, we investigate which acoustic-phonetic features are most relevant and important in automatically assessing intelligibility and in classifying speech as healthy or dysarthric. After feature selection, we applied a stepwise linear regression to predict intelligibility ratings and a Linear Discriminant Analysis to classify healthy and dysarthric speech. We observed a very strong correlation between actual and predicted intelligibility ratings in the regression analysis. We also observed a high classification accuracy of 98.06% by using 17 features and a comparable, high accuracy of 96.11% with only two features. These results indicate the usefulness of the acoustic-phonetic features in automatic assessments of dysarthric speech",
    "checked": true,
    "id": "8174cc3c98bb4fe0af86fd75c3dc000a54a39489",
    "semantic_title": "automatic assessments of dysarthric speech: the usability of acoustic-phonetic features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/venkatathirumalakumar23_interspeech.html": {
    "title": "Classification of Multi-class Vowels and Fricatives From Patients Having Amyotrophic Lateral Sclerosis with Varied Levels of Dysarthria Severity",
    "volume": "main",
    "abstract": "Dysarthria due to Amyotrophic Lateral Sclerosis (ALS) progressively distorts the acoustic space affecting the discriminability of different vowels and fricatives. However, the extent to which this happens with increasing severity is not thoroughly investigated. In this work, we perform automatic 4-class vowel (/a/, /i/, /o/, /u/) and 3-class fricative (/s/, /sh/, /f/) classification at varied severity levels and compare the performances with those from manual classification (through listening tests). Experiments with speech data from 119 ALS and 40 healthy subjects suggest that the manual and automatic classification accuracies reduce with an increase in dysarthria severity reaching 59.22% and 61.67% for vowels and 41.78% and 38.00% for fricatives, respectively, at the most severe cases. While manual classification is better than automatic one for all severity levels except the highest severity case for vowels, the difference between the two gradually reduces with an increase in severity",
    "checked": true,
    "id": "5eb48172191d12d829d3d4646b9d600cfb3751c5",
    "semantic_title": "classification of multi-class vowels and fricatives from patients having amyotrophic lateral sclerosis with varied levels of dysarthria severity",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qi23b_interspeech.html": {
    "title": "Parameter-efficient Dysarthric Speech Recognition Using Adapter Fusion and Householder Transformation",
    "volume": "main",
    "abstract": "In dysarthric speech recognition, data scarcity and the vast diversity between dysarthric speakers pose significant challenges. While finetuning has been a popular solution, it can lead to overfitting and low parameter efficiency. Adapter modules offer a better solution, with their small size and easy applicability. Additionally, Adapter Fusion can facilitate knowledge transfer from multiple learned adapters, but may employ more parameters. In this work, we apply Adapter Fusion for target speaker adaptation and speech recognition, achieving acceptable accuracy with significantly fewer speaker-specific trainable parameters than classical finetuning methods. We further improve the parameter efficiency of the fusion layer by reducing the size of query and key layers and using Householder transformation to reparameterize the value linear layer. Our proposed fusion layer achieves comparable recognition results to the original method with only one third of the parameters",
    "checked": true,
    "id": "c8c29732ed33b019854bbeb56e7ad8c707c6f270",
    "semantic_title": "parameter-efficient dysarthric speech recognition using adapter fusion and householder transformation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hermann23_interspeech.html": {
    "title": "Few-shot Dysarthric Speech Recognition with Text-to-Speech Data Augmentation",
    "volume": "main",
    "abstract": "Speakers with dysarthria could particularly benefit from assistive speech technology, but are underserved by current automatic speech recognition (ASR) systems. The differences of dysarthric speech pose challenges, while recording large amounts of training data can be exhausting for patients. In this paper, we synthesise dysarthric speech with a FastSpeech 2-based multi-speaker text-to-speech (TTS) system for ASR data augmentation. We evaluate its few-shot capability by generating dysarthric speech with as few as 5 words from an unseen target speaker and then using it to train speaker-dependent ASR systems. The results indicated that, while the TTS output is not yet of sufficient quality, this could allow easy development of personalised acoustic models for new dysarthric speakers and domains in the future",
    "checked": true,
    "id": "4692b33dca3e8dd24bef1ce0e2ba1a8f2acc2e44",
    "semantic_title": "few-shot dysarthric speech recognition with text-to-speech data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yee23_interspeech.html": {
    "title": "Latent Phrase Matching for Dysarthric Speech",
    "volume": "main",
    "abstract": "Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies has emphasized interest in designing and improving personalized speech models for atypical speech. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases",
    "checked": true,
    "id": "d757a58200254625c3326a32a1da6fa8eaa2eff3",
    "semantic_title": "latent phrase matching for dysarthric speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeo23_interspeech.html": {
    "title": "Speech Intelligibility Assessment of Dysarthric Speech by using Goodness of Pronunciation with Uncertainty Quantification",
    "volume": "main",
    "abstract": "This paper proposes an improved Goodness of Pronunciation (GoP) that utilizes Uncertainty Quantification (UQ) for automatic speech intelligibility assessment for dysarthric speech. Current GoP methods rely heavily on neural network-driven overconfident predictions, which is unsuitable for assessing dysarthric speech due to its significant acoustic differences from healthy speech. To alleviate the problem, UQ techniques were used on GoP by 1) normalizing the phoneme prediction (entropy, margin, maxlogit, logit-margin) and 2) modifying the scoring function (scaling, prior normalization). As a result, prior-normalized maxlogit GoP achieves the best performance, with a relative increase of 5.66%, 3.91%, and 23.65% compared to the baseline GoP for English, Korean, and Tamil, respectively. Furthermore, phoneme analysis is conducted to identify which phoneme scores significantly correlate with intelligibility scores in each language",
    "checked": true,
    "id": "4879abd5687f59bc4123ebe5c0de84f94f6d8583",
    "semantic_title": "speech intelligibility assessment of dysarthric speech by using goodness of pronunciation with uncertainty quantification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23c_interspeech.html": {
    "title": "CQNV: A Combination of Coarsely Quantized Bitstream and Neural Vocoder for Low Rate Speech Coding",
    "volume": "main",
    "abstract": "Recently, speech codecs based on neural networks have proven to perform better than traditional methods. However, redundancy in traditional parameter quantization is visible within the codec architecture of combining the traditional codec with the neural vocoder. In this paper, we propose a novel framework named CQNV, which combines the coarsely quantized parameters of a traditional parametric codec to reduce the bitrate with a neural vocoder to improve the quality of the decoded speech. Furthermore, we introduce a parameters processing module into the neural vocoder to enhance the application of the bitstream of traditional speech coding parameters to the neural vocoder, further improving the reconstructed speech's quality. In the experiments, both subjective and objective evaluations demonstrate the effectiveness of the proposed CQNV framework. Specifically, our proposed method can achieve higher quality reconstructed speech at 1.1 kbps than Lyra and Encodec at 3 kbps",
    "checked": true,
    "id": "79ccc49646b09821403ae4f0d76dace82b19db22",
    "semantic_title": "cqnv: a combination of coarsely quantized bitstream and neural vocoder for low rate speech coding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kamo23_interspeech.html": {
    "title": "Target Speech Extraction with Conditional Diffusion Model",
    "volume": "main",
    "abstract": "Diffusion model-based speech enhancement has received increased attention since it can generate very natural enhanced signals and generalizes well to unseen conditions. Diffusion models have been explored for several sub-tasks of speech enhancement, such as speech denoising, dereverberation, and source separation. In this paper, we investigate their use for target speech extraction (TSE), which consists of estimating the clean speech signal of a target speaker in a mixture of multi-talkers. TSE is realized by conditioning the extraction process on a clue identifying the target speaker. We show we can realize TSE using a conditional diffusion model conditioned on the clue. Besides, we introduce ensemble inference to reduce potential extraction errors caused by the diffusion process. In experiments on Libri2mix corpus, we show that the proposed diffusion model-based TSE combined with ensemble inference outperforms a comparable TSE system trained discriminatively",
    "checked": true,
    "id": "cb8a0a610ddfd79794603b4729c501a662d461f0",
    "semantic_title": "target speech extraction with conditional diffusion model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cohen23_interspeech.html": {
    "title": "Towards Fully Quantized Neural Networks For Speech Enhancement",
    "volume": "main",
    "abstract": "Deep learning models have shown state-of-the-art results in speech enhancement. However, deploying such models on an eight-bit integer-only device is challenging. In this work, we analyze the gaps in deploying a vanilla quantization-aware training method for speech enhancement, revealing two significant observations. First, quantization mainly affects signals with a high input Signal-to-Noise Ratio (SNR). Second, quantizing the model's input and output shows major performance degradation. Based on our analysis, we propose Fully Quantized Speech Enhancement (FQSE), a new quantization-aware training method that closes these gaps and enables eight-bit integer-only quantization. FQSE introduces data augmentation to mitigate the quantization effect on high SNR. Additionally, we add an input splitter and a residual quantization block to the model to overcome the error of the input-output quantization. We show that FQSE closes the performance gaps induced by eight-bit quantization",
    "checked": true,
    "id": "e85c275be3f166e3208ed6a981c9e90ce4498f93",
    "semantic_title": "towards fully quantized neural networks for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23p_interspeech.html": {
    "title": "Complex Image Generation SwinTransformer Network for Audio Denoising",
    "volume": "main",
    "abstract": "Achieving high-performance audio denoising is still a challenging task in real-world applications. Existing time-frequency methods often ignore the quality of generated frequency domain images. This paper converts the audio denoising problem into an image generation task. We first develop a complex image generation SwinTransformer network to capture more information from the complex Fourier domain. We then impose structure similarity and detailed loss functions to generate high-quality images and develop an SDR loss to minimize the difference between denoised and clean audios. Extensive experiments on two benchmark datasets demonstrate that our proposed model is better than state-of-the-art methods",
    "checked": true,
    "id": "34e95df66dc9419ea92343fce9b10f0ccf24b687",
    "semantic_title": "complex image generation swintransformer network for audio denoising",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/blau23_interspeech.html": {
    "title": "Using Text Injection to Improve Recognition of Personal Identifiers in Speech",
    "volume": "main",
    "abstract": "Accurate recognition of specific categories, such as persons' names, dates or other identifiers is critical in many Automatic Speech Recognition (ASR) applications. As these categories represent personal information, ethical use of this data including collection, transcription, training and evaluation demands special care. One way of ensuring the security and privacy of individuals is to redact or eliminate Personally Identifiable Information (PII) from collection altogether. However, this results in ASR models that tend to have lower recognition accuracy of these categories. We use text-injection to improve the recognition of PII categories by including fake textual substitutes of PII categories in the training data using a text injection method. We demonstrate substantial improvement to Recall of Names and Dates in medical notes while improving overall WER. For alphanumeric digit sequences we show improvements to Character Error Rate and Sentence Accuracy",
    "checked": true,
    "id": "645e9909180d729a069b71f7c52750b534e28a83",
    "semantic_title": "using text injection to improve recognition of personal identifiers in speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grosz23_interspeech.html": {
    "title": "Investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a Finnish model",
    "volume": "main",
    "abstract": "Self-supervised speech models, such as the wav2vec2, have become extremely popular in the past few years. Their main appeal is that after their pre-training on a large amount of audio, they require only a small amount of supervised, finetuning data to achieve outstanding results. Despite their immense success, very little is understood about the pre-trained models and how finetuning changes them. In this work, we take the first steps towards a better understanding of wav2vec2 systems using model interpretation tools such as visualization and latent embedding clustering. Through our analysis, we gain new insights into the abilities of the pre-trained networks and the effect that finetuning has on them. We demonstrate that the clusters learned by the pre-trained model are just as important a factor as the supervised training data distribution in determining the accuracy of the finetuned system, which could aid us in selecting the most suitable pre-trained model for the supervised data",
    "checked": true,
    "id": "2ae665744fc5662c89168f14f3153144936cdd37",
    "semantic_title": "investigating wav2vec2 context representations and the effects of fine-tuning, a case-study of a finnish model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lehecka23_interspeech.html": {
    "title": "Transformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech",
    "volume": "main",
    "abstract": "This paper is a step forward in our effort to make vast oral history archives more accessible to the public and researchers by breaking down the decoding barriers between the knowledge encoded in the spoken testimonies and users who want to search for the information of their interest. We present new Transformer-based monolingual models suitable for speech recognition of oral history archives in English, German, and Czech. Our experiments show that although the all-purpose speech recognition systems have recently made tremendous progress, the transcription of oral history archives is still a challenging task for them; our tailored models significantly outperformed larger public multilingual models and scored new state-of-the-art results on all tested datasets. Due to the 2-phase fine-tuning process, our models are robust and can be used for oral history archives of various domains. We publicly release our models within a public speech recognition service",
    "checked": true,
    "id": "d26d8f4c79120558aa4f8131df408af091aff001",
    "semantic_title": "transformer-based speech recognition models for oral history archives in english, german, and czech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23_interspeech.html": {
    "title": "Iteratively Improving Speech Recognition and Voice Conversion",
    "volume": "main",
    "abstract": "Many existing works on voice conversion (VC) tasks use automatic speech recognition (ASR) models for ensuring linguistic consistency between source and converted samples. However, for the low-data resource domains, training a high-quality ASR remains to be a challenging task. In this work, we propose a novel iterative way of improving both the ASR and VC models. We first train an ASR model which is used to ensure content preservation while training a VC model. In the next iteration, the VC model is used as a data augmentation method to further fine-tune the ASR model and generalize it to diverse speakers. By iteratively leveraging the improved ASR model to train VC model and vice-versa, we experimentally show improvement in both the models. Our proposed framework outperforms the ASR and one-shot VC baseline models on English singing and Hindi speech domains in subjective and objective evaluations in low-data resource settings",
    "checked": true,
    "id": "e9f0e92d7a18cc78b71146d67358fcfd1a6bdc4d",
    "semantic_title": "iteratively improving speech recognition and voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fatehi23_interspeech.html": {
    "title": "LABERT: A Combination of Local Aggregation and Self-Supervised Speech Representation Learning for Detecting Informative Hidden Units in Low-Resource ASR Systems",
    "volume": "main",
    "abstract": "With advances in deep learning methodologies, Automatic Speech Recognition (ASR) systems have seen impressive results. However, ASR in Low-Resource Environments (LREs) are challenged by a lack of training data for the specific target domain. We propose that data sampling criteria for choosing more informative speech samples can be critical to addressing the problem of training data bottleneck. Our proposed Local Aggregation BERT (LABERT) method for self-supervised speech representation learning fuses an active learning model with an adapted local aggregation metric. Active learning is used to pick informative speech units, whereas the aggregation metric forces the model to move similar data together in the latent space while separating dissimilar instances to detect hidden units in LRE tasks. We evaluate LABERT with two LRE datasets: I-CUBE and UASpeech to explore the performance of our model in the LRE ASR problems",
    "checked": true,
    "id": "046609639dcd941f5438bde0eb73481f60411735",
    "semantic_title": "labert: a combination of local aggregation and self-supervised speech representation learning for detecting informative hidden units in low-resource asr systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xue23_interspeech.html": {
    "title": "TranUSR: Phoneme-to-word Transcoder Based Unified Speech Representation Learning for Cross-lingual Speech Recognition",
    "volume": "main",
    "abstract": "UniSpeech has achieved superior performance in cross-lingual automatic speech recognition (ASR) by explicitly aligning latent representations to phoneme units using multi-task self-supervised learning. While the learned representations transfer well from high-resource to low-resource languages, predicting words directly from these phonetic representations in downstream ASR is challenging. In this paper, we propose TranUSR, a two-stage model comprising a pre-trained UniData2vec and a phoneme-to-word Transcoder. Different from UniSpeech, UniData2vec replaces the quantized discrete representations with continuous and contextual representations from a teacher model for phonetically-aware pre-training. Then, Transcoder learns to translate phonemes to words with the aid of extra texts, enabling direct word generation. Experiments on Common Voice show that UniData2vec reduces PER by 5.3% compared to UniSpeech, while Transcoder yields a 14.4% WER reduction compared to grapheme fine-tuning",
    "checked": true,
    "id": "c03ec12dd7bd78123cd4b7959d6eacd0d4b2ae7b",
    "semantic_title": "tranusr: phoneme-to-word transcoder based unified speech representation learning for cross-lingual speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23e_interspeech.html": {
    "title": "Dual-Mode NAM: Effective Top-K Context Injection for End-to-End ASR",
    "volume": "main",
    "abstract": "ASR systems in real applications must be adapted on the fly to correctly recognize task-specific contextual terms, such as contacts, application names and media entities. However, it is challenging to achieve scalability, large in-domain quality gains, and minimal out-of-domain quality regressions simultaneously. In this work, we introduce an effective neural biasing architecture called Dual-Mode NAM. Dual-Mode NAM embeds a top-k search process in its attention mechanism in a trainable fashion to perform an accurate top-k phrase selection before injecting the corresponding word-piece context into the acoustic encoder. We further propose a controllable mechanism to enable the ASR system to be able to trade off its in-domain and out-of-domain quality at inference time. When evaluated on a large-scale biasing benchmark, the combined techniques improve a previously proposed method with an average in-domain and out-of-domain WER reduction by up to 53.3% and 12.0% relative respectively",
    "checked": true,
    "id": "7e39237918eb9b5c0b3baa66b174a268c684a975",
    "semantic_title": "dual-mode nam: effective top-k context injection for end-to-end asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23g_interspeech.html": {
    "title": "GhostRNN: Reducing State Redundancy in RNN with Cheap Operations",
    "volume": "main",
    "abstract": "Recurrent neural network (RNNs) that are capable of modeling long-distance dependencies are widely used in various speech tasks, eg., keyword spotting (KWS) and speech enhancement (SE). Due to the limitation of power and memory in low-resource devices, efficient RNN models are urgently required for real-world applications. In this paper, we propose an efficient RNN architecture, GhostRNN, which reduces hidden state redundancy with cheap operations. In particular, we observe that partial dimensions of hidden states are similar to the others in trained RNN models, suggesting that redundancy exists in specific RNNs. To reduce the redundancy and hence computational cost, we propose to first generate a few intrinsic states, and then apply cheap operations to produce ghost states based on the intrinsic states. Experiments on KWS and SE tasks demonstrate that the proposed GhostRNN significantly reduces the memory usage (~40%) and computation cost while keeping performance similar. Codes will be available at https://gitee.com/mindspore/models/tree/master/research/audio/ghostrnn",
    "checked": true,
    "id": "2ca7637abf82a22f345829ff6a6e065c67a3925f",
    "semantic_title": "ghostrnn: reducing state redundancy in rnn with cheap operations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23da_interspeech.html": {
    "title": "Task-Agnostic Structured Pruning of Speech Representation Models",
    "volume": "main",
    "abstract": "Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have been shown to significantly improve many speech tasks. However, their large memory and strong computational requirements hinder their industrial applicability. Structured pruning is a hardware-friendly model compression technique but usually results in a larger loss of accuracy. In this paper, we propose a fine-grained attention head pruning method to compensate for the performance degradation. In addition, we also introduce the straight through estimator into the L0 regularization to further accelerate the pruned model. Experiments on the SUPERB benchmark show that our model can achieve comparable performance to the dense model in multiple tasks and outperforms the Wav2vec 2.0 base model on average, with 72% fewer parameters and 2 times faster inference speed",
    "checked": true,
    "id": "99aa56c8136a83756c4b8d901941c5bb2b2dcdac",
    "semantic_title": "task-agnostic structured pruning of speech representation models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kanda23_interspeech.html": {
    "title": "Factual Consistency Oriented Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents a novel optimization framework for automatic speech recognition (ASR) with the aim of reducing hallucinations produced by an ASR model. The proposed framework optimizes the ASR model to maximize an expected factual consistency score between ASR hypotheses and ground-truth transcriptions, where the factual consistency score is computed by a separately trained estimator. Experimental results using the AMI meeting corpus and the VoxPopuli corpus show that the ASR model trained with the proposed framework generates ASR hypotheses that have significantly higher consistency scores with ground-truth transcriptions while maintaining the word error rates close to those of cross entropy-trained ASR models. Furthermore, it is shown that training the ASR models with the proposed framework improves the speech summarization quality as measured by the factual consistency of meeting conversation summaries generated by a large language model",
    "checked": true,
    "id": "979a68d069fe7b41105085e9c6182da5058665b6",
    "semantic_title": "factual consistency oriented speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fathullah23_interspeech.html": {
    "title": "Multi-Head State Space Model for Speech Recognition",
    "volume": "main",
    "abstract": "State space models (SSMs) have recently shown promising results on small-scale sequence and language modelling tasks, rivalling and outperforming many attention-based approaches. In this paper, we propose a multi-head state space (MH-SSM) architecture equipped with special gating mechanisms, where parallel heads are taught to learn local and global temporal dynamics on sequence data. As a drop-in replacement for multi-head attention in transformer encoders, this new model significantly outperforms the transformer transducer on the LibriSpeech speech recognition corpus. Furthermore, we augment the transformer block with MH-SSMs layers, referred to as the Stateformer, achieving state-of-the-art performance on the LibriSpeech task, with word error rates of 1.76%/4.37% on the development and 1.91%/4.36% on the test sets without using an external language model",
    "checked": true,
    "id": "067aaf0d1cde4ee21063be137559f2fe50125570",
    "semantic_title": "multi-head state space model for speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23_interspeech.html": {
    "title": "Cascaded Multi-task Adaptive Learning Based on Neural Architecture Search",
    "volume": "main",
    "abstract": "Cascading multiple pre-trained models is an effective way to compose an end-to-end system. However, fine-tuning the full cascaded model is parameter and memory inefficient and our observations reveal that only applying adapter modules on cascaded model can not achieve considerable performance as fine-tuning. We propose an automatic and effective adaptive learning method to optimize end-to-end cascaded multi-task models based on Neural Architecture Search (NAS) framework. The candidate adaptive operations on each specific module consist of frozen, inserting an adapter and fine-tuning. We further add a penalty item on the loss to limit the learned structure which takes the amount of trainable parameters into account. The penalty item successfully restrict the searched architecture and the proposed approach is able to search similar tuning scheme with hand-craft, compressing the optimizing parameters to 8.7% corresponding to full fine-tuning on SLURP with an even better performance",
    "checked": true,
    "id": "9c607471820fe74572e142fc6e9ce432716048c8",
    "semantic_title": "cascaded multi-task adaptive learning based on neural architecture search",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martin23_interspeech.html": {
    "title": "Probing Self-supervised Speech Models for Phonetic and Phonemic Information: A Case Study in Aspiration",
    "volume": "main",
    "abstract": "Textless self-supervised speech models have grown in capabilities in recent years, but the nature of the linguistic information they encode has not yet been thoroughly examined. We evaluate the extent to which these models' learned representations align with basic representational distinctions made by humans, focusing on a set of phonetic (low-level) and phonemic (more abstract) contrasts instantiated in word-initial stops. We find that robust representations of both phonetic and phonemic distinctions emerge in early layers of these models' architectures, and are preserved in the principal components of deeper layer representations. Our analyses suggest two sources for this success: some can only be explained by the optimization of the models on speech data, while some can be attributed to these models' high-dimensional architectures. Our findings show that speech-trained HuBERT derives a low-noise and low-dimensional subspace corresponding to abstract phonological distinctions",
    "checked": true,
    "id": "7373477778b12d453191b09d46e2f77f4295ca52",
    "semantic_title": "probing self-supervised speech models for phonetic and phonemic information: a case study in aspiration",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/harding23_interspeech.html": {
    "title": "Selective Biasing with Trie-based Contextual Adapters for Personalised Speech Recognition using Neural Transducers",
    "volume": "main",
    "abstract": "Neural transducer ASR models achieve state of the art accuracy on many tasks, however rare word recognition poses a particular challenge as models often fail to recognise words that occur rarely, or not at all, in the training data. Methods of contextual biasing, where models are dynamically adapted to bias their outputs towards a given list of relevant words and phrases, have been shown to be effective at alleviating this issue. While such methods are effective at improving rare word recognition, over-biasing can lead to degradation on common words. In this work we propose several extensions to a recently proposed trie-based method of contextual biasing. We show how performance of the method can be improved in terms of rare word recognition, especially in the case of very large catalogues, by introducing a simple normalisation term, how the method can be trained as an adapter module, and how selective biasing can be applied to practically eliminate over-biasing on common words",
    "checked": true,
    "id": "c261ebb5eec148522963b9c6bdd958e463ebcc2c",
    "semantic_title": "selective biasing with trie-based contextual adapters for personalised speech recognition using neural transducers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23b_interspeech.html": {
    "title": "Robust Prototype Learning for Anomalous Sound Detection",
    "volume": "main",
    "abstract": "In this paper, we present a robust prototype learning framework for anomalous sound detection (ASD), where prototypical loss is exploited to measure the similarity between samples and prototypes. We show that existing generative and discriminative based ASD methods can be unified into this framework from the perspective of prototypical learning. For ASD in recent DCASE challenges, extensions related to imbalanced learning are proposed to improve the robustness of prototypes learned from source and target domains. Specifically, balanced sampling and multiple-prototype expansion (MPE) strategies are proposed to address imbalances across attributes of source and target domains. Furthermore, a novel negative-prototype expansion (NPE) method is used to construct pseudo-anomalies to learn a more compact and effective embedding space for normal sounds. Evaluation on the DCASE2022 Task2 development dataset demonstrates the validity of the proposed prototype learning framework",
    "checked": true,
    "id": "d6a1b187a270900853a4a89c0dcc6ae3c0a2830e",
    "semantic_title": "robust prototype learning for anomalous sound detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kushwaha23_interspeech.html": {
    "title": "A multimodal prototypical approach for unsupervised sound classification",
    "volume": "main",
    "abstract": "In the context of environmental sound classification, the adaptability of systems is key: which sound classes are interesting depends on the context and the user's needs. Recent advances in text-to-audio retrieval allow for zero-shot audio classification, but performance compared to supervised models remains limited. This work proposes a multimodal prototypical approach that exploits local audio-text embeddings to provide more relevant answers to audio queries, augmenting the adaptability of sound detection in the wild. We do this by first using text to query a nearby community of audio embeddings that best characterize each query sound, and select the group's centroids as our prototypes. Second, we compare unseen audio to these prototypes for classification. We perform multiple ablation studies to understand the impact of the embedding models and prompts. Our unsupervised approach improves upon the zero-shot state-of-the-art in three sound recognition benchmarks by an average of 12%",
    "checked": true,
    "id": "fb082d89b1f8906509991f738961e1cbe21d7435",
    "semantic_title": "a multimodal prototypical approach for unsupervised sound classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wen23_interspeech.html": {
    "title": "Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms",
    "volume": "main",
    "abstract": "Robust audio anti-spoofing has been increasingly challeng- ing due to the recent advancements on deepfake techniques. While spectrograms have demonstrated their capability for anti- spoofing, complementary information presented in multi-order spectral patterns have not been well explored, which limits their effectiveness for varying spoofing attacks. Therefore, we propose a novel deep learning method with a spectral fusion- reconstruction strategy, namely S2pecNet, to utilise multi-order spectral patterns for robust audio anti-spoofing representations. Specifically, spectral patterns up to second-order are fused in a coarse-to-fine manner and two branches are designed for the fine-level fusion from the spectral and temporal contexts. A reconstruction from the fused representation to the input spec- trograms further reduces the potential fused information loss. Our method achieved the state-of-the-art performance with an EER of 0.77% on a widely used dataset - ASVspoof2019 LA Challenge",
    "checked": true,
    "id": "9f8e5fa471adab4938b3145fcb41b79c22ec2f0b",
    "semantic_title": "robust audio anti-spoofing with fusion-reconstruction learning on multi-order spectrograms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23c_interspeech.html": {
    "title": "Adapting Language-Audio Models as Few-Shot Audio Learners",
    "volume": "main",
    "abstract": "Contrastive language-audio pretraining (CLAP) has become a new paradigm to learn audio concepts with audio-text pairs. CLAP models have shown unprecedented performance as zero-shot classifiers on downstream tasks. To further adapt CLAP with domain-specific knowledge, a popular method is to finetune its audio encoder with available labelled examples. However, this is challenging in low-shot scenarios, as the amount of annotations is limited compared to the model size. In this work, we introduce a Training-efficient (Treff) adapter to rapidly learn with a small set of examples while maintaining the capacity for zero-shot classification. First, we propose a cross-attention linear model (CALM) to map a set of labelled examples and test audio to test labels. Second, we find initialising CALM as a cosine measurement improves our Treff adapter even without training. The Treff adapter beats metric-based methods in few-shot settings and yields competitive results to fully-supervised methods",
    "checked": true,
    "id": "237032b6256087766e6d366a47227aef980fd2b7",
    "semantic_title": "adapting language-audio models as few-shot audio learners",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23l_interspeech.html": {
    "title": "TFECN: Time-Frequency Enhanced ConvNet for Audio Classification",
    "volume": "main",
    "abstract": "Recently, transformer-based models have shown leading performance in audio classification, gradually replacing the dominant ConvNet in the past. However, some research has shown that certain characteristics and designs in transformers can be applied to other architectures and make them achieve similar performance as transformers. In this paper, we introduce TFECN, a pure ConvNet that combines the design in transformers and has time-frequency enhanced convolution with large kernels. It can provide a global receptive field on the frequency dimension as well as avoid the influence of the convolution's shift-equivariance on the recognition of not shift-invariant patterns along the frequency axis. Furthermore, to use ImageNet-pretrained weights, we propose a method for transferring weights between kernels of different sizes. On the commonly used datasets AudioSet, FSD50K, and ESC50, our TFECN outperforms the models trained in the same way",
    "checked": true,
    "id": "8cb1a3348c7004f0d3c4aa666edd09532da0b4db",
    "semantic_title": "tfecn: time-frequency enhanced convnet for audio classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23b_interspeech.html": {
    "title": "Resolution Consistency Training on Time-Frequency Domain for Semi-Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "The fact that unlabeled data can be used for supervised learning is of considerable relevance concerning polyphonic sound event detection (PSED) because of the high costs of frame-wise labeling. While semi-supervised learning (SSL) for image tasks has been extensively developed, SSL for PSED has not been substantially explored due to data augmentation limitations. In this paper, we propose a novel SSL strategy for PSED called resolution consistency training (ResCT), combining unsupervised terms with the mean teacher using different resolutions of a spectrogram for data augmentation. The proposed method regularizes the consistency between the model predictions for different resolutions by controlling the sampling rate and window size. Experimental results show that ResCT outperforms other SSL methods on various evaluation metrics: event-f1 score, intersection-f1 score, and PSDSs. Finally, we report on some ablation studies for the weak and strong augmentation policies",
    "checked": true,
    "id": "24624c92335e516840fe5f362d34ab272c66f78b",
    "semantic_title": "resolution consistency training on time-frequency domain for semi-supervised sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23n_interspeech.html": {
    "title": "Fine-tuning Audio Spectrogram Transformer with Task-aware Adapters for Sound Event Detection",
    "volume": "main",
    "abstract": "In this paper, we present a task-aware fine-tuning method to transfer Patchout faSt Spectrogram Transformer (PaSST) model to sound event detection (SED) task. Pretrained PaSST has shown significant performance on audio tagging (AT) and SED tasks, but it is not optimal to fine-tune the model from a single layer as the local and semantic information have not been well exploited. To address this, we first introduce task-aware adapters including SED-adapter and AT-adapter to fine-tune PaSST for SED and AT task respectively, and then propose task-aware fine-tuning to combine local information from shallower layer with semantic information from deeper layer, based on task-aware adapters. Besides, we propose the self-distillated mean teacher (SdMT) to train a robust student model with soft pseudo labels from teacher. Experiments are conducted on DCASE2022 task4 development set, the EB-F1 of 64.85% and PSDS1 of 0.5548 are achieved which outperform previous state-of-the-art systems",
    "checked": true,
    "id": "5035bf01ae576ab415d822284e629eb3734c6708",
    "semantic_title": "fine-tuning audio spectrogram transformer with task-aware adapters for sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23b_interspeech.html": {
    "title": "Small Footprint Multi-channel Network for Keyword Spotting with Centroid Based Awareness",
    "volume": "main",
    "abstract": "Spoken Keyword Spotting (KWS) in noisy far-field environments is challenging for small-footprint models, given the restrictions on computational resources (e.g., model size, running memory). This is even more intricate when handling noises from multiple microphones. To address this, we present a new multi-channel model that uses a CNN-based network with a linear mixing unit to achieve local-global dependency representations. Our method enhances noise-robustness while ensuring more efficient computation. Besides, we propose an end-to-end centroid-based awareness module that provides class similarity awareness at the bottleneck level to correct ambiguous cases during prediction. We conducted experiments using real noisy far-field data from the MISP challenge 2021 and achieved SOTA results compared to existing small-footprint KWS models. Our best score of 0.126 is highly competitive against larger models like 3D-ResNet, which is 0.122, but ours is much smaller at 473K compared to 13M",
    "checked": true,
    "id": "ca0781a2185f29b95cb0d7b627911a3e4162c975",
    "semantic_title": "small footprint multi-channel network for keyword spotting with centroid based awareness",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23b_interspeech.html": {
    "title": "Few-shot Class-incremental Audio Classification Using Adaptively-refined Prototypes",
    "volume": "main",
    "abstract": "New classes of sounds constantly emerge with a few samples, making it challenging for models to adapt to dynamic acoustic environments. This challenge motivates us to address the new problem of few-shot class-incremental audio classification. This study aims to enable a model to continuously recognize new classes of sounds with a few training samples of new classes while remembering the learned ones. To this end, we propose a method to generate discriminative prototypes and use them to expand the model's classifier for recognizing sounds of new and learned classes. The model is first trained with a random episodic training strategy, and then its backbone is used to generate the prototypes. A dynamic relation projection module refines the prototypes to enhance their discriminability. Results on two datasets (derived from the corpora of Nsynth and FSD-MIX-CLIPS) show that the proposed method exceeds three state-of-the-art methods in average accuracy and performance dropping rate",
    "checked": true,
    "id": "178677fe7e568509249e657635287bf6285ba00e",
    "semantic_title": "few-shot class-incremental audio classification using adaptively-refined prototypes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vali23_interspeech.html": {
    "title": "Interpretable Latent Space Using Space-Filling Curves for Phonetic Analysis in Voice Conversion",
    "volume": "main",
    "abstract": "Vector quantized variational autoencoders (VQ-VAE) are well-known deep generative models, which map input data to a latent space that is used for data generation. Such latent spaces are unstructured and can thus be difficult to interpret. Some earlier approaches have introduced a structure to the latent space through supervised learning by defining data labels as latent variables. In contrast, we propose an unsupervised technique incorporating space-filling curves into vector quantization (VQ), which yields an arranged form of latent vectors such that adjacent elements in the VQ codebook refer to similar content. We applied this technique to the latent codebook vectors of a VQ-VAE, which encode the phonetic information of a speech signal in a voice conversion task. Our experiments show there is a clear arrangement in latent vectors representing speech phones, which clarifies what phone each latent vector corresponds to and facilitates other detailed interpretations of latent vectors",
    "checked": true,
    "id": "4f3e14e1d39ca360c76239dde618ea44500ed98a",
    "semantic_title": "interpretable latent space using space-filling curves for phonetic analysis in voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tulchinskii23_interspeech.html": {
    "title": "Topological Data Analysis for Speech Processing",
    "volume": "main",
    "abstract": "We apply topological data analysis (TDA) to speech classification problems and to the introspection of a pretrained speech model, HuBERT. To this end, we introduce a number of topological and algebraic features derived from Transformer attention maps and embeddings. We show that a simple linear classifier built on top of such features outperforms a fine-tuned classification head. We achieve an improvement of about 9% accuracy and 5% ERR on two common datasets; on CREMA-D, the proposed feature set reaches a new state of the art performance with accuracy 80.155. We also show that topological features are able to reveal functional roles of speech Transformer heads; e.g., we find the heads capable to distinguish between pairs of sample sources (natural/synthetic) or voices without any downstream fine-tuning. Our results demonstrate that TDA is a promising new approach for speech analysis, especially for tasks that require structural prediction",
    "checked": true,
    "id": "12c37eaa9ab37d66e1b014fd79b62bf544522065",
    "semantic_title": "topological data analysis for speech processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jang23_interspeech.html": {
    "title": "Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation",
    "volume": "main",
    "abstract": "Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark",
    "checked": true,
    "id": "d65dd690635d6a220360b2193e6d020da24330c2",
    "semantic_title": "recycle-and-distill: universal compression strategy for transformer-based speech ssl models with attention map reusing and masking distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koppelmann23_interspeech.html": {
    "title": "Personalized Acoustic Scene Classification in Ultra-low Power Embedded Devices Using Privacy-preserving Data Augmentation",
    "volume": "main",
    "abstract": "In this work we present an adaptation method for personalized acoustic scene classification in ultra-low power embedded devices (EDs). The computational limitation of EDs and a large variety of acoustic scenes may lead to poor performance of the embedded classifier in specific real-world user environments. We propose a semi-supervised scheme that estimates the audio feature distribution at ED level and then samples this statistical model to generate artificial data points which emulate user-specific audio features. Then, a second, cloud-based classifier assigns pseudo labels to samples, which are merged with existing labeled data for retraining the embedded classifier. The proposed method leads to significant performance improvements on user-specific data sets and does neither require a persistent connection to a cloud service nor the transmission of raw audio or audio features. It thus results in low data rates, high utility, and privacy-preservation",
    "checked": true,
    "id": "0e85062bba41b628de2d96cac8d77d03191e6a5c",
    "semantic_title": "personalized acoustic scene classification in ultra-low power embedded devices using privacy-preserving data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23_interspeech.html": {
    "title": "Background Domain Switch: A Novel Data Augmentation Technique for Robust Sound Event Detection",
    "volume": "main",
    "abstract": "Data augmentation is a key component to achieve robust and generalizable performance in sound event detection (SED). A well trained SED model should be able to resist the interference of non-target audio events and maintain a robust recognition rate under unknown and possibly mismatched testing conditions. In this study, we propose a novel background domain switch (BDS) data augmentation technique for SED. BDS utilizes a trained SED model on-the-fly to detect backgrounds in audio clips, and switches them among the data points to increase sample variability. This approach can be easily combined with other types of data augmentation techniques. We evaluate the effectiveness of BDS by applying it to several state-of-the-art SED frameworks, and used both publicly available datasets as well as a synthesized mismatched test set. Experiment results systematically show that BDS obtains significant performance improvements from all evaluation aspects. The code is available at: https://github.com/boschresearch/soundseebackgrounddomainswitch",
    "checked": true,
    "id": "cfcf644b1805c24fdbc9e6cddfac41383a2f0d8f",
    "semantic_title": "background domain switch: a novel data augmentation technique for robust sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hou23_interspeech.html": {
    "title": "Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning",
    "volume": "main",
    "abstract": "Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans' listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR",
    "checked": true,
    "id": "e74522543b25c3eca86f7dd62793dbf8d527e8be",
    "semantic_title": "joint prediction of audio event and annoyance rating in an urban soundscape by hierarchical graph representation learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23fa_interspeech.html": {
    "title": "Anomalous Sound Detection Using Self-Attention-Based Frequency Pattern Analysis of Machine Sounds",
    "volume": "main",
    "abstract": "Different machines can exhibit diverse frequency patterns in their emitted sound. This feature has been recently explored in anomaly sound detection and reached state-of-the-art performance. However, existing methods rely on the manual or empirical determination of the frequency filter by observing the effective frequency range in the training data, which may be impractical for general application. This paper proposes an anomalous sound detection method using self-attention-based frequency pattern analysis and spectral-temporal information fusion. Our experiments demonstrate that the self-attention module automatically and adaptively analyses the effective frequencies of a machine sound and enhances that information in the spectral feature representation. With spectral-temporal information fusion, the obtained audio feature eventually improves the anomaly detection performance on the DCASE 2020 Challenge Task 2 dataset",
    "checked": true,
    "id": "5656e4b77179dbc012460460bf92334f45b9235a",
    "semantic_title": "anomalous sound detection using self-attention-based frequency pattern analysis of machine sounds",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23c_interspeech.html": {
    "title": "Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions",
    "volume": "main",
    "abstract": "Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained crossmodal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical crossmodal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets",
    "checked": true,
    "id": "29dc27e49654fc7cd0a9001bbf44a57d78bb74ca",
    "semantic_title": "improving audio-text retrieval via hierarchical cross-modal interaction and auxiliary captions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bn23_interspeech.html": {
    "title": "Differential Privacy enabled Dementia Classification: An Exploration of the Privacy-Accuracy Trade-off in Speech Signal Data",
    "volume": "main",
    "abstract": "Early detection of dementia is critical for effective symptom management. Recent studies have aimed to develop machine learning (ML) models to identify dementia onset and severity using language and speech features. However, existing methods can lead to serious privacy concerns due to sensitive data collected from a vulnerable population. In this work, we aim to establish the privacy-accuracy tradeoff benchmark for dementia classification models using audio and speech features. Specifically, we explore the effects of differential privacy (DP) on the training phase of the audio model. We then compare the classification accuracy of DP and non-DP models using a publicly available dataset. The resultant comparison provides useful insights to make informed decisions about the need for balancing privacy and accuracy tradeoff for dementia classification tasks. Our findings have implications for real-world deployment of ML models to support early detection and effective management of dementia",
    "checked": true,
    "id": "7304c74495f8c4630a852f614e8a613a3b0e410b",
    "semantic_title": "differential privacy enabled dementia classification: an exploration of the privacy-accuracy trade-off in speech signal data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ka_interspeech.html": {
    "title": "Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech",
    "volume": "main",
    "abstract": "Effective speech emotional representations play a key role in Speech Emotion Recognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional speech samples are more difficult and expensive to acquire compared with Neutral style speech, which causes one issue that most related works unfortunately neglect: imbalanced datasets. Models might overfit to the majority Neutral class and fail to produce robust and effective emotional representations. In this paper, we propose an Emotion Extractor to address this issue. We use augmentation approaches to train the model and enable it to extract effective and generalizable emotional representations from imbalanced datasets. Our empirical results show that (1) for the SER task, the proposed Emotion Extractor surpasses the state-of-the-art baseline on three imbalanced datasets; (2) the produced representations from our Emotion Extractor benefit the TTS model, and enable it to synthesize more expressive speech",
    "checked": true,
    "id": "044e47bc8995d603122963d92b84aa6dabea1a53",
    "semantic_title": "learning emotional representations from imbalanced speech data for speech emotion recognition and emotional text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/behera23_interspeech.html": {
    "title": "Towards Multi-Lingual Audio Question Answering",
    "volume": "main",
    "abstract": "Audio Question Answering (AQA) is a multi-modal translation task where a system analyzes an audio signal and a natural language question to generate a desirable natural language answer. AQA has been primarily studied through the lens of the English language. However, addressing AQA in other languages, in the same manner, would require a considerable amount of resources. This paper proposes scalable solutions to multi-lingual audio question answering on both data and modeling fronts. We propose mClothoAQA, a translation-based multi-lingual AQA dataset in eight languages. The dataset consists of 1991 audio files and nearly 0.3 million question-answer pairs. Finally, we introduce a multi-lingual AQA model and demonstrate its strong performance in eight languages. The dataset and code can be accessed at https://github.com/swarupbehera/mAQA",
    "checked": true,
    "id": "431ec63f76d5d64c18e3ab92008cca98975a1678",
    "semantic_title": "towards multi-lingual audio question answering",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/aldarmaki23_interspeech.html": {
    "title": "Diacritic Recognition Performance in Arabic ASR",
    "volume": "main",
    "abstract": "In Arabic text, most vowels are encoded in the form of diacritics that are often omitted, so most speech corpora and ASR models are undiacritized. Text-based diacritization has previously been used to preprocess the input or post-processs ASR hypotheses. It is generally believed that input diacritization degrades ASR quality, but no systematic evaluation of ASR diacritization performance has been conducted to date. We experimentally clarify whether input diacritiztation indeed degrades ASR quality and compare ASR diacritization with text-based diacritization. We fine-tune pre-trained ASR models on transcribed speech with different diacritization conditions: manual, automatic, and no diacritization. We isolate diacritic recognition performance from the overall ASR performance using coverage and precision metrics. We find that ASR diacritization significantly outperforms text-based diacritization, particularly when the ASR model is fine-tuned with manually diacritized transcripts",
    "checked": true,
    "id": "cd31445ba366d10bd11e2a7b2da5c8281c8f1564",
    "semantic_title": "diacritic recognition performance in arabic asr",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kolehmainen23_interspeech.html": {
    "title": "Personalization for BERT-based Discriminative Speech Recognition Rescoring",
    "volume": "main",
    "abstract": "Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/krishnan23_interspeech.html": {
    "title": "On the N-gram Approximation of Pre-trained Language Models",
    "volume": "main",
    "abstract": "Large pre-trained language models (PLMs) have shown remarkable performance across various natural language understanding (NLU) tasks, particularly in low-resource settings. Nevertheless, their potential in Automatic Speech Recognition (ASR) remains largely unexplored. This study investigates the potential usage of PLMs for language modelling in ASR. We compare the application of large-scale text sampling and probability conversion for approximating GPT-2 into an n-gram model. Furthermore, we introduce a vocabulary-restricted decoding method for random sampling, and evaluate the effects of domain difficulty and data size on the usability of generated text. Our findings across eight domain-specific corpora support the use of sampling-based approximation and show that interpolating with a large sampled corpus improves test perplexity over a baseline trigram by 15%. Our vocabulary-restricted decoding method pushes this improvement further by 5% in domain-specific settings",
    "checked": true,
    "id": "821d918fd7c316a1d48579979fa7bde2f7a63c50",
    "semantic_title": "on the n-gram approximation of pre-trained language models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23g_interspeech.html": {
    "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
    "volume": "main",
    "abstract": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers' true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction",
    "checked": true,
    "id": "b3c7d778e5bdafa8b2cd62b996d0e6dfc670effc",
    "semantic_title": "record deduplication for entity distribution modeling in asr transcripts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/agrawal23_interspeech.html": {
    "title": "Learning When to Trust Which Teacher for Weakly Supervised ASR",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) training can utilize multiple experts as teacher models, each trained on a specific domain or accent. Teacher models may be opaque in nature since their architecture may be not be known or their training cadence is different from that of the student ASR model. Still, the student models are updated incrementally using the pseudo-labels generated independently by the expert teachers. In this paper, we exploit supervision from multiple domain experts in training student ASR models. This training strategy is especially useful in scenarios where few or no human transcriptions are available. To that end, we propose a Smart-Weighter mechanism that selects an appropriate expert based on the input audio, and then trains the student model in an unsupervised setting. We show the efficacy of our approach using LibriSpeech and LibriLight benchmarks and find an improvement of 4 to 25% over baselines that uniformly weight all the experts, use a single expert model, or combine experts using ROVER",
    "checked": true,
    "id": "124c16b1243ef8f727df9db70d3c2f1b47ec31c1",
    "semantic_title": "learning when to trust which teacher for weakly supervised asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23f_interspeech.html": {
    "title": "Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer",
    "volume": "main",
    "abstract": "Domain adaptation using text-only corpus is challenging in end-to-end(E2E) speech recognition. Adaptation by synthesizing audio from text through TTS is resource-consuming. We present a method to learn Unified Speech-Text Representation in Conformer Transducer(USTR-CT) to enable fast domain adaptation using the text-only corpus. Different from the previous textogram method, an extra text encoder is introduced in our work to learn text representation and is removed during inference, so there is no modification for online deployment. To improve the efficiency of adaptation, single-step and multistep adaptations are also explored. The experiments on adapting LibriSpeech to SPGISpeech show the proposed method reduces the word error rate(WER) by relatively 44% on the target domain, which is better than those of TTS method and textogram method. Also, it is shown the proposed method can be combined with internal language model estimation(ILME) to further improve the performance",
    "checked": true,
    "id": "f035b38c98d79dd43dda10b919604b1d46cb63df",
    "semantic_title": "text-only domain adaptation using unified speech-text representation in transducer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23e_interspeech.html": {
    "title": "Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model",
    "volume": "main",
    "abstract": "In this paper, we show that representations capturing syllabic units emerge when training a self-supervised speech model with a visually-grounded training objective. We demonstrate that a nearly identical model architecture (HuBErT) trained with a masked language modeling loss does not exhibit this same ability, suggesting that the visual grounding objective is responsible for the emergence of this phenomenon. We propose the use of a minimum cut algorithm to automatically predict syllable boundaries in speech, followed by a 2-stage clustering method to group identical syllables together. We show that our model not only outperforms a state-of-the-art syllabic segmentation method on the language it was trained on (English), but also generalizes in a zero-shot fashion to Estonian. Finally, we show that the same model is capable of zero-shot generalization for a word segmentation task on 4 other languages from the Zerospeech Challenge, in some cases beating the previous state-of-the-art",
    "checked": true,
    "id": "47ba6504f14a3d16f25b1b9afaf5531e41671faf",
    "semantic_title": "syllable discovery and cross-lingual generalization in a visually grounded, self-supervised speech model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23d_interspeech.html": {
    "title": "Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization",
    "volume": "main",
    "abstract": "We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. we design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available here",
    "checked": true,
    "id": "10e8dc07ea256c6a88d7043cf135417402ed38f4",
    "semantic_title": "prompting the hidden talent of web-scale speech models for zero-shot task generalization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moore23_interspeech.html": {
    "title": "Progress and Prospects for Spoken Language Technology: Results from Five Sexennial Surveys",
    "volume": "main",
    "abstract": "Every six years (since 1997), a survey has been conducted at the IEEE workshop on Automatic Speech Recognition and Understanding (ASRU). The aim has been to gain an insight into the research community's perspective on the 'progress and prospects' for spoken language technology. These surveys have been based on a set of 'statements' describing possible scenarios, and respondents are asked to estimate the year (in the future or in the past) when each given scenario might be realised. A number of the statements have appeared in multiple surveys, hence it has been possible to track changes in opinion over time. This paper presents the combined results from five such surveys, the most recent of which was conducted at ASRU-2021. The latest results reveal a dramatic increase in optimism",
    "checked": true,
    "id": "9098d7a04eb5d8a1d64e57e808d240a426a16be1",
    "semantic_title": "progress and prospects for spoken language technology: results from five sexennial surveys",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sanabria23_interspeech.html": {
    "title": "Acoustic Word Embeddings for Untranscribed Target Languages with Continued Pretraining and Learned Pooling",
    "volume": "main",
    "abstract": "Acoustic word embeddings are typically created by training a pooling function using pairs of word-like units. For unsupervised systems, these are mined using k-nearest neighbor (KNN) search, which is slow. Recently, mean-pooled representations from a pre-trained self-supervised English model were suggested as a promising alternative, but their performance on target languages was not fully competitive. Here, we explore improvements to both approaches: we use continued pre-training to adapt the self-supervised model to the target language, and we use a multilingual phone recognizer (MPR) to mine phone n-gram pairs for training the pooling function. Evaluating on four languages, we show that both methods outperform a recent approach on word discrimination. Moreover, the MPR method is orders of magnitude faster than KNN, and is highly data efficient. We also show a small improvement from performing learned pooling on top of the continued pre-trained representations",
    "checked": true,
    "id": "2c82c551b151835cec78df7a2ab86f2a58d0a682",
    "semantic_title": "acoustic word embeddings for untranscribed target languages with continued pretraining and learned pooling",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23d_interspeech.html": {
    "title": "CASA-ASR: Context-Aware Speaker-Attributed ASR",
    "volume": "main",
    "abstract": "Recently, speaker-attributed automatic speech recognition (SAASR) has attracted a wide attention, which aims at answering the question \"who spoke what\". Different from modular systems, end-to-end (E2E) SA-ASR minimizes the speaker-dependent recognition errors directly and shows a promising applicability. In this paper, we propose a context-aware SAASR (CASA-ASR) model by enhancing the contextual modeling ability of E2E SA-ASR. Specifically, in CASA-ASR, a contextual text encoder is involved to aggregate the semantic information of the whole utterance, and a context-dependent scorer is employed to model the speaker discriminability by contrasting with speakers in the context. In addition, a two-pass decoding strategy is further proposed to fully leverage the contextual modeling ability resulting in a better recognition performance. Experimental results on AliMeeting corpus show that the proposed CASA-ASR model outperforms the original E2E SAASR system with a relative improvement of 11.76% in terms of speaker-dependent character error rate",
    "checked": true,
    "id": "eaf765c07764a802c5200a9abd739a921349caab",
    "semantic_title": "casa-asr: context-aware speaker-attributed asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/takahashi23_interspeech.html": {
    "title": "Unsupervised Learning of Discrete Latent Representations with Data-Adaptive Dimensionality from Continuous Speech Streams",
    "volume": "main",
    "abstract": "This work presents a novel deep generative model for unsupervised learning of sparse binary feature representations with data-adaptive dimensionality directly from continuous speech streams. Sharing the critical assumption of unbounded latent dimensionality with previously proposed Bayesian non-parametric approaches, our proposed model can capture the much richer, non-Markovian dependencies between its latent representations. The present work focuses on an investigation of our proposed model's performance in learning linguistically meaningful representations under challenging, realistic scenarios. We train our model with highly speaker-imbalanced datasets and evaluate it on the ABX phone discriminability test. Our model achieves a promising, competitive performance to the state-of-the-art model, despite its huge disadvantage: limited or no access to speaker information during training",
    "checked": true,
    "id": "1355151151f18fb3e98e65b57549ea2e87d5cf80",
    "semantic_title": "unsupervised learning of discrete latent representations with data-adaptive dimensionality from continuous speech streams",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23n_interspeech.html": {
    "title": "AD-TUNING: An Adaptive CHILD-TUNING Approach to Efficient Hyperparameter Optimization of Child Networks for Speech Processing Tasks in the SUPERB Benchmark",
    "volume": "main",
    "abstract": "In this paper, we propose AD-TUNING, an adaptive CHILD-TUNING approach for hyperparameter tuning of child networks. To address the issue of selecting an optimal hyperparameter set P, which often varies for different tasks in CHILD-TUNING, we first analyze the distribution of parameter importance to ascertain the range of P. Next, we propose a simple yet efficient early-stop algorithm to select the appropriate child network from different sizes for various speech tasks. When evaluated on seven speech processing tasks in the SUPERB benchmark, our proposed framework only requires fine-tuning less than 0.1%~10% of pre-trained model parameters for each task to achieve state-of-the-art results in most of the tasks. For instance, the DER of the speaker diarization task is 9.22% relatively lower than the previously reported best results. Other benchmark results are also very competitive. Our code is available at https://github.com/liyunlongaaa/AD-TUNING",
    "checked": true,
    "id": "8d3bbe7ded99577cf07844e63a58c51aaebe25e5",
    "semantic_title": "ad-tuning: an adaptive child-tuning approach to efficient hyperparameter optimization of child networks for speech processing tasks in the superb benchmark",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wong23_interspeech.html": {
    "title": "Distilling knowledge from Gaussian process teacher to neural network student",
    "volume": "main",
    "abstract": "Neural Networks (NN) and Gaussian Processes (GP) are different modelling approaches. The former stores characteristics of the training data in its many parameters, and then performs inference by parsing inputs through these parameters. The latter instead performs inference by computing a similarity between the test and training inputs, and then predicts test outputs that are correlated with the reference training outputs of similar inputs. These models may be combined to leverage upon their diversity. However, both combination and the matrix computations for GP inference are expensive. This paper investigates whether a NN student is able to effectively learn from the information distilled from a GP or ensemble teacher. It is computationally cheaper to infer using this student. Experiments on the speechocean762 spoken language assessment dataset suggest that learning is effective",
    "checked": true,
    "id": "480c5f9471a8822eacd04f2545b01f23539313f7",
    "semantic_title": "distilling knowledge from gaussian process teacher to neural network student",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhati23_interspeech.html": {
    "title": "Segmental SpeechCLIP: Utilizing Pretrained Image-text Models for Audio-Visual Learning",
    "volume": "main",
    "abstract": "Visually grounded models learn from paired images and their spoken captions. Recently, there have been attempts to utilize the visually grounded models trained from images and their corresponding text captions, such as CLIP, to improve speech-based visually grounded models' performance. However, the majority of these models only utilize the pretrained image encoder. Cascaded SpeechCLIP attempted to generate localized word-level information and utilize both the pretrained image and text encoders. Despite using both, they noticed a substantial drop in retrieval performance. Here, we propose to use a hierarchical segmental audio encoder that can generate a sequence of word-like units from audio. We use the pretrained CLIP text encoder on top of these word-like units representations and show significant improvements over the cascaded variant of SpeechCLIP",
    "checked": true,
    "id": "1617d389b7947161f2943e2d30afeb1856052b14",
    "semantic_title": "segmental speechclip: utilizing pretrained image-text models for audio-visual learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jacobs23_interspeech.html": {
    "title": "Towards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili",
    "volume": "main",
    "abstract": "We consider hate speech detection through keyword spotting on radio broadcasts. One approach is to build an automatic speech recognition (ASR) system for the target low-resource language. We compare this to using acoustic word embedding (AWE) models that map speech segments to a space where matching words have similar vectors. We specifically use a multilingual AWE model trained on labelled data from well-resourced languages to spot keywords in data in the unseen target language. In contrast to ASR, the AWE approach only requires a few keyword exemplars. In controlled experiments on Wolof and Swahili where training and test data are from the same domain, an ASR model trained on just five minutes of data outperforms the AWE approach. But in an in-the-wild test on Swahili radio broadcasts with actual hate speech keywords, the AWE model (using one minute of template data) is more robust, giving similar performance to an ASR system trained on 30 hours of labelled data",
    "checked": true,
    "id": "ce1c8b4655157435f87f9b4c5eb3589e64d3f0da",
    "semantic_title": "towards hate speech detection in low-resource languages: comparing asr to acoustic word embeddings on wolof and swahili",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vandermerwe23_interspeech.html": {
    "title": "Mitigating Catastrophic Forgetting for Few-Shot Spoken Word Classification Through Meta-Learning",
    "volume": "main",
    "abstract": "We consider the problem of few-shot spoken word classification in a setting where a model is incrementally introduced to new word classes. This would occur in a user-defined keyword system where new words can be added as the system is used. In such a continual learning scenario, a model might start to misclassify earlier words as newer classes are added, i.e. catastrophic forgetting. To address this, we propose an extension to model-agnostic meta-learning (MAML). In our new approach, each inner learning loop—where a model \"learns how to learn\" new classes—ends with a single gradient update using stored templates from all the classes that the model has already seen (one template per class). We compare this method to OML (another extension of MAML) in few-shot isolated-word classification experiments on Google Commands and FACC. Our method consistently outperforms OML in experiments where the number of shots and the final number of classes are varied",
    "checked": true,
    "id": "af849704754c3cb8ea621cca465c4c13372c9148",
    "semantic_title": "mitigating catastrophic forgetting for few-shot spoken word classification through meta-learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/polacek23_interspeech.html": {
    "title": "Online Punctuation Restoration using ELECTRA Model for streaming ASR Systems",
    "volume": "main",
    "abstract": "In this work, we propose a lightweight online approach to automatic punctuation restoration (APR), which can be utilized in speech transcription systems for, e.g., live captioning TV or radio streams. It uses only text input without prosodic features and a fine-tuned ELECTRA-Small model with a two-layer classification head. It allows for restoring question marks, commas, and periods with a very short inference time and a low latency of just three words. Our APR scheme is first tuned and compared to other architectures on a set of manual TV news transcripts. The resulting system is then compared to another real-time APR module utilizing a recurrent network and a combination of text and acoustic features. The test data we use contains automatic transcripts of radio talks and TV debates; we are also publishing this data. The results show that our APR module performs better than the above-mentioned system and yields on the two test sets an average F1 of 71.2% and 69.4%, respectively",
    "checked": true,
    "id": "01372fd6f9c099ea351050a99e547ebe2a36f746",
    "semantic_title": "online punctuation restoration using electra model for streaming asr systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23s_interspeech.html": {
    "title": "Language Agnostic Data-Driven Inverse Text Normalization",
    "volume": "main",
    "abstract": "The rise of automatic speech recognition (ASR) models has created an urgent need for converting spoken language into written text to provide better user experiences. This has drawn the attention of researchers, particularly for real-time on-device ASR deployment, towards the inverse text normalization (ITN) problem. While data-driven ITN methods have shown great promise in recent studies, the lack of labeled spoken-written datasets is hindering the development for non-English data-driven ITN. To bridge this gap, we propose a language-agnostic data-driven ITN framework that leverages data augmentation and neural machine translation specifically designed for real-time miniature models and low-resource languages. Additionally, we have developed an evaluation method for language-agnostic ITN models when only English data is available. Our empirical evaluation attests to the efficacy of this language-agnostic ITN modeling with data augmentation approach for multiple non-English languages",
    "checked": true,
    "id": "68f9e81413ea8bfbc10e88191983a7f4dc7b1b30",
    "semantic_title": "language agnostic data-driven inverse text normalization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23j_interspeech.html": {
    "title": "How to Estimate Model Transferability of Pre-Trained Speech Models?",
    "volume": "main",
    "abstract": "In this work, we introduce a ''score-based assessment'' framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks. We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations. Our framework efficiently computes transferability scores without actual fine-tuning of candidate models or layers by making a temporal independent hypothesis. We evaluate some popular supervised speech models (and self-supervised speech models in cross-layer and cross-model settings using public data. Experimental results show a high Spearman's rank correlation and low p-value between our estimation framework and fine-tuning ground truth. Our proposed transferability framework requires less computational time and resources, making it a resource-saving and time-efficient approach for tuning speech foundation models",
    "checked": true,
    "id": "81a6b9cba431287c46fc29148ccbf6e01bf52d30",
    "semantic_title": "how to estimate model transferability of pre-trained speech models?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ihori23_interspeech.html": {
    "title": "Transcribing Speech as Spoken and Written Dual Text Using an Autoregressive Model",
    "volume": "main",
    "abstract": "This paper proposes a novel method to jointly generate spoken and written text from input speech for expanding use cases of speech-based applications. The spoken text generated using speech-to-spoken text systems, i.e., speech recognition systems, has disfluencies and no punctuation marks. Thus, spoken text is often converted into written text using a spoken text-to-written text system. However, this cascading is unsuitable for overall optimization and computationally expensive. Although a speech-to-written-text system that directly outputs the written text from the speech is also developed, it cannot output the spoken text. To efficiently produce both spoken and written text from speech, our key advance is to handle a joint text of spoken and written texts in an autoregressive model. This enables us to correctly generate both spoken and written texts by considering their dependencies via a single decoding process. Our experiments demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "1ddf41cf13aeda48ce9a79df0eb08e8255bc233f",
    "semantic_title": "transcribing speech as spoken and written dual text using an autoregressive model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuksel23_interspeech.html": {
    "title": "NoRefER: a Referenceless Quality Metric for Automatic Speech Recognition via Semi-Supervised Language Model Fine-Tuning with Contrastive Learning",
    "volume": "main",
    "abstract": "This paper introduces NoRefER, a novel referenceless quality metric for automatic speech recognition (ASR) systems. Traditional reference-based metrics for evaluating ASR systems require costly ground-truth transcripts. NoRefER overcomes this limitation by fine-tuning a multilingual language model for pair-wise ranking ASR hypotheses using contrastive learning with Siamese network architecture. The self-supervised NoRefER exploits the known quality relationships between hypotheses from multiple compression levels of an ASR for learning to rank intra-sample hypotheses by quality, which is essential for model comparisons. The semi-supervised version also uses a referenced dataset to improve its inter-sample quality ranking, which is crucial for selecting potentially erroneous samples. The results indicate that NoRefER correlates highly with reference-based metrics and their intra-sample ranks, indicating a high potential for referenceless ASR evaluation or a/b testing",
    "checked": true,
    "id": "b99733670ef7c2553944208a16a68968cbb85946",
    "semantic_title": "norefer: a referenceless quality metric for automatic speech recognition via semi-supervised language model fine-tuning with contrastive learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23c_interspeech.html": {
    "title": "Scaling Laws for Discriminative Speech Recognition Rescoring Models",
    "volume": "main",
    "abstract": "Recent studies have found that model performance has a smooth power-law relationship, or scaling laws, with training data and model size, for a wide range of problems. These scaling laws allow one to choose nearly optimal data and model sizes. We study whether this scaling property is also applicable to second-pass rescoring, which is an important component of speech recognition systems. We focus on RescoreBERT as the rescoring model, which uses a pre-trained Transformer-based architecture fined tuned with an ASR discriminative loss. Using such a rescoring model, we show that the word error rate (WER) follows a scaling law for over two orders of magnitude as training data and model size increase. In addition, it is found that a pre-trained model would require less data than a randomly initialized model of the same size, representing effective data transferred from pre-training step. This effective data transferred is found to also follow a scaling law with the data and model size",
    "checked": true,
    "id": "a20c6ce80872dbc6a8e6403b2a366973061a9f89",
    "semantic_title": "scaling laws for discriminative speech recognition rescoring models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23w_interspeech.html": {
    "title": "Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition",
    "volume": "main",
    "abstract": "Energy-based language models (ELMs) parameterize an unnormalized distribution for natural sentences and are radically different from popular autoregressive language models (ALMs). As an important application, ELMs have been successfully used as a means for calculating sentence scores in speech recognition, but they all use less-modern CNN or LSTM networks. The recent progress in Transformer networks and large pretrained models such as BERT and GPT2 opens new possibility to further advancing ELMs. In this paper, we explore different architectures of energy functions and different training methods to investigate the capabilities of ELMs in rescoring for speech recognition, all using large pretrained models as backbones. Extensive experiments are conducted on two datasets, AISHELL-1 and WenetSpeech. The results show that the best ELM achieves competitive results with the finetuned GPT2 and performs significantly better than the finetuned BERT. Further analysis show that the ELM obtains better confidence estimate performance than the finetuned GPT2",
    "checked": true,
    "id": "1f99d057382445a7c83f15693e287f56d6305185",
    "semantic_title": "exploring energy-based language models with different architectures and training methods for speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23d_interspeech.html": {
    "title": "Memory Augmented Lookup Dictionary Based Language Modeling for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Recent studies have shown that using an external Language Model (LM) benefits the end-to-end Automatic Speech Recognition (ASR). However, predicting tokens that appear less frequently in the training set is still quite challenging. The long-tail prediction problems have been widely studied in many applications, but only been addressed by a few studies for ASR and LMs. In this paper, we propose a new memory augmented lookup dictionary based Transformer architecture for LM. The newly introduced lookup dictionary incorporates rich contextual information in training set, which is vital to correctly predict long-tail tokens. With intensive experiments on Chinese and English data sets, our proposed method is proved to outperform the baseline Transformer LM by a great margin on both word/character error rate and tail tokens error rate. This is achieved without impact on the decoding efficiency",
    "checked": true,
    "id": "b0c037b4037597f6e023cf300ea864bb9b7aa6dc",
    "semantic_title": "memory augmented lookup dictionary based language modeling for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/iwamoto23_interspeech.html": {
    "title": "Memory Network-Based End-To-End Neural ES-KMeans for Improved Word Segmentation",
    "volume": "main",
    "abstract": "Unsupervised word learning from unlabeled speech is a fundamental problem in zero-resource speech processing, which enables dialogue agents to learn new words directly from spoken utterances. The embedded segmental K-means (ES-KMeans) is a representative unsupervised word segmentation method. However, it has a heterogeneous structure consisting of word boundary search based on Dynamic Programming, segment embedding, and K-Means clustering, which prevents unified optimization. This paper proposes an end-to-end neural network version of the ES-KMeans model. We apply the memory network to hold a dictionary of word embeddings and realize the word boundary search and the clustering respectively as forward and backward propagations. Moreover, we replace the fixed embedding function of the original method with a learnable neural network. Experimental results using the ZeroSpeech Challenge 2020 package show the proposed approach provides superior performance to the state-of-the-art methods",
    "checked": true,
    "id": "88768f2934aa59d668a3e18a4e9c77694702230c",
    "semantic_title": "memory network-based end-to-end neural es-kmeans for improved word segmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23b_interspeech.html": {
    "title": "Retraining-free Customized ASR for Enharmonic Words Based on a Named-Entity-Aware Model and Phoneme Similarity Estimation",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (E2E-ASR) has the potential to improve performance, but a specific issue that needs to be addressed is the difficulty it has in handling enharmonic words: named entities (NEs) with the same pronunciation and part of speech that are spelled differently. This often occurs with Japanese personal names that have the same pronunciation but different Kanji characters. Since such NE words tend to be important keywords, ASR easily loses user trust if it misrecognizes them. To solve these problems, this paper proposes a novel retraining-free customized method for E2E-ASRs based on a named-entity-aware E2E-ASR model and phoneme similarity estimation. Experimental results show that the proposed method improves the target NE character error rate by 35.7% on average relative to the conventional E2E-ASR model when selecting personal names as a target NE",
    "checked": true,
    "id": "2c9beae437c30cd16bea013c10568e0a428feb88",
    "semantic_title": "retraining-free customized asr for enharmonic words based on a named-entity-aware model and phoneme similarity estimation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23c_interspeech.html": {
    "title": "Lightweight and Efficient Spoken Language Identification of Long-form Audio",
    "volume": "main",
    "abstract": "State-of-the-art Spoken Language Identification (SLI) systems usually focus on tackling short audio clips, and thus their performance degrade drastically when applied to long-form audio, such as podcast, which poses peculiar challenges to existing SLI approaches due to its long duration and diverse content that frequently involves multiple speakers as well as various languages, topics, and speech styles. In this paper, we propose the first system to tackle SLI for long-form audio using podcast data by training a lightweight, multi-class feedforward neural classifier using speaker embeddings as input. We demonstrate that our approach can make inference on long audio input efficiently; furthermore, our system can handle long audio files with multiple speakers and can be further extended into utterance-level inference and code-switching detection, which is currently not covered by any existing SLI system",
    "checked": true,
    "id": "1ff7218b81c3d54eb4c17e70def2d0675de3bd91",
    "semantic_title": "lightweight and efficient spoken language identification of long-form audio",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mishra23_interspeech.html": {
    "title": "End to End Spoken Language Diarization with Wav2vec Embeddings",
    "volume": "main",
    "abstract": "The performance of the available end-to-end (E2E) spoken language diarization (LD) systems is biased toward primary language. This is due to the unavailability of sufficient secondary language data. Because in code-switched (CS) utterances, the duration of the primary language is significant over the secondary language. Hence, to resolve the issue, this work initially uses wav2vec (W2V) pre-trained embedding in place of x-vector and can reduce the primary language bias and provides a relative improvement of 30.7% in terms of Jaccard error rate (JER) over the baseline x-vector based E2E (X-E2E) framework. Further, the performance of LD is improved by fine-tuning the W2V embedding extractor and modifying the temporal aggregation strategy from statistical pooling to attention pooling. The Final performance achieved in terms of JER is 22.5, which provides a relative improvement of 38.8% and 62.6% over the standalone W2V fine-tuned and the baseline X-E2E framework, respectively",
    "checked": true,
    "id": "fc2236674339b0c280b16d452c811164f8fd09bb",
    "semantic_title": "end to end spoken language diarization with wav2vec embeddings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nieto23_interspeech.html": {
    "title": "Efficient Spoken Language Recognition via Multilabel Classification",
    "volume": "main",
    "abstract": "Spoken language recognition (SLR) is the task of automatically identifying the language present in a speech signal. Existing SLR models are either too computationally expensive or too large to run effectively on devices with limited resources. For real-world deployment, a model should also gracefully handle unseen languages outside of the target language set, yet prior work has focused on closed-set classification where all input languages are known a-priori. In this paper we address these two limitations: we explore efficient model architectures for SLR based on convolutional networks, and propose a multilabel training strategy to handle non-target languages at inference time. Using the VoxLingua107 dataset, we show that our models obtain competitive results while being orders of magnitude smaller and faster than current state-of-the-art methods, and that our multilabel strategy is more robust to unseen non-target languages compared to multiclass classification",
    "checked": true,
    "id": "bdddcef337decd84c51d34091e52cc0e6f51204d",
    "semantic_title": "efficient spoken language recognition via multilabel classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matejka23_interspeech.html": {
    "title": "Description and Analysis of ABC Submission to NIST LRE 2022",
    "volume": "main",
    "abstract": "This paper summarizes our efforts in the NIST Language Recognition Evaluations 2022 resulting in systems providing competitive performance. We provide both the description and analysis of the systems. We describe what data we have used to train our models, and we follow with embedding extractors and backend classifiers. After covering the architecture, we concentrate on post-evaluation analysis. We compare different topologies of DNN, different backend classifiers, and the impact of the data used to train them. We also report results with XLS-R pre-trained models. We present the performance of the systems in the Fixed condition, where participants are required to use only predefined data sets, and also in the Open condition allowing to use any data to train the systems",
    "checked": true,
    "id": "a570f70209140ef4d62fb3e8c68bf7465e549710",
    "semantic_title": "description and analysis of abc submission to nist lre 2022",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alumae23_interspeech.html": {
    "title": "Exploring the Impact of Pretrained Models and Web-Scraped Data for the 2022 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "This paper describes Vocapia-TalTech team systems developed for the 2022 NIST Language Recognition Evaluation (LRE22) which focused on spoken language identication of African languages, including low-resource languages. In both fixed and open conditions, our primary systems were fused from multiple individual systems using logistic regression. In the fixed condition, we largely relied on wav2vec2.0 conformer models pretrained on the provided training data. In the open condition, we used external pretrained wav2vec2.0 models, phonotactic models and features derived from a multilingual speech recognition system, and also augmented the provided target language development data with additional data scraped from the web. On the LRE22 evaluation data, our final fixed and open condition systems obtained excellent results, with primary metric Cact values of 0.111 and 0.067, respectively. A post-evaluation study shows that both pretrained models as well as additional data are important for accurate models",
    "checked": true,
    "id": "dfb3b676ea6cda7b184911db7fc60fd9a466ed57",
    "semantic_title": "exploring the impact of pretrained models and web-scraped data for the 2022 nist language recognition evaluation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/villalba23_interspeech.html": {
    "title": "Advances in Language Recognition in Low Resource African Languages: The JHU-MIT Submission for NIST LRE22",
    "volume": "main",
    "abstract": "We present the effort of JHU-CLSP/HLTCOE and MIT Lincoln labs for NIST Language Recognition Evaluation (LRE) 2022. LRE22 consisted of a language detection task, i.e., determining whether a given target language was spoken in a speech segment. LRE22 focused on telephone and broadcast narrowband speech in African languages. Since LRE17, there has been large progress in neural embeddings, combined or not, with self-supervised models like Wav2Vec2. Therefore, one of our goals was to investigate these new models, i.e., ECAPA-TDNN, Res2Net or Wav2Vec2+ECAPA-TDNN, in the LRE scenario. In the fixed training condition, LRE22 target languages were only included in a small development set. Hence, we focused on tuning our models to exploit the limited data. For the open condition, we built a massive training set including African data, which improved Cprimary by 50% w.r.t. fixed. Wav2Vec2 embeddings were the best, outperforming ECAPA and Res2Net by 11 and 3%, respectively",
    "checked": true,
    "id": "51aca07d500c44ebde896b8df3b0388dd3ade489",
    "semantic_title": "advances in language recognition in low resource african languages: the jhu-mit submission for nist lre22",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23d_interspeech.html": {
    "title": "DeePMOS: Deep Posterior Mean-Opinion-Score of Speech",
    "volume": "main",
    "abstract": "We propose a deep neural network (DNN) based method that provides a posterior distribution of mean-opinion-score (MOS) for an input speech signal. The DNN outputs parameters of the posterior, mainly the posterior's mean and variance. The proposed method is referred to as deep posterior MOS (DeePMOS). The relevant training data is inherently limited in size (limited number of labeled samples) and noisy due to the subjective nature of human listeners. For robust training of DeePMOS, we use a combination of maximum-likelihood learning, stochastic gradient noise, and a student-teacher learning setup. Using the mean of the posterior as a point estimate, we evaluate standard performance measures of the proposed DeePMOS. The results show comparable performance with existing DNN-based methods that only provide point estimates of the MOS. Then we provide an ablation study showing the importance of various components in DeePMOS",
    "checked": true,
    "id": "8a69d5a2b10234ac6cf64e2a3dafe3b6a7a22e84",
    "semantic_title": "deepmos: deep posterior mean-opinion-score of speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dasare23_interspeech.html": {
    "title": "The Role of Formant and Excitation Source Features in Perceived Naturalness of Low Resource Tribal Language TTS: An Empirical Study",
    "volume": "main",
    "abstract": "Text-to-speech synthesis is a prominent area in the speechprocessing domain that has significant use in reading digital content in a given language. In the proposed work, we worked on two tribal languages of India viz., Lambani and Soliga, which are zero-resource languages. The study began with a dataset collection for both tribal languages. Secondly, a Text-To-Speech (TTS) system was built separately based on the transfer learning approach. To validate the voice quality of TTS-generated speech, subjective as well as objective evaluations were performed. As a part of objective analysis, the voice source and vocal tract filter properties of the synthetic speech have been explored. The extensive study on various aspects of speech, such as LP residual, F0 contour, and formants (F1 & sF2) has shown interesting results that can correlate to the subjective listening test results. The link to the original and synthetic speech can be found online",
    "checked": true,
    "id": "7828e3abbedaecbd890ccf7475783cfa3f397c71",
    "semantic_title": "the role of formant and excitation source features in perceived naturalness of low resource tribal language tts: an empirical study",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23_interspeech.html": {
    "title": "A no-reference speech quality assessment method based on neural network with densely connected convolutional architecture",
    "volume": "main",
    "abstract": "Most speech quality assessment methods require a perfect reference signal to evaluate the damaged speech's quality. However, it is challenging to obtain clean reference signals due to various types and levels of noise in reality. Meanwhile, no-reference speech quality assessment is less accurate than full-reference method. To address these issues, we propose a novel no-reference speech quality assessment model that improves evaluation accuracy with lower complexity. The model is primarily composed of three densely connected convolutional (DCC) modules and a bidirectional long short-term memory (BLSTM) module. Experiment results demonstrate that our method outperforms the baselines, achieving state-of-the-art on the no-reference speech quality assessment task. When using PESQ as optimization targets, the MSE, PLCC and SRCC reach 0.0389, 0.9695 and 0.9715, whereas when using STOI, these metrics reach 0.0019, 0.9608, and 0.9630, respectively",
    "checked": true,
    "id": "8e9a4eb13c54dbadee82df8389a683ef27f9c994",
    "semantic_title": "a no-reference speech quality assessment method based on neural network with densely connected convolutional architecture",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ta23_interspeech.html": {
    "title": "Probing Speech Quality Information in ASR Systems",
    "volume": "main",
    "abstract": "This paper investigates how intermediate speech representations in a state-of-the-art automatic speech recognition (ASR) system encode multi-dimensional speech quality, including MOS, Noisiness, Coloration, Discontinuity, and Loudness. We found that speech quality information is encoded in the ASR encoder layers at various levels but is still much richer than the Mel-spectrogram, an input widely used in previous works. This discovery inspires us to develop the Attentive Conformer with ASR pretraining, a novel deep learning model that enables the utilization of rich information from pretrained ASR models and the ability to focus on specific layers. Experiments on the NISQA speech quality assessment dataset demonstrate that the proposed model achieves state-of-the-art performance with significantly less training data",
    "checked": true,
    "id": "ffa5ed4bd8625dc803fe3de4e0c0f72752ff6df2",
    "semantic_title": "probing speech quality information in asr systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23d_interspeech.html": {
    "title": "Preference-based training framework for automatic speech quality assessment using deep neural network",
    "volume": "main",
    "abstract": "One objective of Speech Quality Assessment (SQA) is to estimate the ranks of synthetic speech systems. However, recent SQA models are typically trained using low-precision direct scores such as mean opinion scores (MOS) as the training objective, which is not straightforward to estimate ranking. Although it is effective for predicting quality scores of individual sentences, this approach does not account for speech and system preferences when ranking multiple systems. We propose a training framework of SQA models that can be trained with only preference scores derived from pairs of MOS to improve ranking prediction. Our experiment reveals conditions where our framework works the best in terms of pair generation, aggregation functions to derive system score from utterance preferences, and threshold functions to determine preference from a pair of MOS. Our results demonstrate that our proposed method significantly outperforms the baseline model in Spearman's Rank Correlation Coefficient",
    "checked": true,
    "id": "3c54bd226841a3994ab5239ffdc4b9031081d961",
    "semantic_title": "preference-based training framework for automatic speech quality assessment using deep neural network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/phatthiyaphaibun23_interspeech.html": {
    "title": "Crowdsourced Data Validation for ASR Training",
    "volume": "main",
    "abstract": "Many ASR engines are based on crowdsourced speech corpora, such as Common Voice. Although crowdsourced data is inexpensive, the utterances obtained from crowdsourcing can be noisy because of uncontrollable factors such as accents, environments, etc. Another issue with the Common Voice corpus is the lack of validators to cover a vast collection of crowdsourced utterances. This issue presents a significant challenge to speech data validation. To mitigate this bottleneck, we propose a machine-learning classifier that predicts the correctness of the data, which can act as either the validator itself or a prescreen for the validator. Our system achieves more than 95% F1-score in the three Common Voice languages, including Thai, Japanese, and Turkish, and performs even better when we have only one human judge involved in the decision. Furthermore, we also found that the data obtained from our method outperformed the current crowdsourcing validation method when used to train the ASR model",
    "checked": true,
    "id": "7dae6964be1b4dd1e254c7f2c2af88945b26c643",
    "semantic_title": "crowdsourced data validation for asr training",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huo23b_interspeech.html": {
    "title": "Re-investigating the Efficient Transfer Learning of Speech Foundation Model using Feature Fusion Methods",
    "volume": "main",
    "abstract": "Speech foundation models, pre-trained on large amounts of unsupervised or supervised audio data, have demonstrated an impressive ability to transfer their learning to specific domains for speech recognition. Parameter-efficient fine-tuning methods offer an efficient paradigm where a small set of parameters are updated to adapt the foundation model to new tasks. However, it is unclear how the intermediate features of the foundation model behave, and how to utilize them in a more efficient way. In this paper, we compare the performance of three speech foundation models for speech recognition. We re-investigate how features from different layers behave and propose a simple and effective feature fusion method for efficient transfer learning. Experimental results demonstrate that the proposed method uses 31.7% fewer trainable encoder parameters, 13.4% less computational memory cost than compared method, and does not compromise quality on the target task",
    "checked": true,
    "id": "d3ccf04f65bfa037227ebf1637e0b1bc9654ff59",
    "semantic_title": "re-investigating the efficient transfer learning of speech foundation model using feature fusion methods",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qi23_interspeech.html": {
    "title": "Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training",
    "volume": "main",
    "abstract": "Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (WAPAT). WAPAT use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, WAPAT utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of WAPAT on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-WAPAT outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art",
    "checked": true,
    "id": "1628c06f7b63c10c0aa78317aa9ca6c8da68198f",
    "semantic_title": "robust automatic speech recognition via wavaugment guided phoneme adversarial training",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23b_interspeech.html": {
    "title": "InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the baseline models",
    "checked": true,
    "id": "a86370b202b4856f32ba9db9ed10cb2ba4aca8e6",
    "semantic_title": "interformer: interactive local and global features fusion for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23_interspeech.html": {
    "title": "Transductive Feature Space Regularization for Few-shot Bioacoustic Event Detection",
    "volume": "main",
    "abstract": "In few-shot bioacoustic event detection, besides interested target events, background noises and various uninterested sound events lead to complex decision boundaries, which require regularized feature distributions in feature space. Due to the low label availability of uncertain noise events, existing few-shot learning methods with entropy-based regularizers suffer from overfitting during optimization. In this paper, we propose a transductive inference model with a prior knowledge based regularizer (PKR) to overcome the overfitting problem. We use a task-adaptive feature extractor to reconstruct a regularized feature space. A PKR is proposed to minimize the divergence between the original and reconstructed feature space. The development set of DCASE 2022 Task 5 is adopted as the experimental dataset. With the increasing iterations, the proposed model performs with long-lasting results around 55.43 F-measure, and well solves the overfitting problem in transductive inference",
    "checked": true,
    "id": "2a4b8260e63debd06db059f02b14fa36b603e7e7",
    "semantic_title": "transductive feature space regularization for few-shot bioacoustic event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23e_interspeech.html": {
    "title": "Incorporating L2 Phonemes Using Articulatory Features for Robust Speech Recognition",
    "volume": "main",
    "abstract": "The limited availability of non-native speech datasets presents a major challenge in automatic speech recognition (ASR) to narrow the performance gap between native and non-native speakers. To address this, the focus of this study is on the efficient incorporation of the L2 phonemes, which in this work refer to Korean phonemes, through articulatory feature analysis. This not only enables accurate modeling of pronunciation variants but also allows for the utilization of both native Korean and English speech datasets. We employ the lattice-free maximum mutual information (LF-MMI) objective in an end-to-end manner, to train the acoustic model to align and predict one of multiple pronunciation candidates. Experimental results show that the proposed method improves ASR accuracy for Korean L2 speech by training solely on L1 speech data. Furthermore, fine-tuning on L2 speech improves recognition accuracy for both L1 and L2 speech without performance trade-offs",
    "checked": true,
    "id": "64abb0112d24dc62105e302cf98af445651b86ab",
    "semantic_title": "incorporating l2 phonemes using articulatory features for robust speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/parcollet23_interspeech.html": {
    "title": "On the (In)Efficiency of Acoustic Feature Extractors for Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Speech representations learned with self-supervised learning (SSL) have the potential to significantly improve the performance of a number of audio applications, especially when availability of labeled data from the deployment domain is limited. Despite their successes, SSL training methods are compute- and memory-heavy, and require large investments in computing infrastructure, thus putting it out of the reach of most institutions. Therefore, building efficient model architectures is essential for the wide-scale adoption of SSL in speech technologies. CNN-based Acoustic Feature Extractors (AFE), which are widely used as encoders of acoustic waveforms, remain one of the main efficiency bottlenecks. This work proposes replacing CNN-based AFEs with more efficient ones and demonstrates that SSL pre-training time and memory consumption can be reduced by a factor of two to three over existing methods while preserving performances in speech-, command-, and speaker-recognition tasks",
    "checked": true,
    "id": "67377c8759a5a21ed27a8f6eee585dde1c3bd6a7",
    "semantic_title": "on the (in)efficiency of acoustic feature extractors for self-supervised speech representation learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tenbosch23_interspeech.html": {
    "title": "Phonemic competition in end-to-end ASR models",
    "volume": "main",
    "abstract": "Advanced end-to-end ASR systems encode speech signals by means of a multi-layer network architecture. In Wav2vec2.0, for example, a CNN is used as feature encoder on top of which transformer layers are used to map the high-dimensional CNN representations to the elements of some lexicon. Compared to the previous generation of 'modular' ASR systems it is much more difficult to interpret the processing and representations in an end-to-end system from a phonetic point of view. We built a Wav2vec2.0-based end-to-end system for producing broad phonetic transcriptions of Dutch. In this paper we investigate to what extent the CNN features and the representations on several transformer layers of a pre-trained and fine-tuned model reflect widely-shared phonetic knowledge. For that purpose we analyze distances between phones and the phonetic features of the most-activated phones in the output of an MLP classifier operating on the representations in several layers",
    "checked": true,
    "id": "294e96df613afb8a84c37ffabd183ba049cedd99",
    "semantic_title": "phonemic competition in end-to-end asr models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hughes23_interspeech.html": {
    "title": "Automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics",
    "volume": "main",
    "abstract": "Automatic Speaker Recognition (ASR) involves a complex range of processes to extract, model, and compare speaker-specific information from a pair of voice samples. Using heavily controlled recordings, this paper explores the impact of specific vocal conditions (i.e. vocal setting, disguise, accent guises) on ASR performance. When vocal conditions are matched, ASR performance is generally excellent (whisper is an exception). When conditions are mismatched, as in most forensic cases, we see an increase in discrimination and calibration error in some cases. The most problematic mismatches are those involving whisper and supralaryngeal vocal settings; these produce the greatest phonetic changes to speech. Mismatches involving high pitch also produce poor performance, although this appears to be driven by speaker-specific differences in articulatory implementation. We discuss the implications of the findings for the use of ASR in forensic casework and the interpretability of system output",
    "checked": true,
    "id": "199229d3865f23548de42544004502ffee6f843c",
    "semantic_title": "automatic speaker recognition with variation across vocal conditions: a controlled experiment with implications for forensics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geiger23_interspeech.html": {
    "title": "Exploring Graph Theory Methods For the Analysis of Pronunciation Variation in Spontaneous Speech",
    "volume": "main",
    "abstract": "Given the development of automatic speech recognition based techniques for creating phonetic annotations of large speech corpora, there has been a growing interest in investigating the frequencies of occurrence of phonological and reduction processes. Given that most studies have analyzed these processes separately, they did not provide insights about their co-occurrences. This paper contributes with introducing graph theory methods for the analysis of pronunciation variation in a large corpus of Austrian German conversational speech. More specifically, we investigate how reduction processes that are typical for spontaneous German in general co-occur with phonological processes typical for the Austrian German variety. Whereas our concrete findings are of special interest to scientists investigating variation in German, the approach presented opens new possibilities to analyze pronunciation variation in large corpora of different speaking styles in any language",
    "checked": true,
    "id": "338f9ef7acb8d1b25b8eda135ec9e8fb710f8085",
    "semantic_title": "exploring graph theory methods for the analysis of pronunciation variation in spontaneous speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nuttall23_interspeech.html": {
    "title": "Automatic Speaker Recognition performance with matched and mismatched female bilingual speech data",
    "volume": "main",
    "abstract": "Validation of forensic voice comparison methods requires testing using speech samples that are representative of forensic casework conditions. Increasingly, around the world, forensic voice comparison casework is being undertaken using automatic speaker recognition (ASR) systems. However, multilingualism remains a key issue in applying automatic systems to forensic casework. This research aims to consider the effect of language on ASR performance, testing developers' claims of 'language independency'. Specifically, we examine the extent to which language mismatch either between the known and questioned samples, or between the evidential samples and the calibration data, affects overall system performance and the resulting strength of evidence (i.e., likelihood ratios for individual comparisons). Results indicate that mixed language trials produce more errors than single language trials which makes drawing evidential conclusions based on bilingual data challenging",
    "checked": true,
    "id": "1e58d3ff954b2690435fcb91d712ced6875d06a4",
    "semantic_title": "automatic speaker recognition performance with matched and mismatched female bilingual speech data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23x_interspeech.html": {
    "title": "FACTSpeech: Speaking a Foreign Language Pronunciation Using Only Your Native Characters",
    "volume": "main",
    "abstract": "Recent text-to-speech models have been requested to synthesize natural speech from language-mixed sentences because they are commonly used in real-world applications. However, most models do not consider transliterated words as input. When generating speech from transliterated text, it is not always natural to pronounce transliterated words as they are written, such as in the case of song titles. To address this issue, we introduce FACTSpeech, a system that can synthesize natural speech from transliterated text while allowing users to control the pronunciation between native and literal languages. Specifically, we propose a new language shift embedding to control the pronunciation of input text between native or literal pronunciation. Moreover, we leverage conditional instance normalization to improve pronunciation while preserving the speaker identity. The experimental results show that FACTSpeech generates native speech even from the sentences of transliterated form",
    "checked": true,
    "id": "8b0feedfec68ce2e39a5d9604b3b16fdfd3fcf07",
    "semantic_title": "factspeech: speaking a foreign language pronunciation using only your native characters",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23e_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model",
    "volume": "main",
    "abstract": "Phrase break prediction is a crucial task for improving the prosody naturalness of a text-to-speech (TTS) system. However, most proposed phrase break prediction models are monolingual, trained exclusively on a large amount of labeled data. In this paper, we address this issue for low-resource languages with limited labeled data using cross-lingual transfer. We investigate the effectiveness of zero-shot and few-shot cross-lingual transfer for phrase break prediction using a pre-trained multilingual language model. We use manually collected datasets in four Indo-European languages: one high-resource language and three with limited resources. Our findings demonstrate that cross-lingual transfer learning can be a particularly effective approach, especially in the few-shot setting, for improving performance in low-resource languages. This suggests that cross-lingual transfer can be inexpensive and effective for developing TTS front-end in resource-poor languages",
    "checked": true,
    "id": "07ecfeb52ff0768c3067cac5309d8150701d5906",
    "semantic_title": "cross-lingual transfer learning for phrase break prediction with multilingual language model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23d_interspeech.html": {
    "title": "DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "Although high-fidelity speech can be obtained for intralingual speech synthesis, cross-lingual text-to-speech (CTTS) is still far from satisfactory as it is difficult to accurately retain the speaker timbres (i.e. speaker similarity) and eliminate the accents from their first language (i.e. nativeness). In this paper, we demonstrated that vector-quantized (VQ) acoustic feature contains less speaker information than mel-spectrogram. Based on this finding, we propose a novel dual speaker embedding TTS (DSE-TTS) framework for CTTS with authentic speaking style. Here, one embedding is fed to the acoustic model to learn the linguistic speaking style, while the other one is integrated into the vocoder to mimic the target speaker's timbre. Experiments show that by combining both embeddings, DSE-TTS significantly outperforms the state-of-the-art SANE-TTS in cross-lingual synthesis, especially in terms of nativeness",
    "checked": true,
    "id": "45a04efc208faa3eb97c60b96dd07bbbb853f69c",
    "semantic_title": "dse-tts: dual speaker embedding for cross-lingual text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/markopoulos23_interspeech.html": {
    "title": "Generating Multilingual Gender-Ambiguous Text-to-Speech Voices",
    "volume": "main",
    "abstract": "The gender of any voice user interface is a key element of its perceived identity. Recently there has been increasing interest in interfaces where the gender is ambiguous rather than clearly identifying as female or male. This work addresses the task of generating novel gender-ambiguous TTS voices in a multi-speaker, multilingual setting. This is accomplished by efficiently sampling from a latent speaker embedding space using a proposed gender-aware method. Extensive objective and subjective evaluations clearly indicate that this method is able to efficiently generate a range of novel, diverse voices that are consistent and perceived as more gender-ambiguous than a baseline voice across all the languages examined. Interestingly, the gender perception is found to be robust across two demographic factors of the listeners: native language and gender. To our knowledge, this is the first systematic and validated approach that can reliably generate a variety of gender-ambiguous voices",
    "checked": true,
    "id": "be0afc3192c002797d767930ce271fccab713471",
    "semantic_title": "generating multilingual gender-ambiguous text-to-speech voices",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/badlani23_interspeech.html": {
    "title": "RAD-MMM: Multilingual Multiaccented Multispeaker Text To Speech",
    "volume": "main",
    "abstract": "We create a multilingual speech synthesis system that can generate speech with a native accent in any seen language while retaining the characteristics of an individual's voice. It is expensive to obtain bilingual training data for a speaker and the lack of such data results in strong correlations that entangle speaker, language, and accent, resulting in poor transfer capabilities. To overcome this, we present RADMMM, a speech synthesis model based on RADTTS with explicit control over accent, language, speaker, and fine-grained F0 and energy features. Our proposed model does not rely on bilingual training data. We demonstrate an ability to control synthesized accent for any speaker in an open-source dataset comprising of 7 languages, with one native speaker per language. Human subjective evaluation demonstrates that, when compared to controlled baselines, our model better retains a speaker's voice and target accent, while synthesizing fluent speech in all target languages and accents in our dataset",
    "checked": true,
    "id": "3beba85b45c7ee6e567a8e44fbaa31a6867cd17b",
    "semantic_title": "rad-mmm: multilingual multiaccented multispeaker text to speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/comini23_interspeech.html": {
    "title": "Multilingual context-based pronunciation learning for Text-to-Speech",
    "volume": "main",
    "abstract": "Phonetic information and linguistic knowledge are an essential component of a Text-to-speech (TTS) front-end. Given a language, a lexicon can be collected offline and Grapheme-to-Phoneme (G2P) relationships are usually modeled in order to predict the pronunciation for out-of-vocabulary (OOV) words. Additionally, post-lexical phonology, often defined in the form of rule-based systems, is used to correct pronunciation within or between words. In this work we showcase a multilingual unified front-end system that addresses any pronunciation related task, typically handled by separate modules. We evaluate the proposed model on G2P conversion and other language-specific challenges, such as homograph and polyphones disambiguation, post-lexical rules and implicit diacritization. We find that the multilingual model is competitive across languages and tasks, however, some trade-offs exists when compared to equivalent monolingual solutions",
    "checked": true,
    "id": "0d2776b80f93b96be8f4542ffdeb3a6a31f9b770",
    "semantic_title": "multilingual context-based pronunciation learning for text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23c_interspeech.html": {
    "title": "Personalized Adaptation with Pre-trained Speech Encoders for Continuous Emotion Recognition",
    "volume": "main",
    "abstract": "There are individual differences in expressive behaviors driven by cultural norms and personality. This between-person variation can result in reduced emotion recognition performance. Therefore, personalization is an important step in improving the generalization and robustness of speech emotion recognition. In this paper, to achieve unsupervised personalized emotion recognition, we first pre-train an encoder with learnable speaker embeddings in a self-supervised manner to learn robust speech representations conditioned on speakers. Second, we propose an unsupervised method to compensate for the label distribution shifts by finding similar speakers and leveraging their label distributions from the training set. Extensive experimental results on the MSP-Podcast corpus indicate that our method consistently outperforms strong personalization baselines and achieves state-of-the-art performance for valence estimation",
    "checked": true,
    "id": "a8e465a05d423f6201b4cd5631f04d00261e41b8",
    "semantic_title": "personalized adaptation with pre-trained speech encoders for continuous emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chou23_interspeech.html": {
    "title": "The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers",
    "volume": "main",
    "abstract": "The uncertainty in modeling emotions makes speech emotion recognition (SER) systems less reliable. An intuitive way to increase trust in SER is to reject predictions with low confidence. This approach assumes that an SER system is well calibrated, where highly confident predictions are often right and low confident predictions are often wrong. Hence, it is desirable to calibrate the confidence of SER classifiers. We evaluate the reliability of SER systems by exploring the relationship between confidence and accuracy, using the expected calibration error (ECE) metric. We develop a multi-label variant of the post-hoc temperature scaling (TS) method to calibrate SER systems, while preserving their accuracy. The best method combines an emotion co-occurrence weight penalty function, a class-balanced objective function, and the proposed multi-label TS calibration method. The experiments show the effectiveness of our developed multi-label calibration method in terms of accuracy and ECE",
    "checked": true,
    "id": "dfb438c044685e25a970c622d5c282c79b4831c1",
    "semantic_title": "the importance of calibration: rethinking confidence and performance of speech multi-label emotion classifiers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/malik23_interspeech.html": {
    "title": "A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model",
    "volume": "main",
    "abstract": "In this paper, we propose to utilise diffusion models for data augmentation in speech emotion recognition (SER). In particular, we present an effective approach to utilise improved denoising diffusion probabilistic models (IDDPM) to generate synthetic emotional data. We condition the IDDPM with the textual embedding from bidirectional encoder representations from transformers (BERT) to generate high-quality synthetic emotional samples in different speakers' voiceswe uploaded the synthetic samples for reviewers to listen.. We implement a series of experiments and show that better quality synthetic data helps improve SER performance. We compare results with generative adversarial networks (GANs) and show that the proposed model generates better-quality synthetic samples that can considerably improve the performance of SER when augmented with synthetic data",
    "checked": true,
    "id": "21da715d3a9bee5e91206347560a898587303ce0",
    "semantic_title": "a preliminary study on augmenting speech emotion recognition using a diffusion model",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alsenani23_interspeech.html": {
    "title": "Privacy Risks in Speech Emotion Recognition: A Systematic Study on Gender Inference Attack",
    "volume": "main",
    "abstract": "Increasingly more applications now use deep networks to analyse speaker's affective states. An undesirable side effect is that models trained to perform one task (e.g, emotion from speech) can be attacked to infer other, possibly privacy-sensitive attributes (e.g., gender) of the speaker. The amount of information an attacker can infer through such attacks is called leakage, and this article presents the first systematic study of the interplay between gender leakage and the main characteristics of the attacker model (family, architecture and training condition). To this end, we define various attack scenarios, and perform extensive experiments to analyse privacy risks in Speech Emotion Recognition (SER). Results show that SER models can leak a speaker's gender with an accuracy of 51% to 95% (upper bound) depending on the attack condition. Furthermore, our results provide fresh insights on how to limit the effectiveness of possible attacks and, thereby, to ensure privacy preservation",
    "checked": true,
    "id": "9982df071491764bee55ec7ea6c2f896934785f7",
    "semantic_title": "privacy risks in speech emotion recognition: a systematic study on gender inference attack",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tavernor23_interspeech.html": {
    "title": "Episodic Memory For Domain-Adaptable, Robust Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Emotion conveys abundant information that can improve the user experience of various automated systems, in addition to communicating information important for managing well-being. Human speech conveys emotion, but speech emotion recognition models do not perform well in unseen environments. This limits the ubiquitous use of speech emotion recognition models. In this paper, we investigate how a model can be adapted to unseen environments without forgetting previously learned environments. We show that memory-based methods maintain performance on previously seen environments while still being able to adapt to new environments. These methods enable continual training of speech emotion recognition models following deployment while retaining previous knowledge, working towards a more general, adaptable, acoustic model",
    "checked": true,
    "id": "f734a8f825d3c0b2982f343d95d622a4e3eecaa1",
    "semantic_title": "episodic memory for domain-adaptable, robust speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ding23_interspeech.html": {
    "title": "Stable Speech Emotion Recognition with Head-k-Pooling Loss",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) aims to detect the emotion of the speaker involved in a given utterance. Most existing SER methods focus on local speech features by stacking convolutions and training all segments of an utterance with an utterance-level label. Two deficiencies exist in these methods: i) learning only local speech features may be insufficient for SER due to the ambiguity of emotions; ii) consistent supervision of each segment may lead to label error propagation, as the true emotions of some segments may not match the utterance label. To solve the two issues, we first devise a global-local fusion network to model both long- and short-range relations in speech. Second, we tailor a novel head-k-pooling loss for SER tasks, which dynamically assigns labels for each segment and selectively performs loss calculation across segments. We test our method on the IEMOCAP and the newly collected ST-EMO dataset, and the results show its superiority and stability",
    "checked": true,
    "id": "b8f958dcae03dcb7a3f27277a1a10ab784a49e0f",
    "semantic_title": "stable speech emotion recognition with head-k-pooling loss",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gibson23b_interspeech.html": {
    "title": "A Personalised Speech Communication Application for Dysarthric Speakers",
    "volume": "main",
    "abstract": "Individuals with impaired speech are often understood only by those familiar with their speech e.g. a care-giver or close family member. These impaired speakers are therefore highly dependent upon those familiar listeners for their spoken communication needs. These needs vary from basic expressions of hunger or thirst to much more advanced requirements like being understood at a work meeting. A significant subset of individuals with impaired speech also have reduced motor function which limits their mobility or dexterity. For this subset of individuals, the ability to communicate via the medium of speech is crucial. This paper describes a personalised speech communication application targeted towards English language speakers with impaired speech. This application enables the user to hold conversations with other humans, dictate text to a machine and participate in meetings via closed captioning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23l_interspeech.html": {
    "title": "Video Multimodal Emotion Recognition System for Real World Applications",
    "volume": "main",
    "abstract": "This paper proposes a system capable of recognizing a speaker's utterance-level emotion through multimodal cues in a video. The system seamlessly integrates multiple AI models to first extract and pre-process multimodal information from the raw video input. Next, an end-to-end MER model sequentially predicts the speaker's emotions at the utterance level. Additionally, users can interactively demonstrate the system through the implemented interface",
    "checked": true,
    "id": "74d9bea6a26e928b042f009bc277bb3fe04190bb",
    "semantic_title": "video multimodal emotion recognition system for real world applications",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rohmatillah23_interspeech.html": {
    "title": "Promoting Mental Self-Disclosure in a Spoken Dialogue System",
    "volume": "main",
    "abstract": "This paper proposes a mental health spoken dialogue to relax mental illness for university students by acting as an active listener to promote self-disclosure. The proposed system is designed for Mandarin with the specific accent and lexicon in Taiwan which is known as one of the underrepresented spoken languages. To achieve the objective, this work considers three key factors which are high quality speech components including automatic speech recognition and text-to-speech models, and the personalized responses while keeping the trustworthiness and seamless integration among dialogue system components",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bujnowski23_interspeech.html": {
    "title": "Select language, modality or put on a mask!\" Experiments with Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "We propose a system designed for multimodal emotion recognition. Our research focuses on showing the impact of various signals in the emotion recognition process. Apart from reporting the average results of our models, we would like to encourage individual engagement of conference participants and explore how a unique emotional scene recorded on the spot can be interpreted by the models - for individual modalities as well as their combinations. Our models work for English, German and Korean. We show the comparison of emotion recognition accuracy for these 3 languages, including the influence of each modality. Our second experiment explores emotion recognition for people wearing face masks. We show that the use of face masks affects not only the video signal but also audio and text. To our knowledge, no other study shows the effects of wearing a mask for three modalities. Unlike other studies where masks are added artificially, we use real recordings with actors in masks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/valentine23_interspeech.html": {
    "title": "My Vowels Matter: Formant Automation Tools for Diverse Child Speech",
    "volume": "main",
    "abstract": "Tools to automate formant measurement in vowels have been developed recently, but they have not been tested on pediatric speech samples. Critically, child speech includes unique acoustic challenges including high fundamental frequencies, wide formant bandwidths, more variable formant values, and increased subglottal coupling relative to adult speech. More importantly, these tools have not been tested on the diverse linguistic variations spoken by children. This study compares three tools for automatic formant estimation: Voweltine, Fast Track, and SpeechMark. The tools are tested on vowel productions from a young child with a speech sound disorder from a Black-identifying family. Benefits and tradeoffs of each automation tool are discussed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chongwhite23_interspeech.html": {
    "title": "NEMA: An Ecologically Valid Tool for Assessing Hearing Devices, Advanced Algorithms, and Communication in Diverse Listening Environments",
    "volume": "main",
    "abstract": "Ecological Momentary Assessment (EMA) is valuable research method for evaluating the real-world performance of novel computational algorithms and device technologies, addressing the shortcomings of objective metrics and laboratory assessments. Our customisable, cloud-connected smartphone app, NEMA, gathers repeated self-reports and related acoustic features in users' natural environments, providing personalised insights on how specific technologies impact daily activities. NEMA has proven effective in assessing the real-world performance of novel hearing aid algorithms and features, while also improving our understanding of the challenges faced by those with hearing loss which drive new developments. This paper outlines NEMA's innovative features designed to facilitate efficient data collection and presents findings from a recent clinical trial where NEMA played a key role in providing real-world evidence of user benefits for a medical device seeking FDA approval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ramanarayanan23_interspeech.html": {
    "title": "When Words Speak Just as Loudly as Actions: Virtual Agent Based Remote Health Assessment Integrating What Patients Say with What They Do",
    "volume": "main",
    "abstract": "We present a unified multimodal dialog platform for the remote assessment and monitoring of patients' neurological and mental health. Tina, a virtual agent, guides participants through an immersive interaction wherein objective speech, facial, linguistic and cognitive biomarkers can be automatically computed from participant speech and video in near real time. Furthermore, Tina encourages participants to describe, in their own words, their most bothersome problems and what makes them better or worse, through the Patient Report of Problems (PROP) instrument. The PROP captures unfiltered verbatim replies of patients, in contrast with traditional patient reported outcomes that typically rely on categorical assessments. We argue that combining these patient reports (i.e., what they say) with objective biomarkers (i.e., how they say it and what they do) can greatly enhance the quality of telemedicine and improve the efficacy of siteless trials and digital therapeutic interventions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/motepalli23_interspeech.html": {
    "title": "Stuttering Detection Application",
    "volume": "main",
    "abstract": "Stuttering is a prevalent speech disorder that affects millions of people worldwide. In this Show and Tell presentation, we demonstrate a novel platform that takes speech samples in English and Kannada to detect and analyze stuttering in patients. The user-friendly interface includes demographic details and speech samples, generating comprehensive reports for different stuttering disfluencies. The platform has four different user types, providing full read-only access for admins and full write access for super admins. Our platform provides valuable assistance for speech-language pathologists to evaluate speech samples. The proposed platform supports both live and recorded speech samples and presents a flexible approach to stuttering detection and analysis. Our research demonstrates the potential of technology to improve speech-language pathology for stuttering. Used F-score as a metric for evaluating the models for the stutter detection task",
    "checked": false,
    "id": "7da56460260e74076d524527eb89a981747864e7",
    "semantic_title": "drowsiness detection application",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zusag23b_interspeech.html": {
    "title": "Providing Interpretable Insights for Neurological Speech and Cognitive Disorders from Interactive Serious Games",
    "volume": "main",
    "abstract": "We propose an automated pipeline for robustly identifying neurological disorders from interactive therapeutic exercises, which are gathered via the mobile therapy app myReha. The app captures speech and cognitive parameters from over 30.000 tasks in various scenarios. Users get immediate and highly accurate feedback for pronunciation and coherency for language tasks, while voice recordings are fed to a feature extraction pipeline in the backend. These features are then used to construct speech characteristics, which are highly indicative of different neurological disorders, such as acquired aphasia after stroke. The data is visually presented in a web application nyra.insights, which allows medical professionals to quickly derive recommendations for treatment and closely monitor outcomes. During the Show and Tell session, users can experiment with the interactive myReha app and experience the real-time speech analysis capabilities via the nyra.insights web platform",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/solinsky23_interspeech.html": {
    "title": "Automated Neural Nursing Assistant (ANNA): An Over-The-Phone System for Cognitive Monitoring",
    "volume": "main",
    "abstract": "ANNA is a telephony-based cognitive assessment tool designed to aid nurses in caring for patients who require close monitoring for the development of confusion or neurological impairment. Of particular concern is the treatment of Immune Effector Cell-Associated Neurotoxicity Syndrome (ICANS), a condition which occurs quite frequently as an adverse outcome of Chimeric Antigen Receptor-T (CAR-T) cancer immunotherapy. ANNA employs both traditional verbal tests for cognitive impairment and novel linguistic methods which identify abnormalities in the patient's speech during ordinary conversation. To collect ordinary speech it uses a lightweight instance of the Facebook's Large Language Model BlenderBot to engage the patient in a partially unscripted conversation. ANNA is designed with easy employment by healthcare providers in mind, being sufficiently lightweight to run on consumer-grade hardware and needing access only to a patient's phone number to interact with them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gupta23b_interspeech.html": {
    "title": "5G-IoT Cloud based Demonstration of Real-Time Audio-Visual Speech Enhancement for Multimodal Hearing-aids",
    "volume": "main",
    "abstract": "Over twenty percent of the world's population suffers from some form of hearing loss, making it one of the most significant public health challenges. Current hearing aids commonly amplify noises while failing to improve speech comprehension in crowded social settings. In this demonstration, we showcase a proof-of-concept implementation of the world's first 5G and Internet of Things (IoT) enabled multi-modal hearing aid (MM HA) prototype. This integrates an innovative 5G cloud-radio access network (C-RAN) and IoT based transceiver model for real-time audio-visual speech enhancement (AVSE). Specifically, we demonstrate a transceiver model for Cloud-based AVSE which satisfies high data rate and low latency requirements for future MM HAs. The innovative 5G-IoT transceiver application is shown to satisfy HA latency limitations while transmitting raw noisy AV data from an MM HA prototype device to the cloud for deep learning-based real-time AVSE processing and obtaining a clean audio signal",
    "checked": false,
    "id": "672c2b5508b040435a90193b8a2e127b42bf6bea",
    "semantic_title": "live demonstration: cloud-based audio-visual speech enhancement in multimodal hearing-aids",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raza23_interspeech.html": {
    "title": "Towards Two-point Neuron-inspired Energy-efficient Multimodal Open Master Hearing Aid",
    "volume": "main",
    "abstract": "Here we demonstrate a two-point neuron-inspired audio-visual (AV) open Master Hearing Aid (OpenMHA) framework for on-chip energy-efficient speech enhancement (SE). The developed system is compared against state-of-the-art cepstrum-based audio-only (A-only) SE and conventional point-neuron inspired deep neural net (DNN) driven multimodal (MM) SE. Pilot experiments demonstrate that the proposed system outperforms audio-only SE in terms of speech quality and intelligibility and performs comparably to point neuron-inspired DNN with significantly reduced energy consumption at any time --- both during training and inferencing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23b_interspeech.html": {
    "title": "FC-MTLF: A Fine- and Coarse-grained Multi-Task Learning Framework for Cross-Lingual Spoken Language Understanding",
    "volume": "main",
    "abstract": "Currently, zero-shot cross-lingual spoken language understanding (SLU) attracts increasing attention. Most of existing methods construct a mixed-language context via the code-switching approach. However, due to the different syntactic structures of each language, code-switching might fail to perform well and result in the loss of semantics. To address this issue, we propose a novel framework termed FC-MTLF, which applies a multi-task learning by introducing an auxiliary multilingual neural machine translation (NMT) task to compensate for the shortcomings of code-switching. In addition, we also adopt the curriculum learning strategy to further improve the performance. Experimental results show that our framework achieves the new state-of-the-art performance on the MultiATIS++ dataset. Further analysis verifies that our FC-MTLF can effectively transfer knowledge from source languages to target languages",
    "checked": true,
    "id": "118a703723c066895be4477f8d7bfb62e370ac2f",
    "semantic_title": "fc-mtlf: a fine- and coarse-grained multi-task learning framework for cross-lingual spoken language understanding",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23c_interspeech.html": {
    "title": "C²A-SLU: Cross and Contrastive Attention for Improving ASR Robustness in Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) is a critical task in task-oriented dialogue systems. However, automatic speech recognition (ASR) errors often impair the understanding performance. Despite many previous models have obtained promising results for improving ASR robustness in SLU, most of them treat clean manual transcripts and ASR transcripts equally during the fine-tuning stage. To tackle this issue, in this paper, we propose a novel method termed C²A-SLU. Specifically speaking, we add calculated cross attention to the original hidden states and apply contrastive attention to compare the input transcript with clean manual transcripts to distill the contrastive information, which can better capture distinctive features of ASR transcripts. Experiments on three datasets show that C²A-SLU surpasses existing models and achieves a new state-of-the-art performance, with a relative improvement of 3.4% in terms of accuracy over the previous best model on SLURP dataset",
    "checked": true,
    "id": "55b95df9297c2eb3dbf32b2cbc65c402ad67a856",
    "semantic_title": "c²a-slu: cross and contrastive attention for improving asr robustness in spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/weld23_interspeech.html": {
    "title": "Tri-level Joint Natural Language Understanding for Multi-turn Conversational Datasets",
    "volume": "main",
    "abstract": "Natural language understanding typically maps single utterances to a dual level semantic frame, sentence level intent and slot labels at the word level. The best performing models force explicit interaction between intent detection and slot filling. We present a novel tri-level joint natural language understanding approach, adding domain, and explicitly exchange semantic information between all levels. This approach enables the use of multi-turn datasets which are a more natural conversational environment than single utterance. We evaluate our model on two multi-turn datasets for which we are the first to conduct joint slot-filling and intent detection. Our model outperforms state-of-the-art joint models in slot filling and intent detection on multi-turn data sets. We provide an analysis of explicit interaction locations between the layers. We conclude that including domain information improves model performance",
    "checked": true,
    "id": "bbebba31b33f265c2e9b475080051b1a8246f089",
    "semantic_title": "tri-level joint natural language understanding for multi-turn conversational datasets",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/laperriere23_interspeech.html": {
    "title": "Semantic Enrichment Towards Efficient Speech Representations",
    "volume": "main",
    "abstract": "Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR",
    "checked": true,
    "id": "6b0aff42f33d169a0ac5668e23f5f99f695da170",
    "semantic_title": "semantic enrichment towards efficient speech representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kashiwagi23b_interspeech.html": {
    "title": "Tensor decomposition for minimization of E2E SLU model toward on-device processing",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters",
    "checked": true,
    "id": "0534fd0ed04acaa60f820b730bf3c4816767fa43",
    "semantic_title": "tensor decomposition for minimization of e2e slu model toward on-device processing",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mao23_interspeech.html": {
    "title": "DiffSLU: Knowledge Distillation Based Diffusion Model for Cross-Lingual Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) has achieved great success in high-resource languages, but it still remains challenging in the low-resource languages due to the scarcity of labeled training data. Hence, there is an increasing interest in zero-shot cross-lingual SLU. SLU typically has two subtasks, including intent detection and slot filling. Slots and intent in the same utterance are correlated, thus it is beneficial to achieve mutual guidance between them. In this paper, we propose a novel cross-lingual SLU framework termed DiffSLU, which leverages powerful diffusion model to enhance the mutual guidance. In addition, we also utilize knowledge distillation to facilitate knowledge transfer. Experimental results demonstrate that our DiffSLU can improve the performance compared with the strong baselines and achieves the new state-of-the-art performance on MultiATIS++ dataset, obtaining a relative improvement of 3.1% over the previous best model in overall accuracy",
    "checked": true,
    "id": "9e46a2f7d0193fee8b3d702bfeb9692ed6660e14",
    "semantic_title": "diffslu: knowledge distillation based diffusion model for cross-lingual spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arora23_interspeech.html": {
    "title": "Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding",
    "volume": "main",
    "abstract": "There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances",
    "checked": true,
    "id": "0c7018db4a00df1792a7b3de3cb0b48aa19ca041",
    "semantic_title": "integrating pretrained asr and lm to perform sequence generation for spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23e_interspeech.html": {
    "title": "Contrastive Learning Based ASR Robust Knowledge Selection For Spoken Dialogue System",
    "volume": "main",
    "abstract": "The construction of knowledge-based, task-oriented systems for spoken conversations is a challenging task. Given the spoken dialogue history information, a knowledge selection model selects the appropriate knowledge snippet from an unstructured knowledge base. However, the performance of this model is sensitive to automatic speech recognizer (ASR) recognition errors. To address this problem, we propose a method called CLKS, which develops a knowledge selection model that is robust to ASR recognition errors. This approach involves: 1) To leverage a wide range of information from various ASR outputs, we employ the self-attention mechanism to aggregate the representation of the N-best hypotheses of the dialogue history. 2) We use the written dialogue representation to guide the aggregated spoken dialogue representation to select the correct knowledge candidate through contrastive learning. Experimental results on the DSTC10 dataset demonstrate the effectiveness of our method",
    "checked": true,
    "id": "519f73c93cf31cce7d760f146235e8f0fd2c179c",
    "semantic_title": "contrastive learning based asr robust knowledge selection for spoken dialogue system",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23f_interspeech.html": {
    "title": "Unsupervised Dialogue Topic Segmentation in Hyperdimensional Space",
    "volume": "main",
    "abstract": "We present HyperSeg, a hyperdimensional computing (HDC) approach to unsupervised dialogue topic segmentation. HDC is a class of vector symbolic architectures that leverages the probabilistic orthogonality of randomly drawn vectors at extremely high dimensions (typically over 10,000). HDC generates rich token representations through its low-cost initialization of many unrelated vectors. This is especially beneficial in topic segmentation, which often operates as a resource-constrained pre-processing step for downstream transcript understanding tasks. HyperSeg outperforms the current state-of-the-art in 4 out of 5 segmentation benchmarks -- even when baselines are given partial access to the ground truth -- and is 10 times faster on average. We show that HyperSeg also improves downstream summarization accuracy. With HyperSeg, we demonstrate the viability of HDC in a major language task. We open-source HyperSeg to provide a strong baseline for unsupervised topic segmentation",
    "checked": true,
    "id": "073c91fc6082111154d4525b48eca8e54477f71f",
    "semantic_title": "unsupervised dialogue topic segmentation in hyperdimensional space",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cappellazzo23_interspeech.html": {
    "title": "An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Continual learning refers to a dynamical framework in which a model receives a stream of non-stationary data over time and must adapt to new data while preserving previously acquired knowledge. Unluckily, neural networks fail to meet these two desiderata, incurring the so-called catastrophic forgetting phenomenon. Whereas a vast array of strategies have been proposed to attenuate forgetting in the computer vision domain, for speech-related tasks, on the other hand, there is a dearth of works. In this paper, we consider the joint use of rehearsal and knowledge distillation (KD) approaches for spoken language understanding under a class-incremental learning scenario. We report on multiple KD combinations at different levels in the network, showing that combining feature-level and predictions-level KDs leads to the best results. Finally, we provide an ablation study on the effect of the size of the rehearsal memory that corroborates the efficacy of our approach for low-resource devices",
    "checked": true,
    "id": "2802090f23e01cc1341a767daf825c6256341a0f",
    "semantic_title": "an investigation of the combination of rehearsal and knowledge distillation in continual learning for spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23h_interspeech.html": {
    "title": "Enhancing New Intent Discovery via Robust Neighbor-based Contrastive Learning",
    "volume": "main",
    "abstract": "New intent discovery (NID) has become a hot topic for dialogue system, which aims to discover the Out-Of-Domain intents from conversation corpus and classify these utterances correctly. Existing methods usually focus on learning compact representations of utterances, and leverage the clustering algorithm to generate new intents. Inspired by the recent progress of contrastive learning, in this work, we propose a novel neighbor-based contrastive learning (NCL) to obtain discriminative representations for utterances. Specifically, to enhance the robustness of NCL, on the one hand, we pick out diverse samples as positive pairs by considering both the anchor neighborhood and nearby neighborhood. On the other hand, we also devise a boundary distance constraint to avoid introducing noisy samples when extending the positives via neighbors. Extensive experiments are conducted on three public NID datasets and the results demonstrate the competitiveness and effectiveness of our proposed approach",
    "checked": true,
    "id": "b789782866e0ba2078353dc5a3331a589263b9f5",
    "semantic_title": "enhancing new intent discovery via robust neighbor-based contrastive learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schwarz23_interspeech.html": {
    "title": "Personalized Predictive ASR for Latency Reduction in Voice Assistants",
    "volume": "main",
    "abstract": "Streaming Automatic Speech Recognition (ASR) in voice assistants can utilize prefetching to partially hide the latency of response generation. Prefetching involves passing a preliminary ASR hypothesis to downstream systems in order to prefetch and cache a response. If the final ASR hypothesis after endpoint detection matches the preliminary one, the cached response can be delivered to the user, thus saving latency. In this paper, we extend this idea by introducing predictive automatic speech recognition, where we predict the full utterance from a partially observed utterance, and prefetch the response based on the predicted utterance. We introduce two personalization approaches and investigate the tradeoff between potential latency gains from successful predictions and the cost increase from failed predictions. We evaluate our methods on an internal voice assistant dataset as well as the public SLURP dataset",
    "checked": true,
    "id": "0fed61ee16a5fe55a74a7a1791f869c1ac2b67c8",
    "semantic_title": "personalized predictive asr for latency reduction in voice assistants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ray23_interspeech.html": {
    "title": "Compositional Generalization in Spoken Language Understanding",
    "volume": "main",
    "abstract": "State-of-the-art spoken language understanding (SLU) models have shown tremendous success in benchmark SLU datasets, yet they still fail in many practical scenario due to the lack of model compositionality when trained on limited training data. In this paper, we study two types of compositionality: novel slot combination, and length generalization. We first conduct in-depth analysis, and find that state-of-the-art SLU models often learn spurious slot correlations during training, which leads to poor performance in both compositional cases. To mitigate these limitations, we create the first compositional splits of benchmark SLU datasets and we propose the first compositional SLU model, including compositional loss and paired training that tackle each compositional case respectively. On both benchmark and compositional splits in ATIS and SNIPS, we show that our compositional SLU model significantly outperforms (up to 5% F1 score) state-of-the-art BERT SLU model",
    "checked": true,
    "id": "0543cd2268bb70b7557a620019022f37350b92b7",
    "semantic_title": "compositional generalization in spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ha_interspeech.html": {
    "title": "Sampling bias in NLU models: Impact and Mitigation",
    "volume": "main",
    "abstract": "Natural Language Understanding (NLU) systems such as chatbots or virtual assistants have seen a significant rise in popularity in recent times, thanks to availability of large volumes of user data. However, typical user data collected for training such models may suffer from sampling biases due to a variety of factors. In this paper, we study the impact of bias in the training data for intent classification task, a core component of NLU systems. We experiment with three kinds of data bias settings: (i) random down-sampling, (ii) class-dependent bias, and (iii) class-independent bias injection. For each setting, we report the loss in model performance and survey strategies to mitigate the loss from two families of methods: (i) semi-supervised learning (SSL), and (ii) synthetic data generation. Overall, we find that while both methods perform well with random down-sampling, synthetic data generation out-performs SSL when only biased training data is available",
    "checked": true,
    "id": "fd60853c71938007a8564d53b25f4a15e746a4d5",
    "semantic_title": "sampling bias in nlu models: impact and mitigation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23d_interspeech.html": {
    "title": "5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair",
    "volume": "main",
    "abstract": "Providing voice assistants the ability to navigate multi-turn conversations is a challenging problem. Handling multi-turn interactions requires the system to understand various conversational use-cases, such as steering, intent carryover, disfluencies, entity carryover, and repair. The complexity of this problem is compounded by the fact that these use-cases mix with each other, often appearing simultaneously in natural language. This work proposes a non-autoregressive query rewriting architecture that can handle not only the five aforementioned tasks, but also complex compositions of these use-cases. We show that our proposed model has competitive single task performance compared to the baseline approach, and even outperforms a fine-tuned T5 model in use-case compositions, despite being 15 times smaller in parameters and 25 times faster in latency",
    "checked": true,
    "id": "f7b21c1a591129d926d8ddb6b1babe2b0e140b50",
    "semantic_title": "5ider: unified query rewriting for steering, intent carryover, disfluencies, entity carryover and repair",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23e_interspeech.html": {
    "title": "Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation",
    "volume": "main",
    "abstract": "The aim of emotion prediction in conversation (EPC) is to predict the future emotional state of a speaker based on context information, which is essential for conducting a friendly human-computer conversation. Most EPC works only investigated context information by merging a speaker's multiple utterances into a single utterance per turn and focused on conversations in a dual-speaker scenario, which ignored the information in multi-utterance turn and a more complex and natural scenario of multi-speaker conversations. This paper introduces a context information modeling approach that considers potential emotional interactive information within a speaker's multi-utterance turn, which dominates his/her future emotions. Moreover, our approach advances emotion prediction in both dual- and multi-speaker conversations. Experimental results show that such an approach significantly enhances context information modeling and renders a higher accuracy in EPC than reported in the literature",
    "checked": true,
    "id": "e45ee1978b572b07ffb578c0dcd855510015d56c",
    "semantic_title": "emotion awareness in multi-utterance turn for improving emotion prediction in multi-speaker conversation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ga_interspeech.html": {
    "title": "WhiSLU: End-to-End Spoken Language Understanding with Whisper",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) systems commonly use cascading structures. However, these systems are prone to error propagation, information loss, high costs, and latency, leading researchers to explore end-to-end (E2E) SLU as a hot topic. However, E2E SLU faces the challenge of insufficient data, resulting in most previous work relying on pretrained acoustic models. Nevertheless, pre-training task and SLU task solution spaces are often substantially different, making it difficult for E2E SLU models to surpass cascading models. To address this, we propose using OpenAI's Whisper model for SLU tasks. We employ the Sequence-level Multitask Learning (SML) paradigm, which encodes multiple ASR-related tasks into a sequence for learning. Our method significantly outperforms the E2E baseline by a large margin (with a 10% improvement in EM score) and even outperforms cascading models, achieving a 77% EM score on the STOP dataset, demonstrating its effectiveness",
    "checked": true,
    "id": "005d3c8d253c940f5b709258df5975dde7f17259",
    "semantic_title": "whislu: end-to-end spoken language understanding with whisper",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wen23b_interspeech.html": {
    "title": "Biophysically-inspired single-channel speech enhancement in the time domain",
    "volume": "main",
    "abstract": "Most state-of-the-art speech enhancement (SE) methods utilize time-frequency (T-F) features or waveforms as input features and have poor generalizability at negative signal-to-noise ratios (SNR). To overcome these issues, we propose a novel network that integrates biophysical properties of the human auditory system known to perform even at negative SNRs. We generated biophysical features using CoNNear, a neural network auditory model, which were fed into a SOTA speech enhancement model AECNN. The model was trained on the INTERSPEECH 2021 DNS Challenge dataset and evaluated on mismatched noise conditions at various SNRs. The experimental results revealed that the bio-inspired approaches outperformed T-F and waveform features under positive SNRs and demonstrated stronger robustness to unseen noise at negative SNRs. We conclude that incorporating human-like features can extend the operating range of SE systems to more negative SNRs",
    "checked": true,
    "id": "b1a4342950e0081fb86220105d660e36eb59a6fa",
    "semantic_title": "biophysically-inspired single-channel speech enhancement in the time domain",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jalal23_interspeech.html": {
    "title": "On-Device Speaker Anonymization of Acoustic Embeddings for ASR based on Flexible Location Gradient Reversal Layer",
    "volume": "main",
    "abstract": "Smart devices serviced by large-scale AI models necessitates user data transfer to the cloud for inference. For speech applications, this means transferring private user information, e.g., speaker identity. Our paper proposes a privacy-enhancing framework that targets speaker identity anonymization while preserving speech recognition accuracy for our downstream task Automatic Speech Recognition (ASR). The proposed framework attaches flexible gradient reversal based speaker adversarial layers to target layers within an ASR model, where speaker adversarial training anonymizes acoustic embeddings generated by the targeted layers to remove speaker identityy. We propose on-device deployment by execution of initial layers of the ASR model, and transmitting anonymized embeddings to the cloud, where the rest of the model is executed while preserving privacy. The results show that our method efficiently reduces speaker recognition relative accuracy by 33%, and improves ASR performance by achieving 6.2% relative Word Error Rate (WER) reduction",
    "checked": false,
    "id": "a9563078af069997af352dfe6e9205404d787f5c",
    "semantic_title": "on-device speaker anonymization of acoustic embeddings for asr based onflexible location gradient reversal layer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23b_interspeech.html": {
    "title": "How to Construct Perfect and Worse-than-Coin-Flip Spoofing Countermeasures: A Word of Warning on Shortcut Learning",
    "volume": "main",
    "abstract": "Shortcut learning, or 'Clever Hans effect' refers to situations where a learning agent (e.g., deep neural networks) learns spurious correlations present in data, resulting in biased models. We focus on finding shortcuts in deep learning based spoofing countermeasures (CMs) that predict whether a given utterance is spoofed or not. While prior work has addressed specific data artifacts, such as silence, no general normative framework has been explored for analyzing shortcut learning in CMs. In this study, we propose a generic approach to identifying shortcuts by introducing systematic interventions on the training and test sides, including the boundary cases of 'near-perfect' and 'worse than coin flip' (label flip). By using three different models, ranging from classic to state-of-the-art, we demonstrate the presence of shortcut learning in five simulated conditions. We also analyze the results using a regression model to understand how biases affect the class-conditional score statistics",
    "checked": true,
    "id": "d499af874ef3bd4c7de351b42d479efad8c79a7c",
    "semantic_title": "how to construct perfect and worse-than-coin-flip spoofing countermeasures: a word of warning on shortcut learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23c_interspeech.html": {
    "title": "CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram",
    "volume": "main",
    "abstract": "In this work, we present CleanUNet 2, a speech denoising model that combines the advantages of waveform denoiser and spectrogram denoiser and achieves the best of both worlds. CleanUNet 2 uses a two-stage framework inspired by popular speech synthesis methods that consist of a waveform model and a spectrogram model. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art waveform denoiser, and further boosts its performance by taking predicted spectrograms from a spectrogram denoiser as the input. We demonstrate that CleanUNet 2 outperforms previous methods in terms of various objective and subjective evaluations",
    "checked": true,
    "id": "8926261bf0181a30d92efb0990e924ce2cb5f552",
    "semantic_title": "cleanunet 2: a hybrid speech denoising model on waveform and spectrogram",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23e_interspeech.html": {
    "title": "A Two-stage Progressive Neural Network for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Recent studies in deep learning based acoustic echo cancellation proves the benefits of introducing a linear echo cancellation module. However, the convergence problem and potential target speech distortion impose an additional learning burden for the neural network. In this paper, we propose a two-stage progressive neural network consisting of a coarse-stage and a fine-stage module. For the coarse-stage, a light-weighted network module is designed to suppress partial echo and potential noise, where a voice activity detection path is used to enhance the learned features. For the fine-stage, a larger network is employed to deal with the more complex echo path and restore the near-end speech. We have conducted extensive experiments to verify the proposed method, and the results show that the proposed two-stage method provides a superior performance to other state-of-the-art methods",
    "checked": true,
    "id": "ff45fad1ca0be6c76264aa11a4c4e11eed8c718b",
    "semantic_title": "a two-stage progressive neural network for acoustic echo cancellation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23_interspeech.html": {
    "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
    "volume": "main",
    "abstract": "Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, underutilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beamsearch Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps",
    "checked": true,
    "id": "223569e02810238817ea7fbb12386410a04bcb7f",
    "semantic_title": "an intra-brnn and gb-rvq based end-to-end neural audio codec",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23r_interspeech.html": {
    "title": "Real-Time Personalised Speech Enhancement Transformers with Dynamic Cross-attended Speaker Representations",
    "volume": "main",
    "abstract": "Personalised speech enhancement (PSE) extracts only the speech of a target user and removes everything else from corrupted input audio. This can greatly improve on-device streaming audio processing, such as voice calls and speech recognition, which has strict requirements on model size and latency. To focus the PSE system on the target speaker, it is conditioned on a recording of the user's voice. This recording is usually summarised as a single static vector. However, a static vector cannot reflect all the target user's voice characteristics. Thus, we propose using the full recording. To condition on such a variable-length sequence, we propose fully Transformer-based PSE models with a cross-attention mechanism which generates target speaker representations dynamically. To better reflect the on-device scenario, we carefully design and publish a new PSE dataset. On the dataset, our proposed model significantly surpasses strong baselines while halving the model size and reducing latency",
    "checked": true,
    "id": "f0e33ad6855ba4f977b73f897484d8d46faee9c1",
    "semantic_title": "real-time personalised speech enhancement transformers with dynamic cross-attended speaker representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mamun23_interspeech.html": {
    "title": "CFTNet: Complex-valued Frequency Transformation Network for Speech Enhancement",
    "volume": "main",
    "abstract": "It is widely known that the presence of multi-speaker babble noise greatly degrades speech intelligibility. However, suppressing noise without creating artifacts in human speech is challenging in environments with a low signal-to-noise ratio (SNR), and even more so if noise is speechlike such as babble noise. Deep learning-based systems either enhance the magnitude response and reuse distorted phases or enhance the complex spectrogram. Frequency transformation block (FTB) has emerged as a useful architecture to implicitly capture harmonic correlation which is especially important for people with hearing loss (hearing aid/ cochlear implant users). This study proposes a complex-valued frequency transformation network (CFTNet) for speech enhancement, which leverages both a complex-valued U-Net and FTB to capture sufficient low-level contextual information. The proposed system learns a complex transformation matrix to accurately recover speech in the time-frequency domain from a noisy spectrogram. Experimental results demonstrate that the proposed system can achieve significant improvements in both seen and unseen noise over state-of-art networks. Furthermore, the proposed CFTNet can suppress highly nonstationary noise without creating musical artifacts commonly observed in conventional enhancement methods",
    "checked": true,
    "id": "202b65cb3c783aea57daecbd350b643bbc9acca9",
    "semantic_title": "cftnet: complex-valued frequency transformation network for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23h_interspeech.html": {
    "title": "Feature Normalization for Fine-tuning Self-Supervised Models in Speech Enhancement",
    "volume": "main",
    "abstract": "Large, pre-trained representation models trained using self-supervised learning have gained popularity in various fields of machine learning because they are able to extract high-quality salient features from input data. As such, they have been frequently used as base networks for various pattern classification tasks such as speech recognition. However, not much research has been conducted on applying these types of models to the field of speech signal generation. In this paper, we investigate the feasibility of using pre-trained speech representation models for a downstream speech enhancement task. To alleviate mismatches between the input features of the pre-trained model and the target enhancement model, we adopt a novel feature normalization technique to smoothly link these modules together. Our proposed method enables significant improvements in speech quality compared to baselines when combined with various types of pre-trained speech models",
    "checked": true,
    "id": "a323e0e959f8d8d3df94b2f05c2be3e9d1cfae68",
    "semantic_title": "feature normalization for fine-tuning self-supervised models in speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23c_interspeech.html": {
    "title": "Multi-mode Neural Speech Coding Based on Deep Generative Networks",
    "volume": "main",
    "abstract": "The wideband or super wideband speech is one of the most prominent features in real-time communication services, with higher resolution spectrum. However, it requires higher computing expenses. In this paper, we introduce the Penguins codec, based on a multi-mode neural speech coding structure that combines sub-band speech processing and applies different strategies from the low band to the high band. Especially, it refers to deep generative networks with perceptual constraint loss functions and knowledge distillations to reconstruct wideband components and bandwidth extension to generate artificial super wideband components. The method results in high-quality speech at very low bitrates. Several subjective and objective experiments, including ablation studies, were organized, and the results proved the merit of the proposed scheme when compared with traditional coding schemes and state-of-the-art neural coding methods",
    "checked": true,
    "id": "dbc492a0c1f2981be9d7a447730e52c72a5444a5",
    "semantic_title": "multi-mode neural speech coding based on deep generative networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bae23_interspeech.html": {
    "title": "Streaming Dual-Path Transformer for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement employing a dual-path transformer (DPT) with a dilated DenseNet-based encoder and decoder has shown state-of-the-art performance. By applying attention in both time and frequency paths, the DPT learns the long-term dependency of speech and the relationship between frequency components. However, the batch processing of the DPT, which performs attention on all past and future frames, makes it impractical for real-time applications. To satisfy the real-time requirement, we propose a streaming dual-path transformer (stDPT) with zero look-ahead structure. In the training phase, we apply masking techniques to control the context length, and in the inference phase, caching methods are utilized to preserve sequential information. Extensive experiments have been conducted to show the performance based on different context lengths, and the results verify that the proposed method outperforms the current state-of-the-art speech enhancement models based on real-time processing",
    "checked": true,
    "id": "4e2f2070a4ac85aec9ba32c6864868b3d4632560",
    "semantic_title": "streaming dual-path transformer for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kadkhodaeielyaderani23_interspeech.html": {
    "title": "Sequence-to-Sequence Multi-Modal Speech In-Painting",
    "volume": "main",
    "abstract": "Speech in-painting is the task of regenerating missing audio contents using reliable context information. Despite various recent studies in multi-modal perception of audio in-painting, there is still a need for an effective infusion of visual and auditory information in speech in-painting. In this paper, we introduce a novel sequence-to-sequence model that leverages the visual information to in-paint audio signals via an encoder-decoder architecture. The encoder plays the role of a lip-reader for facial recordings and the decoder takes both encoder outputs as well as the distorted audio spectrograms to restore the original speech. Our model outperforms an audio-only speech in-painting model and has comparable results with a recent multi-modal speech in-painter in terms of speech quality and intelligibility metrics for distortions of 300 ms to 1500 ms duration, which proves the effectiveness of the introduced multi-modality in speech in-painting",
    "checked": true,
    "id": "0ef005eaa93b572ff419bfe37f55b9d4358c9f63",
    "semantic_title": "sequence-to-sequence multi-modal speech in-painting",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23q_interspeech.html": {
    "title": "Hybrid AHS: A Hybrid of Kalman Filter and Deep Learning for Acoustic Howling Suppression",
    "volume": "main",
    "abstract": "Deep learning has been recently introduced for efficient acoustic howling suppression (AHS). However, the recurrent nature of howling creates a mismatch between offline training and streaming inference, limiting the quality of enhanced speech. To address this limitation, we propose a hybrid method that combines a Kalman filter with a self-attentive recurrent neural network (SARNN) to leverage their respective advantages for robust AHS. During offline training, a pre-processed signal obtained from the Kalman filter and an ideal microphone signal generated via teacher-forced training strategy are used to train the deep neural network (DNN). During streaming inference, the DNN's parameters are fixed while its output serves as a reference signal for updating the Kalman filter. Evaluation in both offline and streaming inference scenarios using simulated and real-recorded data shows that the proposed method efficiently suppresses howling and consistently outperforms baselines",
    "checked": true,
    "id": "ef989cfd7f93b3a8d61bfd546ee467f7548ac47e",
    "semantic_title": "hybrid ahs: a hybrid of kalman filter and deep learning for acoustic howling suppression",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ho23_interspeech.html": {
    "title": "Differentially Private Adapters for Parameter Efficient Acoustic Modeling",
    "volume": "main",
    "abstract": "In this work, we devise a parameter-efficient solution to bring differential privacy (DP) guarantees into adaptation of a cross-lingual speech classifier. We investigate a new frozen pretrained adaptation framework for DP-preserving speech modeling without full model fine-tuning. First, we introduce a noisy teacher-student ensemble into a conventional adaptation scheme leveraging a frozen pre-trained acoustic model and attain superior performance than DP-based stochastic gradient descent (DPSGD). Next, we insert residual adapters (RA) between layers of the frozen pre-trained acoustic model. The RAs reduce training cost and time significantly with a negligible performance drop. Evaluated on the open-access Multilingual Spoken Words (MLSW) dataset, our solution reduces the number of trainable parameters by 97.5% using the RAs with only a 4% performance drop with respect to fine-tuning the cross-lingual speech classifier while preserving DP guarantees",
    "checked": true,
    "id": "fad5ed6c02d2f6a2be6c474d14c70d9608777c5d",
    "semantic_title": "differentially private adapters for parameter efficient acoustic modeling",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23b_interspeech.html": {
    "title": "Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement through Knowledge Distillation",
    "volume": "main",
    "abstract": "Audio-visual speech enhancement (AV-SE) aims to enhance degraded speech along with extra visual information such as lip videos, and has been shown to be more effective than audio-only speech enhancement. This paper proposes further incorporating ultrasound tongue images to improve lip-based AV-SE systems' performance. Knowledge distillation is employed at the training stage to address the challenge of acquiring ultrasound tongue images during inference, enabling an audio-lip speech enhancement student model to learn from a pre-trained audio-lip-tongue speech enhancement teacher model. Experimental results demonstrate significant improvements in the quality and intelligibility of the speech enhanced by the proposed method compared to the traditional audio-lip speech enhancement baselines. Further analysis using phone error rates (PER) of automatic speech recognition (ASR) shows that palatal and velar consonants benefit most from the introduction of ultrasound tongue images",
    "checked": true,
    "id": "e9fe7cbfd3a4dca8d0ab9d37e03741c89e98fc7c",
    "semantic_title": "incorporating ultrasound tongue images for audio-visual speech enhancement through knowledge distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/uezu23_interspeech.html": {
    "title": "Consonant-emphasis Method Incorporating Robust Consonant-section Detection to Improve Intelligibility of Bone-conducted speech",
    "volume": "main",
    "abstract": "A consonant-emphasis (CE) method was proposed to improve the word intelligibility of presented speech by using bone-conducted (BC) headphones. However, the consonant-section detection (CSD) performance of this method is not robust against certain consonants. Therefore, a CE method with robust CSD is necessary for presented BC speech. We focused on improving the word intelligibility of presented BC speech in noisy environments and propose a CE method with robust CSD that combines the detection processes of voiced and unvoiced consonant sections. The evaluation of CSD procedures showed that more robust CSD procedure outperformed those of the conventional CE method as well as voiced CSD only and unvoiced CSD only. Word-intelligibility tests were also conducted on presented BC speech in noisy environments to compare the proposed and conventional methods, and the proposed method significantly improved word intelligibility over these conventional methods at a noise level of 75 dB",
    "checked": true,
    "id": "4589bb8446d21caa7d3e06e1e4f4a8778b1a3266",
    "semantic_title": "consonant-emphasis method incorporating robust consonant-section detection to improve intelligibility of bone-conducted speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sato23_interspeech.html": {
    "title": "Downstream Task Agnostic Speech Enhancement with Self-Supervised Representation Loss",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) is the latest breakthrough in speech processing, especially for label-scarce downstream tasks by leveraging massive unlabeled audio data. The noise robustness of the SSL is one of the important challenges to expanding its application. We can use speech enhancement (SE) to tackle this issue. However, the mismatch between the SE model and SSL models potentially limits its effect. In this work, we propose a new SE training criterion that minimizes the distance between clean and enhanced signals in the feature representation of the SSL model to alleviate the mismatch. We expect that the loss in the SSL domain could guide SE training to preserve or enhance various levels of characteristics of the speech signals that may be required for high-level downstream tasks. Experiments show that our proposal improves the performance of an SE and SSL pipeline on five downstream tasks with noisy input while maintaining the SE performance",
    "checked": true,
    "id": "649fd45481be158627e602252ce2d6c6c6a8713e",
    "semantic_title": "downstream task agnostic speech enhancement with self-supervised representation loss",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/byun23_interspeech.html": {
    "title": "Perceptual Improvement of Deep Neural Network (DNN) Speech Coder Using Parametric and Non-parametric Density Models",
    "volume": "main",
    "abstract": "This paper proposes a method to improve the perceptual quality of an end-to-end neural speech coder using density models for bottleneck samples. Two parametric and non-parametric approaches are explored for modeling the bottleneck sample density. The first approach utilizes a sub-network to generate mean-scale hyperpriors for bottleneck samples, while the second approach models the bottleneck samples using a separate sub-network without any side information. The whole network, including the sub-network, is trained using PAM-based perceptual losses in different timescales to shape quantization noise below the masking threshold. The proposed method achieves a frame-dependent entropy model that enhances arithmetic coding efficiency while emphasizing perceptually relevant audio cues. Experimental results show that the proposed density model combined with PAM-based losses improves perceptual quality compared to conventional speech coders in both objective and subjective tests",
    "checked": true,
    "id": "91c932d16623a7c8904af0aec728064a059843b6",
    "semantic_title": "perceptual improvement of deep neural network (dnn) speech coder using parametric and non-parametric density models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23j_interspeech.html": {
    "title": "DeFT-AN RT: Real-time Multichannel Speech Enhancement using Dense Frequency-Time Attentive Network and Non-overlapping Synthesis Window",
    "volume": "main",
    "abstract": "In real-time speech enhancement models based on the short-time Fourier transform (STFT), algorithmic latency induced by the STFT window size can induce perceptible delays, leading to reduced immersion in real-time applications. This study proposes an efficient real-time enhancement model based on dense frequency-time attentive network (DeFT-AN). The vanilla DeFT-AN consists of cascaded dense blocks and time-frequency transformers, which allow for a smooth transition between time frames through a temporal attention mechanism. To inherit this advantage and reduce algorithmic latency, we develop the lightweight and causal version of DeFT-AN with dual-window size processing that utilizes synthesis windows shorter than analysis windows. The benefit of DeFT-AN in identifying temporal context enables the use of non-overlapping synthesis windows, and experimental results show that the model can achieve the highest performance with the lowest algorithmic latency among STFT-based models",
    "checked": true,
    "id": "fc3e2cb1b0b3a850621bd8d293d9ab62abb5abaf",
    "semantic_title": "deft-an rt: real-time multichannel speech enhancement using dense frequency-time attentive network and non-overlapping synthesis window",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23b_interspeech.html": {
    "title": "A More Accurate Internal Language Model Score Estimation for the Hybrid Autoregressive Transducer",
    "volume": "main",
    "abstract": "We present a novel constrained learning method for hybrid autoregressive transducer (HAT) models that results in more validated language model (LM) adaptation. LM adaptation in HAT is justified only when the transducer logits and the sum of speech and text logits in the label estimation sub-networks are approximately the same. The mean squared error (MSE) between the two logits was added to the HAT loss to encourage the HAT models to satisfy the required condition. The proposed method exhibited significantly lower and more stable internal language model perplexities than those of HAT. Consequently, it attained lower word error rates (WERs) compared to HAT in various model architecture settings and in both cases with and without LM adaptation. In the television content task, the proposed method achieved a relative reduction in WERs of up to 28.60% compared to HAT. In most cases, the accuracy of pre-trained HAT models also improved upon training with the additional MSE loss",
    "checked": true,
    "id": "c1f4fd83d318525db351c7fac8e4dc7863662c87",
    "semantic_title": "a more accurate internal language model score estimation for the hybrid autoregressive transducer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23_interspeech.html": {
    "title": "Attention Gate Between Capsules in Fully Capsule-Network Speech Recognition",
    "volume": "main",
    "abstract": "We present a novel capsule network-based speech recognition model that effectively utilizes the full context of past time capsules. The input capsule sequences are recurrently used by filtering unnecessary contextual information using multi-head attention, which uses previous time output vectors as keys and values, and current time output vectors as queries. We applied the attention gate to the sequential dynamic routing (SDR), an all-capsule speech recognition model. The proposed method attained higher accuracy than the existing SDR with two attention heads on all test sets of the TIMIT and Wall Street Journal (WSJ) corpora while maintaining the same algorithmic delay. For the WSJ corpus, 10.75% of a relative word error rate (WER) reduction was achieved when the required delay was set to 525 ms. In addition, the model showed a 1.76x reduction in delay while maintaining the WERs. The proposed method results in an increase of approximately 0.1% in the number of parameters",
    "checked": true,
    "id": "97bb673f61e3b5370f22ff5527680e3e48f91201",
    "semantic_title": "attention gate between capsules in fully capsule-network speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rakib23_interspeech.html": {
    "title": "OOD-Speech: A Large Bengali Speech Recognition Dataset for Out-of-Distribution Benchmarking",
    "volume": "main",
    "abstract": "We present OOD-Speech, the first out-of-distribution (OOD) benchmarking dataset for Bengali automatic speech recognition (ASR). Bengali, being one of the most spoken languages, portrays large diversity in dialects and prosodic features, which demands ASR frameworks robust toward distribution shifts. For example, Islamic religious sermons in Bengali are delivered with a tonality significantly different from regular speech. To reflect this, our work comprises a large train set and a diverse test set. Our train dataset is collected via massively online crowd-sourcing campaigns resulting in 1178.30 hours being voluntarily contributed and curated from 22,645 native Bengali speakers from South Asia. Our test dataset comprises 22.67 hours of speech collected and manually annotated from 17 different sources, e.g., TV drama, Audiobooks, Online classes, Islamic sermons, etc. OOD-Speech is jointly the largest publicly available speech dataset & the first OOD ASR benchmarking dataset for Bengali",
    "checked": true,
    "id": "0079e59b1e2a4251ef0762defca108991aaf0744",
    "semantic_title": "ood-speech: a large bengali speech recognition dataset for out-of-distribution benchmarking",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23g_interspeech.html": {
    "title": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark",
    "volume": "main",
    "abstract": "Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research",
    "checked": true,
    "id": "090b284b2f8fc93ac3e7a92fc9f91bf4965ba75c",
    "semantic_title": "ml-superb: multilingual speech universal performance benchmark",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23l_interspeech.html": {
    "title": "General-purpose Adversarial Training for Enhanced Automatic Speech Recognition Model Generalization",
    "volume": "main",
    "abstract": "We present a new adversarial training method called General-purpose adversarial training (GPAT) that enhances the performance of automatic speech recognition models. In GPAT, we propose the followings: (1) a plausible adversarial examples converter (PAC); (2) a distribution matching regularization term (DM reg.). Compared to previous studies that directly compute gradients with respect to the input, PAC incorporates non-linearity to achieve performance improvement while eliminating the need for extra forward passes. Furthermore, unlike previous studies that use fixed norms, GPAT can generate similar yet diverse samples through DM reg. We demonstrate that the GPAT elevates the performance of various models on the LibriSpeech dataset. Specifically, by applying GPAT to the conformer model, we achieved 5.3% average relative improvements. With respect to the wav2vec 2.0 experiments, our method yielded a 2.0%/4.4% word error rate on the LibriSpeech test sets without a language model",
    "checked": true,
    "id": "818da1cd8dae525aec31b499e468003d0741a964",
    "semantic_title": "general-purpose adversarial training for enhanced automatic speech recognition model generalization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23_interspeech.html": {
    "title": "Joint Instance Reconstruction and Feature Subspace Alignment for Cross-Domain Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition is a popular research branch of speech signal processing. Many previous studies have proven that the generalization ability of the emotion recognition model across domains can be improved by using transfer learning methods. To solve the cross-domain speech emotion recognition problem, this paper proposes a novel transfer learning method, which simultaneously performs the instance reconstruction and subspace alignment. Firstly, we conduct the instance transferring based on coupled projection, which utilizes a weighting reconstruction strategy to exploit the intrinsic information of cross-domain samples and improve the contribution of essential features through an adaptive weighting matrix. Then, we conduct the feature transferring through a novel co-regularized term, which can make the source and target subspace be well aligned. Finally, extensive experiments indicate that our method is superior to several state-of-the-art methods",
    "checked": true,
    "id": "0e22929ae837d48dad4a47352754dc0b51c6e313",
    "semantic_title": "joint instance reconstruction and feature subspace alignment for cross-domain speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moriya23_interspeech.html": {
    "title": "Knowledge Distillation for Neural Transducer-based Target-Speaker ASR: Exploiting Parallel Mixture/Single-Talker Speech Data",
    "volume": "main",
    "abstract": "Neural transducer (RNNT)-based target-speaker speech recognition (TS-RNNT) directly transcribes a target speaker's voice from a multi-talker mixture. It is a promising approach for streaming applications because it does not incur the extra computation costs of a target speech extraction frontend, which is a critical barrier to quick response. TS-RNNT is trained end-to-end given the input speech (i.e., mixtures and enrollment speech) and reference transcriptions. The training mixtures are generally simulated by mixing single-talker signals, but conventional TS-RNNT training does not utilize single-speaker signals. This paper proposes using knowledge distillation (KD) to exploit the parallel mixture/single-talker speech data. Our proposed KD scheme uses an RNNT system pretrained with the target single-talker speech input to generate pseudo labels for the TS-RNNT training. Experimental results show that TS-RNNT systems trained with the proposed KD scheme outperform a baseline TS-RNNT",
    "checked": true,
    "id": "000ca94ed121225aa997088def719c747c4fb797",
    "semantic_title": "knowledge distillation for neural transducer-based target-speaker asr: exploiting parallel mixture/single-talker speech data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23i_interspeech.html": {
    "title": "Random Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition",
    "volume": "main",
    "abstract": "One of limitations in end-to-end automatic speech recognition (ASR) framework is its performance would be compromised if train-test utterance lengths are mismatched. In this paper, we propose an on-the-fly random utterance concatenation (RUC) based data augmentation method to alleviate train-test utterance length mismatch issue for short-video ASR task. Specifically, we are motivated by observations that our human-transcribed training utterances tend to be much shorter for short-video spontaneous speech (∼3 seconds on average), while our test utterance generated from voice activity detection front-end is much longer (∼10 seconds on average). Such a mismatch can lead to suboptimal performance. Empirically, it's observed the proposed RUC method significantly improves long utterance recognition without performance drop on short one. Overall, it achieves 5.72% word error rate reduction on average for 15 languages and improved robustness to various utterance length",
    "checked": true,
    "id": "92f621eec89837c3b3f353690881a9d409b07e67",
    "semantic_title": "random utterance concatenation based data augmentation for improving short-video speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muthuchamyselvaraj23_interspeech.html": {
    "title": "Adapter Incremental Continual Learning of Efficient Audio Spectrogram Transformers",
    "volume": "main",
    "abstract": "Efficient tuning of neural networks for continual learning with minimal computational resources remains a challenge. In this paper, we propose continual learning of audio classifiers with parameter and compute efficient Audio Spectrogram Transformers (AST). To reduce the trainable parameters without performance degradation we propose AST with Convolutional Adapter, which has less than 5% of trainable parameters of full fine-tuning. To reduce the computational complexity of self-attention, we introduce a novel Frequency-Time factorized Attention (FTA) method that achieves competitive performance with only a factor of the computations. Finally, we formulate our method called Adapter Incremental Continual Learning (AI-CL), as a combination of the parameter-efficient Convolutional Adapter and the compute-efficient FTA. Experiments on ESC-50, SpeechCommandsV2, and Audio-Visual Event benchmarks show that our proposed method efficiently learns new tasks and prevents catastrophic forgetting",
    "checked": true,
    "id": "81018236b57da31875a453866b884e3b3dda71e7",
    "semantic_title": "adapter incremental continual learning of efficient audio spectrogram transformers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23_interspeech.html": {
    "title": "Rethinking Speech Recognition with A Multimodal Perspective via Acoustic and Semantic Cooperative Decoding",
    "volume": "main",
    "abstract": "Attention-based encoder-decoder (AED) models have shown impressive performance in ASR. However, most existing AED methods neglect to simultaneously leverage both acoustic and semantic features in decoder, which is crucial for generating more accurate and informative semantic states. In this paper, we propose an Acoustic and Semantic Cooperative Decoder (ASCD) for ASR. In particular, unlike vanilla decoders that process acoustic and semantic features in two separate stages, ASCD integrates them cooperatively. To prevent information leakage during training, we design a Causal Multimodal Mask. Moreover, a variant Semi-ASCD is proposed to balance accuracy and computational cost. Our proposal is evaluated on the publicly available AISHELL-1 and aidatatang_200zh datasets using Transformer, Conformer, and Branchformer as encoders, respectively. The experimental results show that ASCD significantly improves the performance by leveraging both the acoustic and semantic information cooperatively",
    "checked": true,
    "id": "0be940a9578fe944175394ec8e0042cec258f7d2",
    "semantic_title": "rethinking speech recognition with a multimodal perspective via acoustic and semantic cooperative decoding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23b_interspeech.html": {
    "title": "Improving Code-Switching and Name Entity Recognition in ASR with Speech Editing based Data Augmentation",
    "volume": "main",
    "abstract": "Recently, end-to-end (E2E) automatic speech recognition (ASR) models have made great strides and exhibit excellent performance in general speech recognition. However, there remain several challenging scenarios that E2E models are not competent in, such as code-switching and named entity recognition (NER). Data augmentation is a common and effective practice for these two scenarios. However, the current data augmentation methods mainly rely on audio splicing and text-to-speech (TTS) models, which might result in discontinuous, unrealistic, and less diversified speech. To mitigate these potential issues, we propose a novel data augmentation method by applying the text-based speech editing model. The augmented speech from speech editing systems is more coherent and diversified, also more akin to real speech. The experimental results on code-switching and NER tasks show that our proposed method can significantly outperform the audio splicing and neural TTS based data augmentation systems",
    "checked": true,
    "id": "85fe4894afe95e326814d8b7fa41b308a0054255",
    "semantic_title": "improving code-switching and name entity recognition in asr with speech editing based data augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23h_interspeech.html": {
    "title": "Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts",
    "volume": "main",
    "abstract": "This paper presents a novel algorithm for building an automatic speech recognition (ASR) model with imperfect training data. Imperfectly transcribed speech is a prevalent issue in human-annotated speech corpora, which degrades the performance of ASR models. To address this problem, we propose Bypass Temporal Classification (BTC) as an expansion of the Connectionist Temporal Classification (CTC) criterion. BTC explicitly encodes the uncertainties associated with transcripts during training. This is accomplished by enhancing the flexibility of the training graph, which is implemented as a weighted finite-state transducer (WFST) composition. The proposed algorithm improves the robustness and accuracy of ASR systems, particularly when working with imprecisely transcribed speech corpora. Our implementation will be open-sourced",
    "checked": true,
    "id": "b620d46668d62930e41393168434118bb9a2bfcb",
    "semantic_title": "bypass temporal classification: weakly supervised automatic speech recognition with imperfect transcripts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lv23_interspeech.html": {
    "title": "DCCRN-KWS: An Audio Bias Based Model for Noise Robust Small-Footprint Keyword Spotting",
    "volume": "main",
    "abstract": "Real-world complex acoustic environments especially the ones with a low signal-to-noise ratio (SNR) will bring tremendous challenges to a keyword spotting (KWS) system. Inspired by the recent advances of neural speech enhancement and context bias in speech recognition, we propose a robust audio context bias based DCCRN-KWS model to address this challenge. We form the whole architecture as a multi-task learning framework for both denoising and keyword spotting, where the DCCRN encoder is connected with the KWS model. Helped with the denoising task, we further introduce an audio context bias module to leverage the real keyword samples and bias the network to better discriminate keywords in noisy conditions. Feature merge and complex context linear modules are also introduced to strengthen such discrimination and to effectively leverage contextual information respectively. Experiments on an internal challenging dataset and the HIMIYA public dataset show that DCCRN-KWS is superior in performance, while the ablation study demonstrates the good design of the whole model",
    "checked": true,
    "id": "4f9f170be7b22dc9ad5f220e8f1ea414a9399bbe",
    "semantic_title": "dccrn-kws: an audio bias based model for noise robust small-footprint keyword spotting",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23b_interspeech.html": {
    "title": "OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models have shown great promise over Supervised Learning (SL) ones in low-resource settings. However, the advantages of SSL are gradually weakened when the amount of labeled data increases in many industrial applications. To further improve the ASR performance when abundant labels are available, we first explore the potential of combining SL and SSL ASR models via analyzing their complementarity in recognition accuracy and optimization property. Then, we propose a novel Optimal Transport based Fusion (OTF) method for SL and SSL models without incurring extra computation cost in inference. Specifically, optimal transport is adopted to softly align the layer-wise weights to unify the two different networks into a single one. Experimental results on the public 1k-hour English LibriSpeech dataset and our in-house 2.6k-hour Chinese dataset show that OTF largely outperforms the individual models with lower error rates",
    "checked": true,
    "id": "3820ec258664fab9e279db8ca7d31d375e6e530b",
    "semantic_title": "otf: optimal transport based fusion of supervised and self-supervised learning models for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bleeker23_interspeech.html": {
    "title": "Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition",
    "volume": "main",
    "abstract": "This paper presents an extension to train end-to-end Context-Aware Transformer Transducer (CATT) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases in the context list during training, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications",
    "checked": true,
    "id": "96a2c0877721ab1915dd9baa38286d2d950691d6",
    "semantic_title": "approximate nearest neighbour phrase mining for contextual speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vandereeckt23_interspeech.html": {
    "title": "Rehearsal-Free Online Continual Learning for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Fine-tuning an Automatic Speech Recognition (ASR) model to new domains results in degradation on original domains, referred to as Catastrophic Forgetting (CF). Continual Learning (CL) attempts to train ASR models without suffering from CF. While in ASR, offline CL is usually considered, online CL is a more realistic but also more challenging scenario where the model, unlike in offline CL, does not know when a task boundary occurs. Rehearsal-based methods, which store previously seen utterances in a memory, are often considered for online CL, in ASR and other research domains. However, recent research has shown that weight averaging is an effective method for offline CL in ASR. Based on this result, we propose, in this paper, a rehearsal-free method applicable for online CL. Our method outperforms all baselines, including rehearsal-based methods, in two experiments. Our method is a next step towards general CL for ASR, which should enable CL in all scenarios with few if any constraints",
    "checked": true,
    "id": "2febc2e47c944378865796121466056156d195ae",
    "semantic_title": "rehearsal-free online continual learning for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23_interspeech.html": {
    "title": "Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring",
    "volume": "main",
    "abstract": "Speech fluency/disfluency can be evaluated by analyzing a range of phonetic and prosodic features. Deep neural networks are commonly trained to map fluency-related features into the human scores. However, the effectiveness of deep learning-based models is constrained by the limited amount of labeled training samples. To address this, we introduce a self-supervised learning (SSL) approach that takes into account phonetic and prosody awareness for fluency scoring. Specifically, we first pre-train the model using a reconstruction loss function, by masking phones and their durations jointly on a large amount of unlabeled speech and text prompts. We then fine-tune the pre-trained model using human-annotated scoring data. Our experimental results, conducted on datasets such as Speechocean762 and our non-native datasets, show that our proposed method outperforms the baseline systems in terms of Pearson correlation coefficients (PCC). Moreover, we also conduct an ablation study to better understand the contribution of phonetic and prosody factors during the pre-training stage",
    "checked": true,
    "id": "cf8e7ecad1de2403baff4e0b035fc7daa623bfda",
    "semantic_title": "phonetic and prosody-aware self-supervised learning approach for non-native fluency scoring",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23_interspeech.html": {
    "title": "Disentangling the Contribution of Non-native Speech in Automated Pronunciation Assessment",
    "volume": "main",
    "abstract": "This study explores the impact of using non-native speech data in acoustic model training for pronunciation assessment systems. The goal is to determine how introducing non-native data in acoustic model training can influence alignment accuracy and assessment performance. Acoustic models are trained using different combinations of native and non-native speech data, and the Goodness of Pronunciation (GOP) metric is used to evaluate performance. Results show that models trained with manually labeled non-native data yield the highest assessment performance and alignment accuracy. Models trained with mixed non-native and native data perform best when considering the GOP distribution on both non-native and native speech. Additionally, models trained with native data are more robust to alignment variations. These findings highlight the importance of carefully selecting and incorporating non-native data in acoustic model training for pronunciation assessment systems",
    "checked": true,
    "id": "d107c6611d5c705d0c2f1924b8b03166fadf4f5b",
    "semantic_title": "disentangling the contribution of non-native speech in automated pronunciation assessment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ryu23_interspeech.html": {
    "title": "A Joint Model for Pronunciation Assessment and Mispronunciation Detection and Diagnosis with Multi-task Learning",
    "volume": "main",
    "abstract": "Empirical studies report a strong correlation between pronunciation proficiency scores and phonetic errors in non-native speech assessments of human evaluators. However, the existing system of computer-assisted pronunciation training (CAPT) regards automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD) as independent and focuses on individual performance improvement. Motivated by the correlation between two tasks, we propose a novel architecture that jointly tackles APA and MDD using CTC and cross-entropy criteria with a multi-task learning scheme to benefit both tasks. To leverage additional knowledge transfer, Wav2Vec2-robust finetuned on TIMIT is used for the joint optimization. The integrated model significantly outperforms single-task learning, with a mean of 0.057 PCC increase for APA and 0.004 F1 increase for MDD on Speechocean762, which reveals that proficiency scores and phonetic errors are correlated for both human and model assessments",
    "checked": true,
    "id": "64a86c00541526821c4f8cf650691d1ae36879dd",
    "semantic_title": "a joint model for pronunciation assessment and mispronunciation detection and diagnosis with multi-task learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23d_interspeech.html": {
    "title": "Assessing Intelligibility in Non-native Speech: Comparing Measures Obtained at Different Levels",
    "volume": "main",
    "abstract": "Speech intelligibility (SI) is essential in communication and second language learning. In this study, non-native SI was measured through Visual Analogue Scale (VAS) scores and Orthographic Transcriptions (OTs) of read aloud sentences. Seven measures automatically derived from the OTs at word and subword levels were studied. The reliability of the intelligibility measures and the correlations between VAS scores and OT-based measures were also explored. Despite the different speaker language backgrounds, the recruited raters exhibited high scoring reliability. The correlations between VAS scores and OT-based measures were weak, corroborating previous assumptions that they refer to two related but distinct notions, comprehensibility (VAS) and intelligibility (OT). OT-based measures are reliable and valid indicators of SI. The results are discussed in relation to previous studies and avenues for future research are proposed",
    "checked": true,
    "id": "8de6875bdf6b1090d4e3f74ca1aee1f5ba47fd07",
    "semantic_title": "assessing intelligibility in non-native speech: comparing measures obtained at different levels",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23_interspeech.html": {
    "title": "End-to-End Word-Level Pronunciation Assessment with MASK Pre-training",
    "volume": "main",
    "abstract": "Pronunciation assessment is a major challenge in the computer-aided pronunciation training system, especially at the word (phoneme)-level. To obtain word (phoneme)-level scores, current methods usually rely on aligning components to obtain acoustic features of each word (phoneme), which limits the performance of assessment to the accuracy of alignments. Therefore, to address this problem, we propose a simple yet effective method, namely Masked pre-training for Pronunciation Assessment (MPA). Specifically, by incorporating a mask-predict strategy, our MPA supports end-to-end training without leveraging any aligning components and can solve misalignment issues to a large extent during prediction. Furthermore, we design two evaluation strategies to enable our model to conduct assessments in both unsupervised and supervised settings. Experimental results on SpeechOcean762 dataset demonstrate that MPA could achieve better performance than previous methods, without any explicit alignment. In spite of this, MPA still has some limitations, such as requiring more inference time and reference text. They expect to be addressed in future work",
    "checked": true,
    "id": "9c90d47bc4e4cdc53e409d810ed96882865f9c15",
    "semantic_title": "end-to-end word-level pronunciation assessment with mask pre-training",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chao23_interspeech.html": {
    "title": "A Hierarchical Context-aware Modeling Approach for Multi-aspect and Multi-granular Pronunciation Assessment",
    "volume": "main",
    "abstract": "Automatic Pronunciation Assessment (APA) plays a vital role in Computer-assisted Pronunciation Training (CAPT) when evaluating a second language (L2) learner's speaking proficiency. However, an apparent downside of most de facto methods is that they parallelize the modeling process throughout different speech granularities without accounting for the hierarchical and local contextual relationships among them. In light of this, a novel hierarchical approach is proposed in this paper for multi-aspect and multi-granular APA. Specifically, we first introduce the notion of sup-phonemes to explore more subtle semantic traits of L2 speakers. Second, a depth-wise separable convolution layer is exploited to better encapsulate the local context cues at the sub-word level. Finally, we use a score-restraint attention pooling mechanism to predict the sentence-level scores and optimize the component models with a multitask learning (MTL) framework. Extensive experiments carried out on a publicly-available benchmark dataset, viz. speechocean762, demonstrate the efficacy of our approach in relation to some cutting-edge baselines",
    "checked": true,
    "id": "6fc89eb8f2f8db50aff10ccdd2f2f5480e98e75e",
    "semantic_title": "a hierarchical context-aware modeling approach for multi-aspect and multi-granular pronunciation assessment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23j_interspeech.html": {
    "title": "Automatic Prediction of Language Learners' Listenability Using Speech and Text Features Extracted from Listening Drills",
    "volume": "main",
    "abstract": "When language learners are listening to L2 speech, they experience listening disfluencies or breakdowns not rarely. Although listening disfluencies are mental phenomena, previous studies showed that they can be measured acoustically by asking the learners to shadow the L2 speech, where inarticulate productions in shadowing are reasonably attributed to listening disfluencies. In this paper, we model the measured listening disfluencies by BLSTM and attempt to predict which words in new listening drills are difficult to perceive correctly. Taking some studies in psycholinguistics and applied linguistics into account, which revealed what kind of factors influence human perception of spoken words, speech and text features are extracted from listening drills and used for prediction. Experiments show that our model shows a better performance than other models previously proposed and that learners' factors are very effective for prediction because learners are developing through training",
    "checked": true,
    "id": "921014073a118aa8383b3101e2c4bde5fa0911c1",
    "semantic_title": "automatic prediction of language learners' listenability using speech and text features extracted from listening drills",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shekar23b_interspeech.html": {
    "title": "Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-level Goodness of Pronunciation Transformer",
    "volume": "main",
    "abstract": "Automatic pronunciation assessment (APA) plays an important role in providing feedback for self-directed language learners in computer-assisted pronunciation training (CAPT). Several mispronunciation detection and diagnosis (MDD) systems have achieved promising performance based on end-to-end phoneme recognition. However, assessing the intelligibility of second language (L2) remains a challenging problem. One issue is the lack of large-scale labeled speech data from non-native speakers. Additionally, relying only on one aspect (e.g., accuracy) at a phonetic level may not provide a sufficient assessment of pronunciation quality and L2 intelligibility. It is possible to leverage segmental/phonetic-level features such as goodness of pronunciation (GOP), however, feature granularity may cause a discrepancy in prosodic-level (suprasegmental) pronunciation assessment. In this study, Wav2vec 2.0-based MDD and Goodness Of Pronunciation feature-based Transformer are employed to characterize L2 intelligibility. Here, an L2 speech dataset, with human-annotated prosodic (suprasegmental) labels, is used for multi-granular and multi-aspect pronunciation assessment and identification of factors important for intelligibility in L2 English speech. The study provides a transformative comparative assessment of automated pronunciation scores versus the relationship between suprasegmental features and listener perceptions, which taken collectively can help support the development of instantaneous assessment tools and solutions for L2 learners",
    "checked": true,
    "id": "5028fa83522d5df0e7080f537b6b740bfc8f3718",
    "semantic_title": "assessment of non-native speech intelligibility using wav2vec2-based mispronunciation detection and multi-level goodness of pronunciation transformer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23f_interspeech.html": {
    "title": "Adapting an Unadaptable ASR System",
    "volume": "main",
    "abstract": "As speech recognition model sizes and training data requirements grow, it is increasingly common for systems to only be available via APIs from online service providers rather than having direct access to models themselves. In this scenario it is challenging to adapt systems to a specific target domain. To address this problem we consider the recently released OpenAI Whisper ASR as an example of a large-scale ASR system to assess adaptation methods. An error correction based approach is adopted, as this does not require access to the model, but can be trained from either 1-best or N-best outputs that are normally available via the ASR API. LibriSpeech is used as the primary target domain for adaptation. The generalization ability of the system in two distinct dimensions are then evaluated. First, whether the form of correction model is portable to other speech recognition domains, and secondly whether it can be used for ASR models having a different architecture",
    "checked": true,
    "id": "76f816ad49600bc0d4b5e2f0a8d6d935c994ad37",
    "semantic_title": "adapting an unadaptable asr system",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23c_interspeech.html": {
    "title": "Addressing Cold Start Problem for End-to-end Automatic Speech Scoring",
    "volume": "main",
    "abstract": "Integrating automatic speech scoring/assessment systems has become a critical aspect of second-language speaking education. With self-supervised learning advancements, end-to-end speech scoring approaches have exhibited promising results. However, this study highlights the significant decrease in the performance of speech scoring systems in new question contexts, thereby identifying this as a cold start problem in terms of items. With the finding of cold-start phenomena, this paper seeks to alleviate the problem by following methods: 1) prompt embeddings, 2) question context embeddings using BERT or CLIP models, and 3) choice of the pretrained acoustic model. Experiments are conducted on TOEIC speaking test datasets collected from English-as-a-second-language (ESL) learners rated by professional TOEIC speaking evaluators. The results demonstrate that the proposed framework not only exhibits robustness in a cold-start environment but also outperforms the baselines for known content",
    "checked": true,
    "id": "1da84c2b98eddd89b4c0248d425f315ccfedccbf",
    "semantic_title": "addressing cold start problem for end-to-end automatic speech scoring",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ribeiro23b_interspeech.html": {
    "title": "Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings",
    "volume": "main",
    "abstract": "The Grapheme-to-Phoneme (G2P) task aims to convert orthographic input into a discrete phonetic representation. G2P conversion is beneficial to various speech processing applications, such as text-to-speech and speech recognition. However, these tend to rely on manually-annotated pronunciation dictionaries, which are often time-consuming and costly to acquire. In this paper, we propose a method to improve the G2P conversion task by learning pronunciation examples from audio recordings. Our approach bootstraps a G2P with a small set of annotated examples. The G2P model is used to train a multilingual phone recognition system, which then decodes speech recordings with a phonetic representation. Given hypothesized phoneme labels, we learn pronunciation dictionaries for out-of-vocabulary words, and we use those to re-train the G2P system. Results indicate that our approach consistently improves the phone error rate of G2P systems across languages and amount of available data",
    "checked": true,
    "id": "a67dc585d180ec3a646d01799293dc34e3e05c18",
    "semantic_title": "improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/richter23b_interspeech.html": {
    "title": "Orthography-based Pronunciation Scoring for Better CAPT Feedback",
    "volume": "main",
    "abstract": "We establish the viability of a streamlined architecture for pedagogically appropriate computer assisted pronunciation training (CAPT), to give second language learners automatic feedback about their mispronunciations. This takes advantage of end-to-end speech recognition models to detect mispronunciation in audio segments that correspond directly to orthographic letters, in contrast to standard mispronunciation detection using phone representations. Results in a classification task show the potential for similar sensitivity to non-nativelike phonetic errors in grapheme-aligned segments as in phone-aligned segments. Advantages of this approach over phone-based pronunciation scoring can include providing naturally comprehensible (orthographic, not phonemic) feedback to learners, being inherently open-vocabulary in the target language, and evaluating pronunciations with reference to a full range of target-language acoustic variants rather than a prespecified canonical phone sequence",
    "checked": true,
    "id": "156d0b46faf275d83cb82e802611d7cde4dc0007",
    "semantic_title": "orthography-based pronunciation scoring for better capt feedback",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23r_interspeech.html": {
    "title": "Zero-Shot Automatic Pronunciation Assessment",
    "volume": "main",
    "abstract": "Automatic Pronunciation Assessment (APA) is vital for computer-assisted language learning. Prior methods rely on annotated speech-text data to train Automatic Speech Recognition (ASR) models or speech-score data to train regression models. In this work, we propose a novel zero-shot APA method based on the pre-trained acoustic model, HuBERT. Our method involves encoding speech input and corrupting them via a masking module. We then employ the Transformer encoder and apply k-means clustering to obtain token sequences. Finally, a scoring module is designed to measure the number of wrongly recovered tokens. Experimental results on speechocean762 demonstrate that the proposed method achieves comparable performance to supervised regression baselines and outperforms non-regression baselines in terms of Pearson Correlation Coefficient (PCC). Additionally, we analyze how masking strategies affect the performance of APA",
    "checked": true,
    "id": "0540a9c3710d68c880463e4f55ddeccd7f4cac35",
    "semantic_title": "zero-shot automatic pronunciation assessment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huu23_interspeech.html": {
    "title": "Mispronunciation detection and diagnosis model for tonal language, applied to Vietnamese",
    "volume": "main",
    "abstract": "A tonal language is a language in which the meaning of words is not only determined by the sounds of the consonants and vowels, but also by the pitch or tone used to pronounce them. Mispronunciation Detection and Diagnosis (MD&D) of tonal languages is challenging since tone presentation is difficult to be detected correctly. There has been relatively little research conducted on tonal languages, with most focusing on Mandarin. Furthermore, there are no publicly available datasets and source codes for the task. This work constructs and publishes a Vietnamese dataset for experimenting with MD&D, as well as proposes an end-to-end model that utilizes pitch analysis to detect and diagnose mispronunciations for tonal languages, especially focusing on Vietnamese. Experiments show that the proposed model achieved a relative improvement in phone error rate of 7.1% and detection accuracy of 7.4% compared to a state-of-the-art baseline",
    "checked": true,
    "id": "64cde42cf0e4b3dcda9ab7a0a4ecb552aa96ee00",
    "semantic_title": "mispronunciation detection and diagnosis model for tonal language, applied to vietnamese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dignum23_interspeech.html": {
    "title": "Beyond the AI hype: Balancing Innovation and Social Responsibility",
    "volume": "main",
    "abstract": "AI can extend human capabilities but requires addressing challenges in education, jobs, and biases. Taking a responsible approach involves understanding AI's nature, design choices, societal role, and ethical considerations. Recent AI developments, including foundational models, transformer models, generative models, and large language models (LLMs), raise questions about whether they are changing the paradigm of AI, and about the responsibility of those that are developing and deploying AI systems. In all these developments, is vital to understand that AI is not an autonomous entity but rather dependent on human responsibility and decision-making In this talk, I will further discuss the need for a responsible approach to AI that emphasize trust, cooperation, and the common good. Taking responsibility involves regulation, governance, and awareness. Ethics and dilemmas are ongoing considerations, but require understanding that trade-offs must be made and that decision processes are always contextual. Taking responsibility requires designing AI systems with values in mind, implementing regulations, governance, monitoring, agreements, and norms. Rather than viewing regulation as a constraint, it should be seen as a stepping stone for innovation, ensuring public acceptance, driving transformation, and promoting business differentiation. Responsible Artificial Intelligence (AI) is not an option but the only possible way to go forward in AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/stemmer23_interspeech.html": {
    "title": "Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach",
    "volume": "main",
    "abstract": "Speech emotion recognition for natural human-to-human conversations has many useful applications, including generating comprehensive meeting transcripts or detecting communication problems. We investigate the detection of emotional hotspots, i.e., regions of increased speaker involvement in technical meetings. As there is a scarcity of annotated, not-acted corpora, and to avoid introducing unwanted biases to our models, we follow a cross-corpus approach where models are trained on data from domains unrelated to the test data. In this work we propose a model ensemble trained on spontaneous phone conversations, political discussions and acted emotions. Evaluation is performed on the natural ICSI and AMI meeting corpora, where we used existing hotspot annotations for ICSI and created labels for the AMI corpus. A semi-supervised fine-tuning procedure is introduced to adapt the model. We show that an equal error rate of below 21% can be achieved using the proposed cross-corpus approach",
    "checked": true,
    "id": "c69cd5ba401a20261bbb9dd301c12e58aef7b3d7",
    "semantic_title": "detection of emotional hotspots in meetings using a cross-corpus approach",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matsuda23_interspeech.html": {
    "title": "Detection of Laughter and Screaming Using the Attention and CTC Models",
    "volume": "main",
    "abstract": "This study aimed to detect social signals, such as laughter and screams, in real environments. Social signals influence human-to-human communication. To effectively apply these signals in various systems, computer systems must appropriately detect social signals. In this study, social signal detection (SSD) experiments were conducted to demonstrate which of three feature sets, i.e., a spectral feature set, prosodic feature set, and spectral and prosodic feature set, was best for detecting laughter and screaming. The results showed that using both the spectral and prosodic feature sets yielded the best performance, with 81.83% accuracy for laughter and 81.68% accuracy for screams. Moreover, the detection model comparison results revealed that the bidirectional long short-term memory (BiLSTM)-connectionist temporal classification (CTC) yielded the best laughter detection performance, while attention-CTC was best for scream detection. These results suggest that CTC is effective for SSD",
    "checked": true,
    "id": "42671c0d5013b123fd5dc43e9d7e45d8fb8b6a7a",
    "semantic_title": "detection of laughter and screaming using the attention and ctc models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhattacharya23_interspeech.html": {
    "title": "Capturing Formality in Speech Across Domains and Languages",
    "volume": "main",
    "abstract": "The linguistic notion of formality is one dimension of stylistic variation in human communication. A universal characteristic of language production, formality has surface-level realizations in written and spoken language. In this work, we explore ways of measuring the formality of such realizations in multilingual speech corpora across a wide range of domains. We compare measures of formality, contrasting textual and acoustic-prosodic metrics. We believe that a combination of these should correlate well with downstream applications. Our findings include: an indication that certain prosodic variables might play a stronger role than others; no correlation between prosodic and textual measures; limited evidence for anticipated inter-domain trends, but some evidence of consistency of measures between languages. We conclude that non-lexical indicators of formality in speech may be more subtle than our initial expectations, motivating further work on reliably encoding spoken formality",
    "checked": true,
    "id": "36ca450788efeee276b1be7c1ababe05a32bc8e2",
    "semantic_title": "capturing formality in speech across domains and languages",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23e_interspeech.html": {
    "title": "Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio",
    "volume": "main",
    "abstract": "To perform automatic family audio analysis, past studies have collected recordings using phone, video, or audio-only recording device like LENA, investigated supervised learning methods, and used or fine-tuned general-purpose embeddings learned from large pretrained models. In this study, we advance the audio component of a new infant wearable multi-modal device called LittleBeats (LB) by learning family audio representation via wav2vec 2.0 (W2V2) pretraining. We show given a limited number of labeled LB home recordings, W2V2 pretrained using 1k-hour of unlabeled home recordings outperforms oracle W2V2 pretrained on 52k-hour unlabeled audio in terms of parent/infant speaker diarization (SD) and vocalization classifications (VC) at home. Extra relevant external unlabeled and labeled data further benefit W2V2 pretraining and fine-tuning. With SpecAug and environmental speech corruptions, we obtain 12% relative gain on SD and moderate boost on VC. Code and model weights are available",
    "checked": true,
    "id": "62a233ec8d3f5e9e64b725c99892cf7d42070839",
    "semantic_title": "towards robust family-infant audio analysis based on unsupervised pretraining of wav2vec 2.0 on large-scale unlabeled family audio",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feindt23_interspeech.html": {
    "title": "Cues to next-speaker projection in conversational Swedish: Evidence from reaction times",
    "volume": "main",
    "abstract": "We present first results of a study investigating the salience and typicality of prosodic markers in Swedish at turn ends for turn-yielding and turn-keeping purposes. We performed an experiment where participants (N=32) were presented with conversational chunks and, after the audio ended, were asked to determine which of two speakers would speak next by clicking a picture on a screen. Audio stimuli were manipulated by (i) raising and (ii) lowering fo over the last 500 ms of a turn, (iii) speeding up or (iv) slowing down duration over the last 500 ms, and (v) raising and (vi) lowering the last pitch peak. Out of all manipulations, increasing the speech rate was found to be the most disruptive p<.005). Higher speech rate led to longer reaction times in turn-keeping, which were shorter in turn-yielding. Other manipulations did not significantly alter reaction times. Results may be complemented with eye movement data, to elucidate cognitive mechanisms underlying turn-taking behavior",
    "checked": true,
    "id": "810fc956a506312d3690678525013e9be3bb7875",
    "semantic_title": "cues to next-speaker projection in conversational swedish: evidence from reaction times",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/buker23_interspeech.html": {
    "title": "Multiple Instance Learning for Inference of Child Attachment From Paralinguistic Aspects of Speech",
    "volume": "main",
    "abstract": "Attachment is a psychological construct that accounts for the way children perceive their relationship with their caregivers. Depending on the attachment condition, a child can either be secure or insecure. Identifying as many insecure children as possible is important to mitigate the negative consequences of insecure attachment in adult life. For this reason, this article proposes an attachment recognition approach that, compared to other approaches, increases the Recall, the percentage of insecure children identified as such. The approach is based on Multiple Instance Learning, a body of methodologies dealing with data represented as \"bags\" of feature vectors. This is suitable for speech recordings because these are typically represented as vector sequences. The experiments involved 104 participants of age 5 to 9. The results show that insecure children can be identified with Recall up to 63.3% (accuracy up to 75%), an improvement with respect to most existing models",
    "checked": true,
    "id": "20b135d33966d60a5fb042cee8cf34b7087b9822",
    "semantic_title": "multiple instance learning for inference of child attachment from paralinguistic aspects of speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eskimez23_interspeech.html": {
    "title": "Real-Time Joint Personalized Speech Enhancement and Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Personalized speech enhancement (PSE) is a real-time SE approach utilizing a speaker embedding of a target person to remove background noise, reverberation, and interfering voices. To deploy a PSE model for full duplex communications, the model must be combined with acoustic echo cancellation (AEC), although such a combination has been less explored. This paper proposes a series of methods that are applicable to various model architectures to develop efficient causal models that can handle the tasks of PSE, AEC, and joint PSE-AEC. We present extensive evaluation results using both simulated data and real recordings, covering various acoustic conditions and evaluation metrics. The results show the effectiveness of the proposed methods for two different model architectures. Our best joint PSE-AEC model comes close to the expert models optimized for individual tasks of PSE and AEC in their respective scenarios and significantly outperforms the expert models for the combined PSE-AEC task",
    "checked": true,
    "id": "fbfb5091ebbad691125d22ab76d2785bb0385b85",
    "semantic_title": "real-time joint personalized speech enhancement and acoustic echo cancellation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23g_interspeech.html": {
    "title": "TaylorBeamixer: Learning Taylor-Inspired All-Neural Multi-Channel Speech Enhancement from Beam-Space Dictionary Perspective",
    "volume": "main",
    "abstract": "Despite the promising performance of existing frame-wise all-neural beamformers in the speech enhancement field, it remains unclear what the underlying mechanism exists. In this paper, we revisit the beamforming behavior from the beam-space dictionary perspective and formulate it into the learning and mixing of different beam-space components. Based on that, we propose an all-neural beamformer called TaylorBM to simulate Taylor's series expansion operation in which the 0th-order term serves as a spatial filter to conduct the beam mixing, and several high-order terms are tasked with residual noise cancellation for post-processing. The whole system is devised to work in an end-to-end manner. Experiments are conducted on the spatialized LibriSpeech corpus and results show that the proposed approach outperforms existing advanced baselines in terms of evaluation metrics",
    "checked": true,
    "id": "74cf3364ccbfb60dbceffe5ba2e61429c6ceda45",
    "semantic_title": "taylorbeamixer: learning taylor-inspired all-neural multi-channel speech enhancement from beam-space dictionary perspective",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23s_interspeech.html": {
    "title": "MFT-CRN:Multi-scale Fourier Transform for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Convolutional recurrent networks (CRN) that combine a convolutional encoder-decoder (CED) structure with a recurrent structure have shown promising results in monaural speech enhancement. However, the commonly used short-time Fourier transform fails to balance the needs of frequency and time resolution effectively, which is crucial for accurate speech estimation. To address this issue, we propose MFT-CRN, a multi-scale short-time Fourier transform fusion model. We process the input speech signal through short-time Fourier transforms with different window functions, and add them layer by layer in the encoder and decoder of the network to achieve feature fusion with different window functions, effectively balancing frequency and temporal resolution. Comprehensive experiments on the WSJ0 dataset show that MFT-CRN significantly outperforms the method using only a single window function in terms of short-time intelligibility and perceptual evaluation of speech quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/guo23_interspeech.html": {
    "title": "Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement",
    "volume": "main",
    "abstract": "The goal of this study is to implement diffusion models for speech enhancement (SE). The first step is to emphasize the theoretical foundation of variance-preserving (VP)-based interpolation diffusion under continuous conditions. Subsequently, we present a more concise framework that encapsulates both the VP- and variance-exploding (VE)-based interpolation diffusion methods. We demonstrate that these two methods are special cases of the proposed framework. Additionally, we provide a practical example of VP-based interpolation diffusion for the SE task. To improve performance and ease model training, we analyze the common difficulties encountered in diffusion models and suggest amenable hyper-parameters. Finally, we evaluate our model against several methods using a public benchmark to showcase the effectiveness of our approach",
    "checked": true,
    "id": "84e6f4d2d30e289ae0131dba82b696836f1d0724",
    "semantic_title": "variance-preserving-based interpolation diffusion models for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/taherian23_interspeech.html": {
    "title": "Multi-input Multi-output Complex Spectral Mapping for Speaker Separation",
    "volume": "main",
    "abstract": "Current deep learning based multi-channel speaker separation methods produce a monaural estimate of speaker signals captured by a reference microphone. This work presents a new multi-channel complex spectral mapping approach that simultaneously estimates the real and imaginary spectrograms of all speakers at all microphones. The proposed multi-input multi-output (MIMO) separation model uses a location-based training (LBT) criterion to resolve the permutation ambiguity in talker-independent speaker separation across microphones. Experimental results show that the proposed MIMO separation model outperforms a multi-input single-output (MISO) speaker separation model with monaural estimates. We also combine the MIMO separation model with a beamformer and a MISO speech enhancement model to further improve separation performance. The proposed approach achieves the state-of-the-art speaker separation on the open LibriCSS dataset",
    "checked": true,
    "id": "fce2ec66047942842e5e92ac767f17360929c1c4",
    "semantic_title": "multi-input multi-output complex spectral mapping for speaker separation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oberhag23_interspeech.html": {
    "title": "Short-term Extrapolation of Speech Signals Using Recursive Neural Networks in the STFT Domain",
    "volume": "main",
    "abstract": "This paper investigates several approaches for the short-term extrapolation of speech signals. The signal extrapolation methods are embedded into a nested two-stage spectral analysis-synthesis system for single-channel noise reduction in hearing aids. They predict additional signal samples in the low-frequency sub-bands of the first analysis stage and may compensate the additional algorithmic latency of the second, higher-resolution analysis stage in these bands. We thus achieve a higher spectral resolution in frequency bands below 3 kHz without increasing the algorithmic latency of the overall system. In the context of noise reduction, especially female voices benefit from the increased spectral resolution in the lower sub-bands of the first stage. We show that among the investigated approaches, both recursive neural-network-based extrapolation methods provide benefits in conjunction with a noise reduction algorithm and outperform our baseline linear extrapolation method",
    "checked": true,
    "id": "e33774540f41f85d22584c5e401ebddb1a419622",
    "semantic_title": "short-term extrapolation of speech signals using recursive neural networks in the stft domain",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pandey23_interspeech.html": {
    "title": "Listener sensitivity to deviating obstruents in WaveNet",
    "volume": "main",
    "abstract": "This paper investigates the perceptual significance of the deviation in obstruents previously observed in WaveNet vocoders. The study involved presenting stimuli of varying lengths to 128 participants, who were asked to identify whether each stimulus was produced by a human or a machine. The participants' responses were captured using a 2-alternative forced choice task. The study found that while the length of the stimuli did not reliably affect participants' accuracy in the task, the concentration of obstruents did have a significant effect. Participants were consistently more accurate in identifying WaveNet stimuli as machine when the phrases were obstruent-rich. These findings show that the deviation in obstruents reported in WaveNet voices is perceivable by human listeners. The test protocol may be of wider utility in TTS",
    "checked": true,
    "id": "4a1bc694b2f7eb7b15bb885e7d6b8b124cfee36a",
    "semantic_title": "listener sensitivity to deviating obstruents in wavenet",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23d_interspeech.html": {
    "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
    "volume": "main",
    "abstract": "We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech",
    "checked": true,
    "id": "f70d0bb5685b8a119fcaa91c7e9b29c10cf403b6",
    "semantic_title": "how generative spoken language modeling encodes noisy speech: investigation from phonetics to syntactics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/camp23_interspeech.html": {
    "title": "MOS vs. AB: Evaluating Text-to-Speech Systems Reliably Using Clustered Standard Errors",
    "volume": "main",
    "abstract": "The quality of synthetic speech is typically evaluated using subjective listening tests. An underlying assumption is that these tests are reliable, i.e., running the test multiple times gives consistent results. A common approach to study reliability is a replication study. Existing studies focus primarily on Mean Opinion Score (MOS), and few consider the error bounds from the original test. In contrast, we present a replication study of both MOS and AB preference tests to answer two questions: (1) which of the two test types is more reliable for system comparison, and (2) for both test types, how reliable are the results with respect to their estimated standard error? We find that while AB tests are more reliable for system comparison, standard errors are underestimated for both test types. We show that these underestimates are partially due to broken independence assumptions, and suggest alternate methods of standard error estimation that account for dependencies among ratings",
    "checked": true,
    "id": "504bdd978507d86c3f27989479c7efe24b272e59",
    "semantic_title": "mos vs. ab: evaluating text-to-speech systems reliably using clustered standard errors",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23r_interspeech.html": {
    "title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting",
    "volume": "main",
    "abstract": "Automatic Mean Opinion Score (MOS) prediction is crucial to evaluate the perceptual quality of the synthetic speech. While recent approaches using pre-trained self-supervised learning (SSL) models have shown promising results, they only partly address the data scarcity issue for the feature extractor. This leaves the data scarcity issue for the decoder unresolved and leading to suboptimal performance. To address this challenge, we propose a retrieval-augmented MOS prediction method, dubbed RAMP, to enhance the decoder's ability against the data scarcity issue. A fusing network is also proposed to dynamically adjust the retrieval scope for each instance and the fusion weights based on the predictive confidence. Experimental results show that our proposed method outperforms the existing methods in multiple scenarios",
    "checked": true,
    "id": "6f6393908a5dd3059182da815876e1d5a7a7acfe",
    "semantic_title": "ramp: retrieval-augmented mos prediction via confidence-based dynamic weighting",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/melnikleroy23_interspeech.html": {
    "title": "Can Better Perception Become a Disadvantage? Synthetic Speech Perception in Congenitally Blind Users",
    "volume": "main",
    "abstract": "Modern Text-To-Speech systems are rarely tested on non-standard user groups, such as people with impairments. Nevertheless, evidence suggests that some of these groups might perceive synthetic speech differently (better or worse) than regular users. The current study investigated for the first time how synthetic speech is perceived by blind vs. sighted users. For this purpose, we used a speeded AX discrimination task and tested how sighted and blind listeners perceive synthetic speech of different qualities. Results show that blind participants had significantly better discrimination on this task, and both groups performed worse when the perceptual differences in the synthetic speech were smaller. This suggests that blind participants were indeed more sensitive to the acoustic characteristics of synthetic speech compared to their sighted peers. We discuss implications for speech perception and the development of modern speech technologies",
    "checked": true,
    "id": "0ea5ab42fe6c3d3cfc2409436bf8ea935b3a288e",
    "semantic_title": "can better perception become a disadvantage? synthetic speech perception in congenitally blind users",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cooper23_interspeech.html": {
    "title": "Investigating Range-Equalizing Bias in Mean Opinion Score Ratings of Synthesized Speech",
    "volume": "main",
    "abstract": "Mean Opinion Score (MOS) is a popular measure for evaluating synthesized speech. However, the scores obtained in MOS tests are heavily dependent upon many contextual factors. One such factor is the overall range of quality of the samples presented in the test -- listeners tend to try to use the entire range of scoring options available to them regardless of this, a phenomenon which is known as range-equalizing bias. In this paper, we systematically investigate the effects of range-equalizing bias on MOS tests for synthesized speech by conducting a series of listening tests in which we progressively \"zoom in\" on a smaller number of systems in the higher-quality range. This allows us to better understand and quantify the effects of range-equalizing bias in MOS tests",
    "checked": true,
    "id": "0eaa565c3745308f67993e8fa8e2d9c402e009d4",
    "semantic_title": "investigating range-equalizing bias in mean opinion score ratings of synthesized speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/he23_interspeech.html": {
    "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Recently, large pretrained language models have demonstrated strong language understanding capabilities. This is particularly reflected in their zero-shot and in-context learning abilities on downstream tasks through prompting. To assess their impact on spoken language understanding (SLU), we evaluate several such models like ChatGPT and OPT of different sizes on multiple benchmarks. We verify the emergent ability unique to the largest models as they can reach intent classification accuracy close to that of supervised models with zero or few shots on various languages given oracle transcripts. By contrast, the results for smaller models fitting a single GPU fall far behind. We note that the error cases often arise from the annotation scheme of the dataset; responses from ChatGPT are still reasonable. We show, however, that the model is worse at slot filling, and its performance is sensitive to ASR errors, suggesting serious challenges for the application of those textual models on SLU",
    "checked": true,
    "id": "daf9e24adbba3d1aead91cbac26502d3043db069",
    "semantic_title": "can chatgpt detect intent? evaluating large language models for spoken language understanding",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rajaa23_interspeech.html": {
    "title": "Improving End-to-End SLU performance with Prosodic Attention and Distillation",
    "volume": "main",
    "abstract": "Most End-to-End SLU methods depend on the pretrained ASR or language model features for intent prediction. However, other essential information in speech, such as prosody, is often ignored. Recent research has shown improved results in classifying dialogue acts by incorporating prosodic information. The margins of improvement in these methods are minimal as the neural models ignore prosodic features. In this work, we propose prosody-attention, which uses the prosodic features differently to generate attention maps across time frames of the utterance. Then we propose prosody-distillation to explicitly learn the prosodic information in the acoustic encoder rather than concatenating the implicit prosodic features. Both the proposed methods improve the baseline results, and the prosody-distillation method gives an intent classification accuracy improvement of 8% and 2% on SLURP and STOP datasets over the prosody baseline",
    "checked": true,
    "id": "2735ddc5618b54d5ce717adf3752921a4de511ce",
    "semantic_title": "improving end-to-end slu performance with prosodic attention and distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23n_interspeech.html": {
    "title": "Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "3c5470149173a6343346694d8da09a09c33c7d49",
    "semantic_title": "modality confidence aware training for robust end-to-end spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23c_interspeech.html": {
    "title": "Cross-Modal Semantic Alignment before Fusion for Two-Pass End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "The deliberation-based two-pass model that combines both semantic and acoustic information can effectively improve the performance of end-to-end (E2E) spoken language understanding (SLU). However, existing two-pass models usually simply fuse speech embedding and text embedding without taking into account the inherent distinctions between these two modalities. We propose a novel approach named Cross-modal Semantic Alignment before Fusion (CSAF), which adopt contrastive loss aligning speech and text embeddings before fusing them. We introduce a shared semantic memory transformer to project the embeddings from two modalities into a common semantic space, and a multi-modal gated network to generate the fused embeddings. We conduct experiments on the FSC Challenge test set and SLURP dataset. The results demonstrate that our method can significantly promote intent classification accuracy, achieving an absolute improvement of 3.1% over previous works in the FSC Challenge Utterance Set",
    "checked": true,
    "id": "426965f0e3b3dc24a55821915523b6032b2ad01a",
    "semantic_title": "cross-modal semantic alignment before fusion for two-pass end-to-end spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sunder23_interspeech.html": {
    "title": "ConvKT: Conversation-Level Knowledge Transfer for Context Aware End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "Dialog history enhances downstream classification performance in both speech and text based dialog systems. However, there still exists a gap in dialog history integration in a fully end-to-end (E2E) spoken dialog system (SDS) versus a textual dialog system. Text-based dialog systems use large language models (LLMs) to encode long-range dependencies by attending to the entire conversation as a contiguous token sequence. This is not possible in an E2E SDS, as speech sequences can be intractably long. We propose a convolution subsampling approach to make the speech sequence of a conversation tractable and use a conformer to attend to the speech-based conversation in a fine-grained manner. This model is further enhanced via a conversation-level knowledge transfer from a LLM using a token-level alignment strategy. Finetuning the E2E model pretrained this way gives significant gains, of up to 8%, over strong non-contextual baselines in the E2E dialog act classification task on two datasets",
    "checked": true,
    "id": "0d388b5c4a617792d62655af6b8ee1f71dab09f4",
    "semantic_title": "convkt: conversation-level knowledge transfer for context aware end-to-end spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23_interspeech.html": {
    "title": "GhostT5: Generate More Features with Cheap Operations to Improve Textless Spoken Question Answering",
    "volume": "main",
    "abstract": "Spoken question answering (SQA) aims to identify the correct answer to the given the question from a spoken passage. Most conventional SQA frameworks combine an automatic speech recognition (ASR) module and a text question answering (TQA) module in a cascaded manner, which might suffer from error propagation and high latency. To tackle these issues, several end-to-end SQA frameworks based on Textless NLP are proposed. However, existing end-to-end models still fail to outperform the cascade models with the similar number of parameters. In this paper, to improve textless SQA, we propose GhostT5, which generates more features from the remaining features with very cheap operations for stronger performance. Experiment results and further analysis show that our GhostT5 achieves the new state-of-the-art performance on NMSQA dataset and surpasses cascaded SQA models. More encouragingly, GhostT5 surpasses the previous best end-to-end SQA model with less than half of the parameters",
    "checked": true,
    "id": "dc0436318a08f4df8f9653f164a830f245caca8b",
    "semantic_title": "ghostt5: generate more features with cheap operations to improve textless spoken question answering",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23b_interspeech.html": {
    "title": "Obstructive Sleep Apnea Detection using Pre-trained Speech Representations",
    "volume": "main",
    "abstract": "Obstructive sleep apnea (OSA) is a condition commonly affecting middle-aged men that can disturb sleep, cause daytime tiredness, and increase the risk of heart disease. Speech can serve as a valuable biomarker for identifying and predicting the severity of OSA due to its connection with changes in throat structure. This study proposes a new deep-learning-based method for detecting OSA by analyzing speech recordings of participants in sitting and lying positions. The method utilizes a Siamese structure that employs a pre-trained XLSR model to encode ten utterances for each position, reducing the amount of necessary training data and enabling comparison of throat structure changes between the two positions through voice analysis. The study also explores the use of patient characteristic features. Results show this approach achieves an F1 value of 0.725 on our in-house dataset, proving the feasibility of end-to-end speech OSA detection with foundation models",
    "checked": true,
    "id": "0dc21b1086d042d696930e17b984b4327e4f12d0",
    "semantic_title": "obstructive sleep apnea detection using pre-trained speech representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23f_interspeech.html": {
    "title": "EEG-based Auditory Attention Detection with Spatiotemporal Graph and Graph Convolutional Network",
    "volume": "main",
    "abstract": "The ability to detect auditory attention from electroencephalography (EEG) offers many possibilities for brain-computer interface (BCI) applications, such as hearing assistive devices. However, effective feature representation for EEG signals remains a challenge due to the complex spatial and temporal dynamics of EEG signals. To overcome this challenge, we introduce a Spatiotemporal Graph Convolutional Network (ST-GCN), which combines a temporal attention mechanism and a graph convolutional module. The temporal attention mechanism captures the temporal dynamics of EEG segments, while the graph convolutional module learns the spatial pattern of multi-channel EEG signals. We evaluate the performance of our proposed ST-GCN on two publicly available datasets and demonstrate significant improvements over existing state-of-the-art models. These findings suggest that the ST-GCN model has the potential to advance auditory attention detection in real-life BCI applications",
    "checked": true,
    "id": "7d5bcf3e0a7274326628c5f406942e7ae859a671",
    "semantic_title": "eeg-based auditory attention detection with spatiotemporal graph and graph convolutional network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/beeson23_interspeech.html": {
    "title": "Silent Speech Recognition with Articulator Positions Estimated from Tongue Ultrasound and Lip Video",
    "volume": "main",
    "abstract": "We present a multi-speaker silent speech recognition system trained on articulator features derived from the Tongue and Lips corpus, a multi-speaker corpus of ultrasound tongue imaging and lip video data. We extracted articulator features using the pose estimation software DeepLabCut, then trained recognition models with these point-tracking features using Kaldi. We trained with voiced utterances, then tested performance on both voiced and silent utterances. Our multi-speaker SSR improved WER by 23.06% when compared to a previous similar multi-speaker SSR system which used image-based instead of point-tracking features. We also found great improvements (up to 15.45% decrease in WER) in recognition of silent speech using fMLLR adaptation compared to raw features. Finally, we investigated differences in articulator trajectories between voiced and silent speech and found that speakers tend to miss articulatory targets that are present in voiced speech when speaking silently",
    "checked": true,
    "id": "e1c80da2f7cf1fa7d3db4f8220bc7df0a8ff7041",
    "semantic_title": "silent speech recognition with articulator positions estimated from tongue ultrasound and lip video",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23r_interspeech.html": {
    "title": "Auditory Attention Detection in Real-Life Scenarios Using Common Spatial Patterns from EEG",
    "volume": "main",
    "abstract": "Auditory attention detection (AAD) methods based on electroencephalography (EEG) could be used in neuro-steered hearing devices to help hearing-loss people improve their hearing ability. However, previous studies have mostly obtained EEG data in laboratory settings which limits the practical application of neuro-steered hearing devices. In this study, we employ a common spatial pattern (CSP) algorithm to perform AAD using EEG signals collected by a wireless mobile EEG system, from real-life scenarios when people are walking and sitting. The results show that the CSP method can achieve AAD accuracy between 81.3% and 87.5% when using different decision windows (1 s- 30 s), which is better than previous methods based on linear mapping methods and convolutional neural networks (CNN). This proves that the CSP algorithm can decode people's attention efficiently even outside the laboratory. Analysis of EEG frequency bands shows that the δ and β bands have high activity in attention tasks",
    "checked": true,
    "id": "cbf24317cd7e287298ff2c4acce166adc2b4e35a",
    "semantic_title": "auditory attention detection in real-life scenarios using common spatial patterns from eeg",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23g_interspeech.html": {
    "title": "Diff-E: Diffusion-based Learning for Decoding Imagined Speech EEG",
    "volume": "main",
    "abstract": "Decoding EEG signals for imagined speech is a challenging task due to the high-dimensional nature of the data and low signal-to-noise ratio. In recent years, denoising diffusion probabilistic models (DDPMs) have emerged as promising approaches for representation learning in various domains. Our study proposes a novel method for decoding EEG signals for imagined speech using DDPMs and a conditional autoencoder named Diff-E. Results indicate that Diff-E significantly improves the accuracy of decoding EEG signals for imagined speech compared to traditional machine learning techniques and baseline models. Our findings suggest that DDPMs can be an effective tool for EEG signal decoding, with potential implications for the development of brain-computer interfaces that enable communication through imagined speech",
    "checked": true,
    "id": "465e3528f56edc662cbdde43fe9a02758411e5f1",
    "semantic_title": "diff-e: diffusion-based learning for decoding imagined speech eeg",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/csapo23_interspeech.html": {
    "title": "Towards Ultrasound Tongue Image prediction from EEG during speech production",
    "volume": "main",
    "abstract": "Previous initial research has already been carried out to propose speech-based BCI using brain signals (e.g. non-invasive EEG and invasive sEEG / ECoG), but there is a lack of combined methods that investigate non-invasive brain, articulation, and speech signals together and analyze the cognitive processes in the brain, the kinematics of the articulatory movement and the resulting speech signal. In this paper, we describe our multimodal (electroencephalography, ultrasound tongue imaging, and speech) analysis and synthesis experiments, as a feasibility study. We extend the analysis of brain signals recorded during speech production with ultrasound-based articulation data. From the brain signal measured with EEG, we predict ultrasound images of the tongue with a fully connected deep neural network. The results show that there is a weak but noticeable relationship between EEG and ultrasound tongue images, i.e. the network can differentiate articulated speech and neutral tongue position",
    "checked": true,
    "id": "a6987b9e00e23ef1b96d586404097bb3f7a1cc02",
    "semantic_title": "towards ultrasound tongue image prediction from eeg during speech production",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/toth23_interspeech.html": {
    "title": "Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks",
    "volume": "main",
    "abstract": "Thanks to the latest deep learning algorithms, silent speech interfaces (SSI) are now able to synthesize intelligible speech from articulatory movement data under certain conditions. However, the resulting models are rather speaker-specific, making a quick switch between users troublesome. Even for the same speaker, these models perform poorly cross-session, i.e. after dismounting and re-mounting the recording equipment. To aid quick speaker and session adaptation of ultrasound tongue imaging-based SSI models, we extend our deep networks with a spatial transformer network (STN) module, capable of performing an affine transformation on the input images. Although the STN part takes up only about 10% of the network, our experiments show that adapting just the STN module might allow to reduce MSE by 88% on the average, compared to retraining the whole network. The improvement is even larger (around 92%) when adapting the network to different recording sessions from the same speaker",
    "checked": true,
    "id": "9cf0034b9b1f4060b3dd171eb7b4289846e12315",
    "semantic_title": "adaptation of tongue ultrasound-based silent speech interfaces using spatial transformer networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/scheck23_interspeech.html": {
    "title": "STE-GAN: Speech-to-Electromyography Signal Conversion using Generative Adversarial Networks",
    "volume": "main",
    "abstract": "With Speech-to-Electromyography Generative Adversarial Network (STE-GAN), we propose a model which can synthesize Electromyography (EMG) signals from acoustic speech. We condition the generator network on representations of the spoken content obtained from a voice conversion model. Given these representations, the generator outputs an EMG signal corresponding to the articulated content of the acoustic speech in the setting of a specific EMG recording session. In comparison to previous work, STE-GAN directly generates EMG signals from acoustic speech. As it uses more speaker-independent content representations as input, it can synthesize EMG signals from speech of speakers who were unseen during training",
    "checked": true,
    "id": "432875ec218e05e1ab77c1b4266d1754b5239945",
    "semantic_title": "ste-gan: speech-to-electromyography signal conversion using generative adversarial networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/salomons23_interspeech.html": {
    "title": "Spanish Phone Confusion Analysis for EMG-Based Silent Speech Interfaces",
    "volume": "main",
    "abstract": "This paper describes a set of phone classification experiments based on electromyography (EMG) signals and a subsequent phone confusion analysis, as part of a project that aims to restore speech for Spanish laryngectomees by developing a Silent Speech Interface (SSI). Understanding the relationship between speech and the muscles used for speaking is essential to learn the possibilities and limitations of such EMG-based SSIs, before advancing to a complex task such as direct EMG-to-speech conversion. When considering only information from the muscles of the face and neck, important information from the tongue and vocal cords is missing. This is reflected in the results, which show confusion between pairs of phones that only differ in the position of the tongue or the voicing feature",
    "checked": true,
    "id": "bac5c07481ad7b769b72370b64f692b7c5ca046c",
    "semantic_title": "spanish phone confusion analysis for emg-based silent speech interfaces",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23l_interspeech.html": {
    "title": "Hybrid Silent Speech Interface Through Fusion of Electroencephalography and Electromyography",
    "volume": "main",
    "abstract": "Silent Speech Interface (SSI) can enable interaction in a new and natural way based on no-audible biosignals generated by the human body. Electroencephalography (EEG) or surface electromyography (sEMG) generated during speech production can be utilized to decode silent speech. However, obtaining complementary information from EEG and sEMG is still challenging. This paper presents a hybrid SSI based on the converter between bimodal electrophysiological signals and audio signals. EEG and sEMG are fused through two sequence-to-sequence models, and multi-task losses are applied to achieve complementarity between speech intention and muscle activity in silent speech. The feasibility of the proposed fusion method is validated in the silent speech dataset, and an average objective character error rate (CER) of 7.22% among eight speakers is obtained. The experimental results show that our bimodal-based hybrid SSI facilitates the conversion of electrophysiological signals to audio",
    "checked": true,
    "id": "474caa51703da1b8d2bb0d89833efbe54d205714",
    "semantic_title": "hybrid silent speech interface through fusion of electroencephalography and electromyography",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarkar23_interspeech.html": {
    "title": "Can Self-Supervised Neural Representations Pre-Trained on Human Speech distinguish Animal Callers?",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) models use only the intrinsic structure of a given signal, independent of its acoustic domain, to extract essential information from the input to an embedding space. This implies that the utility of such representations is not limited to modeling human speech alone. Building on this understanding, this paper explores the cross-transferability of SSL neural representations learned from human speech to analyze bio-acoustic signals. We conduct a caller discrimination analysis and a caller detection study on Marmoset vocalizations using eleven SSL models pre-trained with various pretext tasks. The results show that the embedding spaces carry meaningful caller information and can successfully distinguish the individual identities of Marmoset callers without fine-tuning. This demonstrates that representations pre-trained on human speech can be effectively applied to the bio-acoustics domain, providing valuable insights for future investigations in this field",
    "checked": true,
    "id": "f904b9ca82972a983b661a7d7855f52352d84dd3",
    "semantic_title": "can self-supervised neural representations pre-trained on human speech distinguish animal callers?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cai23b_interspeech.html": {
    "title": "Discovering COVID-19 Coughing and Breathing Patterns from Unlabeled Data Using Contrastive Learning with Varying Pre-Training Domains",
    "volume": "main",
    "abstract": "Rapid discovery of new diseases, such as COVID-19 can enable a timely epidemic response, preventing the large-scale spread and protecting public health. However, limited research efforts have been taken on this problem. In this paper, we propose a contrastive learning-based modeling approach for COVID-19 coughing and breathing pattern discovery from non-COVID coughs. To validate our models, extensive experiments have been conducted using four large audio datasets and one image dataset. We further explore the effects of different factors, such as domain relevance and augmentation order on the pre-trained models. Our results show that the proposed model can effectively distinguish COVID-19 coughing and breathing from unlabeled data and labeled non-COVID coughs with an accuracy of up to 0.81 and 0.86, respectively. Findings from this work will guide future research to detect an outbreak of a new disease early",
    "checked": true,
    "id": "0f28b2ef2d098bf8c194811afb0bb1280d48394d",
    "semantic_title": "discovering covid-19 coughing and breathing patterns from unlabeled data using contrastive learning with varying pre-training domains",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23_interspeech.html": {
    "title": "Background-aware Modeling for Weakly Supervised Sound Event Detection",
    "volume": "main",
    "abstract": "Nowadays, a common framework for weakly supervised sound event detection (WSSED) is multiple instance learning (MIL). However, MIL directly optimizes the clip-level classification results, so it tends to localize the most distinct part rather than the entire sound event, making the indiscriminating parts of sound events mistakenly identified as background sounds. In this paper, we focus on adding background awareness for WSSED by proposing a learning structure called BA-WSSED. Our BA-WSSED first introduces a pseudo separator with softmax activation and two aggregators to purify and aggregate the event feature and the background feature, respectively. Then, with the help of the proposed background-aware staggered (BAS) loss, both the event classifier and the background classifier are learned to generate staggered classification scores for discerning and suppressing background sounds. Experiments show that our BA-WSSED significantly improves the performance of the general MIL-based WSSED method on multiple datasets and can be employed on various baseline models",
    "checked": true,
    "id": "65ae26f921b0abebaaba29af2bcba82591a068e9",
    "semantic_title": "background-aware modeling for weakly supervised sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/srivastava23_interspeech.html": {
    "title": "How to (Virtually) Train Your Speaker Localizer",
    "volume": "main",
    "abstract": "Learning-based methods have become ubiquitous in speaker localization. Existing systems rely on simulated training sets for the lack of sufficiently large, diverse and annotated real datasets. Most room acoustics simulators used for this purpose rely on the image source method (ISM) because of its computational efficiency. This paper argues that carefully extending the ISM to incorporate more realistic surface, source and microphone responses into training sets can significantly boost the real-world performance of speaker localization systems. It is shown that increasing the training-set realism of a state-of-the-art direction-of-arrival estimator yields consistent improvements across three different real test sets featuring human speakers in a variety of rooms and various microphone arrays. An ablation study further reveals that every added layer of realism contributes positively to these improvements",
    "checked": true,
    "id": "50ec9df4395264e474850f4ed9ffd9573c4ad23b",
    "semantic_title": "how to (virtually) train your speaker localizer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghosh23b_interspeech.html": {
    "title": "MMER: Multimodal Multi-task Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose MMER, a novel Multimodal Multi-task learning approach for Speech Emotion Recognition. MMER leverages a novel multimodal network based on early-fusion and cross-modal self-attention between text and acoustic modalities and solves three novel auxiliary tasks for learning emotion recognition from spoken utterances. In practice, MMER outperforms all our baselines and achieves state-of-the-art performance on the IEMOCAP benchmark. Additionally, we conduct extensive ablation studies and results analysis to prove the effectiveness of our proposed approach",
    "checked": true,
    "id": "10ab7df49d5a61cf656d97092590af3ed4defd4f",
    "semantic_title": "mmer: multimodal multi-task learning for speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/khandelwal23_interspeech.html": {
    "title": "A Multi-Task Learning Framework for Sound Event Detection using High-level Acoustic Characteristics of Sounds",
    "volume": "main",
    "abstract": "Sound event detection (SED) entails identifying the type of sound and estimating its temporal boundaries from acoustic signals. These events are uniquely characterized by their spatio-temporal features, which are determined by the way they are produced. In this study, we leverage some distinctive high-level acoustic characteristics of various sound events to assist the SED model training, without requiring additional labeled data. Specifically, we use the DCASE Task 4 2022 dataset and categorize the 10 classes into four subcategories based on their high-level acoustic characteristics. We then introduce a novel multi-task learning framework that jointly trains the SED and high-level acoustic characteristics classification tasks, using shared layers and weighted loss. Our method significantly improves the performance of the SED system, achieving a 36.3% improvement in terms of the polyphonic sound event detection score compared to the baseline on the DCASE 2022 Task 4 validation set",
    "checked": true,
    "id": "3296c89f3338e5ff53097366dacfe44768450c4e",
    "semantic_title": "a multi-task learning framework for sound event detection using high-level acoustic characteristics of sounds",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/neumann23b_interspeech.html": {
    "title": "A Multimodal Investigation of Speech, Text, Cognitive and Facial Video Features for Characterizing Depression With and Without Medication",
    "volume": "main",
    "abstract": "Clinical depression is one of the most common mental disorders and technology for remote assessment of depression, including monitoring of treatment responses, is gaining more and more importance. Using a cloud-based multimodal dialog platform, we conducted a crowdsourced study to investigate the effect of depression severity and antidepressant use on various acoustic, linguistic, cognitive, and orofacial features. Our findings show that multiple features from all tested modalities show statistically significant differences between subjects with no or minimal depression and subjects with more severe depression symptoms. Moreover, certain acoustic and visual features show significant differences between subjects with moderately severe or severe symptoms who take antidepressants and those who do not take any. Machine learning experiments show that subjects with and without medication can be better discriminated from each other at higher severity levels",
    "checked": true,
    "id": "649703d09c09fd1d06a9c1f802302ba4bac533ce",
    "semantic_title": "a multimodal investigation of speech, text, cognitive and facial video features for characterizing depression with and without medication",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/addlesee23_interspeech.html": {
    "title": "Understanding Disrupted Sentences Using Underspecified Abstract Meaning Representation",
    "volume": "main",
    "abstract": "Voice assistant accessibility is generally overlooked as today's spoken dialogue systems are trained on huge corpora to help them understand the 'average' user. This raises frustrating barriers for certain user groups as their speech shifts from the average. People with dementia pause more frequently mid-sentence for example, and people with hearing impairments may mispronounce words learned post-diagnosis. We explore whether semantic parsing can improve accessibility for people with non-standard speech, and consequently become more robust to external disruptions like dogs barking, sirens passing, or doors slamming mid-utterance. We generate corpora of disrupted sentences paired with their underspecified Abstract Meaning Representation (AMR) graphs, and use these to train pipelines to understand and repair disruptions. Our best disruption recovery pipeline lost only 1.6% graph similarity f-score when compared to a model given the full original sentence",
    "checked": true,
    "id": "269a77ef89c0a88e6aac7a1bb0455bcbc39a7efa",
    "semantic_title": "understanding disrupted sentences using underspecified abstract meaning representation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/field23_interspeech.html": {
    "title": "Developing Speech Processing Pipelines for Police Accountability",
    "volume": "main",
    "abstract": "Police body-worn cameras have the potential to improve accountability and transparency in policing. Yet in practice, they result in millions of hours of footage that is never reviewed. We investigate the potential of large pre-trained speech models for facilitating reviews, focusing on ASR and officer speech detection in footage from traffic stops. Our proposed pipeline includes training data alignment and filtering, fine-tuning with resource constraints, and combining officer speech detection with ASR for a fully automated approach. We find that (1) fine-tuning strongly improves ASR performance on officer speech (WER=12-13%), (2) ASR on officer speech is much more accurate than on community member speech (WER=43.55-49.07%), (3) domain-specific tasks like officer speech detection and diarization remain challenging. Our work offers practical applications for reviewing body camera footage and general guidance for adapting pre-trained speech models to noisy multi-speaker domains",
    "checked": true,
    "id": "38c2e4e54a50ea5027c7a06ab325c22ece7b6c40",
    "semantic_title": "developing speech processing pipelines for police accountability",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szekely23_interspeech.html": {
    "title": "Prosody-controllable Gender-ambiguous Speech Synthesis: A Tool for Investigating Implicit Bias in Speech Perception",
    "volume": "main",
    "abstract": "This paper proposes a novel method to develop gender-ambiguous TTS, which can be used to investigate hidden gender bias in speech perception. Our aim is to provide a tool for researchers to conduct experiments on language use associated with specific genders. Ambiguous voices can also be beneficial for virtual assistants, to help reduce stereotypes and increase acceptance. Our approach uses a multi-speaker embedding in a neural TTS engine, combining two corpora recorded by a male and a female speaker to achieve a gender-ambiguous timbre. We also propose speaker-disentangled prosody control to ensure that the timbre is robust across a range of prosodies and enable more expressive speech. We optimised the output using an SSL-based network trained on hundreds of speakers. We conducted perceptual evaluations on the settings that were judged most ambiguous by the network, which showed that listeners perceived the speech samples as gender-ambiguous, also in prosody-controlled conditions",
    "checked": true,
    "id": "9428c0298159678aec0bc96e5ed1c4f79d7700c5",
    "semantic_title": "prosody-controllable gender-ambiguous speech synthesis: a tool for investigating implicit bias in speech perception",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rouas23_interspeech.html": {
    "title": "Affective attributes of French caregivers' professional speech",
    "volume": "main",
    "abstract": "In this paper, we detail our approach to studying the vocal characteristics of caregivers in retirement homes. To achieve this goal, we conducted recordings of 20 professional caregivers across two retirement homes. Using headset microphones connected to smartphones, we were able to capture the caregivers' speech while allowing them complete freedom of movement without compromising sound quality. The recordings consisted of three tasks: reading text, informal interviews, and professional role-play scenarios with a fictitious patient. We processed the recordings using an automatic speech recognition system, which provided word or phone sequences and their corresponding timestamps. Our analysis focused on identifying differences in emotional tone, lexical content, speech rate, fundamental frequency, and intensity between spontaneous speech conditions. Ultimately, our aim is to develop automated training tools that capture the unique vocal characteristics of professional caregivers",
    "checked": true,
    "id": "4fd98e75381201ca8eb87ff2e25ccd21e67dcc3c",
    "semantic_title": "affective attributes of french caregivers' professional speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/casanova23_interspeech.html": {
    "title": "ASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion",
    "volume": "main",
    "abstract": "We explore cross-lingual multi-speaker speech synthesis and cross-lingual voice conversion applied to data augmentation for automatic speech recognition (ASR) systems in low/medium-resource scenarios. Through extensive experiments, we show that our approach permits the application of speech synthesis and voice conversion to improve ASR systems using only one target-language speaker during model training. We also managed to close the gap between ASR models trained with synthesized versus human speech compared to other works that use many speakers. Finally, we show that it is possible to obtain promising ASR training results with our data augmentation method using only a single real speaker in a target language",
    "checked": true,
    "id": "32e3b5de1dca765c6a50f0797bb30e1dd012ae0e",
    "semantic_title": "asr data augmentation in low-resource settings using cross-lingual multi-speaker tts and cross-lingual voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23_interspeech.html": {
    "title": "Personality-aware Training based Speaker Adaptation for End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "Speaker adaptation has been widely studied to solve the mismatch between training and test conditions for end-to-end automatic speech recognition (ASR). A key challenge of speaker adaptation is lack of sufficient annotated target-speaker data. Considering the training set is always a large-scale one and contains various speakers, it is likely that utterances in the training set can have similar voice characters with the target speaker, and naturally those similar utterances can be treated as a supplement for target speaker data in the adaptation process. Therefore, we propose personality-aware training (PAT) framework to adapt a pre-trained ASR to the target speaker. In PAT, the small-scale target speaker data is viewed as anchors, and the losses of training samples are re-weighted according to the voice character similarity between the anchors and training samples, where the voice character similarity is derived from the speaker or prosody embedding extractor. Experiments on KeSpeech and MagicData corpora show that, compared with the unadapted system, the proposed method achieves 6.35% and 11.86% relative reduction on character error rate with only 10-minute pseudo-label and true-label adaptation data, respectively",
    "checked": true,
    "id": "fef7c232ad0a9c39e1f7887045dc7a27aca428e8",
    "semantic_title": "personality-aware training based speaker adaptation for end-to-end speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ito23b_interspeech.html": {
    "title": "Target Vocabulary Recognition Based on Multi-Task Learning with Decomposed Teacher Sequences",
    "volume": "main",
    "abstract": "This paper proposes a method for target vocabulary recognition based on multi-task learning with decomposed teacher sequences. The proposed method first decomposes teacher sequences into the target vocabulary and the non-target vocabulary sequences. Then, multi-task learning is performed by calculating losses for both the target vocabulary sequence and the non-target vocabulary sequence. By utilizing information from both target and non-target vocabulary, our proposed method provides more stable training and more accurate recognition of target vocabulary than single-task learning using only the target vocabulary. Experiments conducted on the Corpus of Spontaneous Japanese (CSJ) dataset, using numerals and katakana as target vocabulary, demonstrate the effectiveness of our proposed method. The results show a maximum CER improvement rate of 27% for katakana and 34% for numerals in target vocabulary recognition, as well as an 84% reduction in insertion errors in non-target vocabulary utterances",
    "checked": true,
    "id": "8923410a16dfd8aefb7d4d91ad7859f7019519b3",
    "semantic_title": "target vocabulary recognition based on multi-task learning with decomposed teacher sequences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shen23_interspeech.html": {
    "title": "Wave to Syntax: Probing spoken language models for syntax",
    "volume": "main",
    "abstract": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters",
    "checked": true,
    "id": "61190f218433126828863d20425202503ee4eaaf",
    "semantic_title": "wave to syntax: probing spoken language models for syntax",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naowarat23_interspeech.html": {
    "title": "Effective Training of Attention-based Contextual Biasing Adapters with Synthetic Audio for Personalised ASR",
    "volume": "main",
    "abstract": "Contextual biasing (CB) is an effective approach for contextualising hidden features of neural transducer ASR models to improve rare word recognition. CB relies on relatively large quantities of relevant human annotated natural speech during training, limiting its effectiveness in low-resource scenarios. In this work, we propose a novel approach that reduces the reliance on real speech by using synthesised audios for training CB adapters. We introduce a projection module (PM) that transforms encoder features of synthesised audios prior to CB training to better match real speech. We penalise PM with consistency regularisation to encourage higher similarity between features of real and synthesised speech. The proposed method maintains the same performance on both named-entity and general datasets while using half of the real speech data for CB training. Furthermore, we show a 16% word error rate reduction when the full real-speech training dataset is extended with synthetic utterances",
    "checked": true,
    "id": "4b873a002e365a593c5841ad02f7cad4e0d7835d",
    "semantic_title": "effective training of attention-based contextual biasing adapters with synthetic audio for personalised asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23b_interspeech.html": {
    "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
    "volume": "main",
    "abstract": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments",
    "checked": true,
    "id": "3e4397cf03ffb19d7bd6b7ce7b9dcb70da48d2f6",
    "semantic_title": "pushing the limits of unsupervised unit discovery for ssl speech representation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/haque23_interspeech.html": {
    "title": "SlothSpeech: Denial-of-service Attack Against Speech Recognition Models",
    "volume": "main",
    "abstract": "Deep Learning (DL) models have been popular nowadays to execute different speech-related tasks, including automatic speech recognition (ASR). As ASR is being used in different real-time scenarios, it is important that the ASR model remains efficient against minor perturbations to the input. Hence, evaluating efficiency robustness of the ASR model is the need of the hour. We show that popular ASR models like Speech2Text model and Whisper model have dynamic computation based on different inputs, causing dynamic efficiency. In this work, we propose SlothSpeech, a denial-of-service attack against ASR models, which exploits the dynamic behaviour of the model. SlothSpeech uses the probability distribution of the output text tokens to generate perturbations to the audio such that efficiency of the ASR model is decreased. We find that SlothSpeech generated inputs can increase the latency up to 40X times the latency induced by benign input",
    "checked": true,
    "id": "83be6de4499357fe78daeb2548c48cedc1bd8196",
    "semantic_title": "slothspeech: denial-of-service attack against speech recognition models",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23d_interspeech.html": {
    "title": "CLRL-Tuning: A Novel Continual Learning Approach for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a novel Continual Learning approach, which is Randomly Layer-wise Tuning (CLRL-Tuning) of a pre-trained Automatic Speech Recognition (ASR) model. CLRL-Tuning tackles the randomness of subsequent datasets by updating the parameters of randomly selected encoder layers of the pre-trained model (such as wav2vec 2.0) for every training epoch. CLRL-Tuning is different from the previous approaches in that it neither uses previous datasets, nor expands/runs previous models. Furthermore, we perform experiments to evaluate our approach compared with four strong baselines, including Knowledge Distillation and Gradient Episodic Memory. Our approach achieves significant improvements over the baselines in average word error rate (WER) for the wav2vec 2.0 model. Additionally, we implement ablation studies for our approach by tuning one, three, six and full encoder layers of the model, and experimental results show only tuning one encoder layer of the model at each training epoch is the most effective way to mitigate catastrophic forgetting",
    "checked": true,
    "id": "b6e2b95f5026a079a22d663592376aa9b8d4ab71",
    "semantic_title": "clrl-tuning: a novel continual learning approach for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23_interspeech.html": {
    "title": "Exploring Sources of Racial Bias in Automatic Speech Recognition through the Lens of Rhythmic Variation",
    "volume": "main",
    "abstract": "Although studies have shown that one issue of bias in modern automatic speech recognition (ASR) technologies is degraded performance for African American English (AAE) speakers, the mechanism by which systems fail for AAE speakers is still not well-understood. The present study aims to offer insight into this issue by examining whether errors are driven by rhythmic variation in ethnolects. We computed seven quantitative measures of speech rhythm in a reading task as produced by AAE and General American English (GAE) speakers and related these metrics to word error rates. The results confirmed racial bias against AAE speakers with higher error rates when AAE speakers produced more variable durations in vowel sounds. Rhythmic variation, on the other hand, is not a contributing factor for the errors in GAE. The result calls for interdisciplinary collaboration between linguists and ASR builders to add timing components of speech to the system to ensure fairness in artificial intelligence for currently underserved groups",
    "checked": true,
    "id": "36c54e4d686f7aca920bcf297258590ad158b638",
    "semantic_title": "exploring sources of racial bias in automatic speech recognition through the lens of rhythmic variation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23e_interspeech.html": {
    "title": "Can Contextual Biasing Remain Effective with Whisper and GPT-2?",
    "volume": "main",
    "abstract": "End-to-end automatic speech recognition (ASR) and large language models, such as Whisper and GPT-2, have recently been scaled to use vast amounts of training data. Despite the large amount of training data, infrequent content words that occur in a particular task may still exhibit poor ASR performance, with contextual biasing a possible remedy. This paper investigates the effectiveness of neural contextual biasing for Whisper combined with GPT-2. Specifically, this paper proposes integrating an adapted tree-constrained pointer generator (TCPGen) component for Whisper and a dedicated training scheme to dynamically adjust the final output without modifying any Whisper model parameters. Experiments across three datasets show a considerable reduction in errors on biasing words with a biasing list of 1000 words. Contextual biasing was more effective when applied to domain-specific data and can boost the performance of Whisper and GPT-2 without losing their generality",
    "checked": true,
    "id": "44c05783a548cd91110d901eb51b2b529c4b4fbc",
    "semantic_title": "can contextual biasing remain effective with whisper and gpt-2?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niizumi23_interspeech.html": {
    "title": "Masked Modeling Duo for Speech: Specializing General-Purpose Audio Representation to Speech using Denoising Distillation",
    "volume": "main",
    "abstract": "Self-supervised learning general-purpose audio representations have demonstrated high performance in a variety of tasks. Although they can be optimized for application by fine-tuning, even higher performance can be expected if they can be specialized to pre-train for an application. This paper explores the challenges and solutions in specializing general-purpose audio representations for a specific application using speech, a highly demanding field, as an example. We enhance Masked Modeling Duo (M2D), a general-purpose model, to close the performance gap with state-of-the-art (SOTA) speech models. To do so, we propose a new task, denoising distillation, to learn from fine-grained clustered features, and M2D for Speech (M2D-S), which jointly learns the denoising distillation task and M2D masked prediction task. Experimental results show that M2D-S performs comparably to or outperforms SOTA speech models on the SUPERB benchmark, demonstrating that M2D can specialize in a demanding field",
    "checked": true,
    "id": "032d17249f59232152b3b3ff8477213f786a8a93",
    "semantic_title": "masked modeling duo for speech: specializing general-purpose audio representation to speech using denoising distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23c_interspeech.html": {
    "title": "Improving RNN Transducer Acoustic Models for English Conversational Speech Recognition",
    "volume": "main",
    "abstract": "In this paper we investigate several techniques for improving the performance of RNN transducer (RNNT) acoustic models for conversational speech recognition and report state-of-the-art word error rates (WERs) on the 2000-hour Switchboard dataset. We show that n-best label smoothing and length perturbation which show improved performance on the smaller 300-hour dataset are also very effective on large datasets. We further give a rigorous theoretical interpretation of the n-best label smoothing based on stochastic approximation for training RNNT under the maximum likelihood criterion. Random quantization is also introduced to improve the generalization of RNNT models. On the 2000-hour Switchboard dataset, we report a single model performance of 4.9% and 7.7% WERs on the Switchboard and CallHome portions of NIST Hub5 2000, 7.1% on NIST Hub5 2001 and 6.8% on NIST RT03, without using external LMs",
    "checked": true,
    "id": "196c307f1ba2eecd7f890d51ab8e6916c826536d",
    "semantic_title": "improving rnn transducer acoustic models for english conversational speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23_interspeech.html": {
    "title": "MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we present MixRep, a simple and effective data augmentation strategy based on mixup for low-resource ASR. MixRep interpolates the feature dimensions of hidden representations in the neural network that can be applied to both the acoustic feature input and the output of each layer, which generalizes the previous MixSpeech method. Further, we propose to combine the mixup with a regularization along the time axis of the input, which is shown as complementary. We apply MixRep to a Conformer encoder of an E2E LAS architecture trained with a joint CTC loss. We experiment on the WSJ dataset and subsets of the SWB dataset, covering reading and telephony conversational speech. Experimental results show that MixRep consistently outperforms other regularization methods for low-resource ASR. Compared to a strong SpecAugment baseline, MixRep achieves a +6.5% and a +6.7% relative WER reduction on the eval92 set and the Callhome part of the eval'2000 set",
    "checked": true,
    "id": "e5780eae634fde7c84e0a97badd49f93afc681d6",
    "semantic_title": "mixrep: hidden representation mixup for low-resource speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23h_interspeech.html": {
    "title": "Improving Chinese Mandarin Speech Recognition Using Semantic Graph Embedding Regularization",
    "volume": "main",
    "abstract": "In this paper,we investigate the benefit that Chinese character semantic graph embedding can bring to the End-to-End automatic speech recognition(ASR). We first introduce the method of constructing Chinese character graph. The Chinese character graph is converted into an embedding vector by the graph embedding method. Then we introduce the Chinese character graph embedding regularization by maximizing the cosine similarity between a transformed decoder feature and the target graph embedding. We conducted speech recognition experiments on the Aishell-1 dataset, and the character error rate was 4.36%, which was reduced to 4.25 after adding the language model. Results on Aishell demonstrated that graph embedding can significantly lower character error rate than other end-to-end ASR method",
    "checked": true,
    "id": "c02a5eae99bffb0807597a03accc1273cdb1dee9",
    "semantic_title": "improving chinese mandarin speech recognition using semantic graph embedding regularization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23o_interspeech.html": {
    "title": "Adapting Multi-Lingual ASR Models for Handling Multiple Talkers",
    "volume": "main",
    "abstract": "State-of-the-art large-scale universal speech models (USMs) show a decent automatic speech recognition (ASR) performance across multiple domains and languages. However, it remains a challenge for these models to recognize overlapped speech, which is often seen in meeting conversations. We propose an approach to adapt USMs for multi-talker ASR. We first develop an enhanced version of serialized output training to jointly perform multi-talker ASR and utterance timestamp prediction. That is, we predict the ASR hypotheses for all speakers, count the speakers, and estimate the utterance timestamps at the same time. We further introduce a lightweight adapter module to maintain the multilingual property of the USMs even when we perform the adaptation with only a single language. Experimental results obtained using the AMI and AliMeeting corpora show that our proposed approach effectively transfers the USMs to a strong multilingual multi-talker ASR model with timestamp prediction capability",
    "checked": true,
    "id": "1a8186e075325e7e475fee8d4bb9589afb6a4bfc",
    "semantic_title": "adapting multi-lingual asr models for handling multiple talkers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23c_interspeech.html": {
    "title": "Adapter-tuning with Effective Token-dependent Representation Shift for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "The use of self-supervised pre-trained speech models has greatly improved speech tasks in low-resource settings. However, fine-tuning the entire model can be computationally expensive and not scalable for multiple tasks (e.g., personalized ASR). While recent approaches have tried to solve this issue by training adapters, they fail to match the performance of full fine-tuning models, possibly due to the challenge of task domain transferability. Our proposed method enhances the performance of vanilla adapter tuning for ASR by using a simple yet effective token-dependent bias. This approach adds a token-specific representation shift (bias) to the intermediate representations of a pre-trained model, which better maps the latent features of the frozen network to the task domain. Our approach yields better recognition results with the adapter tuning strategy and achieves the performance of a full fine-tuning model on clean LibriSpeech while maintaining its lightweight nature",
    "checked": true,
    "id": "0984970b6edb8961441c3e07217acccfc58ce1ad",
    "semantic_title": "adapter-tuning with effective token-dependent representation shift for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23c_interspeech.html": {
    "title": "Model-Internal Slot-triggered Biasing for Domain Expansion in Neural Transducer ASR Models",
    "volume": "main",
    "abstract": "Personal rare word recognition is an important yet challenging task for end-to-end speech recognition. Contextual biasing has demonstrated success in tackling this problem. Though effective in improving rare word recognition, these mechanisms can lead to errors due to false-biasing while facing further challenges when attempting to expand them to many domains. To address these limitations, in this work we propose a neural biasing design with a streaming model-internal slot classifier, trained to categorise the domain of each word piece before it is emitted. The neural biasing module can therefore be triggered in a controlled way, permitting natural scaling to many domains while reducing false-biasing and computational cost. Experiments on diverse domain slot types of application names, communications and playlist names demonstrate the proposed architecture results in 26% to 58% relative improvements on personal rare word recognition with minimal impact (0.6% rel.) on general data",
    "checked": true,
    "id": "ed61d5f9f030623bafceeaea91ce53fd2dd13fac",
    "semantic_title": "model-internal slot-triggered biasing for domain expansion in neural transducer asr models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yao23b_interspeech.html": {
    "title": "Delay-penalized CTC Implemented Based on Finite State Transducer",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification (CTC) suffers from the latency problem when applied to streaming models. We argue that in CTC lattice, the alignments that can access more future context are preferred during training, thereby leading to higher symbol delay. In this work we propose the delay-penalized CTC which is augmented with latency penalty regularization. We devise a flexible and efficient implementation based on the differentiable Finite State Transducer (FST). Specifically, by attaching a binary attribute to CTC topology, we can locate the frames that firstly emit non-blank tokens on the resulting CTC lattice, and add the frame offsets to the log-probabilities. Experimental results demonstrate the effectiveness of our proposed delay-penalized CTC, which is able to balance the delay-accuracy trade-off. Furthermore, combining the delay-penalized transducer enables the CTC model to achieve better performance and lower latency. Our work is open-sourced and publicly available",
    "checked": true,
    "id": "6554dead720c0289dbbd04aaf81929c218034c1a",
    "semantic_title": "delay-penalized ctc implemented based on finite state transducer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23f_interspeech.html": {
    "title": "Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation",
    "volume": "main",
    "abstract": "Mapping two modalities, speech and text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed a novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method",
    "checked": true,
    "id": "afae99c77ad03db427e72d5f6498fa0920944b84",
    "semantic_title": "text-only domain adaptation for end-to-end speech recognition through down-sampling acoustic representation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23u_interspeech.html": {
    "title": "Knowledge Distillation Approach for Efficient Internal Language Model Estimation",
    "volume": "main",
    "abstract": "Internal language model estimation (ILME) has demonstrated its efficacy in domain adaptation for end-to-end (E2E) ASR. However, the performance improvement is achieved at the expense of computational cost, compared with conventional shallow fusion. To estimate the internal language model prior, one should run an extra forward operation on either ASR decoder or a separate density ratio (DR) language model (LM) for each decoding utterance. In this paper, we propose to employ knowledge distillation (KD) approach to realize efficient ILME for the Listen-Attend-Spell (LAS) E2E ASR model. First, we extensively explore diverse ILME and DR methods. We find that the ILM can be approximated with a DR-LM much smaller than the original ASR decoder. Furthermore, to reach the performance of ILME, we propose to employ the estimated ILM as teacher to teach a small DR-LM by KD. In this way, we achieve the best of both worlds: comparable performance to ILME and high efficiency of DR with a small DR-LM",
    "checked": true,
    "id": "63a5c7f60191dfb6e451855bdedd63424a1bd2c0",
    "semantic_title": "knowledge distillation approach for efficient internal language model estimation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/adhikary23_interspeech.html": {
    "title": "Language Model Personalization for Improved Touchscreen Typing",
    "volume": "main",
    "abstract": "Touchscreen keyboards rely on language modeling to auto-correct noisy typing and to offer word predictions. While language models can be pre-trained on huge amounts of text, they may fail to capture a user's unique writing style. Using a recently released email personalization dataset, we show improved performance compared to a unigram cache by adapting to a user's text via language models based on prediction by partial match (PPM) and recurrent neural networks. On simulated noisy touchscreen typing of 44 users, our best model increased keystroke savings by 9.9% relative and reduced word error rate by 36% relative compared to a static background language model",
    "checked": true,
    "id": "51f4dc7d6b82632c4226a8f0c47eddfd9cf4b78e",
    "semantic_title": "language model personalization for improved touchscreen typing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23b_interspeech.html": {
    "title": "Blank Collapse: Compressing CTC Emission for the Faster Decoding",
    "volume": "main",
    "abstract": "Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use the CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper, we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher",
    "checked": true,
    "id": "6498d95d3f988e684bc6a70004decbefec655222",
    "semantic_title": "blank collapse: compressing ctc emission for the faster decoding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peyser23_interspeech.html": {
    "title": "Improving Joint Speech-Text Representations Without Alignment",
    "volume": "main",
    "abstract": "The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system",
    "checked": true,
    "id": "12a3d43d9bc97b7d0e93893b98b5ed1c54c1ced2",
    "semantic_title": "improving joint speech-text representations without alignment",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/flynn23_interspeech.html": {
    "title": "Leveraging Cross-Utterance Context For ASR Decoding",
    "volume": "main",
    "abstract": "While external language models (LMs) are often incorporated into the decoding stage of automated speech recognition systems, these models usually operate with limited context. Cross utterance information has been shown to be beneficial during second pass re-scoring, however this limits the hypothesis space based on the local information available to the first pass LM. In this work, we investigate the incorporation of long-context transformer LMs for cross-utterance decoding of acoustic models via beam search, and compare against results from n-best rescoring. Results demonstrate that beam search allows for an improved use of cross-utterance context. When evaluating on the long-format dataset AMI, results show a 0.7% and 0.3% absolute reduction on dev and test sets compared to the single-utterance setting, with improvements when including up to 500 tokens of prior context. Evaluations are also provided for Tedlium-1 with less significant improvements of around 0.1% absolute",
    "checked": true,
    "id": "98c523dc3c971b96327232056ae7e9ec003310f9",
    "semantic_title": "leveraging cross-utterance context for asr decoding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23_interspeech.html": {
    "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation",
    "volume": "main",
    "abstract": "Large-scale pre-trained language models (PLMs) have shown great potential in natural language processing tasks. Leveraging the capabilities of PLMs to enhance automatic speech recognition (ASR) systems has also emerged as a promising research direction. However, previous works may be limited by the inflexible structures of PLMs and the insufficient utilization of PLMs. To alleviate these problems, we propose the hierarchical knowledge distillation (HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer knowledge from PLMs to the ASR models, HKD employs cross-modal knowledge distillation with contrastive loss at the acoustic level and knowledge distillation with regression loss at the linguistic level. Compared with the original CIF-based model, our method achieves 15% and 9% relative error rate reduction on the AISHELL-1 and LibriSpeech datasets, respectively",
    "checked": true,
    "id": "57eb41c7b5cbffb134cdcf67e455c9c852024cbd",
    "semantic_title": "knowledge transfer from pre-trained language models to cif-based speech recognizers via hierarchical distillation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tsunoo23_interspeech.html": {
    "title": "Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition",
    "volume": "main",
    "abstract": "Although frame-based models, such as CTC and transducers, have an affinity for streaming automatic speech recognition, their decoding uses no future knowledge, which could lead to incorrect pruning. Conversely, label-based attention encoder--decoder mitigates this issue using soft attention to the input, while it tends to overestimate labels biased towards its training domain, unlike CTC. We exploit these complementary attributes and propose to integrate the frame- and label-synchronous (F-/L-Sync) decoding alternately performed within a single beam-search scheme. F-Sync decoding leads the decoding for block-wise processing, while L-Sync decoding provides the prioritized hypotheses using look-ahead future frames within a block. We maintain the hypotheses from both decoding methods to perform effective pruning. Experiments demonstrate that the proposed search algorithm achieves lower error rates compared to the other search methods, while being robust against out-of-domain situations",
    "checked": true,
    "id": "6c2b800cd03ad064922c8596a18d784ce25d47ac",
    "semantic_title": "integration of frame- and label-synchronous beam search for streaming encoder-decoder speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23e_interspeech.html": {
    "title": "A Neural Time Alignment Module for End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end trainable (E2E) automatic speech recognition (ASR) systems have low word error rates, but they do not model timings or silence by default unlike hidden Markov model (HMM)-based systems. In this paper, an extra neural aligner module is proposed for E2E ASR models, which labels the word timings in a post-processing stage. Pre-trained neural transducer and attention-based encoder-decoder models are adopted as the ASR backbones for experiments. The aligner module uses self-attention and cross-attention and takes the hidden representations from the backbone to predict the durations of each word and the possible silences. A novel loss is proposed for aligner training with the backbone frozen. Experimental results showed that when trained using the references from an existing HMM-based forced aligner, the proposed methods can make time predictions at accuracy about 95% for matched recognised words, and about 99% for utterances up to 10 s with reference text, with 200 ms tolerance",
    "checked": true,
    "id": "28810a2d8a0943541ab4e9f60642b8a977a883dc",
    "semantic_title": "a neural time alignment module for end-to-end automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23i_interspeech.html": {
    "title": "Accelerating Transducers through Adjacent Token Merging",
    "volume": "main",
    "abstract": "Recent end-to-end automatic speech recognition (ASR) systems often utilize a Transformer-based acoustic encoder that generates embedding at a high frame rate. However, this design is inefficient, particularly for long speech signals due to the quadratic computation of self-attention. To address this, we propose a new method, Adjacent Token Merging (A-ToMe), which gradually combines adjacent tokens with high similarity scores between their key values. In this way, the total time step could be reduced, and the inference of both the encoder and joint network is accelerated. Experiments on LibriSpeech show that our method can reduce 57% of tokens and improve the inference speed on GPU by 70% without any notable loss of accuracy. Additionally, we demonstrate that A-ToMe is also an effective solution to reduce tokens in long-form ASR, where the input speech consists of multiple utterances",
    "checked": true,
    "id": "66378a4a0b32a076c69dcb8586ac3639a0e951d8",
    "semantic_title": "accelerating transducers through adjacent token merging",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23_interspeech.html": {
    "title": "Language-Universal Phonetic Representation in Multilingual Speech Pretraining for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "We improve low-resource ASR by integrating the ideas of multilingual training and self-supervised learning. Concretely, we leverage an International Phonetic Alphabet (IPA) multilingual model to create frame-level pseudo labels for unlabeled speech, and use these pseudo labels to guide hidden-unit BERT (HuBERT) based speech pretraining in a phonetically-informed manner. The experiments on the Multilingual Speech (MLS) Corpus show that the proposed approach consistently outperforms the standard HuBERT on all the target languages. Moreover, on 3 of the 4 languages, comparing to the standard HuBERT, the approach performs better, meanwhile is able to save supervised training data by 1.5k hours (75%) at most. Our approach outperforms most of the state of the arts, with much less pretraining data in terms of hours and language diversity. Compared to XLSR-53 and a retraining based multilingual method, our approach performs better with full and limited finetuning data scenarios",
    "checked": true,
    "id": "b7bc0e232456aefc029bb661ef3310bbff279fda",
    "semantic_title": "language-universal phonetic representation in multilingual speech pretraining for low-resource speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23sa_interspeech.html": {
    "title": "Language-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition",
    "volume": "main",
    "abstract": "Multilingual speech recognition for both monolingual and code-switching speech is a challenging task. Recently, based on the Mixture of Experts (MoE), many works have made good progress in multilingual and code-switching ASR, but present huge computational complexity with the increase of supported languages. In this work, we propose a computation-efficient network named Language-Routing Mixture of Experts (LR-MoE) for multilingual and code-switching ASR. LR-MoE extracts language-specific representations through the Mixture of Language Experts (MLE), which is guided to learn by a frame-wise language routing mechanism. The weight-shared frame-level language identification (LID) network is jointly trained as the shared pre-router of each MoE layer. Experiments show that the proposed method significantly improves multilingual and code-switching speech recognition performances over baseline with comparable computational efficiency",
    "checked": true,
    "id": "0557a70130c6d10b0f93d83a33627b883d2936c5",
    "semantic_title": "language-routing mixture of experts for multilingual and code-switching speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23h_interspeech.html": {
    "title": "Embedding Articulatory Constraints for Low-resource Speech Recognition Based on Large Pre-trained Model",
    "volume": "main",
    "abstract": "Knowledge about phonemes and their articulatory attributes can help improve automatic speech recognition (ASR) of low-resource languages. In this study, we propose a simple and effective approach to embed prior knowledge about phonemes into end-to-end ASR based on a large pre-trained model. An articulatory attribute prediction layer is constructed by embedding articulatory constraints in layer initialization, which allows for predicting articulatory attributes without the need for explicit training. The final ASR transcript is inferred by combining the output of this layer with encoded speech features. We apply our method to finetune a pre-trained XLS-R model using Ainu and Mboshi corpora, and achieve a 12% relative improvement when target data of only 1 hour is available. This demonstrates that the approach of incorporating phonetic prior knowledge is useful when combined with a large pre-trained model",
    "checked": true,
    "id": "89fb7e5bc354e76ed55057b188a699683056716a",
    "semantic_title": "embedding articulatory constraints for low-resource speech recognition based on large pre-trained model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23b_interspeech.html": {
    "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs",
    "checked": true,
    "id": "47ba7df38e24da9bad9266d2b58abbb2b70db6e5",
    "semantic_title": "exploration of efficient end-to-end asr using discretized input from self-supervised learning",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/antonova23_interspeech.html": {
    "title": "SpellMapper: A non-autoregressive neural spellchecker for ASR customization with candidate retrieval based on n-gram mappings",
    "volume": "main",
    "abstract": "Contextual spelling correction models are an alternative to shallow fusion to improve automatic speech recognition (ASR) quality given user vocabulary. To deal with large user vocabularies, most of these models include candidate retrieval mechanisms, usually based on minimum edit distance between fragments of ASR hypothesis and user phrases. However, the edit-distance approach is slow, non-trainable, and may have low recall as it relies only on common letters. We propose: 1) a novel algorithm for candidate retrieval, based on misspelled n-gram mappings, which gives up to 90% recall with just the top 10 candidates on Spoken Wikipedia; 2) a non-autoregressive neural model based on BERT architecture, where the initial transcript and ten candidates are combined into one input. The experiments on Spoken Wikipedia show 21.4% word error rate improvement compared to a baseline ASR system",
    "checked": true,
    "id": "843cfd1c36be2c0fed94f62795f9df9c5bab5a9c",
    "semantic_title": "spellmapper: a non-autoregressive neural spellchecker for asr customization with candidate retrieval based on n-gram mappings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bijwadia23_interspeech.html": {
    "title": "Text Injection for Capitalization and Turn-Taking Prediction in Speech Models",
    "volume": "main",
    "abstract": "Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall",
    "checked": true,
    "id": "c86bd94782bdfa25396f7bb5c108e522138f3192",
    "semantic_title": "text injection for capitalization and turn-taking prediction in speech models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gitman23_interspeech.html": {
    "title": "Confidence-based Ensembles of End-to-End Speech Recognition Models",
    "volume": "main",
    "abstract": "The number of end-to-end speech recognition models grows every year. These models are often adapted to new domains or languages resulting in a proliferation of expert systems that achieve great results on target data, while generally showing inferior performance outside of their domain of expertise. We explore combination of such experts via confidence-based ensembles: ensembles of models where only the output of the most-confident model is used. We assume that models' target data is not available except for a small validation set. We demonstrate effectiveness of our approach with two applications. First, we show that a confidence-based ensemble of 5 monolingual models outperforms a system where model selection is performed via a dedicated language identification block. Second, we demonstrate that it is possible to combine base and adapted models to achieve strong results on both original and target data. We validate all our results on multiple datasets and model architectures",
    "checked": true,
    "id": "2f61008b996599d10fc0f091415215378c0decc5",
    "semantic_title": "confidence-based ensembles of end-to-end speech recognition models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chi23_interspeech.html": {
    "title": "Unsupervised Code-switched Text Generation from Parallel Text",
    "volume": "main",
    "abstract": "There has been great interest in developing automatic speech recognition (ASR) systems that can handle code-switched (CS) speech to meet the needs of a growing bilingual population. However, existing datasets are limited in size. It is expensive and difficult to collect real transcribed spoken CS data due to the challenges of finding and identifying CS data in the wild. As a result, many attempts have been made to generate synthetic CS data. Existing methods either require the existence of CS data during training, or are driven by linguistic knowledge. We introduce a novel approach of forcing a multilingual MT system that was trained on non-CS data to generate CS translations. Comparing against two prior methods, we show that simply leveraging the shared representations of two languages (Mandarin and English) yields better CS text generation and, ultimately, better CS ASR",
    "checked": true,
    "id": "3c96fe235783a42a2061f7010da756531e2ba5b8",
    "semantic_title": "unsupervised code-switched text generation from parallel text",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23b_interspeech.html": {
    "title": "A Binary Keyword Spotting System with Error-Diffusion Based Feature Binarization",
    "volume": "main",
    "abstract": "Binary-neural-network based keyword spotting (KWS) for resource-constrained devices has gained much attention in recent years. Although several works proved their success, a fully binary KWS system is yet to come, considering high-precision speech feature maps are still required for satisfying accuracy. Such precision mismatch results in non-binary activation layers, thus leading to extra computational costs. In this paper, we present an extremely compact KWS system using a binary neural network and error-diffusion binarized speech features. The system eliminates all high-precision multiplications and requires only hardware-friendly bit-wise operations and additions for inference. Experiments on the Google speech commands show that our binary KWS system yields 98.54% accuracy on a 1-keyword task and 95.05% on a 2-keyword task, outperforming 8-bit KWS systems of bigger size. The result proves the feasibility of a fully binary KWS system and can be inspiring for hardware implementations",
    "checked": true,
    "id": "c908f8141cf59b1bdcba92f93ee3ce783988cac2",
    "semantic_title": "a binary keyword spotting system with error-diffusion based feature binarization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23b_interspeech.html": {
    "title": "Language-universal Phonetic Encoder for Low-resource Speech Recognition",
    "volume": "main",
    "abstract": "Multilingual training is effective in improving low-resource ASR, which may partially be explained by phonetic representation sharing between languages. In end-to-end (E2E) ASR systems, graphemes are often used as basic modeling units, however graphemes may not be ideal for multilingual phonetic sharing. In this paper, we leverage International Phonetic Alphabet (IPA) based language-universal phonetic model to improve low-resource ASR performances, for the first time within the attention encoder-decoder architecture. We propose an adaptation method on the phonetic IPA model to further improve the proposed approach on extreme low-resource languages. Experiments carried out on the open-source MLS corpus and our internal databases show our approach outperforms baseline monolingual models and most state-of-the-art works. Our main approach and adaptation are effective on extremely low-resource languages, even within domain- and language-mismatched scenarios",
    "checked": true,
    "id": "fb7d5882e98b9ea3b0b45711379029928b24cfd0",
    "semantic_title": "language-universal phonetic encoder for low-resource speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23g_interspeech.html": {
    "title": "A Lexical-aware Non-autoregressive Transformer-based ASR Model",
    "volume": "main",
    "abstract": "Non-autoregressive automatic speech recognition (ASR) has become a mainstream of ASR modeling because of its fast decoding speed and satisfactory result. To further boost the performance, relaxing the conditional independence assumption and cascading large-scaled pre-trained models are two active research directions. In addition to these strategies, we propose a lexical-aware non-autoregressive Transformer-based (LA-NAT) ASR framework, which consists of an acoustic encoder, a speech-text shared encoder, and a speech-text shared decoder. The acoustic encoder is used to process the input speech features as usual, and the speech-text shared encoder and decoder are designed to train speech and text data simultaneously. By doing so, LA-NAT aims to make the ASR model aware of lexical information, so the resulting model is expected to achieve better results by leveraging the learned linguistic knowledge",
    "checked": true,
    "id": "25d825e6439e7c9f5619262ad551047a2b833ba6",
    "semantic_title": "a lexical-aware non-autoregressive transformer-based asr model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vanvuren23_interspeech.html": {
    "title": "Improving Under-Resourced Code-Switched Speech Recognition: Large Pre-trained Models or Architectural Interventions",
    "volume": "main",
    "abstract": "We present three approaches to improve language modelling of under-resourced code-switched speech. First, we challenge the practice of fine-tuning large pre-trained language models on small datasets. Secondly, we investigate the advantages of sub-word encodings for our multilingual code-switched speech. Thirdly, we propose an architectural innovation to the RNN language model that is specifically designed for code-switched text. We show a clear reduction in absolute word error rate of 0.17% for the adapted LSTM language model compared to M-BERT when employed in n-best rescoring experiments. Further, the LSTM models afford a seven-fold reduction in total number of parameters and reduces runtime during rescoring 100-fold. Contrary to recent research trends, our LSTM models do not outperform the word-level models when using sub-word vocabularies. Finally, the new architectural mechanism applied to the LSTM improves language prediction for a span of several words following a code-switch",
    "checked": true,
    "id": "2ddca6863b3291254546ff9db69a3c3d629610ee",
    "semantic_title": "improving under-resourced code-switched speech recognition: large pre-trained models or architectural interventions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bellegarda23_interspeech.html": {
    "title": "Pragmatic Pertinence: A Learnable Confidence Metric to Assess the Subjective Quality of LM-Generated Text",
    "volume": "main",
    "abstract": "To be perceived as trustworthy, artificially generated text must be sufficiently congruent with the available discourse history. Pre-trained language models (LMs) operating in generative mode are capable of predicting locally coherent phrases, but those do not always reflect salient syntactic, semantic, or pragmatic facets of prior content. This paper introduces a learnable evaluation metric to assess the pragmatic pertinence of LM-generated text for a given history. Pertinence is closely aligned with qualitative human judgments of acceptability, thereby emerging as a blend of sensibleness and specificity. Experiments conducted across different domains using different learning architectures show that this approach circumvents the issue of multiple valid ground-truths, while providing a reliable quantitative ranking of generated text completion candidates in context. Pertinence scoring could thus prove useful for the detection of hallucinations",
    "checked": true,
    "id": "71427159f87bd9b2534966b3f32dab73b869c589",
    "semantic_title": "pragmatic pertinence: a learnable confidence metric to assess the subjective quality of lm-generated text",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ea_interspeech.html": {
    "title": "ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition",
    "volume": "main",
    "abstract": "In Speech Emotion Recognition (SER), textual data is often used alongside audio signals to address their inherent variability. However, the reliance on human annotated text in most research hinders the development of practical SER systems. To overcome this challenge, we investigate how Automatic Speech Recognition (ASR) performs on emotional speech by analyzing the ASR performance on emotion corpora and examining the distribution of word errors and confidence scores in ASR transcripts to gain insight into how emotion affects ASR. We utilize four ASR systems, namely Kaldi ASR, wav2vec2, Conformer, and Whisper, and three corpora: IEMOCAP, MOSI, and MELD to ensure generalizability. Additionally, we conduct text-based SER on ASR transcripts with increasing word error rates to investigate how ASR affects SER. The objective of this study is to uncover the relationship and mutual impact of ASR and SER, in order to facilitate ASR adaptation to emotional speech and the use of SER in real world",
    "checked": true,
    "id": "c40e3331d374cf18e96878fafe6477b5030519aa",
    "semantic_title": "asr and emotional speech: a word-level investigation of the mutual impact of speech and emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharma23_interspeech.html": {
    "title": "BASS: Block-wise Adaptation for Speech Summarization",
    "volume": "main",
    "abstract": "End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to computing restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline",
    "checked": true,
    "id": "3bd320ddb25886417ae90011b00f13f5d558097b",
    "semantic_title": "bass: block-wise adaptation for speech summarization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shekar23_interspeech.html": {
    "title": "Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data: Fearless Steps Apollo-11 Audio Corpus",
    "volume": "main",
    "abstract": "Speaker tracking in spontaneous naturalistic data continues to be a major research challenge, especially for short turn-taking communications. The NASA Apollo-11 space mission brought astronauts to the moon and back, where team based voice communications were captured. Building robust speaker classification models for this corpus has significant challenges due to variability of speaker turns, imbalanced speaker classes, and time-varying background noise/distortions. This study proposes a novel approach for speaker classification and tracking, utilizing a graph attention network framework that builds upon pretrained speaker embeddings. The model's robustness is evaluated on a number of speakers (10-140), achieving classification accuracy of 90.78% for 10 speakers, and 79.86% for 140 speakers. Furthermore, a secondary investigation focused on tracking speakers-of-interest(SoI) during mission critical phases, essentially serves as a lasting tribute to the 'Heroes Behind the Heroes'",
    "checked": true,
    "id": "90f1640e143cfd5a822e0d0f1cc662b2108e422f",
    "semantic_title": "speaker tracking using graph attention networks with varying duration utterances across multi-channel naturalistic data: fearless steps apollo-11 audio corpus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yan23_interspeech.html": {
    "title": "Combining language corpora in a Japanese electromagnetic articulography database for acoustic-to-articulatory inversion",
    "volume": "main",
    "abstract": "This paper presents an electromagnetic articulography database of Japanese sentences. The database includes aligned acoustics and articulatory data from seven males and three females, with a total of five recorded hours. The database is now in preparation for public release to further research in areas of acoustic-to-articulatory inversion, brain-machine interface communication systems, artificial speech synthesis, and dialect recognition. Moreover, based on this database we established an acoustic-to-articulatory inversion system using a deep, bidirectional, long short-term memory recurrent neural network structure. The results showed that, for the Japanese language, adding English corpora to the training was not beneficial for this speaker-independent model",
    "checked": true,
    "id": "ad0ec08e019b9ab8e2174a6184ddcfc3588a86d8",
    "semantic_title": "combining language corpora in a japanese electromagnetic articulography database for acoustic-to-articulatory inversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23g_interspeech.html": {
    "title": "A Dual Attention-based Modality-Collaborative Fusion Network for Emotion Recognition",
    "volume": "main",
    "abstract": "Multi-modal emotion recognition (MER) is an emerging research field in human-computer interactions. However, previous studies have explored several fusion methods to deal with the asynchronism and the heterogeneity of multimodal data but they mostly neglect the importance of discriminative unimodal information resulting in the ignorance of independence of uni-modality. Furthermore, the complementarity among different fusion strategies is seldom taken in consideration. To address these limitations, we propose a modality-collaborative fusion network (MCFN) consisting of three main components: a dual attention-based intra-modal learning module which is devoted to build the initial embedding spaces, a modality-collaborative learning approach is to reconcile the emotional information across modalities, and a two-stage fusion strategy to integrate multimodal features which are improved by a mutual adjustment approach. The proposed framework outperforms the state-of-the-art methods in overall experiments on two well-known public datasets. Our model will be available at https://github.com/zxiaohen/ Speech-emotion-recognition-MCFN",
    "checked": true,
    "id": "d6d2f6d901e83a2699b317b9a54c79658a1d064d",
    "semantic_title": "a dual attention-based modality-collaborative fusion network for emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chivriga23_interspeech.html": {
    "title": "Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm",
    "volume": "main",
    "abstract": "Large models (e.g., GPT-3, CLIP, DALL-E) show remarkable few-shot and zero-shot capabilities when trained on hundreds of millions of samples. Despite this trend, no publicly available synchronized music audio and lyrics dataset of sufficient scale exists, nor does a reliable evaluation benchmark to assess a model's performance. To address this issue, we build and release MusicLyric, a large public dataset with over 320k audio sequences and lyrics pairs for a total duration of 1,200 hours based on a collection of over 32,000 songs. The generation process is based on the teacher-student paradigm where the student seeks to outclass the teacher with more data available using the newly generated pseudo-alignments. The method is efficient and straightforward with at least 3 iterations needed to create high-quality data that can be scaled to a hundred thousand samples. We make our dataset, toolkit, and pre-trained models open-source",
    "checked": true,
    "id": "1cff88a49880a269d7017329fbd842c124183ef7",
    "semantic_title": "large dataset generation of synchronized music audio and lyrics at scale using teacher-student paradigm",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/banerjee23_interspeech.html": {
    "title": "Enc-Dec RNN Acoustic Word Embeddings learned via Pairwise Prediction",
    "volume": "main",
    "abstract": "Learning discriminative Acoustic Word Embeddings (AWEs) summarising variable length spoken word segments brings efficiency in speech retrieval tasks, notably, Query-by-Example (QbE) Speech or Spoken Term Detection (STD). In this paper, we add on to RNN based approaches for generating acoustic word embeddings. The model is trained in an encoder-decoder fashion on pairs of similar word segments by optimizing a pairwise self-supervised loss where the targets are generated offline via clustering. The pairs may be generated with word boundaries (weak supervision) or via augmentation of unlabelled word segments (no supervision). Experiments with word discrimination task on TIMIT and LibriSpeech show state of the art performance of the proposed approach outperforming popular RNN AWE approaches in both weakly supervised and unsupervised settings. The AWEs generated by our model generalise well to OOV words. On STD tasks performed on TIMIT, the proposed approach provides speed advantages",
    "checked": true,
    "id": "d6eeb0a6d2dc6a130fadf043c27ae2b4ecdfa27d",
    "semantic_title": "enc-dec rnn acoustic word embeddings learned via pairwise prediction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kotey23_interspeech.html": {
    "title": "Query Based Acoustic Summarization for Podcasts",
    "volume": "main",
    "abstract": "Podcasts are a rich storytelling medium of long diverse conversations. Typically, listeners preview an episode through an audio clip, before deciding to consume the content. An automatic system that produces promotional clips, by supporting acoustic queries would greatly benefit podcasters. Previous text based methods do not use the acoustic signal directly or incorporate acoustic defined queries. Therefore, we propose a query based summarization approach, to produce audio clip summaries from podcast data. Leveraging unsupervised clustering methods, we apply our framework to the Spotify podcasts dataset. Audio signals are transformed into acoustic word embeddings, along with a pre-selected candidate query. We initiate the cluster centroids with the query vector and obtain the final snippets by computing a global and local similarity score. Additionally, we apply our framework to the AMI meeting dataset and demonstrate how audio can successfully be utilized to perform summarization",
    "checked": true,
    "id": "7ae9630ae9ac1b78d1ee18107a26c8d6d4351174",
    "semantic_title": "query based acoustic summarization for podcasts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23f_interspeech.html": {
    "title": "Spot Keywords From Very Noisy and Mixed Speech",
    "volume": "main",
    "abstract": "Most existing keyword spotting research focuses on conditions with slight or moderate noise. In this paper, we try to tackle a more challenging task: detecting keywords buried under strong interfering speech (10 times higher than the keyword in amplitude), and even worse, mixed with other keywords. We propose a novel Mix Training (MT) strategy that encourages the model to discover low-energy keywords from noisy and mixed speech. Experiments were conducted with a vanilla CNN and two EfficientNet (B0/B2) architectures. The results evaluated with the Google Speech Command dataset demonstrated that the proposed mix training approach is highly effective and outperforms standard data augmentation and mixup training",
    "checked": true,
    "id": "e366833a629c8faa0a01ce802e0cadfa977302fd",
    "semantic_title": "spot keywords from very noisy and mixed speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nayem23_interspeech.html": {
    "title": "Knowledge Distillation on Joint Task End-to-End Speech Translation",
    "volume": "main",
    "abstract": "An End-to-End Speech Translation (E2E-ST) model takes input audio in one language and directly produces output text in another language. The model requires to learn both speech-to-text modality conversion and translation tasks, which demands a large architecture for effective learning of this joint task. Yet, to the best of our knowledge, we are the first to optimize compression of E2E-ST models. In this work, we explore knowledge distillation for a cross-modality joint-task E2E-ST system from 3 dimensions: 1) student architecture and weight initialization scheme, 2) importance of loss terms associated with different tasks and data modalities, 3) knowledge distillation training scheme customized for the multi-task/module model. Comparing with the full size model, our compressed model's encoder and decoder size are 50% smaller, while it retains 90% and > 95% performance on speech translation task and machine translation task respectively on MUST-C en→de testset",
    "checked": true,
    "id": "5011e031d2d4cc8ab83424256e53444d31d54ed4",
    "semantic_title": "knowledge distillation on joint task end-to-end speech translation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23d_interspeech.html": {
    "title": "Investigating Pre-trained Audio Encoders in the Low-Resource Condition",
    "volume": "main",
    "abstract": "Pre-trained speech encoders have been central to pushing state-of-the-art results across various speech understanding and generation tasks. Nonetheless, the capabilities of these encoders in low-resource settings are yet to be thoroughly explored. To address this, we conduct a comprehensive set of experiments using a representative set of 3 state-of-the-art encoders (Wav2vec2, WavLM, Whisper) in the low-resource setting across 7 speech understanding and generation tasks. We provide various quantitative and qualitative analyses on task performance, convergence speed, and representational properties of the encoders. We observe a connection between the pre-training protocols of these encoders and the way in which they capture information in their internal layers. In particular, we observe the Whisper encoder exhibits the greatest low-resource capabilities on content-driven tasks in terms of performance and convergence speed",
    "checked": true,
    "id": "9ee4bd748ecbc4f4b191189bdae48729d58c7fa9",
    "semantic_title": "investigating pre-trained audio encoders in the low-resource condition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23g_interspeech.html": {
    "title": "Improving Textless Spoken Language Understanding with Discrete Units as Intermediate Target",
    "volume": "main",
    "abstract": "Spoken Language Understanding (SLU) is a task that aims to extract semantic information from spoken utterances. Previous research has made progress in end-to-end SLU by using paired speech-text data, such as pre-trained Automatic Speech Recognition (ASR) models or paired text as intermediate targets. However, acquiring paired transcripts is expensive and impractical for unwritten languages. On the other hand, Textless SLU extracts semantic information from speech without utilizing paired transcripts. However, the absence of intermediate targets and training guidance for textless SLU often leads to suboptimal performance. In this work, inspired by the content-disentangled discrete units from self-supervised speech models, we proposed to use discrete units as intermediate guidance to improve textless SLU performance. Our method surpasses the baseline method on five SLU benchmark corpora. Additionally, we find that unit guidance facilitates few-shot learning and enhances noise robustness",
    "checked": true,
    "id": "de3b0f07daa9c717ddf20247eb05a2d373ba4ec7",
    "semantic_title": "improving textless spoken language understanding with discrete units as intermediate target",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23m_interspeech.html": {
    "title": "Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test",
    "volume": "main",
    "abstract": "Automatic speech recognition systems based on deep learning are mainly trained under empirical risk minimization (ERM). Since ERM utilizes the averaged performance on the data samples regardless of a group such as healthy or dysarthric speakers, ASR systems are unaware of the performance disparities across the groups. This results in biased ASR systems whose performance differences among groups are severe. In this study, we aim to improve the ASR system in terms of group robustness for dysarthric speakers. To achieve our goal, we present a novel approach, sample reweighting with sample affinity test (Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given data sample and then mitigates the bias by debiasing helpfulness-based sample reweighting. Experimental results demonstrate that Re-SAT contributes to improved ASR performance on dysarthric speech without performance degradation on healthy speech",
    "checked": true,
    "id": "1b94e2e75d6be5884efbb725b540b50c7b97f4cb",
    "semantic_title": "debiased automatic speech recognition for dysarthric speech via sample reweighting with sample affinity test",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/papadimitriou23_interspeech.html": {
    "title": "Multimodal Locally Enhanced Transformer for Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a novel Transformer-based approach for continuous sign language recognition (CSLR) from videos, aiming to address the shortcomings of traditional Transformers in learning local semantic context of SL. Specifically, the proposed relies on two distinct components: (a) a window-based RNN module to capture local temporal context and (b) a Transformer encoder, enhanced with local modeling via Gaussian bias and relative position information, as well as with global structure modeling through multi-head attention. To further improve model performance, we design a multimodal framework that applies the proposed to both appearance and motion signing streams, aligning their posteriors through a guiding CTC technique. Further, we achieve visual feature and gloss sequence alignment by incorporating a knowledge distillation loss. Experimental evaluation on two popular German CSLR datasets, demonstrates the superiority of our model",
    "checked": true,
    "id": "6ead5cf59020f89638a0b2fc5d9cd4821d060897",
    "semantic_title": "multimodal locally enhanced transformer for continuous sign language recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gonzalezmachorro23_interspeech.html": {
    "title": "Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features",
    "volume": "main",
    "abstract": "Multiple sclerosis (MS) is a neuroinflammatory disease that affects millions of people worldwide. Since dysarthria is prominent in people with MS (pwMS), this paper aims to identify acoustic features that differ between people with MS and healthy controls (HC). Additionally, we develop automatic classification methods to distinguish between pwMS and HC. In this work, we present a new dataset of a German-speaking cohort which contains 39 patients with low disability of relapsing MS and 16 HC. Findings suggest that certain interpretable speech features could be useful in diagnosing MS, and that machine learning methods could potentially support fast and unobtrusive screening in clinical practice. The study emphasises the importance of analysing free speech compared to read speech",
    "checked": true,
    "id": "1c6e47c14c3fca53e2e6f2a61a6935043eac6634",
    "semantic_title": "towards supporting an early diagnosis of multiple sclerosis using vocal features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rathod23_interspeech.html": {
    "title": "Whisper Features for Dysarthric Severity-Level Classification",
    "volume": "main",
    "abstract": "Dysarthria is a speech disorder caused by improper coordination between the brain and the muscles that produce intelligible speech. Accurately diagnosing the severity of dysarthria is critical for determining the appropriate treatment and directing speech to suitable Automatic Speech Recognition systems. Recently, various methods have been employed to investigate the classification of dysarthria severity-levels using advanced features, including STFT and MFCC. This study proposes utilizing Web-scale Supervised Pretraining for Speech Recognition (WSPSR), also known as Whisper, encoder module for dysarthric severity-level classification using transfer learning approach. Whisper model is an advanced machine learning model used for speech recognition, which is trained on a large scale of 680,000 hours of labeled audio data. The proposed approach demonstrated a high accuracy rate of 98.02%, surpassing the accuracies achieved by MFCC (95.2%) and LFCC (96.05%)",
    "checked": true,
    "id": "fd01dc873d3593507879bc3cbf9100505565cef1",
    "semantic_title": "whisper features for dysarthric severity-level classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tang23b_interspeech.html": {
    "title": "A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning",
    "volume": "main",
    "abstract": "Aphasia is a language disorder that affects the speaking ability of millions of patients. This paper presents a new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset. Specifically, we introduce two multi-task learning methods based on the CTC/Attention architecture to perform both tasks simultaneously. Our system achieves state-of-the-art speaker-level detection accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia patients. In addition, we demonstrate the generalizability of our approach by applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing",
    "checked": true,
    "id": "4d35540aaf993c8fa7e1fa5fc6a990f1eb830263",
    "semantic_title": "a new benchmark of aphasia speech recognition and detection based on e-branchformer and multi-task learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yue23_interspeech.html": {
    "title": "Dysarthric Speech Recognition, Detection and Classification using Raw Phase and Magnitude Spectra",
    "volume": "main",
    "abstract": "In this paper, we explore the effectiveness of deploying the raw phase and magnitude spectra for dysarthric speech recognition, detection and classification. In particular, we scrutinise the usefulness of various raw phase-based representations along with their combinations with the raw magnitude spectrum and filterbank features. We employed single and multi-stream architectures consisting of a cascade of convolutional, recurrent and fully-connected layers for acoustic modelling. Furthermore, we investigate various configurations and fusion schemes as well as their training dynamics. In addition, the accuracies of the raw phase and magnitude based systems in the detection and classification tasks are studied and discussed. We report the performance on the UASpeech and TORGO dysarthric speech databases and for different severity levels. Our best system achieved WERs of 31.2% and 9.1% for dysarthric and typical speech on TORGO and 30.2% on UASpeech, respectively",
    "checked": true,
    "id": "6c4677fc175694ceaeda613f22ea315b5a17693d",
    "semantic_title": "dysarthric speech recognition, detection and classification using raw phase and magnitude spectra",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bayerl23_interspeech.html": {
    "title": "A Stutter Seldom Comes Alone – Cross-Corpus Stuttering Detection as a Multi-label Problem",
    "volume": "main",
    "abstract": "Most stuttering detection and classification research has viewed stuttering as a multi-class classification problem or a binary detection task for each dysfluency type; however, this does not match the nature of stuttering, in which one dysfluency seldom comes alone but rather co-occurs with others. This paper explores multi-language and cross-corpus end-to-end stuttering detection as a multi-label problem using a modified wav2vec 2.0 system with an attention-based classification head and multi-task learning. We evaluate the method using combinations of three datasets containing English and German stuttered speech, one containing speech modified by fluency shaping. The experimental results and an error analysis show that multi-label stuttering detection systems trained on cross-corpus and multi-language data achieve competitive results but performance on samples with multiple labels stays below overall detection results",
    "checked": false,
    "id": "ac2ca47a9c0c547b4cedb4d6a7c39e8616823bbc",
    "semantic_title": "a stutter seldom comes alone - cross-corpus stuttering detection as a multi-label problem",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhattacharjee23_interspeech.html": {
    "title": "Transfer Learning to Aid Dysarthria Severity Classification for Patients with Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "A major challenge involved in automatic dysarthria severity classification for patients with Amyotrophic Lateral Sclerosis (ALS) is the difficulty to build a speech corpus which is large enough to train accurate and generalizable classifiers. To overcome this constraint, we employ transfer learning approaches, specifically, fine-tuning from an auxiliary task and multi-task learning. Input feature reconstruction and gender classification, on the same ALS speech dataset or other healthy speech corpora, are explored as the auxiliary tasks. We use temporal statistics of mel-frequency cepstral coefficients as the features and dense neural networks for performing the primary and auxiliary tasks. Experiments suggest that transfer learning aids severity classification with up to 11.03% absolute increase in the average classification accuracy as compared to direct single task learning. The improvement is attributed mainly to better classification of the mild class than severe/normal classes",
    "checked": true,
    "id": "bcbe04bc505e5bfe3d10ea30b40834361718d268",
    "semantic_title": "transfer learning to aid dysarthria severity classification for patients with amyotrophic lateral sclerosis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23qa_interspeech.html": {
    "title": "DuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach with Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "We present a novel typical-to-atypical voice conversion approach (DuTa-VC), which (i) can be trained with nonparallel data (ii) first introduces diffusion probabilistic model (iii) preserves the target speaker identity (iv) is aware of the phoneme duration of the target speaker. DuTa-VC consists of three parts: an encoder transforms the source mel-spectrogram into a duration-modified speaker-independent mel-spectrogram, a decoder performs the reverse diffusion to generate the target mel-spectrogram, and a vocoder is applied to reconstruct the waveform. Objective evaluations conducted on the UASpeech show that DuTa-VC is able to capture severity characteristics of dysarthric speech, reserves speaker identity, and significantly improves dysarthric speech recognition as a data augmentation. Subjective evaluations by two expert speech pathologists validate that DuTa-VC can preserve the severity and type of dysarthria of the target speakers in the synthesized speech",
    "checked": true,
    "id": "bf6339e920466f2dc7dc0da5edde5b3187cf9d0d",
    "semantic_title": "duta-vc: a duration-aware typical-to-atypical voice conversion approach with diffusion probabilistic model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hedeshy23_interspeech.html": {
    "title": "CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice",
    "volume": "main",
    "abstract": "Non-verbal voice expressions (NVVEs) have been adopted as a means of human-computer interaction in research studies. However, exploring non-verbal voice-based interactions has been constrained by the limited availability of suitable training data and computational methods for classifying such expressions, leading to a focus on simple binary inputs. We address this issue with a new dataset containing 950 audio samples comprising 6 classes of voice expressions. The data were collected from 42 speakers who donated voice recordings. The classifier was trained on the data using features derived from mel-spectrograms. Furthermore, we studied the effectiveness of data augmentation and improved over the baseline model accuracy significantly with a test accuracy of 96.6% in a 5-fold cross-validation. We have made CNVVE publicly accessible in the hope that it will serve as a benchmark for future research",
    "checked": true,
    "id": "a820d95886178b82ec595146d46f2e98acf75f1e",
    "semantic_title": "cnvve: dataset and benchmark for classifying non-verbal voice",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baali23_interspeech.html": {
    "title": "Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based Augmentation",
    "volume": "main",
    "abstract": "Despite major advancements in Automatic Speech Recognition (ASR), the state-of-the-art ASR systems struggle to deal with impaired speech even with high-resource languages. In Arabic, this challenge gets amplified, with added complexities in collecting data from dysarthric speakers. In this paper, we aim to improve the performance of Arabic dysarthric automatic speech recognition through a multi-stage augmentation approach. To this effect, we first propose a signal-based approach to generate dysarthric Arabic speech from healthy Arabic speech by modifying its speed and tempo. We also propose a second stage Parallel Wave Generative (PWG) adversarial model that is trained on an English dysarthric dataset to capture language-independant dysarthric speech patterns and further augment the signal-adjusted speech samples. Furthermore, we propose a fine-tuning and text-correction strategies for Arabic Conformer at different dysarthric speech severity levels. Our fine-tuned Conformer achieved 18% Word Error Rate (WER) and 17.2% Character Error Rate (CER) on synthetically generated dysarthric speech from the Arabic common voice speech dataset. This shows significant WER improvement of 81.8% compared to the baseline model trained solely on healthy data. We perform further validation on real English dysarthric speech showing a WER improvement of 124% compared to the baseline trained only on healthy English LJSpeech dataset",
    "checked": true,
    "id": "c175253d07a07e62815d9d902da8b92adaf1ab31",
    "semantic_title": "arabic dysarthric speech recognition using adversarial and signal-based augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kouzelis23_interspeech.html": {
    "title": "Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling",
    "volume": "main",
    "abstract": "The study of speech disorders can benefit greatly from time-aligned data. However, audio-text mismatches in disfluent speech cause rapid performance degradation for modern speech aligners, hindering the use of automatic approaches. In this work, we propose a simple and effective modification of alignment graph construction of CTC-based models using Weighted Finite State Transducers. The proposed weakly-supervised approach alleviates the need for verbatim transcription of speech disfluencies for forced alignment. During the graph construction, we allow the modeling of common speech disfluencies, i.e. repetitions and omissions. Further, we show that by assessing the degree of audio-text mismatch through the use of Oracle Error Rate, our method can be effectively used in the wild. Our evaluation on a corrupted version of the TIMIT test set and the UCLASS dataset shows significant improvements, particularly for recall, achieving a 23-25% relative improvement over our baselines",
    "checked": true,
    "id": "c03d509344651cc7334693ab419bd36ec7d490f2",
    "semantic_title": "weakly-supervised forced alignment of disfluent speech using phoneme-level modeling",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/novotny23_interspeech.html": {
    "title": "Glottal source analysis of voice deficits in basal ganglia dysfunction: evidence from de novo Parkinson's disease and Huntington's disease",
    "volume": "main",
    "abstract": "Dysphonia is a common speech disruption in people with Parkinson's (PD) and Huntington's (HD). Though the glottal source analysis (GSS) yielded promising results in PD, no study analyzed utility of the GSS in HD. In addition, the potential GSS sex-dependency remains unknown. This study examines sustained vowel phonations provided by 40 PD, 40 HD and 40 age- and sex-matched healthy participants using six GSS features including normalized amplitude quotient, quasi-open quotient, magnitude difference of first two spectral peaks, harmonic richness factor, maximum dispersion quotient (MDQ), and peak slope. Our results showed significant differences in HD men and women compared to the healthy counterpart, suggesting breathiness (p<0.01), tension (p<0.001), and decreased timbre (p<0.01) in HD. Reported sex-related differences highlighted the sensitivity of the GSS towards the speaker's sex. The correlation analysis revealed significant relationship between disease severity and MDQ in HD men",
    "checked": true,
    "id": "449d9ef13b24b5df16435969745cd8cb4480f6e5",
    "semantic_title": "glottal source analysis of voice deficits in basal ganglia dysfunction: evidence from de novo parkinson's disease and huntington's disease",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mun23b_interspeech.html": {
    "title": "An Analysis of Glottal Features of Chronic Kidney Disease Speech and Its Application to CKD Detection",
    "volume": "main",
    "abstract": "Chronic kidney disease (CKD) causes a continuous decline in kidney function and structural damage to the kidneys. The speech characteristics of CKD speakers will be different from those of non-CKD speakers because the typical characteristics of CKD, which are impairment of respiratory and laryngeal muscles, can affect respiration, the primary source of speech. In this paper, we identify the glottal characteristics of CKD speech and then investigate whether CKD can be automatically detected using the glottal features. Statistical analysis shows significant differences between groups in glottal source features, representing the breathy characteristic of CKD speech. Through the classification experiment, we compare the performance of solely using voice quality features (baseline) against additional glottal and spectral features. When glottal source features and voice quality features are used together, an F1-score of 0.88 with a 76% relative increase compared to the baseline is obtained",
    "checked": true,
    "id": "9729f24f79bb5ccb2a4ededa68fdfc3aef1fc505",
    "semantic_title": "an analysis of glottal features of chronic kidney disease speech and its application to ckd detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/belagali23_interspeech.html": {
    "title": "Weakly supervised glottis segmentation in high-speed videoendoscopy using bounding box labels",
    "volume": "main",
    "abstract": "Analysis of vocal fold vibration in high-speed videoendoscopy can aid in the assessment of voice disorders. Glottis segmentation is a preliminary step of this analysis. Previous deep learning approaches have focused on fully supervised learning methods for glottis segmentation which require pixel-level annotation. Collection of pixel-level annotated data is time consuming and tedious. To overcome this challenge, in this work, we explore the use of bounding box labels for weakly supervised glottis segmentation. As such, bounding box labels are relatively easier to annotate. The proposed method uses multiple instance learning to leverage bounding box labels in the form of bag labels. The method outperforms the baseline method (trained with bounding box as mask) by 0.20 in terms of dice score, and matches the performance of fully supervised version after fine-tuning",
    "checked": true,
    "id": "a8ace6742cdf9bc3e5dfbc87e6edf9dedb25fb20",
    "semantic_title": "weakly supervised glottis segmentation in high-speed videoendoscopy using bounding box labels",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23k_interspeech.html": {
    "title": "An Efficient and Noise-Robust Audiovisual Encoder for Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "Boosted by self-supervised learning (SSL) on large amounts of unlabeled data, computationally demanding transformer-based audiovisual ASR (AV-ASR) achieves state-of-the-art performance. In this work, we are the first to propose teacher-student model distillation for an efficient and noise-robust AV encoder for AV-ASR. First, we compare two options for the teacher, a non-task-specific and a task-specific one. Second, we investigate the design and the components in the student neural network. Third, we explore loss function choices during distillation. By distillation with a simplified loss function, the final efficient conformer-based student has 69% fewer parameters and 23% less computational power than the teacher, but excels the baseline student with a WER of 4.6% (11.4%) in clean condition, and with 20.2% (35.7%) in 0dB babble noise. On average over noise types in 0dB SNR, our proposed student even achieves more than 50% relative WER reduction compared to the baseline student",
    "checked": true,
    "id": "47b16dd124960faa7d1529d0a8063f3c7995fde8",
    "semantic_title": "an efficient and noise-robust audiovisual encoder for audiovisual speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23b_interspeech.html": {
    "title": "A Novel Self-training Approach for Low-resource Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we propose a self-training approach for automatic speech recognition (ASR) for low-resource settings. While self-training approaches have been extensively developed and evaluated for high-resource languages such as English, their applications to low-resource languages like Punjabi have been limited, despite the language being spoken by millions globally. The scarcity of annotated data has hindered the development of accurate ASR systems, especially for low-resource languages (e.g., Punjabi and Māori languages). To address this issue, we propose an effective self-training approach that generates highly accurate pseudo-labels for unlabeled low-resource speech. Our experimental analysis demonstrates that our approach significantly improves word error rate, achieving a relative improvement of 14.94% compared to a baseline model across four real-speech datasets. Further, our proposed approach reports the best results on the Common Voice Punjabi dataset",
    "checked": true,
    "id": "6c72d274fe460c3f0a4b6d0e66c8ddc698c6943e",
    "semantic_title": "a novel self-training approach for low-resource speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23g_interspeech.html": {
    "title": "FunASR: A Fundamental End-to-End Speech Recognition Toolkit",
    "volume": "main",
    "abstract": "This paper introduces FunASR, an open-source speech recognition toolkit designed to bridge the gap between academic research and industrial applications. FunASR offers models trained on large-scale industrial corpora and the ability to deploy them in applications. The toolkit's flagship model, Paraformer, is a non-autoregressive end-to-end speech recognition model that has been trained on a manually annotated Mandarin speech recognition dataset that contains 60,000 hours of speech. To improve the performance of Paraformer, we have added timestamp prediction and hotword customization capabilities to the standard Paraformer backbone. In addition, to facilitate model deployment, we have open-sourced a voice activity detection model based on the Feedforward Sequential Memory Network (FSMN-VAD) and a text post-processing punctuation model based on the controllable time-delay Transformer (CT-Transformer), both of which were trained on industrial corpora. These functional modules provide a solid foundation for building high-precision long audio speech recognition services. Compared to other models trained on open datasets, Paraformer demonstrates superior performance",
    "checked": true,
    "id": "5dc148a0548f63e125acb37badfda87deacd28a4",
    "semantic_title": "funasr: a fundamental end-to-end speech recognition toolkit",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23_interspeech.html": {
    "title": "Streaming Audio-Visual Speech Recognition with Alignment Regularization",
    "volume": "main",
    "abstract": "In this work, we propose a streaming AV-ASR system based on a hybrid connectionist temporal classification (CTC)/attention neural network architecture. The audio and the visual encoder neural networks are both based on the conformer architecture, which is made streamable using chunk-wise self-attention (CSA) and causal convolution. Streaming recognition with a decoder neural network is realized by using the triggered attention technique, which performs time-synchronous decoding with joint CTC/attention scoring. Additionally, we propose a novel alignment regularization technique that promotes synchronization of the audio and visual encoder, which in turn results in better word error rates (WERs) at all SNR levels for streaming and offline AV-ASR models. The proposed AV-ASR model achieves WERs of 2.0% and 2.6% on the Lip Reading Sentences 3 (LRS3) dataset in an offline and online setup, respectively, which both present state-of-the-art results when no external training data are used",
    "checked": true,
    "id": "2b2843ac65f370fcf64c1e8fe07d7da9304c1287",
    "semantic_title": "streaming audio-visual speech recognition with alignment regularization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fernandezlopez23_interspeech.html": {
    "title": "SparseVSR: Lightweight and Noise Robust Visual Speech Recognition",
    "volume": "main",
    "abstract": "Recent advances in deep neural networks have achieved unprecedented success in visual speech recognition. However, there remains substantial disparity between current methods and their deployment in resource-constrained devices. In this work, we explore different magnitude-based pruning techniques to generate a lightweight model that achieves higher performance than its dense model equivalent, especially under the presence of visual noise. Our sparse models achieve state-of-the-art results at 10% sparsity on the LRS3 dataset and outperform the dense equivalent up to 70% sparsity. We evaluate our 50% sparse model on 7 different visual noise types and achieve an overall absolute improvement of more than 2% WER compared to the dense equivalent. Our results confirm that sparse networks are more resistant to noise than dense networks",
    "checked": true,
    "id": "531ae1f7fcb29911a6d519c454e7035a79a3abf9",
    "semantic_title": "sparsevsr: lightweight and noise robust visual speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23c_interspeech.html": {
    "title": "Multimodal Speech Recognition for Language-Guided Embodied Agents",
    "volume": "main",
    "abstract": "Benchmarks for language-guided embodied agents typically assume text-based instructions, but deployed agents will encounter spoken instructions. While Automatic Speech Recognition (ASR) models can bridge the input gap, erroneous ASR transcripts can hurt the agents' ability to complete tasks. We propose training a multimodal ASR model that utilizes the accompanying visual context to reduce errors in spoken instruction transcripts. We train our model on a dataset of synthetic spoken instructions, derived from the ALFRED household task dataset, where we simulate acoustic noise by systematically masking spoken words. We find that utilizing visual observations facilitates masked word recovery, with multimodal ASR models recovering up to 30% more masked words than unimodal baselines. We also find that spoken instructions transcribed by multimodal ASR models result in higher task completion success rates for a language-guided embodied agent. github.com/Cylumn/embodied-multimodal-asr",
    "checked": true,
    "id": "db59aef7b6c0a6804769eed43ed9797e5fab778a",
    "semantic_title": "multimodal speech recognition for language-guided embodied agents",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nishu23_interspeech.html": {
    "title": "Matching Latent Encoding for Audio-Text based Keyword Spotting",
    "volume": "main",
    "abstract": "Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown high-quality results, but the key challenge of how to semantically align two embeddings for multi-word keywords of different sequence lengths remains largely unsolved. In this paper, we propose an audio-text-based end-to-end model architecture for flexible keyword spotting (KWS), which builds upon learned audio and text embeddings. Our architecture uses a novel dynamic programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally partition the audio sequence into the same length as the word-based text sequence using the monotonic alignment of spoken content. Our proposed model consists of an encoder block to get audio and text embeddings, a projector block to project individual embeddings to a common latent space, and an audio-text aligner containing a novel DSP algorithm, which aligns the audio and text embeddings to determine if the spoken content is the same as the text. Experimental results show that our DSP is more effective than other partitioning schemes, and the proposed architecture outperformed the state-of-the-art results on the public dataset in terms of Area Under the ROC Curve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively",
    "checked": true,
    "id": "1198506b8dfa2a001ed0bab82a5403f231a3431e",
    "semantic_title": "matching latent encoding for audio-text based keyword spotting",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/p23_interspeech.html": {
    "title": "Self-Paced Pattern Augmentation for Spoken Term Detection in Zero-Resource",
    "volume": "main",
    "abstract": "The spoken term detection task is challenging when a large volume of spoken content is generated without annotation. The pattern discovery approach aims to overcome the challenges by capturing the pattern similarities directly from the representation of the speech signal. A challenge to the pattern discovery task is handling the variabilities in natural speech. In the proposed approach, we aim to overcome the pattern variability challenges in the spoken term similarity region in three stages. At first, the pattern similarities between two spoken terms were captured using our heuristic search, and the pattern variabilities in the similarity region were observed. In the second stage, the observed pattern variabilities were augmented to the Siamese network to learn the relationship. Finally, the learned network is used to identify the matches between spoken query and document. Based on the experimental studies, it is observed that the proposed approach reduces the false alarms by 17.7% and improves the spoken term detection accuracy by 7.1% against the Microsoft Low-Resource Language corpus",
    "checked": true,
    "id": "2836db777602f1911406c7d6bd0efc83e6a874f4",
    "semantic_title": "self-paced pattern augmentation for spoken term detection in zero-resource",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23y_interspeech.html": {
    "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
    "volume": "main",
    "abstract": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints",
    "checked": true,
    "id": "44c72391f943ac7106484fc6eee405df0d87d629",
    "semantic_title": "on-device constrained self-supervised speech representation learning for keyword spotting via knowledge distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/michieli23_interspeech.html": {
    "title": "Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics",
    "volume": "main",
    "abstract": "Keyword Spotting (KWS) models on embedded devices should adapt fast to new user-defined words without forgetting previous ones. Embedded devices have limited storage and computational resources, thus, they cannot save samples or update large models. We consider the setup of embedded online continual learning (EOCL), where KWS models with frozen backbone are trained to incrementally recognize new words from a non-repeated stream of samples, seen one at a time. To this end, we propose Temporal Aware Pooling (TAP) which constructs an enriched feature space computing high-order moments of speech features extracted by a pre-trained backbone. Our method, TAP-SLDA, updates a Gaussian model for each class on the enriched feature space to effectively use audio representations. In experimental analyses, TAP-SLDA outperforms competitors on several setups, backbones, and baselines, bringing a relative average gain of 11.3% on the GSC dataset",
    "checked": true,
    "id": "e6c7f8f3089a46d1efacc3b9c2612f1534b83719",
    "semantic_title": "online continual learning in keyword spotting for low-resource devices via pooling high-order temporal statistics",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23j_interspeech.html": {
    "title": "Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data",
    "volume": "main",
    "abstract": "Few-shot keyword spotting (FS-KWS) models usually require large-scale annotated datasets to generalize to unseen target keywords. However, existing KWS datasets are limited in scale and gathering keyword-like labeled data is costly undertaking. To mitigate this issue, we propose a framework that uses easily collectible, unlabeled reading speech data as an auxiliary source. Self-supervised learning has been widely adopted for learning representations from unlabeled data; however, it is known to be suitable for large models with enough capacity and is not practical for training a small footprint FS-KWS model. Instead, we automatically annotate and filter the data to construct a keyword-like dataset, LibriWord, enabling supervision on auxiliary data. We then adopt multi-task learning that helps the model to enhance the representation power from out-of-domain auxiliary data. Our method notably improves the performance over competitive methods in the FS-KWS benchmark",
    "checked": true,
    "id": "d658235e1878f7cbef6e40e6fa9585fe2cc5891b",
    "semantic_title": "improving small footprint few-shot keyword spotting with supervision on auxiliary data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23t_interspeech.html": {
    "title": "Robust Keyword Spotting for Noisy Environments by Leveraging Speech Enhancement and Speech Presence Probability",
    "volume": "main",
    "abstract": "Although various deep keyword spotting (KWS) systems have demonstrated promising performance under relatively noiseless environments, accurate keyword detection in the presence of strong noise remains challenging. Room acoustics and noise conditions can be highly diverse, leading to drastic performance degradation if not handled carefully. In this paper, we propose a noise management front-end called SE-SPP Net performing speech enhancement (SE) and speech presence probability (SPP) estimation jointly for robust KWS in noise. The SE-SPP Net estimates both the denoised Mel spectrogram and the position of the speech utterance in the noisy signal, where the latter is estimated as the probability of a particular time-frequency bin containing speech. Further, it comes at relatively no cost in model size when compared to a model estimating the denoised speech. Our SE-SPP Net can improve noisy KWS performance by up to 7% compared to a similar sized state-of-the-art model at SNR -10dB",
    "checked": true,
    "id": "9d880267e6f8db8719ce226aef3dd5b707cb0c1e",
    "semantic_title": "robust keyword spotting for noisy environments by leveraging speech enhancement and speech presence probability",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23m_interspeech.html": {
    "title": "Enhancing the Unified Streaming and Non-streaming Model with Contrastive Learning",
    "volume": "main",
    "abstract": "The unified streaming and non-streaming speech recognition model has achieved great success due to its comprehensive capabilities. In this paper, we propose to improve the accuracy of the unified model by bridging the inherent representation gap between the streaming and non-streaming modes with a contrastive objective. Specifically, the top-layer hidden representation at the same frame of the streaming and non-streaming modes are regarded as a positive pair, encouraging the representation of the streaming mode close to its non-streaming counterpart. The multiple negative samples are randomly selected from the rest frames of the same sample under the non-streaming mode. Experimental results demonstrate that the proposed method achieves consistent improvements toward the unified model in both streaming and non-streaming modes. Our method achieves CER of 4.66% in the streaming mode and CER of 4.31% in the non-streaming mode, which sets a new state-of-the-art on the AISHELL-1 benchmark",
    "checked": true,
    "id": "c4d4ff1c6bd7363e24245c73894fabc9c552170a",
    "semantic_title": "enhancing the unified streaming and non-streaming model with contrastive learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23c_interspeech.html": {
    "title": "ZeroPrompt: Streaming Acoustic Encoders are Zero-Shot Masked LMs",
    "volume": "main",
    "abstract": "In this paper, we present ZeroPrompt (Figure 1-(a)) and the corresponding Prompt-and-Refine strategy (Figure 3), two simple but effective training-free methods to decrease the Token Display Time (TDT) of streaming ASR models without any accuracy loss. The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken. We argue that streaming acoustic encoders naturally have the modeling ability of Masked Language Models and our experiments demonstrate that ZeroPrompt is engineering cheap and can be applied to streaming acoustic encoders on any dataset without any accuracy loss. Specifically, compared with our baseline models, we achieve 350~700ms reduction on First Token Display Time (TDT-F) and 100~400ms reduction on Last Token Display Time (TDT-L), with theoretically and experimentally equal WER on both Aishell-1 and Librispeech datasets",
    "checked": true,
    "id": "635e5a007dd2e31503c9a5b0668f44fd6b10c767",
    "semantic_title": "zeroprompt: streaming acoustic encoders are zero-shot masked lms",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23_interspeech.html": {
    "title": "Improved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation",
    "volume": "main",
    "abstract": "Punctuated text prediction is crucial for automatic speech recognition as it enhances readability and impacts downstream natural language processing tasks. In streaming scenarios, the ability to predict punctuation in real-time is particularly desirable but presents a difficult technical challenge. In this work, we propose a method for predicting punctuated text from input speech using a chunk-based Transformer encoder trained with Connectionist Temporal Classification (CTC) loss. The acoustic model trained with long sequences by concatenating the input and target sequences can learn punctuation marks attached to the end of sentences more effectively. Additionally, by combining CTC losses on the chunks and utterances, we achieved both the improved F1 score of punctuation prediction and Word Error Rate (WER)",
    "checked": true,
    "id": "22dc06d4ca6591ac6f23490c9d2e434330a6e1ce",
    "semantic_title": "improved training for end-to-end streaming automatic speech recognition model with punctuation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huybrechts23_interspeech.html": {
    "title": "DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer",
    "volume": "main",
    "abstract": "Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings",
    "checked": true,
    "id": "ba99f8a00047f12d803f49325f6e5b8cef154b08",
    "semantic_title": "dctx-conformer: dynamic context carry-over for low latency unified streaming and non-streaming conformer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23_interspeech.html": {
    "title": "Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer",
    "volume": "main",
    "abstract": "Streaming automatic speech recognition (ASR) models are restricted from accessing future context, which results in worse performance compared to the non-streaming models. To improve the performance of streaming ASR, knowledge distillation (KD) from the non-streaming to streaming model has been studied, mainly focusing on aligning the output token probabilities. In this paper, we propose a layer-to-layer KD from the teacher encoder to the student encoder. To ensure that features are extracted using the same context, we insert auxiliary non-streaming branches to the student and perform KD from the non-streaming teacher layer to the non-streaming auxiliary layer. We design a special KD loss that leverages the autoregressive predictive coding (APC) mechanism to encourage the streaming model to predict unseen future contexts. Experimental results show that the proposed method can significantly reduce the word error rate compared to previous token probability distillation methods",
    "checked": true,
    "id": "448071368f036bc8178d0be8ef1c3c3db127b33e",
    "semantic_title": "knowledge distillation from non-streaming to streaming asr encoder using auxiliary non-streaming layer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23d_interspeech.html": {
    "title": "Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition",
    "volume": "main",
    "abstract": "By incorporating additional contextual information, deep biasing methods have emerged as a promising solution for speech recognition of personalized words. However, for real-world voice assistants, always biasing on personalized words with high prediction scores can degrade the performance of recognizing common words. To address this issue, we propose an adaptive contextual biasing method based on Context-Aware Transformer Transducer (CATT) that utilizes the biased encoder and predictor embeddings to perform streaming prediction of contextual phrase occurrences. This prediction is then used to switch the bias list on and off, enabling the model to adapt to both personalized and common scenarios. Experiments on Librispeech and internal voice assistant datasets show that our approach can achieve up to 6.7% and 20.7% relative reduction in WER and CER compared to the baseline respectively, mitigating up to 96.7% and 84.9% of the relative WER and CER increase for common cases",
    "checked": true,
    "id": "53103c3746965b40f95a5870a45d9e3ea28d5d8e",
    "semantic_title": "adaptive contextual biasing for transducer based streaming speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martel23_interspeech.html": {
    "title": "Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model",
    "volume": "main",
    "abstract": "We propose Audio-Visual Lightweight ITerative model (AVLIT), an effective and lightweight neural network that uses Progressive Learning (PL) to perform audio-visual speech separation in noisy environments. To this end, we adopt the Asynchronous Fully Recurrent Convolutional Neural Network (A-FRCNN), which has shown successful results in audio-only speech separation. Our architecture consists of an audio branch and a video branch, with iterative A-FRCNN blocks sharing weights for each modality. We evaluated our model in a controlled environment using the NTCD-TIMIT dataset and in-the-wild using a synthetic dataset that combines LRS3 and WHAM!. The experiments demonstrate the superiority of our model in both settings with respect to various audio-only and audio-visual baselines. Furthermore, the reduced footprint of our model makes it suitable for low resource applications",
    "checked": true,
    "id": "eb02d89acb27f523d7b1505b24ea9bfd3d27ab6f",
    "semantic_title": "audio-visual speech separation in noisy environments with a lightweight iterative model",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saijo23_interspeech.html": {
    "title": "Remixing-based Unsupervised Source Separation from Scratch",
    "volume": "main",
    "abstract": "We propose an unsupervised approach for training separation models from scratch using RemixIT and Self-Remixing, which are recently proposed self-supervised learning methods for refining pre-trained models. They first separate mixtures with a teacher model and create pseudo-mixtures by shuffling and remixing the separated signals. A student model is then trained to separate the pseudo-mixtures using either the teacher's outputs or the initial mixtures as supervision. To refine the teacher's outputs, the teacher's weights are updated with the student's weights. While these methods originally assumed that the teacher is pre-trained, we show that they are capable of training models from scratch. We also introduce a simple remixing method to stabilize training. Experimental results demonstrate that the proposed approach outperforms mixture invariant training, which is currently the only available approach for training a monaural separation model from scratch",
    "checked": true,
    "id": "49604747961bc89e85e90079806e77840b68663f",
    "semantic_title": "remixing-based unsupervised source separation from scratch",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/okamoto23_interspeech.html": {
    "title": "CAPTDURE: Captioned Sound Dataset of Single Sources",
    "volume": "main",
    "abstract": "In conventional studies on environmental sound separation and synthesis using captions, datasets consisting of multiple-source sounds with their captions were used for model training. However, when we collect the captions for multiple-source sound, it is not easy to collect detailed captions for each sound source, such as the number of sound occurrences and timbre. Therefore, it is difficult to extract only the single-source target sound by the model-training method using a conventional captioned sound dataset. In this work, we constructed a dataset with captions for a single-source sound named CAPTDURE, which can be used in various tasks such as environmental sound separation and synthesis. Our dataset consists of 1,044 sounds and 4,902 captions. We evaluated the performance of environmental sound extraction using our dataset. The experimental results show that the captions for single-source sounds are effective in extracting only the single-source target sound from the mixture sound",
    "checked": true,
    "id": "028bb4bd56a29ee955d46f28f640b4c34bafdecd",
    "semantic_title": "captdure: captioned sound dataset of single sources",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/munakata23_interspeech.html": {
    "title": "Recursive Sound Source Separation with Deep Learning-based Beamforming for Unknown Number of Sources",
    "volume": "main",
    "abstract": "We propose a recursive separation model for an unknown number of sound sources based on deep learning-based beamforming. Recursive separation models have been investigated as a way to separate a mixture signal composed of an unknown number of sources in a single-channel condition. The mixture signal is separated with residual information in a recursive manner. Although the recursive separation model can be extended to a multi-channel condition using a beamforming-based filter, the separation performance is degraded because the beamforming-based filter tends to accumulate estimation errors in the recursions. To address this problem, we introduce a local Gaussian model (LGM)-based recursive separation model. The proposed method mitigates the accumulation of errors by reusing estimated parameters and applying only one filter to the mixture signal. Experimental results show that our proposed method outperforms a separation model using an accumulative filter",
    "checked": true,
    "id": "a95917f32e83120ed9527d87fc1c1993861fb74f",
    "semantic_title": "recursive sound source separation with deep learning-based beamforming for unknown number of sources",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mosner23_interspeech.html": {
    "title": "Multi-Channel Speech Separation with Cross-Attention and Beamforming",
    "volume": "main",
    "abstract": "Originally, single-channel source separation gained more research interest. It resulted in immense progress. Multi-channel (MC) separation comes with new challenges posed by adverse indoor conditions making it an important field of study. We seek to combine promising ideas from the two worlds. First, we build MC models by extending current single-channel time-domain separators relying on their strength. Our approach allows reusing pre-trained models by inserting designed lightweight reference channel attention (RCA) combiner, the only trained module. It comprises two blocks: the former allows attending to different parts of other channels w.r.t. the reference one, and the latter provides an attention-based combination of channels. Second, like many successful MC models, our system incorporates beamforming and allows for the fusion of the network and beamformer outputs. We compare our approach with the SOTA models on the SMS-WSJ dataset and show better or similar performance",
    "checked": true,
    "id": "59a438952214794f978593e72f56430357e75b7a",
    "semantic_title": "multi-channel speech separation with cross-attention and beamforming",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eom23_interspeech.html": {
    "title": "Background-Sound Controllable Voice Source Separation",
    "volume": "main",
    "abstract": "There have been various approaches to separate mixed voices. In the real world, input voices contain many different kinds of background sounds but existing methods have not considered the background sounds in model architectures. These approaches are difficult to control the background sounds directly and the voice separation results include the background sounds randomly. In this paper, we propose an extended voice separation framework, background-sound controllable voice source separation that can control the degrees of background sounds of voice separation outputs using a control parameter that ranges from 0 to 1 without additional mixing procedures. Several experiments show the controllability of background sounds on various real world datasets with preserving voice separation performances",
    "checked": true,
    "id": "bec608753780f2505559a010abb500ccacbae345",
    "semantic_title": "background-sound controllable voice source separation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/escobargrisales23_interspeech.html": {
    "title": "An Automatic Multimodal Approach to Analyze Linguistic and Acoustic Cues on Parkinson's Disease Patients",
    "volume": "main",
    "abstract": "Early detection and monitoring of Parkinson's disease are crucial for properly treating and managing the symptoms. Automatic speech and language analysis has emerged as a promising non-invasive method to monitor the patient's state. This study analyzed different speech and language representations for automatic classification between Parkinson's disease patients and healthy controls. First, each modality is analyzed independently. General representations such as Wav2vec or BETO are used together with representations oriented to model disease traits such as phonemic identifiability in speech modality and grammatical units analysis in language modality. The best speech and language representations were combined using a fusion strategy based on Gated Multimodal Units. The best results are achieved with the multimodal approach, outperforming all results obtained with unimodal representations and the traditional fusion strategy",
    "checked": true,
    "id": "655d462fc9955620257053801d6253e9fe56f723",
    "semantic_title": "an automatic multimodal approach to analyze linguistic and acoustic cues on parkinson's disease patients",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23_interspeech.html": {
    "title": "Personalization for Robust Voice Pathology Detection in Sound Waves",
    "volume": "main",
    "abstract": "Automatic voice pathology detection is promising for non-invasive screening and early intervention using sound signals. Nevertheless, existing methods are susceptible to covariate shifts due to background noises, human voice variations, and data selection biases leading to severe performance degradation in real-world scenarios. Hence, we propose a non-invasive framework that contrastively learns personalization from sound waves as a pre-train and predicts latent-spaced profile features through semi-supervised learning. It allows all subjects from various distributions (e.g., regionality, gender, age) to benefit from personalized predictions for robust voice pathology in a privacy-fulfilled manner. We extensively evaluate the framework on four real-world respiratory illnesses datasets, including Coswara, COUGHVID, ICBHI and our private dataset - ASound, under multiple covariate shift settings (i.e., cross-dataset), improving up to 4.12% in overall performance",
    "checked": true,
    "id": "dd4f872c22a6b7c28a81b9ea1f803f1b34bb29e1",
    "semantic_title": "personalization for robust voice pathology detection in sound waves",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23d_interspeech.html": {
    "title": "Integrated and Enhanced Pipeline System to Support Spoken Language Analytics for Screening Neurocognitive Disorders",
    "volume": "main",
    "abstract": "This paper presents an enhanced pipeline system for automated screening of neurocognitive disorders, e.g. Alzheimer's Disease (AD), using spoken language technologies. To ensure local relevance, the pipeline is applied to two-way interactions between clinical assessors and older adult participants in spoken Cantonese, the predominant language used in Hong Kong. The pipeline includes: (i) Speaker diarization using speaker-turn-aware scoring to capture the temporal structure of conversations. (ii) ASR using XLS-R wav2vec 2.0 models further pre-trained on Cantonese speech data and fine-tuned. (iii) Language modelling using RoBERTa with further fine-tuning. (iv) AD screening with neural network classification. A reference benchmark is obtained using the ADReSS corpus where no diarization is needed, and the partial pipeline attained a competitive detection accuracy of 87.5%",
    "checked": true,
    "id": "e5346f94b1e6138cbbebc6b7a3c01d666e29755a",
    "semantic_title": "integrated and enhanced pipeline system to support spoken language analytics for screening neurocognitive disorders",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niu23b_interspeech.html": {
    "title": "Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder",
    "volume": "main",
    "abstract": "Emotion is a complex behavioral phenomenon, which is expressed and perceived through various modalities, such as language, vocal and facial expressions. Psychiatric research has suggested that the lack of emotional alignment between modalities is a symptom of emotion disorders. In this work, we quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional MisMatch (EMM), as an intermediate step for mood identification. We use a longitudinal dataset collected from people with Bipolar Disorder (BP) and show that symptomatic mood episodes show significantly more EMM, compared to euthymic moods. We propose a fully automatic mood identification pipeline with automatic speech transcription, emotion recognition, and EMM feature extraction. We find that EMM features, although smaller in size, outperform a language-based baseline, and consistently provide improvement when combined with language and/or raw emotion features on mood classification",
    "checked": true,
    "id": "015ee16bb1c149c53f486c60c44bea54b7812fe0",
    "semantic_title": "capturing mismatch between textual and acoustic emotion expressions for mood identification in bipolar disorder",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23d_interspeech.html": {
    "title": "FTA-net: A Frequency and Time Attention Network for Speech Depression Detection",
    "volume": "main",
    "abstract": "Depression is one of the most common mental diseases nowadays, which seriously affects the health of individuals. Some researchers have shown an association between the level of depression and speech features in individuals, so a lot of automatic speech-based depression detection systems have been proposed. A number of studies utilized convolutional neural network (CNN) to realize the speech depression detection. However, most of these studies did not take into account that different frequencies and time steps in the speech spectrum features contribute unequally to the detection of depression. In order to extract more significant and distinctive features, this paper proposes an effective frequency-time attention (FTA) module for CNN, which is based on squeeze and excitation operations and can emphasize the time steps and frequencies associated with depression. Experimental results based on the AVEC 2013 and AVEC 2014 benchmarks demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "369c69aea9cfd1d6d2bf5bfb6def022ee43c85ee",
    "semantic_title": "fta-net: a frequency and time attention network for speech depression detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fara23_interspeech.html": {
    "title": "Bayesian Networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data",
    "volume": "main",
    "abstract": "Predicting the presence of major depressive disorder (MDD) using speech is highly non-trivial. The heterogeneous clinical profile of MDD means that any given speech pattern may be associated with a unique combination of depressive symptoms. Conventional discriminative machine learning models may lack the complexity to robustly model this heterogeneity. Bayesian networks, however, are well-suited to such a scenario. They provide further advantages over standard discriminative modeling by offering the possibility to (i) fuse with other data streams; (ii) incorporate expert opinion into the models; (iii) generate explainable model predictions, inform about the uncertainty of predictions, and (iv) handle missing data. In this study, we apply a Bayesian framework to capture the relationships between depression, depression symptoms, and features derived from speech, facial expression and cognitive game data. Presented results also highlight our model is not subject to demographic biases",
    "checked": true,
    "id": "48e3815ac637d6d1d4f38119c4e2a77dea855661",
    "semantic_title": "bayesian networks for the robust and unbiased prediction of depression and its symptoms utilizing speech and multimodal data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23y_interspeech.html": {
    "title": "Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "Automatic recognition of disordered and elderly speech remains highly challenging tasks to date due to data scarcity. Parameter fine-tuning is often used to exploit the large quantities of non-aged and healthy speech pre-trained models, while neural architecture hyper-parameters are set using expert knowledge and remain unchanged. This paper investigates hyper-parameter adaptation for Conformer ASR systems that are pre-trained on the Librispeech corpus before being domain adapted to the DementiaBank elderly and UASpeech dysarthric speech datasets. Experimental results suggest that hyper-parameter adaptation produced word error rate (WER) reductions of 0.45% and 0.67% over parameter-only fine-tuning on DBank and UASpeech tasks respectively. An intuitive correlation is found between the performance improvements by hyper-parameter domain adaptation and the relative utterance length ratio between the source and target domain data",
    "checked": true,
    "id": "daaf51daf49a7e05458ffe686f2972c0fe41cdc3",
    "semantic_title": "hyper-parameter adaptation of conformer asr systems for elderly and dysarthric speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/campbell23_interspeech.html": {
    "title": "Classifying depression symptom severity: Assessment of speech representations in personalized and generalized machine learning models",
    "volume": "main",
    "abstract": "There is an urgent need for new methods that improve the management and treatment of Major Depressive Disorder (MDD). Speech has long been regarded as a promising digital marker in this regard, with many works highlighting that speech changes associated with MDD can be captured through machine learning models. Typically, findings are based on cross-sectional data, with little work exploring the advantages of personalization in building more robust and reliable models. This work assesses the strengths of different combinations of speech representations and machine learning models, in personalized and generalized settings in a two-class depression severity classification paradigm. Key results on a longitudinal dataset highlight the benefits of personalization. Our strongest performing model set-up utilized self-supervised learning features and convolutional neural network (CNN) and long short-term memory (LSTM) back-end",
    "checked": true,
    "id": "524989f8798a4e5424ce912545646aadf5390e56",
    "semantic_title": "classifying depression symptom severity: assessment of speech representations in personalized and generalized machine learning models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghaffarzadegan23_interspeech.html": {
    "title": "Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma",
    "volume": "main",
    "abstract": "Existing audio-based asthma monitoring solutions rely on feature engineering designs paired with contact-based auscultation which are brittle in practice and do not scale beyond point of care setups. Data-driven methods utilizing contactless microphones have the potential to address such limitations. These solutions are under-explored in healthcare due to high cost of data curation requiring physicians-in-the-loop. Here, we propose an active learning (AL) system to facilitate audio data collection and annotation. It detects lung sound abnormalities in asthma. AL reduces the annotation cost while increasing the model performance under a constrained annotation budget. It automatically extracts interesting audio segments from the continuous recordings, and efficiently annotates and trains anomaly detector model. The experimental results confirm the effectiveness of the proposed system as an enabler for larger scale data curation on a newly collected audio corpus for pediatric asthma",
    "checked": true,
    "id": "8d3e9790f07f70fc661a4319516a6fb8d32fb58e",
    "semantic_title": "active learning for abnormal lung sound data curation and detection in asthma",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pereztoro23_interspeech.html": {
    "title": "Automatic Assessment of Alzheimer's across Three Languages Using Speech and Language Features",
    "volume": "main",
    "abstract": "With the increasing prevalence of Alzheimer's Disease (AD) worldwide, it is essential to develop non-invasive methods to monitor the progression of the disease. Speech and language analyses are suitable for detecting the cognitive impairment of AD patients; thus, by analyzing changes in speech patterns and language use, researchers can develop methods to monitor AD remotely. In this paper, we investigated several speech and language techniques commonly used for the automatic detection of AD. Furthermore, we considered speech recordings of 448 patients in three different languages: Spanish (57), German (205), and English (186). Cross-lingual analysis was carried out using two classification approaches: (1) training/testing in one or more languages and (2) training in one language and testing in another. We obtained unweighted average recall values of up to 83% to classify AD using the first classification approach and up to 70% with the second",
    "checked": true,
    "id": "5e485d7b072b3cf5b8ef260039d5c4b106c652a9",
    "semantic_title": "automatic assessment of alzheimer's across three languages using speech and language features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geng23_interspeech.html": {
    "title": "On-the-Fly Feature Based Rapid Speaker Adaptation for Dysarthric and Elderly Speech Recognition",
    "volume": "main",
    "abstract": "Accurate recognition of dysarthric and elderly speech remain challenging tasks to date. Speaker-level heterogeneity attributed to accent or gender, when aggregated with age and speech impairment, create large diversity among these speakers. Scarcity of speaker-level data limits the practical use of data-intensive model based speaker adaptation methods. To this end, this paper proposes two novel forms of data-efficient, feature-based on-the-fly speaker adaptation methods: variance-regularized spectral basis embedding (SVR) and spectral feature driven f-LHUC transforms. Experiments conducted on UASpeech dysarthric and DementiaBank Pitt elderly speech corpora suggest the proposed on-the-fly speaker adaptation approaches consistently outperform baseline iVector adapted hybrid DNN/TDNN and E2E Conformer systems by statistically significant WER reduction of 2.48%-2.85% absolute (7.92%-8.06% relative), and offline model based LHUC adaptation by 1.82% absolute (5.63% relative) respectively",
    "checked": true,
    "id": "403db1c9d91c4e99fd82b77d4cd7b0d09624faff",
    "semantic_title": "on-the-fly feature based rapid speaker adaptation for dysarthric and elderly speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/svihlik23_interspeech.html": {
    "title": "Relationship between LTAS-based spectral moments and acoustic parameters of hypokinetic dysarthria in Parkinson's disease",
    "volume": "main",
    "abstract": "Although long-term averaged spectrum (LTAS) descriptors can detect the change in dysarthria of patients with Parkinson's disease (PD) due to subthalamic nucleus deep brain stimulation (STN-DBS), the relationship between LTAS variables with measures that relate to laryngeal physiology remain unknown. We aimed to find connections between LTAS-based moments and the main acoustic characteristics of hypokinetic dysarthria in PD as the response to STN-DBS stimulation changes. We analyzed reading passages of 23 PD patients in ON and OFF STN-DBS states compared to 23 healthy controls. We found a relation between the stimulation-induced change in several spectral moments and acoustic parameters representing voice quality, articulatory decay, net speech rate, and mean fundamental frequency. While the difference between PD and controls was significant across most acoustic descriptors, only the spectral mean and fundamental frequency variability could differentiate between ON and OFF conditions",
    "checked": true,
    "id": "311d95c6600b6cdca5d07b4e4e9842a5f33a296c",
    "semantic_title": "relationship between ltas-based spectral moments and acoustic parameters of hypokinetic dysarthria in parkinson's disease",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alvarado23_interspeech.html": {
    "title": "Respiratory distress estimation in human-robot interaction scenario",
    "volume": "main",
    "abstract": "Social robotics and human-robot partnership are becoming very relevant topics in the next decades defining many challenges for speech technology. In addition, the COVID pandemic imposed an awareness of technology challenges to fight massive health problems. In this paper, the first system to estimate respiratory distress in a human-robot interaction (HRI) environment is presented. The training procedure of the dyspnea estimation models by simulating the HRI acoustic environment with real room impulse responses (estimated with a PR2 robot) and additive noise is described. The training and testing data were processed using two beamforming techniques: delay-and-sum and MVDR. The results suggest that it should be possible to reduce significantly the degradation in precision of estimates of respiratory distress in a real HRI scenario. The improvements in accuracy and AUC with MVDR when compared to baseline processing without beamforming are 7% and 4%, respectively",
    "checked": true,
    "id": "0b5193f9a2ac1ce8a6225ebf99bc1d7a8f22f84b",
    "semantic_title": "respiratory distress estimation in human-robot interaction scenario",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/reynerfuentes23_interspeech.html": {
    "title": "Prediction of the Gender-based Violence Victim Condition using Speech: What do Machine Learning Models rely on?",
    "volume": "main",
    "abstract": "Women who have experienced gender-based violence (GBV) are at an increased risk of developing mental illnesses such as depression, anxiety, and post-traumatic stress disorder (PTSD). Recently, Artificial Intelligence (AI) has provided new tools to assist mental health clinical diagnosis, including speech-based detection. However, there is not much work done on the GBV victim (GBVV) condition detection. This study aims to identify specific speech features that aid this detection, analyse the relationship of such results with the user's psychological evaluation, and evaluate whether the models rely on the speaker identity or self-reported emotions to predict the GBVV condition. Our results indicate that it is possible to distinguish GBVV with controlled sequelae from non-victims, which may suggest that such differentiation for GBVV with more severe mental aftereffects-such as PTSD-may be even more meaningful. We believe that our work can help future mental health AI therapy assistants",
    "checked": true,
    "id": "e60da852d8948e172b963a2dd875683e82db8064",
    "semantic_title": "prediction of the gender-based violence victim condition using speech: what do machine learning models rely on?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/charola23_interspeech.html": {
    "title": "Whisper Encoder features for Infant Cry Classification",
    "volume": "main",
    "abstract": "Identifying the pathology in infant cry is an important and socially relevant research problem, as it can save the lives of many infants. This study proposes the use of transfer learning based approach using Whisper Encoder Module which is compared against state-of-the art MFCC feature set for classification of normal vs. pathological infant cry. Moreover, we also present multi-class pathological infant cry classification using CNN and Bi-LSTM networks. Our study finds that whisper encoder module coupled with DNN classifiers such as CNN and Bi-LSTM outperform MFCC features with absolute increment of 4% and 1% on CNN and Bi-LSTM respectively. Furthermore, whisper encoder features are analysed using statistical parameters and t-SNE plots. The experiments are performed using the 10-fold cross-validation on Baby Chillanto dataset, In-house DA-IICT dataset, and also on the datasets formed by combining these two datasets",
    "checked": true,
    "id": "929d422e098c08a0edb20bf91572b195e1241c15",
    "semantic_title": "whisper encoder features for infant cry classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jurov23_interspeech.html": {
    "title": "A neural architecture for selective attention to speech features",
    "volume": "main",
    "abstract": "Speech perception is complex and demands constant adaptations to the speaker and the environment (i.e. noisy speech, accent, etc.). To adapt, the listener relies on one speech feature more than another. This cognitive mechanism is called selective attention. We present a model that captures the idea of selective attention: we show that this dynamic adaptation process can be captured in a neural architecture by using a multiple encoder beta variational auto encoder (beta-ME-VAE), which is based on rate distortion theory. This model implements the idea that optimal feature weighting looks different under different listening conditions and provides insight into how listeners can adapt their listening strategy on a moment-to-moment basis, even in listening situations they haven't experienced before",
    "checked": true,
    "id": "a845c05ecc36a90596fa68db8917674448fb2fac",
    "semantic_title": "a neural architecture for selective attention to speech features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huo23_interspeech.html": {
    "title": "Quantifying Informational Masking due to Masker Intelligibility in Same-talker Speech-in-speech Perception",
    "volume": "main",
    "abstract": "Intelligibility of the competing speech plays a significant role in causing informational masking (IM) to the target speech during speech-in-speech perception, especially in same-talker conditions where the target and the masker share a large number of similarities in acoustics. Few studies have quantitatively measured IM as a function of intelligibility of competing speech. Evidence shows that voiced segments are robust cues for speech intelligibility. In this study, the contribution of masker intelligibility to IM was studied by adjusting the voice-to-noise ratio (VNR) on voiced segments of the competing speech, while maintaining energetic masking (EM) at different target-to-masker ratios. Although model estimations suggested that the intelligibility due to EM converged when VNR<0 dB, listener performance showed that more release from IM was received with a further decrease in VNR. It was projected that masker intelligibility could lead to target intelligibility decreased by 50%",
    "checked": true,
    "id": "0651d3c1fe5dc628697300c6d1670aa63f2ab0af",
    "semantic_title": "quantifying informational masking due to masker intelligibility in same-talker speech-in-speech perception",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cuervo23_interspeech.html": {
    "title": "On the Benefits of Self-supervised Learned Speech Representations for Predicting Human Phonetic Misperceptions",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) trained by self-supervised learning (SSL) have recently been shown to produce representations similar to brain activations for the same speech input. Can SSL representations help to explain human speech perception errors? Aiming to shed light on this question, we study their use for phonetic misperception prediction. We extract representations from wav2vec 2.0, a recent SSL architecture for speech, and use them to compute features for a model predicting the presence of phonetic perception errors in speech-in-noise signals. We perform our experiments on a corpus of over 3000 consistent word-in-noise confusions in English. We consider multiple SSL-based features and compare them against conventional acoustic baselines and features obtained from DNNs fine-tuned through supervised learning for ASR. Our results show the superiority of SSL representations when extracted from the proper layer, further suggesting their potential to model human speech perception",
    "checked": true,
    "id": "fab47c9386872f92dc8c7a83447783012da85f46",
    "semantic_title": "on the benefits of self-supervised learned speech representations for predicting human phonetic misperceptions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schulz23_interspeech.html": {
    "title": "Predicting Perceptual Centers Located at Vowel Onset in German Speech Using Long Short-Term Memory Networks",
    "volume": "main",
    "abstract": "Perceptual centers (p-centers) can be defined as the perceived centers of a syllable. Previous research regarding the location of p-centers in speech relied on experimental methods, and among the suggested acoustic features contributing to the location of p-centers in Germanic languages is the transition of the consonant to the vowel onset. The current study investigates the prediction of the location of p-centers in German, by means of machine learning. Machine learning is a promising tool to capture possible non-linear relationships that may occur among the acoustic features used in the complexity that is the human perception. Therefore, an LSTM neural network approach was used for the identification of p-centers in a set of spoken German sentences, with input data features being Mel Frequency Cepstral Coefficients (MFCC), amplitude envelope and root mean squared energy. The model was able to achieve a balanced accuracy of 84% with MFCCs being the best predictor of p-center location",
    "checked": true,
    "id": "f8f4ddfb50b76b881e7f27b271330b634893e1c0",
    "semantic_title": "predicting perceptual centers located at vowel onset in german speech using long short-term memory networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cooke23_interspeech.html": {
    "title": "Exploring the mutual intelligibility breakdown caused by sculpting speech from a competing speech signal",
    "volume": "main",
    "abstract": "Passing noise through a binary mask representing speech leads to remarkably intelligible speech. However, if the mask input is a competing speech signal, both the competing speech and the target speech represented by the mask are rendered unintelligible. The current study considers potential explanations for this abrupt breakdown. Competing speech was modified to reduce the influence of properties that may have interacted adversely with those of the target, including speaker, language, F0 and spectral detail. Properties were modified by noise-vocoding, envelope substitution and preservation of temporal modulations. The outcome of a listening experiment indicated that the impact of competing speech is largely due to conflicting formant-scale spectral detail and the absence of sufficient energy in specific temporal epochs, while conflicting F0 plays no role. These findings contribute to a broader understanding of the minimal representational basis that underlies speech perception",
    "checked": true,
    "id": "a3b060c86a8f3c63cf94f45962d15fc8f05b9c3e",
    "semantic_title": "exploring the mutual intelligibility breakdown caused by sculpting speech from a competing speech signal",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kitahara23_interspeech.html": {
    "title": "Perception of Incomplete Voicing Neutralization of Obstruents in Tohoku Japanese",
    "volume": "main",
    "abstract": "Intervocalic voicing neutralization has been generally accepted as a peculiar feature of Tohoku dialects. The present paper reports the results of perception experiments on this phenomenon. Natural and resynthesized stimuli spoken by Tohoku speakers were presented to both Tohoku and Tokyo listeners in a series of online experiments. A comparison between these two listener groups reveals that, for Tohoku listeners whose perception was biased by the voicing neutralization in their phonology, the boundary between voiced and voiceless tokens was more blurred compared to Tokyo listeners whose phonology had no such neutralization. These results suggest that neutralization can be bidirectional: i.e., voiced tokens become less voiced and voiceless tokens become less voiceless in contrast to the traditional view of neutralization which assumes a unidirectional process where one category remains intact and the other category merges with the former",
    "checked": true,
    "id": "39a688d168daaffc2a87e3aba5a01ce0509539eb",
    "semantic_title": "perception of incomplete voicing neutralization of obstruents in tohoku japanese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pohnlein23_interspeech.html": {
    "title": "The emergence of obstruent-intrinsic f0 and VOT as cues to the fortis/lenis contrast in West Central Bavarian",
    "volume": "main",
    "abstract": "This study examines the effects of underlying voicing in word initial and medial stops on f0 at the onset of the following stressed and unstressed vowel (CF0), respectively, in Standard German (SG) and West Central Bavarian (WB). As opposed to SG, WB hitherto did not contrast fortis and lenis stops by VOT, but the importance of this cue increases in younger WB speakers. The replacement of VOT by f0 as acoustic cue in connection with voicing mergers and tonogenesis is well-studied but not the emergence of CF0 effects together with an evolving VOT contrast. An acoustic analysis of twenty SG speakers as well as ten older and ten younger WB speakers showed higher f0 after fortis compared to lenis stops in SG but only in initial position where VOT was much longer. Younger but not older WB speakers showed signs of developing CF0 effects in initial stops as found in SG which may forecast VOT differences in this position at the population level possibly due to speaker-specific cue enhancement",
    "checked": true,
    "id": "8cd448ede105cebb732610457c959b814121eb7b",
    "semantic_title": "the emergence of obstruent-intrinsic f0 and vot as cues to the fortis/lenis contrast in west central bavarian",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/havard23_interspeech.html": {
    "title": "〈'〉 in Tsimane': a Preliminary Investigation",
    "volume": "main",
    "abstract": "Tsimane' is a language spoken in Bolivia by several thousand people. Yet, it has not been described in detail. We aim to take a step towards a better description by focusing on an aspect of language: the sound represented in spelling with 〈'〉, informally described as a glottal stop. We recorded two adult speakers of Tsimane' producing (near-)minimal pairs involving this sound. Perceptual analyses suggested 〈'〉 is very rarely realised as a full glottal stop, and is more often cued by creaky-voiced vow- els and nasals. Despite the variability in implementation, presentation of syllabic minimal pairs to these two informants and two other adult Tsimane' listeners revealed evidence that they could easily perceive when 〈'〉 was intended. Together, these data suffice to rule out the hypothesis that 〈'〉 is systematically realised as a full stop, and suggests instead a more complex set of perceptual cues may be at speakers' and listeners' disposal",
    "checked": true,
    "id": "44ef277670749de4289317e909b37de4100237f4",
    "semantic_title": "〈'〉 in tsimane': a preliminary investigation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hoffmann23_interspeech.html": {
    "title": "Segmental features of Brazilian (Santa Catarina) Hunsrik",
    "volume": "main",
    "abstract": "This paper explores the segmentals of Brazilian Hunsrik, Santa Catarina (SC). The account proposed here is merely the starting point for a more comprehensive analysis of this unique, under-resourced dialect of German for which no written standard and only limited prior linguistic descriptions exist. The goal is to characterise the segmental features of SC Hunsrik and compare them to those reported for the Hunsrik of Rio Grande do Sul (RS). The description is based on two Praat-analysed recordings of three speakers: one saying prayers, and two chatting about daily chores. The recurrent features include vowel raising, de-rounding and schwa epenthesis; consonantal aspects involve obstruent voicing assimilation, consonant deletion and /l/-velarisation. The next steps will involve unravelling the features of German Hunsrück and the Brazilian Portuguese influences as well as analysing the dialect's prosody",
    "checked": true,
    "id": "ef88d67eeb539a12d344ec691a86be65f25a9b97",
    "semantic_title": "segmental features of brazilian (santa catarina) hunsrik",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ratko23_interspeech.html": {
    "title": "Opening or Closing? An Electroglottographic Analysis of Voiceless Coda Consonants in Australian English",
    "volume": "main",
    "abstract": "In voiceless sounds, the glottis may be spread or constricted. Glottal spreading is associated with breathiness, and constriction with glottalisation. In many dialects of English, glottalisation often occurs with coda /t/ and sometimes with /p, k/, suggesting coda stop voicelessness is achieved through glottal constriction. Conversely, voiceless coda fricatives are associated with breathiness of the preceding vowel, with voicelessness achieved through glottal spreading. However, analyses specifically measuring glottal activity in coda consonant contexts in English are sparse. We conducted an electroglottographic analysis of vowels preceding voiceless codas /p, t, k, s/ to examine how coda voicelessness is achieved in Australian English (AusE). We found that coda /t/ and /p/ show glottal constriction towards vowel offset. Conversely, /k/ patterns with /s/ and exhibits glottal spreading. This suggests that different glottal configurations are used to achieve coda voicelessness in AusE",
    "checked": true,
    "id": "08c531064e40a81b91656faea94270b242f1f904",
    "semantic_title": "opening or closing? an electroglottographic analysis of voiceless coda consonants in australian english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zebe23_interspeech.html": {
    "title": "Increasing aspiration of word-medial fortis plosives in Swiss Standard German",
    "volume": "main",
    "abstract": "There is evidence for a sound change in progress in German-speaking Switzerland: Namely, Swiss German speakers of Alemannic increasingly use aspiration in fortis plosives, particularly in word-initial position. This study aims to extend the research by investigating word-medial plosives in Swiss Standard German (SSG). Using the apparent-time paradigm, the main goal is to compare younger to older speakers. Since the increasing aspiration is probably driven by the contact to German Standard German (GSG), this study focuses on speakers from both rural and urban areas, assuming that the latter have more contact to speakers of GSG than the former. Results show that younger urban speakers produce longer VOT values in alveolar plosives than the other speakers, while all younger speakers show this pattern for bilabial plosives. Furthermore, only the younger speakers from the urban group produce shorter closure durations in fortis plosives",
    "checked": true,
    "id": "90a01d4e87178b945e22b8706aaae6f226f5b9bc",
    "semantic_title": "increasing aspiration of word-medial fortis plosives in swiss standard german",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shao23_interspeech.html": {
    "title": "Lexical Stress and Velar Palatalization in Italian: A spatio-temporal Interaction",
    "volume": "main",
    "abstract": "Palatalization is the process whereby a velar stop is fronted to a palatal affricate or fricative. In Italian, it takes place at the boundary between the root and /i/ suffixes. In nouns and adjectives, palatalization occurs in words with antepenultimate stress ([ˈko.mi.t͡ʃi]), while it is much rarer in words with penultimate stress ([ka.ˈdu.ki]) Based on one acoustic and one articulatory study (EMA), we postulate that the resistance of post-tonic /k, g/ to palatalize is related to the stressed vowel directly preceding. In the acoustic domain, post-tonic consonants show longer closure duration. This increase in closure duration is directly related to a larger and longer tongue dorsum movement in the articulatory domain. We show an interaction between temporal (closure duration) and spatial (tongue dorsum displacement) aspects of lexical stress, which we interpret as the cause of resistance to palatalization in post-tonic velars. The findings are discussed within the μ-gesture framework",
    "checked": true,
    "id": "48acad41f7b7f28f466b90f987ae92223ec833a5",
    "semantic_title": "lexical stress and velar palatalization in italian: a spatio-temporal interaction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23i_interspeech.html": {
    "title": "Speaker Embeddings as Individuality Proxy for Voice Stress Detection",
    "volume": "main",
    "abstract": "Since the mental states of the speaker modulate speech, stress introduced by cognitive or physical loads could be detected in the voice. The existing voice stress detection benchmark has shown that the audio embeddings extracted from the Hybrid BYOL-S self-supervised model perform well. However, the benchmark only evaluates performance separately on each dataset, but does not evaluate performance across the different types of stress and different languages. Moreover, previous studies found strong individual differences in stress susceptibility. This paper presents the design and development of voice stress detection, trained on more than 100 speakers from 9 language groups and five different types of stress. We address individual variabilities in voice stress analysis by adding speaker embeddings to the hybrid BYOL-S features. The proposed method significantly improves voice stress detection performance with an input audio length of only 3-5 seconds",
    "checked": true,
    "id": "24e7803d58a7ed3f6279bb90ccd21e9ba58f11a8",
    "semantic_title": "speaker embeddings as individuality proxy for voice stress detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23j_interspeech.html": {
    "title": "From Interval to Ordinal: A HMM based Approach for Emotion Label Conversion",
    "volume": "main",
    "abstract": "Ordinal labels along affect dimensions are garnering increasing interest in computation paralinguistics. However, they are rarely obtained directly from raters, and instead typically obtained by conversion from interval labels. Current approaches to such conversion map interval labels to either absolute ordinal labels (AOL) (e.g., low and high), or to relative ordinal labels (ROL) (e.g., one has higher arousal than the other), but never take both into account. This paper presents a novel approach to map time-continuous interval labels to time-continuous ordinal labels. It simultaneously considers both inter-rater ambiguity about where AOLs sit on the interval label scale and the consistency amongst different raters in terms of ROLs. We validate the proposed approach by comparing the converted ordinal labels to original interval labels and the categorical labels for the same speech using the publicly available MSP-Podcast and MSP-Conversation corpora",
    "checked": true,
    "id": "0cc978e550da47a48d39020d0618d2439a22f5a4",
    "semantic_title": "from interval to ordinal: a hmm based approach for emotion label conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23l_interspeech.html": {
    "title": "Turbo your multi-modal classification with contrastive learning",
    "volume": "main",
    "abstract": "Contrastive learning has become one of the most impressive approaches for multi-modal representation learning. However, previous multi-modal works mainly focused on cross-modal understanding, ignoring in-modal contrastive learning, which limits the representation of each modality. In this paper, we propose a novel contrastive learning strategy, called Turbo, to promote multi-modal understanding by joint in-modal and cross-modal contrastive learning. Specifically, multi-modal data pairs are sent through the forward pass twice with different hidden dropout masks to get two different representations for each modality. With these representations, we obtain multiple in-modal and cross-modal contrastive objectives for training. Finally, we combine the self-supervised Turbo with the supervised multi-modal classification and demonstrate its effectiveness on two audio-text classification tasks, where the state-of-the-art performance is achieved on a speech emotion recognition benchmark dataset",
    "checked": true,
    "id": "0dc58873c1765110311faf0afeb9dbde8d6fca44",
    "semantic_title": "turbo your multi-modal classification with contrastive learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ioannides23_interspeech.html": {
    "title": "Towards Paralinguistic-Only Speech Representations for End-to-End Speech Emotion Recognition",
    "volume": "main",
    "abstract": "We propose a methodology for information aggregation from the various transformer layer outputs of a generic speech Encoder (e.g. WavLM, HuBERT) for the downstream task of Speech Emotion Recognition (SER). The proposed methodology significantly reduces the dependency of model predictions on linguistic content, while leading to competitive performance without requiring costly Encoder re-training. The proposed paradigm is evaluated via Accuracy, Positive Pointwise Mutual Information, and visualization of the learned attention weights. This methodology generalizes well to a multi-language SER setting in addition to single-language SER, suggesting existing cultural commonalities in the paralinguistic domain between different languages. Experimental results demonstrate this ability by testing our model on unseen languages in a zero-shot fashion, suggesting our proposed method is inclusive in the context of speech and language, thus, making it applicable to a wide audience of speakers",
    "checked": true,
    "id": "469416e547656f2fb1e1218f1ccbaef631ec83c5",
    "semantic_title": "towards paralinguistic-only speech representations for end-to-end speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23y_interspeech.html": {
    "title": "SOT: Self-supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In cross-domain speech emotion recognition (SER), reducing the global probability distribution distance (GPDD) between different domains plays a crucial role in unsupervised domain adaptation (UDA), which can be naturally measured by optimal transport (OT). However, owing to the large intra-variations of emotion categories, samples distributed in overlap may induce negative transports. Moreover, OT only considers the GPDD and therefore cannot efficiently transport hard-discriminative samples without utilizing the local structures from intra-class distributions. We propose a self-supervised learning (SSL)-assisted optimal transport (SOT) algorithm for cross-domain SER. First, we regularized OT's transport coupling to mitigate negative transports; then, we designed an SSL module to emphasize local intra-class structure to assist OT in capturing those nontransferable acknowledge. Cross-domain SER experimental results showed that SOT dramatically outperformed state-of-the-art UDAs",
    "checked": true,
    "id": "8de6d819fda2a243da79e20eba9b266d44b7633f",
    "semantic_title": "sot: self-supervised learning-assisted optimal transport for unsupervised adaptive speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bansal23_interspeech.html": {
    "title": "On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition",
    "volume": "main",
    "abstract": "New-age conversational agent systems perform both speech-emotion recognition (SER) and automatic speech recognition (ASR) using two separate and often independent approaches for real-world application in noisy environments. In this paper, we investigate a joint ASR-SER multitask learning approach in a low-resource setting and show that improvements are observed not only in SER but also in ASR. We also investigate the robustness of such jointly trained models to the presence of background noise, babble, and music. Experimental results on the IEMOCAP dataset show that joint learning can improve ASR word error rate (WER) and SER classification accuracy by 10.7% and 2.3% respectively in clean scenarios. In noisy scenarios, results on data augmented with MUSAN show that the joint approach outperforms the independent ASR and SER approaches across many noisy conditions. Overall, the joint ASR-SER approach yielded more noise-resistant models than the independent ASR and SER approaches",
    "checked": true,
    "id": "60bd26b2b926bcc9817506dff9895540a7b0a856",
    "semantic_title": "on the efficacy and noise-robustness of jointly learned speech emotion and automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23b_interspeech.html": {
    "title": "Speaking State Decoder with Transition Detection for Next Speaker Prediction",
    "volume": "main",
    "abstract": "Next speaker prediction and turn change prediction are two important tasks in group interaction and human-agent interaction. In order to carry out a fluent conversation, we need to identify who is currently speaking, who is the next speaker and when the next speaker starts to speak. These questions are computationally designed as the task of next speaker prediction. Behaviors such as gaze direction, speaking prosody or gestures have been modeled to perform this task. In this work, we propose a decoder-based speaking state decoder (SSD) for next speaker prediction, which jointly considers current behavior features, past history of talking and speaking state transition detection model. Our decoder approach achieves next speaker prediction with UAR of 78.11%, which is 3.41% improvement over the champion model in MultiMediate challenge 2021",
    "checked": true,
    "id": "12425c65dcf4bb80e122f73780a3935ce51d5692",
    "semantic_title": "speaking state decoder with transition detection for next speaker prediction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kitagishi23_interspeech.html": {
    "title": "What are differences? Comparing DNN and Human by Their Performance and Characteristics in Speaker Age Estimation",
    "volume": "main",
    "abstract": "We compare speaker age estimation results obtained by human listeners and a latest deep neural network (DNN) model to reveal differences in their estimation characteristics. A DNN model can achieve high speaker age estimation performance and is expected to be utilized in practical applications. Only a few studies compared speaker-age estimation performance between human listeners and machine learning models. However, the differences in their estimation characteristics have yet to be revealed. Our experimental results reveal that the DNN model performs comparable or superior to the listeners but is more sensitive to elderly speech, acoustic characteristics, and lengths of speech samples than the listeners. The results also reveal that the speakers' gender and some specific acoustic features negatively affect the listeners' estimation performance",
    "checked": true,
    "id": "d311ae30da7189f70c32fae911db719e99ab2e16",
    "semantic_title": "what are differences? comparing dnn and human by their performance and characteristics in speaker age estimation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arts23_interspeech.html": {
    "title": "Effects of perceived gender on the perceived social function of laughter",
    "volume": "main",
    "abstract": "Previously, the sex of the speaker has been found to play an important role in how we perceive human and artificial voices. It was found, for example, that sex mediates in how we perceive the social function of laughter. We, however, are interested in how socially-formed concepts of gender influence how laughter is perceived. To investigate this, we carried out a within-subjects study of listeners who judged social functions of the same laugh stimulus twice, which was framed as produced by either a man or woman. Mixed-effects ordinal regression modelling showed no statistically significant relations between the perceived gender of a laugh and its perceived social functions",
    "checked": true,
    "id": "e8979be87ba6fabd07afaa9a5147492a8900f053",
    "semantic_title": "effects of perceived gender on the perceived social function of laughter",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/purohit23_interspeech.html": {
    "title": "Implicit phonetic information modeling for speech emotion recognition",
    "volume": "main",
    "abstract": "This study investigates the efficacy of utilizing embedding spaces to model phonetic information in emotion utterances for speech emotion recognition. Our approach involves implicit modeling of phone information by deriving phone-based embeddings from networks specifically trained for phone recognition and pre-trained models fine-tuned for phone/character recognition. The results from evaluating our approach on three speech emotion databases, using both intra-corpus and inter-corpus evaluation methods demonstrate the competitive performance of implicit modeling of phonetic information compared to knowledge-based handcrafted features",
    "checked": true,
    "id": "1ec3a6312aefc68da51618a04c648c64b219a16a",
    "semantic_title": "implicit phonetic information modeling for speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/leem23_interspeech.html": {
    "title": "Computation and Memory Efficient Noise Adaptation of Wav2Vec2.0 for Noisy Speech Emotion Recognition with Skip Connection Adapters",
    "volume": "main",
    "abstract": "An appealing approach for speech emotion recognition (SER) is to pre-train a large speech representation model such as Wav2Vec2.0 or HuBERT. However, this large model should be adapted to different environments when deployed on real-world applications. This approach demands additional training time and stored parameters for each target environment. This paper proposes a computation and memory-efficient adaptation method. The approach trains skip connection adapters that generate environmental representations from the convolutional encoder, and denoise the self-supervised speech representations. Our experiments with the clean and contaminated version of the MSP-Podcast corpus show that our adapter-based approach not only improves the performance of the original fine-tuned SER model, but also reduces the computation and memory requirements. For each environment, the approach requires 59.16% decreased adaptation time and only 0.98% of the parameters of the transformer encoder",
    "checked": true,
    "id": "bcb740386b0b2d3756ec5d948c2f91ae966eff17",
    "semantic_title": "computation and memory efficient noise adaptation of wav2vec2.0 for noisy speech emotion recognition with skip connection adapters",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23b_interspeech.html": {
    "title": "Multi-Level Knowledge Distillation for Speech Emotion Recognition in Noisy Conditions",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) performance deteriorates significantly in the presence of noise, making it challenging to achieve competitive performance in noisy conditions. To this end, we propose a multi-level knowledge distillation (MLKD) method, which aims to transfer the knowledge from a teacher model trained on clean speech to a simpler student model trained on noisy speech. Specifically, we use clean speech features extracted by the wav2vec-2.0 as the learning goal and train the distil wav2vec-2.0 to approximate the feature extraction ability of the original wav2vec-2.0 under noisy conditions. Furthermore, we leverage the multi-level knowledge of the original wav2vec-2.0 to supervise the single-level output of the distil wav2vec-2.0. We evaluate the effectiveness of our proposed method by conducting extensive experiments using five types of noise-contaminated speech on the IEMOCAP dataset, which show promising results compared to state-of-the-art models",
    "checked": true,
    "id": "bfd3a01166217bb5776edfe536c4320699171d19",
    "semantic_title": "multi-level knowledge distillation for speech emotion recognition in noisy conditions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naini23_interspeech.html": {
    "title": "Preference Learning Labels by Anchoring on Consecutive Annotations",
    "volume": "main",
    "abstract": "An important task in human-computer interaction is to rank speech samples according to their expressive content. A preference learning framework is appropriate for obtaining an emotional rank for a set of speech samples. However, obtaining reliable labels for training a preference learning framework is a challenging task. Most existing databases provide sentence-level absolute attribute scores annotated by multiple raters, which have to be transformed to obtain preference labels. Previous studies have shown that evaluators anchor their absolute assessments on previously annotated samples. Hence, this study proposes a novel formulation for obtaining preference learning labels by only considering annotation trends assigned by a rater to consecutive samples within an evaluation session. The experiments show that the use of the proposed anchor-based ordinal labels leads to significantly better performance than models trained using existing alternative labels",
    "checked": true,
    "id": "52ceaffd6a8460f0a20714a516f2a62f79a74d17",
    "semantic_title": "preference learning labels by anchoring on consecutive annotations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chetiaphukan23_interspeech.html": {
    "title": "Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is a field that has drawn a lot of attention due to its applications in diverse fields. A cur- rent trend in methods used for SER is to leverage embeddings from pre-trained models (PTMs) as input features to down- stream models. However, the use of embeddings from speaker recognition PTMs hasn't garnered much focus in comparison to other PTM embeddings. To fill this gap and in order to understand the efficacy of speaker recognition PTM embed- dings, we perform a comparative analysis of five PTM embed- dings. Among all, x-vector embeddings performed the best possibly due to its training for speaker recognition leading to capturing various components of speech such as tone, pitch, etc. Our modeling approach which utilizes x-vector embed- dings and mel-frequency cepstral coefficients (MFCC) as input features is the most lightweight approach while achieving com- parable accuracy to previous state-of-the-art (SOTA) methods in the CREMA-D benchmark",
    "checked": true,
    "id": "4cbf260c8dd0a401b1577ce22d0d9a5a92f0e50d",
    "semantic_title": "transforming the embeddings: a lightweight technique for speech emotion recognition tasks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23_interspeech.html": {
    "title": "Learning Local to Global Feature Aggregation for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Transformer has emerged in speech emotion recognition (SER) at present. However, its equal patch division not only damages frequency information but also ignores local emotion correlations across frames, which are key cues to represent emotion. To handle the issue, we propose a Local to Global Feature Aggregation learning (LGFA) for SER, which can aggregate long-term emotion correlations at different scales both inside frames and segments with entire frequency information to enhance the emotion discrimination of utterance-level speech features. For this purpose, we nest a Frame Transformer inside a Segment Transformer. Firstly, Frame Transformer is designed to excavate local emotion correlations between frames for frame embeddings. Then, the frame embeddings and their corresponding segment features are aggregated as different-level complements to be fed into Segment Transformer for learning utterance-level global emotion features. Experimental results show that the performance of LGFA is superior to the state-of-the-art methods",
    "checked": true,
    "id": "3a6550e3aee316f68156b1e7d5e4c6caf30b600e",
    "semantic_title": "learning local to global feature aggregation for speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23q_interspeech.html": {
    "title": "Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) is a challenging task due to limited data and blurred boundaries of certain emotions. In this paper, we present a comprehensive approach to improve the SER performance throughout the model lifecycle, including pre-training, fine-tuning, and inference stages. To address the data scarcity issue, we utilize a pre-trained model, wav2vec2.0. During fine-tuning, we propose a novel loss function that combines cross-entropy loss with supervised contrastive learning loss to improve the model's discriminative ability. This approach increases the inter-class distances and decreases the intra-class distances, mitigating the issue of blurred boundaries. Finally, to leverage the improved distances, we propose an interpolation method at the inference stage that combines the model prediction with the output from a k-nearest neighbors model. Our experiments on IEMOCAP demonstrate that our proposed methods outperform current state-of-the-art results",
    "checked": true,
    "id": "9a4aa8375fb30114b4d0b358f0c72e306b94f2cb",
    "semantic_title": "supervised contrastive learning with nearest neighbor search for speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pham23b_interspeech.html": {
    "title": "Vietnam-Celeb: a large-scale dataset for Vietnamese speaker recognition",
    "volume": "main",
    "abstract": "The success of speaker recognition systems heavily depends on large training datasets collected under real-world conditions. While common languages like English or Chinese have vastly available datasets, low-resource ones like Vietnamese remain limited. This paper presents a large-scale spontaneous dataset gathered under noisy environments, with over 87,000 utterances from 1,000 Vietnamese speakers of many professions, covering 3 main Vietnamese dialects. To build the dataset, we propose a sophisticated construction pipeline that can also be applied to other languages, with efficient visual-aided processing techniques to boost data precision. With the state-of-the-art x-vector model, training with the proposed dataset shows an average absolute and relative EER improvement of 5.48% and 41.61% when compared to the model trained on VLSP 2021, a publicly available Vietnamese speaker dataset",
    "checked": true,
    "id": "8b673225d277d3928dab3c1725210040c6ea34df",
    "semantic_title": "vietnam-celeb: a large-scale dataset for vietnamese speaker recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23v_interspeech.html": {
    "title": "What Can an Accent Identifier Learn? Probing Phonetic and Prosodic Information in a Wav2vec2-based Accent Identification Model",
    "volume": "main",
    "abstract": "This study is focused on understanding and quantifying the change in phoneme and prosody information encoded in the Self-Supervised Learning (SSL) model, brought by an accent identification (AID) fine-tuning task. This problem is addressed based on model probing. Specifically, we conduct a systematic layer-wise analysis of the representations of the Transformer layers on a phoneme correlation task, and a novel word-level prosody prediction task. We compare the probing performance of the pre-trained and fine-tuned SSL models. Results show that the AID fine-tuning task steers the top 2 layers to learn richer phoneme and prosody representation. These changes share some similarities with the effects of fine-tuning with an Automatic Speech Recognition task. In addition, we observe strong accent-specific phoneme representations in layer 9. To sum up, this study provides insights into the understanding of SSL features and their interactions with fine-tuning tasks",
    "checked": true,
    "id": "f098620d3e19388937839afd9c9c856a821ca3ac",
    "semantic_title": "what can an accent identifier learn? probing phonetic and prosodic information in a wav2vec2-based accent identification model",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23c_interspeech.html": {
    "title": "The 2022 NIST Language Recognition Evaluation",
    "volume": "main",
    "abstract": "In 2022, the U.S. National Institute of Standards and Technology (NIST) conducted the latest Language Recognition Evaluation (LRE) in an ongoing series administered by NIST since 1996 to foster research in language recognition and to measure state-of-the-art technology. Similar to previous LREs, LRE22 focused on conversational telephone speech (CTS) and broadcast narrowband speech (BNBS) data. LRE22 also introduced new evaluation features, such as an emphasis on African languages, including low resource languages, and a test set consisting of segments containing between 3s and 35s of speech randomly sampled and extracted from longer recordings. A total of 21 research organizations, forming 16 teams, participated in this 3-month long evaluation and made a total of 65 valid system submissions to be evaluated. This paper presents an overview of LRE22 and an analysis of system performance over different evaluation conditions. The evaluation results suggest that Oromo and Tigrinya are easier to detect while Xhosa and Zulu are more challenging. A greater confusability is seen for some language pairs. When speech duration increased, system performance significantly increased up to a certain duration, and then a diminishing return on system performance is observed afterward",
    "checked": true,
    "id": "ae444d811900fbd3d211c94683c7660eab521870",
    "semantic_title": "the 2022 nist language recognition evaluation",
    "citation_count": 118
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarni23_interspeech.html": {
    "title": "Description and analysis of the KPT system for NIST Language Recognition Evaluation 2022",
    "volume": "main",
    "abstract": "This paper presents an analysis of the KPT system for the 2022 NIST Language Recognition Evaluation. The KPT submission focuses on the fixed training condition where only specific speech data can be used to develop all the modules and auxiliary systems used to build the language recognizer. Our solution consists of several sub-systems based on different neural network front-ends and a common back-end for classification and fusion. The goal of each front-end is to extract language-related embeddings. Gaussian linear models are used to classify the embeddings of each front-end, followed by multi-class logistic regression to calibrate and fuse the different sub-systems. Experimental results from the NIST LRE 2022 evaluation task show that our approach achieves competitive performance",
    "checked": true,
    "id": "0814d74df7c3d486bcf3b442816251ba5e752147",
    "semantic_title": "description and analysis of the kpt system for nist language recognition evaluation 2022",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yip23_interspeech.html": {
    "title": "ACA-Net: Towards Lightweight Speaker Verification using Asymmetric Cross Attention",
    "volume": "main",
    "abstract": "In this paper, we propose ACA-Net, a lightweight, global context-aware speaker embedding extractor for Speaker Verification (SV) that improves upon existing work by using Asymmetric Cross Attention (ACA) to replace temporal pooling. ACA is able to distill large, variable-length sequences into small, fixed-sized latents by attending a small query to large key and value matrices. In ACA-Net, we build a Multi-Layer Aggregation (MLA) block using ACA to generate fixed-sized identity vectors from variable-length inputs. Through global attention, ACA-Net acts as an efficient global feature extractor that adapts to temporal variability unlike existing SV models that apply a fixed function for pooling over the temporal dimension which may obscure information about the signal's nonstationary temporal variability. Our experiments on the WSJ0- 1talker show ACA-Net outperforms a strong baseline by 5% relative improvement in EER using only 1/5 of the parameters",
    "checked": true,
    "id": "c83a635dd31bdf7332fddf1dd0767863b5b13ae6",
    "semantic_title": "aca-net: towards lightweight speaker verification using asymmetric cross attention",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yao23_interspeech.html": {
    "title": "Branch-ECAPA-TDNN: A Parallel Branch Architecture to Capture Local and Global Features for Speaker Verification",
    "volume": "main",
    "abstract": "Currently, ECAPA-TDNN is one of the state-of-the-art deep models for automatic speaker verification (ASV). However, it focuses too much on local feature extraction with fixed local ranges, without paying much attention to global feature extraction. To deal with this issue, in this paper, we propose Branch-ECAPA-TDNN, which uses two parallel branches to extract features with various ranges and abstract levels. One branch employs multi-head self-attention to capture long-range dependencies, while the other branch utilizes an SE-Res2Block module to model local multi-scale characteristics. To improve the feature fusion, we further apply different merging methods to aggregate features from both branches. Experimental results demonstrate that the proposed Branch-ECAPA-TDNN achieves a relative EER reduction of 24.10% and 7.92% over ECAPA-TDNN on the VoxCeleb and CN-Celeb datasets, respectively",
    "checked": true,
    "id": "2b8f227dc8187616167bd4267d2e1fada7051790",
    "semantic_title": "branch-ecapa-tdnn: a parallel branch architecture to capture local and global features for speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23d_interspeech.html": {
    "title": "Speaker Verification Across Ages: Investigating Deep Speaker Embedding Sensitivity to Age Mismatch in Enrollment and Test Speech",
    "volume": "main",
    "abstract": "In this paper, we study the impact of the ageing on modern deep speaker embedding based automatic speaker verification (ASV) systems. We have selected two different datasets to examine ageing on the state-of-the-art ECAPA-TDNN system. The first dataset, used for addressing short-term aging (up to 10 years time difference between enrollment and test) under un-controlled conditions, is VoxCeleb. The second dataset, used for addressing long-term aging effect (up to 40 years difference) of Finnish speakers under a more controlled setup, is Longitudinal Corpus of Finnish Spoken in Helsinki (LCFSH). Our study provides new insights into the impact of speaker ageing on modern ASV systems. Specifically, we establish a quantitative measure between ageing and ASV scores. Further, our research indicates that ageing affects female English speakers to a greater degree than male English speakers, while in the case of Finnish, it has a greater impact on male speakers than female speakers",
    "checked": true,
    "id": "852eaf5fc69a0ec67211824ec94763f54c40a59e",
    "semantic_title": "speaker verification across ages: investigating deep speaker embedding sensitivity to age mismatch in enrollment and test speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dey23_interspeech.html": {
    "title": "Wavelet Scattering Transform for Improving Generalization in Low-Resourced Spoken Language Identification",
    "volume": "main",
    "abstract": "Commonly used features in spoken language identification (LID), such as mel-spectrogram or MFCC, lose high-frequency information due to windowing. The loss further increases for longer temporal contexts. To improve generalization of the low-resourced LID systems, we investigate an alternate feature representation, wavelet scattering transform (WST), that compensates for the shortcomings. To our knowledge, WST is not explored earlier in LID tasks. We first optimize WST features for multiple South Asian LID corpora. We show that LID requires low octave resolution and frequency-scattering is not useful. Further, cross-corpora evaluations show that the optimal WST hyper-parameters depend on both train and test corpora. Hence, we develop fused ECAPA-TDNN based LID systems with different sets of WST hyper-parameters to improve generalization for unknown data. Compared to MFCC, EER is reduced upto 14.05% and 6.40% for same-corpora and blind VoxLingua107 evaluations, respectively",
    "checked": true,
    "id": "ae668f5d1570bc46d1cb431d3c9c44b5704f2254",
    "semantic_title": "wavelet scattering transform for improving generalization in low-resourced spoken language identification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/radhakrishnan23_interspeech.html": {
    "title": "A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model",
    "volume": "main",
    "abstract": "In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open-source code at https://github.com/Srijith-rkr/KAUST-Whisper-Adapter",
    "checked": true,
    "id": "395ff451c1314c42c0b72de15127e8f1b1e7d20f",
    "semantic_title": "a parameter-efficient learning approach to arabic dialect identification with pre-trained general-purpose speech model",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tamayoflorez23_interspeech.html": {
    "title": "HABLA: A Dataset of Latin American Spanish Accents for Voice Anti-spoofing",
    "volume": "main",
    "abstract": "Research on improving automatic speaker verification systems to detect speech spoofing has focused mainly on English, with little attention given to other languages creating a significant gap in language coverage. This paper introduces HABLA, the first voice anti-spoofing dataset in the Spanish language including Argentinian, Colombian, Peruvian, Venezuelan, and Chilean accents. The dataset provided by HABLA comprises over 22,000 authentic speech samples from male and female speakers hailing from five distinct Latin American nations as well as 58,000 spoof samples that were generated through the use of six different speech synthesis strategies, including recent voice conversion and text-to-speech algorithms. Finally, initial findings on the efficacy of pre-existing Antispoofing Systems models are presented along with concerns regarding their performance in languages other than English",
    "checked": true,
    "id": "3052be3179f7f6660a397f2a81a8fbc4392b2a4a",
    "semantic_title": "habla: a dataset of latin american spanish accents for voice anti-spoofing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23aa_interspeech.html": {
    "title": "Self-supervised Learning Representation based Accent Recognition with Persistent Accent Memory",
    "volume": "main",
    "abstract": "Accent recognition (AR) is challenging due to the lack of training data as well as the accents are entangled with speakers and regional characteristics. This paper aims to improve AR performance from two perspectives. First, to alleviate the data insufficiency problem, we employ the self-supervised learning representations (SSLRs) extracted from a pre-trained model to build the AR models. With the help of SSLRs, it gains significant performance improvement compared with the traditional acoustic features. Secondly, we proposed a persistent accent memory (PAM) as contextual knowledge to bias the AR model. The accent embeddings that are extracted from all training data by the encoder of AR models are clustered to form an accent codebook, i.e. PAM. In addition, we propose diverse attention mechanisms to investigate the optimal utilization of PAM. We observe that the best performance is obtained by selecting the most relevant accent embeddings",
    "checked": true,
    "id": "4d5ab653df0e74e0460d2413588d564471eb942a",
    "semantic_title": "self-supervised learning representation based accent recognition with persistent accent memory",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23g_interspeech.html": {
    "title": "Extremely Low Bit Quantization for Mobile Speaker Verification Systems Under 1MB Memory",
    "volume": "main",
    "abstract": "How to develop lightweight systems customized for mobile devices is an urgent and intriguing topic for speaker verification. In this paper, we investigate extremely low bit quantization for small-footprint speaker verification. Specifically, two different binary quantization schemes are proposed, namely static and adaptive quantizer. By applying them to the pre-trained full-precision ResNet, we successfully obtain binarized variants named as b-vector with a model size of under 1MB memory. Experiments on Voxceleb dataset illustrate that compared with the previous best small-footprint system, our best b-vector system achieves 38%, 36% and 30% relative improvements on Vox1-O, E and H respectively, while maintaining almost identical model size. In addition, the analysis of the binarized weight histograms reveals that adaptive quantization scheme, when compared to the static method, can better match the real-valued distribution, and hence presents more effective representation ability",
    "checked": true,
    "id": "42a9a61094b9b30c97c7fc0bc614c0d29a16a600",
    "semantic_title": "extremely low bit quantization for mobile speaker verification systems under 1mb memory",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/das23_interspeech.html": {
    "title": "Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance",
    "volume": "main",
    "abstract": "Dialect classification is used in a variety of applications, such as machine translation and speech recognition, to improve the overall performance of the system. In a real-world scenario, a deployed dialect classification model can encounter anomalous inputs that differ from the training data distribution, also called out-of-distribution (OOD) samples. Those OOD samples can lead to unexpected outputs, as dialects of those samples are unseen during model training. Out-of-distribution detection is a new research area that has received little attention in the context of dialect classification. Towards this, we proposed a simple yet effective unsupervised Mahalanobis distance feature-based method to detect out-of-distribution samples. We utilize the latent embeddings from all intermediate layers of a wav2vec 2.0 transformer-based dialect classifier model for multi-task learning. Our proposed approach outperforms other state-of-the-art OOD detection methods significantly",
    "checked": true,
    "id": "779ee9ab6b25d514b4be761af350274d3391e999",
    "semantic_title": "unsupervised out-of-distribution dialect detection with mahalanobis distance",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bredin23_interspeech.html": {
    "title": "pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe",
    "volume": "main",
    "abstract": "pyannote.audio is an open-source toolkit written in Python for speaker diarization. Version 2.1 introduces a major overhaul of pyannote.audio default speaker diarization pipeline, made of three main stages: speaker segmentation applied to a short sliding window, neural speaker embedding of each (local) speakers, and (global) agglomerative clustering. One of the main objectives of the toolkit is to democratize speaker diarization. Therefore, on top of a pretrained speaker diarization pipeline that gives good results out of the box, we also provide a recipe that practitioners can follow to improve its performance on their own (manually annotated) dataset. It has been used for various challenges and reached 1st place at Ego4D 2022, 1st place at Albayzin 2022, and 6th place at VoxSRC 2022",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23t_interspeech.html": {
    "title": "Model Compression for DNN-based Speaker Verification Using Weight Quantization",
    "volume": "main",
    "abstract": "DNN-based speaker verification (SV) models demonstrate significant performance at relatively high computation costs. Model compression can be applied to reduce the model size for lower resource consumption. The present study exploits weight quantization to compress two widely-used SV models, namely ECAPA-TDNN and ResNet. Experimental results on VoxCeleb show that weight quantization is effective for compressing SV models. The model size can be reduced multiple times without noticeable degradation in performance. Compression of ResNet shows more robust results than ECAPA-TDNN with lower-bitwidth quantization. Analysis of the layer weights suggests that the smooth weight distribution of ResNet may be related to its better robustness. The generalization ability of the quantized model is validated via a language-mismatched SV task. Furthermore, analysis by information probing reveals that the quantized models can retain most of the speaker-relevant knowledge learned by the original models",
    "checked": true,
    "id": "71c691cfc265b86cf45ea6dfb98f8d8b2c410150",
    "semantic_title": "model compression for dnn-based speaker verification using weight quantization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vachhani23_interspeech.html": {
    "title": "Multi-resolution Approach to Identification of Spoken Languages and To Improve Overall Language Diarization System Using Whisper Model",
    "volume": "main",
    "abstract": "This research paper investigates the effectiveness of the Whisper decoder for Language Identification (LI) and Language Diarization (LD) tasks. An audio accent detection system was used as an attention mechanism to narrow down the Whisper LI output classes. The LI system was tested on different audio resolutions ranging from 1.0 to 11.0 seconds, and the segments obtained were combined to generate RTTM per audio resolution. Lastly, we ensemble different multi-resolution diarization systems using DOVER-Lap algorithm. This work was part of DISPLACE challenge organized in INTERSPEECH 2023 and hence the challenge dataset was utilized for all the experiments. It shows that 5-second of audio resolution (i.e.,S-1) yield optimum result of 38.12% and 42.45% DER on development and evaluation data respectively. Furthermore, combining multi-resolution diarization systems (i.e.,S-2) produced an absolute improvement of 3.22% over S-1 and 11.66% over the challenge baseline, with a total DER of 34.9% on the Development set",
    "checked": true,
    "id": "825635531b01812cca1364d5e953799c776819d5",
    "semantic_title": "multi-resolution approach to identification of spoken languages and to improve overall language diarization system using whisper model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23_interspeech.html": {
    "title": "Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms",
    "volume": "main",
    "abstract": "The ability of countermeasure models to generalize from seen speech synthesis methods to unseen ones has been investigated in the ASVspoof challenge. However, a new mismatch scenario in which fake audio may be generated from real audio with unseen genres has not been studied thoroughly. To this end, we first use five different vocoders to create a new dataset called CN-Spoof based on the CN-Celeb1&2 datasets. Then, we design two auxiliary objectives for regularization via meta-optimization and a genre alignment module, respectively, and combine them with the main anti-spoofing objective using learnable weights for multiple loss terms. The results on our cross-genre evaluation dataset for anti-spoofing show that the proposed method significantly improved the generalization ability of the countermeasures compared with the baseline system in the genre mismatch scenario",
    "checked": true,
    "id": "a24f7d8d68e2142953da0a627cd09e94e253dde2",
    "semantic_title": "improving generalization ability of countermeasures for new mismatch scenario by combining multiple advanced regularization terms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/song23b_interspeech.html": {
    "title": "Dynamic Fully-Connected Layer for Large-Scale Speaker Verification",
    "volume": "main",
    "abstract": "Recently, the mainstream x-vector for speaker verification usually adopts a one-hot encoded fully-connected (FC) layer for classification at the training stage. Suppose a large-scale dataset (e.g., one million speakers) is prepared to optimize the network. The unbearable computation cost and memory requirement are mainly from the FC layer. We propose a dynamic fully-connected (Dynamic FC) layer for speaker verification to achieve a tradeoff between hardware resources and system performance. The proposed Dynamic FC uses a dynamic class queue (DCQ) to store a subset of speaker identity centers and uses an identity-based data loading mechanism to realize memory and time savings. The virtue of the proposed method is that the required memory only depends on the size of the DCQ and does not increase with the number of speakers in the training dataset. The proposed method on the VoxCeleb dataset achieves an EER of 2.345% and a minDCF of 0.261 at a low memory and computation cost",
    "checked": true,
    "id": "215d6e789d2a94fa7d298a9471e4cd8104fe4350",
    "semantic_title": "dynamic fully-connected layer for large-scale speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schroter23b_interspeech.html": {
    "title": "DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "Multi-frame algorithms for single-channel speech enhancement are able to take advantage from short-time correlations within the speech signal. Deep Filtering (DF) was proposed to directly estimate a complex filter in frequency domain to take advantage of these correlations. In this work, we present a real-time speech enhancement demo using DeepFilterNet. DeepFilterNet's efficiency is enabled by exploiting domain knowledge of speech production and psychoacoustic perception. Our model is able to match state-of-the-art speech enhancement benchmarks while achieving a real-time-factor of 0.19 on a single threaded notebook CPU. The framework as well as pretrained weights have been published under an open source license",
    "checked": true,
    "id": "dcb4c9e7c47fec600c3208adaf20e9254f698ea9",
    "semantic_title": "deepfilternet: perceptually motivated real-time speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burkhardt23_interspeech.html": {
    "title": "Nkululeko: Machine Learning Experiments on Speaker Characteristics Without Programming",
    "volume": "main",
    "abstract": "We would like to present Nkululeko, a template based system that lets users perform machine learning experiments in the speaker characteristics domain. It is mainly targeted on users not being familiar with machine learning, or computer programming at all, to being used as a teaching tool or a simple entry level tool to the field of artificial intelligence",
    "checked": false,
    "id": "47ce754ef77553bc77090ffc6af0699bb27ed3d1",
    "semantic_title": "nkululeko: a tool for rapid speaker characteristics detection",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lemaguer23_interspeech.html": {
    "title": "Sp1NY: A Quick and Flexible Speech Visualisation Tool in Python",
    "volume": "main",
    "abstract": "In this submission, we describe Sp1NY, a Python toolkit to visualise and annotate speech. Inspired by Praat and music notation software, we designed Sp1NY to be accessible and flexible. By introducing a control panel, Sp1NY provides a quick way for the user to interact with it. By focusing Sp1NY only on visualisation and annotation and, by reducing the core of the software to a minimum, we ensure that the software will remain stable. Finally, Sp1NY integrates a plugin mechanism which allows researchers to adapt the tool to their needs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/corkey23_interspeech.html": {
    "title": "Intonation Control for Neural Text-to-Speech Synthesis with Polynomial Models of F0",
    "volume": "main",
    "abstract": "We present a novel, user-friendly approach for controlling patterns of intonation (a fundamental aspect of prosody) within a neural TTS system. This involves concisely representing F0 contours with the coefficients of their Legendre polynomial series expansion, and implementing a model (based on FastPitch) which is conditioned on these sets of coefficients during training. At inference time the model will explicitly predict a coefficient set, or a user (eg. human-in-the-loop) can provide a target coefficient set which audibly alters the intonation of the output speech, based on just a few values. This is particularly effective for intonation transfer: where these coefficient targets are extracted from a ground truth recording, making the synthesised utterance more closely reflect the intonation of the real speaker",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szekely23b_interspeech.html": {
    "title": "So-to-Speak: An Exploratory Platform for Investigating the Interplay between Style and Prosody in TTS",
    "volume": "main",
    "abstract": "In recent years, numerous speech synthesis systems have been proposed that feature multi-dimensional controllability, generating a level of variability that surpasses traditional TTS systems by orders of magnitude. However, it remains challenging for developers to comprehend and demonstrate the potential of these advanced systems. We introduce So-to-Speak, a customisable interface tailored for showcasing the capabilities of different controllable TTS systems. The interface allows for the generation, synthesis, and playback of hundreds of samples simultaneously, displayed on an interactive grid, with variation both low level prosodic features and high level style controls. To offer insights into speech quality, automatic estimates of MOS scores are presented for each sample. So-to-Speak facilitates the audiovisual exploration of the interaction between various speech features, which can be useful in a range of applications in speech technology",
    "checked": true,
    "id": "0c0b4a31e280b8fe6048d179427b50862cd09fba",
    "semantic_title": "so-to-speak: an exploratory platform for investigating the interplay between style and prosody in tts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arai23_interspeech.html": {
    "title": "Comparing /b/ and /d/ with a Single Physical Model of the Human Vocal Tract to Visualize Droplets Produced while Speaking",
    "volume": "main",
    "abstract": "The BMW-RL model, a physical model of the human vocal tract that produces /b/, /m/, /w/, /r/, and /l/, has been utilized to investigate not only each single sound but also consonant clusters such as /br/. This model started gaining attention in 2021 in light of the COVID-19 pandemic, as it can demonstrate how droplets are expelled from the lips when producing the /b/ sound by applying a laser sheet. In this study, we redesigned the model to produce both /b/ and /d/, since both are voiced plosives and the only difference is the place of articulation. With the original BMW-RL model, the first half of the tongue rotates and produces /r/ and /l/, while in the newly proposed model, the width of the tongue is wide enough to make a complete closure at the alveolar position for producing /d/. We tested how the different place of articulation affects the ways of expelling droplets by using the single model producing /b/ and /d/ and found that more droplets were expelled with /b/ than /d/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ekstedt23b_interspeech.html": {
    "title": "Show & Tell: Voice Activity Projection and Turn-taking",
    "volume": "main",
    "abstract": "We present Voice Activity Projection (VAP), a model trained on spontaneous spoken dialog with the objective to incrementally predict future voice activity. Similar to a language model, it is trained through self-supervised learning and outputs a probability distribution over discrete states that corresponds to the joint future voice activity of the dialog interlocutors. The model is well-defined over overlapping speech regions, resilient towards microphone \"bleed-over\" and considers the speech of both speakers (e.g., a user and an agent) to provide the most likely next speaker. VAP is a general turn-taking model which can serve as the base for turn-taking decisions in spoken dialog systems, an automatic tool useful for linguistics and conversational analysis, an automatic evaluation metric for conversational text-to-speech models, and possibly many other tasks related to spoken dialog interaction",
    "checked": false,
    "id": "fdd6785da1fadfb44513aecce40ef41b0d34610c",
    "semantic_title": "voice activity projection: self-supervised learning of turn-taking events",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cordourier23_interspeech.html": {
    "title": "Real Time Detection of Soft Voice for Speech Enhancement",
    "volume": "main",
    "abstract": "People in remote meetings in open spaces might choose to speak with a restrained voice due to concerns around privacy or disturbing others. Research shows that persons prefer to use soft voice (voice with lower amplitude and pitch, but with harmonic tones in its spectrum) over whispered voice (voice with the lowest amplitude, and no harmonics at all) to avoid being overheard during such calls. We present a lightweight classifier based in a simple feed-forward neural network, which uses normalized Log-Mel spectrum of voice captured by a headset as input, and can detect if the person is using soft voice. This allows to enhance soft voice with more precision and responsiveness than regular amplitude compensation (\"auto-gain\") systems. In this show and tell, we present a real-time demo of the voice classifier. Viewers will see our algorithm detect in real-time soft voice vs other voice types, in a regular PC, with voice captured with a headset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tanna23_interspeech.html": {
    "title": "Data Augmentation for Diverse Voice Conversion in Noisy Environments",
    "volume": "main",
    "abstract": "Voice conversion (VC) models have demonstrated impressive few-shot conversion quality on the clean, native speech populations they're trained on. However, when source or target speech accents, background noise conditions, or microphone characteristics differ from training, quality voice conversion is not guaranteed. These problems are often left unexamined in VC research, giving rise to frustration in users trying to use pretrained VC models on their own data. We are interested in accent-preserving voice conversion for name pronunciation from self-recorded examples, a domain in which all three of the aforementioned conditions are present, and posit that demonstrating higher performance in this domain correlates with creating VC models that are more usable by otherwise frustrated users. We demonstrate that existing SOTA encoder-decoder VC models can be made robust to these varations and endowed with natural denoising capabilities using more diverse data and simple data augmentation techniques in pretraining",
    "checked": true,
    "id": "9848df593e72aa4374238575b2fdae86d4d1bb7e",
    "semantic_title": "data augmentation for diverse voice conversion in noisy environments",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gogate23_interspeech.html": {
    "title": "Application for Real-time Audio-Visual Speech Enhancement",
    "volume": "main",
    "abstract": "This short paper demonstrates a first of its kind audio-visual (AV) speech enhancement (SE) desktop application that isolates, in real-time, the voice of a target speaker from noisy audio input. The deep neural network model integrated in this application exploits the AV nature of speech from the target speaker to suppress all speech and non-speech background sounds. In the context of a growing need for video conferencing solutions, AV SE enables the practical deployment such technology in challenging acoustic environments with multiple competing background noise sources. In these scenarios, classical audio-only SE typically fails as they are usually trained to isolate speech from non-speech noises. The application comprises a graphical user interface and modules for real-time AV speech acquisition, preprocessing, and enhancement. The participants will experience a significant improvement in the speech quality and intelligibility of a target speaker who will be physically situated in a real noisy environment with a range of real-world noises. Moreover, participants can evaluate the performance of the application with their own voice by recording videos in challenging multi-talker conversational environments",
    "checked": false,
    "id": "38d36e9afce7bf306e6cca92c8e467b7f276df8f",
    "semantic_title": "towards robust real-time audio-visual speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23d_interspeech.html": {
    "title": "Mitigating the Exposure Bias in Sentence-Level Grapheme-to-Phoneme (G2P) Transduction",
    "volume": "main",
    "abstract": "Text-to-Text Transfer Transformer (T5) has recently been considered for the Grapheme-to-Phoneme (G2P) transduction. As a follow-up, a tokenizer-free byte-level model based on T5 referred to as ByT5, recently gave promising results on word-level G2P conversion by representing each input character with its corresponding UTF-8 encoding. Although it is generally understood that sentence-level or paragraph-level G2P can improve usability in real-world applications as it is better suited to perform on heteronyms and linking sounds between words, we find that using ByT5 for these scenarios is nontrivial. Since ByT5 operates on the character level, it requires longer decoding steps, which deteriorates the performance due to the exposure bias commonly observed in auto-regressive generation models. This paper shows that the performance of sentence-level and paragraph-level G2P can be improved by mitigating such exposure bias using our proposed loss-based sampling method",
    "checked": true,
    "id": "064af570792c61f5cc46814c648fd3969f0999e7",
    "semantic_title": "mitigating the exposure bias in sentence-level grapheme-to-phoneme (g2p) transduction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23_interspeech.html": {
    "title": "Streaming Parrotron for on-device speech-to-speech conversion",
    "volume": "main",
    "abstract": "We present a fully on-device streaming Speech2Speech conversion model that normalizes a given input speech directly to synthesized output speech. Deploying such a model on mobile devices pose significant challenges in terms of memory footprint and computation requirements. We present a streaming-based approach to produce an acceptable delay, with minimal loss in speech conversion quality, when compared to a reference state of the art non-streaming approach. Our method consists of first streaming the encoder in real time while the speaker is speaking. Then, as soon as the speaker stops speaking, we run the spectrogram decoder in streaming mode along the side of a streaming vocoder to generate output speech. To achieve an acceptable delay-quality trade-off, we propose a novel hybrid approach for look-ahead in the encoder which combines a look-ahead feature stacker with a look-ahead self-attention. We show that our streaming approach is 2x faster than real time on the Pixel4 CPU",
    "checked": true,
    "id": "1f89e67be1fc3bdcd8e6de807068f33f183aafd2",
    "semantic_title": "streaming parrotron for on-device speech-to-speech conversion",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shaheen23_interspeech.html": {
    "title": "Exploiting Emotion Information in Speaker Embeddings for Expressive Text-to-Speech",
    "volume": "main",
    "abstract": "Text-to-Speech (TTS) systems have recently seen great progress in synthesizing high-quality speech. However, the prosody of generated utterances often is not as diverse as prosody of the natural speech. In the case of multi-speaker or voice cloning systems, this problem becomes even worse as information about prosody may be present in the input text and the speaker embedding. In this paper, we study the phenomenon of the presence of emotional information in speaker embeddings recently revealed for i-vectors and x-vectors. We show that the produced embeddings may include devoted components encoding prosodic information. We further propose a technique for finding such components and generating emotional speaker embeddings by manipulating them. We then demonstrate that the emotional TTS system based on the proposed method shows good performance and has a smaller number of trained parameters compared to solutions based on fine-tuning",
    "checked": true,
    "id": "e9f0608602e1d9cc17edd1f80f8a598b4de33a0c",
    "semantic_title": "exploiting emotion information in speaker embeddings for expressive text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/okamoto23b_interspeech.html": {
    "title": "E2E-S2S-VC: End-To-End Sequence-To-Sequence Voice Conversion",
    "volume": "main",
    "abstract": "This paper proposes end-to-end (E2E) non-autoregressive sequence-to-sequence (S2S) voice conversion (VC) models that extend two E2E text-to-speech models, VITS and JETS. In the proposed E2E-S2S-VC models, VITS-VC and JETS-VC, the input text sequences of VITS and JETS are replaced by the source speaker's acoustic feature sequences, and E2E models (including HiFi-GAN waveform synthesizers) are trained using monotonic alignment search (MAS) without external aligners. To successfully train MAS for VC, the proposed models use a reduction factor only for the encoder. The voice of a source speaker is converted directly to that of a target speaker using a single neural network in the proposed models in an S2S manner; the duration and prosody between the source and target speech can be directly converted. The results of experiments using 1,000 parallel utterances of Japanese male and female speakers demonstrate that the proposed JETS-VC outperformed cascade non-autoregressive S2S VC models",
    "checked": true,
    "id": "338f92e4041fd832f2454e68afe9d31453587674",
    "semantic_title": "e2e-s2s-vc: end-to-end sequence-to-sequence voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23f_interspeech.html": {
    "title": "DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer",
    "volume": "main",
    "abstract": "Despite the huge successes made in neutral TTS, content-leakage remains a challenge. In this paper, we propose a new input representation and simple architecture to achieve improved prosody modeling. Inspired by the recent success in the use of discrete code in TTS, we introduce discrete code to the input of the reference encoder. Specifically, we leverage the vector quantizer from the audio compression model to exploit the diverse acoustic information it has already been trained on. In addition, we apply the modified MLP-Mixer to the reference encoder, making the architecture lighter. As a result, we train the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of our method through both subjective and objective evaluations. We demonstrate that the reference encoder learns better speaker-independent prosody when discrete code is utilized as input in the experiments. In addition, we obtain comparable results even when fewer parameters are inputted",
    "checked": true,
    "id": "9f1516735be1648a2f283528eba7ffb7067761fd",
    "semantic_title": "dc comix tts: an end-to-end expressive tts with discrete code collaborated with mixer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baas23_interspeech.html": {
    "title": "Voice Conversion With Just Nearest Neighbors",
    "volume": "main",
    "abstract": "Any-to-any voice conversion aims to transform source speech into a target voice with just a few examples of the target speaker as a reference. Recent methods produce convincing conversions, but at the cost of increased complexity – making results difficult to reproduce and build on. Instead, we keep it simple. We propose k-nearest neighbors voice conversion (kNN-VC): a straightforward yet effective method for any-to-any conversion. First, we extract self-supervised representations of the source and reference speech. To convert to the target speaker, we replace each frame of the source representation with its nearest neighbor in the reference. Finally, a pretrained vocoder synthesizes audio from the converted representation. Objective and subjective evaluations show that kNN-VC improves speaker similarity with similar intelligibility scores to existing methods. Code, samples, trained models: https://bshall.github.io/knn-vc",
    "checked": true,
    "id": "cc5c8defe3023a45b7425375c3ebda5fccbfcb66",
    "semantic_title": "voice conversion with just nearest neighbors",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tanaka23_interspeech.html": {
    "title": "CFVC: Conditional Filtering for Controllable Voice Conversion",
    "volume": "main",
    "abstract": "This paper describes a many-to-many voice conversion model that filters the speaker vector to control high-level attributes such as speaking rate while preserving voice timbre. In order to control only the speaking rate, it is essential to decompose the speaker vector into a speaking rate vector and others. The challenge is to train such disentangled representations with no/few annotation data. Motivated by this difficulty, we propose an approach combining the conditional filtering method with data augmentation. The experimental results showed that our method disentangled complex attributes without annotation and separately controlled speaking rate and voice timbre. Audio samples can be accessed on our web page",
    "checked": true,
    "id": "e99758ec9cea00717475c87fac118fc10ae68506",
    "semantic_title": "cfvc: conditional filtering for controllable voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ning23_interspeech.html": {
    "title": "DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding",
    "volume": "main",
    "abstract": "Real-time applications require voice conversion models with streaming conversion capabilities, and streaming voice conversion faces significant challenges due to limited future information. To address this challenge, we propose DualVC, a dual-mode neural voice conversion approach that supports both streaming and non-streaming modes using jointly trained separate network parameters. Furthermore, we propose intra-model knowledge distillation and hybrid predictive coding (HPC) to enhance the performance of streaming conversion.Additionally, we incorporate data augmentation to train a noise-robust autoregressive decoder, improving the model's performance on long-form speech conversion. Experimental results demonstrate that the proposed model outperforms the baseline models in the context of streaming voice conversion, while maintaining comparable performance to the non-streaming topline system that leverages the complete context, albeit with a latency of only 252ms",
    "checked": true,
    "id": "cfa76e619225b0689dbccfa4c361911e209f8ba2",
    "semantic_title": "dualvc: dual-mode voice conversion using intra-model knowledge distillation and hybrid predictive coding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23_interspeech.html": {
    "title": "Attention-based Interactive Disentangling Network for Instance-level Emotional Voice Conversion",
    "volume": "main",
    "abstract": "Emotional Voice Conversion aims to manipulate a speech according to a given emotion while preserving non-emotion components. Existing approaches cannot well express fine-grained emotional attributes. In this paper, we propose an Attention-based Interactive diseNtangling Network (AINN) that leverages instance-wise emotional knowledge for voice conversion. We introduce a two-stage pipeline to effectively train our network: Stage I utilizes inter-speech contrastive learning to model fine-grained emotion and intra-speech disentanglement learning to better separate emotion and content. In Stage II, we propose to regularize the conversion with a multi-view consistency mechanism. This technique helps us transfer fine-grained emotion and maintain speech content. Extensive experiments show that our AINN outperforms state-of-the-arts in both objective and subjective metrics",
    "checked": true,
    "id": "038cde0278fc3f25e208ba537c796e39179bebf2",
    "semantic_title": "attention-based interactive disentangling network for instance-level emotional voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23p_interspeech.html": {
    "title": "ALO-VC: Any-to-any Low-latency One-shot Voice Conversion",
    "volume": "main",
    "abstract": "This paper presents ALO-VC, a non-parallel low-latency one-shot phonetic posteriorgrams (PPGs) based voice conversion method. ALO-VC enables any-to-any voice conversion using only one utterance from the target speaker, with only 47.5 ms future look-ahead. The proposed hybrid signal processing and machine learning pipeline combines a pre-trained speaker encoder, a pitch predictor to predict the converted speech's prosody, and positional encoding to convey the phoneme's location information. We introduce two system versions: ALO-VC-R, which uses a pre-trained d-vector speaker encoder, and ALO-VC-E, which improves performance using the ECAPA-TDNN speaker encoder. The experimental results demonstrate both ALO-VC-R and ALO-VC-E can achieve comparable performance to non-causal baseline systems on the VCTK dataset and two out-of-domain datasets. Furthermore, both proposed systems can be deployed on a single CPU core with 55 ms latency and 0.78 real-time factor. Our demo is available online",
    "checked": true,
    "id": "5830528b8eda61534d7912f3b3be51b62ee93cb1",
    "semantic_title": "alo-vc: any-to-any low-latency one-shot voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/minixhofer23_interspeech.html": {
    "title": "Evaluating and reducing the distance between synthetic and real speech distributions",
    "volume": "main",
    "abstract": "While modern Text-to-Speech (TTS) systems can produce natural-sounding speech, they remain unable to reproduce the full diversity found in natural speech data. We consider the distribution of all possible real speech samples that could be generated by these speakers alongside the distribution of all synthetic samples that could be generated for the same set of speakers, using a particular TTS system. We set out to quantify the distance between real and synthetic speech via a range of utterance-level statistics related to properties of the speaker, speech prosody and acoustic environment. Differences in the distribution of these statistics are evaluated using the Wasserstein distance. We reduce these distances by providing ground-truth values at generation time, and quantify the improvements to the overall distribution distance, approximated using an automatic speech recognition system. Our best system achieves a 10% reduction in distribution distance",
    "checked": true,
    "id": "da7c0e5afb152b35defb1ee6a6926503c648f36a",
    "semantic_title": "evaluating and reducing the distance between synthetic and real speech distributions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/quamer23_interspeech.html": {
    "title": "Decoupling Segmental and Prosodic Cues of Non-native Speech through Vector Quantization",
    "volume": "main",
    "abstract": "Accent conversion (AC) seeks to transform utterances from a non-native speaker to appear native-like. Compared to voice conversion, which generally treats accent and voice quality as one, AC provides a finer-grained decomposition of speech. This paper presents an AC system that further decomposes an accent into its segmental and prosodic characteristics, and provides independent control of both channels. The system uses conventional modules (acoustic model, speaker/prosody encoders, seq2seq model) to generate accent conversions that combine (1) the segmental characteristics from a source utterance, (2) the voice characteristics from a target utterance, and (3) the prosody of a reference utterance. However, naive application of this idea prevents the system from learning and transferring prosody. We show that vector quantization and removal of repeated codewords allows the system to transfer prosody and improve voice similarity, as verified by objective and perceptual measures",
    "checked": true,
    "id": "65ed82c466e2228658a8dd23fed56c0a181f04b9",
    "semantic_title": "decoupling segmental and prosodic cues of non-native speech through vector quantization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kanagawa23_interspeech.html": {
    "title": "VC-T: Streaming Voice Conversion Based on Neural Transducer",
    "volume": "main",
    "abstract": "A conventional sequence-to-sequence voice conversion (seq2seq VC), i.e., attentional encoder-decoder, can be trained without the speech sequence pre-aligning normally used to counter the different lengths of the source and target speakers. However, if alignments rendered by attention are not monotonic, speech drops and repeats will happen, and the linguistic contents will not be kept. To address this issue, we propose VC-T, a novel streaming VC framework based on a neural transducer (RNNT); RNNT is effective in the automatic speech recognition field as it offers robust alignment against collapse. We also introduce an alignment design scheme for VC-T training. Experiments show that our offline and streaming VC-T variants outperform two modern seq2seq parallel VCs while offering a lower character error rate as a result of the proposal robust alignment. Our VC-T also achieves better naturalness the drastic degradation suffered by the conventional alternatives, especially for streaming VC",
    "checked": true,
    "id": "4cd3e8bb37b02295c98e86f03552f888cc57b581",
    "semantic_title": "vc-t: streaming voice conversion based on neural transducer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ghosh23_interspeech.html": {
    "title": "Emo-StarGAN: A Semi-Supervised Any-to-Many Non-Parallel Emotion-Preserving Voice Conversion",
    "volume": "main",
    "abstract": "Speech anonymisation prevents misuse of spoken data by removing any personal identifier while preserving at least linguistic content. However, emotion preservation is crucial for natural human-computer interaction. The well-known voice conversion technique StarGANv2-VC achieves anonymisation but fails to preserve emotion. This work presents an any-to-many semi-supervised StarGANv2-VC variant trained on partially emotion-labelled non-parallel data. We propose emotion-aware losses computed on the emotion embeddings and acoustic features correlated to emotion. Additionally, we use an emotion classifier to provide direct emotion supervision. Objective and subjective evaluations show that the proposed approach significantly improves emotion preservation over the vanilla StarGANv2-VC. This considerable improvement is seen over diverse datasets, emotions, target speakers, and inter-group conversions without compromising intelligibility and anonymisation",
    "checked": true,
    "id": "0105c8443d029e857c08f8cb9d9ca4a5b1abb1a6",
    "semantic_title": "emo-stargan: a semi-supervised any-to-many non-parallel emotion-preserving voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23r_interspeech.html": {
    "title": "ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Speed",
    "volume": "main",
    "abstract": "Recent advancements in neural speech synthesis have renewed interest in voice conversion (VC) to go beyond timbre transfer. Achieving controllability of para-linguistic parameters like pitch and speed is crucial in various applications. However, existing studies either lack interpretability or only provide global control at the utterance level. This paper introduces ControlVC, the first neural voice conversion system to enable time-varying controls on pitch and speed. ControlVC uses pre-trained encoders to generate pitch and linguistic embeddings, combined and converted to speech using a vocoder. Speed control is achieved by TD-PSOLA pre-processing, while pitch control is achieved by manipulating the pitch contour before feeding it into the encoder. Systematic subjective and objective evaluations show that this work significantly outperforms self-constructed baselines on speech quality and controllability for non-parallel zero-shot conversion while achieving time-varying control",
    "checked": true,
    "id": "6270e7e4a33b0c53c4b92f3755d599d318b760a1",
    "semantic_title": "controlvc: zero-shot voice conversion with time-varying controls on pitch and speed",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23e_interspeech.html": {
    "title": "Reverberation-Controllable Voice Conversion Using Reverberation Time Estimator",
    "volume": "main",
    "abstract": "Recent trends have emerged to implement voice conversion (VC) in real-world scenarios where background sounds and reverberation are inevitable. However, most VC studies mainly focus on clean speech conversion, where high-quality speech data are required for training and testing. Moreover, the background sounds and reverberation are treated as interferences to be discarded, despite being informative to be retained in some scenarios, such as movie dubbing and singing VC. In this paper, we propose a reverberation-robust VC framework consisting of a reverberation time (T60) estimation module and a VC module. The T60 estimator is introduced to provide the VC module with the reverberation information to model the reverberant speech. Experimental results show that 1) our framework can disentangle and control the speaker identity and reverberation from the speech, and 2) we can get acceptable VC performances dealing with reverberation, even when clean training data are not available",
    "checked": true,
    "id": "b5edf316b94a95ee63e5bfc35116248bddc958ee",
    "semantic_title": "reverberation-controllable voice conversion using reverberation time estimator",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23d_interspeech.html": {
    "title": "Cross-utterance Conditioned Coherent Speech Editing",
    "volume": "main",
    "abstract": "Text-based speech editing systems are developed to enable users to modify speech based on the transcript. Existing state-of-the-art editing systems based on neural networks do partial inferences with no exception, that is, only generate new words that need to be replaced or inserted. This manner usually leads to the prosody of the edited part being inconsistent with the surrounding speech and a failure to handle the alteration of intonation. To address these problems, we propose a cross-utterance conditioned coherent speech editing system, that first does the entire reasoning at the inference time. Our proposed system can generate speech by utilizing speaker information, context, acoustic features, and the mel-spectrogram from the original audio. Experiments conducted on subjective and objective metrics demonstrate that our approach outperforms the baseline on various editing operations regarding naturalness and prosody consistency",
    "checked": true,
    "id": "6e0d7809007b9699e20530e3cee092315e2de43e",
    "semantic_title": "cross-utterance conditioned coherent speech editing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23o_interspeech.html": {
    "title": "MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information",
    "volume": "main",
    "abstract": "Audio-visual speech recognition (AVSR) gains increasing attention from researchers as an important part of human-computer interaction. However, the existing available Mandarin audio-visual datasets are limited and lack the depth information. To address this issue, this work establishes the MAVD, a new large-scale Mandarin multimodal corpus comprising 12,484 utterances spoken by 64 native Chinese speakers. To ensure the dataset covers diverse real-world scenarios, a pipeline for cleaning and filtering the raw text material has been developed to create a well-balanced reading material. In particular, the latest data acquisition device of Microsoft, Azure Kinect is used to capture depth information in addition to the traditional audio signals and RGB images during data acquisition. We also provide a baseline experiment, which could be used to evaluate the effectiveness of the dataset. The dataset and code will be released at https://github.com/SpringHuo/MAVD",
    "checked": true,
    "id": "2eefbcb4fabf8804940a89bc62559860628a5942",
    "semantic_title": "mavd: the first open large-scale mandarin audio-visual dataset with depth information",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23y_interspeech.html": {
    "title": "CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition",
    "volume": "main",
    "abstract": "Audio-visual person recognition (AVPR) has received extensive attention. However, most datasets used for AVPR research so far are collected in constrained environments, and thus cannot reflect the true performance of AVPR systems in real-world scenarios. To meet the request for research on AVPR in unconstrained conditions, this paper presents a multi-genre AVPR dataset collected 'in the wild', named CN-Celeb-AV. This dataset contains more than 420k video segments from 1,136 persons from public media. In particular, we put more emphasis on two real-world complexities: (1) data in multiple genres; (2) segments with partial information. A comprehensive study was conducted to compare CN-Celeb-AV with two popular public AVPR benchmark datasets, and the results demonstrated that CN-Celeb-AV is more in line with real-world scenarios and can be regarded as a new benchmark dataset for AVPR research. The dataset also involves a development set that can be used to boost the performance of AVPR systems in real-life situations. The dataset is free for researchers and can be downloaded from http://cnceleb.org/",
    "checked": true,
    "id": "7fd4d6f025350dd35bb9657fb2936fd6c4557941",
    "semantic_title": "cn-celeb-av: a multi-genre audio-visual dataset for person recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ca_interspeech.html": {
    "title": "Improving Zero-shot Cross-domain Slot Filling via Transformer-based Slot Semantics Fusion",
    "volume": "main",
    "abstract": "Slot filling is an essential component in task-oriented dialogue systems. Due to the scarcity of annotated data, zero-shot slot filling has been studied to transfer knowledge from source domains to a target domain. Previous methods adopt slot descriptions or questions as slot semantics, where they utilize slot descriptions to calculate similarity scores, or reformat the task as a question-answering problem. However, these methods do not fully exploit the token-level dependency between the slot semantics and utterances. In this study, we propose a Transformer-based Slot semantics fusion method for Slot Filling (TSSF). We first adopt two encoders with shared weights to obtain the representations of utterances and slot semantics. Then, we design a transformer-based fusion module for effectively integrating slot semantics into utterances. Experimental results on the public benchmark SNIPS show that our model significantly outperforms the state-of-the-art model by 6.09% in terms of slot F1",
    "checked": true,
    "id": "d7af953e4ede906dd677257a902d22b5ead13cdb",
    "semantic_title": "improving zero-shot cross-domain slot filling via transformer-based slot semantics fusion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shin23_interspeech.html": {
    "title": "Rethinking Transfer and Auxiliary Learning for Improving Audio Captioning Transformer",
    "volume": "main",
    "abstract": "The performance of automated audio captioning (AAC) has been improved considerably through a transformer-based encoder and transfer learning. However, their performance improvement is constrained by the following problems: (1) discrepancy in the input patch size between pretraining and fine-tuning steps. (2) lack of local-level relations between inputs and captions. In this paper, we propose a simple transfer learning scheme that maintains input patch sizes, unlike previous methods, to avoid input discrepancies. Furthermore, we propose a patch-wise keyword estimation branch that utilizes an attention pooling method to effectively represent both global- and local-level information. The results on the AudioCaps dataset reveal that the proposed learning scheme and method considerably contribute to performance gain. Finally, the visualization results demonstrate that the proposed attention-pooling method effectively detects local-level information in the AAC system",
    "checked": true,
    "id": "bf7469a65ae60077a2bc478e594a3203d82d0268",
    "semantic_title": "rethinking transfer and auxiliary learning for improving audio captioning transformer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lai23c_interspeech.html": {
    "title": "Boosting Punctuation Restoration with Data Generation and Reinforcement Learning",
    "volume": "main",
    "abstract": "Punctuation restoration is an important task in automatic speech recognition (ASR) which aim to restore the syntactic structure of generated ASR texts to improve readability. While punctuated texts are abundant from written documents, the discrepancy between written punctuated texts and ASR texts limits the usability of written texts in training punctuation restoration systems for ASR texts. This paper proposes a reinforcement learning method to exploit in-topic written texts and recent advances in large pre-trained generative language models to bridge this gap. The experiments show that our method achieves state-of-the-art performance on the ASR test set on two benchmark datasets for punctuation restoration. The source code of this work is publicly accessible at https://github.com/laiviet/pr-rl",
    "checked": true,
    "id": "d32b8a89d7385aa7db6e95ba7d920cff8bc746ff",
    "semantic_title": "boosting punctuation restoration with data generation and reinforcement learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23e_interspeech.html": {
    "title": "J-ToneNet: A Transformer-based Encoding Network for Improving Tone Classification in Continuous Speech via F0 Sequences",
    "volume": "main",
    "abstract": "Currently, tone classification studies mainly focus on training classifiers by using intrinsic features of isolated segments, i.e. often the syllables. Mostly, the works are not merely in use of fundamental frequency (f0) but utilizing more information on the spectrograms, MFCCs, or energy to improve model accuracy. However, as we know, more challenges on tone classification lie on modeling the complex f0 variations from the tonal coarticulations and the interactive effects among tonality in continuous speech. To tackle down this issue, we first aim at in using the sequence of f0 samples in speech utterance only. In addition, we propose a transformer based network with an extendable BERT input architecture and a joint learning technique to consolidate the contour representations of consecutive tones. Leveraging or fusing more information affected from speech rhythm in utterance, the experiments show that the proposed J-ToneNet is very robust for read speech",
    "checked": true,
    "id": "a0dd2506538ad0655154108d5cc2fc0d708be935",
    "semantic_title": "j-tonenet: a transformer-based encoding network for improving tone classification in continuous speech via f0 sequences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/avila23_interspeech.html": {
    "title": "Towards Cross-Language Prosody Transfer for Dialog",
    "volume": "main",
    "abstract": "Speech-to-speech translation systems today do not adequately support use for dialog purposes. In particular, nuances of speaker intent and stance can be lost due to improper prosody transfer. We present an exploration of what needs to be done to overcome this. First, we developed a data collection protocol in which bilingual speakers re-enact utterances from an earlier conversation in their other language, and used this to collect an English-Spanish corpus, so far comprising 1871 matched utterance pairs. Second, we developed a simple prosodic dissimilarity metric based on Euclidean distance over a broad set of prosodic features. We then used these to investigate cross-language prosodic differences, measure the likely utility of three simple baseline models, and identify phenomena which will require more powerful modeling. Our findings should inform future research on cross-language prosody and the design of speech-to-speech translation systems capable of effective prosody transfer",
    "checked": true,
    "id": "3bdbadcd443d6fea9faa90b46ab28df61246b1d0",
    "semantic_title": "towards cross-language prosody transfer for dialog",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kesiraju23_interspeech.html": {
    "title": "Strategies for Improving Low Resource Speech to Text Translation Relying on Pre-trained ASR Models",
    "volume": "main",
    "abstract": "This paper presents techniques and findings for improving the performance of low-resource speech to text translation (ST). We conducted experiments on both simulated and real-low resource setups, on language pairs English - Portuguese, and Tamasheq - French respectively. Using the encoder-decoder framework for ST, our results show that a multilingual automatic speech recognition system acts as a good initialization under low-resource scenarios. Furthermore, using the CTC as an additional objective for translation during training and decoding helps to reorder the internal representations and improves the final translation. Through our experiments, we try to identify various factors (initializations, objectives, and hyper-parameters) that contribute the most for improvements in low-resource setups. With only 300 hours of pre-training data, our model achieved 7.3 BLEU score on Tamasheq - French data, outperforming prior published works from IWSLT 2022 by 1.6 points",
    "checked": true,
    "id": "f12b04da6de0995583d4e8aea21a5fefc54ecd2b",
    "semantic_title": "strategies for improving low resource speech to text translation relying on pre-trained asr models",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koudounas23_interspeech.html": {
    "title": "ITALIC: An Italian Intent Classification Dataset",
    "volume": "main",
    "abstract": "Recent large-scale Spoken Language Understanding datasets focus predominantly on English and do not account for language-specific phenomena such as particular phonemes or words in different lects. We introduce ITALIC, the first large-scale speech dataset designed for intent classification in Italian. The dataset comprises 16,521 crowdsourced audio samples recorded by 70 speakers from various Italian regions and annotated with intent labels and additional metadata. We explore the versatility of ITALIC by evaluating current state-of-the-art speech and text models. Results on intent classification suggest that increasing scale and running language adaptation yield better speech models, monolingual text models outscore multilingual ones, and that speech recognition on ITALIC is more challenging than on existing Italian benchmarks. We release both the dataset and the annotation scheme to streamline the development of new Italian SLU models and language-specific datasets",
    "checked": true,
    "id": "7d8ce3eefbddeb72c1a1c78cc8a775d96426c81c",
    "semantic_title": "italic: an italian intent classification dataset",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rugayan23_interspeech.html": {
    "title": "Perceptual and Task-Oriented Assessment of a Semantic Metric for ASR Evaluation",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems have become a vital part of our everyday lives through their many applications. However, as much as we have developed in this regard, our most common evaluation method for ASR systems still remains to be word error rate (WER). WER does not give information on the severity of errors, which strongly impacts practical performance. As such, we examine a semantic-based metric called Aligned Semantic Distance (ASD) against WER and demonstrate its advantage over WER in two facets. First, we conduct a survey asking participants to score reference text and ASR transcription pairs. We perform a correlation analysis and show that ASD is more correlated to the human evaluation scores compared to WER. We also explore the feasibility of predicting human perception using ASD. Second, we demonstrate that ASD is more effective than WER as an indicator of performance on downstream NLP tasks such as named entity recognition and sentiment classification",
    "checked": true,
    "id": "babdea20aa5eed770cde3e7946574dccc90c8d37",
    "semantic_title": "perceptual and task-oriented assessment of a semantic metric for asr evaluation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23s_interspeech.html": {
    "title": "How ChatGPT is Robust for Spoken Language Understanding?",
    "volume": "main",
    "abstract": "Large language models (LLMs), e.g. ChatGPT, have shown super performance on various NLP tasks. There is a doubt whether these LLMs, which are trained on a large corpus of written text, can show the robustness of understanding the spoken text. Therefore in this paper, we give a detailed investigation of robust spoken language understanding (SLU) with ChatGPT. In our experiments, we evaluate ChatGPT on two sets of public datasets, Spoken SQuAD and ASR-GLUE, in which there are ASR errors in the text. Quantitative and qualitative analyses on the experimental results are conducted to show that ChatGPT not only performs very well for SLU tasks but also can recover some ASR errors with its super reason ability",
    "checked": true,
    "id": "fc667b8705a35a637142dc1d9270326ebc952475",
    "semantic_title": "how chatgpt is robust for spoken language understanding?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ye23b_interspeech.html": {
    "title": "GigaST: A 10,000-hour Pseudo Speech Translation Corpus",
    "volume": "main",
    "abstract": "This paper introduces GigaST, a large-scale pseudo speech-to-text translation (ST) corpus. We create the corpus by translating the transcript in GigaSpeech, an English ASR corpus, into German and Chinese. The training set is translated by a strong machine translation system and the test set is translated by human. ST models trained with an addition of our corpus obtain new state-of-the-art results on the MuST-C English-German benchmark test set. We provide a detailed description of the translation process and verify its quality. We make the translated text data public and hope to facilitate research in speech translation. Additionally, we also release the training scripts on NeurST1 to make it easy to replicate our systems. GigaST dataset is available at https://st-benchmark.github.io/resources/GigaST",
    "checked": false,
    "id": "141d1e80096b0fa67904257b8270afa3d5180510",
    "semantic_title": "gigast: a 10, 000-hour pseudo speech translation corpus",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fan23b_interspeech.html": {
    "title": "Boosting Chinese ASR Error Correction with Dynamic Error Scaling Mechanism",
    "volume": "main",
    "abstract": "Chinese Automatic Speech Recognition (ASR) error correction presents significant challenges due to the Chinese language's unique features, including a large character set and borderless, morpheme-based structure. Current mainstream models often struggle with effectively utilizing word-level features and phonetic information. This paper introduces a novel approach that incorporates a dynamic error scaling mechanism to detect and correct phonetically erroneous text generated by ASR output. This mechanism operates by dynamically fusing word-level features and phonetic information, thereby enriching the model with additional semantic data. Furthermore, our method implements unique error reduction and amplification strategies to address the issues of matching wrong words caused by incorrect characters. Experimental results indicate substantial improvements in ASR error correction, demonstrating the effectiveness of our proposed method and yielding promising results on established datasets",
    "checked": true,
    "id": "2a8ec6c5d72594bb92159522ac76e81cb865f26a",
    "semantic_title": "boosting chinese asr error correction with dynamic error scaling mechanism",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fallgren23_interspeech.html": {
    "title": "Crowdsource-based Validation of the Audio Cocktail as a Sound Browsing Tool",
    "volume": "main",
    "abstract": "We conduct two crowdsourcing experiments designed to examine the usefulness of audio cocktails to quickly find out information on the contents of large audio data. Several thousand crowd workers were engaged to listen to audio cocktails with systematically varied composition. They were then asked to state either which sound out of four categories (Children, Women, Men, Orchestra) they heard the most of, or if they heard anything of a specific category at all. The results show that their responses have high reliability and provide information as to whether a specific task can be performed using audio cocktails. We also propose that the combination of crowd workers and audio cocktails can be used directly as a tool to investigate the contents of large audio data",
    "checked": true,
    "id": "309ec568cea3b99123545ace000924212d055d59",
    "semantic_title": "crowdsource-based validation of the audio cocktail as a sound browsing tool",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23z_interspeech.html": {
    "title": "PunCantonese: A Benchmark Corpus for Low-Resource Cantonese Punctuation Restoration from Speech Transcripts",
    "volume": "main",
    "abstract": "Punctuation restoration from unsegmented speech transcripts is an essential task to improve the readability of transcripts and can facilitate various downstream NLP tasks. However, there is still lack of systematic studies on punctuation restoration for Cantonese as a low-resource language. This paper introduces a new Cantonese punctuation corpus named PunCantonese, which consists of annotated spoken transcripts and written-style Wikipedia sentences, covering the major punctuations such as \",.?!\" and code-switched sentences in Cantonese and English. We also propose a Transformer-based punctuation model which exploits pre-trained multilingual language models, adopts multitask learning for style and punctuation prediction, and introduces a novel Jyutping embedding layer to inject the phonetic features not explicitly available in Cantonese characters. Experimental results show that these methods are effective in improving punctuation restoration, and the Jyutping embedding layer brings an absolute F1 increase by more than 2%",
    "checked": true,
    "id": "c20f97155221a72625bfd6c6572d97b26138fce1",
    "semantic_title": "puncantonese: a benchmark corpus for low-resource cantonese punctuation restoration from speech transcripts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kato23_interspeech.html": {
    "title": "Speech-to-Face Conversion Using Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "Speech-to-face conversion is the task of generating face images from speech signals. Many studies have been conducted to address this task, and achieved good performances. In this paper, we introduce denoising diffusion probabilistic models (DDPMs) to generate face images instead of generative adversarial networks (GANs) or autoencoders, which are used in most of the prior studies. Moreover, unlike prior studies, several components of our system are designed to use high-resolution face image datasets instead of audio-visual paired data. As a result, our system can generate high-resolution face images from speech signals with an architecture that is simpler and more flexible than the ones used in prior studies. In addition, introducing DDPMs enables us to utilize techniques that control out- puts of DDPMs or improve performance of them in succeeding studies",
    "checked": true,
    "id": "573eb81bf5d37fb10588fafa33ccd852d2ca65ca",
    "semantic_title": "speech-to-face conversion using denoising diffusion probabilistic models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nishikawa23_interspeech.html": {
    "title": "Inter-connection: Effective Connection between Pre-trained Encoder and Decoder for Speech Translation",
    "volume": "main",
    "abstract": "In end-to-end speech translation, speech and text pre-trained models improve translation quality. Recently proposed models simply connect the pre-trained models of speech and text as encoder and decoder. Therefore, only the information from the final layer of encoders is input to the decoder. Since it is clear that the speech pre-trained model outputs different information from each layer, the simple connection method cannot fully utilize the information that the speech pre-trained model has. In this study, we propose an inter-connection mechanism that aggregates the information from each layer of the speech pre-trained model by weighted sums and inputs into the decoder. This mechanism increased BLEU by approximately 2 points in en-de, en-ja, and en-zh by increasing parameters by 2K when the speech pre-trained model was frozen. Furthermore, we investigated the contribution of each layer for each language by visualizing layer weights and found that the contributions were different",
    "checked": true,
    "id": "14a01b6001c5301a2d89a86a9a35d4013fc17cae",
    "semantic_title": "inter-connection: effective connection between pre-trained encoder and decoder for speech translation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/radfar23_interspeech.html": {
    "title": "Conmer: Streaming Conformer Without Self-attention for Interactive Voice Assistants",
    "volume": "main",
    "abstract": "Conformer is an extension of transformer-based neural ASR models whose fundamental component is the self-attention module. In this paper, we show that we can remove the self-attention module from Conformer and achieve the same or even better recognition performance for utterances whose length is up to around 10 seconds. This is particularly important for streaming interactive voice assistants as input is often very short and a fast response is expected. Since the computational complexity of self-attention is quadratic, this modification allows for faster, smaller sized models, two requirements for on-device applications. Using this finding, we propose Conmer, a neural architecture based on Conformer but without self-attention for streaming interactive voice assistants. We conduct experiments on public and real-world data and show the streaming Conmer reduces the WER and computational complexity relatively by 4.03% and 10%, respectively",
    "checked": true,
    "id": "7bd6a6d55c55f5b66ab04f2f5fa6659bad928c68",
    "semantic_title": "conmer: streaming conformer without self-attention for interactive voice assistants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23e_interspeech.html": {
    "title": "Intra-ensemble: A New Method for Combining Intermediate Outputs in Transformer-based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Deep learning models employ various regularization techniques to prevent overfitting and enhance generalization. In particular, an auxiliary loss, as proposed for connectionist temporal classification (CTC) models, demonstrated the potential for intermediate prediction to be useful by enabling sub-models to recognize speech accurately. We propose a new method called Intra-ensemble, which combines these accurate intermediate outputs into a single output for both training and inference, considering the importance of the intermediate layer using learnable parameters. Our approach is applicable to CTC models, attention-based encoder-decoder models, and transducer structures and demonstrated performance improvements of 13.5%, 3.0%, and 4.1% respectively, in the LibriSpeech evaluation. Furthermore, through various analytical experiments, we found that the sub-models contributed significantly to performance improvement",
    "checked": true,
    "id": "16f9d44d9b83e806837276dddb3967af0d0596b6",
    "semantic_title": "intra-ensemble: a new method for combining intermediate outputs in transformer-based automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23b_interspeech.html": {
    "title": "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks",
    "volume": "main",
    "abstract": "Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community",
    "checked": true,
    "id": "14f5fd91d75bc10d9fff53dfe7ee73484fc4273b",
    "semantic_title": "a comparative study on e-branchformer vs conformer in speech recognition, translation, and understanding tasks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mai23_interspeech.html": {
    "title": "HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition",
    "volume": "main",
    "abstract": "State-of-the-art ASR systems have achieved promising results by modeling local and global interactions separately. While the former can be computed efficiently, global interactions are usually modeled via attention mechanisms, which are expensive for long input sequences. Here, we address this by extending HyperMixer, an efficient alternative to attention exhibiting linear complexity, to the Conformer architecture for speech recognition, leading to HyperConformer. In particular, multi-head HyperConformer achieves comparable or higher recognition performance while being more efficient than Conformer in terms of inference speed, memory, parameter count, and available training data. HyperConformer achieves a word error rate of 2.9% on LibriSpeech test-clean with less than 8M neural parameters and a peak memory during training of 5.7GB, hence trainable with accessible hardware. Inference speed is between 38% on mid-length speech and 56% on long speech faster than an equivalent Conformer",
    "checked": true,
    "id": "6702329634ed6876fc03bcc8ea68bdbf34c254e4",
    "semantic_title": "hyperconformer: multi-head hypermixer for efficient speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/carvalho23_interspeech.html": {
    "title": "Memory-augmented conformer for improved end-to-end long-form ASR",
    "volume": "main",
    "abstract": "Conformers have recently been proposed as a promising modelling approach for automatic speech recognition (ASR), outperforming recurrent neural network-based approaches and transformers. Nevertheless, in general, the performance of these end-to-end models, especially attention-based models, is particularly degraded in the case of long utterances. To address this limitation, we propose adding a fully-differentiable memory-augmented neural network between the encoder and decoder of a conformer. This external memory can enrich the generalization for longer utterances since it allows the system to store and retrieve more information recurrently. Notably, we explore the neural Turing machine (NTM) that results in our proposed Conformer-NTM model architecture for ASR. Experimental results using Librispeech train-clean-100 and train-960 sets show that the proposed system outperforms the baseline conformer without memory for long utterances",
    "checked": true,
    "id": "c1ca1e4953bd0a3ce553fae7f954813575b13416",
    "semantic_title": "memory-augmented conformer for improved end-to-end long-form asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23_interspeech.html": {
    "title": "Towards Effective and Compact Contextual Representation for Conformer Transducer Speech Recognition Systems",
    "volume": "main",
    "abstract": "Current ASR systems are mainly trained and evaluated at the utterance level. Long range cross utterance context can be incorporated. A key task is to derive a suitable compact representation of the most relevant history contexts. In contrast to previous researches based on either LSTM-RNN encoded histories that attenuate the information from longer range contexts, or frame level concatenation of transformer context embeddings, in this paper compact low-dimensional cross utterance contextual features are learned in the Conformer-Transducer Encoder using specially designed attention pooling layers that are applied over efficiently cached preceding utterances' history vectors. Experiments on the 1000-hr Gigaspeech corpus demonstrate that the proposed contextualized streaming Conformer-Transducers outperform the baseline using utterance internal context only with statistically significant WER reductions of 0.7% to 0.5% absolute (4.3% to 3.1% relative) on the dev and test data",
    "checked": true,
    "id": "1abae0addac14b4889fa4396a31b407ffd3f22c5",
    "semantic_title": "towards effective and compact contextual representation for conformer transducer speech recognition systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23o_interspeech.html": {
    "title": "An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification",
    "volume": "main",
    "abstract": "Effective fusion of multi-scale features is crucial for improving speaker verification performance. While most existing methods aggregate multi-scale features in a layer-wise manner via simple operations, such as summation or concatenation. This paper proposes a novel architecture called Enhanced Res2Net (ERes2Net), which incorporates both local and global feature fusion techniques to improve the performance. The local feature fusion (LFF) fuses the features within one single residual block to extract the local signal. The global feature fusion (GFF) takes acoustic features of different scales as input to aggregate global signal. To facilitate effective feature fusion in both LFF and GFF, an attentional feature fusion module is employed in the ERes2Net architecture, replacing summation or concatenation operations. A range of experiments conducted on the VoxCeleb datasets demonstrate the superiority of the ERes2Net in speaker verification",
    "checked": true,
    "id": "db22321ee7316104b7e69cca6babcfbc7ee0f4b3",
    "semantic_title": "an enhanced res2net with local and global feature fusion for speaker verification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23x_interspeech.html": {
    "title": "A Study on Visualization of Voiceprint Feature",
    "volume": "main",
    "abstract": "Despite the remarkable success of convolutional neural networks (CNNs) in voiceprint recognition, we still lack a comprehensive understanding of the specific features extracted by these models. To address this issue, we adopt an attribution approach in this paper to explain the voiceprint identification model and visualize the relevant features. Using five attribution methods, we successfully identify the features extracted by the ECAPA-TDNN model and confirm the reliability of our attribution techniques.We also explore two distinct methods for visualizing voiceprint features, with one approach aimed at interpreting features in unknown speech and the other focused on known speech. Through the attribution method, we are able to more precisely capture voiceprint features within speech data without significantly impacting the performance of the voiceprint recognition model. It would help us to do a more detailed study of the voiceprint features in the future",
    "checked": true,
    "id": "02945f6eaa65762651cf9f409e1e7e5d53469257",
    "semantic_title": "a study on visualization of voiceprint feature",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yakovlev23_interspeech.html": {
    "title": "VoxTube: a multilingual speaker recognition dataset",
    "volume": "main",
    "abstract": "The objective of this paper is to advance the development of technologies in the fields of speaker recognition and speaker identification by introducing a large labeled audio database VoxTube collected from the open-source media. We propose a fully automated unsupervised approach for audio labeling that requires any pre-trained speaker recognition model. Collected with this approach from videos with CC BY license the VoxTube dataset contains more than 5.000 speakers with more than 4 million utterances pronounced in more than 10 languages. In our paper we show the VoxTube's high generalization ability across multiple domains by evaluating the accuracy metrics on various speaker recognition benchmarks. We also show how well this dataset complements an already existing VoxCeleb2 dataset",
    "checked": true,
    "id": "1a67c57dcf3f0d4517b5abe2509870e586191e65",
    "semantic_title": "voxtube: a multilingual speaker recognition dataset",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23p_interspeech.html": {
    "title": "Visualizing Data Augmentation in Deep Speaker Recognition",
    "volume": "main",
    "abstract": "Visualization is of great value in understanding the internal mechanisms of neural networks. Previous work found that LayerCAM is a reliable visualization tool for deep speaker models. In this paper, we use LayerCAM to analyze the widely-adopted data augmentation (DA) approach, to understand how it leads to model robustness. We conduct experiments on the VoxCeleb1 dataset for speaker identification, which shows that both vanilla and activation-based (Act) DA approaches enhance robustness against interference, with Act DA being consistently superior. Visualization with LayerCAM suggests DA helps models learn to delete temporal-frequency (TF) bins that are corrupted by interference. The ‘learn to delete' behavior explained why DA models are more robust than clean models, and why the Act DA is superior over the vanilla DA when the interference is non-target speech. However, LayerCAM still cannot clearly explain the superiority of Act DA in other situations, suggesting further research",
    "checked": true,
    "id": "c40b9b1e751d7e9cddcfbfa53e03d9c8943e2433",
    "semantic_title": "visualizing data augmentation in deep speaker recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ba_interspeech.html": {
    "title": "Fast and Efficient Multilingual Self-Supervised Pre-training for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "Recent advances in self-supervised learning (SSL) have remarkably improved speech recognition performance for low-resource languages. On the other hand, with data of an increasingly larger scale required for SSL, the pretraining process has become extremely time-consuming. To address this problem, we propose an unsupervised data selection method based on utterance-level language similarity and a curriculum learning strategy to boost the efficiency of multilingual SSL pretraining while maintaining performance. We conduct experiments on five languages in COMMONVOICE dataset. Compared to the baseline with all data for pretraining, we pretrained on only 25% of the data and saved 60% of the training steps with even better performance on the target low-resource language",
    "checked": true,
    "id": "f570c974088d8aa621ecdb16334a9c94e15d1c52",
    "semantic_title": "fast and efficient multilingual self-supervised pre-training for low-resource speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23z_interspeech.html": {
    "title": "UniSplice: Universal Cross-Lingual Data Splicing for Low-Resource ASR",
    "volume": "main",
    "abstract": "End-to-end (E2E) automatic speech recognition (ASR) has made remarkable progress thanks to the abundant annotated data for a few rich-resource languages. However, data scarcity remains a challenge for the majority of the world's languages. To address this issue, we propose UniSplice, a novel cross-lingual speech synthesis framework based on data splicing that leverages self-supervised learning (SSL) units from Hidden Unit BERT (HuBERT) as universal phonetic units. Our approach involves splicing speech fragments from rich-resource languages into complete speech that conforms acoustically to text from low-resource languages. UniSplice eliminates the need for computationally expensive neural text-to-speech (TTS) models, enabling the training of ASR models using on-the-fly synthesized speech. Experimental results on the COMMON-VOICE dataset show 20-30% relative improvement for four Indo-European languages and about 15% for Turkish with a 4-gram language model for rescoring, in a 10-hour low-resource setup",
    "checked": true,
    "id": "db4eb16e11488479d28a750df53032f1c4a3362c",
    "semantic_title": "unisplice: universal cross-lingual data splicing for low-resource asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/glocker23_interspeech.html": {
    "title": "Allophant: Cross-lingual Phoneme Recognition with Articulatory Attributes",
    "volume": "main",
    "abstract": "This paper proposes Allophant, a multilingual phoneme recognizer. It requires only a phoneme inventory for cross-lingual transfer to a target language, allowing for low-resource recognition. The architecture combines a compositional phone embedding approach with individually supervised phonetic attribute classifiers in a multi-task architecture. We also introduce Allophoible, an extension of the PHOIBLE database. When combined with a distance based mapping approach for grapheme-to-phoneme outputs, it allows us to train on PHOIBLE inventories directly. By training and evaluating on 34 languages, we found that the addition of multi-task learning improves the model's capability of being applied to unseen phonemes and phoneme inventories. On supervised languages we achieve phoneme error rate improvements of 11 percentage points (pp.) compared to a baseline without multi-task learning. Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of 2.63 pp. over the baseline",
    "checked": true,
    "id": "340a65d3bd9d2e12edc2ee77041451cf41049409",
    "semantic_title": "allophant: cross-lingual phoneme recognition with articulatory attributes",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23_interspeech.html": {
    "title": "Phonetic-assisted Multi-Target Units Modeling for Improving Conformer-Transducer ASR system",
    "volume": "main",
    "abstract": "Exploiting effective target modeling units is very important and has always been a concern in end-to-end automatic speech recognition (ASR). In this work, we propose a phonetic-assisted multi-target units (PMU) modeling approach, to enhance the Conformer-Transducer ASR system in a progressive representation learning manner. Specifically, PMU first uses the pronunciation-assisted subword modeling (PASM) and byte pair encoding (BPE) to produce phonetic-induced and text-induced target units separately; Then, three new frameworks are investigated to enhance the acoustic encoder, including a basic PMU, a paraCTC and a paCTC, they integrate the PASM and BPE units at different levels for CTC and transducer multi-task training. Experiments on both LibriSpeech and accented ASR tasks show that, the proposed PMU significantly outperforms the conventional BPE, it reduces the WER of LibriSpeech clean, other, and six accented ASR testsets by relative 12.7%, 4.3% and 7.7%, respectively",
    "checked": true,
    "id": "276d8a49c459635693d63a5de26735b3d34a3e50",
    "semantic_title": "phonetic-assisted multi-target units modeling for improving conformer-transducer asr system",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rouditchenko23_interspeech.html": {
    "title": "Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages",
    "volume": "main",
    "abstract": "Recent models such as XLS-R and Whisper have made multilingual speech technologies more accessible by pre-training on audio from around 100 spoken languages each. However, there are thousands of spoken languages worldwide, and adapting to new languages is an important problem. In this work, we aim to understand which model adapts better to languages unseen during pre-training. We fine-tune both models on 13 unseen languages and 18 seen languages. Our results show that the number of hours seen per language and language family during pre-training is predictive of how the models compare, despite the significant differences in the pre-training methods",
    "checked": true,
    "id": "5f5dd6a960b61d978253856bbb487c81cee16ce3",
    "semantic_title": "comparison of multilingual self-supervised and weakly-supervised speech pre-training for adaptation to unseen languages",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ea_interspeech.html": {
    "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
    "volume": "main",
    "abstract": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models",
    "checked": true,
    "id": "100d625fcc926cbce2282197a920d589b3eae11c",
    "semantic_title": "distilxlsr: a light weight cross-lingual speech representation model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23b_interspeech.html": {
    "title": "Emotional Voice Conversion with Semi-Supervised Generative Modeling",
    "volume": "main",
    "abstract": "Emotional Voice Conversion (EVC) is a task that aims to convert the emotional state of speech from one to another while preserving the linguistic information and identity of the speaker. However, many studies are limited by the requirement for parallel speech data between different emotional patterns, which is not widely available in real-life applications. Furthermore, the annotation of emotional data is highly time-consuming and labor-intensive. To address these problems, in this paper, we propose SGEVC, a novel semi-supervised generative model for emotional voice conversion. This paper demonstrates that using as little as 1% supervised data is sufficient to achieve EVC. Experimental results show that our proposed model achieves state-of-the-art (SOTA) performance and consistently outperforms EVC baseline frameworks",
    "checked": true,
    "id": "27c4d28829b7c089f5c7e6e5fd79e5c865cd7eac",
    "semantic_title": "emotional voice conversion with semi-supervised generative modeling",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23d_interspeech.html": {
    "title": "Diff-HierVC: Diffusion-based Hierarchical Voice Conversion with Robust Pitch Generation and Masked Prior for Zero-shot Speaker Adaptation",
    "volume": "main",
    "abstract": "Although voice conversion (VC) systems have shown a remarkable ability to transfer voice style, existing methods still have an inaccurate pitch and low speaker adaptation quality. To address these challenges, we introduce Diff-HierVC, a hierarchical VC system based on two diffusion models. We first introduce DiffPitch, which can effectively generate F0 with the target voice style. Subsequently, the generated F0 is fed to DiffVoice to convert the speech with a target voice style. Furthermore, using the source-filter encoder, we disentangle the speech and use the converted Mel-spectrogram as a data-driven prior in DiffVoice to improve the voice style transfer capacity. Finally, by using the masked prior in diffusion models, our model can improve the speaker adaptation quality. Experimental results verify the superiority of our model in pitch generation and voice style transfer performance, and our model also achieves a CER of 0.83% and EER of 3.29% in zero-shot VC scenarios",
    "checked": true,
    "id": "69f1ed9c41db6ab3fd505c5cc6fdcc3ee1233b04",
    "semantic_title": "diff-hiervc: diffusion-based hierarchical voice conversion with robust pitch generation and masked prior for zero-shot speaker adaptation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23_interspeech.html": {
    "title": "S2CD: Self-heuristic Speaker Content Disentanglement for Any-to-Any Voice Conversion",
    "volume": "main",
    "abstract": "In this paper, we propose a Selfheuristic Speaker Content Disentanglement (S2CD) model for any to any voice conversion without using any external resources, e.g., speaker labels or vectors, linguistic models, and transcriptions. S2CD is built on the disentanglement sequential variational autoencoder (DSVAE), but improves DSVAE at the model architecture level from three perspectives. Specifically, we develop different structures for speaker and content encoders based on their underlying static/dynamic property. We further propose a generative graph, modelled by S2CD, so as to make S2CD well mimic the multi-speaker speech generation process. Finally, we propose a self-heuristic way to introduce bias to the prior modelling. Extensive empirical evaluations show the effectiveness of S2CD for any to any voice conversion",
    "checked": true,
    "id": "bc5ceb87dd6f77111ee1707ef71251915ce8ff89",
    "semantic_title": "s2cd: self-heuristic speaker content disentanglement for any-to-any voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23g_interspeech.html": {
    "title": "Flow-VAE VC: End-to-End Flow Framework with Contrastive Loss for Zero-shot Voice Conversion",
    "volume": "main",
    "abstract": "Voice conversion (VC) seeks to modify one speaker's voice to generate speech as if it came from another speaker. It is challenging especially when source and target speakers are unseen during training (zero-shot VC). Recent work in this area made progress with disentanglement methods that separate utterance content and speaker characteristics from speech audio recordings. However, these models either lack adequate disentanglement ability or rely on the use of a trained vocoder to reconstruct the speech from acoustic features. We propose Flow-VAE VC, which is an end-to-end system processing directly on the raw audio waveform for zero-shot tasks. Flow-VAE VC adopts a conditional Variational Autoencoder (VAE) with normalizing flows and an adversarial training process to improve the expressive power of generative modeling. Specifically, we learn context-invariant representations by applying frame-level contrastive loss to speech different augment samples. The experiments show that the proposed method achieves a decent performance on zero-shot voice conversion and significantly improves converted speech naturalness and speaker similarity. Readers can get the source code and listen to some audio samples on the demo webpage",
    "checked": true,
    "id": "0c24b4199b42557ff6b0b54c87281b58cbff464f",
    "semantic_title": "flow-vae vc: end-to-end flow framework with contrastive loss for zero-shot voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23s_interspeech.html": {
    "title": "Automatic Speech Disentanglement for Voice Conversion using Rank Module and Speech Augmentation",
    "volume": "main",
    "abstract": "Voice Conversion (VC) converts the voice of a source speech to that of a target while maintaining the source's content. Speech can be mainly decomposed into four components: content, timbre, rhythm and pitch. Unfortunately, most related works only take into account content and timbre, which results in less natural speech. Some recent works are able to disentangle speech into several components, but they require laborious bottleneck tuning or various hand-crafted features, each assumed to contain disentangled speech information. In this paper, we propose a VC model that can automatically disentangle speech into four components using only two augmentation functions, without the requirement of multiple hand-crafted features or laborious bottleneck tuning. The proposed model is straightforward yet efficient, and the empirical results demonstrate that our model can achieve a better performance than the baseline, regarding disentanglement effectiveness and speech naturalness",
    "checked": true,
    "id": "76933d7de40927127a313911d97f2061de36ea69",
    "semantic_title": "automatic speech disentanglement for voice conversion using rank module and speech augmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23b_interspeech.html": {
    "title": "End-to-End Zero-Shot Voice Conversion with Location-Variable Convolutions",
    "volume": "main",
    "abstract": "Zero-shot voice conversion is becoming an increasingly popular research topic, as it promises the ability to transform speech to sound like any speaker. However, relatively little work has been done on end-to-end methods for this task, which are appealing because they remove the need for a separate vocoder to generate audio from intermediate features. In this work, we propose LVC-VC, an end-to-end zero-shot voice conversion model that uses location-variable convolutions (LVCs) to jointly model the conversion and speech synthesis processes. LVC-VC utilizes carefully designed input features that have disentangled content and speaker information, and it uses a neural vocoder-like architecture that utilizes LVCs to efficiently combine them and perform voice conversion while directly synthesizing time domain audio. Experiments show that our model achieves especially well balanced performance between voice style transfer and speech intelligibility compared to several baselines",
    "checked": true,
    "id": "5ee405675554a192ee452b3bf9d12b65fcbd582a",
    "semantic_title": "end-to-end zero-shot voice conversion with location-variable convolutions",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/braun23_interspeech.html": {
    "title": "Classifying Dementia in the Presence of Depression: A Cross-Corpus Study",
    "volume": "main",
    "abstract": "Automated dementia screening enables early detection and intervention, reducing costs to healthcare systems and increasing quality of life for those affected. Depression has shared symptoms with dementia, adding complexity to diagnoses. The research focus so far has been on binary classification of dementia (DEM) and healthy controls (HC) using speech from picture description tests from a single dataset. In this work, we apply established baseline systems to discriminate cognitive impairment in speech from the semantic Verbal Fluency Test and the Boston Naming Test using text, audio and emotion embeddings in a 3-class classification problem (HC vs. MCI vs. DEM). We perform cross-corpus and mixed-corpus experiments on two independently recorded German datasets to investigate generalization to larger populations and different recording conditions. In a detailed error analysis, we look at depression as a secondary diagnosis to understand what our classifiers actually learn",
    "checked": true,
    "id": "0404aebc2dda1b968ea12ccc8d77b4ba829fd954",
    "semantic_title": "classifying dementia in the presence of depression: a cross-corpus study",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23b_interspeech.html": {
    "title": "Exploiting Cross-Domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "Articulatory features (AFs) are inherently invariant to acoustic signal distortion. Their practical application to atypical domains such as elderly, disordered speech across languages is limited by data scarcity. This paper presents a cross-domain and cross-lingual Acoustic-to-Articulatory (A2A) inversion approach that utilizes the parallel audio and ultrasound tongue imaging (UTI) data of the 24-hour TaL corpus in A2A model training before being adapted to three datasets: the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora; and the English TORGO dysarthric speech data, to produce UTI based AFs. Experiments suggest incorporating the generated AFs consistently outperforms the baseline TDNN/Conformer ASR systems using acoustic features only by statistically significant word/character error rate reductions up to 4.75%, 2.59% and 2.07% absolute (14.69%, 10.64% and 22.72% relative) after data augmentation, speaker adaptation and cross system multi-pass decoding",
    "checked": true,
    "id": "72a6db85488881e312f1d366fd670f326567b5c7",
    "semantic_title": "exploiting cross-domain and cross-lingual ultrasound tongue imaging features for elderly and dysarthric speech recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wagner23_interspeech.html": {
    "title": "Multi-class Detection of Pathological Speech with Latent Features: How does it perform on unseen data?",
    "volume": "main",
    "abstract": "The detection of pathologies from speech features is usually defined as a binary classification task with one class representing a specific pathology and the other class representing healthy speech. In this work, we train neural networks, large margin classifiers, and tree boosting machines to distinguish between four pathologies: Parkinson's disease, laryngeal cancer, cleft lip and palate, and oral squamous cell carcinoma. We show that latent representations extracted at different layers of a pre-trained wav2vec 2.0 system can be effectively used to classify these types of pathological voices. We evaluate the robustness of our classifiers by adding room impulse responses to the test data and by applying them to unseen speech corpora. Our approach achieves unweighted average F1-Scores between 74.1% and 97.0%, depending on the model and the noise conditions used. The systems generalize and perform well on unseen data of healthy speakers sampled from a variety of different sources",
    "checked": true,
    "id": "d2b4d7208dd5c564262801ff90e64dd99c7d2324",
    "semantic_title": "multi-class detection of pathological speech with latent features: how does it perform on unseen data?",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kothare23_interspeech.html": {
    "title": "Responsiveness, Sensitivity and Clinical Utility of Timing-Related Speech Biomarkers for Remote Monitoring of ALS Disease Progression",
    "volume": "main",
    "abstract": "In this study, we describe the responsiveness of timing-related measures extracted from read speech in persons with ALS (pALS) collected via a remote patient monitoring platform in an effort to quantify how long it takes to detect a clinically-meaningful change associated with disease progression. We found that the timing alignment of pALS speech relative to a canonical elicitation of the same prompt is the most responsive measure, of the ones considered in this study, at detecting such change in both pALS with bulbar (n = 35) and non-bulbar onset (n = 94). We further evaluated the sensitivity of speech metrics in tracking disease progression in pALS while their ALSFRS-R speech score remained unchanged at 3 out of a total possible score of 4. We observed that timing-related speech metrics showed significant longitudinal changes even after accounting for learning effects. The findings of this study have the potential to inform disease prognosis and functional outcomes of clinical trials",
    "checked": true,
    "id": "164985ced1ba9de4f05b103ce0e93ac4da17635a",
    "semantic_title": "responsiveness, sensitivity and clinical utility of timing-related speech biomarkers for remote monitoring of als disease progression",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geng23b_interspeech.html": {
    "title": "Use of Speech Impairment Severity for Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "A key challenge in dysarthric speech recognition is the speaker-level diversity attributed to both speaker-identity related factors (e.g. gender) and speech impairment severity. Most prior researches on addressing this issue focused on using speaker-identity only. To this end, this paper proposes a novel set of techniques to use both severity and speaker-identity in dysarthric speech recognition: a) multitask training incorporating severity prediction error; b) speaker-severity aware auxiliary feature adaptation; and c) structured LHUC transforms separately conditioned on speaker-identity and severity. Experiments conducted on UASpeech suggest incorporating additional speech impairment severity into state-of-the-art hybrid DNN, E2E Conformer and pre-trained Wav2vec 2.0 ASR systems produced statistically significant WER reductions up to 4.78% (14.03% relative). Using the best system the lowest published WER of 17.82% (51.25% on very low intelligibility) was obtained on UASpeech",
    "checked": true,
    "id": "9f4844c79226025f5bb61f2ee62fca8ffdaf3cce",
    "semantic_title": "use of speech impairment severity for dysarthric speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mosuily23_interspeech.html": {
    "title": "MMLung: Moving Closer to Practical Lung Health Estimation using Smartphones",
    "volume": "main",
    "abstract": "Long-term respiratory illnesses like Chronic Obstructive Pulmonary Disease (COPD) and Asthma are commonly diagnosed with the gold standard spirometry, which is a lung health test that requires specialized equipment and trained healthcare experts, making it expensive and difficult to scale. Moreover, blowing into a spirometer can be quite hard for people suffering from pulmonary illnesses. To solve the aforementioned limitations, we introduce MMLung, an approach that leverages information obtained from multiple audio signals by combining multiple tasks and different modalities performed on the microphone of a smartphone to estimate lung function. Our proposed approach achieves the best mean absolute percentage error (MAPE) of 1.3% on a cohort of 40 participants. Compared to the reported performances (5%-10% MAPE) on lung health estimation using smartphones, MMLung shows that practical lung health estimation is viable by combining multiple tasks utilizing multiple modalities",
    "checked": true,
    "id": "dff184f8b52489c6d53fefca7dfa4e739ba5ec68",
    "semantic_title": "mmlung: moving closer to practical lung health estimation using smartphones",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23i_interspeech.html": {
    "title": "Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization",
    "volume": "main",
    "abstract": "Large-scale pre-training has been a successful strategy for training transformer models. However, maintaining a large clinical dataset for pre-training is not always possible, and access to data in this domain can be time-limited and costly. We explore using synthetic data in pre-training sequence-to-sequence (seq-to-seq) transformer models to generate clinical notes from Doctor-Patient-Conversations (DoPaCos). Using a generative language model fine-tuned on authentic conversations, a synthetic DoPaCo dataset was created and used with a corpus of clinical notes to pre-train a Longformer-Encoder-Decoder (LED) model. Results show that synthetic data leads to comparable performance in the downstream summarization task compared to pre-training with authentic data. Pre-training on synthetic conversations first, followed by clinical notes, yields higher performance across most of our evaluation metrics",
    "checked": true,
    "id": "bad2eef437f08e7c67e9455134327328e5a3e528",
    "semantic_title": "investigating the utility of synthetic data for doctor-patient conversation summarization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23pa_interspeech.html": {
    "title": "Non-uniform Speaker Disentanglement For Depression Detection From Raw Speech Signals",
    "volume": "main",
    "abstract": "While speech-based depression detection methods that use speaker-identity features, such as speaker embeddings, are popular, they often compromise patient privacy. To address this issue, we propose a speaker disentanglement method that utilizes a non-uniform mechanism of adversarial SID loss maximization. This is achieved by varying the adversarial weight between different layers of a model during training. We find that a greater adversarial weight for the initial layers leads to performance improvement. Our approach using the ECAPA-TDNN model achieves an F1-score of 0.7349 (a 3.7% improvement over audio-only SOTA) on the DAIC-WoZ dataset, while simultaneously reducing the speaker-identification accuracy by 50%. Our findings suggest that identifying depression through speech signals can be accomplished without placing undue reliance on a speaker's identity, paving the way for privacy-preserving approaches of depression detection",
    "checked": true,
    "id": "67fb87e9475118e09863291c34241823b701f49a",
    "semantic_title": "non-uniform speaker disentanglement for depression detection from raw speech signals",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/demir23_interspeech.html": {
    "title": "PoCaPNet: A Novel Approach for Surgical Phase Recognition Using Speech and X-Ray Images",
    "volume": "main",
    "abstract": "Surgical phase recognition is a challenging and necessary task for the development of context-aware intelligent systems that can support medical personnel for better patient care and effective operating room management. In this paper, we present a surgical phase recognition framework that employs a Multi-Stage Temporal Convolution Network using speech and X-Ray images for the first time. We evaluate our proposed approach using our dataset that comprises 31 port-catheter placement operations and report 82.56 % frame-wise accuracy with eight surgical phases. Additionally, we investigate the design choices in the temporal model and solutions for the class-imbalance problem. Our experiments demonstrate that speech and X-Ray data can be effectively utilized for surgical phase recognition, providing a foundation for the development of speech assistants in operating rooms of the future",
    "checked": true,
    "id": "e3769ece1c4fcc201aa455b8e1f4ecd85399bf70",
    "semantic_title": "pocapnet: a novel approach for surgical phase recognition using speech and x-ray images",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/neumann23_interspeech.html": {
    "title": "Combining Multiple Multimodal Speech Features into an Interpretable Index Score for Capturing Disease Progression in Amyotrophic Lateral Sclerosis",
    "volume": "main",
    "abstract": "Multiple speech biomarkers have been shown to carry useful information regarding Amyotrophic Lateral Sclerosis (ALS) pathology. We propose a two-step framework to compute optimal linear combinations (indexes) of these biomarkers that are more discriminative and noise-robust than the individual markers, which is important for clinical care and pharmaceutical trial applications. First, we use a hierarchical clustering based method to select representative speech metrics from a dataset comprising 143 people with ALS and 135 age- and sex-matched healthy controls. Second, we analyze three methods of index computation that optimize linear discriminability, Youden Index, and sparsity of logistic regression model weights, respectively, and evaluate their performance with 5-fold cross validation. We find that the proposed indexes are generally more discriminative of bulbar vs non-bulbar onset in ALS than their individual component metrics as well as an equally-weighted baseline",
    "checked": true,
    "id": "774ed9112b2c4fa7b64c85e42a132a25742d2513",
    "semantic_title": "combining multiple multimodal speech features into an interpretable index score for capturing disease progression in amyotrophic lateral sclerosis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mallolragolta23_interspeech.html": {
    "title": "The MASCFLICHT Corpus: Face Mask Type and Coverage Area Recognition from Speech",
    "volume": "main",
    "abstract": "We present a novel speech dataset for face mask type and coverage area recognition collected with a smartphone. The dataset contains 2h 27m 55s of data from 30 German speakers (15f, 15m). The baseline results exploit the functionals of the eGeMAPS feature set, the Mel-spectrogram, and the spectrogram representations of the audio samples. To model the one-dimensional features, we investigate Support Vector Classifiers (SVC) and a neural network classifier. We extract salient information from the two-dimensional representations with Convolutional Neural Network (CNN) based encoders, coupled with a classification block. We use the Unweighted Average Recall (UAR) as the evaluation metric. For the face mask type and the coverage area recognition tasks (3-class problems), the best models on the test partition score a UAR of 49.3% and 47.8%, respectively. For the face mask type and coverage area recognition task (5-class problem), the optimal model on the test partition obtains a UAR of 35.0%",
    "checked": true,
    "id": "2e610972854ef00908769f11d1e8014178f6d657",
    "semantic_title": "the mascflicht corpus: face mask type and coverage area recognition from speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/botelho23_interspeech.html": {
    "title": "Towards Reference Speech Characterization for Health Applications",
    "volume": "main",
    "abstract": "Speech has been used as a biomarker for the binary classification of multiple diseases, with promising results. However these speech affecting diseases often co-exist in the same individual and produce similar manifestations in the speech signal. Thus we propose to characterize normative speech using reference intervals for interpretable speech features (acoustic and linguistic), as a first step towards the adoption of speech analysis for multidisease screening in health applications. We discuss the impact of demographics and speech tasks. Finally, we compare the reference intervals with subjects suffering from Parkinson's disease, Alzheimer's disease and depression",
    "checked": true,
    "id": "9c836950492d3c43e17c1eb11020298e7a73a992",
    "semantic_title": "towards reference speech characterization for health applications",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/riosurrego23_interspeech.html": {
    "title": "Automatic Classification of Hypokinetic and Hyperkinetic Dysarthria based on GMM-Supervectors",
    "volume": "main",
    "abstract": "Hypokinetic and hyperkinetic dysarthria are motor speech disorders that appear in patients with Parkinson's and Huntington's disease, respectively. They are caused due to progressive lesions or alterations in the basal ganglia. In particular, Huntington's disease (HD) is known to be more invasive and difficult to treat than Parkinson's disease (PD), producing more aggressive motor and cognitive alterations. Since speech production requires the movement and control of many different muscles and limbs, it constitutes a highly complex motor activity that may reflect relevant aspects of the patient's health state. This paper proposes the discrimination between patients with PD, HD, and healthy controls (HC) based on different speech dimensions. Speaker models based on Gaussian-mixture model supervectors are created with the features extracted from each speech dimension. The results suggest that it is possible to distinguish between PD and HD patients using the supervectors-based approach",
    "checked": true,
    "id": "f5514993b1c5bc25a7ff4a55117f221fbe0dd464",
    "semantic_title": "automatic classification of hypokinetic and hyperkinetic dysarthria based on gmm-supervectors",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dineley23_interspeech.html": {
    "title": "Towards robust paralinguistic assessment for real-world mobile health (mHealth) monitoring: an initial study of reverberation effects on speech",
    "volume": "main",
    "abstract": "Speech is promising as an objective, convenient tool to monitor health remotely over time using mobile devices. Numerous paralinguistic features have been demonstrated to contain salient information related to an individual's health. However, mobile device specification and acoustic environments vary widely, risking the reliability of the extracted features. In an initial step towards quantifying these effects, we report the variability of 13 exemplar paralinguistic features commonly reported in the speech-health literature and extracted from the speech of 42 healthy volunteers recorded consecutively in rooms with low and high reverberation with one budget and two higher-end smartphones, and a condenser microphone. Our results show reverberation has a clear effect on several features, in particular voice quality markers. Our findings point to new research directions investigating how best to record and process in-the-wild speech for reliable longitudinal mobile health state assessment",
    "checked": true,
    "id": "e1cad4e1650b0f7b55e5255f0d97cb36da9a712b",
    "semantic_title": "towards robust paralinguistic assessment for real-world mobile health (mhealth) monitoring: an initial study of reverberation effects on speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simmatis23_interspeech.html": {
    "title": "Multimodal Assessment of Bulbar Amyotrophic Lateral Sclerosis (ALS) Using a Novel Remote Speech Assessment App",
    "volume": "main",
    "abstract": "Speech is a valuable marker of disease onset and progression in amyotrophic lateral sclerosis (ALS). Acoustic and kinematic data have characterized speech impairments in ALS previously, and there is growing interest in combining these modalities in novel analytical platforms. We explored the use of a multimodal (audio/video) speech assessment pipeline in ALS patients with varying severities. Participants performed a passage reading task, and clinical outcomes of e.g., speech function were collected. Speech data were analyzed using a custom automated acoustic and kinematic pipeline. Sparse canonical correlation analysis (SCCA) was then used. Both acoustic and kinematic features loaded strongly with clinical data (loadings ≥|0.50|), indicating that multimodal features captured complementary speech function information. This reinforces the value of multimodal assessment techniques and points the way towards future remote assessment development steps",
    "checked": true,
    "id": "1b727de8e19ed1a2fbdbdc7d2e02c555e5c61696",
    "semantic_title": "multimodal assessment of bulbar amyotrophic lateral sclerosis (als) using a novel remote speech assessment app",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martinez23_interspeech.html": {
    "title": "On the Use of High Frequency Information for Voice Pathology Classification",
    "volume": "main",
    "abstract": "We have observed significant differences in the high frequency content of the spectrum between healthy and pathological voices. Pathologies like larynx cancer, vocal fold lesions, and patients with larynx or vocal fold removal are examples of this. This finding invites to use high sampling frequencies in voice pathology classification systems to benefit from this high frequency information, which has been traditionally ignored. With a GMM classifier fed with MFCCs and a sampling frquency of 48 kHz we are able to improve AUC almost a 5% compared to a system using a sampling frequency of 8 kHz and more than 2% compared to a system using a sampling frequency of 16 kHz",
    "checked": true,
    "id": "c1c33c4f240aa74462a7635c574e74555a6ee2f5",
    "semantic_title": "on the use of high frequency information for voice pathology classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/favaro23_interspeech.html": {
    "title": "Do Phonatory Features Display Robustness to Characterize Parkinsonian Speech Across Corpora?",
    "volume": "main",
    "abstract": "Sustained vowels have been largely used to quantify vocal impairment in Parkinson's disease (PD), with most studies focusing on a single corpus. Presumably, features obtained from sustained vowels are language-independent, but how findings generalize across cohorts is unclear. This work analyzes 61 phonatory features from 5 corpora in American English, Italian, Castilian Spanish, Colombian Spanish, and German, respectively, by conducting a statistical and correlation analysis. We use robustness as a criterion in which a feature displays the same behavior across corpora. The statistical analysis showed that the features provided good separability between PD and controls in only two out of five corpora, and none of the features displayed robustness. However, experiments report significant correlations between feature values and clinical scores. These findings provide valuable insights into the acoustic corpora-based dissimilarities, which should be considered when generalizing findings",
    "checked": true,
    "id": "562d06b0cddb553a76e6b68f6f2ba470a17bb5d4",
    "semantic_title": "do phonatory features display robustness to characterize parkinsonian speech across corpora?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kadiri23_interspeech.html": {
    "title": "Severity Classification of Parkinson's Disease from Speech using Single Frequency Filtering-based Features",
    "volume": "main",
    "abstract": "Developing objective methods for assessing the severity of Parkinson's disease (PD) is crucial for improving the diagnosis and treatment. This study proposes two sets of novel features derived from the single frequency filtering (SFF) method: (1) SFF cepstral coefficients (SFFCC) and (2) MFCCs from the SFF (MFCC-SFF) for the severity classification of PD. Prior studies have demonstrated that SFF offers greater spectro-temporal resolution compared to the short-time Fourier transform. The study uses the PC-GITA database, which includes speech of PD patients and healthy controls produced in three speaking tasks (vowels, sentences, text reading). Experiments using the SVM classifier revealed that the proposed features outperformed the conventional MFCCs in all three speaking tasks. The proposed SFFCC and MFCC-SFF features gave a relative improvement of 5.8% & 2.3% for the vowel task, 7.0% & 1.8% for the sentence task, and 2.4% & 1.1% for the read text task, in comparison to MFCC features",
    "checked": true,
    "id": "75ce6a82935bf18bc9beb1b667306520cd3ed6e2",
    "semantic_title": "severity classification of parkinson's disease from speech using single frequency filtering-based features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simek23_interspeech.html": {
    "title": "Comparison of acoustic measures of dysphonia in Parkinson's disease and Huntington's disease: Effect of sex and speaking task",
    "volume": "main",
    "abstract": "This study investigated whether voice quality is differentially affected in two distinct basal ganglia disorders causing hypokinetic and hyperkinetic dysarthria, including effects of gender and speaking task. The sustained vowel phonations and monologues of 40 de novo Parkinson's disease (PD) patients, 40 Huntington's disease (HD) patients, and 40 healthy control participants were evaluated. Using cepstral peak prominence extracted from sustained phonation, differences from controls were found for male and female HD patients (p < 0.05) but only male PD patients (p < 0.05). Using the glottal-to-noise excitation ratio obtained from monologue, differences from controls were detected for male and female PD groups (p < 0. 05) but only male HD group (p < 0.05). In general, female patients show better voice quality. Our findings highlight that selecting suitable acoustic measures and speaking material is essential for adequate evaluation of dysphonia severity across differing etiologies",
    "checked": true,
    "id": "826bed7d291ca97822d48c425a50d245f3e89136",
    "semantic_title": "comparison of acoustic measures of dysphonia in parkinson's disease and huntington's disease: effect of sex and speaking task",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gomezzaragoza23_interspeech.html": {
    "title": "Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses",
    "volume": "main",
    "abstract": "Alzheimer's Disease (AD) is the world's leading neurodegenerative disease, which often results in communication difficulties. Analysing speech can serve as a diagnostic tool for identifying the condition. The recent ADReSS challenge provided a dataset for AD classification and highlighted the utility of manual transcriptions. In this study, we used the new state-of-the-art Automatic Speech Recognition (ASR) model Whisper to obtain the transcriptions, which also include automatic punctuation. The classification models achieved test accuracy scores of 0.854 and 0.833 combining the pretrained FastText word embeddings and recurrent neural networks on manual and ASR transcripts respectively. Additionally, we explored the influence of including pause information and punctuation in the transcriptions. We found that punctuation only yielded minor improvements in some cases, whereas pause encoding aided AD classification for both manual and ASR transcriptions across all approaches investigated",
    "checked": true,
    "id": "9b13b17f817de5f6fd315d3a113edd1562b9e2ff",
    "semantic_title": "alzheimer disease classification through asr-based transcriptions: exploring the impact of punctuation and pauses",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23c_interspeech.html": {
    "title": "LanSER: Language-Model Supported Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) models typically rely on costly human-labeled data for training, making scaling methods to large speech datasets and nuanced emotion taxonomies difficult. We present LanSER, a method that enables the use of unlabeled data by inferring weak emotion labels via pre-trained large language models through weakly-supervised learning. For inferring weak labels constrained to a taxonomy, we use a textual entailment approach that selects an emotion label with the highest entailment score for a speech transcript extracted via automatic speech recognition. Our experimental results show that models pre-trained on large datasets with this weak supervision outperform other baseline models on standard SER datasets when fine-tuned, and show improved label efficiency. Despite being pre-trained on labels derived only from text, we show that the resulting representations appear to model the prosodic content of speech",
    "checked": true,
    "id": "74381622d5931fb073deccf758d37cf45e41820f",
    "semantic_title": "lanser: language-model supported speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23_interspeech.html": {
    "title": "Fine-tuned RoBERTa Model with a CNN-LSTM Network for Conversational Emotion Recognition",
    "volume": "main",
    "abstract": "Textual emotion recognition in conversations has gained increasing attention in recent years for the growing amount of applications it can serve, e.g., human-robot interactions, recommended systems. However, most existing approaches are either based on BERT-based model which fail to exploit crucial information about the long-text context, or resort to complex entanglement of neural network architectures resulting in less stable training procedures and slower inference time. To bridge this gap, we first propose a fast, compact and parameter-efficient framework based on fine-tuned pre-trained RoBERTa model with a CNN-LSTM network for textual emotion recognition in conversations. First, we fine-tune the pre-tranined RoBERTa model to effectively learn long-term emotion-relevant context information. Second, convolutional neural network coupled with the bidirectional long short-term memory and joint reinforced blocks are utilized to recognize emotion in conversations. Extensive experiments are conducted on benchmark emotion MELD dataset, and the results show that our model outperforms a wide range of strong baselines and achieves competitive results with the state-of-art approaches",
    "checked": true,
    "id": "8a2bb15380530066cd385b2f94cc7c98f1547656",
    "semantic_title": "fine-tuned roberta model with a cnn-lstm network for conversational emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/stanley23_interspeech.html": {
    "title": "Emotion Label Encoding Using Word Embeddings for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) is an important and challenging task for human-computer interaction. Human emotions are complex and nuanced, hence difficult to represent. The standard representations of emotions, categorical or continuous, tend to oversimplify the problem. Recently, the label encoding approach has been proposed, where vectors are used to represent the emotion space. In this paper, we hypothesise that using a pre-existing vector space that encodes semantic information about emotion is beneficial for the task. To this aim, we propose using word embeddings obtained from a Language Model (LM) as labels for SER. We evaluate the performance of the proposed approach on the IEMOCAP corpus and show that it yields better performance than a standard baseline. We also present a method to combine free text labels, which are unusable in conventional approaches, and by doing so we show that the model can learn more nuanced representations of emotions",
    "checked": true,
    "id": "7f3a77502d719ad6ba275d6951868cc220518f49",
    "semantic_title": "emotion label encoding using word embeddings for speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ia_interspeech.html": {
    "title": "Discrimination of the Different Intents Carried by the Same Text Through Integrating Multimodal Information",
    "volume": "main",
    "abstract": "Many intent understanding studies neglect the impact of paralinguistic information, resulting in misunderstandings during speech interactions, particularly when different intentions are conveyed by the same text with varying paralinguistic information. To address this issue, this study developed a Chinese multimodal spoken language intention understanding dataset that features different spoken intentions for identical texts. Our proposed attention-based BiLSTM model integrates textual and acoustic features and introduces an acoustic information gate mechanism to supplement or correct linguistic intention with paralinguistic intention. Experimental results demonstrate that our multimodal integration model improves intent discrimination accuracy by 11.0% compared to models that incorporate only linguistic information. The result highlights the effectiveness of our proposed model for intent discrimination, particularly in cases with identical text but varying intentions",
    "checked": true,
    "id": "fc54740746ceb8e98f9d2c659b7d2e5673f19432",
    "semantic_title": "discrimination of the different intents carried by the same text through integrating multimodal information",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23f_interspeech.html": {
    "title": "Meta-domain Adversarial Contrastive Learning for Alleviating Individual Bias in Self-sentiment Predictions",
    "volume": "main",
    "abstract": "Self-sentiment provides direct feedback from users and is vital in accurately evaluating and improving the quality of dialogue systems. However, few studies focus on self-sentiment prediction, and the works on third-party sentiment prediction suffer from two problems when predicting self-sentiments: Self-sentiment annotations are labeled by the speakers themselves, leading to solid individual bias in annotations and a sub-optimal prediction; The hardness of collecting sufficient data with self-sentiment annotations limits the size of the data, resulting in the overlapping problem. This work hence proposes a novel meta-learning domain adversarial contrastive neural network (MetaDACNN) that extracts user-shared prior knowledge and learns user-specific classifiers to handle individual bias and to alleviate overfitting. Experimental results on two public datasets show that MetaDACNN improves the prediction performance and alleviates individual bias compared to state-of-the-art models",
    "checked": true,
    "id": "da83dfcddcd6729e40947c821a34ec613f3d80a5",
    "semantic_title": "meta-domain adversarial contrastive learning for alleviating individual bias in self-sentiment predictions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23b_interspeech.html": {
    "title": "SWRR: Feature Map Classifier Based on Sliding Window Attention and High-Response Feature Reuse for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "To achieve efficient feature fusion, existing research tends to employ cross-attention to control the contributions of different modalities in fusion. However, this inevitably causes high computational effort and introduces noise weights due to redundant computations. Therefore, this paper proposes sliding window attention (SliWa) to control the feature perception range and dynamically model the modality fusion at different granularities. In addition, we present a novel feature map classifier (FMC) based on high-response feature reuse (HRFR), which explicitly preserves the deep emotional feature structure, thus preventing the submersion of the crucial classification information after average flattening and the negative impacts of parameter flooding. We unify the mentioned modules in the SWRR framework, and the experimental results on the commonly used datasets IEMOCAP and CMU-MOSEI reveal the effectiveness of SWRR in improving the performance of emotion recognition",
    "checked": true,
    "id": "9439dccba5ad90d30cee18785b5cd03bc9a43990",
    "semantic_title": "swrr: feature map classifier based on sliding window attention and high-response feature reuse for multimodal emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23f_interspeech.html": {
    "title": "PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNN) and Transformer have wildly succeeded in multimedia applications. However, more effort needs to be made to harmonize these two architectures effectively to satisfy speech enhancement. This paper aims to unify these two architectures and presents a Parallel Conformer for speech enhancement. In particular, the CNN and the self-attention (SA) in the Transformer are fully exploited for local format patterns and global structure representations. Based on the small receptive field size of CNN and the high computational complexity of SA, we specially designed a multi-branch dilated convolution (MBDC) and a self-channel-time-frequency attention (Self-CTFA) module. MBDC contains three convolutional layers with different dilation rates for the feature from local to non-local processing. Experimental results show that our method performs better than state-of-the-art methods in most evaluation criteria while maintaining the lowest model parameters",
    "checked": true,
    "id": "05a4de246391946ec9cb4e59c18c9268f0a64c1e",
    "semantic_title": "pcnn: a lightweight parallel conformer neural network for efficient monaural speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23b_interspeech.html": {
    "title": "Exploring the Interactions Between Target Positive and Negative Information for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "Acoustic echo cancellation (AEC) aims to remove interference signals while leaving near-end speech least distorted. As the indistinguishable patterns between near-end speech and interference signals, near-end speech can't be separated completely, causing speech distortion and interference signals residual. We observe that besides target positive information, e.g., ground-truth speech and features, the target negative information, such as interference signals and features, helps make pattern of target speech and interference signals more discriminative. Therefore, we present a novel AEC model encoder-decoder architecture with the guidance of negative information termed as CMNet. A collaboration module (CM) is designed to establish the correlation between the target positive and negative information in a learnable manner via three blocks: target-positive, target-negative, and interactive block. Experimental results demonstrate our CMNet achieves superior performance than recent methods",
    "checked": true,
    "id": "745280a794d1a53f167830d0f742db62554a289e",
    "semantic_title": "exploring the interactions between target positive and negative information for acoustic echo cancellation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/andreev23_interspeech.html": {
    "title": "Iterative autoregression: a novel trick to improve your low-latency speech enhancement model",
    "volume": "main",
    "abstract": "Streaming models are an essential component of real-time speech enhancement tools. The streaming regime constrains speech enhancement models to use only a tiny context of future information. As a result, the low-latency streaming setup is generally considered a challenging task and has a significant negative impact on the model's quality. However, the sequential nature of streaming generation offers a natural possibility for autoregression, that is, utilizing previous predictions while making current ones. The conventional method for training autoregressive models is teacher forcing, but its primary drawback lies in the training-inference mismatch that can lead to a substantial degradation in quality. In this study, we propose a straightforward yet effective alternative technique for training autoregressive low-latency speech enhancement models. We demonstrate that the proposed approach leads to stable improvement across diverse architectures and training scenarios",
    "checked": true,
    "id": "27ce54919d7b81f6f949ee43180e33132c3eb5d5",
    "semantic_title": "iterative autoregression: a novel trick to improve your low-latency speech enhancement model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ku23_interspeech.html": {
    "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
    "volume": "main",
    "abstract": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer to build a new model with a small footprint that also achieve good performances. We explore several S4-based deep architectures in both time (T) and time-frequency (TF) domains. The 2-D S4 layer can be thought of as a special convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18",
    "checked": true,
    "id": "154d5cf9a40d313cdf62372621f554a809fbb103",
    "semantic_title": "a multi-dimensional deep structured state space approach to speech enhancement using small-footprint models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/frenkel23_interspeech.html": {
    "title": "Domain Adaptation for Speech Enhancement in a Large Domain Gap",
    "volume": "main",
    "abstract": "Speech enhancement approaches based on neural networks, aim to learn a noisy-to-clean transformation using a supervised learning paradigm. However, networks trained in this way may not be effective at handling languages and types of noise that were not present in the training data. To address this issue, this study focuses on unsupervised domain adaptation, specifically for large-domain-gap cases. In this setup, we have noisy speech data from the new domain but the corresponding clean speech data are not available. We propose an adaptation method that is based on domain-adversarial training followed by iterative self-training where the quality of the estimated speech used as pseudo labels is monitored by the performance of the adapted network on labeled data from the source domain. Experimental results show that our method effectively mitigates the domain mismatch between training and test sets, and surpasses the current baseline",
    "checked": true,
    "id": "fc5424fe6021a9554e00ee3f8214f645521d171a",
    "semantic_title": "domain adaptation for speech enhancement in a large domain gap",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zadorozhnyy23_interspeech.html": {
    "title": "SCP-GAN: Self-Correcting Discriminator Optimization for Training Consistency Preserving Metric GAN on Speech Enhancement Tasks",
    "volume": "main",
    "abstract": "In recent years, Generative Adversarial Networks (GANs) have produced significantly improved speech enhancement (SE) task results. However, they are challenging to train. In this work, we introduce several improvements to GAN training schemes, which can be applied to most GAN-based SE models. We propose using consistency loss functions, which target the inconsistency in time and time-frequency domains caused by Fourier and Inverse Fourier Transforms. We also present self-correcting optimization for training a GAN discriminator on SE tasks which helps avoid \"harmful\" training directions for parts of the discriminator loss function. We have tested our proposed methods on several state-of-the-art GAN-based SE models and obtained consistent improvements, including new state-of-the-art results for the Voice Bank+DEMAND dataset",
    "checked": true,
    "id": "6d644c54b07dcb53acf977b796c49cf27b698c38",
    "semantic_title": "scp-gan: self-correcting discriminator optimization for training consistency preserving metric gan on speech enhancement tasks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23c_interspeech.html": {
    "title": "A Mask Free Neural Network for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "In speech enhancement, the lack of clear structural characteristics in the target speech phase requires the use of conservative and cumbersome network frameworks. It seems difficult to achieve competitive performance using direct methods and simple network architectures. However, we propose the MFNet, a direct and simple network that can not only map speech but also map reverse noise. This network is constructed by stacking global local former blocks (GLFBs), which combine the advantages of Mobileblock for global processing and Metaformer architecture for local interaction. Our experimental results demonstrate that our network using mapping method outperforms masking methods, and direct mapping of reverse noise is the optimal solution in strong noise environments. In a horizontal comparison on the 2020 Deep Noise Suppression (DNS) challenge test set without reverberation, to the best of our knowledge, MFNet is the current state-of-the-art (SOTA) mapping model",
    "checked": true,
    "id": "8bbcf7df910cd6eacf92168ec81220fd62fcf21b",
    "semantic_title": "a mask free neural network for monaural speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23p_interspeech.html": {
    "title": "A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech",
    "volume": "main",
    "abstract": "The lack of clean speech is a practical challenge to the development of speech enhancement systems, which means that there is an inevitable mismatch between their training criterion and evaluation metric. In response to this unfavorable situation, we propose a training and inference strategy that additionally uses enhanced speech as a target by improving the previously proposed noisy-target training (NyTT). Because homogeneity between in-domain noise and extraneous noise is the key to the effectiveness of NyTT, we train various student models by remixing 1) the teacher model's estimated speech and noise for enhanced-target training or 2) raw noisy speech and the teacher model's estimated noise for noisy-target training. Experimental results show that our proposed method outperforms several baselines, especially with the teacher/student inference, where predicted clean speech is derived successively through the teacher and final student models",
    "checked": true,
    "id": "e685129b7dac8f2b460ec7f024f4fbdeb8be069f",
    "semantic_title": "a training and inference strategy using noisy and enhanced speech as target for speech enhancement without clean speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pandey23b_interspeech.html": {
    "title": "A Simple RNN Model for Lightweight, Low-compute and Low-latency Multichannel Speech Enhancement in the Time Domain",
    "volume": "main",
    "abstract": "Deep learning has led to unprecedented advances in speech enhancement. However, deep neural networks (DNNs) typically require large amount of computation, memory, signal buffer and processing time to achieve strong performance. Designing a DNN to meet a given resource constraint requires dedicated efforts. This study proposes a novel recurrent neural network (RNN) based model for time-domain multichannel speech enhancement that can be easily tuned to meet a given constraint. We present results of training the model at different scales, where algorithmic latency varies from 1 ms to 16 ms, model size varies from 100 Thousand to 25 Million parameters, and compute to process one second of speech varies from 100 Mega to 25 Giga multiply-accumulates (MACs). Experimental results demonstrate that the proposed model can obtain similar or better performance using fewer computes and parameters than competitive approaches to low-latency multichannel speech enhancement",
    "checked": true,
    "id": "cf371579ee406fda6847b29e32b747297cfbb6c9",
    "semantic_title": "a simple rnn model for lightweight, low-compute and low-latency multichannel speech enhancement in the time domain",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23b_interspeech.html": {
    "title": "High Fidelity Speech Enhancement with Band-split RNN",
    "volume": "main",
    "abstract": "Despite the rapid progress in speech enhancement (SE) research, improving the intelligibility and perceptual quality of desired speech in noisy environments with interfering speakers remains challenging. This paper attempts to achieve high-fidelity full-band SE and personalized SE (PSE) by modifying the recently proposed band-split RNN (BSRNN) model. To reduce the negative impact of unstable high-frequency components in full-band speech recording, we perform bi-directional and uni-directional band-level modeling to low-frequency and high-frequency subbands, respectively. For the PSE task, an additional speaker enrollment module is added to BSRNN to make use of the target speaker information for suppressing the interfering speech. Moreover, we utilize a MetricGAN discriminator (MGD) and a multi-resolution spectrogram discriminator (MRSD) to further improve the human auditory perceptual quality of the enhanced speech. Experimental results show that our system outperforms various top-ranking SE systems, achieves state-of-the-art (SOTA) SE performance on the DNS-2020 test set, and ranks among the top 3 in the DNS-2023 challenge on the PSE task",
    "checked": true,
    "id": "6d8358a9e819eb0088f229da45b953a977e83c93",
    "semantic_title": "high fidelity speech enhancement with band-split rnn",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23b_interspeech.html": {
    "title": "Focus on the Sound around You: Monaural Target Speaker Extraction via Distance and Speaker Information",
    "volume": "main",
    "abstract": "Previously, Target Speaker Extraction (TSE) has yielded outstanding performance in certain application scenarios for speech enhancement and source separation. However, obtaining auxiliary speaker-related information is still challenging in noisy environments with significant reverberation. inspired by the recently proposed distance-based sound separation, we propose the near sound (NS) extractor, which leverages distance information for TSE to reliably extract speaker information without requiring previous speaker enrolment, called speaker embedding self-enrollment (SESE). Full- & sub-band modeling is introduced to enhance our NS-Extractor's adaptability towards environments with significant reverberation. Experimental results on several cross-datasets demonstrate the effectiveness of our improvements and the excellent performance of our proposed NS-Extractor in different application scenarios",
    "checked": true,
    "id": "1a25e1fe145727deb9502b5153b9e67f3337fcbd",
    "semantic_title": "focus on the sound around you: monaural target speaker extraction via distance and speaker information",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kovalyov23_interspeech.html": {
    "title": "DFSNet: A Steerable Neural Beamformer Invariant to Microphone Array Configuration for Real-Time, Low-Latency Speech Enhancement",
    "volume": "main",
    "abstract": "Invariance to microphone array configuration is a rare attribute in neural beamformers. Filter-and-sum (FS) methods in this class define the target signal with respect to a reference channel. However, this not only complicates formulation in reverberant conditions but also the network, which must have a mechanism to infer what the reference channel is. To address these issues, this study presents Delay-Filter-and-Sum Network (DFSNet), a steerable neural beamformer invariant to microphone number and array geometry for causal speech enhancement. In DFSNet, acquired signals are first steered toward the speech source direction prior to the FS operation, which simplifies the task into the estimation of delay-and-sum clean reverberant speech. The proposed model is designed to incur low latency, distortion, and memory and computational burden, giving rise to high potential in hearing aid applications. Simulation results reveal comparable performance to noncausal state-of-the-art",
    "checked": true,
    "id": "c62781a17717d115068f42adf20031e1fc83de59",
    "semantic_title": "dfsnet: a steerable neural beamformer invariant to microphone array configuration for real-time, low-latency speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23o_interspeech.html": {
    "title": "Speaker-Aware Anti-spoofing",
    "volume": "main",
    "abstract": "We address speaker-aware anti-spoofing, where prior knowledge of the target speaker is incorporated into a voice spoofing countermeasure (CM). In contrast to the frequently used speaker-independent solutions, we train the CM in a speaker-conditioned way. As a proof of concept, we consider speaker-aware extension to the state-of-the-art AASIST (audio anti-spoofing using integrated spectro-temporal graph attention networks) model. To this end, we consider two alternative strategies to incorporate target speaker information at the frame and utterance levels, respectively. The experimental results on a custom protocol based on ASVspoof 2019 dataset indicates the efficiency of the speaker information via enrollment: we obtain maximum relative improvements of 25.1% and 11.6% in equal error rate (EER) and minimum tandem detection cost function (t-DCF) over a speaker-independent baseline, respectively",
    "checked": true,
    "id": "cf63c74d496ee64921ea493955649018bd3f4ae2",
    "semantic_title": "speaker-aware anti-spoofing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/araki23_interspeech.html": {
    "title": "Impact of Residual Noise and Artifacts in Speech Enhancement Errors on Intelligibility of Human and Machine",
    "volume": "main",
    "abstract": "Single-channel (1-ch) speech enhancement (SE) has been widely studied, and high accuracy has been achieved recently. However, enhanced speech still includes some errors that affect human hearing quality and SE applications, e.g., automatic speech recognition (ASR). Previously, [Iwamoto et al., in Interspeech 2022, pp. 5418-5422] decomposed the errors in an enhanced signal into residual noise and artifact components and analyzed their impacts on ASR performance. They showed that the artifacts have a greater impact than the residual noise on ASR. Although the impact on human intelligibility has not been investigated yet, it is essential to get the knowledge to develop SE techniques suitable for both humans and machines. This paper, therefore, investigates the effects of such error factors on human listening. Our subjective test results show that the artifacts have a large impact on human intelligibility, and that residual noise has a lesser impact",
    "checked": true,
    "id": "40a78f647aa9365746598a359c4c7804bcf7182f",
    "semantic_title": "impact of residual noise and artifacts in speech enhancement errors on intelligibility of human and machine",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sach23_interspeech.html": {
    "title": "EffCRN: An Efficient Convolutional Recurrent Network for High-Performance Speech Enhancement",
    "volume": "main",
    "abstract": "Fully convolutional recurrent neural networks (FCRNs) have shown state-of-the-art performance in single-channel speech enhancement. However, the number of parameters and the FLOPs/second of the original FCRN are restrictively high. A further important class of efficient networks is the CRUSE topology, serving as reference in our work. By applying a number of topological changes at once, we propose both an efficient FCRN (FCRN15), and a new family of efficient convolutional recurrent neural networks (EffCRN23, EffCRN23lite). We show that our FCRN15 (875K parameters) and EffCRN23lite (396K) outperform the already efficient CRUSE5 (85M) and CRUSE4 (7.2M) networks, respectively, w.r.t. PESQ, DNSMOS and ΔSNR, while requiring about 94% less parameters and about 20% less #FLOPs/frame. Thereby, according to these metrics, the FCRN/EffCRN class of networks provides new best-in-class network topologies for speech enhancement",
    "checked": true,
    "id": "a0e1ade0b2187d56f0c9284b31b528b273671fbe",
    "semantic_title": "effcrn: an efficient convolutional recurrent network for high-performance speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23e_interspeech.html": {
    "title": "HAD-ANC: A Hybrid System Comprising an Adaptive Filter and Deep Neural Networks for Active Noise Control",
    "volume": "main",
    "abstract": "Our study proposes a novel hybrid active noise control (ANC) system, called HAD-ANC, that combines an adaptive filter with deep neural networks. HAD-ANC employs a cascade design comprising the frequency-domain block least mean square algorithm and two gated convolutional recurrent networks (GCRNs). The first GCRN follows the adaptive filter to handle nonlinear distortion by reducing the residual error of linear filtering and models the reverse of both loudspeaker and secondary path. The second GCRN models the loudspeaker and secondary path to force the adaptive filter to estimate the primary path. Additionally, we utilize a delay-compensated reference signal to consider the causal constraints of frequency-domain ANC system. Experimental results based on NOISEX-92 dataset show that the proposed system outperforms recent ANC methods, enables wideband noise reduction, and indicates robustness to path changes",
    "checked": true,
    "id": "d41ebfbaa531d906aeb57bc048e10e5bc10ddb54",
    "semantic_title": "had-anc: a hybrid system comprising an adaptive filter and deep neural networks for active noise control",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chu23_interspeech.html": {
    "title": "MSAF: A Multiple Self-Attention Field Method for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement (SE) systems, based on generative adversarial networks (GANs), are limited in improving speech quality and intelligibility. In this study, we propose a novel multiple self-attention field method for speech enhancement (MSAF). The models with different positions of the self-attention layers focus on different features. The output of each model is assigned a different feature weight, which is obtained by training. Then, we fuse the models according to the feature weights to obtain a clean speech signal. For speech quality, the proposed method improves by 8.22%, 8.52%, 9.28%, and 9.40% in CBAK, CSIG, COVL, and PESQ on average compared with the baseline SASEGANs. The results show that the MSAF comprehensively improves the performance of the baseline SASEGAN and performs better than the mainstream GAN-based SE methods. Importantly, the proposed method can be extended to other GAN-based SE methods",
    "checked": true,
    "id": "4e276d1aa1da695673c93b2a52d7b229bb9609c4",
    "semantic_title": "msaf: a multiple self-attention field method for speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23t_interspeech.html": {
    "title": "Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression",
    "volume": "main",
    "abstract": "Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet",
    "checked": true,
    "id": "86f5ddb176f159ac26e6dde5abb4d4ebcb2ad5a5",
    "semantic_title": "ultra dual-path compression for joint echo cancellation and noise suppression",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wan23_interspeech.html": {
    "title": "ABC-KD: Attention-Based-Compression Knowledge Distillation for Deep Learning-Based Noise Suppression",
    "volume": "main",
    "abstract": "Noise suppression (NS) models have been widely applied to enhance speech quality. Recently, Deep Learning-Based NS, which we denote as Deep Noise Suppression (DNS), became the mainstream NS method due to its excelling performance over traditional ones. However, DNS models face 2 major challenges for supporting the real-world applications. First, high-performing DNS models are usually large in size, causing deployment difficulties. Second, DNS models require extensive training data, including noisy audios as inputs and clean audios as labels. It is often difficult to obtain clean labels for training DNS models. We propose the use of knowledge distillation (KD) to resolve both challenges. Our study serves 2 main purposes. First, we are among the first to comprehensively investigate mainstream KD techniques on DNS models to resolve the challenges. Furthermore, we propose a novel Attention-Based-Compression KD method that outperforms all investigated mainstream KD frameworks on DNS task",
    "checked": true,
    "id": "a3ade1e455423118c3820259ffdc41ebcc37c97d",
    "semantic_title": "abc-kd: attention-based-compression knowledge distillation for deep learning-based noise suppression",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/diener23_interspeech.html": {
    "title": "PLCMOS – A Data-driven Non-intrusive Metric for The Evaluation of Packet Loss Concealment Algorithms",
    "volume": "main",
    "abstract": "Speech quality assessment is a problem for every researcher working on models that produce or process speech. Human subjective ratings, the gold standard in speech quality assessment, are expensive and time-consuming to acquire in a quantity that is sufficient to get reliable data, while automated objective metrics show a low correlation with gold standard ratings. This paper presents PLCMOS, a non-intrusive data-driven tool for generating a robust, accurate estimate of the mean opinion score a human rater would assign an audio file that has been processed by being transmitted over a degraded packet-switched network with missing packets being healed by a packet loss concealment algorithm. Our new model shows a model-wise Pearson's correlation of ~0.97 and rank correlation of ~0.95 with human ratings, substantially above all other available intrusive and non-intrusive metrics",
    "checked": false,
    "id": "2f90c238092e7889e198d1c280d178c47cf32b64",
    "semantic_title": "plcmos - a data-driven non-intrusive metric for the evaluation of packet loss concealment algorithms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wagner23b_interspeech.html": {
    "title": "Effects of Meter, Genre and Experience on Pausing, Lengthening and Prosodic Phrasing in German Poetry Reading",
    "volume": "main",
    "abstract": "The adequate and pleasant delivery of poetic speech remains a challenge for humans and machines alike. The present corpus study analyzes factors and strategies that characterize the stylistic expression of poetry and prose by professional actors as well as laypersons with musical training, focusing on their pausing, lengthening and intonation at verse boundaries. Our results show a clear influence on speakers' experience in modulating their speech with respect to prosodic timing: professional actors systematically insert more and more diverse prosodic boundaries and pauses than laypersons, and make strategic use of lengthening at verse endings in poetic speech. Our results further point out the relevance of pausing and lengthening as a time-buying strategy that enhances speech fluency, and we make tentative suggestions for modeling (poetic) speech in expressive speech synthesis",
    "checked": true,
    "id": "8255182f999b77a8a4331e41a5a86bebbd6f49c7",
    "semantic_title": "effects of meter, genre and experience on pausing, lengthening and prosodic phrasing in german poetry reading",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/szalay23_interspeech.html": {
    "title": "Comparing first spectral moment of Australian English /s/ between straight and gay voices using three analysis window sizes",
    "volume": "main",
    "abstract": "Men producing /s/ with higher first spectral moment (M1) are more likely to be perceived as gay, yet it is unclear if M1 differs in production. Inconsistent results might be caused by inherent change in M1 over time. Therefore, we explored M1 change over time and tested if the length and location of the analysis window affects results on gay-straight differences. 37 gay and 29 straight male speakers of Australian English produced two /s/ tokens in continuous speech. M1 was extracted in each quarter of /s/ to explore change over time and in three windows: the entire fricative, the mid-50 ms, and the third quarter. Gay and straight men produced lower M1 in the first and last quarters relative to the midpoint. Despite the M1 change over time, we found no effect of analysis window on gay-straight differences, as gay men consistently showed higher M1. The lack of effect of analysis window on M1 is attributed to the overlap between the analysis windows caused by the duration of /s/",
    "checked": true,
    "id": "dd266e37aa668507f72dc00945cbc16a8b9efb81",
    "semantic_title": "comparing first spectral moment of australian english /s/ between straight and gay voices using three analysis window sizes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/taguchi23_interspeech.html": {
    "title": "Universal Automatic Phonetic Transcription into the International Phonetic Alphabet",
    "volume": "main",
    "abstract": "This paper presents a state-of-the-art model for transcribing speech in any language into the International Phonetic Alphabet (IPA). Transcription of spoken languages into IPA is an essential yet time-consuming process in language documentation, and even partially automating this process has the potential to drastically speed up the documentation of endangered languages. Like the previous best speech-to-IPA model (Wav2Vec2Phoneme), our model is based on wav2vec 2.0 and is fine-tuned to predict IPA from audio input. We use training data from seven languages from CommonVoice 11.0, transcribed into IPA semi-automatically. Although this training dataset is much smaller than Wav2Vec2Phoneme's, its higher quality lets our model achieve comparable or better results. Furthermore, we show that the quality of our universal speech-to-IPA models is close to that of human annotators",
    "checked": true,
    "id": "a79bca31fc93e6a9b6754aae4bb4ad91dde7de12",
    "semantic_title": "universal automatic phonetic transcription into the international phonetic alphabet",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gerlach23_interspeech.html": {
    "title": "Voice Twins: Discovering Extremely Similar-sounding, Unrelated Speakers",
    "volume": "main",
    "abstract": "This paper deals with extremely similar-sounding, unrelated speakers ('voice twins') and presents an automatic approach to voice twin discovery applied to different speaker databases. An automatic speaker recognition system relying on perceptually relevant phonetic features including formants and a tuned clustering algorithm DBSCAN was used to group recordings within diverse datasets. 18 voice twin pairs selected from 2-speaker clusters were evaluated by 50 listeners in a 2-alternative forced choice experiment. Same/different decisions and confidence ratings were collected for same-speaker, random different-speaker and voice twin comparisons. Listeners were unable to differentiate between the candidate voice twin pairs much better than chance level while they performed well (80% accuracy) for random same- or different-speaker comparisons indicating the voice twin speakers were perceptually very similar. The implications and forensic relevance of identifying voice twins are discussed",
    "checked": true,
    "id": "6a735c94b52b72d500d68aef6fe4352367a99678",
    "semantic_title": "voice twins: discovering extremely similar-sounding, unrelated speakers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hedegard23_interspeech.html": {
    "title": "Filling the population statistics gap: Swiss German reference data on F0 and speech tempo for forensic contexts",
    "volume": "main",
    "abstract": "The increased focus on big data in phonetics, and Bayesian statistics in the forensic sciences, prompt a fundamental issue in common applications of forensic phonetics. Relevant population distributions for most features, a key element when evaluating the similarity and distinctiveness of voices, remain lacking for a substantial number of languages and dialects. This paper provides population statistics for two phonetic features in the Swiss German context, speech tempo and F0, and outlines a potential method for big data analysis. The speech data is taken from 1000 SwG speakers and include two different style conditions: spontaneous and read speech. Results indicate significant variation for both parameters: we contradict previous findings on gender differences in speech tempo and note discrepancies for both features between the two styles. These findings constitute an important contribution to the field of forensic phonetics, as well as the field of general phonetics more broadly",
    "checked": true,
    "id": "477228f8a466febaff48befa18dd1596c2281cc9",
    "semantic_title": "filling the population statistics gap: swiss german reference data on f0 and speech tempo for forensic contexts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hutin23_interspeech.html": {
    "title": "Investigating the Syntax-Discourse Interface in the Phonetic Implementation of Discourse Markers",
    "volume": "main",
    "abstract": "Discourse markers (DMs) are (chunks of) words stemming from the diachronic development of other parts-of-speech that tag the discourse's organization (ex. \"well then\", \"innit\"...). However, in synchrony, the formal accounts for the DM class vary from purely discourse-oriented definitions to models relying on a combination of lexico-grammatical and discursive information. We propose to bring new evidence into this debate by comparing the phonetic realizations of 4 DM types: stemming originally from adverbs, coordinators, subordinators and interjections. A discourse-only account would predict that the 4 types would be realized similarly, while a syntactic-discursive account predicts that subordinators would stand out, as they are less prone to syntactic independence. The analysis of various acoustic parameters (segment duration, F0, F1, F2 and HNR) in a finely-annotated 4-hour long corpus of French indicates that a hybrid approach may indeed be more accurate",
    "checked": true,
    "id": "caf85e2a0e73bd4153c190f6776b3ac8095148ea",
    "semantic_title": "investigating the syntax-discourse interface in the phonetic implementation of discourse markers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/essery23_interspeech.html": {
    "title": "Evaluation of a Forensic Automatic Speaker Recognition System with Emotional Speech Recordings",
    "volume": "main",
    "abstract": "In forensic contexts, speakers often feel emotional, which will likely influence their speech. Emotional mismatch between samples is therefore a source of variability which could have substantial effects on the performance of a forensic automatic speaker recognition system. This paper examines the issue of emotional speech in forensic casework, both in terms of emotional match and mismatch between test samples and in terms of the data used to calibrate the system (i.e. the reference population). Specifically, we tested system performance on samples of neutral and acted angry and fearful speech data across 37 test conditions. The best system performance was achieved when the test data and reference population conditions matched exactly. However, in 16 of the 37 tests, the system produced a Cllr greater than 0.8, 10 of which also exceeded a Cllr of 1. As a result, caution should be used to interpret the results of automatic and semi-automatic forensic analysis on emotional speech data",
    "checked": true,
    "id": "d3740056c2869da7f1d693ee6e886c38976b3ad5",
    "semantic_title": "evaluation of a forensic automatic speaker recognition system with emotional speech recordings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahn23_interspeech.html": {
    "title": "An Outlier Analysis of Vowel Formants from a Corpus Phonetics Pipeline",
    "volume": "main",
    "abstract": "With the growing availability of large-scale spoken databases, linguists are increasingly relying on automated tools to obtain time alignments of sound units to the speech signal. A typical automated pipeline may involve grapheme-to-phoneme conversion, forced alignment, and acoustic-phonetic measurement, and each of these stages requires a strong assumption regarding the output quality. We investigate these assumptions by auditing outliers in vowel formants from two multilingual read speech corpora, CMU Wilderness and Mozilla Common Voice, across three languages: Hausa, Kazakh, and Swedish. From this audit, we develop a novel outlier taxonomy that includes the broad outlier categories of transcript errors, alignment errors, formant tracking errors, linguistic variations, and fine samples. We show the utility of this outlier analysis in identifying weaknesses in corpus-specific and corpus-general pipeline assumptions, and discovering characteristics of particular languages",
    "checked": true,
    "id": "844e4c88fcd590e979d1a804d0494a07c6c9cb0b",
    "semantic_title": "an outlier analysis of vowel formants from a corpus phonetics pipeline",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qu23_interspeech.html": {
    "title": "The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features",
    "volume": "main",
    "abstract": "This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech - phoneme corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes vs. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning",
    "checked": true,
    "id": "a6e3a10a6286967413e3406374bbeea533640030",
    "semantic_title": "the hidden dance of phonemes and visage: unveiling the enigmatic link between phonemes and facial features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/blaylock23_interspeech.html": {
    "title": "Beatboxing Kick Drum Kinematics",
    "volume": "main",
    "abstract": "Study of the vocal movements of beatboxing can benefit speech science in a number of ways; but while there are established models of speech motor control that make deterministic predictions about vocal kinematics, little is known about beatboxing motor control. A region of interest method was applied to real-time MRI videos of beatboxing Kick Drums to measure the time to peak velocity-a measurement that is used to assess how well models of speech action predict actual movement trajectories. The average time to peak velocity for Kick Drums is about half of the way (58%) through the total movement duration, similar to the times to peak velocity reported for speech actions. However, while the times to peak velocity for Kick Drums tend to be just above 50%, the times to peak velocity reported for speech sounds are usually a bit below 50%. Further study is needed to assess whether this difference reflects a more extreme constriction goal or a qualitatively different movement pattern",
    "checked": true,
    "id": "16efca1d110ea1b3fbc6708d580d522281f6291e",
    "semantic_title": "beatboxing kick drum kinematics",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23b_interspeech.html": {
    "title": "Effects of hearing loss and amplification on Mandarin consonant perception",
    "volume": "main",
    "abstract": "This study investigates the effects of hearing loss and amplification on Mandarin consonant perception. 44 listeners with varying degrees of hearing loss were tested, both with and without the use of hearing aids. Consonant recognition was strongly correlated with the hearing threshold (r = -0.87), and was significantly improved by hearing-aid amplification (by more than 20% in group means) but was still not perfect. The underlying reasons are discussed. Furthermore, confusion patterns were analyzed and compared with those of normal-hearing listeners in the literature. The most challenging Mandarin consonants for hearing-impaired listeners include consonants with a spectral center of gravity in the high-frequency range (such as s and z), consonants with short duration (such as b, d, and g), and aspirated stops (such as p, t, and k). The findings of this study contribute to a better understanding of the difficulties experienced by Mandarin-speaking listeners with hearing loss",
    "checked": true,
    "id": "2523b7dec797b65bd700fd3879c33670f6b6ab29",
    "semantic_title": "effects of hearing loss and amplification on mandarin consonant perception",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/adams23_interspeech.html": {
    "title": "An Acoustic Analysis of Fricative Variation in Three Accents of English",
    "volume": "main",
    "abstract": "This study reports on an analysis of accent and gender differences in the realisation of the fricative /s/ within three accents of English: London, Cambridge, Belfast. There were 30 speakers in the study. Using multilevel modelling, significant differences between accents in the dynamic acoustics of the alveolar fricative /s/ are evident. Significant gender differences in the fricative energy measure trajectories are also found, within each accent. We further discuss the implications of these differences to our understanding of the role of gender and accent in the realisation of spectra movements in English fricatives, highlighting the necessity of a dynamic approach to sociophonetic acoustic variation",
    "checked": true,
    "id": "66d18ef1010a598db68f1ae55f0ef596b5f6296a",
    "semantic_title": "an acoustic analysis of fricative variation in three accents of english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bros23_interspeech.html": {
    "title": "Acoustic cues to stress perception in Spanish – a mismatch negativity study",
    "volume": "main",
    "abstract": "In this paper we investigate the cues governing stress perception in Spanish - an issue that has been subject to debate and remains largely unresolved. While there is general agreement as to the ability of Spanish listeners to detect and reliably produce stress contrasts, there is some disagreement on the roles played by individual stress cues. In this study, we focus on early stress processing in a passive oddball paradigm aimed at eliciting a mismatch negativity response to changes in stress. Individual features (spectral tilt and f0) rather than feature bundles are used to induce stress shift. A vowel change is used as a control condition. The results show that while both spectral tilt and f0 manipulations result in mismatch negativity, the latter evokes a stronger response that equals the effect of a change in the quality of the stressed vowel. The results are in line with previous studies on stress correlates in other languages, pointing to a possible cross-linguistic pattern",
    "checked": true,
    "id": "0709fc902a6de0fb35cb070b62218af3a4d204fd",
    "semantic_title": "acoustic cues to stress perception in spanish – a mismatch negativity study",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sabev23_interspeech.html": {
    "title": "Bulgarian Unstressed Vowel Reduction: Received Views vs Corpus Findings",
    "volume": "main",
    "abstract": "Bulgarian is often cited in phonological work for its vowel reduction, with the assumption that the six-vowel stressed inventory, /ɛ a ɔ i ɤ u/, shrinks to three unstressed contrastive vowels, /i ɤ u/, by virtue of /ɛ a ɔ/ raising and merging with /i ɤ u/. The literature in Bulgarian, on the other hand, maintains that /ɛ-i/ do not merge in Standard Bulgarian; that vowels are less reduced in immediately pretonic syllables than elsewhere; that unstressed high vowels are lowered, while nonhigh vowels are raised; and that /a-ɤ/ are more likely to merge than /ɔ-u/. These claims have been challenged in recent work, and we present a new investigation based on 11,615 vowel tokens from 140 speakers in the BulPhonC speech corpus. MANOVA and GLMM results provide clear evidence that there is no unstressed high-vowel lowering, no difference between pretonic vs other unstressed vowels, and that both unstressed /a-ɤ/ and /ɔ-u/ merge completely, while /ɛ-i/ remain spectrally distinct",
    "checked": true,
    "id": "1207d703d3e003a8e90bddd518f0487ff887f9ff",
    "semantic_title": "bulgarian unstressed vowel reduction: received views vs corpus findings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jain23b_interspeech.html": {
    "title": "An Investigation of Indian Native Language Phonemic Influences on L2 English Pronunciations",
    "volume": "main",
    "abstract": "Speech systems are sensitive to accent variations. This is a challenge in India, which has numerous languages but few linguistic studies on pronunciation variation. The growing number of L2 English speakers reinforces the need to study accents and L1-L2 interactions. We investigate Indian English (IE) accents and report our observations on regional and shared features. Specifically, we observe phonemic variations and phonotactics in speakers' native languages and apply this to their English pronunciations. We demonstrate the influence of 18 Indian languages on IE by comparing native language features with IE pronunciations obtained from literature studies and phonetically annotated speech. Hence, we validate Indian language influences on IE by justifying pronunciation rules from the perspective of Indian language phonology. We obtain a comprehensive description of generalised and region-specific IE characteristics, which facilitates accent adaptation of existing speech systems",
    "checked": true,
    "id": "46c231d8fe8b349c8f9efc2c735e6e47cb876bed",
    "semantic_title": "an investigation of indian native language phonemic influences on l2 english pronunciations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23b_interspeech.html": {
    "title": "Identifying Stable Sections for Formant Frequency Extraction of French Nasal Vowels Based on Difference Thresholds",
    "volume": "main",
    "abstract": "Formant frequencies of a vowel are generally extracted from midpoints or central sections on the time axis. Nasal vowels present a challenge for obtaining stable formant frequencies, as the midpoint often falls in an anti-formant section where vocal energy is lost through the nasal cavity. This study proposes a stable section for extracting nasal vowel formant frequencies using difference thresholds, which identify a vowel as being distinct when F1 is above 60 Hz and/or F2 is above 200 Hz. For the experiment, 481 disyllabic words (232 nasal vowels and 294 oral counterparts) are selected from an online French-Korean Dictionary. Each vowel is divided into 10 intervals, and the stable section is identified as one or more continuous intervals with lower frequencies than the difference thresholds. The results show that the stable section for nasal vowels is identified in 20%~50% of the vowels, while the stable section for oral vowels is identified in 20%~80% of the vowels",
    "checked": true,
    "id": "ffc0917fa476bd69e07807bd7fd5c6e04c3ee97c",
    "semantic_title": "identifying stable sections for formant frequency extraction of french nasal vowels based on difference thresholds",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/audibert23_interspeech.html": {
    "title": "Evaluation of delexicalization methods for research on emotional speech",
    "volume": "main",
    "abstract": "Perceptual evaluation of non-controlled emotional speech requires delexicalization to neutralize semantic variation. However, most existing methods imply losing spectral cues crucial to emotional attribution, related to both laryngeal and supralaryngeal settings. We propose a method relying on voice morphing to retain part of the spectral information of the original stimuli, as an additional step to diphone synthesis delexicalization. After previous assessment of intelligibility loss, this study evaluates the naturalness of angry and neutral expressions in French films, delexicalized using low-pass filtering and the proposed method implemented with MBROLA and STRAIGHT. Results show that morphing-based delexicalization, which leads to accurate emotional attribution, is rated with a higher degree of naturalness than low-pass filtering. Implications for research in affective speech are discussed with regards to other delexicalization methods proposed in the literature",
    "checked": true,
    "id": "265234299bdb877577147edb9988999d87d0ed1d",
    "semantic_title": "evaluation of delexicalization methods for research on emotional speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kejriwal23b_interspeech.html": {
    "title": "Relationship between auditory and semantic entrainment using Deep Neural Networks (DNN)",
    "volume": "main",
    "abstract": "The tendency of people to engage in similar, matching, or synchronized behaviour when interacting is known as entrainment. Many studies examined linguistic (syntactic and lexical structures) and paralinguistic (pitch, intensity) entrainment, but less attention was given to finding the relationship between them. In this study, we utilized state-of-the-art DNN embeddings such as BERT and TRIpLet Loss network (TRILL) vectors to extract features for measuring semantic and auditory similarities of turns within dialogues in two comparable spoken corpora of two different languages. We found people's tendency to entrain on semantic features more when compared to auditory features. Additionally, we found that entrainment in semantic and auditory linguistic features are positively correlated. The findings of this study might assist in implementing the mechanism of entrainment in human-machine interaction (HMI)",
    "checked": true,
    "id": "586faa3c2f190c8535dabaf5a50326b8091e4ca7",
    "semantic_title": "relationship between auditory and semantic entrainment using deep neural networks (dnn)",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kejriwal23_interspeech.html": {
    "title": "Unsupervised Auditory and Semantic Entrainment Models with Deep Neural Networks",
    "volume": "main",
    "abstract": "Speakers tend to engage in adaptive behavior, known as entrainment, when they become similar to their interlocutor in various aspects of speaking. We present an unsupervised deep learning framework that derives meaningful representation from textual features for developing semantic entrainment. We investigate the model's performance by extracting features using different variations of the BERT model (DistilBERT and XLM-RoBERTa) and Google's universal sentence encoder (USE) embeddings on two human-human (HH) corpora (The Fisher Corpus English Part 1, Columbia games corpus) and one human-machine (HM) corpus (Voice Assistant Conversation Corpus (VACC)). In addition to semantic features we also trained DNN-based models utilizing two auditory embeddings (TRIpLet Loss network (TRILL) vectors, Low-level descriptors (LLD) features) and two units of analysis (Inter pausal unit and Turn). The results show that semantic entrainment can be assessed with our model, that models can distinguish between HH and HM interactions and that the two units of analysis for extracting acoustic features provide comparable findings",
    "checked": true,
    "id": "b7ed57a9b5643889007f909f6d9fa3eb586dbcf7",
    "semantic_title": "unsupervised auditory and semantic entrainment models with deep neural networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nielsen23_interspeech.html": {
    "title": "Parsing dialog turns with prosodic features in English",
    "volume": "main",
    "abstract": "Parsing spoken dialogue presents challenges that parsing text does not, including a lack of clear sentence boundaries. We know from previous work that prosody helps in parsing single sentences (Tran et al. 2018), but we want to show the effect of prosody on parsing speech that isn't segmented into sentences. In experiments on the English Switchboard corpus, we find prosody helps our model both with parsing and with accurately identifying sentence boundaries. However, we find that the best-performing parser is not necessarily the parser that produces the best sentence segmentation performance. We suggest that the best parses instead come from modelling sentence boundaries jointly with other constituent boundaries",
    "checked": true,
    "id": "ace1bbfa1475cacc40fa9bcd7735c8ff3f072fd0",
    "semantic_title": "parsing dialog turns with prosodic features in english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muromachi23_interspeech.html": {
    "title": "Estimation of Listening Response Timing by Generative Model and Parameter Control of Response Substantialness Using Dynamic-Prompt-Tune",
    "volume": "main",
    "abstract": "A spoken dialogue system is required to continuously listen to a human user for smooth conversation. We propose a method that simultaneously performs response generation and listener response timing estimation. Our proposed method estimates listener response timing by adding pseudo-samples where listener response should be irrelevant, which allows using text-only conversation dataset without audio information. Furthermore, our proposed method can control substantialness of responses by user-specified parameter integrated with the Dynamic-Prompt-Tune method, which uses prompt token embedding dynamically generated from the parameter. Our automatic and manual evaluation showed that the proposed method can generate responses with more natural timing and more in line with the response substantialness parameter compared to the baseline model",
    "checked": true,
    "id": "7e79bb2d459fc45d41eb1e450867ebfb150f63c1",
    "semantic_title": "estimation of listening response timing by generative model and parameter control of response substantialness using dynamic-prompt-tune",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chowdhury23_interspeech.html": {
    "title": "Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder",
    "volume": "main",
    "abstract": "The diagnosis of autism spectrum disorder (ASD) is a complex, challenging task as it depends on the analysis of interactional behaviors by psychologists rather than the use of biochemical diagnostics. In this paper, we present a modeling approach to ASD diagnosis by analyzing acoustic/prosodic and linguistic features extracted from diagnostic conversations between a psychologist and children who either are typically developing (TD) or have ASD. We compare the contributions of different features across a range of conversation tasks. We focus on finding a minimal set of parameters that characterize conversational behaviors of children with ASD. Because ASD is diagnosed through conversational interaction, in addition to analyzing the behavior of the children, we also investigate whether the psychologist's conversational behaviors vary across diagnostic groups. Our results can facilitate fine-grained analysis of conversation data for children with ASD to support diagnosis and intervention",
    "checked": true,
    "id": "f48c0c9a314df02c45901e84c5fb83234e5b3123",
    "semantic_title": "parameter selection for analyzing conversations with autism spectrum disorder",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/buddi23_interspeech.html": {
    "title": "Efficient Multimodal Neural Networks for Trigger-less Voice Assistants",
    "volume": "main",
    "abstract": "The adoption of multimodal interactions by Voice Assistants (VAs) is growing rapidly to enhance human-computer interactions. Smartwatches have now incorporated trigger-less methods of invoking VAs, such as Raise To Speak (RTS), where the user raises their watch and speaks to VAs without an explicit trigger. Current state-of-the-art RTS systems rely on heuristics and engineered Finite State Machines to fuse gesture and audio data for multimodal decision-making. However, these methods have limitations, including limited adaptability, scalability, and induced human biases. In this work, we propose a neural network based audio-gesture multimodal fusion system that (1) Better understands temporal correlation between audio and gesture data, leading to precise invocations (2) Generalizes to a wide range of environments and scenarios (3) Is lightweight and deployable on low-power devices, such as smartwatches, with quick launch times (4) Improves productivity in asset development processes",
    "checked": true,
    "id": "4d3bfb15afee927cd63dc3640acd0ed6f1a446a2",
    "semantic_title": "efficient multimodal neural networks for trigger-less voice assistants",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ostrand23_interspeech.html": {
    "title": "Rapid Lexical Alignment to a Conversational Agent",
    "volume": "main",
    "abstract": "Conversational partners modify their language to be more similar to each other during interactions. This phenomenon, known as alignment, has been shown in human-human interactions, but there is little work on lexical alignment in human-computer interactions. We investigate whether people lexically align to a conversational agent, and whether the degree of alignment depends on feedback from the agent. This study compared three feedback conditions for how the agent responded to users' word choice: (1) the agent only understood the specific words that it produced itself; (2) the agent understood the words that it produced as well as more appropriate synonyms; (3) the agent's understanding of words that it did not produce was random. Participants significantly aligned to the agent in all conditions, and aligned more when they learned that the agent's comprehension was contingent on their alignment. Thus, inducing lexical alignment may be an effective way to increase dialogue success",
    "checked": true,
    "id": "c9bfaf9a2830e79bca9e64a8f8c4d08f843a2815",
    "semantic_title": "rapid lexical alignment to a conversational agent",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kurata23_interspeech.html": {
    "title": "Multimodal Turn-Taking Model Using Visual Cues for End-of-Utterance Prediction in Spoken Dialogue Systems",
    "volume": "main",
    "abstract": "In this study, we propose a multimodal model for predicting the end-of-utterance probability in spoken dialogue systems, highlighting the unique role of visual cues in addition to acoustic and linguistic information. Although the effectiveness of visual cues, such as gaze, mouth, and head movements, has been suggested, few studies have fully incorporated them into turn-taking models, and the relative importance of these visual cues has also been underresearched. To address these issues, we first conducted an ablation study on visual features, showing the larger contribution of eye movements than mouth and head movements. Additionally, an end-to-end visual feature extraction model utilizing 3D-CNN is employed to comprehensively capture these visual cues. By combining visual features with acoustic and verbal information, AUC score for end-of-utterance prediction improved from 0.896 to 0.920, demonstrating the effectiveness of incorporating these visual cues in turn-taking models",
    "checked": true,
    "id": "539a253a8fcebbf54499a8ee0aaade8a6b788697",
    "semantic_title": "multimodal turn-taking model using visual cues for end-of-utterance prediction in spoken dialogue systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hojo23_interspeech.html": {
    "title": "Audio-Visual Praise Estimation for Conversational Video based on Synchronization-Guided Multimodal Transformer",
    "volume": "main",
    "abstract": "This study investigates praise estimation, the task of estimating the existence of preferable behaviors of a speaker in a conversational video. To estimate praises from multimodal information, considering synchronized behavior across modalities is important. Such cross-modal synchronization can be modeled by the conventional multimodal Transformer in a time-axis concatenation architecture because it models relevance between all time steps of all input modalities using attention matrices. However, the attention matrices are so high-dimensional that the model training can be difficult with a limited amount of training data. To alleviate this problem, we propose introducing a loss function representing the prior knowledge that the attention should link around the synchronized time steps across the input modalities. Our experiments on a business negotiation conversation corpus showed that the proposed method could improve the praise estimation's macro F1",
    "checked": true,
    "id": "7b24d9f4367e9413e9f892a9d836e1ffc8ff6001",
    "semantic_title": "audio-visual praise estimation for conversational video based on synchronization-guided multimodal transformer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sakuma23_interspeech.html": {
    "title": "Improving the response timing estimation for spoken dialogue systems by reducing the effect of speech recognition delay",
    "volume": "main",
    "abstract": "In conversational systems, the proper timing of the system's response is critical to maintaining a comfortable conversation. To achieve appropriate timing estimation, it is important to know what the users have said, including their most recent words, but ASR delay usually prevents the use of full user utterance. In this paper, we attempted to employ an extremely low latency ASR model called Multi-Look-Ahead ASR by Zhao et al. to enable near full utterance for response timing estimation. Additionally, we examined the effectiveness of using low latency ASR in combination with a parameter called Estimates of Syntactic Completeness (ESC), which indicates how soon the user's speech is completed. We evaluated on a Japanese simulated dialog database of a restaurant information center. The results confirmed that reducing ASR delay improves the accuracy of response timing estimation. This effect also appeared when the method using ESC is combined with the use of low latency ASR",
    "checked": true,
    "id": "26ecebd086c6b7ece6e321400d741dc62e965cb5",
    "semantic_title": "improving the response timing estimation for spoken dialogue systems by reducing the effect of speech recognition delay",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23c_interspeech.html": {
    "title": "Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Recognizing emotions in speech is essential for improving human-computer interactions, which require understanding and responding to the users' emotional states. Integrating multiple modalities, such as speech and text, enhances the performance of speech emotion recognition systems by providing a varied source of emotional information. In this context, we propose a model that enhances cross-modal transformer fusion by applying focus attention mechanisms to align and combine the salient features of two different modalities, namely, speech and text. The analysis of the disentanglement of the emotional representation various multiple embedding spaces using deep metric learning confirmed that our method shows enhanced emotion recognition performance. Furthermore, the proposed approach was evaluated on the IEMOCAP dataset. Experimental results demonstrated that our model achieves the best performance among other relevant multimodal speech emotion recognition systems",
    "checked": true,
    "id": "09d1fa85766d64f8db42a72d7c117d2356848f02",
    "semantic_title": "focus-attention-enhanced crossmodal transformer with metric learning for multimodal speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23la_interspeech.html": {
    "title": "A Multiple-Teacher Pruning Based Self-Distillation (MT-PSD) Approach to Model Compression for Audio-Visual Wake Word Spotting",
    "volume": "main",
    "abstract": "We propose a novel model compression approach using multiple-teacher pruning based self-distillation for audio-visual wake word spotting, facilitating compact neural network implementations without sacrificing system performances. In each stage of the proposed framework, we prune a teacher model obtained in the previous stage to generate a student model, then fine-tune it with teacher-student learning and use it as a new teacher model for following stages. A normalized intra-class loss is designed to optimize this pruning based self-distillation (PSD) process. Both single-teacher PSD (ST-PSD) and multi-teacher PSD (MT-PSD) are adopted in the fine-tuning process each stage. When tested on audio-visual wake word spotting in MISP2021 Challenge, the two proposed techniques outperform state-of-the-art methods in both system performances and model efficiencies. Moreover, MT-PSD that leverages upon the complementarity of multiple teachers obtained in different stages also outperforms ST-PSD",
    "checked": true,
    "id": "9ffffa0547892da041c60c6bac37073b271ef3e4",
    "semantic_title": "a multiple-teacher pruning based self-distillation (mt-psd) approach to model compression for audio-visual wake word spotting",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/spiesberger23_interspeech.html": {
    "title": "Abusive Speech Detection in Indic Languages Using Acoustic Features",
    "volume": "main",
    "abstract": "Abusive content in online social networks is a well-known problem that can cause serious psychological harm and incite hatred. The ability to upload audio data increases the importance of developing methods to detect abusive content in speech recordings. However, simply transferring the mechanisms from written abuse detection would ignore relevant information such as emotion and tone. In addition, many current algorithms require training in the specific language for which they are being used. This paper proposes to use acoustic and prosodic features to classify abusive content. We used the ADIMA data set, which contains recordings from ten Indic languages, and trained different models in multilingual and cross-lingual settings. Our results show that it is possible to classify abusive and non-abusive content using only acoustic and prosodic features. The most important and influential features are discussed",
    "checked": true,
    "id": "56cbba328eb0be875798f24b5dd52b04655329b5",
    "semantic_title": "abusive speech detection in indic languages using acoustic features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ingle23_interspeech.html": {
    "title": "Listening To Silences In Contact Center Conversations Using Textual Cues",
    "volume": "main",
    "abstract": "Contact center conversations often consist of silent segments, where neither the customer nor the agent is speaking. These silences if continued beyond an acceptable level can negatively impact contact center KPIs. Thus, understanding silences and defining measures to handle them better via appropriate coach- ing and alerting for agents is one of the key focus areas for contact centers. In this paper, we demonstrate how dialogue turns around silences could be used to understand the characteristics of silences (expected vs unexpected and agent vs cusomer-caused silences) via two text classification tasks. We propose a methodology to pre-train a silence-aware language model in contact center domain, called Silence-RoBERTa and demonstrate its ability to better capture the conversational characteristics around silences. Finally, we discuss the application of the above methodology in real-time and post-call settings and demonstrate its usability to reduce silences via a reallife case study",
    "checked": true,
    "id": "564b74b682a3919f945c77e91af54913e1ddd202",
    "semantic_title": "listening to silences in contact center conversations using textual cues",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeen23_interspeech.html": {
    "title": "I Learned Error, I Can Fix It! : A Detector-Corrector Structure for ASR Error Calibration",
    "volume": "main",
    "abstract": "Speech recognition technology has improved recently. However, in the context of spoken language understanding (SLU), containing automatic speech recognition (ASR) errors causes significant downstream performance degradation. To address this issue, various ASR error correction methodologies have been proposed. ASR error correction mainly focuses on correcting and generating only the error span using a conditional decoding method. To this end, we propose a structure with a Detector that uses collaborative training to predict various error patterns and a Corrector that corrects the detected error span by Detector. This pipeline reduces Word Error Rate (WER) and shows less performance degradation in downstream tasks compared with the original ASR hypotheses. In addition, it was shown that it could be generalized to various downstream data. By leveraging this Detector-Corrector pipeline, we expect to achieve effective ASR error correction and enable high-quality SLU downstream tasks",
    "checked": true,
    "id": "1e1df3c1e57f86d5b9eaa77cb73ec0ae35bc05a3",
    "semantic_title": "i learned error, i can fix it! : a detector-corrector structure for asr error calibration",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/garnier23b_interspeech.html": {
    "title": "Verbal and nonverbal feedback signals in response to increasing levels of miscommunication",
    "volume": "main",
    "abstract": "This study aims to explore in detail how listeners respond to communication disruptions in a task-oriented dialogue. We conducted an experiment with participants playing a map task with a partner via a video conferencing system that showed seemingly random breakdowns. In fact, the breakdowns were scripted to induce increasing levels of miscommunication. After an initial interactive session, a second non-interactive session was recorded with one-sided communication from the task leader. Among the fifty or so verbal and nonverbal feedback signals observed, twelve were produced by more than half of the participants. A detailed analysis of their use in different situations, their timing and their co-occurrence, supported that they may have different functions: some appear to be personal reactions of uncertainty, misunderstanding, or inability to complete the task, whereas others were clear repair initiators or turn-taking signals deliberately addressed to the interlocutor",
    "checked": true,
    "id": "9f23c2e883b72f76495823c9f891041f7c337b73",
    "semantic_title": "verbal and nonverbal feedback signals in response to increasing levels of miscommunication",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/amiriparian23_interspeech.html": {
    "title": "Speech-Based Classification of Defensive Communication: A Novel Dataset and Results",
    "volume": "main",
    "abstract": "Defensive communication is known to have detrimental effects on the quality of social interactions. Hence, recognising and reducing defensive behaviour is crucial to improving professional and personal communication. We introduce DefComm-DB, a novel multimodal dataset comprising video recordings in which one of the following types of defensive communication is present: (i) verbally attacking the conversation partner, (ii) withdrawing from the communication, (iii) making oneself greater, and (iv) making oneself smaller. Subsequently, we present a machine learning approach for the automatic classification of DefComm-DB. In particular, we utilise wav2vec2, autoencoders, a pre-trained CNN and openSMILE for feature extraction from the audio modality. For the text stream, we apply ELECTRA and SBERT. On the unseen test set, our models achieve an Unweighted Average Recall of 49.4 % and 52.2 % for the audio and text modalities, respectively, showing the feasibility of the introduced challenge",
    "checked": true,
    "id": "3c77a9bfe90a4ff9abfe9634cb6051ef5e2e9c7d",
    "semantic_title": "speech-based classification of defensive communication: a novel dataset and results",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wallbridge23_interspeech.html": {
    "title": "Quantifying the perceptual value of lexical and non-lexical channels in speech",
    "volume": "main",
    "abstract": "Speech is a fundamental means of communication that can be seen to provide two channels for transmitting information: the lexical channel of which words are said, and the non-lexical channel of how they are spoken. Both channels shape listener expectations of upcoming communication; however, directly quantifying their relative effect on expectations is challenging. Previous attempts require spoken variations of lexically-equivalent dialogue turns or conspicuous acoustic manipulations. This paper introduces a generalised paradigm to study the value of non-lexical information in dialogue across unconstrained lexical content. By quantifying the perceptual value of the non-lexical channel with both accuracy and entropy reduction, we show that non-lexical information produces a consistent effect on expectations of upcoming dialogue: even when it leads to poorer discriminative turn judgements than lexical content alone, it yields higher consensus among participants",
    "checked": true,
    "id": "9b9bfca7ac8b4a44c68a755901435c8d96f345b0",
    "semantic_title": "quantifying the perceptual value of lexical and non-lexical channels in speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tsubokura23_interspeech.html": {
    "title": "Relationships Between Gender, Personality Traits and Features of Multi-Modal Data to Responses to Spoken Dialog Systems Breakdown",
    "volume": "main",
    "abstract": "Automated dialog systems are currently being used in various applications, but it is unclear if they will ever be able to converse as naturally as humans do. One challenge is avoiding breakdowns during conversations due to inappropriate system utterances. Although many studies have focused on dialog breakdown detection, the influence of differences among individual users on dialog breakdowns and breakdown detection has not been sufficiently examined. In this study, we focus on individual differences thought to be related to emotional responses after breakdowns, specifically language, acoustic, and facial features, as well as gender and BigFive personality traits, to analyze differences in user responses to breakdowns. Our results suggest that gender and personality traits influence user responses to dialog breakdowns. For example, users with low Openness scores were more likely to express anger, while women were less likely to do so",
    "checked": true,
    "id": "91883360bfd0dcb77bc13264a43af8a600a2153f",
    "semantic_title": "relationships between gender, personality traits and features of multi-modal data to responses to spoken dialog systems breakdown",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23e_interspeech.html": {
    "title": "Speaker-aware Cross-modal Fusion Architecture for Conversational Emotion Recognition",
    "volume": "main",
    "abstract": "Conversational Emotion Recognition (CER) is an important topic in the construction of intelligent human-machine interaction systems. The emotion is mainly influenced by the conversational context and the speakers. In addition, sufficient utilization of the relevant features of both speech and text modes is also crucial to the performance of CER. Based on the above considerations, we propose a novel Speaker-aware Cross-modal Fusion Architecture (SCFA). Within a single modality, we design a conversation encoder, including a context encoder and a speaker-aware encoder, to model the conversational content and the intra- and inter-speaker influence, respectively. On this basis, cross-modal fusion attention is introduced to extract the cross-modal characteristics of the conversation, so as to better detect the emotions in conversation. We conduct experiments on the IEMOCAP and MELD datasets. Compared with state-of-the-art baselines, SCFA achieves better performance on average",
    "checked": true,
    "id": "585a679992bbb908c7c7f9ce3cdd033a832ea47e",
    "semantic_title": "speaker-aware cross-modal fusion architecture for conversational emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liao23_interspeech.html": {
    "title": "Blind Estimation of Room Impulse Response from Monaural Reverberant Speech with Segmental Generative Neural Network",
    "volume": "main",
    "abstract": "This paper presents a generative neural network to estimate room impulse response (RIR) directly from the received reverberant speech in single-channel scenario. Complex spectrogram of the reverberant speech is used as the input of an encoder to produce the compact acoustic embedding, which is then fed to a generator to construct the related time-domain acoustic response. To avoid a large model to generate the RIR with long taps, we propose SG-RIR, a novel segmental generative network that splits the RIR into segments and shares the network parameters across segments for blind RIR estimation. Experimental results show that the proposed model is capable of estimating the time-domain RIR with mean error of 0.008 in terms of both simulated and measured RIR test sets. The effectiveness is further verified by the achieved competitive estimation accuracy of two key room acoustic parameters (the reverberation time RT and the direct-to-reverberant ratio DRR) as compared to state-of-the-art approaches that are specific for RT and DRR estimation",
    "checked": true,
    "id": "6faeffd6dfc853a89248feaf78856e770651ed90",
    "semantic_title": "blind estimation of room impulse response from monaural reverberant speech with segmental generative neural network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ren23_interspeech.html": {
    "title": "Emotion-Aware Audio-Driven Face Animation via Contrastive Feature Disentanglement",
    "volume": "main",
    "abstract": "In this paper, we tackle the problem of audio-driven face animation which aims to synthesize a realistic talking face given a piece of driven speech. Directly modeling the mapping from audio feature to facial expression is challenging, since people tend to have different talking styles with momentary emotion states as well as identity-dependent vocal characteristics. To address this challenge, we propose a contrastive feature disentanglement method for emotion-aware face animation. The key idea is to disentangle the features for speech content, momentary emotion and identity-dependent vocal characteristics from audio features with a contrastive learning strategy. Experiments on public datasets show that our method can generate more realistic facial expression and enables synthesis of diversified face animation with different emotion",
    "checked": true,
    "id": "551829633213b7a42ccc471884885a091ac14ea2",
    "semantic_title": "emotion-aware audio-driven face animation via contrastive feature disentanglement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shimonishi23_interspeech.html": {
    "title": "Anomalous Sound Detection Based on Sound Separation",
    "volume": "main",
    "abstract": "This paper proposes an unsupervised anomalous sound detection method using sound separation. In factory environments, background noise and non-objective sounds obscure desired machine sounds, making it challenging to detect anomalous sounds. Therefore, using sounds not mixed with background noise or non-purpose sounds in the detection system is desirable. We compared two versions of our proposed method, one using sound separation as a pre-processing step and the other using separation-based outlier exposure that uses the error between two separated sounds. Based on the assumption that differences in separation performance between normal and anomalous sounds affect detection results, a sound separation model specific to a particular product type was used in both versions. Experimental results indicate that the proposed method improved anomalous sound detection performance for all Machine IDs, achieving a maximum improvement of 39%",
    "checked": true,
    "id": "03658a4a0f5a9f9fa7609f46b699d224e16ecce1",
    "semantic_title": "anomalous sound detection based on sound separation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fahed23_interspeech.html": {
    "title": "Random Forest Classification of Breathing Phases from Audio Signals Recorded using Mobile Devices",
    "volume": "main",
    "abstract": "Respiration rate (RR) and other respiratory features, such as inhale-to-exhale ratio (IER) and duration of breathing phases can be used as marker of respiratory and lung conditions and to modulate autonomic function in biofeedback applications. In this study, audio respiration signals were recorded by 112 participants using smartphones. RR was estimated using a frequency-domain method. Acoustic features were extracted from the audio signals and random forest was used to classify inhales, exhales and respiratory pauses, with ROC AUCs of 0.84 and 0.95 for inhales and exhales respectively. RR was estimated with a mean absolute error (MAE) of 0.63 bpm. IER was estimated with a MAE of 0.37, with 76% of the dataset reporting a MAE of less than 0.20. The results demonstrate a computationally efficient approach to estimate respiratory features from audio signals recorded using smartphones that can be easily implemented in real-time for large-scale home monitoring or biofeedback applications",
    "checked": true,
    "id": "4af40d14e5cbd97a3701b042d10a5d3c994fe678",
    "semantic_title": "random forest classification of breathing phases from audio signals recorded using mobile devices",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahn23b_interspeech.html": {
    "title": "GRAVO: Learning to Generate Relevant Audio from Visual Features with Noisy Online Videos",
    "volume": "main",
    "abstract": "Given a video, previous video-to-audio generation methods use a hierarchical auto-regressive language model to produce a sequence of audio tokens to be decoded into a waveform. The audio generation depends only on the previous audio token and the current image but ignores the surrounding images that may have useful information. To learn the relationships between image frames, in this paper, we introduce GRAVO (Generate Relevant Audio from Visual features with Online videos), which employs multi-head attention (MHA) to encode rich context information and guide the audio decoder to produce more accurate audio tokens. Moreover, two auxiliary losses are introduced to explicitly supervise the MHA behavior, maximizing the similarity between the MHA output vector and the target waveform representation while preserving the original visual semantic information. Experimental results demonstrate that GRAVO surpasses state-of-the-art models on ImageHear and VGG-Sound datasets",
    "checked": true,
    "id": "d23ce9a9db54a747b73d811b8eaf4dba6721c748",
    "semantic_title": "gravo: learning to generate relevant audio from visual features with noisy online videos",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhai23_interspeech.html": {
    "title": "Wav2ToBI: a new approach to automatic ToBI transcription",
    "volume": "main",
    "abstract": "ToBI is a prosody labeling system that transcribes American English prosody in terms of phonological tones and break indices. Previous works on automatic ToBI transcription require additional information such as word boundaries and use modular feature extraction with separately optimized feature detectors and classifiers. We are interested in investigating if a neural network-based approach would also result in high performance on automatic ToBI transcription without additional information. In this paper, we investigate the problem of pitch accent detection and prosody boundary detection using the Wav2vec 2.0 model with only acoustic information. Our model is trained on the Boston University Radio News Corpus and evaluated on both the Boston University Radio News Corpus and the Boston Directions Corpus. We show that it achieves an F1 score of 0.82 on pitch accent detection and 0.86 on phrase boundary detection. Code and model weights are available",
    "checked": true,
    "id": "c9644183da9c149073baf94a2fecc6a3910df439",
    "semantic_title": "wav2tobi: a new approach to automatic tobi transcription",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23b_interspeech.html": {
    "title": "Joint-Former: Jointly Regularized and Locally Down-sampled Conformer for Semi-supervised Sound Event Detection",
    "volume": "main",
    "abstract": "Semi-supervised Sound Event Detection (SSED) is to recognize the categories of events and mark their onset and offset times using a small amount of weakly-labeled and a large-scale of unlabeled data. To exploit unlabeled data effectively and reduce over-fitting, regularization techniques play a critical role in SSED. In this paper, we proposed a novel jointly regularized and locally down-sampled Conformer (Joint-Former) model for SSED. Joint-Former first locally down-samples the spectrogram and learns the token representations with high temporal resolution and low computational cost. Then, Joint-Former effectively exploits unlabelled data in SSED by integrating Mean-Teacher and Masked Spectrogram Modeling using joint regularization through a multitask learning framework. Extensive experiments on DCASE 2019, DCASE 2020, and DCASE 2021 task4 SSED datasets show that Joint-Former greatly outperformed existing methods",
    "checked": true,
    "id": "71f15440c6f9369eeb2b9399416a7c329a8dca5c",
    "semantic_title": "joint-former: jointly regularized and locally down-sampled conformer for semi-supervised sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/goel23_interspeech.html": {
    "title": "Towards Attention-based Contrastive Learning for Audio Spoof Detection",
    "volume": "main",
    "abstract": "Vision transformers (ViT) have made substantial progress for classification tasks in computer vision. Recently, Gong et. al. '21, introduced attention-based modeling for several audio tasks. However, relatively unexplored is the use of a ViT for audio spoof detection task. We bridge this gap and introduce ViTs for this task. A vanilla baseline built on fine-tuning the SSAST (Gong et. al. '22) audio ViT model achieves sub-optimal equal error rates (EERs). To improve performance, we propose a novel attention-based contrastive learning framework (SSAST-CL) that uses cross-attention to aid the representation learning. Experiments show that our framework successfully disentangles the bonafide and spoof classes and helps learn better classifiers for the task. With appropriate data augmentations policy, a model trained on our framework achieves competitive performance on the ASVSpoof 2021 challenge. We provide comparisons and ablation studies to justify our claim",
    "checked": true,
    "id": "7e97e03e9c4a8dfb45bfaeb3296716f7b261850d",
    "semantic_title": "towards attention-based contrastive learning for audio spoof detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xin23d_interspeech.html": {
    "title": "Masked Audio Modeling with CLAP and Multi-Objective Learning",
    "volume": "main",
    "abstract": "Most existing masked audio modeling (MAM) methods learn audio representations by masking and reconstructing local spectrogram patches. However, the reconstruction loss mainly accounts for the signal-level quality of the reconstructed spectrogram and is still limited in extracting high-level audio semantics. In this paper, we propose to enhance the semantic modeling of MAM by distilling cross-modality knowledge from contrastive language-audio pretraining (CLAP) representations for both masked and unmasked regions (MAM-CLAP) and leveraging a multi-objective learning strategy with a supervised classification branch (SupMAM), thereby providing more semantic knowledge for MAM and enabling it to effectively learn global features from labels. Experiments show that our methods significantly improve the performance on multiple downstream tasks. Furthermore, by combining our MAM-CLAP with SupMAM, we can achieve new state-of-the-art on various audio and speech classification tasks, exceeding previous self-supervised and supervised pretraining methods",
    "checked": true,
    "id": "664edaab7a29dda2c59be94f05c654c3e61f6c3f",
    "semantic_title": "masked audio modeling with clap and multi-objective learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rusci23_interspeech.html": {
    "title": "Few-Shot Open-Set Learning for On-Device Customization of KeyWord Spotting Systems",
    "volume": "main",
    "abstract": "A personalized KeyWord Spotting (KWS) pipeline typically requires the training of a Deep Learning model on a large set of user-defined speech utterances, preventing fast customization directly applied on-device. To fill this gap, this paper investigates few-shot learning methods for open-set KWS classification by combining a deep feature encoder with a prototype-based classifier. With user-defined keywords from 10 classes of the Google Speech Command dataset, our study reports an accuracy of up to 76% in a 10-shot scenario while the false acceptance rate of unknown data is kept to 5%. In the analyzed settings, the usage of the triplet loss to train an encoder with normalized output features performs better than the prototypical networks jointly trained with a generator of dummy unknown-class prototypes. This design is also more effective than encoders trained on a classification problem and features fewer parameters than other iso-accuracy approaches",
    "checked": true,
    "id": "190f2b31d19003f02e25ea4a153fb0dfbb381dfc",
    "semantic_title": "few-shot open-set learning for on-device customization of keyword spotting systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/azeemi23_interspeech.html": {
    "title": "Self-Supervised Dataset Pruning for Efficient Training in Audio Anti-spoofing",
    "volume": "main",
    "abstract": "The computational cost for training neural anti-spoofing models has rapidly increased due to larger network architectures. Several dataset-pruning metrics have been proposed to increase the training efficiency of these models. However, these metrics require example labels and an initial training step to compute example scores which is computationally intensive. We propose a novel self-supervised pruning metric for efficient dataset pruning in neural anti-spoofing models. Our method identifies important examples and prunes the dataset in an efficient, self-supervised manner using the clustered embedding representation of audios. We demonstrate that our method exceeds the performance of four other pruning metrics on the ASVSpoof 2019 dataset across two anti-spoofing models while being 91% computationally more efficient. We also find differences in the distribution of certain attacks, which helps explain the better performance of self-supervised pruning over other metrics",
    "checked": true,
    "id": "00e19f0636f49ea712087972b710b0ff467a4787",
    "semantic_title": "self-supervised dataset pruning for efficient training in audio anti-spoofing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23b_interspeech.html": {
    "title": "Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR",
    "volume": "main",
    "abstract": "We propose a method of segmenting long-form speech by separating semantically complete sentences within the utterance. This prevents the ASR decoder from needlessly processing faraway context while also preventing it from missing relevant context within the current sentence. Semantically complete sentence boundaries are typically demarcated by punctuation in written text; but unfortunately, spoken real-world utterances rarely contain punctuation. We address this limitation by distilling punctuation knowledge from a bidirectional teacher language model (LM) trained on written, punctuated text. We compare our segmenter, which is distilled from the LM teacher, against a segmenter distilled from a acoustic-pause-based teacher used in other works, on a streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2% relative WER gain along with a 60 ms median end-of-segment latency reduction on a YouTube captioning task",
    "checked": true,
    "id": "a18c778ec3b8aa2e0846c7d81f260db0013bc6bd",
    "semantic_title": "semantic segmentation with bidirectional language models improves long-form asr",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mariotte23_interspeech.html": {
    "title": "Multi-microphone Automatic Speech Segmentation in Meetings Based on Circular Harmonics Features",
    "volume": "main",
    "abstract": "Speaker diarization is the task of answering Who spoke and when? in an audio stream. Pipeline systems rely on speech segmentation to extract speakers' segments and achieve robust speaker diarization. This paper proposes a common framework to solve three segmentation tasks in the distant speech scenario: Voice Activity Detection (VAD), Overlapped Speech Detection (OSD), and Speaker Change Detection (SCD). In the literature, a few studies investigate the multi-microphone distant speech scenario. In this work, we propose a new set of spatial features based on direction-of-arrival estimations in the circular harmonic domain (CH-DOA). These spatial features are extracted from multi-microphone audio data and combined with standard acoustic features. Experiments on the AMI meeting corpus show that CH-DOA can improve the segmentation while being robust in case of deactivated microphones",
    "checked": true,
    "id": "8730f1ae4447bfad9372a58625dac229da43087f",
    "semantic_title": "multi-microphone automatic speech segmentation in meetings based on circular harmonics features",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23h_interspeech.html": {
    "title": "Advanced RawNet2 with Attention-based Channel Masking for Synthetic Speech Detection",
    "volume": "main",
    "abstract": "Automatic speaker verification (ASV) systems are often vulnerable to spoofing attacks, particularly unseen attacks. Due to the diversity of text-to-speech and voice conversion algorithms, how to improve the generalization ability of synthetic speech detection systems is a challenging issue. To address this issue, we propose an advanced RawNet2 (ARawNet2) by introducing an attention-based channel masking (ACM) block to improve the RawNet2, with three main components: the squeeze-and-excitation, the channel masking, and a global-local feature aggregation. The effectiveness of the proposed system is evaluated on both the ASVspoof 2019 and ASVspoof 2021 datasets. Specifically, the ARawNet2 achieves an EER of 4.61% on the ASVspoof 2019 logical access (LA) task, and on the ASVspoof 2021 LA and speech deepfake (DF) tasks, it achieves EER of 8.36% and 19.03%, which obtains relative 12.00% and 14.97% EER reductions over the RawNet2 baseline, respectively",
    "checked": true,
    "id": "1568e8059166e695436c05d7af619f130793f7b0",
    "semantic_title": "advanced rawnet2 with attention-based channel masking for synthetic speech detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/martinezsevilla23_interspeech.html": {
    "title": "Insights into end-to-end audio-to-score transcription with real recordings: A case study with saxophone works",
    "volume": "main",
    "abstract": "Neural end-to-end Audio-to-Score (A2S) transcription aims to retrieve a score that encodes the music content of an audio recording in a single step. Due to the recentness of this formulation, the existing works have exclusively addressed controlled scenarios with synthetic data that fail to provide conclusions applicable to real-world cases. In response to this gap in the literature, this work introduces a novel assortment of real saxophone recordings---together with their digital scores---and poses several experimental scenarios involving real and synthetic data. The obtained results confirm the adequacy of this A2S framework to deal with real data as well as proving the relevance of leveraging synthetic interpretations to improve the recognition rate in scenarios with real-data scarcity",
    "checked": true,
    "id": "1ab7b812887b4b1073ed532bcc005d63aad8f5f2",
    "semantic_title": "insights into end-to-end audio-to-score transcription with real recordings: a case study with saxophone works",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23d_interspeech.html": {
    "title": "Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers",
    "volume": "main",
    "abstract": "In this paper, we focus on Whisper, a recent automatic speech recognition model trained with a massive 680k hour labeled speech corpus recorded in diverse conditions. We first show an interesting finding that while Whisper is very robust against real-world background sounds (e.g., music), its audio representation is actually not noise-invariant, but is instead highly correlated to non-speech sounds, indicating that Whisper recognizes speech conditioned on the noise type. With this finding, we build a unified audio tagging and speech recognition model Whisper-AT by freezing the backbone of Whisper, and training a lightweight audio tagging model on top of it. With <1% extra computational cost, Whisper-AT can recognize audio events, in addition to spoken text, in a single forward pass",
    "checked": true,
    "id": "98bd673440c579bcca2d434bca86f97c9b40c74c",
    "semantic_title": "whisper-at: noise-robust automatic speech recognizers are also strong general audio event taggers",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gong23b_interspeech.html": {
    "title": "Synthetic Voice Spoofing Detection based on Feature Pyramid Conformer",
    "volume": "main",
    "abstract": "In speech anti-spoofing, artefacts used to detect spoofed speech are often located in specific sub-bands. Previous works often use Convolution Neural Networks (CNNs) as backbone which are good at capturing local features. However, if artefacts simultaneously exist in different sub-bands, CNNs cannot model this kind of information. Thus, we propose to use Feature Pyramid Conformer to solve this issue. Conformer can capture both local and global features. We aggregate the outputs of each Conformer block with Feature Pyramid Module. Through addition and lateral connection, the aggregation can be better integrated. Besides, to improve generalization of detecting unknown attacks, we propose to adopt Elastic penalty Margin Softmax. It can enhance intra-class compactness and inter-class discrepancy flexibly. Without data augmentaion, our system achieve an Equal Error Rate (EER) of 1.65% on the evaluation set of ASVspooof 2019 logical access, outperforming most existing systems",
    "checked": true,
    "id": "4071c9a56c536bf39d4c107edb1231150a09db38",
    "semantic_title": "synthetic voice spoofing detection based on feature pyramid conformer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23c_interspeech.html": {
    "title": "Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection",
    "volume": "main",
    "abstract": "In recent years, Audio Deepfake Detection (ADD) models have shown promising results in intra-domain. However, they do not perform well in cross-domain scenarios. This is mainly due to the limited variety of domain types and attack methods in training data, as well as insufficient research on hidden feature representation. To address these issues, we present W2VASDG, a generalized ADD system including a self-supervised representation front-end and a domain generalization backbone. Furthermore, we try to learn an ideal feature space which aggregates real speech and separates fake speech. Fake speech varies significantly by different forgery methods, while real speech varies less. In light of this, we further propose the aggregation and separation domain generalization (ASDG) method as the back-end to learn a domain invariant feature representation. Experiments show that our W2V-ASDG outperforms baseline models in cross-domains and gets the lowest average equal error rates (EER) of 4.60%",
    "checked": true,
    "id": "8d48b4395ad4f61717e5fc202d0c45ac4f1f1365",
    "semantic_title": "learning a self-supervised domain-invariant feature representation for generalized audio deepfake detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kerpicci23_interspeech.html": {
    "title": "Application of Knowledge Distillation to Multi-Task Speech Representation Learning",
    "volume": "main",
    "abstract": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models",
    "checked": true,
    "id": "925ac929940720f16d0c54cd3ec0cba341f95bdf",
    "semantic_title": "application of knowledge distillation to multi-task speech representation learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23g_interspeech.html": {
    "title": "DeCoR: Defy Knowledge Forgetting by Predicting Earlier Audio Codes",
    "volume": "main",
    "abstract": "Lifelong audio feature extraction involves learning new sound classes incrementally, which is essential for adapting to new data distributions over time. However, optimizing the model only on new data can lead to catastrophic forgetting of previously learned tasks, which undermines the model's ability to perform well over the long term. This paper introduces a new approach to continual audio representation learning called DeCoR. Unlike other methods that store previous data, features, or models, DeCoR indirectly distills knowledge from an earlier model to the latest by predicting quantization indices from a delayed codebook. We demonstrate that DeCoR improves acoustic scene classification accuracy and integrates well with continual self-supervised representation learning. Our approach introduces minimal storage and computation overhead, making it a lightweight and efficient solution for continual learning",
    "checked": true,
    "id": "39f1ee5c35c7f514d57c8465dba2d26502ee140c",
    "semantic_title": "decor: defy knowledge forgetting by predicting earlier audio codes",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/almudevar23_interspeech.html": {
    "title": "Variational Classifier for Unsupervised Anomalous Sound Detection under Domain Generalization",
    "volume": "main",
    "abstract": "Unsupervised anomalous sound detection typically involves using a classifier with the last layer removed to extract embeddings. After that the cosine distance between train and test embeddings as anomaly score is used. In this paper, we propose a new idea which we call variational classifier that force the embeddings to follow a distribution imposed by design that can depend on the class of the input among other factors. To achieve this goal, in addition to the cross-entropy, we add to the loss function the KL divergence between these distributions and the one followed by the training embeddings. This enhances the ability of the system to differentiate between classes and it allows us to use sampling methods and to calculate the log-likelihood of a test embedding in the train embeddings distributions. We tested this proposal on the DCASE 2022 Task 2 dataset and observed improvements in both classification and unsupervised anomaly detection, which is the primary task",
    "checked": true,
    "id": "a07379ca1717c16fbe9c505f6435472aa99bcd0c",
    "semantic_title": "variational classifier for unsupervised anomalous sound detection under domain generalization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/feng23c_interspeech.html": {
    "title": "FlexiAST: Flexibility is What AST Needs",
    "volume": "main",
    "abstract": "The objective of this work is to give patch-size flexibility to Audio Spectrogram Transformers (AST). Recent advancements in ASTs have shown superior performance in various audio-based tasks. However, the performance of standard ASTs drastically drops when evaluated at different patch sizes than they were trained at. As a result, AST models are typically retrained to accommodate changes in patch sizes. To overcome this limitation, this paper demonstrates a training procedure to provide flexibility to standard AST models without architectural changes, allowing them to work with various patch sizes at the inference stage - FlexiAST. This proposed training approach simply utilizes random patch size selection and resizing of patch and positional embedding weights. Our experiments show that FlexiAST gives similar performance to standard AST models while maintaining its evaluation ability at various patch sizes on different datasets for audio classification tasks",
    "checked": true,
    "id": "b61b85112c5553a6546102480758712c82163d13",
    "semantic_title": "flexiast: flexibility is what ast needs",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23c_interspeech.html": {
    "title": "MCR-Data2vec 2.0: Improving Self-supervised Speech Pre-training via Model-level Consistency Regularization",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has shown significant progress in speech processing tasks. However, despite the intrinsic randomness in the Transformer structure, such as dropout variants and layer-drop, improving the model-level consistency remains under-explored in the speech SSL literature. To address this, we propose a new pre-training method that uses consistency regularization to improve Data2vec 2.0, the recent state-of-the-art (SOTA) SSL model. Specifically, the proposed method involves sampling two different student sub-models within the Data2vec 2.0 framework, enabling two output variants derived from a single input without additional parameters. Subsequently, we regularize the outputs from the student sub-models to be consistent and require them to predict the representation of the teacher model. Our experimental results demonstrate that the proposed approach improves the SSL model's robustness and generalization ability, resulting in SOTA results on the SUPERB benchmark",
    "checked": true,
    "id": "b5409e42cb24bca75569183e07f23b0b5c4c06c3",
    "semantic_title": "mcr-data2vec 2.0: improving self-supervised speech pre-training via model-level consistency regularization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23l_interspeech.html": {
    "title": "Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention",
    "volume": "main",
    "abstract": "Audio captioning aims to generate text descriptions of audio clips. In the real world, many objects produce similar sounds. How to accurately recognize ambiguous sounds is a major challenge for audio captioning. In this work, inspired by inherent human multimodal perception, we propose visually-aware audio captioning, which makes use of visual information to help the description of ambiguous sounding objects. Specifically, we introduce an off-the-shelf visual encoder to extract video features and incorporate the visual features into an audio captioning system. Furthermore, to better exploit complementary audio-visual contexts, we propose an audio-visual attention mechanism that adaptively integrates audio and visual context and removes the redundant information in the latent space. Experimental results on AudioCaps, the largest audio captioning dataset, show that our proposed method achieves state-of-the-art results on machine translation metrics",
    "checked": true,
    "id": "2b6e41761ac23c106963677304a899a57c7d75e0",
    "semantic_title": "visually-aware audio captioning with adaptive audio-visual attention",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ditthapron23_interspeech.html": {
    "title": "Masking Kernel for Learning Energy-Efficient Representations for Speaker Recognition and Mobile Health",
    "volume": "main",
    "abstract": "Modern smartphones possess hardware for audio acquisition and to perform speech processing tasks such as speaker recognition and health assessment. However, energy consumption remains a concern, especially for resource-intensive DNNs. Prior work has improved the DNN energy efficiency by utilizing a compact model or reducing the dimensions of speech features. Both approaches reduced energy consumption during DNN inference but not during speech acquisition. This paper proposes using a masking kernel integrated into gradient descent during DNN training to learn the most energy-efficient speech length and sampling rate for windowing, a common step for sample construction. To determine the most energy-optimal parameters, a masking function with non-zero derivatives was combined with a low-pass filter. The proposed approach minimizes the energy consumption of both data collection and inference by 57%, and is competitive with speaker recognition and traumatic brain injury detection baselines",
    "checked": true,
    "id": "6057a23f32f9864191d43068463afe68bd127be2",
    "semantic_title": "masking kernel for learning energy-efficient representations for speaker recognition and mobile health",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiang23_interspeech.html": {
    "title": "eSTImate: A Real-time Speech Transmission Index Estimator With Speech Enhancement Auxiliary Task Using Self-Attention Feature Pyramid Network",
    "volume": "main",
    "abstract": "The Speech Transmission Index (STI) is a crucial metric for evaluating speech intelligibility, but its standard measurement method is too complicated for real-time applications. Though recently proposed deep learning based STI estimation schemes can effectively address the problem, existing methods still fall short of covering all possible STI scenarios. This paper presents eSTImate: an end-to-end deep learning system for real-time STI blind estimation that integrates the tasks of STI estimation and speech enhancement through a feature pyramid auxiliary learning architecture and incorporates multi-head attention mechanisms. The proposed model demonstrates the performance of state-of-the-art, achieving a low mean absolute error of 0.016 and root mean square error of 0.021 on the constructed dataset that covers the whole range of STI, highlighting its potential to provide accurate and consistent real-time STI estimation across diverse real-world scenarios",
    "checked": true,
    "id": "12a5d188e284a0a3671ab48ace8fbee63caa3d0f",
    "semantic_title": "estimate: a real-time speech transmission index estimator with speech enhancement auxiliary task using self-attention feature pyramid network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23n_interspeech.html": {
    "title": "Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive Feature Learning in Speech Enhancement",
    "volume": "main",
    "abstract": "Current speech enhancement (SE) research has largely neglected channel attention and spatial attention, and encoder-decoder architecture-based networks have not adequately considered how to provide efficient inputs to the intermediate enhancement layer. To address these issues, this paper proposes a time-frequency (T-F) domain SE network (DPCFCS-Net) that incorporates improved densely connected blocks, dual-path modules, convolution-augmented transformers (conformers), channel attention, and spatial attention. Compared with previous models, our proposed model has a more efficient encoder-decoder and can learn comprehensive features. Experimental results on the VCTK+DEMAND dataset demonstrate that our method outperforms existing techniques in SE performance. Furthermore, the improved densely connected block and two dimensions attention module developed in this work are highly adaptable and easily integrated into existing networks",
    "checked": true,
    "id": "e20c3e13cff70666a17fa8c246124adeb6218cfa",
    "semantic_title": "efficient encoder-decoder and dual-path conformer for comprehensive feature learning in speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23b_interspeech.html": {
    "title": "Privacy-preserving Representation Learning for Speech Understanding",
    "volume": "main",
    "abstract": "Existing privacy-preserving speech representation learning methods target a single application domain. In this paper, we present a novel framework to anonymize utterance-level speech embeddings generated by pre-trained encoders and show its effectiveness for a range of speech classification tasks. Specifically, given the representations from a pre-trained encoder, we train a Transformer to estimate the representations for the same utterances spoken by other speakers. During inference, the extracted representations can be converted into different identities to preserve privacy. We compare the results with the voice anonymization baselines from the VoicePrivacy 2022 challenge. We evaluate our framework on speaker identification for privacy and emotion recognition, depression classification, and intent classification for utility. Our method outperforms the baselines on privacy and utility in paralinguistic tasks and achieves comparable performance for intent classification",
    "checked": true,
    "id": "ea281a8cc0e071e33592627c31dce77ed88e46e7",
    "semantic_title": "privacy-preserving representation learning for speech understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panariello23_interspeech.html": {
    "title": "Vocoder drift in x-vector–based speaker anonymization",
    "volume": "main",
    "abstract": "State-of-the-art approaches to speaker anonymization typically employ some form of perturbation function to conceal speaker information contained within an x-vector embedding, then resynthesize utterances in the voice of a new pseudo-speaker using a vocoder. Strategies to improve the x-vector anonymization function have attracted considerable research effort, whereas vocoder impacts are generally neglected. In this paper, we show that the impact of the vocoder is substantial and sometimes dominant. The vocoder drift, namely the difference between the x-vector vocoder input and that which can be extracted subsequently from the output, is learnable and can hence be reversed by an attacker; anonymization can be undone and the level of privacy protection provided by such approaches might be weaker than previously thought. The findings call into question the focus upon x-vector anonymization, prompting the need for greater attention to vocoder impacts and stronger attack models alike",
    "checked": true,
    "id": "ad5dcdd727278dbaa5b12f893554770f640ecb61",
    "semantic_title": "vocoder drift in x-vector–based speaker anonymization",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panariello23b_interspeech.html": {
    "title": "Malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems",
    "volume": "main",
    "abstract": "We present Malafide, a universal adversarial attack against automatic speaker verification (ASV) spoofing countermeasures (CMs). By introducing convolutional noise using an optimised linear time-invariant filter, Malafide attacks can be used to compromise CM reliability while preserving other speech attributes such as quality and the speaker's voice. In contrast to other adversarial attacks proposed recently, Malafide filters are optimised independently of the input utterance and duration, are tuned instead to the underlying spoofing attack, and require the optimisation of only a small number of filter coefficients. Even so, they degrade CM performance estimates by an order of magnitude, even in black-box settings, and can also be configured to overcome integrated CM and ASV subsystems. Integrated solutions that use self-supervised learning CMs, however, are more robust, under both black-box and white-box settings",
    "checked": true,
    "id": "247c9fcb129c21d4441d19c44aef97a40cdbf7e5",
    "semantic_title": "malafide: a novel adversarial convolutive noise attack against deepfake and spoofing detection systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zaiem23b_interspeech.html": {
    "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models",
    "checked": true,
    "id": "08a62562feed7b3709f939092021fcc5b7794d07",
    "semantic_title": "speech self-supervised representation benchmarking: are we doing it right?",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23d_interspeech.html": {
    "title": "An extension of disentanglement metrics and its application to voice",
    "volume": "main",
    "abstract": "In representation learning, the promise of disentanglement methods is to decompose an input signal into a set of independent and interpretable attributes. Some metrics, such as the DCI or MIG scores, have been proposed to evaluate how much this goal is reached. They analyse the relationship between the representation components and the desirable attributes. This paper shows that, even when applied to synthetic datasets generated from a closed list of generative factors, these metrics can be too optimistic. In particular, it reports that a generative factor can be recovered from an altered disentangled representation from which it has been supposedly removed, according to the metrics. Based on this observation, a new criterion called latent decimation is proposed to evaluate disentanglement through the accuracy of factors prediction from subsets of latents. A new metric called MIDCI is defined, and its relevance is demonstrated on voice data",
    "checked": true,
    "id": "7420178b45d43b8486465c4fbaa527262cd8cb2b",
    "semantic_title": "an extension of disentanglement metrics and its application to voice",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/abdullah23_interspeech.html": {
    "title": "An Information-Theoretic Analysis of Self-supervised Discrete Representations of Speech",
    "volume": "main",
    "abstract": "Self-supervised representation learning for speech often involves a quantization step that transforms the acoustic input into discrete units. However, it remains unclear how to characterize the relationship between these discrete units and abstract phonetic categories such as phonemes. In this paper, we develop an information-theoretic framework whereby we represent each phonetic category as a distribution over discrete units. We then apply our framework to two different self-supervised models (namely, wav2vec 2.0 and XLSR) and use American English speech as a case study. Our study demonstrates that the entropy of phonetic distributions reflects the variability of the underlying speech sounds, with phonetically similar sounds exhibiting similar distributions. While our study confirms the lack of direct one-to-one correspondence, we find an intriguing indirect relationship between phonetic categories and discrete units",
    "checked": true,
    "id": "2c64730917a56fa78ce37abde56a5c78685c0de8",
    "semantic_title": "an information-theoretic analysis of self-supervised discrete representations of speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ashihara23_interspeech.html": {
    "title": "SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) for speech representation has been successfully applied in various downstream tasks, such as speech and speaker recognition. More recently, speech SSL models have also been shown to be beneficial in advancing spoken language understanding tasks, implying that the SSL models have the potential to learn not only acoustic but also linguistic information. In this paper, we aim to clarify if speech SSL techniques can well capture linguistic knowledge. For this purpose, we introduce SpeechGLUE, a speech version of the General Language Understanding Evaluation (GLUE) benchmark. Since GLUE comprises a variety of natural language understanding tasks, SpeechGLUE can elucidate the degree of linguistic ability of speech SSL models. Experiments demonstrate that speech SSL models, although inferior to text-based SSL models, perform better than baselines, suggesting that they can acquire a certain amount of general linguistic knowledge from just unlabeled speech data",
    "checked": true,
    "id": "21fe2b5fb20f3327baa602fe4171c66284dd1e16",
    "semantic_title": "speechglue: how well can self-supervised speech models capture linguistic knowledge?",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sasou23_interspeech.html": {
    "title": "Comparison of GIF- and SSL-based Features in Pathological-voice Detection",
    "volume": "main",
    "abstract": "A system that automatically detects voice pathology from acoustic signals enables non-invasive, low cost, and objective assessment of speech disorders. Therefore, it is expected to accelerate and improve the diagnosis and clinical treatment of patients. Pathological voices are symptoms of impairments in the articulation of speech sound, fluency, and/or voice. We consider that direct extraction of features from the glottal flow estimated by glottal inverse filtering (GIF) is a promising approach to pathological-voice detection. To precisely estimate the glottal flow, we propose a novel GIF method that combines constrained autoregressive hidden Markov model (CAR–HMM) analysis with automatic topology generation of the excitation HMM. To evaluate the effectiveness of the features extracted from the estimated glottal flow during pathological-voice detection, we employ the Saarbrücken Voice Database. We also compare the features obtained by the proposed CAR–HMM with those obtained by pre-trained models based on self-supervised learning (SSL). The experimental results confirmed that the CAR–HMM-based method can outperform the SSL-based methods",
    "checked": true,
    "id": "c3b1e1518f4de6fad99f76d52fb21560c8d898f9",
    "semantic_title": "comparison of gif- and ssl-based features in pathological-voice detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23c_interspeech.html": {
    "title": "What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions",
    "volume": "main",
    "abstract": "There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of speech processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end. In this paper, we investigate this question on keyword spotting, speech-based emotion recognition and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt. Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions. This in turn enables a system trained on clean speech to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper",
    "checked": true,
    "id": "f278036c2546252222c2e8df454b2cd45789a9e6",
    "semantic_title": "what is learnt by the learnable front-end (leaf)? adapting per-channel energy normalisation (pcen) to noisy conditions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/masumura23_interspeech.html": {
    "title": "End-to-End Joint Target and Non-Target Speakers ASR",
    "volume": "main",
    "abstract": "This paper proposes a novel automatic speech recognition (ASR) system that can transcribe individual speaker's speech while identifying whether they are target or non-target speakers from multi-talker overlapped speech. Target-speaker ASR systems are a promising way to only transcribe a target speaker's speech by enrolling the target speaker's information. However, in conversational ASR applications, transcribing both the target speaker's speech and non-target speakers' ones is often required to understand interactive information. To naturally consider both target and non-target speakers in a single ASR model, our idea is to extend autoregressive modeling-based multi-talker ASR systems to utilize the enrollment speech of the target speaker. Our proposed ASR is performed by recursively generating both textual tokens and tokens that represent target or non-target speakers. Our experiments demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "10b1fa066de6e14c42ed04b7b770575bb462b79e",
    "semantic_title": "end-to-end joint target and non-target speakers asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23c_interspeech.html": {
    "title": "Improving Frame-level Classifier for Word Timings with Non-peaky CTC in End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end (E2E) systems have shown comparable performance to hybrid systems for automatic speech recognition (ASR). Word timings, as a by-product of ASR, are essential in many applications, especially for subtitling and computer-aided pronunciation training. In this paper, we improve the frame-level classifier for word timings in E2E system by introducing label priors in connectionist temporal classification (CTC) loss, which is adopted from prior works, and combining low-level Mel-scale filter banks with high-level ASR encoder output as input feature. On the internal Chinese corpus, the proposed method achieves 95.68%/94.18% compared to the hybrid system 93.0%/90.22% on the word timing accuracy metrics. It also surpass a previous E2E approach with an absolute increase of 4.80%/8.02% on the metrics on 7 languages. In addition, we further improve word timing accuracy by delaying CTC peaks with frame-wise knowledge distillation, though only experimenting on LibriSpeech",
    "checked": true,
    "id": "88567faceac81933a545532f41448e31a0c33b0c",
    "semantic_title": "improving frame-level classifier for word timings with non-peaky ctc in end-to-end automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/makishima23_interspeech.html": {
    "title": "Joint Autoregressive Modeling of End-to-End Multi-Talker Overlapped Speech Recognition and Utterance-level Timestamp Prediction",
    "volume": "main",
    "abstract": "This paper proposes autoregressive modeling of the joint multi-talker automatic speech recognition (ASR) and timestamp prediction. Autoregressive modeling of multi-talker ASR is a simple and promising approach. However, it does not predict utterance timestamp information despite its being important in practice. To address this problem, our key idea is to extend autoregressive-modeling-based multi-talker ASR to predict quantized timestamp tokens representing the start and end time of an utterance. Our method estimates transcription and utterance-level timestamp tokens of multiple speakers one after another. This enables joint modeling of multi-talker ASR and timestamps prediction without changing the simple autoregressive modeling of the conventional multi-talker ASR. Experimental results show that our method outperforms the ASR performance of conventional autoregressive multi-talker ASR without timestamp prediction and achieves promising timestamp prediction accuracy",
    "checked": true,
    "id": "693dc3da48b2babd66ff85d43f546e04710b3055",
    "semantic_title": "joint autoregressive modeling of end-to-end multi-talker overlapped speech recognition and utterance-level timestamp prediction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23_interspeech.html": {
    "title": "Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition",
    "volume": "main",
    "abstract": "Automatic speech recognition (ASR) systems degrade significantly under noisy conditions. Recently, speech enhancement (SE) is introduced as front-end to reduce noise for ASR, but it also suppresses some important speech information, i.e., over-suppression. To alleviate this, we propose a dual-path style learning approach for end-to-end noise-robust speech recognition (DPSL-ASR). Specifically, we first introduce clean speech feature along with the fused feature from IFF-Net as dual-path inputs to recover the suppressed information. Then, we propose style learning to map the fused feature close to clean feature, in order to learn latent speech information from the latter, i.e., clean \"speech style\". Furthermore, we also minimize the distance of final ASR outputs in two paths to improve noise-robustness. Experiments show that the proposed approach achieves relative word error rate (WER) reductions of 10.6% and 8.6% over the best IFF-Net baseline, on RATS and CHiME-4 datasets respectively",
    "checked": true,
    "id": "14098f596e2fec819bed7490a7436fe3584966c9",
    "semantic_title": "dual-path style learning for end-to-end noise-robust speech recognition",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23_interspeech.html": {
    "title": "Multi-pass Training and Cross-information Fusion for Low-resource End-to-end Accented Speech Recognition",
    "volume": "main",
    "abstract": "Low-resource accented speech recognition is one of the important challenges faced by current ASR technology in practical applications. In this study, we propose a Conformer-based architecture, called Aformer, to leverage both the acoustic information from large non-accented and limited accented training data. Specifically, a general encoder and an accent encoder are designed in the Aformer to extract complementary acoustic information. Moreover, we propose to train the Aformer in a multi-pass manner, and investigate three cross-information fusion methods to effectively combine the information from both general and accent encoders. All experiments are conducted on both the accented English and Mandarin ASR tasks. Results show that our proposed methods outperform the strong Conformer baseline by relative 10.2% to 24.5% word/character error rate reduction on six in-domain and out-of-domain accented test sets",
    "checked": true,
    "id": "e40636e20e17f80cc9ea4a917710819b045bc20e",
    "semantic_title": "multi-pass training and cross-information fusion for low-resource end-to-end accented speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bataev23_interspeech.html": {
    "title": "Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator",
    "volume": "main",
    "abstract": "We propose an end-to-end Automatic Speech Recognition (ASR) system that can be trained on transcribed speech data, text-only data, or a mixture of both. The proposed model uses an integrated auxiliary block for text-based training. This block combines a non-autoregressive multi-speaker text-to-mel-spectrogram generator with a GAN-based enhancer to improve the spectrogram quality. The proposed system can generate a mel-spectrogram dynamically during training. It can be used to adapt the ASR model to a new domain by using text-only data from this domain. We demonstrate that the proposed training method significantly improves ASR accuracy compared to the system trained on transcribed speech only. It also surpasses cascade TTS systems with the vocoder in the adaptation quality and training speed",
    "checked": true,
    "id": "be1f6e3160d44bee221e96f4fee6f02fa0adaf0c",
    "semantic_title": "text-only domain adaptation for end-to-end asr using integrated text-to-mel-spectrogram generator",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23_interspeech.html": {
    "title": "Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling",
    "volume": "main",
    "abstract": "We study speech intent classification and slot filling (SICSF) by proposing to use an encoder pretrained on speech recognition (ASR) to initialize an end-to-end (E2E) Conformer-Transformer model, which achieves the new state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and 82.27% SLURP-F1. We compare our model with encoders pre-trained on self-supervised learning (SSL), and show that ASR pretraining is much more effective than SSL for SICSF. To explore parameter efficiency, we freeze the encoder and add Adapter modules, and show that parameter efficiency is only achievable with an ASR-pretrained encoder, while the SSL encoder needs full finetuning to achieve comparable results. In addition, we provide an in-depth comparison on end-to-end models versus cascading models (ASR+NLU), and show that E2E models are better than cascaded models unless an oracle ASR model is provided. Last but not least, our model is the first E2E model that achieves the same performance as cascading models with oracle ASR. Code, checkpoints and configs are available",
    "checked": true,
    "id": "9e8fa730a4bdb65b140afd59881a6655780b7df6",
    "semantic_title": "leveraging pretrained asr encoders for effective and efficient end-to-end speech intent classification and slot filling",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23b_interspeech.html": {
    "title": "Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models",
    "volume": "main",
    "abstract": "Although pre-trained language models show good performance on various natural language processing tasks, they often rely on non-causal features and patterns to determine the outcome. For natural language inference tasks, previous results have shown that even a model trained on a large number of data fails to perform well on counterfactually revised data, indicating that the model is not robustly learning the semantics of the classes. In this paper, we propose a method in which we use token-based and sentence-based augmentation methods to generate counterfactual sentence pairs that belong to each class, and apply contrastive learning to help the model learn the difference between sentence pairs of different classes with similar contexts. Evaluation results with counterfactually-revised dataset and general NLI datasets show that the proposed method can improve the performance and robustness of the NLI model",
    "checked": true,
    "id": "fb8d0982d76945e136836a57e7a23907c21c2fb2",
    "semantic_title": "relation-based counterfactual data augmentation and contrastive learning for robustifying natural language inference models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/matsuura23_interspeech.html": {
    "title": "Transfer Learning from Pre-trained Language Models Improves End-to-End Speech Summarization",
    "volume": "main",
    "abstract": "End-to-end speech summarization (E2E SSum) directly summarizes input speech into easy-to-read short sentences with a single model. This approach is promising because it, in contrast to the conventional cascade approach, can utilize full acoustical information and mitigate to the propagation of transcription errors. However, due to the high cost of collecting speech-summary pairs, an E2E SSum model tends to suffer from training data scarcity and output unnatural sentences. To overcome this drawback, we propose for the first time to integrate a pre-trained language model (LM), which is highly capable of generating natural sentences, into the E2E SSum decoder via transfer learning. In addition, to reduce the gap between the independently pre-trained encoder and decoder, we also propose to transfer the baseline E2E SSum encoder instead of the commonly used automatic speech recognition encoder. Experimental results show that the proposed model outperforms baseline and data augmented models",
    "checked": true,
    "id": "3b1fce8bbd9c326d00e912bef3def5a6455daa3c",
    "semantic_title": "transfer learning from pre-trained language models improves end-to-end speech summarization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deshmukh23_interspeech.html": {
    "title": "Audio Retrieval with WavText5K and CLAP Training",
    "volume": "main",
    "abstract": "Text-based audio retrieval takes a natural language query to retrieve relevant audio files in a database. Most retrieval models are trained, optimized, and evaluated on a single dataset. In this paper, we quantify the effect of adding training data using three datasets and the effect on performance by evaluating the same model on two datasets. For our study, first, we introduce a new collection of about 5000 audio-text pairs called WavText5K. We qualitatively show how WavText5K differs from audio-text datasets and quantitatively show its effectiveness for retrieval. Our results show that adding more audio-text pairs does not necessarily improve performance. Second, we compare two effective audio encoders: CNN and audio transformers. We propose an architecture that demonstrates that utilizing both encoders improves the individual model's performance. Overall, using WavText5K and the proposed encoder combination outperforms the benchmark for AudioCaps and Clotho by 6% and 23%",
    "checked": true,
    "id": "c822486b8f1dcbef3b96b136c85d48d0dc580f31",
    "semantic_title": "audio retrieval with wavtext5k and clap training",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cappellazzo23b_interspeech.html": {
    "title": "Sequence-Level Knowledge Distillation for Class-Incremental End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "The ability to learn new concepts sequentially is a major weakness for modern neural networks, which hinders their use in non-stationary environments. Their propensity to fit the current data distribution to the detriment of the past acquired knowledge leads to the catastrophic forgetting issue. In this work we tackle the problem of Spoken Language Understanding applied to a continual learning setting. We first define a class-incremental scenario for the SLURP dataset. Then, we propose three knowledge distillation (KD) approaches to mitigate forgetting for a sequence-to-sequence transformer model: the first KD method is applied to the encoder output (audio-KD), and the other two work on the decoder output, either directly on the token-level (tok-KD) or on the sequence-level (seq-KD) distributions. We show that the seq-KD substantially improves all the performance metrics, and its combination with the audio-KD further decreases the average WER and enhances the entity prediction metric",
    "checked": true,
    "id": "100926ca1253a5e7673cca246405db089a75b798",
    "semantic_title": "sequence-level knowledge distillation for class-incremental end-to-end spoken language understanding",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chien23b_interspeech.html": {
    "title": "Contrastive Disentangled Learning for Memory-Augmented Transformer",
    "volume": "main",
    "abstract": "This paper developed a new memory-augmented sequential learning based on a contrastive disentangled transformer. Conventionally, transformer is insufficient to characterize long sequences since the sequence length is restricted to avoid the requirement of overlarge memory. A direct solution to handle this issue is to divide long sequence into short segments, but the context fragmentation will happen. In this paper, the contrastive disentangled memory is exploited to deal with the increasing computation cost as well as the overlarge memory requirement due to long sequences. In particular, an informative selection over the disentangled memory slots is proposed for iterative updating in a large-span sequence representation. This paper maximizes the semantic diversity of memory slots and captures the contextual semantics via contrastive learning. The experiments on language understanding show that the context fragmentation is mitigated by the proposed method with reduced computation",
    "checked": true,
    "id": "abfa407b9c4702f4caf7ec49ea98c4b844ee6772",
    "semantic_title": "contrastive disentangled learning for memory-augmented transformer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deseyssel23_interspeech.html": {
    "title": "ProsAudit, a prosodic benchmark for self-supervised speech models",
    "volume": "main",
    "abstract": "We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, and an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when evaluated on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks",
    "checked": true,
    "id": "75ef5877acbd3fff7bce0af51dadfa42f6e4c9c0",
    "semantic_title": "prosaudit, a prosodic benchmark for self-supervised speech models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23j_interspeech.html": {
    "title": "Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces",
    "volume": "main",
    "abstract": "Self-supervised speech representations are known to encode both speaker and phonetic information, but how they are distributed in the high-dimensional space remains largely unexplored. We hypothesize that they are encoded in orthogonal subspaces, a property that lends itself to simple disentanglement. Applying principal component analysis to representations of two predictive coding models, we identify two subspaces that capture speaker and phonetic variances, and confirm that they are nearly orthogonal. Based on this property, we propose a new speaker normalization method which collapses the subspace that encodes speaker information, without requiring transcriptions. Probing experiments show that our method effectively eliminates speaker information and outperforms a previous baseline in phone discrimination tasks. Moreover, the approach generalizes and can be used to remove information of unseen speakers",
    "checked": true,
    "id": "5376990833dd410f30faf48a673da17263b57067",
    "semantic_title": "self-supervised predictive coding models encode speaker and phonetic information in orthogonal subspaces",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hallap23_interspeech.html": {
    "title": "Evaluating context-invariance in unsupervised speech representations",
    "volume": "main",
    "abstract": "Unsupervised speech representations have taken off with benchmarks demonstrating major progress on semi-supervised speech recognition, speech synthesis, and speech-only language modelling. Inspiration comes from the promise of discovering the phonemes of a language or a similar low-bitrate encoding. However, one of the critical properties of phoneme transcriptions is context-invariance: the phonetic context of a speech sound can have massive influence on the way it is pronounced while text remains stable. This is why tokens of the same word have the same transcriptions---key to language understanding. Current benchmarks do not measure context-stability. We develop a new version of the ZeroSpeech ABX benchmark that does, and apply it to recent self-supervised representations. We show that context-independence of representations is predictive of the stability of word-level representations. We suggest research concentrate on improving context-independence of unsupervised representations",
    "checked": true,
    "id": "629d7e7b9fc7b6d6918d384038b1d4dd8c74aec4",
    "semantic_title": "evaluating context-invariance in unsupervised speech representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23_interspeech.html": {
    "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
    "volume": "main",
    "abstract": "Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT",
    "checked": true,
    "id": "9efd5e9d81ca498cc14a13b8c9681501d28d3c13",
    "semantic_title": "cobert: self-supervised speech representation learning through code representation learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chang23_interspeech.html": {
    "title": "Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering",
    "volume": "main",
    "abstract": "Self-supervised speech representation models have succeeded in various tasks, but improving them for content-related problems using unlabeled data is challenging. We propose speaker-invariant clustering (Spin), a novel self-supervised learning method that clusters speech representations and performs swapped prediction between the original and speaker-perturbed utterances. Spin disentangles speaker information and preserves content representations with just 45 minutes of fine-tuning on a single GPU. Spin improves pre-trained networks and outperforms prior methods in speech recognition and acoustic unit discovery",
    "checked": true,
    "id": "f78b17020c8949e9c91e8e3239dcde2cb1bf91d3",
    "semantic_title": "self-supervised fine-tuning for improved content representations by speaker-invariant clustering",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23d_interspeech.html": {
    "title": "Self-Supervised Acoustic Word Embedding Learning via Correspondence Transformer Encoder",
    "volume": "main",
    "abstract": "Acoustic word embeddings (AWEs) aims to map a variable-length speech segment into a fixed-dimensional representation. High-quality AWEs should be invariant to variations, such as duration, pitch and speaker. In this paper, we introduce a novel self-supervised method to learn robust AWEs from a large-scale unlabelled speech corpus. Our model, named Correspondence Transformer Encoder (CTE), employs a teacher-student learning framework. We train the model based on the idea that different realisations of the same word should be close in the underlying embedding space. Specifically, we feed the teacher and student encoder with different acoustic instances of the same word and pre-train the model with a word-level loss. Our experiments show that the embeddings extracted from the proposed CTE model are robust to speech variations, e.g. speakers and domains. Additionally, when evaluated on Xitsonga, a low-resource cross-lingual setting, the CTE model achieves new state-of-the-art performance",
    "checked": true,
    "id": "7715d31722988c5f9fca16997d2ab38ae100a3c7",
    "semantic_title": "self-supervised acoustic word embedding learning via correspondence transformer encoder",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/paterson23_interspeech.html": {
    "title": "A Pipeline to Evaluate the Effects of Noise on Machine Learning Detection of Laryngeal Cancer",
    "volume": "main",
    "abstract": "Cases of laryngeal cancer are rising, with diagnosis often involving invasive biopsy procedures. An alternate approach is to identify high-risk patients by analysis of voice recordings which can alert clinical teams to those patients that need prioritisation. We propose a pipeline for evaluating speech classifier performance in the presence of noise. We perform experiments using the pipeline with several classifiers and denoising techniques. Random forest classifier performed best with an accuracy of 81.2% on clean data dropping to 63.8% when noise was added to recordings. The accuracy of all classifiers was reduced by added noise, signal denoising improved classifier accuracy but could not fully reverse the effects of noise. The effects of noise on classification is a complex issue which must be resolved for these detection systems to be implemented in clinical practice. We show that the proposed pipeline allows for the evaluation of classifier performance in the presence of noise",
    "checked": true,
    "id": "56a59dfab72fdb5205442a070aaa3e217f4d99c4",
    "semantic_title": "a pipeline to evaluate the effects of noise on machine learning detection of laryngeal cancer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ga_interspeech.html": {
    "title": "ReCLR: Reference-Enhanced Contrastive Learning of Audio Representation for Depression Detection",
    "volume": "main",
    "abstract": "Contrastive self-supervised learning has seen great success in computer vision while been less investigated in the audio processing field, in particular depression detection, a socially critical challenge. Detecting depression from one's speech has been examined via various audio representations, including acoustic feature combinations and model-based ones. This paper proposes to obtain depressive audio representations by departing speech via reference features from an emotion recognition model. Furthermore, we propose a reference-enhanced contrastive learning (ReCLR) to select fine-grained positive instances and allocate weight to negative instances. The depression detection results indicate that contrastive learning is effective in such an audio task. Moreover, our modified ReCLR strategy has outperformed contrastive training without references",
    "checked": true,
    "id": "9e0688d5553edadb9b47ce8aec11a5e2ab70cd98",
    "semantic_title": "reclr: reference-enhanced contrastive learning of audio representation for depression detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/egaslopez23_interspeech.html": {
    "title": "Automated Multiple Sclerosis Screening Based on Encoded Speech Representations",
    "volume": "main",
    "abstract": "Multiple Sclerosis (MS) is a chronic disease affecting over 2.5 million people worldwide. Its early detection is crucial for the management and treatment of the disease. Here we present an approach for automatic MS screening based on encoded speech representations. Our methods rely on Wav2Vec2 models to extract relevant traits from speech recordings of patients, which are then fed into a Support Vector Machine. Besides employing Wav2Vec2 models pre-trained on large public corpora, we also fine-tune them on 85 hours of the target language (Hungarian) in two distinct ways: for ASR and for speaker identification. Both variations outperformed the original models and conventional methods (ComParE functionals, x-vectors, and ECAPA-TDNN). Our findings suggest that fine-tuning for the actual speaker provides more advantages than the typical approach of fine-tuning for ASR purposes. Still, we improved our best MS discrimination performance when we fused features from our two fine-tuned models",
    "checked": true,
    "id": "0ecc9271879f70bb0440c2d88e775fd16eb0fc1d",
    "semantic_title": "automated multiple sclerosis screening based on encoded speech representations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/melistas23_interspeech.html": {
    "title": "Cross-Lingual Features for Alzheimer's Dementia Detection from Speech",
    "volume": "main",
    "abstract": "Alzheimer's dementia is a neurodegenerative disease that affects millions of people worldwide. Early detection of Alzheimer's dementia is crucial for effective treatment and management of the disease. In this paper, we present a cross-lingual approach for detecting Alzheimer's dementia from speech, based on multiple feature streams that capture the individual's speech and conversational interactions. In order to validate the ability of the features to perform well in cross-linguistic scenarios, we evaluate in a zero-shot setup, where the target domain is a language that was not available during training and a few-shot setup, where only limited data is available. Experimental results show that an ensemble system using the features trained on English and evaluated on Greek outperforms the baseline system by 4.4 %. Further experiments show promising zero-shot and few-shot performance on a similar Spanish task",
    "checked": true,
    "id": "96f8abfad2dc63136b4408ddb761525197ace431",
    "semantic_title": "cross-lingual features for alzheimer's dementia detection from speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zusag23_interspeech.html": {
    "title": "Careful Whisper - leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification",
    "volume": "main",
    "abstract": "This paper presents a fully automated approach for identifying speech anomalies from voice recordings to aid in the as- sessment of speech impairments. By combining Connectionist Temporal Classification (CTC) and encoder-decoder-based automatic speech recognition models, we generate rich acoustic and clean transcripts. We then apply several natural language processing methods to extract features from these transcripts to produce prototypes of healthy speech. Basic distance measures from these prototypes serve as input features for standard machine learning classifiers, yielding human-level accuracy for the distinction between recordings of people with aphasia and a healthy control group. Furthermore, the most frequently occurring aphasia types can be distinguished with 90% accuracy. The pipeline is directly applicable to other diseases and languages, showing promise for robustly extracting diagnostic speech biomarkers",
    "checked": true,
    "id": "7fbe44f2d9c48794be12bab8832d5a789c34287b",
    "semantic_title": "careful whisper - leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thienpondt23_interspeech.html": {
    "title": "Behavioral Analysis of Pathological Speaker Embeddings of Patients During Oncological Treatment of Oral Cancer",
    "volume": "main",
    "abstract": "In this paper, we analyze the behavior of speaker embeddings of patients during oral cancer treatment. First, we found that pre- and post-treatment speaker embeddings differ significantly, notifying a substantial change in voice characteristics. However, a partial recovery to pre-operative voice traits is observed after 12 months post-operation. Secondly, the same-speaker similarity at distinct treatment stages is similar to healthy speakers, indicating that the embeddings can capture characterizing features of even severely impaired speech. Finally, a speaker verification analysis signifies a stable false positive rate and variable false negative rate when combining speech samples of different treatment stages. This indicates robustness of the embeddings towards other speakers, while still capturing the changing voice characteristics during treatment. To the best of our knowledge, this is the first analysis of speaker embeddings during oral cancer treatment of patients",
    "checked": true,
    "id": "9ab59ced6320180a236a472a61f69fdcfe5e592e",
    "semantic_title": "behavioral analysis of pathological speaker embeddings of patients during oncological treatment of oral cancer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23b_interspeech.html": {
    "title": "Adversarial Learning of Intermediate Acoustic Feature for End-to-End Lightweight Text-to-Speech",
    "volume": "main",
    "abstract": "To simplify the generation process, several text-to-speech (TTS) systems implicitly learn intermediate latent representations instead of relying on predefined features (e.g., mel-spectrogram). However, their generation quality is unsatisfactory as these representations lack speech variances. In this paper, we improve TTS performance by adding prosody embeddings to the latent representations. During training, we extract reference prosody embeddings from mel-spectrograms, and during inference, we estimate these embeddings from text using generative adversarial networks (GANs). Using GANs, we reliably estimate the prosody embeddings in a fast way, which have complex distributions due to the dynamic nature of speech. We also show that the prosody embeddings work as efficient features for learning a robust alignment between text and acoustic features. Our proposed model surpasses several publicly available models with less parameters and computational complexity in comparative experiments",
    "checked": false,
    "id": "918d57e245d434604441ddbcfb7942d0aa1b1f87",
    "semantic_title": "ailtts: adversarial learning of intermediate acoustic feature for end-to-end lightweight text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hsieh23_interspeech.html": {
    "title": "Adapter-Based Extension of Multi-Speaker Text-To-Speech Model for New Speakers",
    "volume": "main",
    "abstract": "Fine-tuning is a popular method for adapting text-to-speech (TTS) models to new speakers. However, this approach has some challenges. Usually, fine-tuning requires several hours of high quality speech per speaker. Fine-tuning might negatively affect the quality of speech synthesis for previously learned speakers. In this paper, we propose an alternative approach for TTS adaptation based on using parameter-efficient adapter modules. In the proposed approach, a few adapter modules are added between the layers of the pretrained network. The pretrained model is frozen, and only the adapters are fine-tuned to the speech of a new speaker. Our approach will produce a new model with a high level of parameter sharing with the original model. Our experiments on LibriTTS, HiFi-TTS and VCTK datasets validate our adapter-based method through objective and subjective metrics. The code is open-sourced and the audio samples are available on our demo page",
    "checked": true,
    "id": "7d42d1a27129c51e618e5127132c32f35260b4c4",
    "semantic_title": "adapter-based extension of multi-speaker text-to-speech model for new speakers",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sivaguru23_interspeech.html": {
    "title": "SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "While FastSpeech2 aims to integrate aspects of speech such as pitch, energy, and duration as conditional inputs, it still leaves scope for richer representations. As a part of this work, we leverage representations from various Self-Supervised Learning (SSL) models to enhance the quality of the synthesized speech. In particular, we pass the FastSpeech2 encoder's length-regulated outputs through a series of encoder layers with the objective of reconstructing the SSL representations. In the SALTTS-parallel implementation, the representations from this second encoder are used for an auxiliary reconstruction loss with the SSL features. The SALTTS-cascade implementation, however, passes these representations through the decoder in addition to having the reconstruction loss. The richness of speech characteristics from the SSL features reflects in the output speech quality, with the objective and subjective evaluation measures of the proposed approach outperforming the baseline FastSpeech2",
    "checked": true,
    "id": "bc20f9517fe8e8895f1cd9b0624290c76d4e91d4",
    "semantic_title": "saltts: leveraging self-supervised speech representations for improved text-to-speech synthesis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23k_interspeech.html": {
    "title": "UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data",
    "volume": "main",
    "abstract": "We propose UnitSpeech, a speaker-adaptive speech synthesis method that fine-tunes a diffusion-based text-to-speech (TTS) model using minimal untranscribed data. To achieve this, we use the self-supervised unit representation as a pseudo transcript and integrate the unit encoder into the pre-trained TTS model. We train the unit encoder to provide speech content to the diffusion-based decoder and then fine-tune the decoder for speaker adaptation to the reference speaker using a single",
    "checked": true,
    "id": "0faf1ee64fe60140e360fea4e85a6c0e715e94e1",
    "semantic_title": "unitspeech: speaker-adaptive speech synthesis with untranscribed data",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dang23b_interspeech.html": {
    "title": "LightVoc: An Upsampling-Free GAN Vocoder Based On Conformer And Inverse Short-time Fourier Transform",
    "volume": "main",
    "abstract": "Most neural vocoders based on generative adversarial networks (GANs) rely on iterative upsampling to generate audio sequences from mel-spectrograms as well as dilated convolution to expand their receptive fields. Nevertheless, iterative upsampling increases the network's complexity and thus decreases the inference speed. Moreover, convolution neural networks are geared towards extracting fine-grained local information and still struggle to capture long-term dependencies. In this work, we propose LightVoc, an efficient and high-quality GAN-based neural vocoder that replaces all upsampling blocks with a stack of Conformer blocks and uses a novel combination of discriminators to generate high-resolution waveforms over the full-band. From our experiments on LJSpeech dataset, LightVoc produces comparable audio quality while being 52.5 times faster in terms of CPU-based inference speed in comparison to HiFi-GAN V1",
    "checked": true,
    "id": "5856df08c211d595d0b0e3b8d850aa4bf0fc3b10",
    "semantic_title": "lightvoc: an upsampling-free gan vocoder based on conformer and inverse short-time fourier transform",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saito23_interspeech.html": {
    "title": "ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings",
    "volume": "main",
    "abstract": "We propose ChatGPT-EDSS, an empathetic dialogue speech synthesis (EDSS) method using ChatGPT for extracting dialogue context. ChatGPT is a chatbot that can deeply understand the content and purpose of an input prompt and appropriately respond to the user's request. We focus on ChatGPT's reading comprehension and introduce it to EDSS, a task of synthesizing speech that can empathize with the interlocutor's emotion. Our method first gives chat history to ChatGPT and asks it to generate three words representing the intention, emotion, and speaking style for each line in the chat. Then, it trains an EDSS model using the embeddings of ChatGPT-derived context words as the conditioning features. The experimental results demonstrate that our method performs comparably to ones using emotion labels or neural network-derived context embeddings learned from chat histories. The collected ChatGPT-derived context information is available at our project page",
    "checked": true,
    "id": "00554edbbe20423cbf7a2f7e3a130c1cb4f56203",
    "semantic_title": "chatgpt-edss: empathetic dialogue speech synthesis trained from chatgpt-derived context word embeddings",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23f_interspeech.html": {
    "title": "Human Transcription Quality Improvement",
    "volume": "main",
    "abstract": "High quality transcription data is crucial for training automatic speech recognition (ASR) systems. However, the existing industry-level data collection pipelines are expensive to researchers, while the quality of crowdsourced transcription is low. In this paper, we propose a reliable method to collect speech transcriptions. We introduce two mechanisms to improve transcription quality: confidence estimation based reprocessing at labeling stage, and automatic word error correction at post-labeling stage. We collect and release LibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100 hours of English speech. Experiment shows the Transcription WER is reduced by over 50%. We further investigate the impact of transcription error on ASR model performance and found a strong correlation. The transcription quality improvement provides over 10% relative WER reduction for ASR models. We release the dataset and code to benefit the research community",
    "checked": true,
    "id": "f3ffd5d42df1f8f75a796290d3ca592caf420f5e",
    "semantic_title": "human transcription quality improvement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/simantiraki23_interspeech.html": {
    "title": "The effect of masking noise on listeners' spectral tilt preferences",
    "volume": "main",
    "abstract": "Speech enhancement algorithms often focus on optimising intelligibility while neglecting other aspects of speech such as naturalness, quality and listening effort which may affect a listener's experience. This paper investigates the impact of spectral tilt on listeners' preferences, using a new corpus of Greek utterances. Participants adjusted spectral tilt with real-time feedback to select their preferred tilt in quiet and in the presence of speech-shaped noise at eight signal-to-noise ratios. Listeners displayed distinct preferences, with a tendency to select flatter tilts with increasing noise. Preferences were not random even for constant intelligibility, indicating that their adjustments were influenced by factors beyond the need to maintain comprehensibility. These findings have the potential to inform the design of speech enhancement algorithms that jointly optimise intelligibility and a listener's overall experience",
    "checked": true,
    "id": "8c776b3976d01f9eab2b3719cb88f22300482efe",
    "semantic_title": "the effect of masking noise on listeners' spectral tilt preferences",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tranngoc23_interspeech.html": {
    "title": "The Effect of Whistled Vowels on Whistled Word Categorization for Naive Listeners",
    "volume": "main",
    "abstract": "In this paper, we explore whistled word perception by naive French speakers. In whistled words of non-tonal languages, vowels are transposed to relatively stable pitches, which contrast with consonant movements or interruptions. Previous studies on whistled speech with naive listeners have tested vowels and consonants separately. Other studies on spoken word recognition have found that vowels and consonants contribute differently to intelligibility, where the role of vowels was highly mediated by the context. Here, naive participants recognize disyllabic whistled words above chance, and vowels are shown to contribute differently than consonants. When focusing on the role of vowels, we found different scales of performance between the vowels tested, mediated by their position in the word. We also highlighted the importance of the vowels' relative frequency difference (called 'interval') in the word",
    "checked": true,
    "id": "ad34c19adbe8afef6efdabb63de21327f935f520",
    "semantic_title": "the effect of whistled vowels on whistled word categorization for naive listeners",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bharati23_interspeech.html": {
    "title": "Automatic Deep Neural Network-Based Segmental Pronunciation Error Detection of L2 English Speech (L1 Bengali)",
    "volume": "main",
    "abstract": "In the last few decades, English has become a popular language as it helps us to communicate with the global world. A large population of English learners find it challenging to achieve an 'acceptable' and 'intelligible' pronunciation. To overcome these issues, various computer-assisted pronunciation training tools are designed where automatic pronunciation error detection (APED) is a core component of the system. Most of the works of APED are based on European English speech, but there is no such work reported for Bengali English speech. This paper proposes a system for pronunciation error detection of L2 English speech (L1 Bengali) at phoneme/segmental level using a hybrid convolutional neural network and long short-term memory modules with CTC loss. Experiments are done based on newly created L2 English speaker (L1 Bengali) speech data. The results demonstrate that the proposed system outperforms the goodness of pronunciation-based methods by 15% in terms of F1 score using fbank",
    "checked": true,
    "id": "99991481355364f3dcf3a68ef21b6b9bc7b7f2bc",
    "semantic_title": "automatic deep neural network-based segmental pronunciation error detection of l2 english speech (l1 bengali)",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hao23_interspeech.html": {
    "title": "The effect of stress on Mandarin tonal perception in continuous speech for Spanish-speaking learners",
    "volume": "main",
    "abstract": "The perception of lexical tones is a well-known challenge for L2 learners especially in continuous speech where the tonal variations are complicated. To investigate whether the stress, by affecting the acoustic manifestations of tones, has an effect on Mandarin tonal perception for L2 learners, we carried out a perceptual experiment based on the Annotated Speech Corpus of Chinese Discourse with 25 native Spanish-speaking participants. The results indicate that: the perceptual accuracy of stressed tones is significantly higher than that of the unstressed ones in general; T3 is the most difficult one to be perceived among the four Mandarin tones (T1-T4) in both stressed and unstressed syllables, and presumably the Spanish-speaking learners' perceptual order of Mandarin tones is T4-T1-T2-T3; the significant interactive effect found between tone and tonal context in continuous speech may lead to great confusion of tonal perception, especially when T2 and T3 are adjacent with one another",
    "checked": true,
    "id": "188daccc97de0e83437a912b69ef6b1dd7e91e67",
    "semantic_title": "the effect of stress on mandarin tonal perception in continuous speech for spanish-speaking learners",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elmerich23_interspeech.html": {
    "title": "Combining acoustic and aerodynamic data collection: A perceptual evaluation of acoustic distortions",
    "volume": "main",
    "abstract": "Combining acoustic and aerodynamic data acquisitions is challenging. Devices for aerodynamic measurements often create severe acoustic distortions, which make it impossible to analyse the simultaneously recorded acoustic data. An improved technique, a pneumotachograph mask made of synthetic fibers, is acoustically transparent while ensuring a high-quality aerodynamic data acquisition. A previous acoustic study confirms the minimal acoustic distortions caused by this technique. The present study evaluates the impact of different aerodynamic devices on the human perception of vowels. Results show that vowels recorded with a fiber mask are almost as accurately categorised as acoustic-only recordings, compared with rigid masks that result in perceptual confusions. Listeners are also less likely to perceive the presence of a mask. Overall, our study provides the perceptual validation of the fiber mask technique, which will be of a great value in the field of speech sciences",
    "checked": true,
    "id": "d9be8791612ae64004afa111a091f9f453fdf6a5",
    "semantic_title": "combining acoustic and aerodynamic data collection: a perceptual evaluation of acoustic distortions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elie23b_interspeech.html": {
    "title": "Estimating virtual targets for lingual stop consonants using general Tau theory",
    "volume": "main",
    "abstract": "This paper investigates the existence and position of virtual targets during the production of stop consonants. Using the equations from general Tau theory to model the time-course of tongue constriction formation movements, targets were estimated by fitting these equations on observed tongue constriction variables extracted from real EMA data from 2 native speakers of English. Results suggest that targets are virtual for 50 to 60% of movements. For these movements, virtual targets of the tongue tip constriction are predicted to occur around 0.1 cm beyond the palate, and virtual targets for the tongue dorsum constriction are predicted to occur between 0.05 and 0.2 cm beyond the palate. Our results suggest that the time-course of movement is planned so that the onset of closure occurs with relatively high velocity: closure onset is generally located very close in time to the time of peak velocity",
    "checked": true,
    "id": "59aae1527082e117b19d8e3d5da2562ec9300bfa",
    "semantic_title": "estimating virtual targets for lingual stop consonants using general tau theory",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gibson23_interspeech.html": {
    "title": "Using Random Forests to classify language as a function of syllable timing in two groups: children with cochlear implants and with normal hearing",
    "volume": "main",
    "abstract": "We trained a series of Random Forest models in a supervised learning environment on different temporal parameters related to syllable structure: voice onset time (VOT), vowel duration following simplex and complex onsets, and lateral duration in word initial (/lV) position and as the second consonant in a C1C2 cluster (where C means consonant). Capitalizing on previous work we trained the models on data from monolingual Spanish- and English-speaking adults. We asked whether the timing productions used by bilingual children with normal hearing (NH) and children with cochlear implants (CI) can be classified as pertaining to the same timing system (i.e. language), or whether the children are applying the same basic timing plan to two different languages. We also asked whether there were differences between the CI and NH groups. Our results indicate that the children from both groups produce qualitatively distinct timing plans for each language with no interference from the other language",
    "checked": true,
    "id": "e6d2b45c3a580e9e2ee38feeebfade94c1938286",
    "semantic_title": "using random forests to classify language as a function of syllable timing in two groups: children with cochlear implants and with normal hearing",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23w_interspeech.html": {
    "title": "An Improved End-to-End Audio-Visual Speech Recognition Model",
    "volume": "main",
    "abstract": "By incorporating lip language, audio-visual speech recognition can effectively improve the recognition effect in noisy environments, and will slightly improve the recognition effect in quiet environments. we use a frequency domain attention based residual network (Fca-Net) as the model of the vision front-end module, which extracts more features that are helpful to the AVSR and VSR system at a small cost. And use the powerful speech pre-training model Hu-BERT as the recognition front-end model of ASR. We compare the impact of different model as visual back-end modules and fusion modules on the AVSR system. Our experiments show that the model selection of the fusion module is critical to the performance of the AVSR system. Ultimately, our proposed model achieves state-of-the-art results on audio-visual speech recognition tasks using the LRS2 dataset",
    "checked": true,
    "id": "eda5321a9897bceebaf1b2a726b5bc22fdcbf134",
    "semantic_title": "an improved end-to-end audio-visual speech recognition model",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wesoek23_interspeech.html": {
    "title": "What influences the foreign accent strength? Phonological and grammatical errors in the perception of accentedness",
    "volume": "main",
    "abstract": "The present study investigates the influence of grammatical and phonological errors on the perceived degree of foreign accent strength. German and Polish participants listened to speech in their native language produced with foreign and native accent. They rated the accent strength of each sentence on a 7-point scale. Grammatical errors consisted of gender agreement violations and phonological errors consisted of controlled vowel substitutions. Both error types significantly affected the perception of accent strength in the foreign and native-accented condition. In Polish, phonological anomalies had significantly more impact than grammatical violations in native-accented sentences. In German, there was no significant difference between phonological and grammatical violations. The study provides evidence that the presence of phonological and grammatical errors increases the perceived accentedness of speech. The weighting of both errors for accent perception can vary between languages",
    "checked": true,
    "id": "705b8611aef3beab246fb6e9a97d1c639b471c33",
    "semantic_title": "what influences the foreign accent strength? phonological and grammatical errors in the perception of accentedness",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huttner23_interspeech.html": {
    "title": "Investigating the Perception Production Link through Perceptual Adaptation and Phonetic Convergence",
    "volume": "main",
    "abstract": "Speech perception and production may be linked. In this pilot study, we aim to investigate the nature of this link by examining how perception and production of VOT adjust in social interaction. In an online experiment with 2x2 conditions participants were asked to categorize nine stimuli on a VOT continuum between /tin/ and /din/ and produce the words din and tin before and after playing a game with a bot. During the game participants were trained on a sound to word correspondence with a bias either towards /din/ or /tin/. In two conditions participants alternated categorizing and producing the stimuli, while in the other two participants only categorized the stimuli. Significant differences in categorization between conditions occurred only during the game. Whereas significant changes in VOT can be observed between pre and post-test for three conditions. This could mean that perception and production do not adjust symmetrically over the course of an interaction",
    "checked": true,
    "id": "198ee250e02d88d20b7dbd5c6c980626a301a16a",
    "semantic_title": "investigating the perception production link through perceptual adaptation and phonetic convergence",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23f_interspeech.html": {
    "title": "Emotion Prompting for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) classifies speech into emotion categories such as: Happy, Angry. Most prior works for SER focused on how to mine compelling features to improve performance. However, these methods ignore the influence of emotional label information on SER. Recent studies have attempted to prompt pre-trained language models and yield good performance for NLP tasks. Nevertheless, few works have attempted to prompt pre-trained speech models (PSM) on speech tasks. In light of these, we propose a simple but effective prompt-based method that prompts PSM for SER. Firstly, we reframe SER as an entailment task. Next, we generate speech prompts and combine them with the raw audio to form the input for PSM. Finally, we build a multi-task learning framework to extract more compelling features by simultaneously performing automatic speech recognition (ASR) and SER. Experiments on the IEMOCAP benchmark show that our method outperforms state-of-the-art baselines on the SER task",
    "checked": true,
    "id": "0d070b78fed9c485959c20012f089b9f390b8504",
    "semantic_title": "emotion prompting for speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chin23_interspeech.html": {
    "title": "Speech-in-Speech Recognition is Modulated by Familiarity to Dialect",
    "volume": "main",
    "abstract": "Listening to speech in competing background speech can be difficult due to elements such as the linguistic content of the signal. Linguistic release from masking occurs when altering the masker language results in less interference in speech recognition. The greater the linguistic differences between the target and masker, the higher the speech recognition accuracy. However, for dialectal variations of the same language, these patterns are less consistent. This study examined speech-in-speech recognition in Australian English monolinguals when the target speech was in either Australian (AU) or American English, and when the masker speech was in either of the dialects or a foreign language (Swedish). Speech recognition performance was greatest when AU was the target and poorest when AU was the masker. There were fewer differences in performance between the Swedish and dialect maskers. Results indicate that speech recognition is modulated by a listener's familiarity to a dialect",
    "checked": true,
    "id": "87c754368d3e9bbcbbcf547e1b14112e6028fe86",
    "semantic_title": "speech-in-speech recognition is modulated by familiarity to dialect",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23m_interspeech.html": {
    "title": "BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions",
    "volume": "main",
    "abstract": "Time-domain single-channel speech enhancement (SE) still remains challenging to extract the target speaker without any prior information on multi-talker conditions. It has been shown via auditory attention decoding that the brain activity of the listener contains the auditory information of the attended speaker. In this paper, we thus propose a novel time-domain brain-assisted SE network (BASEN) incorporating electroencephalography (EEG) signals recorded from the listener for extracting the target speaker from monaural speech mixtures. The proposed BASEN is based on the fully-convolutional time-domain audio separation network. In order to fully leverage the complementary information contained in the EEG signals, we further propose a convolutional multi-layer cross attention module to fuse the dual-branch features. Experimental results on a public dataset show that the proposed model outperforms the state-of-the-art method in several evaluation metrics. The reproducible code is available at https://github.com/jzhangU/Basen",
    "checked": true,
    "id": "a4a8bcf45750cfe4b4e1cfa6e739727c205b194d",
    "semantic_title": "basen: time-domain brain-assisted speech enhancement network with convolutional cross attention in multi-talker conditions",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2023/miodonska23_interspeech.html": {
    "title": "Are retroflex-to-dental sibilant substitutions in Polish children's speech an example of a covert contrast? A preliminary acoustic study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23i_interspeech.html": {
    "title": "Reversible Neural Networks for Memory-Efficient Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23f_interspeech.html": {
    "title": "ECAPA++: Fine-grained Deep Embedding Learning for TDNN Based Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23w_interspeech.html": {
    "title": "TO-Rawnet: Improving RawNet with TCN and Orthogonal Regularization for Fake Audio Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zuo23_interspeech.html": {
    "title": "Fooling Speaker Identification Systems with Adversarial Background Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23r_interspeech.html": {
    "title": "Mutual Information-based Embedding Decoupling for Generalizable Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23c_interspeech.html": {
    "title": "Target Active Speaker Detection with Audio-visual Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/broughton23_interspeech.html": {
    "title": "Improving End-to-End Neural Diarization Using Conversational Summary Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zang23_interspeech.html": {
    "title": "Phase perturbation improves channel robustness for speech spoofing countermeasures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bousquet23_interspeech.html": {
    "title": "Improving training datasets for resource-constrained speaker recognition neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lertpetchpun23_interspeech.html": {
    "title": "Instance-based Temporal Normalization for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/novoselov23_interspeech.html": {
    "title": "On the robustness of wav2vec 2.0 based speaker recognition systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23i_interspeech.html": {
    "title": "P-vectors: A Parallel-coupled TDNN/Transformer Network for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lei23_interspeech.html": {
    "title": "Group GMM-ResNet for Detection of Synthetic Speech Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fang23_interspeech.html": {
    "title": "Robust Training for Speaker Verification against Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jeoung23_interspeech.html": {
    "title": "Self-Distillation into Self-Attention Heads for Improving Transformer-based End-to-End Neural Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23m_interspeech.html": {
    "title": "Build a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benamor23_interspeech.html": {
    "title": "Describing the phonetics in the underlying speech attributes for deep and interpretable speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23v_interspeech.html": {
    "title": "Range-Based Equal Error Rate for Spoof Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tabassum23_interspeech.html": {
    "title": "Exploring the English Accent-independent Features for Speech Emotion Recognition using Filter and Wrapper-based Methods for Feature Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/plaquet23_interspeech.html": {
    "title": "Powerset multi-class cross entropy loss for neural speaker diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23_interspeech.html": {
    "title": "A Method of Audio-Visual Person Verification by Mining Connections between Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fish23_interspeech.html": {
    "title": "A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23_interspeech.html": {
    "title": "Modeling Dependent Structure for Utterances in ASR Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/verma23_interspeech.html": {
    "title": "ASR for Low Resource and Multilingual Noisy Code-Mixed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23b_interspeech.html": {
    "title": "Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mateju23_interspeech.html": {
    "title": "Combining Multilingual Resources and Models to Develop State-of-the-Art E2E ASR for Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23o_interspeech.html": {
    "title": "Two Stage Contextual Word Filtering for Context Bias in Unified Streaming and Non-streaming Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pham23_interspeech.html": {
    "title": "Towards continually learning new languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23e_interspeech.html": {
    "title": "N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23g_interspeech.html": {
    "title": "SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gulzar23_interspeech.html": {
    "title": "miniStreamer: Enhancing Small Conformer with Chunked-Context Masking for Streaming ASR Applications on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23p_interspeech.html": {
    "title": "CoMFLP: Correlation Measure Based Fast Search on ASR Layer Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23h_interspeech.html": {
    "title": "Exploration on HuBERT with Multiple Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23s_interspeech.html": {
    "title": "Quantization-aware and Tensor-compressed Training of Transformers for Natural Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/naowarat23b_interspeech.html": {
    "title": "Word-level Confidence Estimation for CTC Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kulshreshtha23_interspeech.html": {
    "title": "Multilingual Contextual Adapters To Improve Custom Word Recognition In Low-resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zheng23_interspeech.html": {
    "title": "Unsupervised Active Learning: Optimizing Labeling Cost-Effectiveness for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23_interspeech.html": {
    "title": "4D ASR: Joint modeling of CTC, Attention, Transducer, and Mask-Predict decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yen23_interspeech.html": {
    "title": "Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fan23_interspeech.html": {
    "title": "Language-specific Boundary Learning for Improving Mandarin-English Code-switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23c_interspeech.html": {
    "title": "Mixture-of-Expert Conformer for Streaming Multilingual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23x_interspeech.html": {
    "title": "Lossless 4-bit Quantization of Architecture Compressed Conformer ASR Systems on the 300-hr Switchboard Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23c_interspeech.html": {
    "title": "Compressed MoE ASR Model Based on Knowledge Distillation and Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deng23b_interspeech.html": {
    "title": "Factorised Speaker-environment Adaptive Training of Conformer Speech Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23aa_interspeech.html": {
    "title": "Text Only Domain Adaptation with Phoneme Guided Data Splicing for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cahyawijaya23_interspeech.html": {
    "title": "Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23fa_interspeech.html": {
    "title": "Modular Domain Adaptation for Conformer-Based Streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhatia23_interspeech.html": {
    "title": "Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23f_interspeech.html": {
    "title": "SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mori23_interspeech.html": {
    "title": "A Generative Framework for Conversational Laughter: Its 'Language Model' and Laughter Sound Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ba_interspeech.html": {
    "title": "Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lameris23_interspeech.html": {
    "title": "Beyond Style: Synthesizing Speech with Pragmatic Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/abbas23_interspeech.html": {
    "title": "eCat: An End-to-End Model for Multi-Speaker TTS & Many-to-Many Fine-Grained Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deb23_interspeech.html": {
    "title": "BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kashiwagi23_interspeech.html": {
    "title": "Improving the Gap in Visual Speech Recognition Between Normal and Silent Speech Based on Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jakubiak23_interspeech.html": {
    "title": "Whistle-to-text: Automatic recognition of the Silbo Gomero whistled language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23c_interspeech.html": {
    "title": "A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nortje23_interspeech.html": {
    "title": "Visually grounded few-shot word acquisition with fewer shots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23_interspeech.html": {
    "title": "JAMFN: Joint Attention Multi-Scale Fusion Network for Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23z_interspeech.html": {
    "title": "Prompt Guided Copy Mechanism for Conversational Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/faustini23_interspeech.html": {
    "title": "Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/han23c_interspeech.html": {
    "title": "On Monotonic Aggregation for Open-domain QA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nguyen23b_interspeech.html": {
    "title": "Question-Context Alignment and Answer-Context Dependencies for Effective Answer Sentence Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23v_interspeech.html": {
    "title": "Multi-Scale Attention for Audio Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23f_interspeech.html": {
    "title": "Enhancing Visual Question Answering via Deconstructing Questions and Explicating Answers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zeng23c_interspeech.html": {
    "title": "SEF-Net: Speaker Embedding Free Target Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rose23_interspeech.html": {
    "title": "Cascaded encoders for fine-tuning ASR models on overlapped speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/erdogan23_interspeech.html": {
    "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/meng23b_interspeech.html": {
    "title": "Unified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ahmadikalkhorani23_interspeech.html": {
    "title": "Time-domain Transformer-based Audiovisual Speaker Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/delcroix23_interspeech.html": {
    "title": "Multi-Stream Extension of Variational Bayesian HMM Clustering (MS-VBx) for Combined End-to-End and Vector Clustering-based Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/niu23_interspeech.html": {
    "title": "Unsupervised Adaptation with Quality-Aware Masking to Improve Target-Speaker Voice Activity Detection for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liang23e_interspeech.html": {
    "title": "BA-SOT: Boundary-Aware Serialized Output Training for Multi-Talker ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23e_interspeech.html": {
    "title": "Improving Label Assignments Learning by Dynamic Sample Dropout Combined with Layer-wise Optimization in Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gaultier23_interspeech.html": {
    "title": "Joint compensation of multi-talker noise and reverberation for speech enhancement with cochlear implants using one or more microphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yousefi23_interspeech.html": {
    "title": "Speaker Diarization for ASR Output with T-vectors: A Sequence Classification Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raj23_interspeech.html": {
    "title": "GPU-accelerated Guided Source Separation for Meeting Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23c_interspeech.html": {
    "title": "Overlap Aware Continuous Speech Separation without Permutation Invariant Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23w_interspeech.html": {
    "title": "Weakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23j_interspeech.html": {
    "title": "Directional Speech Recognition for Speaker Disambiguation and Cross-talk Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/berger23_interspeech.html": {
    "title": "Mixture Encoder for Joint Speech Separation and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hejna23_interspeech.html": {
    "title": "Aberystwyth English Pre-aspiration in Apparent Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23c_interspeech.html": {
    "title": "Speech Entrainment in Chinese Story-Style Talk Shows: The Interaction Between Gender and Role",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/steiner23_interspeech.html": {
    "title": "Sociodemographic and Attitudinal Effects on Dialect Speakers' Articulation of the Standard Language: Evidence from German-Speaking Switzerland",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burridge23_interspeech.html": {
    "title": "Vowel Normalisation in Latent Space for Sociolinguistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23n_interspeech.html": {
    "title": "Attention-based Encoder-Decoder Network for End-to-End Neural Speaker Diarization with Target Speaker Attractor",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lahiri23_interspeech.html": {
    "title": "Robust Self Supervised Speech Embeddings for Child-Adult Classification in Interactions involving Children with Autism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baghel23_interspeech.html": {
    "title": "The DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/paturi23_interspeech.html": {
    "title": "Lexical Speaker Error Correction: Leveraging Language Models for Speaker Diarization Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pirlogeanu23_interspeech.html": {
    "title": "The SpeeD--ZevoTech submission at DISPLACE 2023",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23g_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with Absolute Speaker Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23d_interspeech.html": {
    "title": "A Context-Constrained Sentence Modeling for Deception Detection in Real Interrogation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23c_interspeech.html": {
    "title": "MetricAug: A Distortion Metric-Lead Augmentation Strategy for Training Noise-Robust Speech Emotion Recognizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ludusan23_interspeech.html": {
    "title": "The co-use of laughter and head gestures across speech styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23d_interspeech.html": {
    "title": "EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23b_interspeech.html": {
    "title": "Pre-Finetuning for Few-Shot Emotional Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23_interspeech.html": {
    "title": "Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lavania23_interspeech.html": {
    "title": "Utility-Preserving Privacy-Enabled Speech Embeddings for Emotion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/burdisso23_interspeech.html": {
    "title": "Node-weighted Graph Convolutional Network for Depression Detection in Transcribed Clinical Interviews",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/branco23_interspeech.html": {
    "title": "Laughter in task-based settings: whom we talk to affects how, when, and how often we laugh",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fang23b_interspeech.html": {
    "title": "Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deoliveira23_interspeech.html": {
    "title": "Leveraging Semantic Information for Efficient Self-Supervised Emotion Recognition with Audio-Textual Distilled Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23d_interspeech.html": {
    "title": "Two-stage Finetuning of Wav2vec 2.0 for Speech Emotion Recognition with ASR and Gender Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thakran23_interspeech.html": {
    "title": "Investigating Acoustic Cues for Multilingual Abuse Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/singh23c_interspeech.html": {
    "title": "A novel frequency warping scale for speech emotion recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23m_interspeech.html": {
    "title": "Multi-Scale Temporal Transformer For Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grageda23_interspeech.html": {
    "title": "Distant Speech Emotion Recognition in an Indoor Human-robot Interaction Scenario",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tao23b_interspeech.html": {
    "title": "A Study on Prosodic Entrainment in Relation to Therapist Empathy in Counseling Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/minematsu23_interspeech.html": {
    "title": "A Unified Framework to Improve Learners' Skills of Perception and Production Based on Speech Shadowing and Overlapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nicholls23_interspeech.html": {
    "title": "Speak & Improve: L2 English Speaking Practice Tool",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nicolao23_interspeech.html": {
    "title": "Measuring prosody in child speech using SoapBox Fluency API",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nissen23_interspeech.html": {
    "title": "Teaching Non-native Sound Contrasts using Visual Biofeedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/walsh23_interspeech.html": {
    "title": "Large-Scale Automatic Audiobook Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elkheir23_interspeech.html": {
    "title": "QVoice: Arabic Speech Pronunciation Learning Application",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/svec23_interspeech.html": {
    "title": "Asking Questions: an Innovative Way to Interact with Oral History Archives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhat23_interspeech.html": {
    "title": "DisfluencyFixer: A tool to enhance Language Learning through Speech To Speech Disfluency Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/prakash23_interspeech.html": {
    "title": "Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elshahawy23_interspeech.html": {
    "title": "MyVoice: Arabic Speech Resource Collaboration Platform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hromada23_interspeech.html": {
    "title": "Personal Primer Prototype 1: Invitation to Make Your Own Embooked Speech-Based Educational Artifact",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deng23_interspeech.html": {
    "title": "Time-frequency Domain Filter-and-sum Network for Multi-channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23h_interspeech.html": {
    "title": "Audio-Visual Fusion using Multiscale Temporal Convolutional Attention for Time-Domain Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ca_interspeech.html": {
    "title": "An Efficient Speech Separation Network Based on Recurrent Fusion Dilated Convolution and Channel Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/phokhinanan23_interspeech.html": {
    "title": "Binaural Sound Localization in Noisy Environments Using Frequency-Based Audio Vision Transformer (FAViT)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23i_interspeech.html": {
    "title": "Contrastive Learning based Deep Latent Masking for Music Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23k_interspeech.html": {
    "title": "Speaker Extraction with Detection of Presence and Absence of Target Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23k_interspeech.html": {
    "title": "PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sarabia23_interspeech.html": {
    "title": "Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23q_interspeech.html": {
    "title": "Image-driven Audio-visual Universal Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fras23_interspeech.html": {
    "title": "Joint Blind Source Separation and Dereverberation for Automatic Speech Recognition using Delayed-Subsource MNMF with Localization Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23na_interspeech.html": {
    "title": "SDNet: Stream-attention and Dual-feature Learning Network for Ad-hoc Array Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baek23_interspeech.html": {
    "title": "Deeply Supervised Curriculum Learning for Deep Neural Network-based Sound Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fujimura23_interspeech.html": {
    "title": "Multi-channel separation of dynamic speech and sound events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ja_interspeech.html": {
    "title": "Rethinking the Visual Cues in Audio-Visual Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dang23_interspeech.html": {
    "title": "Using Semi-supervised Learning for Monaural Time-domain Speech Separation with a Self-supervised Learning-based SI-SNR Estimator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23d_interspeech.html": {
    "title": "Investigation of Training Mute-Expressive End-to-End Speech Separation Networks for an Unknown Number of Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cho23_interspeech.html": {
    "title": "SR-SRP: Super-Resolution based SRP-PHAT for Sound Source Localization and Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23g_interspeech.html": {
    "title": "Dual-Memory Multi-Modal Learning for Continual Spoken Keyword Spotting with Confidence Selection and Diversity Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23j_interspeech.html": {
    "title": "FN-SSL: Full-Band and Narrow-Band Fusion for Sound Source Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23g_interspeech.html": {
    "title": "A Neural State-Space Modeling Approach to Efficient Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fu23c_interspeech.html": {
    "title": "Locate and Beamform: Two-dimensional Locating All-neural Beamformer for Multi-channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23f_interspeech.html": {
    "title": "Monaural Speech Separation Method Based on Recurrent Attention with Parallel Branches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23m_interspeech.html": {
    "title": "Ontology-aware Learning and Evaluation for Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shim23c_interspeech.html": {
    "title": "Multi-Dataset Co-Training with Sharpness-Aware Optimization for Audio Anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lay23_interspeech.html": {
    "title": "Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/muller23_interspeech.html": {
    "title": "Complex-valued neural networks for voice anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ristea23_interspeech.html": {
    "title": "DeepVQE: Real Time Deep Voice Quality Enhancement for Joint Acoustic Echo Cancellation, Noise Suppression and Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sawata23_interspeech.html": {
    "title": "Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23h_interspeech.html": {
    "title": "HD-DEMUCS: General Speech Restoration with Heterogeneous Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23e_interspeech.html": {
    "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yin23_interspeech.html": {
    "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23x_interspeech.html": {
    "title": "Detection of Cross-Dataset Fake Audio Based on Prosodic and Pronunciation Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dowerah23_interspeech.html": {
    "title": "Self-supervised learning with Diffusion-based multichannel speech enhancement for speaker verification under noisy conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nespoli23_interspeech.html": {
    "title": "Two-Stage Voice Anonymization for Enhanced Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23h_interspeech.html": {
    "title": "Personalized Dereverberation of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/binhthien23_interspeech.html": {
    "title": "Weighted Von Mises Distribution-based Loss Function for Real-time STFT Phase Reconstruction Using DNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/schroter23_interspeech.html": {
    "title": "Deep Multi-Frame Filtering for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiong23_interspeech.html": {
    "title": "Aligning Speech Enhancement for Improving Downstream Classification Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23b_interspeech.html": {
    "title": "DNN-based Parameter Estimation for MVDR Beamforming and Post-filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23b_interspeech.html": {
    "title": "FRA-RIR: Fast Random Approximation of the Image-source Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23b_interspeech.html": {
    "title": "Rethinking Complex-Valued Deep Neural Networks for Monaural Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/le23_interspeech.html": {
    "title": "Harmonic enhancement using learnable comb filter for light-weight full-band speech enhancement model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23e_interspeech.html": {
    "title": "How Does Pretraining Improve Discourse-Aware Translation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23t_interspeech.html": {
    "title": "PATCorrect: Non-autoregressive Phoneme-augmented Transformer for ASR Error Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tseng23_interspeech.html": {
    "title": "Model-assisted Lexical Tone Evaluation of three-year-old Chinese-speaking Children by also Considering Segment Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23b_interspeech.html": {
    "title": "Sentence Embedder Guided Utterance Encoder (SEGUE) for Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23u_interspeech.html": {
    "title": "Joint Time and Frequency Transformer for Chinese Opera Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23_interspeech.html": {
    "title": "AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for Acoustic Word Discrimination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/arvan23_interspeech.html": {
    "title": "Investigating Reproducibility at Interspeech Conferences: A Longitudinal and Comparative Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pouranbenveyseh23_interspeech.html": {
    "title": "Combining Heterogeneous Structures for Event Causality Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/biswas23_interspeech.html": {
    "title": "An Efficient Approach for the Automated Segmentation and Transcription of the People's Speech Sorpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23g_interspeech.html": {
    "title": "Diverse Feature Mapping and Fusion via Multitask Learning for Multilingual Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bahar23_interspeech.html": {
    "title": "Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23e_interspeech.html": {
    "title": "Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kim23j_interspeech.html": {
    "title": "Efficient Adaptation of Spoken Language Understanding based on End-to-End Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23d_interspeech.html": {
    "title": "PhonMatchNet: Phoneme-Guided Zero-Shot Keyword Spotting for User-Defined Keywords",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhu23_interspeech.html": {
    "title": "Mix before Align: Towards Zero-shot Cross-lingual Sentiment Analysis via Soft-Mix and Multi-View Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/papi23_interspeech.html": {
    "title": "AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/polak23_interspeech.html": {
    "title": "Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sikasote23_interspeech.html": {
    "title": "Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mun23_interspeech.html": {
    "title": "Towards Single Integrated Spoofing-aware Speaker Verification Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ba_interspeech.html": {
    "title": "Pseudo-Siamese Network based Timbre-reserved Black-box Adversarial Attack in Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23v_interspeech.html": {
    "title": "Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23v_interspeech.html": {
    "title": "Robust Audio Anti-spoofing Countermeasure with Joint Training of Front-end and Back-end Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kawa23b_interspeech.html": {
    "title": "Improved DeepFake Detection Using Whisper Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23c_interspeech.html": {
    "title": "DoubleDeceiver: Deceiving the Speaker Verification System Protected by Spoofing Countermeasures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/panchapagesan23_interspeech.html": {
    "title": "On Training a Neural Residual Acoustic Echo Suppressor for Improved ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lemercier23_interspeech.html": {
    "title": "Extending DNN-based Multiplicative Masking to Deep Subband Filtering for Improved Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23b_interspeech.html": {
    "title": "UnSE: Unsupervised Speech Enhancement Using Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23k_interspeech.html": {
    "title": "MC-SpEx: Towards Effective Speaker Extraction with Multi-Scale Interfusion and Conditional Speaker Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bartolewska23_interspeech.html": {
    "title": "Causal Signal-Based DCCRN with Overlapped-Frame Prediction for Online Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23q_interspeech.html": {
    "title": "Gesper: A Restoration-Enhancement Framework for General Speech Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ryumina23_interspeech.html": {
    "title": "Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pudo23_interspeech.html": {
    "title": "MOCKS 1.0: Multilingual Open Custom Keyword Spotting Testset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eisenstein23_interspeech.html": {
    "title": "MD3: The Multi-Dialect Dataset of Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/anwar23_interspeech.html": {
    "title": "MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/suwanbandit23_interspeech.html": {
    "title": "Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23d_interspeech.html": {
    "title": "HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bekal23_interspeech.html": {
    "title": "A Metric-Driven Approach to Conformer Layer Pruning for Efficient ASR Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gurunathshivakumar23_interspeech.html": {
    "title": "Distillation Strategies for Discriminative Speech Recognition Rescoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pouthier23_interspeech.html": {
    "title": "Another Point of View on Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23e_interspeech.html": {
    "title": "RASR2: The RWTH ASR Toolkit for Generic Sequence-to-sequence Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/filimonov23_interspeech.html": {
    "title": "Streaming Speech-to-Confusion Network Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23d_interspeech.html": {
    "title": "Accurate and Structured Pruning for Efficient Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chua23_interspeech.html": {
    "title": "MERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gupta23_interspeech.html": {
    "title": "Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shahin23_interspeech.html": {
    "title": "Improving wav2vec2-based Spoken Language Identification by Learning Phonological Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/praveen23_interspeech.html": {
    "title": "Language Identification Networks for Multilingual Everyday Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/styles23_interspeech.html": {
    "title": "Investigating model performance in language identification: beyond simple error statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kodali23_interspeech.html": {
    "title": "Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kathan23_interspeech.html": {
    "title": "The effect of clinical intervention on the speech of individuals with PTSD: features and recognition performances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/triantafyllopoulos23_interspeech.html": {
    "title": "Analysis and automatic prediction of exertion from speech: Contrasting objective and subjective measures collected while running",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tao23_interspeech.html": {
    "title": "The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eni23_interspeech.html": {
    "title": "Comparing Hand-Crafted Features to Spectrograms for Autism Severity Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mijnders23_interspeech.html": {
    "title": "Acoustic characteristics of depression in older adults' speech: the role of covariates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23b_interspeech.html": {
    "title": "Dual Transformer Decoder based Features Fusion Network for Automated Audio Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pellegrini23_interspeech.html": {
    "title": "Adapting a ConvNeXt Model to Audio Classification on AudioSet",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23w_interspeech.html": {
    "title": "Few-shot Class-incremental Audio Classification Using Stochastic Classifier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xie23d_interspeech.html": {
    "title": "Enhance Temporal Relations in Audio Captioning with Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23s_interspeech.html": {
    "title": "First Language Effects on Second Language Perception: Evidence from English Low-vowel Nasal Sequences Perceived by L1 Mandarin Chinese Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/maity23_interspeech.html": {
    "title": "Motor Control Similarity Between Speakers Saying \"A Souk\" Using Inverse Atlas Tongue Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23t_interspeech.html": {
    "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoshinaga23_interspeech.html": {
    "title": "A Relationship Between Vocal Fold Vibration and Droplet Production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/garnier23_interspeech.html": {
    "title": "Audio, Visual and Audiovisual intelligibility of vowels produced in noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elie23_interspeech.html": {
    "title": "Optimal control of speech with context-dependent articulatory targets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23d_interspeech.html": {
    "title": "Computational modeling of auditory brainstem responses derived from modified speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ma_interspeech.html": {
    "title": "Leveraging Label Information for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tan23c_interspeech.html": {
    "title": "Improving End-to-End Modeling For Mandarin-English Code-Switching Using Lightweight Switch-Routing Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ea_interspeech.html": {
    "title": "Frequency Patterns of Individual Speaker Characteristics at Higher and Lower Spectral Ranges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gosselkeberthelsen23_interspeech.html": {
    "title": "Adaptation to predictive prosodic cues in non-native standard dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/archerboyd23_interspeech.html": {
    "title": "Head movements in two- and four-person interactive conversational tasks in noisy and moderately reverberant conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23d_interspeech.html": {
    "title": "Second language identification of Vietnamese tones by native Mandarin learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/fagniart23_interspeech.html": {
    "title": "Nasal vowel production and grammatical processing in French-speaking children with cochlear implants and normal-hearing peers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23f_interspeech.html": {
    "title": "Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23b_interspeech.html": {
    "title": "L2-Mandarin regional accent variability during Mandarin tone-word training facilitates English listeners' subsequent tone categorizations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ueda23_interspeech.html": {
    "title": "HumanDiffusion: diffusion model using perceptual gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kachel23_interspeech.html": {
    "title": "Queer Events, Relationships, and Sports: Does Topic Influence Speakers' Acoustic Expression of Sexual Orientation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gunason23_interspeech.html": {
    "title": "Epoch-Based Spectrum Estimation for Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mehta23_interspeech.html": {
    "title": "OverFlow: Putting flows on top of neural transducers for better TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mehrish23_interspeech.html": {
    "title": "ADAPTERMIX: Exploring the Efficacy of Mixture of Adapters for Low-Resource TTS Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23c_interspeech.html": {
    "title": "Prior-free Guided TTS: An Improved and Efficient Diffusion-based Text-Guided Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/iashchenko23_interspeech.html": {
    "title": "UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yoon23_interspeech.html": {
    "title": "Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/guan23_interspeech.html": {
    "title": "Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kogel23_interspeech.html": {
    "title": "Towards Robust FastSpeech 2 by Modelling Residual Multimodality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23c_interspeech.html": {
    "title": "Real time spectrogram inversion on mobile phone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/park23_interspeech.html": {
    "title": "Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wells23_interspeech.html": {
    "title": "A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/krug23_interspeech.html": {
    "title": "Self-Supervised Solution to the Control Problem of Articulatory Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23f_interspeech.html": {
    "title": "Hierarchical Timbre-Cadence Speaker Encoder for Zero-shot Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23_interspeech.html": {
    "title": "ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/du23_interspeech.html": {
    "title": "Improving WaveRNN with Heuristic Dynamic Blending for Fast and High-Quality GPU Vocoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/choi23_interspeech.html": {
    "title": "Intelligible Lip-to-Speech Synthesis with Speech Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23p_interspeech.html": {
    "title": "Parameter-Efficient Learning for Text-to-Speech Accent Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/khan23_interspeech.html": {
    "title": "Controlling formant frequencies with neural text-to-speech for the manipulation of perceived speaker age",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jang23b_interspeech.html": {
    "title": "FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kaneko23_interspeech.html": {
    "title": "iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23_interspeech.html": {
    "title": "VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luong23_interspeech.html": {
    "title": "Controlling Multi-Class Human Vocalization Generation via a Simple Segment-based Labeling Scheme",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bhogale23_interspeech.html": {
    "title": "Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23_interspeech.html": {
    "title": "Domain Adaptive Self-supervised Training of Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/olivier23_interspeech.html": {
    "title": "There is more than one kind of robustness: Fooling Whisper with adversarial examples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heggan23_interspeech.html": {
    "title": "MT-SLVR: Multi-Task Self-Supervised Learning for Transformation In(Variant) Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23l_interspeech.html": {
    "title": "Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23l_interspeech.html": {
    "title": "Blank-regularized CTC for Frame Skipping in Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jayakumar23_interspeech.html": {
    "title": "The Tag-Team Approach: Leveraging CLS and Language Tagging for Enhancing Multilingual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/unni23_interspeech.html": {
    "title": "Improving RNN-Transducers with Acoustic LookAhead",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/markl23_interspeech.html": {
    "title": "Everyone has an accent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/maison23_interspeech.html": {
    "title": "Some Voices are Too Common: Building Fair Speech Recognition Systems Using the CommonVoice Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23u_interspeech.html": {
    "title": "Information Magnitude Based Dynamic Sub-sampling for Speech-to-text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/grice23_interspeech.html": {
    "title": "What's in a Rise? The Relevance of Intonation for Attention Orienting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23i_interspeech.html": {
    "title": "HierVST: Hierarchical Adaptive Zero-shot Voice Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23e_interspeech.html": {
    "title": "VISinger2: High-Fidelity End-to-End Singing Voice Synthesis Enhanced by Digital Signal Processing Synthesizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23c_interspeech.html": {
    "title": "EdenTTS: A Simple and Efficient Parallel Text-to-speech Architecture with Collaborative Duration-alignment Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23c_interspeech.html": {
    "title": "Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/montesinos23_interspeech.html": {
    "title": "Speech inpainting: Context-based speech synthesis guided by video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tran23d_interspeech.html": {
    "title": "STEN-TTS: Improving Zero-shot Cross-Lingual Transfer for Multi-Lingual TTS with Style-Enhanced Normalization Diffusion Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kano23_interspeech.html": {
    "title": "Average Token Delay: A Latency Metric for Simultaneous Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qian23_interspeech.html": {
    "title": "Automatic Speech Recognition Transformer with Global Contextual Information Decoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sudo23c_interspeech.html": {
    "title": "Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/praveen23b_interspeech.html": {
    "title": "Prefix Search Decoding for RNN Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bain23_interspeech.html": {
    "title": "WhisperX: Time-Accurate Speech Transcription of Long-Form Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nigmatulina23_interspeech.html": {
    "title": "Implementing Contextual Biasing in GPU Decoder for Online ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chung23_interspeech.html": {
    "title": "MF-PAM: Accurate Pitch Estimation through Periodicity Analysis and Multi-level Feature Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/attia23_interspeech.html": {
    "title": "Enhancing Speech Articulation Analysis Using A Geometric Transformation of the X-ray Microbeam Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jouaiti23_interspeech.html": {
    "title": "Matching Acoustic and Perceptual Measures of Phonation Assessment in Disordered Speech - A Case Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yuan23_interspeech.html": {
    "title": "Improved Contextualized Speech Representations for Tonal Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chandrasekar23_interspeech.html": {
    "title": "A Study on the Importance of Formant Transitions for Stop-Consonant Classification in VCV Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/eren23_interspeech.html": {
    "title": "FusedF0: Improving DNN-based F0 Estimation by Fusion of Summary-Correlograms and Raw Waveform Representations of Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kyung23_interspeech.html": {
    "title": "Improving Joint Speech and Emotion Recognition Using Global Style Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nagase23_interspeech.html": {
    "title": "Speech Emotion Recognition by Estimating Emotional Label Sequences with Phoneme Class Attribute",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23_interspeech.html": {
    "title": "Unsupervised Transfer Components Learning for Cross-Domain Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/prisayad23_interspeech.html": {
    "title": "Dual Memory Fusion for Multimodal Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kondratenko23_interspeech.html": {
    "title": "Hybrid Dataset for Speech Emotion Recognition in Russian Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hsu23_interspeech.html": {
    "title": "Speech Emotion Recognition using Decomposed Speech via Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23b_interspeech.html": {
    "title": "Prospective Validation of Motor-Based Intervention with Automated Mispronunciation Detection of Rhotics in Residual Speech Sound Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23_interspeech.html": {
    "title": "Classifying Rhoticity of /ɹ/ in Speech Sound Disorder using Age-and-Sex Normalized Formants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/benway23c_interspeech.html": {
    "title": "Acoustic-to-Articulatory Speech Inversion Features for Mispronunciation Detection of /ɹ/ in Child Speech Sound Disorders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/piton23_interspeech.html": {
    "title": "Using Commercial ASR Solutions to Assess Reading Skills in Children: A Case Report",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gebauer23_interspeech.html": {
    "title": "Exploiting Diversity of Automatic Transcripts from Distinct Speech Recognition Techniques for Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rumberg23_interspeech.html": {
    "title": "Uncertainty Estimation for Connectionist Temporal Classification Based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lavechin23_interspeech.html": {
    "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23c_interspeech.html": {
    "title": "Data augmentation for children ASR and child-adult speaker classification using voice conversion methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shetty23_interspeech.html": {
    "title": "Developmental Articulatory and Acoustic Features for Six to Ten Year Old Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23u_interspeech.html": {
    "title": "Automatically Predicting Perceived Conversation Quality in a Pediatric Sample Enriched for Autism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/johnson23_interspeech.html": {
    "title": "An Equitable Framework for Automatically Assessing Children's Oral Narrative Language Abilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cao23_interspeech.html": {
    "title": "An Analysis of Goodness of Pronunciation for Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sy23_interspeech.html": {
    "title": "Measuring Language Development From Child-centered Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hung23_interspeech.html": {
    "title": "Speaking Clearly, Understanding Better: Predicting the L2 Narrative Comprehension of Chinese Bilingual Kindergarten Children Based on Speech Intelligibility Using a Machine Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/charuau23_interspeech.html": {
    "title": "Speech Breathing Behavior During Pauses in Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23e_interspeech.html": {
    "title": "Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ariasvergara23_interspeech.html": {
    "title": "Measuring Phonological Precision in Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ng23_interspeech.html": {
    "title": "A Study on Using Duration and Formant Features in Automatic Detection of Speech Sound Disorder in Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baumann23_interspeech.html": {
    "title": "Influence of Utterance and Speaker Characteristics on the Classification of Children with Cleft Lip and Palate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ma23g_interspeech.html": {
    "title": "Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mcneill23_interspeech.html": {
    "title": "An Autoregressive Conversational Dynamics Model for Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hori23_interspeech.html": {
    "title": "Style-transfer based Speech and Audio-visual Scene understanding for Robot Action Sequence Acquisition from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/soltau23_interspeech.html": {
    "title": "Speech Aware Dialog System Technology Challenge (DSTC11)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cai23_interspeech.html": {
    "title": "Knowledge-Retrieval Task-Oriented Dialog Systems with Semi-Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lee23k_interspeech.html": {
    "title": "Tracking Must Go On : Dialogue State Tracking with Verified Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ja_interspeech.html": {
    "title": "Ordered and Binary Speaker Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kataria23_interspeech.html": {
    "title": "Self-FiLM: Conditioning GANs with self-supervised representations for bandwidth extension based speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heo23b_interspeech.html": {
    "title": "Curriculum Learning for Self-supervised Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23aa_interspeech.html": {
    "title": "Introducing Self-Supervised Phonetic Information for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cordlandwehr23_interspeech.html": {
    "title": "A Teacher-Student Approach for Extracting Informative Speaker Embeddings From Speech Mixtures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lepage23_interspeech.html": {
    "title": "Experimenting with Additive Margins for Contrastive Self-Supervised Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hope23_interspeech.html": {
    "title": "Nonbinary American English speakers encode gender in vowel acoustics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharp23_interspeech.html": {
    "title": "Coarticulation of Sibe Vowels and Dorsal Fricatives in Spontaneous Speech: An Acoustic Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/brown23_interspeech.html": {
    "title": "Using speech synthesis to explain automatic speaker recognition: a new application of synthetic speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23i_interspeech.html": {
    "title": "Same F0, Different Tones: A Multidimensional Investigation of Zhangzhou Tones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/english23_interspeech.html": {
    "title": "Discovering Phonetic Feature Event Patterns in Transformer Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ra_interspeech.html": {
    "title": "A System for Generating Voice Source Signals that Implements the Transformed LF-model Parameter Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/siriwardena23b_interspeech.html": {
    "title": "Speaker-independent Speech Inversion for Estimation of Nasalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/hu23e_interspeech.html": {
    "title": "Effects of Tonal Coarticulation and Prosodic Positions on Tonal Contours of Low Rising Tones: In the Case of Xiamen Dialect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/issa23_interspeech.html": {
    "title": "Durational and Non-durational Correlates of Lexical and Derived Geminates in Arabic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rao23_interspeech.html": {
    "title": "Mapping Phonemes to Acoustic Symbols and Codes Using Synchrony in Speech Modulation Vectors Estimated by the Travellingwave Filter Bank",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ge23_interspeech.html": {
    "title": "Rhythmic Characteristics of L2 German Speech by Advanced Chinese Learners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kelterer23_interspeech.html": {
    "title": "(Dis)agreement and Preference Structure are Reflected in Matching Along Distinct Acoustic-prosodic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/christodoulidou23_interspeech.html": {
    "title": "Vowel reduction by Greek-speaking children: The effect of stress and word length",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lennes23_interspeech.html": {
    "title": "Pitch distributions in a very large corpus of spontaneous Finnish speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kudera23_interspeech.html": {
    "title": "Speech Enhancement Patterns in Human-Robot Interaction: A Cross-Linguistic Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lux23_interspeech.html": {
    "title": "Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23ga_interspeech.html": {
    "title": "Dual Audio Encoders Based Mandarin Prosodic Boundary Prediction by Using Multi-Granularity Prosodic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23i_interspeech.html": {
    "title": "NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23n_interspeech.html": {
    "title": "MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pavankalyan23_interspeech.html": {
    "title": "Narrator or Character: Voice Modulation in an Expressive Multi-speaker TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cui23b_interspeech.html": {
    "title": "CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oh23_interspeech.html": {
    "title": "Semi-supervised Learning for Continuous Emotional Intensity Controllable Speech Synthesis with Disentangled Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nguyen23_interspeech.html": {
    "title": "Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23fa_interspeech.html": {
    "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kunesova23_interspeech.html": {
    "title": "Neural Speech Synthesis with Enriched Phrase Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/swiatkowski23_interspeech.html": {
    "title": "Cross-lingual Prosody Transfer for Expressive Machine Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/elmers23_interspeech.html": {
    "title": "Synthesis after a couple PINTs: Investigating the Role of Pause-Internal Phonetic Particles in Speech Synthesis and Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/geneva23_interspeech.html": {
    "title": "Accentor: An Explicit Lexical Stress Model for TTS Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shechtman23_interspeech.html": {
    "title": "A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23j_interspeech.html": {
    "title": "Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23c_interspeech.html": {
    "title": "Prosody Modeling with 3D Visual Information for Expressive Video Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23f_interspeech.html": {
    "title": "LightClone: Speaker-guided Parallel Subnet Selection for Few-shot Voice Cloning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhong23_interspeech.html": {
    "title": "EE-TTS: Emphatic Expressive TTS with Linguistic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ogun23_interspeech.html": {
    "title": "Stochastic Pitch Prediction Improves the Diversity and Naturalness of Speech in Glow-TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23_interspeech.html": {
    "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23t_interspeech.html": {
    "title": "PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tian23b_interspeech.html": {
    "title": "Creating Personalized Synthetic Voices from Post-Glossectomy Speech with Guided Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vaessen23_interspeech.html": {
    "title": "Towards Multi-task Learning of Speech and Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23f_interspeech.html": {
    "title": "Regarding Topology and Variant Frame Rates for Differentiable WFST-based End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rybakov23b_interspeech.html": {
    "title": "2-bit Conformer quantization for automatic speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23_interspeech.html": {
    "title": "Time-Domain Speech Enhancement for Robust Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yifan23_interspeech.html": {
    "title": "Multi-channel multi-speaker transformer for speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ye23_interspeech.html": {
    "title": "Fake the Real: Backdoor Attack on Deep Speech Classification via Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/miwa23_interspeech.html": {
    "title": "Dialect Speech Recognition Modeling using Corpus of Japanese Dialects and Self-Supervised Learning-based Model XLSR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23d_interspeech.html": {
    "title": "Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/raissi23_interspeech.html": {
    "title": "Competitive and Resource Efficient Factored Hybrid HMM Systems are Simpler Than You Think",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23d_interspeech.html": {
    "title": "MMSpeech: Multi-modal Multi-task Encoder-Decoder Pre-training for speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kreyssig23_interspeech.html": {
    "title": "Biased Self-supervised Learning for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23q_interspeech.html": {
    "title": "A Unified Recognition and Correction Model under Noisy and Accent Speech Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/huang23h_interspeech.html": {
    "title": "wav2vec 2.0 ASR for Cantonese-Speaking Older Adults in a Clinical Setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/an23_interspeech.html": {
    "title": "BAT: Boundary aware transducer for memory-efficient and low-latency ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tian23_interspeech.html": {
    "title": "Bayes Risk Transducer: Transducer with Controllable Alignment Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/alastruey23_interspeech.html": {
    "title": "Multi-View Frequency-Attention Alternative to CNN Frontends for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sankar23_interspeech.html": {
    "title": "Investigating the dynamics of hand and lips in French Cued Speech using attention mechanisms and CTC-based decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23h_interspeech.html": {
    "title": "Hearing Loss Affects Emotion Perception in Older Adults: Evidence from a Prosody-Semantics Stroop Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kong23b_interspeech.html": {
    "title": "Cochlear-implant Listeners Listening to Cochlear-implant Simulated Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/murton23_interspeech.html": {
    "title": "Validation of a Task-Independent Cepstral Peak Prominence Measure with Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23b_interspeech.html": {
    "title": "Score-balanced Loss for Multi-aspect Pronunciation Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tayebiarasteh23_interspeech.html": {
    "title": "Federated Learning for Secure Development of AI Models for Parkinson's Disease Detection Using Speech from Different Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhou23c_interspeech.html": {
    "title": "F0inTFS: A lightweight periodicity enhancement strategy for cochlear implants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/obrien23_interspeech.html": {
    "title": "Differentiating acoustic and physiological features in speech for hypoxia detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23h_interspeech.html": {
    "title": "Mandarin Electrolaryngeal Speech Voice Conversion using Cross-domain Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chien23_interspeech.html": {
    "title": "Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/illner23_interspeech.html": {
    "title": "Which aspects of motor speech disorder are captured by Mel Frequency Cepstral Coefficients? Evidence from the change in STN-DBS conditions in Parkinson's disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/subramanian23_interspeech.html": {
    "title": "Detecting Manifest Huntington's Disease Using Vocal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chen23q_interspeech.html": {
    "title": "Exploring multi-task learning and data augmentation in dementia detection with self-supervised pretrained models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23i_interspeech.html": {
    "title": "GL-SSD: Global and Local Speech Style Disentanglement by vector quantization for robust sentence boundary detection in speech stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shi23c_interspeech.html": {
    "title": "Semantic VAD: Low-Latency Voice Activity Detection for Speech Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gudepu23_interspeech.html": {
    "title": "Dynamic Encoder RNN for Online Voice Activity Detection in Adverse Noise Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/moussa23_interspeech.html": {
    "title": "Point to the Hidden: Exposing Speech Audio Splicing via Signal Pointer Nets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23k_interspeech.html": {
    "title": "Real-Time Causal Spectro-Temporal Voice Activity Detection Based on Convolutional Encoding and Residual Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kang23c_interspeech.html": {
    "title": "SVVAD: Personal Voice Activity Detection for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/farooq23_interspeech.html": {
    "title": "Learning Cross-lingual Mappings for Data Augmentation to Improve Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/olatunji23_interspeech.html": {
    "title": "AfriNames: Most ASR Models \"Butcher\" African Names",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lonergan23_interspeech.html": {
    "title": "Towards Dialect-inclusive Recognition in a Low-resource Language: Are Balanced Corpora the Answer?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/javed23_interspeech.html": {
    "title": "Svarah: Evaluating English ASR Systems on Indian Accents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/talafha23_interspeech.html": {
    "title": "N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/picheny23_interspeech.html": {
    "title": "The MALACH Corpus: Results with End-to-End Architectures and Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23c_interspeech.html": {
    "title": "Unsupervised speech enhancement with deep dynamical generative speech and noise models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lin23f_interspeech.html": {
    "title": "Noise-Robust Bandwidth Expansion for 8K Speech Recordings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/shuai23_interspeech.html": {
    "title": "mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23b_interspeech.html": {
    "title": "Zoneformer: On-device Neural Beamformer For In-car Multi-zone Speech Separation, Enhancement and Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xu23c_interspeech.html": {
    "title": "Low-complexity Broadband Beampattern Synthesis using Array Response Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhao23d_interspeech.html": {
    "title": "A GAN Speech Inpainting Model for Audio Editing Software",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wu23k_interspeech.html": {
    "title": "Deep Speech Synthesis from MRI-Based Articulatory Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/siriwardena23_interspeech.html": {
    "title": "Learning to Compute the Articulatory Representations of Speech with the MIRRORNET",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/strauch23_interspeech.html": {
    "title": "Generating high-resolution 3D real-time MRI of the vocal tract",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bandekar23_interspeech.html": {
    "title": "Exploring a classification approach using quantised articulatory movements for acoustic to articulatory inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oota23b_interspeech.html": {
    "title": "MEG Encoding using Word Context Semantics in Listening Stories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cantisani23_interspeech.html": {
    "title": "Investigating the cortical tracking of speech and music with sung speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/keding23_interspeech.html": {
    "title": "Coherence Estimation Tracks Auditory Attention in Listeners with Hearing Impairment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/oota23_interspeech.html": {
    "title": "Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI Brain Activity?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/qiu23_interspeech.html": {
    "title": "Exploring Auditory Attention Decoding using Speaker Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/soman23_interspeech.html": {
    "title": "Enhancing the EEG Speech Match Mismatch Tasks With Word Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cheng23e_interspeech.html": {
    "title": "Similar Hierarchical Representation of Speech and Other Complex Sounds In the Brain and Deep Residual Networks: An MEG Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/macintyre23_interspeech.html": {
    "title": "Effects of spectral degradation on the cortical tracking of the speech envelope",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/calderondepalma23_interspeech.html": {
    "title": "Effects of spectral and temporal modulation degradation on intelligibility and cortical tracking of speech signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23da_interspeech.html": {
    "title": "Transfer Learning for Personality Perception via Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nagano23_interspeech.html": {
    "title": "A stimulus-organism-response model of willingness to buy from advertising speech using voice quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/doukhan23_interspeech.html": {
    "title": "Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yanagida23_interspeech.html": {
    "title": "Influence of Personal Traits on Impressions of One's Own Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kirkland23_interspeech.html": {
    "title": "Pardon my disfluency: The impact of disfluency effects on the perception of speaker competence and confidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gessinger23_interspeech.html": {
    "title": "Cross-linguistic Emotion Perception in Human and TTS Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/duan23_interspeech.html": {
    "title": "Joint Learning Feature and Model Adaptation for Unsupervised Acoustic Modelling of Child Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/molenaar23_interspeech.html": {
    "title": "Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bai23_interspeech.html": {
    "title": "An ASR-enabled Reading Tutor: Investigating Feedback to Optimize Interaction for Learning to Read",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jain23_interspeech.html": {
    "title": "Adaptation of Whisper models to child speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yin23b_interspeech.html": {
    "title": "Let's Give a Voice to Conversational Agents in Virtual Reality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/baali23b_interspeech.html": {
    "title": "FOOCTTS: Generating Arabic Speech with Acoustic Environment for Football Commentator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/liu23x_interspeech.html": {
    "title": "Video Summarization Leveraging Multimodal Information for Presentations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nathan23_interspeech.html": {
    "title": "What questions are my customers asking?: Towards Actionable Insights from Customer Questions in Contact Center Calls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/tripathi23_interspeech.html": {
    "title": "COnVoy: A Contact Center Operated Pipeline for Voice of Customer Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rastorgueva23_interspeech.html": {
    "title": "NeMo Forced Aligner and its application to word alignment for subtitle generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/pattnaik23_interspeech.html": {
    "title": "CauSE: Causal Search Engine for Understanding Contact-Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sachdeva23_interspeech.html": {
    "title": "Tailored Real-Time Call Summarization System for Contact Centers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mandke23_interspeech.html": {
    "title": "Federated Learning Toolkit with Voice-based User Verification Demo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/dugan23_interspeech.html": {
    "title": "Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cho23b_interspeech.html": {
    "title": "Fast Enrollable Streaming Keyword Spotting System: Training and Inference using a Web Browser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/agrawal23b_interspeech.html": {
    "title": "Cross-lingual/Cross-channel Intent Detection in Contact-Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/heo23_interspeech.html": {
    "title": "One-Step Knowledge Distillation and Fine-Tuning in Using Large Pre-Trained Self-Supervised Learning Models for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kawa23_interspeech.html": {
    "title": "Defense Against Adversarial Attacks on Audio DeepFake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/rosello23_interspeech.html": {
    "title": "A conformer-based classifier for variable-length utterance processing in anti-spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ia_interspeech.html": {
    "title": "Conformer-based Language Embedding with Self-Knowledge Distillation for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zuluagagomez23_interspeech.html": {
    "title": "CommonAccent: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cumani23_interspeech.html": {
    "title": "From adaptive score normalization to adaptive data normalization for speaker verification systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23ha_interspeech.html": {
    "title": "CAM++: A Fast and Efficient Network for Speaker Verification Using Context-Aware Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kakouros23_interspeech.html": {
    "title": "North Sámi Dialect Identification with Self-supervised Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jung23c_interspeech.html": {
    "title": "Encoder-decoder Multimodal Speaker Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/nam23_interspeech.html": {
    "title": "Disentangled Representation Learning for Multilingual Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jia23b_interspeech.html": {
    "title": "A Compact End-to-End Model with Local and Global Context for Spoken Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sullivan23_interspeech.html": {
    "title": "On the Robustness of Arabic Speech Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wang23u_interspeech.html": {
    "title": "Adaptive Neural Network Quantization For Lightweight Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/su23_interspeech.html": {
    "title": "Adversarial Diffusion Probability Model For Cross-domain Speaker Verification Integrating Contrastive Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jiang23f_interspeech.html": {
    "title": "Chinese Dialect Recognition Based on Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ito23_interspeech.html": {
    "title": "Spoofing Attacker Also Benefits from Self-Supervised Pretrained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/vashishth23_interspeech.html": {
    "title": "Label Aware Speech Representation Learning For Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/luo23c_interspeech.html": {
    "title": "Exploring the Impact of Back-End Network on Wav2vec 2.0 for Dialect Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/peng23_interspeech.html": {
    "title": "Improving Speaker Verification with Self-Pretrained Transformer Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ribeiro23_interspeech.html": {
    "title": "Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/linke23_interspeech.html": {
    "title": "What do self-supervised speech representations encode? An analysis of languages, varieties, speaking styles and speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23ca_interspeech.html": {
    "title": "A Compressed Synthetic Speech Detection Method with Compression Feature Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23j_interspeech.html": {
    "title": "Outlier-aware Inlier Modeling and Multi-scale Scoring for Anomalous Sound Detection via Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/li23c_interspeech.html": {
    "title": "MOSLight: A Lightweight Data-Efficient System for Non-Intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23c_interspeech.html": {
    "title": "A Multi-Scale Attentive Transformer for Multi-Instrument Symbolic Music Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gao23i_interspeech.html": {
    "title": "MTANet: Multi-band Time-frequency Attention Network for Singing Melody Extraction from Polyphonic Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chunhui23_interspeech.html": {
    "title": "Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on Generative Adversarial Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/solanki23_interspeech.html": {
    "title": "Do Vocal Breath Sounds Encode Gender Cues for Automatic Gender Classification?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sugiura23_interspeech.html": {
    "title": "Automatic Exploration of Optimal Data Processing Operations for Sound Data Augmentation Using Improved Differentiable Automatic Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/xiao23b_interspeech.html": {
    "title": "A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/wei23b_interspeech.html": {
    "title": "RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/manocha23_interspeech.html": {
    "title": "Spatialization Quality Metric for Binaural Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/roy23_interspeech.html": {
    "title": "AsthmaSCELNet: A Lightweight Supervised Contrastive Embedding Learning Framework for Asthma Classification Using Lung Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/bae23b_interspeech.html": {
    "title": "Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/richter23_interspeech.html": {
    "title": "Remote Assessment for ALS using Multimodal Dialog Agents: Data Quality, Feasibility and Task Compliance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yariv23_interspeech.html": {
    "title": "Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/romero23_interspeech.html": {
    "title": "Obstructive sleep apnea screening with breathing sounds and respiratory effort: a multimodal deep learning approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sun23f_interspeech.html": {
    "title": "Investigation of Music Emotion Recognition Based on Segmented Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23c_interspeech.html": {
    "title": "The Effects of Input Type and Pronunciation Dictionary Usage in Transfer Learning for Low-Resource Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/do23d_interspeech.html": {
    "title": "Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/gu23b_interspeech.html": {
    "title": "Robust Feature Decoupling in Voice Conversion by Using Locality-Based Instance Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/jia23_interspeech.html": {
    "title": "Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/ekstedt23_interspeech.html": {
    "title": "Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/cong23_interspeech.html": {
    "title": "GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yasuda23_interspeech.html": {
    "title": "Analysis of Mean Opinion Scores in Subjective Evaluation of Synthetic Speech Based on Tail Probabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/koizumi23_interspeech.html": {
    "title": "LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/mitsui23_interspeech.html": {
    "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/thenguyen23_interspeech.html": {
    "title": "XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/kulkarni23_interspeech.html": {
    "title": "ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/deja23_interspeech.html": {
    "title": "Diffusion-based accent modelling in speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yeshpanov23_interspeech.html": {
    "title": "Multilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/zhang23h_interspeech.html": {
    "title": "CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yang23k_interspeech.html": {
    "title": "Improving Bilingual TTS Using Language And Phonology Embedding With Embedding Strength Modulator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/lu23f_interspeech.html": {
    "title": "High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/yu23_interspeech.html": {
    "title": "PronScribe: Highly Accurate Multimodal Phonemic Transcription From Speech and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/swiatkowski23b_interspeech.html": {
    "title": "Expressive Machine Dubbing Through Phrase-level Cross-lingual Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/chiang23_interspeech.html": {
    "title": "Why We Should Report the Details in Subjective Evaluation of TTS More Rigorously",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/perezzarazaga23_interspeech.html": {
    "title": "Speaker-independent neural formant synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/saito23b_interspeech.html": {
    "title": "CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2023/sharoni23_interspeech.html": {
    "title": "SASPEECH: A Hebrew Single Speaker Dataset for Text To Speech and Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}