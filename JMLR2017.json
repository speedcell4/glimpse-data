{
  "https://jmlr.org/papers/v18/14-249.html": {
    "title": "Averaged Collapsed Variational Bayes Inference",
    "abstract": "This paper presents the Averaged CVB (ACVB) inference and offers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence- guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non- expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence",
    "volume": "main",
    "checked": true,
    "id": "0f4cfa8fea4047d20217255964169ade91547fcc",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v18/14-400.html": {
    "title": "Scalable Influence Maximization for Multiple Products in Continuous-Time Diffusion Networks",
    "abstract": "A typical viral marketing model identifies influential users in a social network to maximize a single product adoption assuming unlimited user attention, campaign budgets, and time. In reality, multiple products need campaigns, users have limited attention, convincing users incurs costs, and advertisers have limited budgets and expect the adoptions to be maximized soon. Facing these user, monetary, and timing constraints, we formulate the problem as a submodular maximization task in a continuous-time diffusion model under the intersection of one matroid and multiple knapsack constraints. We propose a randomized algorithm estimating the user influence (Partial results in the paper on influence estimation have been published in a conference paper: Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha. Scalable influence estimation in continuous time diffusion networks. In Advances in Neural Information Processing Systems 26, 2013.) in a network ($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$ with $n=\\mathcal{O}(1/\\epsilon^2)$ randomizations and $\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By exploiting the influence estimation algorithm as a subroutine, we develop an adaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2 k)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active. Extensive experiments on networks of millions of nodes demonstrate that the proposed algorithms achieve the state-of- the-art in terms of effectiveness and scalability",
    "volume": "main",
    "checked": true,
    "id": "2689fcd90e0654306b51fd67eb8c4ecafc156293",
    "citation_count": 31
  },
  "https://jmlr.org/papers/v18/15-085.html": {
    "title": "Local algorithms for interactive clustering",
    "abstract": "We study the design of interactive clustering algorithms. The user supervision that we consider is in the form of cluster split/merge requests; such feedback is easy for users to provide because it only requires a high-level understanding of the clusters. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable properties in many applications. Local changes are desirable because in practice edits of other parts of the clustering are considered churn - changes that are perceived as quality-neutral or quality-negative. We show that in this framework we can still design provably correct algorithms given that our data satisfies natural separability properties. We also show that our framework works well in practice",
    "volume": "main",
    "checked": true,
    "id": "042b60abf09707305523b70d87312817654fe180",
    "citation_count": 94
  },
  "https://jmlr.org/papers/v18/15-492.html": {
    "title": "SnapVX: A Network-Based Convex Optimization Solver",
    "abstract": "SnapVX is a high-performance solver for convex optimization problems defined on networks. For problems of this form, SnapVX provides a fast and scalable solution with guaranteed global convergence. It combines the capabilities of two open source software packages: Snap.py and CVXPY. Snap.py is a large scale graph processing library, and CVXPY provides a general modeling framework for small-scale subproblems. SnapVX offers a customizable yet easy-to-use Python interface with out-of- the- box functionality. Based on the Alternating Direction Method of Multipliers (ADMM), it is able to efficiently store, analyze, parallelize, and solve large optimization problems from a variety of different applications. Documentation, examples, and more can be found on the SnapVX website at snap.stanford.edu/snapvx",
    "volume": "main",
    "checked": true,
    "id": "e6ffdba48af4e48a54cadd5fb27bc9244e04b027",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v18/16-002.html": {
    "title": "Communication-efficient Sparse Regression",
    "abstract": "We devise a communication-efficient approach to distributed sparse regression in the high-dimensional setting. The key idea is to average debiased or desparsified lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines, and consistently estimates the support under weaker conditions than the lasso. On the computational side, we propose a new parallel and computationally-efficient algorithm to compute the approximate inverse covariance required in the debiasing approach, when the dataset is split across samples. We further extend the approach to generalized linear models",
    "volume": "main",
    "checked": true,
    "id": "23a96514f84e452aaa921e8ea3b65b396cab52c8",
    "citation_count": 140
  },
  "https://jmlr.org/papers/v18/16-070.html": {
    "title": "Improving Variational Methods via Pairwise Linear Response Identities",
    "abstract": "Inference methods are often formulated as variational approximations: these approxima- tions allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima",
    "volume": "main",
    "checked": true,
    "id": "d5685cbae73b502f0912dcf0f32ccd3d7d84d152",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v18/16-270.html": {
    "title": "Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks",
    "abstract": "Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low- dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs",
    "volume": "main",
    "checked": true,
    "id": "589f5953378461f86d83c303451392bed9eab760",
    "citation_count": 20
  },
  "https://jmlr.org/papers/v18/16-337.html": {
    "title": "Persistence Images: A Stable Vector Representation of Persistent Homology",
    "abstract": "Many data sets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a data set. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite- dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs",
    "volume": "main",
    "checked": true,
    "id": "223841a71f5bce4cb03040e229d13e9a71b78ec3",
    "citation_count": 492
  },
  "https://jmlr.org/papers/v18/14-318.html": {
    "title": "Spectral Clustering Based on Local PCA",
    "abstract": "We propose a spectral clustering method based on local principal components analysis (PCA). After performing local PCA in selected neighborhoods, the algorithm builds a nearest neighbor graph weighted according to a discrepancy between the principal subspaces in the neighborhoods, and then applies spectral clustering. As opposed to standard spectral methods based solely on pairwise distances between points, our algorithm is able to resolve intersections. We establish theoretical guarantees for simpler variants within a prototypical mathematical framework for multi-manifold clustering, and evaluate our algorithm on various simulated data sets",
    "volume": "main",
    "checked": true,
    "id": "d3599be5b9a42da86748b3e87ffff42502413a9a",
    "citation_count": 94
  },
  "https://jmlr.org/papers/v18/15-038.html": {
    "title": "On Perturbed Proximal Gradient Algorithms",
    "abstract": "We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non- asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models",
    "volume": "main",
    "checked": true,
    "id": "aab2c7269ef68b9ef9ee5f8b8e90596b2fbae670",
    "citation_count": 93
  },
  "https://jmlr.org/papers/v18/15-257.html": {
    "title": "Differential Privacy for Bayesian Inference through Posterior Sampling",
    "abstract": "Differential privacy formalises privacy-preserving mechanisms that provide access to a database. Can Bayesian inference be used directly to provide private access to data? The answer is yes: under certain conditions on the prior, sampling from the posterior distribution can lead to a desired level of privacy and utility. For a uniform treatment, we define differential privacy over arbitrary data set metrics, outcome spaces and distribution families. This allows us to also deal with non-i.i.d or non-tabular data sets. We then prove bounds on the sensitivity of the posterior to the data, which delivers a measure of robustness. We also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. Finally, we provide bounds on the utility of answers to queries and on the ability of an adversary to distinguish between data sets. The latter are complemented by a novel use of Le Cam's method to obtain lower bounds on distinguishability. Our results hold for arbitrary metrics, including those for the common definition of differential privacy. For specific choices of the metric, we give a number of examples satisfying our assumptions",
    "volume": "main",
    "checked": true,
    "id": "80de56e417814d71b6b544aa592b2bec28efe1a4",
    "citation_count": 57
  },
  "https://jmlr.org/papers/v18/15-441.html": {
    "title": "Refinery: An Open Source Topic Modeling Web Platform",
    "abstract": "We introduce Refinery, an open source platform for exploring large text document collections with topic models. Refinery is a standalone web application driven by a graphical interface, so it is usable by those without machine learning or programming expertise. Users can interactively organize articles by topic and also refine this organization with phrase-level analysis. Under the hood, we train Bayesian nonparametric topic models that can adapt model complexity to the provided data with scalable learning algorithms.  The project website  contains Python code and further documentation",
    "volume": "main",
    "checked": true,
    "id": "ea3ec90c9c0a921f374f5d6697d60b1527b3eadc",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v18/15-449.html": {
    "title": "Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns",
    "abstract": "Biological brains can learn, recognize, organize, and re- generate large repertoires of temporal patterns. Here I propose a mechanism of neurodynamical pattern learning and representation, called conceptors, which offers an integrated account of a number of such phenomena and functionalities. It becomes possible to store a large number of temporal patterns in a single recurrent neural network. In the recall process, stored patterns can be morphed and focussed. Parametric families of patterns can be learnt from a very small number of examples. Stored temporal patterns can be content- addressed in ways that are analog to recalling static patterns in Hopfield networks",
    "volume": "main",
    "checked": true,
    "id": "0c497a5b95132d141dd7c310a20c7acbcd32c7c4",
    "citation_count": 42
  },
  "https://jmlr.org/papers/v18/16-107.html": {
    "title": "Automatic Differentiation Variational Inference",
    "abstract": "Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop ADVI. Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models ---no conjugacy assumptions are required. We study ADVI across ten modern probabilistic models and apply it to a dataset with millions of observations. We deploy ADVI as part of Stan, a probabilistic programming system",
    "volume": "main",
    "checked": true,
    "id": "30691d2a4eb1a6e88116c357e95b49f9573bcdae",
    "citation_count": 592
  },
  "https://jmlr.org/papers/v18/16-174.html": {
    "title": "Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters",
    "abstract": "Tuning the regularisation and kernel hyperparameters is a vital step in optimising the generalisation performance of kernel methods, such as the support vector machine (SVM). This is most often performed by minimising a resampling/cross-validation based model selection criterion, however there seems little practical guidance on the most suitable form of resampling. This paper presents the results of an extensive empirical evaluation of resampling procedures for SVM hyperparameter selection, designed to address this gap in the machine learning literature. We tested 15 different resampling procedures on 121 binary classification data sets in order to select the best SVM hyperparameters. We used three very different statistical procedures to analyse the results: the standard multi- classifier/multi-data set procedure proposed by Dem\\v{s}ar, the confidence intervals on the excess loss of each procedure in relation to 5-fold cross validation, and the Bayes factor analysis proposed by Barber. We conclude that a 2-fold procedure is appropriate to select the hyperparameters of an SVM for data sets for 1000 or more datapoints, while a 3-fold procedure is appropriate for smaller data sets",
    "volume": "main",
    "checked": true,
    "id": "79043b3bf656e7b3c475e07d3b9a473b37ce047c",
    "citation_count": 44
  },
  "https://jmlr.org/papers/v18/16-274.html": {
    "title": "A Unified Formulation and Fast Accelerated Proximal Gradient Method for Classification",
    "abstract": "Binary classification is the problem of predicting the class a given sample belongs to. To achieve a good prediction performance, it is important to find a suitable model for a given dataset. However, it is often time consuming and impractical for practitioners to try various classification models because each model employs a different formulation and algorithm. The difficulty can be mitigated if we have a unified formulation and an efficient universal algorithmic framework for various classification models to expedite the comparison of performance of different models for a given dataset. In this paper, we present a unified formulation of various classification models (including $C$-SVM, $\\ell_2$-SVM, $\\nu$-SVM, MM-FDA, MM-MPM, logistic regression, distance weighted discrimination) and develop a general optimization algorithm based on an accelerated proximal gradient (APG) method for the formulation. We design various techniques such as backtracking line search and adaptive restarting strategy in order to speed up the practical convergence of our method. We also give a theoretical convergence guarantee for the proposed fast APG algorithm. Numerical experiments show that our algorithm is stable and highly competitive to specialized algorithms designed for specific models (e.g., sequential minimal optimization (SMO) for SVM)",
    "volume": "main",
    "checked": true,
    "id": "810a874a6b9105f48287ca86da7c6ee0ce5300c6",
    "citation_count": 23
  },
  "https://jmlr.org/papers/v18/16-365.html": {
    "title": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning",
    "abstract": "imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the- art methods can be categorized into 4 groups: (i) under- sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn  and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from  github.com/scikit-learn-contrib/imbalanced-learn",
    "volume": "main",
    "checked": true,
    "id": "05c5b732fb92546c7d6eeabfadb5c14610d07373",
    "citation_count": 1457
  },
  "https://jmlr.org/papers/v18/14-467.html": {
    "title": "Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a866479d17163dff4a330cfa1682b6c86f74748b",
    "citation_count": 195
  },
  "https://jmlr.org/papers/v18/14-546.html": {
    "title": "Breaking the Curse of Dimensionality with Convex Neural Networks",
    "abstract": "We consider neural networks with a single hidden layer and non- decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity- inducing norms on the input weights, we show that high- dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non- convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non- convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomial-time algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities",
    "volume": "main",
    "checked": true,
    "id": "f9c2ece8262f9dcf4ec176799e88e51adb1fd052",
    "citation_count": 566
  },
  "https://jmlr.org/papers/v18/15-025.html": {
    "title": "Memory Efficient Kernel Approximation",
    "abstract": "Scaling kernel machines to massive data sets is a major challenge due to storage and computation issues in handling large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low-rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation framework -- Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the covtype dataset with half a million samples, MEKA takes around 70 seconds and uses less than 80 MB memory on a single machine to achieve 10% relative approximation error, while standard NystrÃÂ¶m approximation is about 6 times slower and uses more than 400MB memory to achieve similar approximation. We also present extensive experiments on applying MEKA to speed up kernel ridge regression",
    "volume": "main",
    "checked": true,
    "id": "58c85498e23c86f526223e661e250007794c8d67",
    "citation_count": 149
  },
  "https://jmlr.org/papers/v18/15-178.html": {
    "title": "On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions",
    "abstract": "We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in $L_2$- and $L_\\infty$-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses",
    "volume": "main",
    "checked": true,
    "id": "990f341846223e80a4c5fbd5c2be309eb5c8bec9",
    "citation_count": 251
  },
  "https://jmlr.org/papers/v18/15-486.html": {
    "title": "Analyzing Tensor Power Method Dynamics in Overcomplete Regime",
    "abstract": "We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multi-view mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components $k$ is much larger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models",
    "volume": "main",
    "checked": true,
    "id": "d7d0f0cfe67f84ef3adda30468bdd53fd96d571e",
    "citation_count": 46
  },
  "https://jmlr.org/papers/v18/16-131.html": {
    "title": "JSAT: Java Statistical Analysis Tool, a Library for Machine Learning",
    "abstract": "Java Statistical Analysis Tool (JSAT) is a Machine Learning library written in pure Java. It works to fill a void in the Java ecosystem for a general purpose library that is relatively high performance and flexible, which is not adequately fulfilled by Weka (Hall et al., 2009) and Java-ML (Abeel et al., 2009). Almost all of the algorithms are independently implemented using an Object- Oriented framework. JSAT is made available under the GNU GPL license here:  github.com/EdwardRaff/JSAT",
    "volume": "main",
    "checked": true,
    "id": "8347a5ae3a622ce0aaa7dd674020c18ea9c89fdf",
    "citation_count": 42
  },
  "https://jmlr.org/papers/v18/16-172.html": {
    "title": "Identifying a Minimal Class of Models for High--dimensional Data",
    "abstract": "Model selection consistency in the high--dimensional regression setting can be achieved only if strong assumptions are fulfilled. We therefore suggest to pursue a different goal, which we call a minimal class of models. The minimal class of models includes models that are similar in their prediction accuracy but not necessarily in their elements. We suggest a random search algorithm to reveal candidate models. The algorithm implements simulated annealing while using a score for each predictor that we suggest to derive using a combination of the lasso and the elastic net. The utility of using a minimal class of models is demonstrated in the analysis of two data sets",
    "volume": "main",
    "checked": false,
    "id": "3c7c2f42e87caf4d65a5a0e2d7dafc71a451aa7c",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v18/16-261.html": {
    "title": "Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA",
    "abstract": "WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA's learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm",
    "volume": "main",
    "checked": true,
    "id": "cef5f429a2ee5bad3a258c7baf7cc77e9af047a3",
    "citation_count": 613
  },
  "https://jmlr.org/papers/v18/16-300.html": {
    "title": "POMDPs.jl: A Framework for Sequential Decision Making under Uncertainty",
    "abstract": "POMDPs.jl is an open-source framework for solving Markov decision processes (MDPs) and partially observable MDPs (POMDPs). POMDPs.jl allows users to specify sequential decision making problems with minimal effort without sacrificing the expressive nature of POMDPs, making this framework viable for both educational and research purposes. It is written in the Julia language to allow flexible prototyping and large-scale computation that leverages the high-performance nature of the language. The associated JuliaPOMDP community also provides a number of state-of-the-art MDP and POMDP solvers and a rich library of support tools to help with implementing new solvers and evaluating the solution results. The most recent version of POMDPs.jl, the related packages, and documentation can be found at github.com/ JuliaPOMDP/POMDPs.jl",
    "volume": "main",
    "checked": true,
    "id": "5c84814b138a6c882ffed34d89bd8882077a5fed",
    "citation_count": 100
  },
  "https://jmlr.org/papers/v18/10-231.html": {
    "title": "Generalized P{\\'o}lya Urn for Time-Varying Pitman-Yor Processes",
    "abstract": "This article introduces a class of first-order stationary time- varying Pitman-Yor processes. Subsuming our construction of time-varying Dirichlet processes presented in (Caron et al., 2007), these models can be used for time-dynamic density estimation and clustering. Our intuitive and simple construction relies on a generalized PÃÂ³lya urn scheme. Significantly, this construction yields marginal distributions at each time point that can be explicitly characterized and easily controlled. Inference is performed using Markov chain Monte Carlo and sequential Monte Carlo methods. We demonstrate our models and algorithms on epidemiological and video tracking data",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v18/15-397.html": {
    "title": "Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture Models",
    "abstract": "This paper presents an original Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler introduced in Andrieu et al. (2009, 2010). The resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio and can be implemented using existing sequential Monte Carlo libraries. We investigate its performance experimentally on synthetic problems as well as on geolocation data. Our results show that for a given computational budget, the Particle Gibbs Split-Merge sampler empirically outperforms existing split merge methods. The code and instructions allowing to reproduce the experiments is available at github.com/aroth85/pgsm",
    "volume": "main",
    "checked": true,
    "id": "b9267c0214fcb6e5aacafe93eff1ad76f1fd56a6",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/15-613.html": {
    "title": "Certifiably Optimal Low Rank Factor Analysis",
    "abstract": "Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics, and econometrics. In this paper, we revisit the classical rank-constrained FA problem which seeks to approximate an observed covariance matrix ($\\B\\Sigma$) by the sum of a Positive Semidefinite (PSD) low-rank component ($\\B\\Theta$) and a diagonal matrix ($\\B\\Phi$) (with nonnegative entries) subject to $\\B\\Sigma - \\B\\Phi$ being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex, compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are certifiably optimal in many cases, even for problems with thousands of variables. Our techniques are general and make no assumption on the underlying problem data. The estimator proposed herein aids statistical interpretability and provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in global and discrete optimization",
    "volume": "main",
    "checked": true,
    "id": "2523b796d50720685258144eb66585855c3eb018",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v18/15-651.html": {
    "title": "Group Sparse Optimization via lp,q Regularization",
    "abstract": "In this paper, we investigate a group sparse optimization problem via $\\ell_{p,q}$ regularization in three aspects: theory, algorithm and application. In the theoretical aspect, by introducing a notion of group restricted eigenvalue condition, we establish an oracle property and a global recovery bound of order $\\mathcal{O}(\\lambda^\\frac{2}{2-q})$ for any point in a level set of the $\\ell_{p,q}$ regularization problem, and by virtue of modern variational analysis techniques, we also provide a local analysis of recovery bound of order $\\mathcal{O}(\\lambda^2)$ for a path of local minima. In the algorithmic aspect, we apply the well-known proximal gradient method to solve the $\\ell_{p,q}$ regularization problems, either by analytically solving some specific $\\ell_{p,q}$ regularization subproblems, or by using the Newton method to solve general $\\ell_{p,q}$ regularization subproblems. In particular, we establish a local linear convergence rate of the proximal gradient method for solving the $\\ell_{1,q}$ regularization problem under some mild conditions and by first proving a second-order growth condition. As a consequence, the local linear convergence rate of proximal gradient method for solving the usual $\\ell_{q}$ regularization problem ($0<q<1$) is obtained. Finally in the aspect of application, we present some numerical results on both the simulated data and the real data in gene transcriptional regulation",
    "volume": "main",
    "checked": true,
    "id": "1db7e65053bb5a2e11659c0d851609e1ec8d6f0a",
    "citation_count": 57
  },
  "https://jmlr.org/papers/v18/16-460.html": {
    "title": "Preference-based Teaching",
    "abstract": "We introduce a new model of teaching named preference-based teaching and a corresponding complexity parameter---the preference-based teaching dimension (PBTD)---representing the worst-case number of examples needed to teach any concept in a given concept class. Although the PBTD coincides with the well- known recursive teaching dimension (RTD) on finite classes, it is radically different on infinite ones: the RTD becomes infinite already for trivial infinite classes (such as half- intervals) whereas the PBTD evaluates to reasonably small values for a wide collection of infinite classes including classes consisting of so-called closed sets w.r.t. a given closure operator, including various classes related to linear sets over $\\mathbb{N}_0$ (whose RTD had been studied quite recently) and including the class of Euclidean half-spaces. On top of presenting these concrete results, we provide the reader with a theoretical framework (of a combinatorial flavor) which helps to derive bounds on the PBTD",
    "volume": "main",
    "checked": true,
    "id": "39043c793428b36b53451f89bc885d91bdf5cf29",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/13-336.html": {
    "title": "Nonparametric Risk Bounds for Time-Series Forecasting",
    "abstract": "We derive generalization error bounds for traditional time- series forecasting models. Our results hold for many standard forecasting tools including autoregressive models, moving average models, and, more generally, linear state-space models. These non-asymptotic bounds need only weak assumptions on the data-generating process, yet allow forecasters to select among competing models and to guarantee, with high probability, that their chosen model will perform well. We motivate our techniques with and apply them to standard economic and financial forecasting tools---a GARCH model for predicting equity volatility and a dynamic stochastic general equilibrium model (DSGE), the standard tool in macroeconomic forecasting. We demonstrate in particular how our techniques can aid forecasters and policy makers in choosing models which behave well under uncertainty and mis-specification",
    "volume": "main",
    "checked": true,
    "id": "11699a49dfe5c469267cfa63ae31e7fe176c7d52",
    "citation_count": 26
  },
  "https://jmlr.org/papers/v18/14-188.html": {
    "title": "Online Bayesian Passive-Aggressive Learning",
    "abstract": "We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive- Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms",
    "volume": "main",
    "checked": true,
    "id": "3e4d9583480e21dee3cdd8b5c587ca63af194a7b",
    "citation_count": 37
  },
  "https://jmlr.org/papers/v18/15-104.html": {
    "title": "Asymptotic Analysis of Objectives Based on Fisher Information in Active Learning",
    "abstract": "Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for a more efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective",
    "volume": "main",
    "checked": true,
    "id": "4cdcbc5f8ff509904b432eed361af1f1be714a7c",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v18/15-468.html": {
    "title": "A Spectral Algorithm for Inference in Hidden semi-Markov Models",
    "abstract": "Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large data sets",
    "volume": "main",
    "checked": true,
    "id": "aa3899d2c1d13aaf9670e89cd478a84c8e467274",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/16-166.html": {
    "title": "Simplifying Probabilistic Expressions in Causal Inference",
    "abstract": "Obtaining a non-parametric expression for an interventional distribution is one of the most fundamental tasks in causal inference. Such an expression can be obtained for an identifiable causal effect by an algorithm or by manual application of do-calculus. Often we are left with a complicated expression which can lead to biased or inefficient estimates when missing data or measurement errors are involved. We present an automatic simplification algorithm that seeks to eliminate symbolically unnecessary variables from these expressions by taking advantage of the structure of the underlying graphical model. Our method is applicable to all causal effect formulas and is readily available in the R package causaleffect",
    "volume": "main",
    "checked": true,
    "id": "191d99fb17e4b0e70893fae9f51e1a8d246e4904",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v18/16-217.html": {
    "title": "Nearly optimal classification for semimetrics",
    "abstract": "We initiate the rigorous study of classification in semimetric spaces, which are point sets with a distance function that is non-negative and symmetric, but need not satisfy the triangle inequality. We define the density dimension dens and discover that it plays a central role in the statistical and algorithmic feasibility of learning in semimetric spaces. We compute this quantity for several widely used semimetrics and present nearly optimal sample compression algorithms, which are then used to obtain generalization guarantees, including fast rates. Our claim of near-optimality holds in both computational and statistical senses. When the sample has radius $R$ and margin $\\gamma$, we show that it can be compressed down to roughly $d=(R/\\gamma)^{\\text{dens}}$ points, and further that finding a significantly better compression is algorithmically intractable unless P=NP. This compression implies generalization via standard Occam-type arguments, to which we provide a nearly matching lower bound",
    "volume": "main",
    "checked": true,
    "id": "a96c125f91def81a16ab0c76fd9dceaf7849fa5e",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/16-223.html": {
    "title": "Bridging Supervised Learning and Test-Based Co-optimization",
    "abstract": "This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of co-optimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches",
    "volume": "main",
    "checked": true,
    "id": "cf90f4d719742c9f8974f1603e5329d8b0c65db5",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/16-509.html": {
    "title": "GFA: Exploratory Analysis of Multiple Data Sources with Group Factor Analysis",
    "abstract": "The R package GFA provides a full pipeline for factor analysis of multiple data sources that are represented as matrices with co-occurring samples. It allows learning dependencies between subsets of the data sources, decomposed into latent factors. The package also implements sparse priors for the factorization, providing interpretable biclusters of the multi-source data",
    "volume": "main",
    "checked": true,
    "id": "626b1a48e0ee7f5714693c5dea3fe9f5eab07cd7",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v18/16-537.html": {
    "title": "GPflow: A Gaussian Process Library using TensorFlow",
    "abstract": "GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware",
    "volume": "main",
    "checked": true,
    "id": "c4be5b937c819d1432c5343d1ea07a5269a0380d",
    "citation_count": 532
  },
  "https://jmlr.org/papers/v18/16-132.html": {
    "title": "COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Evolution",
    "abstract": "",
    "volume": "main",
    "checked": false,
    "id": "d4872aebf6f8e8644d93f55a6aef042de18d61a6",
    "citation_count": 209
  },
  "https://jmlr.org/papers/v18/16-198.html": {
    "title": "Learning Local Dependence In Ordered Data",
    "abstract": "In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. Our method is also applied to improve the performance of discriminant analysis in sound recording classification",
    "volume": "main",
    "checked": true,
    "id": "29365331f6a56938bdcd2abd8e186d127a709af0",
    "citation_count": 29
  },
  "https://jmlr.org/papers/v18/16-391.html": {
    "title": "Bayesian Learning of Dynamic Multilayer Networks",
    "abstract": "A plethora of networks is being collected in a growing number of fields, including disease transmission, international relations, social interactions, and others. As data streams continue to grow, the complexity associated with these highly multidimensional connectivity data presents novel challenges. In this paper, we focus on the time-varying interconnections among a set of actors in multiple contexts, called layers. Current literature lacks flexible statistical models for dynamic multilayer networks, which can enhance quality in inference and prediction by efficiently borrowing information within each network, across time, and between layers. Motivated by this gap, we develop a Bayesian nonparametric model leveraging latent space representations. Our formulation characterizes the edge probabilities as a function of shared and layer-specific actors positions in a latent space, with these positions changing in time via Gaussian processes. This representation facilitates dimensionality reduction and incorporates different sources of information in the observed data. In addition, we obtain tractable procedures for posterior computation, inference, and prediction. We provide theoretical results on the flexibility of our model. Our methods are tested on simulations and infection studies monitoring dynamic face-to-face contacts among individuals in multiple days, where we perform better than current methods in inference and prediction",
    "volume": "main",
    "checked": true,
    "id": "727898f95fbdc494e7e02b0406e33cecda39dc7d",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/16-538.html": {
    "title": "Time-Accuracy Tradeoffs in Kernel Prediction: Controlling Prediction Quality",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f68f682f9bf329dbba8c228989162ac7b4855e2c",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v18/16-564.html": {
    "title": "Asymptotic behavior of Support Vector Machine for spiked population model",
    "abstract": "For spiked population model, we investigate the large dimension $N$ and large sample size $M$ asymptotic behavior of the Support Vector Machine (SVM) classification method in the limit of $N,M\\rightarrow\\infty$ at fixed $\\alpha=M/N$. We focus on the generalization performance by analytically evaluating the angle between the normal direction vectors of SVM separating hyperplane and corresponding Bayes optimal separating hyperplane. This is an analogous result to the one shown in Paul (2007) and Nadler (2008) for the angle between the sample eigenvector and the population eigenvector in random matrix theorem. We provide not just bound, but sharp prediction of the asymptotic behavior of SVM that can be determined by a set of nonlinear equations. Based on the analytical results, we propose a new method of selecting tuning parameter which significantly reduces the computational cost. A surprising finding is that SVM achieves its best performance at small value of the tuning parameter under spiked population model. These results are confirmed to be correct by comparing with those of numerical simulations on finite-size systems. We also apply our formulas to an actual dataset of breast cancer and find agreement between analytical derivations and numerical computations based on cross validation",
    "volume": "main",
    "checked": true,
    "id": "6eef85cd80123e770b8304ae08957f59ff22e05a",
    "citation_count": 30
  },
  "https://jmlr.org/papers/v18/16-601.html": {
    "title": "Distributed Semi-supervised Learning with Kernel Ridge Regression",
    "abstract": "This paper provides error analysis for distributed semi- supervised learning with kernel ridge regression (DSKRR) based on a divide-and-conquer strategy. DSKRR applies kernel ridge regression (KRR) to data subsets that are distributively stored on multiple servers to produce individual output functions, and then takes a weighted average of the individual output functions as a final estimator. Using a novel error decomposition which divides the generalization error of DSKRR into the approximation error, sample error and distributed error, we find that the sample error and distributed error reflect the power and limitation of DSKRR, compared with KRR processing the whole data. Thus a small distributed error provides a large range of the number of data subsets to guarantee a small generalization error. Our results show that unlabeled data play important roles in reducing the distributed error and enlarging the number of data subsets in DSKRR. Our analysis also applies to the case when the regression function is out of the reproducing kernel Hilbert space. Numerical experiments including toy simulations and a music-prediction task are employed to demonstrate our theoretical statements and show the power of unlabeled data in distributed learning",
    "volume": "main",
    "checked": true,
    "id": "d2e0340b2949eacfe89e3c899cb8da0815b61ed0",
    "citation_count": 83
  },
  "https://jmlr.org/papers/v18/15-205.html": {
    "title": "On Markov chain Monte Carlo methods for tall data",
    "abstract": "Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number $n$ of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis- Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach relying on a control variate method which samples under regularity conditions from a distribution provably close to the posterior distribution of interest, yet can require less than $O(n)$ data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we emphasize that we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor",
    "volume": "main",
    "checked": true,
    "id": "e01a2082464b3598962ed697e7b6942a0f8477f0",
    "citation_count": 239
  },
  "https://jmlr.org/papers/v18/15-240.html": {
    "title": "Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers",
    "abstract": "There is a large literature explaining why AdaBoost is a successful classifier. The literature on AdaBoost focuses on classifier margins and boosting's interpretation as the optimization of an exponential likelihood function. These existing explanations, however, have been pointed out to be incomplete. A random forest is another popular ensemble method for which there is substantially less explanation in the literature. We introduce a novel perspective on AdaBoost and random forests that proposes that the two algorithms work for similar reasons. While both classifiers achieve similar predictive accuracy, random forests cannot be conceived as a direct optimization procedure. Rather, random forests is a self- averaging, interpolating algorithm which creates what we denote as a spiked-smooth classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and random forests succeed because of this mechanism. We provide a number of examples to support this explanation. In the process, we question the conventional wisdom that suggests that boosting algorithms for classification require regularization or early stopping and should be limited to low complexity classes of learners, such as decision stumps. We conclude that boosting should be used like random forests: with large decision trees, without regularization or early stopping",
    "volume": "main",
    "checked": true,
    "id": "ce6879fbd6cab2688cad1f530a75992ec184d0b9",
    "citation_count": 212
  },
  "https://jmlr.org/papers/v18/15-659.html": {
    "title": "Clustering from General Pairwise Observations with Applications to Time-varying Graphs",
    "abstract": "We present a general framework for graph clustering and bi- clustering where we are given a general observation (called a label) between each pair of nodes. This framework allows a rich encoding of various types of pairwise interactions between nodes. We propose a new tractable and robust approach to this problem based on convex optimization and maximum likelihood estimators. We analyze our algorithms under a general statistical model extending the planted partition and stochastic block models. Both sufficient and necessary conditions are provided for successful recovery of the underlying clusters. Our theoretical results subsume many existing graph clustering results for a wide range of settings, including planted partition, weighted clustering, submatrix localization and partially observed graphs. Furthermore, our results are applicable to novel settings including time-varying graphs, providing new insights to solutions of these problems. We provide empirical results on both synthetic and real data that corroborate with our theoretical findings",
    "volume": "main",
    "checked": true,
    "id": "53ecda4c0acae30d780358e52748030149f34822",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v18/16-100.html": {
    "title": "Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques",
    "abstract": "In a series of recent works, we have generalised the consistency results in the stochastic block model literature to the case of uniform and non-uniform hypergraphs. The present paper continues the same line of study, where we focus on partitioning weighted uniform hypergraphs---a problem often encountered in computer vision. This work is motivated by two issues that arise when a hypergraph partitioning approach is used to tackle computer vision problems: (i) The uniform hypergraphs constructed for higher-order learning contain all edges, but most have negligible weights. Thus, the adjacency tensor is nearly sparse, and yet, not binary. (ii) A more serious concern is that standard partitioning algorithms need to compute all edge weights, which is computationally expensive for hypergraphs. This is usually resolved in practice by merging the clustering algorithm with a tensor sampling strategy---an approach that is yet to be analysed rigorously. We build on our earlier work on partitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati, ICML, 2015), and address the aforementioned issues by proposing provable and efficient partitioning algorithms. Our analysis justifies the empirical success of practical sampling techniques. We also complement our theoretical findings by elaborate empirical comparison of various hypergraph partitioning schemes",
    "volume": "main",
    "checked": true,
    "id": "be849fbd09b263d2c5db0ab9a113002b2ef1d5d8",
    "citation_count": 41
  },
  "https://jmlr.org/papers/v18/16-146.html": {
    "title": "Reconstructing Undirected Graphs from Eigenspaces",
    "abstract": "We aim at recovering the weighted adjacency matrix $\\mathsf{W}$ of an undirected graph from a perturbed version of its eigenspaces. This situation arises for instance when working with stationary signals on graphs or Markov chains observed at random times. Our approach relies on minimizing a cost function based on the Frobenius norm of the commutator $\\mathsf{A} \\mathsf{B}-\\mathsf{B} \\mathsf{A}$ between symmetric matrices $\\mathsf{A}$ and $\\mathsf{B}$. We describe a particular framework in which we have access to an estimation of the eigenspaces and provide support selection procedures from theoretical and practical points of view. In the ErdÃÂs-RÃÂ©nyi model on $N$ vertices with no self-loops, we show that identifiability (i.e., the ability to reconstruct $\\mathsf{W}$ from the knowledge of its eigenspaces) follows a sharp phase transition on the expected number of edges with threshold function $N\\log N/2$. Simulated and real life numerical experiments assert our methodology",
    "volume": "main",
    "checked": true,
    "id": "89b3458006f056a840ea4f4960c6eab0968999dd",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/16-632.html": {
    "title": "An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback",
    "abstract": "We consider the closely related problems of bandit convex optimization with two-point feedback, and zero-order stochastic convex optimization with two function evaluations per round. We provide a simple algorithm and analysis which is optimal for convex Lipschitz functions. This improves on Duchi et al. (2015), which only provides an optimal result for smooth functions; Moreover, the algorithm and analysis are simpler, and readily extend to non-Euclidean problems. The algorithm is based on a small but surprisingly powerful modification of the gradient estimator",
    "volume": "main",
    "checked": true,
    "id": "436445b4cf3fe25e5778b5838eb623accad428e5",
    "citation_count": 183
  },
  "https://jmlr.org/papers/v18/17-061.html": {
    "title": "Perishability of Data: Dynamic Pricing under Varying-Coefficient Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "403594304795a58284fcb3c22e125bc8cae8236b",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/14-453.html": {
    "title": "Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "10ad60495b140ccb0b2c6ed39f8cfd8b8a25a94d",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/15-495.html": {
    "title": "On the Consistency of Ordinal Regression Methods",
    "abstract": "Many of the ordinal regression models that have been proposed in the literature can be seen as methods that minimize a convex surrogate of the zero-one, absolute, or squared loss functions. A key property that allows to study the statistical implications of such approximations is that of Fisher consistency. Fisher consistency is a desirable property for surrogate loss functions and implies that in the population setting, i.e., if the probability distribution that generates the data were available, then optimization of the surrogate would yield the best possible model. In this paper we will characterize the Fisher consistency of a rich family of surrogate loss functions used in the context of ordinal regression, including support vector ordinal regression, ORBoosting and least absolute deviation. We will see that, for a family of surrogate loss functions that subsumes support vector ordinal regression and ORBoosting, consistency can be fully characterized by the derivative of a real-valued function at zero, as happens for convex margin-based surrogates in binary classification. We also derive excess risk bounds for a surrogate of the absolute error that generalize existing risk bounds for binary classification. Finally, our analysis suggests a novel surrogate of the squared error loss. We compare this novel surrogate with competing approaches on 9 different datasets. Our method shows to be highly competitive in practice, outperforming the least squares loss on 7 out of 9 datasets",
    "volume": "main",
    "checked": true,
    "id": "24def5b14643ab40ba1789e86c24516dd6d8a8fa",
    "citation_count": 52
  },
  "https://jmlr.org/papers/v18/15-596.html": {
    "title": "Statistical Inference with Unnormalized Discrete Models and Localized Homogeneous Divergences",
    "abstract": "In this paper, we focus on parameters estimation of probabilistic models in discrete space. A naive calculation of the normalization constant of the probabilistic model on discrete space is often infeasible and statistical inference based on such probabilistic models has difficulty. In this paper, we propose a novel estimator for probabilistic models on discrete space, which is derived from an empirically localized homogeneous divergence. The idea of the empirical localization makes it possible to ignore an unobserved domain on sample space, and the homogeneous divergence is a discrepancy measure between two positive measures and has a weak coincidence axiom. The proposed estimator can be constructed without calculating the normalization constant and is asymptotically consistent and Fisher efficient. We investigate statistical properties of the proposed estimator and reveal a relationship between the empirically localized homogeneous divergence and a mixture of the $\\alpha$-divergence. The $\\alpha$-divergence is a non- homogeneous discrepancy measure that is frequently discussed in the context of information geometry. Using the relationship, we also propose an asymptotically consistent estimator of the normalization constant. Experiments showed that the proposed estimator comparably performs to the maximum likelihood estimator but with drastically lower computational cost",
    "volume": "main",
    "checked": true,
    "id": "1e2215a0eb75dc38b0126838f3234175090bdc78",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/16-011.html": {
    "title": "Density Estimation in Infinite Dimensional Exponential Families",
    "abstract": "In this paper, we consider an infinite dimensional exponential family $\\mathcal{P}$ of probability densities, which are parametrized by functions in a reproducing kernel Hilbert space $\\mathcal{H}$, and show it to be quite rich in the sense that a broad class of densities on $\\mathbb{R}^d$ can be approximated arbitrarily well in Kullback-Leibler (KL) divergence by elements in $\\mathcal{P}$. Motivated by this approximation property, the paper addresses the question of estimating an unknown density $p_0$ through an element in $\\mathcal{P}$. Standard techniques like maximum likelihood estimation (MLE) or pseudo MLE (based on the method of sieves), which are based on minimizing the KL divergence between $p_0$ and $\\mathcal{P}$, do not yield practically useful estimators because of their inability to efficiently handle the log-partition function. We propose an estimator $\\hat{p}_n$ based on minimizing the Fisher divergence, $J(p_0\\Vert p)$ between $p_0$ and $p\\in \\mathcal{P}$, which involves solving a simple finite-dimensional linear system. When $p_0\\in\\mathcal{P}$, we show that the proposed estimator is consistent, and provide a convergence rate of $n^{-\\min\\left\\{\\frac{2}{3},\\frac{2\\beta+1}{2\\beta+2}\\right\\}}$ in Fisher divergence under the smoothness assumption that $\\log p_0\\in\\mathcal{R}(C^\\beta)$ for some $\\beta\\ge 0$, where $C$ is a certain Hilbert-Schmidt operator on $\\mathcal{H}$ and $\\mathcal{R}(C^\\beta)$ denotes the image of $C^\\beta$. We also investigate the misspecified case of $p_0\\notin\\mathcal{P}$ and show that $J(p_0\\Vert\\hat{p}_n)\\rightarrow \\inf_{p\\in\\mathcal{P}}J(p_0\\Vert p)$ as $n\\rightarrow \\infty$, and provide a rate for this convergence under a similar smoothness condition as above. Through numerical simulations we demonstrate that the proposed estimator outperforms the non- parametric kernel density estimator, and that the advantage of the proposed estimator grows as $d$ increases",
    "volume": "main",
    "checked": true,
    "id": "21fafb9789b4c9ddfc324044868109b17419c914",
    "citation_count": 113
  },
  "https://jmlr.org/papers/v18/16-061.html": {
    "title": "Lens Depth Function and k-Relative Neighborhood Graph: Versatile Tools for Ordinal Data Analysis",
    "abstract": "In recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as $d(A,B)<d(C,D)$. For many problems in machine learning and statistics it is unclear how to solve them in such a scenario. Up to now, the main approach is to explicitly construct an ordinal embedding of the data points in the Euclidean space, an approach that has a number of drawbacks. In this paper, we propose algorithms for the problems of medoid estimation, outlier identification, classification, and clustering when given only ordinal data. They are based on estimating the lens depth function and the $k$-relative neighborhood graph on a data set. Our algorithms are simple, are much faster than an ordinal embedding approach and avoid some of its drawbacks, and can easily be parallelized",
    "volume": "main",
    "checked": true,
    "id": "5cb510ea5ab90fc34fb11d74cc23b248dd16515d",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v18/16-214.html": {
    "title": "Joint Label Inference in Networks",
    "abstract": "We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers for people connected by a social network; by predicting these user profile fields, the network can provide a better experience to its users. Existing approaches such as Label Propagation (Zhu et al., 2003) fail to consider interactions between the label types. Our proposed method, called EDGEEXPLAIN explicitly models these interactions, while still allowing scalable inference under a distributed message- passing architecture. On a large subset of the Facebook social network, collected in a previous study (Chakrabarti et al., 2014), EDGEEXPLAIN outperforms label propagation for several label types, with lifts of up to $120\\%$ for recall@1 and $60\\%$ for recall@3",
    "volume": "main",
    "checked": true,
    "id": "1b85d4eaf476a69ca838967bebacfd433d5d5be3",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/16-245.html": {
    "title": "Achieving Optimal Misclassification Proportion in Stochastic Block Models",
    "abstract": "Community detection is a fundamental statistical problem in network data analysis.  In this paper, we present a polynomial time two-stage method that provably achieves optimal statistical performance in misclassification proportion for stochastic block model under weak regularity conditions. Our two-stage procedure consists of a refinement stage motivated by penalized local maximum likelihood estimation. This stage can take a wide range of weakly consistent community detection procedures as its initializer, to which it applies and outputs a community assignment that achieves optimal misclassification proportion with high probability. The theoretical property is confirmed by simulated examples",
    "volume": "main",
    "checked": true,
    "id": "60845819aeedbadda293b6098a8d712f86860981",
    "citation_count": 220
  },
  "https://jmlr.org/papers/v18/16-497.html": {
    "title": "On the Propagation of Low-Rate Measurement Error to Subgraph Counts in Large Networks",
    "abstract": "Our work in this paper is inspired by a statistical observation that is both elementary and broadly relevant to network analysis in practice---that the uncertainty in approximating some true graph $G=(V,E)$ by some estimated graph $\\hat{G}=(V,\\hat{E})$ manifests as errors in our knowledge of the presence/absence of edges between vertex pairs, which must necessarily propagate to any estimates of network summaries $\\eta(G)$ we seek. Motivated by the common practice of using plug-in estimates $\\eta(\\hat{G})$ as proxies for $\\eta(G)$, our focus is on the problem of characterizing the distribution of the discrepancy $D=\\eta(\\hat{G}) - \\eta(G)$, in the case where $\\eta(\\cdot)$ is a subgraph count. Specifically, we study the fundamental case where the statistic of interest is $|E|$, the number of edges in $G$. Our primary contribution in this paper is to show that in the empirically relevant setting of large graphs with low-rate measurement errors, the distribution of $D_E=|\\hat{E}| - |E|$ is well-characterized by a Skellam distribution, when the errors are independent or weakly dependent. Under an assumption of independent errors, we are able to further show conditions under which this characterization is strictly better than that of an appropriate normal distribution. These results derive from our formulation of a general result, quantifying the accuracy with which the difference of two sums of dependent Bernoulli random variables may be approximated by the difference of two independent Poisson random variables, i.e., by a Skellam distribution. This general result is developed through the use of Stein's method, and may be of some general interest. We finish with a discussion of possible extension of our work to subgraph counts $\\eta(G)$ of higher order",
    "volume": "main",
    "checked": true,
    "id": "7dbe30a7e7dad2551c751fefb12d734856684ee5",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v18/16-526.html": {
    "title": "Dense Distributions from Sparse Samples: Improved Gibbs Sampling Parameter Estimators for LDA",
    "abstract": "We introduce a novel approach for estimating Latent Dirichlet Allocation (LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single additional collapsed Gibbs sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. Our estimators can straightforwardly be applied to the output of any existing implementation of CGS, including modern accelerated variants. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multi-label classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so",
    "volume": "main",
    "checked": true,
    "id": "e77cbb9c8ae3a35bcef724b16bc43d5d561550aa",
    "citation_count": 31
  },
  "https://jmlr.org/papers/v18/17-189.html": {
    "title": "Fundamental Conditions for Low-CP-Rank Tensor Completion",
    "abstract": "We consider the problem of low canonical polyadic (CP) rank tensor completion. A completion is a tensor whose entries agree with the observed entries and its rank matches the given CP rank. We analyze the manifold structure corresponding to the tensors with the given rank and define a set of polynomials based on the sampling pattern and CP decomposition. Then, we show that finite completability of the sampled tensor is equivalent to having a certain number of algebraically independent polynomials among the defined polynomials. Our proposed approach results in characterizing the maximum number of algebraically independent polynomials in terms of a simple geometric structure of the sampling pattern, and therefore we obtain the deterministic necessary and sufficient condition on the sampling pattern for finite completability of the sampled tensor. Moreover, assuming that the entries of the tensor are sampled independently with probability $p$ and using the mentioned deterministic analysis, we propose a combinatorial method to derive a lower bound on the sampling probability $p$, or equivalently, the number of sampled entries that guarantees finite completability with high probability. We also show that the existing result for the matrix completion problem can be used to obtain a loose lower bound on the sampling probability $p$. In addition, we obtain deterministic and probabilistic conditions for unique completability. It is seen that the number of samples required for finite or unique completability obtained by the proposed analysis on the CP manifold is orders-of- magnitude lower than that is obtained by the existing analysis on the Grassmannian manifold",
    "volume": "main",
    "checked": true,
    "id": "c897cee9b439e66408faa514d3c69b88051d3de6",
    "citation_count": 43
  },
  "https://jmlr.org/papers/v18/14-317.html": {
    "title": "Parallel Symmetric Class Expression Learning",
    "abstract": "In machine learning, one often encounters data sets where a general pattern is violated by a relatively small number of exceptions (for example, a rule that says that all birds can fly is violated by examples such as penguins). This complicates the concept learning process and may lead to the rejection of some simple and expressive rules that cover many cases. In this paper we present an approach to this problem in description logic learning by computing partial descriptions (which are not necessarily entirely complete) of both positive and negative examples and combining them. Our Symmetric Parallel Class Expression Learning approach enables the generation of general rules with exception patterns included. We demonstrate that this algorithm provides significantly better results (in terms of metrics such as accuracy, search space covered, and learning time) than standard approaches on some typical data sets. Further, the approach has the added benefit that it can be parallelised relatively simply, leading to much faster exploration of the search tree on modern computers",
    "volume": "main",
    "checked": true,
    "id": "9f2e72969b0a795d349c90ba6542217afa7202b6",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/15-251.html": {
    "title": "Learning Partial Policies to Speedup MDP Tree Search via Reduction to I.I.D. Learning",
    "abstract": "A popular approach for online decision-making in large MDPs is time-bounded tree search. The effectiveness of tree search, however, is largely influenced by the action branching factor, which limits the search depth given a time bound. An obvious way to reduce action branching is to consider only a subset of potentially good actions at each state as specified by a provided partial policy. In this work, we consider offline learning of such partial policies with the goal of speeding up search without significantly reducing decision-making quality. Our first contribution consists of reducing the learning problem to set learning. We give a reduction-style analysis of three such algorithms, each making different assumptions, which relates the set learning objectives to the sub-optimality of search using the learned partial policies. Our second contribution is to describe concrete implementations of the algorithms within the popular framework of Monte-Carlo tree search. Finally, the third contribution is to evaluate the learning algorithms on two challenging MDPs with large action branching factors. The results show that the learned partial policies can significantly improve the anytime performance of Monte-Carlo tree search",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v18/15-376.html": {
    "title": "Hierarchically Compositional Kernels for Scalable Nonparametric Learning",
    "abstract": "We propose a novel class of kernels to alleviate the high computational cost of large-scale nonparametric learning with kernel methods. The proposed kernel is defined based on a hierarchical partitioning of the underlying data domain, where the NystrÃÂ¶m method (a globally low-rank approximation) is married with a locally lossless approximation in a hierarchical fashion. The kernel maintains (strict) positive-definiteness. The corresponding kernel matrix admits a recursively off- diagonal low-rank structure, which allows for fast linear algebra computations. Suppressing the factor of data dimension, the memory and arithmetic complexities for training a regression or a classifier are reduced from $O(n^2)$ and $O(n^3)$ to $O(nr)$ and $O(nr^2)$, respectively, where $n$ is the number of training examples and $r$ is the rank on each level of the hierarchy. Although other randomized approximate kernels entail a similar complexity, empirical results show that the proposed kernel achieves a matching performance with a smaller $r$. We demonstrate comprehensive experiments to show the effective use of the proposed kernel on data sizes up to the order of millions",
    "volume": "main",
    "checked": true,
    "id": "581ace3120b3ef3ae67496723f773117bf781260",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/15-482.html": {
    "title": "Sharp Oracle Inequalities for Square Root Regularization",
    "abstract": "We study a set of regularization methods for high-dimensional linear regression models. These penalized estimators have the square root of the residual sum of squared errors as loss function, and any weakly decomposable norm as penalty function. This fit measure is chosen because of its property that the estimator does not depend on the unknown standard deviation of the noise. On the other hand, a generalized weakly decomposable norm penalty is very useful in being able to deal with different underlying sparsity structures. We can choose a different sparsity inducing norm depending on how we want to interpret the unknown parameter vector $\\beta$. Structured sparsity norms, as defined in Micchelli et al. (2010), are special cases of weakly decomposable norms, therefore we also include the square root LASSO (Belloni et al., 2011), the group square root LASSO (Bunea et al., 2014) and a new method called the square root SLOPE (in a similar fashion to the SLOPE from Bogdan et al. 2015). For this collection of estimators our results provide sharp oracle inequalities with the Karush-Kuhn-Tucker conditions. We discuss some examples of estimators. Based on a simulation we illustrate some advantages of the square root SLOPE",
    "volume": "main",
    "checked": true,
    "id": "c0b7cf9b918ffbf7a990610235b0f5f82e7800df",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/15-566.html": {
    "title": "Soft Margin Support Vector Classification as Buffered Probability Minimization",
    "abstract": "In this paper, we show that the popular C-SVM, soft-margin support vector classifier is equivalent to minimization of Buffered Probability of Exceedance (bPOE), a recently introduced characterization of uncertainty. To show this, we introduce a new SVM formulation, called the EC-SVM, which is derived from a simple bPOE minimization problem that is easy to interpret with a meaningful free parameter, optimal objective value, and probabilistic derivation. Over the range of its free parameter, the EC-SVM has both a convex and non-convex case which we connect to existing SVM formulations. We first show that the C-SVM, formulated with any regularization norm, is equivalent to the convex EC-SVM. Similarly, we show that the E$\\nu$-SVM is equivalent to the EC-SVM over its entire parameter range, which includes both the convex and non-convex case. These equivalences, coupled with the interpretability of the EC-SVM, allow us to gain surprising new insights into the C-SVM and fully connect soft margin support vector classification with superquantile and bPOE concepts. We also show that the EC-SVM can easily be cast as a robust optimization problem, where bPOE is minimized with data lying in a fixed uncertainty set. This reformulation allows us to clearly differentiate between the convex and non-convex case, with convexity associated with pessimistic views of uncertainty and non-convexity associated with optimistic views of uncertainty. Finally, we address some practical considerations. First, we show that these new insights can assist in making parameter selection more efficient. Second, we discuss optimization approaches for solving the EC-SVM. Third, we address the issue of generalization, providing generalization bounds for both bPOE and misclassification rate",
    "volume": "main",
    "checked": true,
    "id": "308467c72d9e3b812d76d9bec16edcaca5586231",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v18/15-615.html": {
    "title": "Variational Particle Approximations",
    "abstract": "Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search- based techniques. DPVI is based on a novel family of particle- based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well- defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper defines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can offer appealing time/accuracy trade-offs as compared to multiple alternatives",
    "volume": "main",
    "checked": true,
    "id": "c3048b130ed9a362e40bba0a66971663db1fec73",
    "citation_count": 54
  },
  "https://jmlr.org/papers/v18/16-003.html": {
    "title": "A Bayesian Framework for Learning Rule Sets for Interpretable Classification",
    "abstract": "We present a machine learning algorithm for building classifiers that are comprised of a small number of short rules. These are restricted disjunctive normal form models. An example of a classifier of this form is as follows: If $X$ satisfies (condition $A$ AND condition $B$) OR (condition $C$) OR $\\cdots$, then $Y=1$. Models of this form have the advantage of being interpretable to human experts since they produce a set of rules that concisely describe a specific class. We present two probabilistic models with prior parameters that the user can set to encourage the model to have a desired size and shape, to conform with a domain-specific definition of interpretability. We provide a scalable MAP inference approach and develop theoretical bounds to reduce computation by iteratively pruning the search space. We apply our method (Bayesian Rule Sets -- BRS) to characterize and predict user behavior with respect to in-vehicle context-aware personalized recommender systems. Our method has a major advantage over classical associative classification methods and decision trees in that it does not greedily grow the model",
    "volume": "main",
    "checked": true,
    "id": "8bd4850cc2ac1a328deaeb97bcc76fb43284d50a",
    "citation_count": 168
  },
  "https://jmlr.org/papers/v18/16-079.html": {
    "title": "A Robust-Equitable Measure for Feature Ranking and Selection",
    "abstract": "In many applications, not all the features used to represent data samples are important. Often only a few features are relevant for the prediction task. The choice of dependence measures often affect the final result of many feature selection methods. To select features that have complex nonlinear relationships with the response variable, the dependence measure should be equitable, a concept proposed by Reshef et al. (2011); that is, the dependence measure treats linear and nonlinear relationships equally. Recently, Kinney and Atwal (2014) gave a mathematical definition of self- equitability. In this paper, we introduce a new concept of robust-equitability and identify a robust- equitable copula dependence measure, the robust copula dependence (RCD) measure. RCD is based on the $L_1$-distance of the copula density from uniform and we show that it is equitable under both equitability definitions. We also prove theoretically that RCD is much easier to estimate than mutual information. Because of these theoretical properties, the RCD measure has the following advantages compared to existing dependence measures: it is robust to different relationship forms and robust to unequal sample sizes of different features. Experiments on both synthetic and real-world data sets confirm the theoretical analysis, and illustrate the advantage of using the dependence measure RCD for feature selection",
    "volume": "main",
    "checked": true,
    "id": "2d9072ed612c1cecbf7acae29342ad7dc0a6f7cc",
    "citation_count": 44
  },
  "https://jmlr.org/papers/v18/16-108.html": {
    "title": "Multiscale Strategies for Computing Optimal Transport",
    "abstract": "This paper presents a multiscale approach to efficiently compute approximate optimal transport plans between point sets. It is particularly well-suited for point sets that are in high- dimensions, but are close to being intrinsically low- dimensional. The approach is based on an adaptive multiscale decomposition of the point sets. The multiscale decomposition yields a sequence of optimal transport problems, that are solved in a top-to-bottom fashion from the coarsest to the finest scale. We provide numerical evidence that this multiscale approach scales approximately linearly, in time and memory, in the number of nodes, instead of quadratically or worse for a direct solution. Empirically, the multiscale approach results in less than one percent relative error in the objective function. Furthermore, the multiscale plans constructed are of interest by themselves as they may be used to introduce novel features and notions of distances between point sets. An analysis of sets of brain MRI based on optimal transport distances illustrates the effectiveness of the proposed method on a real world data set. The application demonstrates that multiscale optimal transport distances have the potential to improve on state-of-the-art metrics currently used in computational anatomy",
    "volume": "main",
    "checked": true,
    "id": "a01d9d9c5915bcfd3458b19120f01dbd44633e0d",
    "citation_count": 31
  },
  "https://jmlr.org/papers/v18/16-142.html": {
    "title": "Non-parametric Policy Search with Limited Information Loss",
    "abstract": "Learning complex control policies from non-linear and redundant sensory input is an important challenge for reinforcement learning algorithms. Non-parametric methods that approximate values functions or transition models can address this problem, by adapting to the complexity of the data set. Yet, many current non-parametric approaches rely on unstable greedy maximization of approximate value functions, which might lead to poor convergence or oscillations in the policy update. A more robust policy update can be obtained by limiting the information loss between successive state-action distributions. In this paper, we develop a policy search algorithm with policy updates that are both robust and non-parametric. Our method can learn non- parametric control policies for infinite horizon continuous Markov decision processes with non-linear and redundant sensory representations. We investigate how we can use approximations of the kernel function to reduce the time requirements of the demanding non-parametric computations. In our experiments, we show the strong performance of the proposed method, and how it can be approximated efficiently. Finally, we show that our algorithm can learn a real-robot under-powered swing-up task directly from image data",
    "volume": "main",
    "checked": true,
    "id": "f325b087b0a1a3995ea559aac42cd87c2a796b9a",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/16-184.html": {
    "title": "Tests of Mutual or Serial Independence of Random Vectors with Applications",
    "abstract": "The problem of testing mutual independence between many random vectors is addressed. The closely related problem of testing serial independence of a multivariate stationary sequence is also considered. The MÃÂ¶bius transformation of characteristic functions is used to characterize independence. A generalization to $p$ vectors of distance covariance and Hilbert-Schmidt independence criterion ($HSIC$) tests with the translation invariant kernel of a stable probability distribution is proposed. Both test statistics can be expressed in a simple form as a sum over all elements of a componentwise product of $p$ doubly-centered matrices. It is shown that an $HSIC$ statistic with sufficiently small scale parameters is equivalent to a distance covariance statistic. Consistency and weak convergence of both types of statistics are established. Approximation of $p$-values is made by randomization tests without recomputing interpoint distances for each randomized sample. The dependogram is adapted to the proposed tests for the graphical identification of sources of dependencies. Empirical rejection rates obtained through extensive simulations confirm both the applicability of the testing procedures in small samples and the high level of competitiveness in terms of power. Applications to meteorological and financial data provide some interesting interpretations of dependencies revealed by dependograms",
    "volume": "main",
    "checked": true,
    "id": "f681220b3a123be2f27423e3aec5490bbe269d2d",
    "citation_count": 13
  },
  "https://jmlr.org/papers/v18/16-258.html": {
    "title": "Recovering PCA and Sparse PCA via Hybrid-(l1,l2) Sparse Sampling of Data Elements",
    "abstract": "This paper addresses how well we can recover a data matrix when only given a few of its elements. We present a randomized algorithm that element-wise sparsifies the data, retaining only a few of its entries. Our new algorithm independently samples the data using probabilities that depend on both squares ($\\ell_2$ sampling) and absolute values ($\\ell_1$ sampling) of the entries. We prove that this hybrid algorithm ($i$) achieves a near-PCA reconstruction of the data, and ($ii$) recovers sparse principal components of the data, from a sketch formed by a sublinear sample size. Hybrid-($\\ell_1,\\ell_2$) inherits the $\\ell_2$-ability to sample the important elements, as well as the regularization properties of $\\ell_1$ sampling, and maintains strictly better quality than either $\\ell_1$ or $\\ell_2$ on their own. Extensive experimental results on synthetic, image, text, biological, and financial data show that not only are we able to recover PCA and sparse PCA from incomplete data, but we can speed up such computations significantly using our sparse sketch",
    "volume": "main",
    "checked": false,
    "id": "1c435d538ebcaf2361a89c3f77c568c5ec4eff1c",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v18/16-296.html": {
    "title": "Quantifying the Informativeness of Similarity Measurements",
    "abstract": "In this paper, we describe an unsupervised measure for quantifying the 'informativeness' of correlation matrices formed from the pairwise similarities or relationships among data instances. The measure quantifies the heterogeneity of the correlations and is defined as the distance between a correlation matrix and the nearest correlation matrix with constant off-diagonal entries. This non-parametric notion generalizes existing test statistics for equality of correlation coefficients by allowing for alternative distance metrics, such as the Bures and other distances from quantum information theory. For several distance and dissimilarity metrics, we derive closed-form expressions of informativeness, which can be applied as objective functions for machine learning applications. Empirically, we demonstrate that informativeness is a useful criterion for selecting kernel parameters, choosing the dimension for kernel-based nonlinear dimensionality reduction, and identifying structured graphs. We also consider the problem of finding a maximally informative correlation matrix around a target matrix, and explore parameterizing the optimization in terms of the coordinates of the sample or through a lower-dimensional embedding. In the latter case, we find that maximizing the Bures-based informativeness measure, which is maximal for centered rank-1 correlation matrices, is equivalent to minimizing a specific matrix norm, and present an algorithm to solve the minimization problem using the norm's proximal operator. The proposed correlation denoising algorithm consistently improves spectral clustering. Overall, we find informativeness to be a novel and useful criterion for identifying non-trivial correlation structure",
    "volume": "main",
    "checked": true,
    "id": "be6f7ff9724de39139a9a0bcb1cf73874fc7b5c0",
    "citation_count": 13
  },
  "https://jmlr.org/papers/v18/16-305.html": {
    "title": "Time for a Change: a Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis",
    "abstract": "The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better---more sound and useful--- alternatives for it",
    "volume": "main",
    "checked": true,
    "id": "8ce2c4a374e8b37e3eef080c956f22cfc6ea25d6",
    "citation_count": 304
  },
  "https://jmlr.org/papers/v18/16-326.html": {
    "title": "Relational Reinforcement Learning for Planning with Exogenous Effects",
    "abstract": "Probabilistic planners have improved recently to the point that they can solve difficult tasks with complex and expressive models. In contrast, learners cannot tackle yet the expressive models that planners do, which forces complex models to be mostly handcrafted. We propose a new learning approach that can learn relational probabilistic models with both action effects and exogenous effects. The proposed learning approach combines a multi-valued variant of inductive logic programming for the generation of candidate models, with an optimization method to select the best set of planning operators to model a problem. We also show how to combine this learner with reinforcement learning algorithms to solve complete problems. Finally, experimental validation is provided that shows improvements over previous work in both simulation and a robotic task. The robotic task involves a dynamic scenario with several agents where a manipulator robot has to clear the tableware on a table. We show that the exogenous effects learned by our approach allowed the robot to clear the table in a more efficient way",
    "volume": "main",
    "checked": true,
    "id": "9f06375a01ffbdf1244e5217fdc8fb3d7b41cffb",
    "citation_count": 70
  },
  "https://jmlr.org/papers/v18/16-362.html": {
    "title": "Bayesian Tensor Regression",
    "abstract": "We propose a Bayesian approach to regression with a scalar response on vector and tensor covariates. Vectorization of the tensor prior to analysis fails to exploit the structure, often leading to poor estimation and predictive performance. We introduce a novel class of multiway shrinkage priors for tensor coefficients in the regression setting and present posterior consistency results under mild conditions. A computationally efficient Markov chain Monte Carlo algorithm is developed for posterior computation. Simulation studies illustrate substantial gains over existing tensor regression methods in terms of estimation and parameter inference. Our approach is further illustrated in a neuroimaging application",
    "volume": "main",
    "checked": true,
    "id": "03e2ba1bc02d230661adfcda9ed2953fb2d7f741",
    "citation_count": 87
  },
  "https://jmlr.org/papers/v18/16-429.html": {
    "title": "Robust Discriminative Clustering with Sparse Regularizers",
    "abstract": "Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from noise-looking variables. We cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem; (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions; (c) we propose a natural extension for the multi-label scenario; (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form $d=O(\\sqrt{n})$ for the affine invariant case and $d=O(n)$ for the sparse case, where $n$ is the number of examples and $d$ the ambient dimension; and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to $O(nd^2)$, improving on earlier algorithms for discriminative clustering with the square loss, which had quadratic complexity in the number of examples",
    "volume": "main",
    "checked": true,
    "id": "df66473d58b0ab0aa5decea55e684fd94438b502",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/16-466.html": {
    "title": "Making Decision Trees Feasible in Ultrahigh Feature and Label Dimensions",
    "abstract": "Due to the non-linear but highly interpretable representations, decision tree (DT) models have significantly attracted a lot of attention of researchers. However, it is difficult to understand and interpret DT models in ultrahigh dimensions and DT models usually suffer from the curse of dimensionality and achieve degenerated performance when there are many noisy features. To address these issues, this paper first presents a novel data- dependent generalization error bound for the perceptron decision tree (PDT), which provides the theoretical justification to learn a sparse linear hyperplane in each decision node and to prune the tree. Following our analysis, we introduce the notion of budget-aware classifier (BAC) with a budget constraint on the weight coefficients, and propose a supervised budgeted tree (SBT) algorithm to achieve non-linear prediction performance. To avoid generating an unstable and complicated decision tree and improve the generalization of the SBT, we present a pruning strategy by learning classifiers to minimize cross-validation errors on each BAC. To deal with ultrahigh label dimensions, based on three important phenomena of real-world data sets from a variety of application domains, we develop a sparse coding tree framework for multi-label annotation problems and provide the theoretical analysis. Extensive empirical studies verify that 1) SBT is easy to understand and interpret in ultrahigh dimensions and is more resilient to noisy features. 2) Compared with state-of-the-art algorithms, our proposed sparse coding tree framework is more efficient, yet accurate in ultrahigh label and feature dimensions",
    "volume": "main",
    "checked": true,
    "id": "70c2701361ff5e39ca5250069a1ac8dda4d4123f",
    "citation_count": 65
  },
  "https://jmlr.org/papers/v18/16-498.html": {
    "title": "Learning Scalable Deep Kernels with Recurrent Structure",
    "abstract": "Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure, and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP- LSTM are uniquely valuable",
    "volume": "main",
    "checked": true,
    "id": "0f71a2b6e26a0b97364d221d55ccfd1956fcf0f2",
    "citation_count": 98
  },
  "https://jmlr.org/papers/v18/16-505.html": {
    "title": "Convolutional Neural Networks Analyzed via Convolutional Sparse Coding",
    "abstract": "Convolutional neural networks (CNN) have led to many state-of- the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional and recurrent networks, and also has better theoretical guarantees",
    "volume": "main",
    "checked": true,
    "id": "2ccdebc1ebfd103f6c97cd31059936e2e7c48dc1",
    "citation_count": 246
  },
  "https://jmlr.org/papers/v18/16-568.html": {
    "title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization",
    "abstract": "We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate (SPDC) method, which alternates between maximizing over a randomly chosen dual variable and minimizing over the primal variables. An extrapolation step on the primal variables is performed to obtain accelerated convergence rate. We also develop a mini-batch version of the SPDC method which facilitates parallel computing, and an extension with weighted sampling probabilities on the dual variables, which has a better complexity than uniform sampling on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods",
    "volume": "main",
    "checked": true,
    "id": "75359c49a6abdd5fba642f46ec44812ed8e8a648",
    "citation_count": 250
  },
  "https://jmlr.org/papers/v18/17-003.html": {
    "title": "Angle-based Multicategory Distance-weighted SVM",
    "abstract": "Classification is an important supervised learning technique with numerous applications. We develop an angle-based multicategory distance-weighted support vector machine (MDWSVM) classification method that is motivated from the binary distance-weighted support vector machine (DWSVM) classification method. The new method has the merits of both support vector machine (SVM) and distance-weighted discrimination (DWD) but also alleviates both the data piling issue of SVM and the imbalanced data issue of DWD. Theoretical and numerical studies demonstrate the advantages of MDWSVM method over existing angle-based methods",
    "volume": "main",
    "checked": true,
    "id": "a877b837fbb0942dd3eccc3c1a00d3bc2af16e52",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/17-032.html": {
    "title": "Minimax Estimation of Kernel Mean Embeddings",
    "abstract": "In this paper, we study the minimax estimation of the Bochner integral \\[ \\mu_k(P) := \\int_\\mathcal{X} k(\\cdot,x)\\, dP(x), \\] also called the kernel mean embedding, based on random samples drawn i.i.d. from $P$, where $k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow \\mathbb{R}$ is a positive definite kernel.  Various estimators (including the empirical estimator), $\\hat{\\theta}_n$ of $\\mu_k(P)$ are studied in the literature wherein all of them satisfy $\\|\\hat{\\theta}_n-\\mu_k(P)\\|_{\\mathcal{H}_k}=O_P(n^{-1/2})$ with $\\mathcal{H}_k$ being the reproducing kernel Hilbert space induced by $k$.  The main contribution of the paper is in showing that the above mentioned rate of $n^{-1/2}$ is minimax in $\\|\\cdot\\|_{\\mathcal{H}_k}$ and $\\|\\cdot\\|_{L^2(\\mathbb{R}^d)}$-norms over the class of discrete measures and the class of measures that has an infinitely differentiable density, with $k$ being a continuous translation- invariant kernel on $\\mathbb{R}^d$. The interesting aspect of this result is that the minimax rate is independent of the smoothness of the kernel and the density of $P$ (if it exists)",
    "volume": "main",
    "checked": true,
    "id": "1911e2685d9b471203f53da4d437c9adcf406cac",
    "citation_count": 67
  },
  "https://jmlr.org/papers/v18/17-039.html": {
    "title": "The Impact of Random Models on Clustering Similarity",
    "abstract": "Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, multiple runs of K-means clustering reurns clusterings with a fixed number of clusters, while the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on the ranking of similar clustering pairs, and the evaluation of a clustering method with respect to a random baseline; thus, the choice of random clustering model should be carefully justified",
    "volume": "main",
    "checked": true,
    "id": "9e115c09286b51b20a40e59bffd5d7c7f063ecfe",
    "citation_count": 93
  },
  "https://jmlr.org/papers/v18/17-081.html": {
    "title": "Hierarchical Clustering via Spreading Metrics",
    "abstract": "We study the cost function for hierarchical clusterings introduced by (Dasgupta, 2016) where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in (Dasgupta, 2016) that a top-down algorithm based on the uniform Sparsest Cut problem returns a hierarchical clustering of cost at most $O\\left(\\alpha_n \\log n\\right)$ times the cost of the optimal hierarchical clustering, where $\\alpha_n$ is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao- Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most $O\\left(\\log^{3/2} n\\right)$ times the cost of the optimal solution. We improve this by giving an $O(\\log{n})$-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an $O(\\log{n})$- approximate hierarchical clustering for a generalization of this cost function also studied in (Dasgupta, 2016). Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the Small Set Expansion hypothesis",
    "volume": "main",
    "checked": true,
    "id": "24bdbbfa99c2d951e7d684100cd5b2f70a7a8b56",
    "citation_count": 66
  },
  "https://jmlr.org/papers/v18/17-156.html": {
    "title": "The MADP Toolbox: An Open Source Library for Planning and Learning in (Multi-)Agent Systems",
    "abstract": "This article describes the Multiagent Decision Process (MADP) Toolbox, a software library to support planning and learning for intelligent agents and multiagent systems in uncertain environments. Key features are that it supports partially observable environments and stochastic transition models; has unified support for single- and multiagent systems; provides a large number of models for decision-theoretic decision making, including one-shot and sequential decision making under various assumptions of observability and cooperation, such as Dec-POMDPs and POSGs; provides tools and parsers to quickly prototype new problems; provides an extensive range of planning and learning algorithms for single- and multiagent systems; is released under the GNU GPL v3 license; and is written in C++ and designed to be extensible via the object-oriented paradigm",
    "volume": "main",
    "checked": true,
    "id": "6801be1bcc57e454f0109008d674f081361cfe6c",
    "citation_count": 49
  },
  "https://jmlr.org/papers/v18/14-428.html": {
    "title": "A survey of Algorithms and Analysis for Adaptive Online Learning",
    "abstract": "We present tools for the analysis of Follow-The-Regularized- Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, prox-function or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens previously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between adaptive Mirror Descent algorithms and a corresponding FTRL update, which allows us to analyze Mirror Descent algorithms in the same framework. The key to bridging the gap between Dural Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non- smooth regularizers with time-varying weight",
    "volume": "main",
    "checked": true,
    "id": "b86524dd0e2eba0f1b6e56bd2b1c0b0fcd28d60b",
    "citation_count": 143
  },
  "https://jmlr.org/papers/v18/14-484.html": {
    "title": "A distributed block coordinate descent method for training l1 regularized linear classifiers",
    "abstract": "Distributed training of $l_1$ regularized classifiers has received great attention recently. Most existing methods approach this problem by taking steps obtained from approximating the objective by a quadratic approximation that is decoupled at the individual variable level. These methods are designed for multicore systems where communication costs are low. They are inefficient on systems such as Hadoop running on a cluster of commodity machines where communication costs are substantial. In this paper we design a distributed algorithm for $l_1$ regularization that is much better suited for such systems than existing algorithms. A careful cost analysis is used to support these points and motivate our method. The main idea of our algorithm is to do block optimization of many variables on the actual objective function within each computing node; this increases the computational cost per step that is matched with the communication cost, and decreases the number of outer iterations, thus yielding a faster overall method. Distributed Gauss-Seidel and Gauss-Southwell greedy schemes are used for choosing variables to update in each step. We establish global convergence theory for our algorithm, including Q-linear rate of convergence. Experiments on two benchmark problems show our method to be much faster than existing methods",
    "volume": "main",
    "checked": false,
    "id": "e6a37d32a48faf943d485c3da05f428c90eb1e4d",
    "citation_count": 33
  },
  "https://jmlr.org/papers/v18/15-586.html": {
    "title": "Distributed Learning with Regularized Least Squares",
    "abstract": "We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds and learning rates in expectation in both the $L^2$-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our derived learning rates in expectation are optimal and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in expectation in the literature",
    "volume": "main",
    "checked": true,
    "id": "c1a46d9a0972fb0ec0977d24a191f612e7401369",
    "citation_count": 157
  },
  "https://jmlr.org/papers/v18/15-650.html": {
    "title": "Identifying Unreliable and Adversarial Workers in Crowdsourced Labeling Tasks",
    "abstract": "We study the problem of identifying unreliable and adversarial workers in crowdsourcing systems where workers (or users) provide labels for tasks (or items). Most existing studies assume that worker responses follow specific probabilistic models; however, recent evidence shows the presence of workers adopting non-random or even malicious strategies. To account for such workers, we suppose that workers comprise a mixture of honest and adversarial workers. Honest workers may be reliable or unreliable, and they provide labels according to an unknown but explicit probabilistic model. Adversaries adopt labeling strategies different from those of honest workers, whether probabilistic or not. We propose two reputation algorithms to identify unreliable honest workers and adversarial workers from only their responses. Our algorithms assume that honest workers are in the majority, and they classify workers with outlier label patterns as adversaries. Theoretically, we show that our algorithms successfully identify unreliable honest workers, workers adopting deterministic strategies, and worst- case sophisticated adversaries who can adopt arbitrary labeling strategies to degrade the accuracy of the inferred task labels. Empirically, we show that filtering out outliers using our algorithms can significantly improve the accuracy of several state-of-the-art label aggregation algorithms in real-world crowdsourcing datasets",
    "volume": "main",
    "checked": true,
    "id": "3233a971dc5fee45568f63f6b6a1cac1d1ada66f",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/16-212.html": {
    "title": "An Easy-to-hard Learning Paradigm for Multiple Classes and Multiple Labels",
    "abstract": "Many applications, such as human action recognition and object detection, can be formulated as a multiclass classification problem. One-vs-rest (OVR) is one of the most widely used approaches for multiclass classification due to its simplicity and excellent performance. However, many confusing classes in such applications will degrade its results. For example, hand clap and boxing are two confusing actions. Hand clap is easily misclassified as boxing, and vice versa. Therefore, precisely classifying confusing classes remains a challenging task. To obtain better performance for multiclass classifications that have confusing classes, we first develop a classifier chain model for multiclass classification (CCMC) to transfer class information between classifiers. Then, based on an analysis of our proposed model, we propose an easy- to-hard learning paradigm for multiclass classification to automatically identify easy and hard classes and then use the predictions from simpler classes to help solve harder classes. Similar to CCMC, the classifier chain (CC) model is also proposed by Read et al. (2009) to capture the label dependency for multi-label classification. However, CC does not consider the order of difficulty of the labels and achieves degenerated performance when there are many confusing labels. Therefore, it is non- trivial to learn the appropriate label order for CC. Motivated by our analysis for CCMC, we also propose the easy-to-hard learning paradigm for multi-label classification to automatically identify easy and hard labels, and then use the predictions from simpler labels to help solve harder labels. We also demonstrate that our proposed strategy can be successfully applied to a wide range of applications, such as ordinal classification and relationship prediction. Extensive empirical studies validate our analysis and the effectiveness of our proposed easy-to-hard learning strategies",
    "volume": "main",
    "checked": true,
    "id": "d602ddda1727cc41394ac71f9d418786788d6931",
    "citation_count": 87
  },
  "https://jmlr.org/papers/v18/17-048.html": {
    "title": "Fisher Consistency for Prior Probability Shift",
    "abstract": "We introduce Fisher consistency in the sense of unbiasedness as a desirable property for estimators of class prior probabilities. Lack of Fisher consistency could be used as a criterion to dismiss estimators that are unlikely to deliver precise estimates in test data sets under prior probability and more general data set shift. The usefulness of this unbiasedness concept is demonstrated with three examples of classifiers used for quantification: Adjusted Count, EM-algorithm and CDE- Iterate. We find that Adjusted Count and EM-algorithm are Fisher consistent. A counter-example shows that CDE-Iterate is not Fisher consistent and, therefore, cannot be trusted to deliver reliable estimates of class probabilities",
    "volume": "main",
    "checked": true,
    "id": "0cda0e817717478b58c7b6b222f8344d8ff8ac8b",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v18/17-113.html": {
    "title": "openXBOW -- Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit",
    "abstract": "We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e.g., acoustic or visual descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed sub-bags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool have been exemplified in different scenarios: sentiment analysis in tweets, classification of snore sounds, and time-dependent emotion recognition based on acoustic, linguistic, and visual information, where improved results over other feature representations were observed",
    "volume": "main",
    "checked": false,
    "id": "16cfa5ab8025bccca5c6bd07b62f7716d6926ade",
    "citation_count": 159
  },
  "https://jmlr.org/papers/v18/17-176.html": {
    "title": "Optimal Rates for Multi-pass Stochastic Gradient Methods",
    "abstract": "We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. We study how regularization properties are controlled by the step-size, the number of passes and the mini-batch size. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases. As a byproduct, we derive optimal convergence results for batch gradient methods (even in the non-attainable cases)",
    "volume": "main",
    "checked": true,
    "id": "f053f86502b6d9f025f2dc957c756155536241e4",
    "citation_count": 59
  },
  "https://jmlr.org/papers/v18/17-375.html": {
    "title": "Rank Determination for Low-Rank Data Completion",
    "abstract": "Recently, fundamental conditions on the sampling patterns have been obtained for finite completability of low-rank matrices or tensors given the corresponding ranks. In this paper, we consider the scenario where the rank is not given and we aim to approximate the unknown rank based on the location of sampled entries and some given completion. We consider a number of data models, including single-view matrix, multi-view matrix, CP tensor, tensor-train tensor and Tucker tensor. For each of these data models, we provide an upper bound on the rank when an arbitrary low-rank completion is given. We characterize these bounds both deterministically, i.e., with probability one given that the sampling pattern satisfies certain combinatorial properties, and probabilistically, i.e., with high probability given that the sampling probability is above some threshold. Moreover, for both single-view matrix and CP tensor, we are able to show that the obtained upper bound is exactly equal to the unknown rank if the lowest-rank completion is given. Furthermore, we provide numerical experiments for the case of single-view matrix, where we use nuclear norm minimization to find a low-rank completion of the sampled data and we observe that in most of the cases the proposed upper bound on the rank is equal to the true rank",
    "volume": "main",
    "checked": true,
    "id": "256720dfa089f3cef74621f2df5373771bcda15c",
    "citation_count": 21
  },
  "https://jmlr.org/papers/v18/17-033.html": {
    "title": "Bayesian Network Learning via Topological Order",
    "abstract": "We propose a mixed integer programming (MIP) model and iterative algorithms based on topological orders to solve optimization problems with acyclic constraints on a directed graph. The proposed MIP model has a significantly lower number of constraints compared to popular MIP models based on cycle elimination constraints and triangular inequalities. The proposed iterative algorithms use gradient descent and iterative reordering approaches, respectively, for searching topological orders. A computational experiment is presented for the Gaussian Bayesian network learning problem, an optimization problem minimizing the sum of squared errors of regression models with L1 penalty over a feature network with application of gene network inference in bioinformatics",
    "volume": "main",
    "checked": true,
    "id": "ad9be2c7a8a9952d28243b4758192e96ffbb08ae",
    "citation_count": 21
  },
  "https://jmlr.org/papers/v18/16-590.html": {
    "title": "Stability of Controllers for Gaussian Process Dynamics",
    "abstract": "Learning control has become an appealing alternative to the derivation of control laws based on classic control theory. However, a major shortcoming of learning control is the lack of performance guarantees which prevents its application in many real-world scenarios. As a step towards widespread deployment of learning control, we provide stability analysis tools for controllers acting on dynamics represented by Gaussian processes (GPs). We consider differentiable Markovian control policies and system dynamics given as (i) the mean of a GP, and (ii) the full GP distribution. For both cases, we analyze finite and infinite time horizons. Furthermore, we study the effect of disturbances on the stability results. Empirical evaluations on simulated benchmark problems support our theoretical results",
    "volume": "main",
    "checked": true,
    "id": "2ab73b9498c9b3dac7819d9744c9ca1c88c774f0",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/16-335.html": {
    "title": "Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression",
    "abstract": "We consider the optimization of a quadratic objective function whose gradients are only accessible through a stochastic oracle that returns the gradient at any given point plus a zero-mean finite variance random error. We present the first algorithm that achieves jointly the optimal prediction error rates for least-squares regression, both in terms of forgetting the initial conditions in $O(1/n^2)$, and in terms of dependence on the noise and dimension $d$ of the problem, as $O(d/n)$. Our new algorithm is based on averaged accelerated regularized gradient descent, and may also be analyzed through finer assumptions on initial conditions and the Hessian matrix, leading to dimension- free quantities that may still be small in some distances while the Ã¢ÂÂoptimalÃ¢ÂÂ terms above are large. In order to characterize the tightness of these new bounds, we consider an application to non-parametric regression and use the known lower bounds on the statistical performance (without computational limits), which happen to match our bounds obtained from a single pass on the data and thus show optimality of our algorithm in a wide variety of particular trade-offs between bias and variance",
    "volume": "main",
    "checked": true,
    "id": "9ffc2368a492669f1c330e3d787c04878d9c8ab7",
    "citation_count": 187
  },
  "https://jmlr.org/papers/v18/16-596.html": {
    "title": "Confidence Sets with Expected Sizes for Multiclass Classification",
    "abstract": "Multiclass classification problems such as image annotation can involve a large number of classes. In this context, confusion between classes can occur, and single label classification may be misleading. We provide in the present paper a general device that, given an unlabeled dataset and a score function defined as the minimizer of some empirical and convex risk, outputs a set of class labels, instead of a single one. Interestingly, this procedure does not require that the unlabeled dataset explores the whole classes. Even more, the method is calibrated to control the expected size of the output set while minimizing the classification risk. We show the statistical optimality of the procedure and establish rates of convergence under the Tsybakov margin condition. It turns out that these rates are linear on the number of labels. We apply our methodology to convex aggregation of confidence sets based on the $V$-fold cross validation principle also known as the superlearning principle (van der Laan et al., 2007). We illustrate the numerical performance of the procedure on real data and demonstrate in particular that with moderate expected size, w.r.t. the number of labels, the procedure provides significant improvement of the classification risk",
    "volume": "main",
    "checked": true,
    "id": "5e715aa7f7154405bf797589cdc5b965c083e696",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v18/16-285.html": {
    "title": "Online Learning to Rank with Top-k Feedback",
    "abstract": "We consider two settings of online learning to rank where feedback is restricted to top ranked items. The problem is cast as an online game between a learner and sequence of users, over $T$ rounds. In both settings, the learners objective is to present ranked list of items to the users. The learner's performance is judged on the entire ranked list and true relevances of the items. However, the learner receives highly restricted feedback at end of each round, in form of relevances of only the top $k$ ranked items, where $k \\ll m$. The first setting is non-contextual, where the list of items to be ranked is fixed. The second setting is contextual, where lists of items vary, in form of traditional query-document lists. No stochastic assumption is made on the generation process of relevances of items and contexts. We provide efficient ranking strategies for both the settings. The strategies achieve $O(T^{2/3})$ regret, where regret is based on popular ranking measures in first setting and ranking surrogates in second setting. We also provide impossibility results for certain ranking measures and a certain class of surrogates, when feedback is restricted to the top ranked item, i.e. $k=1$. We empirically demonstrate the performance of our algorithms on simulated and real world data sets",
    "volume": "main",
    "checked": true,
    "id": "e19ca0c780ef6862fc53396d4912de092e582e48",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/16-603.html": {
    "title": "A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation",
    "abstract": "Gaussian processes (GPs) are flexible distributions over functions that enable high-level assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free- energy, EP and Power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way all of the approximation is performed at `inference time' rather than at `modelling time', resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks",
    "volume": "main",
    "checked": true,
    "id": "9e333f1918a7363494d1142ade0d4ef47437030d",
    "citation_count": 122
  },
  "https://jmlr.org/papers/v18/16-504.html": {
    "title": "Accelerating Stochastic Composition Optimization",
    "abstract": "We consider the stochastic nested composition optimization problem where the objective is a composition of two expected- value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method. This algorithm updates the solution based on noisy gradient queries using a two-timescale iteration. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments",
    "volume": "main",
    "checked": true,
    "id": "d53d179dfba4804799278fcefa414c2017f391dc",
    "citation_count": 119
  },
  "https://jmlr.org/papers/v18/16-478.html": {
    "title": "Distributed Bayesian Learning with Stochastic Natural Gradient Expectation Propagation and the Posterior Server",
    "abstract": "This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a data set is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Monte Carlo sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks",
    "volume": "main",
    "checked": true,
    "id": "56d355e49d96d2d1873af5834b9378e2a411e360",
    "citation_count": 63
  },
  "https://jmlr.org/papers/v18/16-046.html": {
    "title": "Optimal Dictionary for Least Squares Representation",
    "abstract": "Dictionaries are collections of vectors used for the representation of a class of vectors in Euclidean spaces. Recent research on optimal dictionaries is focused on constructing dictionaries that offer sparse representations, i.e., $\\ell_0$-optimal representations. Here we consider the problem of finding optimal dictionaries with which representations of a given class of vectors is optimal in an $\\ell_2$-sense: optimality of representation is defined as attaining the minimal average $\\ell_2$-norm of the coefficients used to represent the vectors in the given class. With the help of recent results on rank-1 decompositions of symmetric positive semidefinite matrices, we provide an explicit description of $\\ell_2$-optimal dictionaries as well as their algorithmic constructions in polynomial time",
    "volume": "main",
    "checked": true,
    "id": "158f24e318639c1686859f8a4a7bc2ec596f4939",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v18/16-289.html": {
    "title": "Computational Limits of A Distributed Algorithm for Smoothing Spline",
    "abstract": "In this paper, we explore statistical versus computational trade-off to address a basic question in the application of a distributed algorithm: what is the minimal computational cost in obtaining statistical optimality? In smoothing spline setup, we observe a phase transition phenomenon for the number of deployed machines that ends up being a simple proxy for computing cost. Specifically, a sharp upper bound for the number of machines is established: when the number is below this bound, statistical optimality (in terms of nonparametric estimation or testing) is achievable; otherwise, statistical optimality becomes impossible. These sharp bounds partly capture intrinsic computational limits of the distributed algorithm considered in this paper, and turn out to be fully determined by the smoothness of the regression function. We name the asymptotic analysis on such split-and-aggregation estimation/inference as splitotic theory. As a side remark, we argue that sample splitting may be viewed as an alternative form of regularization, playing a similar role as smoothing parameter",
    "volume": "main",
    "checked": true,
    "id": "509ba8fc524d48f14821531617cdf26d55ac7f51",
    "citation_count": 47
  },
  "https://jmlr.org/papers/v18/15-631.html": {
    "title": "Hinge-Loss Markov Random Fields and Probabilistic Soft Logic",
    "abstract": "A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL- MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible",
    "volume": "main",
    "checked": false,
    "id": "550f7dba7757f22afd87786749509574609d6180",
    "citation_count": 351
  },
  "https://jmlr.org/papers/v18/16-342.html": {
    "title": "Clustering with Hidden Markov Model on Variable Blocks",
    "abstract": "Large-scale data containing multiple important rare clusters, even at moderately high dimensions, pose challenges for existing clustering methods. To address this issue, we propose a new mixture model called Hidden Markov Model on Variable Blocks (HMM-VB) and a new mode search algorithm called Modal Baum-Welch (MBW) for mode-association clustering. HMM-VB leverages prior information about chain-like dependence among groups of variables to achieve the effect of dimension reduction. In case such a dependence structure is unknown or assumed merely for the sake of parsimonious modeling, we develop a recursive search algorithm based on BIC to optimize the formation of ordered variable blocks. The MBW algorithm ensures the feasibility of clustering via mode association, achieving linear complexity in terms of the number of variable blocks despite the exponentially growing number of possible state sequences in HMM-VB. In addition, we provide theoretical investigations about the identifiability of HMM-VB as well as the consistency of our approach to search for the block partition of variables in a special case. Experiments on simulated and real data show that our proposed method outperforms other widely used methods",
    "volume": "main",
    "checked": true,
    "id": "ba606ac6a5fca8cd4899aa7adaeed0a61f9365f7",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v18/16-191.html": {
    "title": "Approximation Vector Machines for Large-scale Online Learning",
    "abstract": "One of the most challenging problems in kernel online learning is to bound the model size and to promote model sparsity. Sparse models not only improve computation and memory usage, but also enhance the generalization capacity -- a principle that concurs with the law of parsimony. However, inappropriate sparsity modeling may also significantly degrade the performance. In this paper, we propose Approximation Vector Machine (AVM), a model that can simultaneously encourage sparsity and safeguard its risk in compromising the performance. In an online setting context, when an incoming instance arrives, we approximate this instance by one of its neighbors whose distance to it is less than a predefined threshold. Our key intuition is that since the newly seen instance is expressed by its nearby neighbor the optimal performance can be analytically formulated and maintained. We develop theoretical foundations to support this intuition and further establish an analysis for the common loss functions including Hinge, smooth Hinge, and Logistic (i.e., for the classification task) and $\\ell_{1}$, $\\ell_{2}$, and $\\varepsilon$-insensitive (i.e., for the regression task) to characterize the gap between the approximation and optimal solutions. This gap crucially depends on two key factors including the frequency of approximation (i.e., how frequent the approximation operation takes place) and the predefined threshold. We conducted extensive experiments for classification and regression tasks in batch and online modes using several benchmark datasets. The quantitative results show that our proposed AVM obtained comparable predictive performances with current state-of-the-art methods while simultaneously achieving significant computational speed-up due to the ability of the proposed AVM in maintaining the model size",
    "volume": "main",
    "checked": true,
    "id": "129df08120e9d74bdc8648f876e9852175f22b5d",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v18/14-223.html": {
    "title": "Efficient Sampling from Time-Varying Log-Concave Distributions",
    "abstract": "We propose a computationally efficient random walk on a convex body which rapidly mixes with respect to a fixed log-concave distribution and closely tracks a time-varying log-concave distribution. We develop general theoretical guarantees on the required number of steps; this number can be calculated on the fly according to the distance from and the shape of the next distribution. We then illustrate the technique on several examples. Within the context of exponential families, the proposed method produces samples from a posterior distribution which is updated as data arrive in a streaming fashion. The sampling technique can be used to track time-varying truncated distributions, as well as to obtain samples from a changing mixture model, fitted in a streaming fashion to data. In the setting of linear optimization, the proposed method has oracle complexity with best known dependence on the dimension for certain geometries. In the context of online learning and repeated games, the algorithm is an efficient method for implementing no-regret mixture forecasting strategies. Remarkably, in some of these examples, only one step of the random walk is needed to track the next distribution",
    "volume": "main",
    "checked": true,
    "id": "60eecd70439ba37a26579a6cd132a5389e829937",
    "citation_count": 44
  },
  "https://jmlr.org/papers/v18/16-017.html": {
    "title": "Document Neural Autoregressive Distribution Estimation",
    "abstract": "We present an approach based on feed-forward neural networks for learning the distribution over textual documents. This approach is inspired by the Neural Autoregressive Distribution Estimator (NADE) model which has been shown to be a good estimator of the distribution over discrete-valued high-dimensional vectors. In this paper, we present how NADE can successfully be adapted to textual data, retaining the property that sampling or computing the probability of an observation can be done exactly and efficiently. The approach can also be used to learn deep representations of documents that are competitive to those learned by alternative topic modeling approaches. Finally, we describe how the approach can be combined with a regular neural network N-gram model and substantially improve its performance, by making its learned representation sensitive to the larger, document-level context",
    "volume": "main",
    "checked": true,
    "id": "5ea43195b341621da3642598749f91804e62ceb7",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/17-007.html": {
    "title": "Target Curricula via Selection of Minimum Feature Sets: a Case Study in Boolean Networks",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "7ef0e4c805ee61ae013f13b247ba8d58ca36cf81",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v18/16-463.html": {
    "title": "A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization",
    "abstract": "In modern large-scale machine learning applications, the training data are often partitioned and stored on multiple machines. It is customary to employ the data parallelism approach, where the aggregated training loss is minimized without moving data across machines. In this paper, we introduce a novel distributed dual formulation for regularized loss minimization problems that can directly handle data parallelism in the distributed setting. This formulation allows us to systematically derive dual coordinate optimization procedures, which we refer to as Distributed Alternating Dual Maximization (DADM). The framework extends earlier studies described in  (Boyd et al., 2011; Ma et al., 2017; Jaggi et al., 2014; Yang, 2013)  and has rigorous theoretical analyses. Moreover, with the help of the new formulation, we develop the accelerated version of DADM (Acc-DADM) by generalizing the acceleration technique from  (Shalev-Shwartz and Zhang, 2014) to the distributed setting. We also provide theoretical results for the proposed accelerated version, and the new result improves previous ones  (Yang, 2013; Ma et al., 2017) whose iteration complexities grow linearly on the condition number. Our empirical studies validate our theory and show that our accelerated approach significantly improves the previous state- of-the-art distributed dual coordinate optimization algorithms",
    "volume": "main",
    "checked": true,
    "id": "0e4ba334d7edadbea528055b43c2c3e3ffd5a5a4",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v18/16-491.html": {
    "title": "Second-Order Stochastic Optimization for Machine Learning in Linear Time",
    "abstract": "First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per- iteration cost of gradient based methods, and in certain settings improve upon the overall running time over popular first-order methods. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data",
    "volume": "main",
    "checked": true,
    "id": "f3ed7343727361a25e77fdef315850bdaf29d20e",
    "citation_count": 158
  },
  "https://jmlr.org/papers/v18/17-055.html": {
    "title": "Regularized Estimation and Testing for High-Dimensional Multi-Block Vector-Autoregressive Models",
    "abstract": "Dynamical systems comprising of multiple components that can be partitioned into distinct blocks originate in many scientific areas. A pertinent example is the interactions between financial assets and selected macroeconomic indicators, which has been studied at aggregate level---e.g. a stock index and an employment index---extensively in the macroeconomics literature. A key shortcoming of this approach is that it ignores potential influences from other related components (e.g. Gross Domestic Product) that may impact the system's dynamics and structure and thus produces incorrect results. To mitigate this issue, we consider a multi-block linear dynamical system with Granger-causal ordering between blocks, wherein the blocks' temporal dynamics are described by vector autoregressive processes and are influenced by blocks higher in the system hierarchy. We derive the maximum likelihood estimator for the posited model for Gaussian data in the high- dimensional setting based on appropriate regularization schemes for the parameters of the block components. To optimize the underlying non-convex likelihood function, we develop an iterative algorithm with convergence guarantees. We establish theoretical properties of the maximum likelihood estimates, leveraging the decomposability of the regularizers and a careful analysis of the iterates. Finally, we develop testing procedures for the null hypothesis of whether a block Granger-causes another block of variables. The performance of the model and the testing procedures are evaluated on synthetic data, and illustrated on a data set involving log-returns of the US S&P100 component stocks and key macroeconomic variables for the 2001--16 period",
    "volume": "main",
    "checked": true,
    "id": "04a78e3d452b2c5241e30d03afe265366fadacac",
    "citation_count": 37
  },
  "https://jmlr.org/papers/v18/17-423.html": {
    "title": "Learning Theory of Distributed Regression with Bias Corrected Regularization Kernel Network",
    "abstract": "Distributed learning is an effective way to analyze big data. In distributed regression, a typical approach is to divide the big data into multiple blocks, apply a base regression algorithm on each of them, and then simply average the output functions learnt from these blocks. Since the average process will decrease the variance, not the bias, bias correction is expected to improve the learning performance if the base regression algorithm is a biased one. Regularization kernel network is an effective and widely used method for nonlinear regression analysis. In this paper we will investigate a bias corrected version of regularization kernel network. We derive the error bounds when it is applied to a single data set and when it is applied as a base algorithm in distributed regression. We show that, under certain appropriate conditions, the optimal learning rates can be reached in both situations",
    "volume": "main",
    "checked": true,
    "id": "2b0afb1b62147b2767c781cf7a758a013c95ae05",
    "citation_count": 37
  },
  "https://jmlr.org/papers/v18/17-049.html": {
    "title": "Probabilistic Line Searches for Stochastic Optimization",
    "abstract": "In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user- controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent",
    "volume": "main",
    "checked": true,
    "id": "c94e0f79f030d142f9c2b9274cbeacaa019231b1",
    "citation_count": 122
  },
  "https://jmlr.org/papers/v18/17-014.html": {
    "title": "Learning Instrumental Variables with Structural and Non-Gaussianity Assumptions",
    "abstract": "Learning a causal effect from observational data requires strong assumptions. One possible method is to use instrumental variables, which are typically justified by background knowledge. It is possible, under further assumptions, to discover whether a variable is structurally instrumental to a target causal effect $X \\rightarrow Y$. However, the few existing approaches are lacking on how general these assumptions can be, and how to express possible equivalence classes of solutions. We present instrumental variable discovery methods that systematically characterize which set of causal effects can and cannot be discovered under local graphical criteria that define instrumental variables, without reconstructing full causal graphs. We also introduce the first methods to exploit non-Gaussianity assumptions, highlighting identifiability problems and solutions. Due to the difficulty of estimating such models from finite data, we investigate how to strengthen assumptions in order to make the statistical problem more manageable",
    "volume": "main",
    "checked": true,
    "id": "d603cc8ae3cfa9418499443f25034359fa8afd17",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v18/15-403.html": {
    "title": "Classification of Time Sequences using Graphs of Temporal Constraints",
    "abstract": "We introduce two algorithms that learn to classify Symbolic and Scalar Time Sequences (SSTS); an extension of multivariate time series. An SSTS is a set of \\emph{events} and a set of scalars. An event is defined by a symbol and a time-stamp. A scalar is defined by a symbol and a function mapping a number for each possible time stamp of the data. The proposed algorithms rely on temporal patterns called Graph of Temporal Constraints (GTC). A GTC is a directed graph in which vertices express occurrences of specific events, and edges express temporal constraints between occurrences of pairs of events. Additionally, each vertex of a GTC can be augmented with numeric constraints on scalar values. We allow GTCs to be cyclic and/or disconnected. The first of the introduced algorithms extracts sets of co-dependent GTCs to be used in a voting mechanism. The second algorithm builds decision forest like representations where each node is a GTC. In both algorithms, extraction of GTCs and model building are interleaved. Both algorithms are closely related to each other and they exhibit complementary properties including complexity, performance, and interpretability. The main novelties of this work reside in direct building of the model and efficient learning of GTC structures. We explain the proposed algorithms and evaluate their performance against a diverse collection of 59 benchmark data sets. In these experiments, our algorithms come across as highly competitive and in most cases closely match or outperform state-of-the-art alternatives in terms of the computational speed while dominating in terms of the accuracy of classification of time sequences",
    "volume": "main",
    "checked": true,
    "id": "b7891d44a1bcd11d5fea06d7d0f1ebadf8beb3be",
    "citation_count": 33
  },
  "https://jmlr.org/papers/v18/16-640.html": {
    "title": "Distributed Stochastic Variance Reduced Gradient Methods by Sampling Extra Data with Replacement",
    "abstract": "We study the round complexity of minimizing the average of convex functions under a new setting of distributed optimization where each machine can receive two subsets of functions. The first subset is from a random partition and the second subset is randomly sampled with replacement. Under this setting, we define a broad class of distributed algorithms whose local computation can utilize both subsets and design a distributed stochastic variance reduced gradient method belonging to in this class. When the condition number of the problem is small, our method achieves the optimal parallel runtime, amount of communication and rounds of communication among all distributed first-order methods up to constant factors. When the condition number is relatively large, a lower bound is provided for the number of rounds of communication needed by any algorithm in this class. Then, we present an accelerated version of our method whose the rounds of communication matches the lower bound up to logarithmic terms, which establishes that this accelerated algorithm has the lowest round complexity among all algorithms in our class under this new setting",
    "volume": "main",
    "checked": true,
    "id": "b8111ccd1a945887783115628c8862baa241608a",
    "citation_count": 61
  },
  "https://jmlr.org/papers/v18/17-306.html": {
    "title": "Kernel Partial Least Squares for Stationary Data",
    "abstract": "We consider the kernel partial least squares algorithm for non- parametric regression with stationary dependent data. Probabilistic convergence rates of the kernel partial least squares estimator to the true regression function are established under a source and an effective dimensionality condition. It is shown both theoretically and in simulations that long range dependence results in slower convergence rates. A protein dynamics example shows high predictive power of kernel partial least squares",
    "volume": "main",
    "checked": true,
    "id": "b025dd91ab89703557f1cfc98dd405065a405723",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v18/16-655.html": {
    "title": "Robust and Scalable Bayes via a Median of Subset Posterior Measures",
    "abstract": "We propose a novel approach to Bayesian analysis that is provably robust to outliers in the data and often has computational advantages over standard methods. Our technique is based on splitting the data into non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the resulting measures. The main novelty of our approach is the proposed aggregation step, which is based on the evaluation of a median in the space of probability measures equipped with a suitable collection of distances that can be quickly and efficiently evaluated in practice. We present both theoretical and numerical evidence illustrating the improvements achieved by our method",
    "volume": "main",
    "checked": true,
    "id": "f3e7f7d55495cca618a90055246defe4b22235d3",
    "citation_count": 106
  },
  "https://jmlr.org/papers/v18/16-093.html": {
    "title": "Statistical and Computational Guarantees for the Baum-Welch Algorithm",
    "abstract": "The Hidden Markov Model (HMM) is one of the mainstays of statistical modeling of discrete time series, with applications including speech recognition, computational biology, computer vision and econometrics. Estimating an HMM from its observation process is often addressed via the Baum-Welch algorithm, which is known to be susceptible to local optima. In this paper, we first give a general characterization of the basin of attraction associated with any global optimum of the population likelihood. By exploiting this characterization, we provide non-asymptotic finite sample guarantees on the Baum-Welch updates and show geometric convergence to a small ball of radius on the order of the minimax rate around a global optimum. As a concrete example, we prove a linear rate of convergence for a hidden Markov mixture of two isotropic Gaussians given a suitable mean separation and an initialization within a ball of large radius around (one of) the true parameters. To our knowledge, these are the first rigorous local convergence guarantees to global optima for the Baum-Welch algorithm in a setting where the likelihood function is nonconvex. We complement our theoretical results with thorough numerical simulations studying the convergence of the Baum-Welch algorithm and illustrating the accuracy of our predictions",
    "volume": "main",
    "checked": true,
    "id": "5b433f3304a9628153ed91669cf9a623636adc84",
    "citation_count": 33
  },
  "https://jmlr.org/papers/v18/16-374.html": {
    "title": "Online but Accurate Inference for Latent Variable Models with Local Gibbs Sampling",
    "abstract": "We study parameter inference in large-scale latent variable models. We first propose a unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. We then propose a novel inference method for the frequentist estimation of parameters, that adapts MCMC methods to online inference of latent variable models with the proper use of local Gibbs sampling. Then, for latent Dirichlet allocation,we provide an extensive set of experiments and comparisons with existing work, where our new approach outperforms all previously proposed methods. In particular, using Gibbs sampling for latent variable inference is superior to variational inference in terms of test log-likelihoods. Moreover, Bayesian inference through variational methods perform poorly, sometimes leading to worse fits with latent variables of higher dimensionality",
    "volume": "main",
    "checked": true,
    "id": "a335da949059107b737d0e61b7db0f8b354bfdb3",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v18/16-541.html": {
    "title": "Poisson Random Fields for Dynamic Feature Models",
    "abstract": "We present the Wright-Fisher Indian buffet process (WF- IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015",
    "volume": "main",
    "checked": true,
    "id": "bfca83a1791ab542ed94060330f95844e92966e0",
    "citation_count": 52
  },
  "https://jmlr.org/papers/v18/16-577.html": {
    "title": "Gap Safe Screening Rules for Sparsity Enforcing Penalties",
    "abstract": "In high dimensional regression settings, sparsity enforcing penalties have proved useful to regularize the data-fitting term. A recently introduced technique called screening rules propose to ignore some variables in the optimization leveraging the expected sparsity of the solutions and consequently leading to faster solvers. When the procedure is guaranteed not to discard variables wrongly the rules are said to be safe. In this work, we propose a unifying framework for generalized linear models regularized with standard sparsity enforcing penalties such as $\\ell_1$ or $\\ell_1/\\ell_2$ norms. Our technique allows to discard safely more variables than previously considered safe rules, particularly for low regularization parameters. Our proposed Gap Safe rules (so called because they rely on duality gap computation) can cope with any iterative solver but are particularly well suited to (block) coordinate descent methods. Applied to many standard learning tasks, Lasso, Sparse Group Lasso, multi-task Lasso, binary and multinomial logistic regression, etc., we report significant speed-ups compared to previously proposed safe rules on all tested data sets",
    "volume": "main",
    "checked": true,
    "id": "c3f7842b05298a6d53fae724c39705c1e5ff098c",
    "citation_count": 98
  },
  "https://jmlr.org/papers/v18/16-501.html": {
    "title": "Minimax Filter: Learning to Preserve Privacy from Inference Attacks",
    "abstract": "Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differentially privacy, which uses a more rigorous definition of privacy loss, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform minimax optimization. In addition, the paper proposes a noisy minimax filter which combines minimax filter and differentially-private mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or higher target task accuracy and lower inference accuracy, often significantly lower than previous methods",
    "volume": "main",
    "checked": true,
    "id": "33ba1c0cd1857b107d658cb900be75083c5a35b3",
    "citation_count": 78
  },
  "https://jmlr.org/papers/v18/16-563.html": {
    "title": "Knowledge Graph Completion via Complex Tensor Factorization",
    "abstract": "In statistical relational learning, knowledge graph completion deals with automatically understanding the structure of large knowledge graphs---labeled directed graphs---and predicting missing relationships---labeled edges. State-of-the-art embedding models propose different trade-offs between modeling expressiveness, and time and space complexity. We reconcile both expressiveness and complexity through the use of complex-valued embeddings and explore the link between such complex-valued embeddings and unitary diagonalization. We corroborate our approach theoretically and show that all real square matrices---thus all possible relation/adjacency matrices---are the real part of some unitarily diagonalizable matrix. This results opens the door to a lot of other applications of square matrices factorization. Our approach based on complex embeddings is arguably simple, as it only involves a Hermitian dot product, the complex counterpart of the standard dot product between real vectors, whereas other methods resort to more and more complicated composition functions to increase their expressiveness. The proposed complex embeddings are scalable to large data sets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks",
    "volume": "main",
    "checked": true,
    "id": "a4dfb121275a6408d290b803baf8c9caeb23dc5b",
    "citation_count": 226
  },
  "https://jmlr.org/papers/v18/16-190.html": {
    "title": "Stabilized Sparse Online Learning for Sparse Data",
    "abstract": "Stochastic gradient descent (SGD) is commonly used for optimization in large-scale machine learning problems. Lanford et al. (2009) introduce a sparse online learning method to induce sparsity via truncated gradient. With high- dimensional sparse data, however, this method suffers from slow convergence and high variance due to heterogeneity in feature sparsity. To mitigate this issue, we introduce a stabilized truncated stochastic gradient descent algorithm. We employ a soft- thresholding scheme on the weight vector where the imposed shrinkage is adaptive to the amount of information available in each feature. The variability in the resulted sparse weight vector is further controlled by stability selection integrated with the informative truncation. To facilitate better convergence, we adopt an annealing strategy on the truncation rate, which leads to a balanced trade-off between exploration and exploitation in learning a sparse weight vector. Numerical experiments show that our algorithm compares favorably with the original truncated gradient SGD in terms of prediction accuracy, achieving both better sparsity and stability",
    "volume": "main",
    "checked": true,
    "id": "82616585490603906c23fddf6ceebadf01179223",
    "citation_count": 9
  },
  "https://jmlr.org/papers/v18/17-101.html": {
    "title": "Active-set Methods for Submodular Minimization Problems",
    "abstract": "We consider the submodular function minimization (SFM) and the quadratic minimization problems regularized by the Lovasz extension of the submodular function. These optimization problems are intimately related; for example, min-cut problems and total variation denoising problems, where the cut function is submodular and its Lovasz extension is given by the associated total variation. When a quadratic loss is regularized by the total variation of a cut function, it thus becomes a total variation denoising problem and we use the same terminology in this paper for general submodular functions. We propose a new active-set algorithm for total variation denoising with the assumption of an oracle that solves the corresponding SFM problem. This can be seen as local descent algorithm over ordered partitions with explicit convergence guarantees. It is more flexible than the existing algorithms with the ability for warm-restarts using the solution of a closely related problem. Further, we also consider the case when a submodular function can be decomposed into the sum of two submodular functions $F_1$ and $F_2$ and assume SFM oracles for these two functions. We propose a new active-set algorithm for total variation denoising (and hence SFM by thresholding the solution at zero). This algorithm also performs local descent over ordered partitions and its ability to warm start considerably improves the performance of the algorithm. In the experiments, we compare the performance of the proposed algorithms with state-of-the-art algorithms, showing that it reduces the calls to SFM oracles",
    "volume": "main",
    "checked": true,
    "id": "76b14a52ff0aac1cddc70dda831405507dd654a5",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v18/17-197.html": {
    "title": "A Bayesian Mixed-Effects Model to Learn Trajectories of Changes from Repeated Manifold-Valued Observations",
    "abstract": "We propose a generic Bayesian mixed-effects model to estimate the temporal progression of a biological phenomenon from observations obtained at multiple time points for a group of individuals. The progression is modeled by continuous trajectories in the space of measurements. Individual trajectories of progression result from spatiotemporal transformations of an average trajectory. These transformations allow for the quantification of changes in direction and pace at which the trajectories are followed. The framework of Riemannian geometry allows the model to be used with any kind of measurements with smooth constraints. A stochastic version of the Expectation-Maximization algorithm is used to produce maximum a posteriori estimates of the parameters. We evaluated our method using a series of neuropsychological test scores from patients with mild cognitive impairments, later diagnosed with Alzheimer's disease, and simulated evolutions of symmetric positive definite matrices. The data-driven model of impairment of cognitive functions illustrated the variability in the ordering and timing of the decline of these functions in the population. We showed that the estimated spatiotemporal transformations effectively put into correspondence significant events in the progression of individuals",
    "volume": "main",
    "checked": true,
    "id": "bfc6019602cc7274322b73875746b964ce7637ca",
    "citation_count": 48
  },
  "https://jmlr.org/papers/v18/17-214.html": {
    "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
    "abstract": "Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also show how to tune SGD with momentum for approximate sampling. (4) We analyze stochastic-gradient MCMC algorithms. For Stochastic- Gradient Langevin Dynamics and Stochastic-Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler",
    "volume": "main",
    "checked": true,
    "id": "ea68a5c75e0e228e54efd91db972f71c1a917e51",
    "citation_count": 501
  },
  "https://jmlr.org/papers/v18/17-203.html": {
    "title": "STORE: Sparse Tensor Response Regression and Neuroimaging Analysis",
    "abstract": "Motivated by applications in neuroimaging analysis, we propose a new regression model, Sparse TensOr REsponse regression (STORE), with a tensor response and a vector predictor. STORE embeds two key sparse structures: element-wise sparsity and low-rankness. It can handle both a non-symmetric and a symmetric tensor response, and thus is applicable to both structural and functional neuroimaging data. We formulate the parameter estimation as a non-convex optimization problem, and develop an efficient alternating updating algorithm. We establish a non- asymptotic estimation error bound for the actual estimator obtained from the proposed algorithm. This error bound reveals an interesting interaction between the computational efficiency and the statistical rate of convergence. When the distribution of the error tensor is Gaussian, we further obtain a fast estimation error rate which allows the tensor dimension to grow exponentially with the sample size. We illustrate the efficacy of our model through intensive simulations and an analysis of the Autism spectrum disorder neuroimaging data",
    "volume": "main",
    "checked": true,
    "id": "e61237c75d7f2aa51cdda7eb5bc936286a8b668d",
    "citation_count": 72
  },
  "https://jmlr.org/papers/v18/16-634.html": {
    "title": "A Survey of Preference-Based Reinforcement Learning Methods",
    "abstract": "Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task- specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL",
    "volume": "main",
    "checked": true,
    "id": "84082634110fcedaaa32632f6cc16a034eedb2a0",
    "citation_count": 196
  },
  "https://jmlr.org/papers/v18/16-266.html": {
    "title": "Generalized SURE for optimal shrinkage of singular values in low-rank matrix denoising",
    "abstract": "We consider the problem of estimating a low-rank signal matrix from noisy measurements under the assumption that the distribution of the data matrix belongs to an exponential family. In this setting, we derive generalized Stein's unbiased risk estimation (SURE) formulas that hold for any spectral estimators which shrink or threshold the singular values of the data matrix. This leads to new data-driven spectral estimators, whose optimality is discussed using tools from random matrix theory and through numerical experiments. Under the spiked population model and in the asymptotic setting where the dimensions of the data matrix are let going to infinity, some theoretical properties of our approach are compared to recent results on asymptotically optimal shrinking rules for Gaussian noise. It also leads to new procedures for singular values shrinkage in finite-dimensional matrix denoising for Gamma- distributed and Poisson-distributed measurements",
    "volume": "main",
    "checked": true,
    "id": "763c32dd1143b7c167057d59ed1412cb142fa63c",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/16-232.html": {
    "title": "Dimension Estimation Using Random Connection Models",
    "abstract": "Information about intrinsic dimension is crucial to perform dimensionality reduction, compress information, design efficient algorithms, and do statistical adaptation. In this paper we propose an estimator for the intrinsic dimension of a data set. The estimator is based on binary neighbourhood information about the observations in the form of two adjacency matrices, and does not require any explicit distance information. The underlying graph is modelled according to a subset of a specific random connection model, sometimes referred to as the Poisson blob model. Computationally the estimator scales like $n\\log n$, and we specify its asymptotic distribution and rate of convergence. A simulation study on both real and simulated data shows that our approach compares favourably with some competing methods from the literature, including approaches that rely on distance information",
    "volume": "main",
    "checked": true,
    "id": "cc06013efbc59a39443942dad79fb99bb2ad03dc",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/15-464.html": {
    "title": "Bayesian Inference for Spatio-temporal Spike-and-Slab Priors",
    "abstract": "In this work, we address the problem of solving a series of underdetermined linear inverse problemblems subject to a sparsity constraint. We generalize the spike-and-slab prior distribution to encode a priori correlation of the support of the solution in both space and time by imposing a transformed Gaussian process on the spike-and-slab probabilities. An expectation propagation (EP) algorithm for posterior inference under the proposed model is derived. For large scale problems, the standard EP algorithm can be prohibitively slow. We therefore introduce three different approximation schemes to reduce the computational complexity. Finally, we demonstrate the proposed model using numerical experiments based on both synthetic and real data sets",
    "volume": "main",
    "checked": true,
    "id": "b5aa2148a44ce224eb2d19c95735c73dd7a2beee",
    "citation_count": 34
  },
  "https://jmlr.org/papers/v18/15-143.html": {
    "title": "Adaptive Randomized Dimension Reduction on Massive Data",
    "abstract": "The scalability of statistical estimators is of increasing importance in modern applications. One approach to implementing scalable algorithms is to compress data into a low dimensional latent space using dimension reduction methods. In this paper, we develop an approach for dimension reduction that exploits the assumption of low rank structure in high dimensional data to gain both computational and statistical advantages. We adapt recent randomized low-rank approximation algorithms to provide an efficient solution to principal component analysis (PCA), and we use this efficient solver to improve estimation in large- scale linear mixed models (LMM) for association mapping in statistical genomics. A key observation in this paper is that randomization serves a dual role, improving both computational and statistical performance by implicitly regularizing the covariance matrix estimate of the random effect in an LMM. These statistical and computational advantages are highlighted in our experiments on simulated data and large-scale genomic studies",
    "volume": "main",
    "checked": true,
    "id": "a6b5020a47f25e2fe7e99d044b759b0adca892af",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v18/16-572.html": {
    "title": "A Nonconvex Approach for Phase Retrieval: Reshaped Wirtinger Flow and Incremental Algorithms",
    "abstract": "We study the problem of solving a quadratic system of equations, i.e., recovering a vector signal $\\boldsymbol{x}\\in \\mathbb{R}^n$ from its magnitude measurements $y_i=|\\langle \\boldsymbol{a}_i, \\boldsymbol{x}\\rangle|, i=1,..., m$. We develop a gradient descent algorithm (referred to as RWF for reshaped Wirtinger flow) by minimizing the quadratic loss of the magnitude measurements. Comparing with Wirtinger flow (WF) (Candes et al., 2015), the loss function of RWF is nonconvex and nonsmooth, but better resembles the least-squares loss when the phase information is also available. We show that for random Gaussian measurements, RWF enjoys linear convergence to the true signal as long as the number of measurements is $\\mathcal{O}(n)$. This improves the sample complexity of WF ($\\mathcal{O}(n\\log n)$), and achieves the same sample complexity as truncated Wirtinger flow (TWF) (Chen and Candes, 2015), but without any sophisticated truncation in the gradient loop. Furthermore, RWF costs less computationally than WF, and runs faster numerically than both WF and TWF. We further develop an incremental (stochastic) version of RWF (IRWF) and connect it with the randomized Kaczmarz method for phase retrieval. We demonstrate that IRWF outperforms existing incremental as well as batch algorithms with experiments",
    "volume": "main",
    "checked": true,
    "id": "4adc2fa636a382d73a33dde767cbbdb46604e631",
    "citation_count": 95
  },
  "https://jmlr.org/papers/v18/16-382.html": {
    "title": "Consistency, Breakdown Robustness, and Algorithms for Robust Improper Maximum Likelihood Clustering",
    "abstract": "The robust improper maximum likelihood estimator (RIMLE) is a new method for robust multivariate clustering finding approximately Gaussian clusters. It maximizes a pseudo- likelihood defined by adding a component with improper constant density for accommodating outliers to a Gaussian mixture. A special case of the RIMLE is MLE for multivariate finite Gaussian mixture models. In this paper we treat existence, consistency, and breakdown theory for the RIMLE comprehensively. RIMLE's existence is proved under non-smooth covariance matrix constraints. It is shown that these can be implemented via a computationally feasible Expectation-Conditional Maximization algorithm",
    "volume": "main",
    "checked": true,
    "id": "5f06539b727930a6927d8288f96b05535aa50481",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v18/17-175.html": {
    "title": "On Computationally Tractable Selection of Experiments in Measurement-Constrained Regression Models",
    "abstract": "We derive computationally tractable methods to select a small subset of experiment settings from a large pool of given design points. The primary focus is on linear regression models, while the technique extends to generalized linear models and Delta's method (estimating functions of linear regression models) as well. The algorithms are based on a continuous relaxation of an otherwise intractable combinatorial optimization problem, with sampling or greedy procedures as post-processing steps. Formal approximation guarantees are established for both algorithms, and numerical results on both synthetic and real-world data confirm the effectiveness of the proposed methods",
    "volume": "main",
    "checked": true,
    "id": "4ffeb1a74cdb4c1f687d00dd3eec044a67509070",
    "citation_count": 35
  },
  "https://jmlr.org/papers/v18/14-348.html": {
    "title": "Generalized Conditional Gradient for Sparse Estimation",
    "abstract": "Sparsity is an important modeling tool that expands the applicability of convex formulations for data analysis, however it also creates significant challenges for efficient algorithm design. In this paper we investigate the generalized conditional gradient (GCG) algorithm for solving sparse optimization problems--- demonstrating that, with some enhancements, it can provide a more efficient alternative to current state of the art approaches. After studying the convergence properties of GCG for general convex composite problems, we develop efficient methods for evaluating polar operators, a subroutine that is required in each GCG iteration. In particular, we show how the polar operator can be efficiently evaluated in learning low-rank matrices, instantiated with detailed examples on matrix completion and dictionary learning. A further improvement is achieved by interleaving GCG with fixed-rank local subspace optimization. A series of experiments on matrix completion, multi-class classification, and multi-view dictionary learning shows that the proposed method can significantly reduce the training cost of current alternatives",
    "volume": "main",
    "checked": true,
    "id": "aa59b3facccb4d40377d5a08ce0d2d26fec8c200",
    "citation_count": 77
  },
  "https://jmlr.org/papers/v18/17-079.html": {
    "title": "Following the Leader and Fast Rates in Online Linear Prediction: Curved Constraint Sets and Other Regularities",
    "abstract": "Follow the leader (FTL) is a simple online learning algorithm that is known to perform well when the loss functions are convex and positively curved. In this paper we ask whether there are other settings when FTL achieves low regret. In particular, we study the fundamental problem of linear prediction over a convex, compact domain with non-empty interior. Amongst other results, we prove that the curvature of the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys logarithmic regret, while for polytope domains and stochastic data it enjoys finite expected regret. The former result is also extended to strongly convex domains by establishing an equivalence between the strong convexity of sets and the minimum curvature of their boundary, which may be of independent interest. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the smaller regret of FTL when the data is `easy'. Finally, we show that such guarantees are achievable directly (e.g., by the follow the regularized leader algorithm or by a shrinkage-based variant of FTL) when the constraint set is an ellipsoid",
    "volume": "main",
    "checked": true,
    "id": "6351807959b22a8181470e57a1e91fdeb687943c",
    "citation_count": 72
  },
  "https://jmlr.org/papers/v18/16-422.html": {
    "title": "Regularization and the small-ball method II: complexity dependent error rates",
    "abstract": "We study estimation properties of regularized procedures of the form $\\hat f \\in\\arg\\min_{f\\in F}\\Big(\\frac{1}{N}\\sum_{i=1}^N\\big(Y_i-f(X_i)\\big)^2+\\lambda \\Psi(f)\\Big)$ for a convex class of functions $F$, regularization function $\\Psi(\\cdot)$ and some well chosen regularization parameter $\\lambda$, where the given data is an independent sample $(X_i, Y_i)_{i=1}^N$. We obtain bounds on the $L_2$ estimation error rate that depend on the complexity of the true model $F^*:=\\{f\\in F: \\Psi(f)\\leq\\Psi(f^*)\\}$, where $f^*\\in\\arg\\min_{f\\in F}\\mathbb{E}(Y-f(X))^2$ and the $(X_i,Y_i)$'s are independent and distributed as $(X,Y)$. Our estimate holds under weak stochastic assumptions -- one of which being a small-ball condition satisfied by $F$ -- and for rather flexible choices of regularization functions $\\Psi(\\cdot)$. Moreover, the result holds in the learning theory framework: we do not assume any a-priori connection between the output $Y$ and the input $X$. As a proof of concept, we apply our general estimation bound to various choices of $\\Psi$, for example, the $\\ell_p$ and $S_p$-norms (for $p\\geq1$), weak-$\\ell_p$, atomic norms, max- norm and SLOPE. In many cases, the estimation rate almost coincides with the minimax rate in the class $F^*$",
    "volume": "main",
    "checked": true,
    "id": "d73d8b9942f72b2c37dae2d654089da74e40ea3d",
    "citation_count": 36
  },
  "https://jmlr.org/papers/v18/15-076.html": {
    "title": "Matrix Completion with Noisy Entries and Outliers",
    "abstract": "This paper considers the problem of matrix completion when the observed entries are noisy and contain outliers. It begins with introducing a new optimization criterion for which the recovered matrix is defined as its solution. This criterion uses the celebrated Huber function from the robust statistics literature to downweigh the effects of outliers. A practical algorithm is developed to solve the optimization involved. This algorithm is fast, straightforward to implement, and monotonic convergent. Furthermore, the proposed methodology is theoretically shown to be stable in a well defined sense. Its promising empirical performance is demonstrated via a sequence of simulation experiments, including image inpainting",
    "volume": "main",
    "checked": true,
    "id": "856dd08dceeb4056ba2eff6465e4e438afa9308e",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v18/17-275.html": {
    "title": "Faithfulness of Probability Distributions and Graphs",
    "abstract": "A main question in graphical models and causal inference is whether, given a probability distribution $P$ (which is usually an underlying distribution of data), there is a graph (or graphs) to which $P$ is faithful. The main goal of this paper is to provide a theoretical answer to this problem. We work with general independence models, which contain probabilistic independence models as a special case. We exploit a generalization of ordering, called preordering, of the nodes of (mixed) graphs. This allows us to provide sufficient conditions for a given independence model to be Markov to a graph with the minimum possible number of edges, and more importantly, necessary and sufficient conditions for a given probability distribution to be faithful to a graph. We present our results for the general case of mixed graphs, but specialize the definitions and results to the better-known subclasses of undirected (concentration) and bidirected (covariance) graphs as well as directed acyclic graphs",
    "volume": "main",
    "checked": true,
    "id": "2dc0e869afaf0a784904c70f90cb0422b69e7d5c",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/16-645.html": {
    "title": "Community Extraction in Multilayer Networks with Heterogeneous Community Structure",
    "abstract": "Multilayer networks are a useful way to capture and model multiple, binary or weighted relationships among a fixed group of objects. While community detection has proven to be a useful exploratory technique for the analysis of single-layer networks, the development of community detection methods for multilayer networks is still in its infancy. We propose and investigate a procedure, called Multilayer Extraction, that identifies densely connected vertex-layer sets in multilayer networks. Multilayer Extraction makes use of a significance based score that quantifies the connectivity of an observed vertex-layer set through comparison with a fixed degree random graph model. Multilayer Extraction directly handles networks with heterogeneous layers where community structure may be different from layer to layer. The procedure can capture overlapping communities, as well as background vertex-layer pairs that do not belong to any community. We establish consistency of the vertex-layer set optimizer of our proposed multilayer score under the multilayer stochastic block model. We investigate the performance of Multilayer Extraction on three applications and a test bed of simulations. Our theoretical and numerical evaluations suggest that Multilayer Extraction is an effective exploratory tool for analyzing complex multilayer networks. Publicly available code is available at github.com/jdwilson4/Multila yerExtraction",
    "volume": "main",
    "checked": true,
    "id": "9b858140e6fccbd628a54bfbd08ff496b138923f",
    "citation_count": 50
  },
  "https://jmlr.org/papers/v18/15-619.html": {
    "title": "On Binary Embedding using Circulant Matrices",
    "abstract": "Binary embeddings provide efficient and powerful ways to perform operations on large scale data. However binary embedding typically requires long codes in order to preserve the discriminative power of the input space. Thus binary coding methods traditionally suffer from high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure allows us to use Fast Fourier Transform algorithms to speed up the computation. For obtaining $k$-bit binary codes from $d$-dimensional data, our method improves the time complexity from $\\mathcal{O}(dk)$ to $\\mathcal{O}(d\\log{d})$, and the space complexity from $\\mathcal{O}(dk)$ to $\\mathcal{O}(d)$. We study two settings, which differ in the way we choose the parameters of the circulant matrix. In the first, the parameters are chosen randomly and in the second, the parameters are learned using the data. For randomized CBE, we give a theoretical analysis comparing it with binary embedding using an unstructured random projection matrix. The challenge here is to show that the dependencies in the entries of the circulant matrix do not lead to a loss in performance. In the second setting, we design a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. In both the settings, we show by extensive experiments that the CBE approach gives much better performance than the state-of-the-art approaches if we fix a running time, and provides much faster computation with negligible performance degradation if we fix the number of bits in the embedding",
    "volume": "main",
    "checked": true,
    "id": "1edb498c7cef4b43783809b0a3e86ac857c69573",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v18/16-579.html": {
    "title": "Variational Fourier Features for Gaussian Processes",
    "abstract": "This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for MatÃ©rn kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the data set, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non- conjugate likelihoods, our MCMC scheme reduces the cost of computation from $\\mathcal{O}(NM^2)$ (for a sparse Gaussian process) to $\\mathcal{O}(NM)$ per iteration, where $N$ is the number of data and $M$ is the number of features",
    "volume": "main",
    "checked": true,
    "id": "a9fae3cbd13c2d0a80a1f0a625167884d2738344",
    "citation_count": 162
  },
  "https://jmlr.org/papers/v18/17-434.html": {
    "title": "HyperTools: a Python Toolbox for Gaining Geometric Insights into High-Dimensional Data",
    "abstract": "Dimensionality reduction algorithms have played a foundational role in facilitating the deep understanding of complex high- dimensional data. One particularly useful application of dimensionality reduction techniques is in data visualization. Low-dimensional visualizations can help practitioners understand where machine learning algorithms might leverage the geometric properties of a dataset to improve performance. Another challenge is to generalize insights across datasets [e.g. data from multiple modalities describing the same system (Haxby et al., 2011), artwork or photographs of similar content in different styles (Zhu et al., 2017), etc.]. Several recently developed techniques(e.g. Haxby et al., 2011; Chen et al., 2015) use the procrustean transformation (Schonemann, 1966) to align the geometries of two or more spaces so that data with different axes may be plotted in a common space. We propose that each of these techniques (dimensionality reduction, alignment, and visualization) applied in sequence should be cast as a single conceptual hyperplot operation for gaining geometric insights into high-dimensional data. Our Python toolbox enables this operation in a single (highly flexible) function call",
    "volume": "main",
    "checked": true,
    "id": "71602749a2f9c449b032f06498edcc8c8dd5a944",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/17-468.html": {
    "title": "Automatic Differentiation in Machine Learning: a Survey",
    "abstract": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply Ã¢ÂÂautodiffÃ¢ÂÂ, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names Ã¢ÂÂdynamic computational graphsÃ¢ÂÂ and Ã¢ÂÂdifferentiable programmingÃ¢ÂÂ. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms Ã¢ÂÂautodiffÃ¢ÂÂ, Ã¢ÂÂautomatic differentiationÃ¢ÂÂ, and Ã¢ÂÂsymbolic differentiationÃ¢ÂÂ as these are encountered more and more in machine learning settings",
    "volume": "main",
    "checked": true,
    "id": "da118b8aa99699edd7609fbbd081d5b93bc2e87b",
    "citation_count": 1807
  },
  "https://jmlr.org/papers/v18/15-154.html": {
    "title": "Normal Bandits of Unknown Means and Variances",
    "abstract": "Consider the problem of sampling sequentially from a finite number of $N \\geq 2$ populations, specified by random variables $X^i_k$, $ i = 1,\\ldots , N,$ and $k = 1, 2, \\ldots$; where $X^i_k$ denotes the outcome from population $i$ the $k^{th}$ time it is sampled. It is assumed that for each fixed $i$, $\\{ X^i_k \\}_{k \\geq 1}$ is a sequence of i.i.d. normal random variables, with unknown mean $\\mu_i$ and unknown variance $\\sigma_i^2$. The objective is to have a policy $\\pi$ for deciding from which of the $N$ populations to sample from at any time $t=1,2,\\ldots$ so as to maximize the expected sum of outcomes of $n$ total samples or equivalently to minimize the regret due to lack on information of the parameters $\\mu_i$ and $\\sigma_i^2$. In this paper, we present a simple inflated sample mean (ISM) index policy that is asymptotically optimal in the sense of Theorem 4 below. This resolves a standing open problem from \\cite{bkmab96}. Additionally, finite horizon regret bounds are given",
    "volume": "main",
    "checked": true,
    "id": "769791248102153914658300fb5fce43ca91d8b6",
    "citation_count": 63
  },
  "https://jmlr.org/papers/v18/15-226.html": {
    "title": "Cost-Sensitive Learning with Noisy Labels",
    "abstract": "We study binary classification in the presence of \\emph{class- conditional} random noise, where the learner gets to see labels that are flipped independently with some probability, and where the flip probability depends on the class. Our goal is to devise learning algorithms that are efficient and statistically consistent with respect to commonly used utility measures. In particular, we look at a family of measures motivated by their application in domains where cost-sensitive learning is necessary (for example, when there is class imbalance). In contrast to most of the existing literature on consistent classification that are limited to the classical 0-1 loss, our analysis includes more general utility measures such as the AM measure (arithmetic mean of True Positive Rate and True Negative Rate). For this problem of cost-sensitive learning under class- conditional random noise, we develop two approaches that are based on suitably modifying surrogate losses. First, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical utility maximization in the presence of i.i.d. data with noisy labels. If the loss function satisfies a simple symmetry condition, we show that using unbiased estimator leads to an efficient algorithm for empirical maximization. Second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong utility bounds. This approach implies that methods already used in practice, such as biased SVM and weighted logistic regression, are provably noise- tolerant. For two practically important measures in our family, we show that the proposed methods are competitive with respect to recently proposed methods for dealing with label noise in several benchmark data sets",
    "volume": "main",
    "checked": true,
    "id": "362510c9e6a97e35074eb01c2528b856143886f4",
    "citation_count": 49
  },
  "https://jmlr.org/papers/v18/15-233.html": {
    "title": "Provably Correct Algorithms for Matrix Column Subset Selection with Selectively Sampled Data",
    "abstract": "We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and limitations in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks (Drineas et al., 2008; Frieze et al., 2004; Deshpande and Vempala, 2006; Krishnamurthy and Singh, 2014). Our analysis shows that, under the assumption that the input data matrix has incoherent rows but possibly coherent columns, all algorithms provably converge to the best low-rank approximation of the original data as number of selected columns increases. Furthermore, two of the proposed algorithms enjoy a relative error bound, which is preferred for column subset selection and matrix approximation purposes. We also demonstrate through both theoretical and empirical analysis the power of feedback driven sampling compared to uniform random sampling on input matrices with highly correlated columns",
    "volume": "main",
    "checked": true,
    "id": "ee87ea9d42996b7ca307ebd1e8867c2981924dcc",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v18/15-373.html": {
    "title": "A Study of the Classification of Low-Dimensional Data with Supervised Manifold Learning",
    "abstract": "Supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes. In this work, we propose a theoretical study of supervised manifold learning for classification. We consider nonlinear dimensionality reduction algorithms that yield linearly separable embeddings of training data and present generalization bounds for this type of algorithms. A necessary condition for satisfactory generalization performance is that the embedding allow the construction of a sufficiently regular interpolation function in relation with the separation margin of the embedding. We show that for supervised embeddings satisfying this condition, the classification error decays at an exponential rate with the number of training samples. Finally, we examine the separability of supervised nonlinear embeddings that aim to preserve the low-dimensional geometric structure of data based on graph representations. The proposed analysis is supported by experiments on several real data sets",
    "volume": "main",
    "checked": true,
    "id": "5d72144f4b414b6ad6f3198b42df36c69be7ce38",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v18/15-481.html": {
    "title": "Probabilistic preference learning with the Mallows rank model",
    "abstract": "Ranking and comparing items is crucial for collecting information about preferences in many areas, from marketing to politics. The Mallows rank model is among the most successful approaches to analyse rank data, but its computational complexity has limited its use to a particular form based on Kendall distance. We develop new computationally tractable methods for Bayesian inference in Mallows models that work with any right-invariant distance. Our method performs inference on the consensus ranking of the items, also when based on partial rankings, such as top-$k$ items or pairwise comparisons. We prove that items that none of the assessors has ranked do not influence the maximum a posteriori consensus ranking, and can therefore be ignored. When assessors are many or heterogeneous, we propose a mixture model for clustering them in homogeneous subgroups, with cluster-specific consensus rankings. We develop approximate stochastic algorithms that allow a fully probabilistic analysis, leading to coherent quantifications of uncertainties. We make probabilistic predictions on the class membership of assessors based on their ranking of just some items, and predict missing individual preferences, as needed in recommendation systems. We test our approach using several experimental and benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "4df6b6751421724d18bc70cb9d4f2ef6d15d6136",
    "citation_count": 80
  },
  "https://jmlr.org/papers/v18/15-484.html": {
    "title": "Robust Topological Inference: Distance To a Measure and Kernel Distance",
    "abstract": "Let $P$ be a distribution with support $S$. The salient features of $S$ can be quantified with persistent homology, which summarizes topological features of the sublevel sets of the distance function (the distance of any point $x$ to $S$). Given a sample from $P$ we can infer the persistent homology using an empirical version of the distance function. However, the empirical distance function is highly non-robust to noise and outliers. Even one outlier is deadly. The distance-to-a-measure (DTM), introduced by \\cite{chazal2011geometric}, and the kernel distance, introduced by \\cite{phillips2014goemetric}, are smooth functions that provide useful topological information but are robust to noise and outliers. \\cite{massart2014} derived concentration bounds for DTM. Building on these results, we derive limiting distributions and confidence sets, and we propose a method for choosing tuning parameters",
    "volume": "main",
    "checked": true,
    "id": "c9c8cabbb15c7341324c010fe205c8c676fbb375",
    "citation_count": 161
  },
  "https://jmlr.org/papers/v18/15-506.html": {
    "title": "Training Gaussian Mixture Models at Scale via Coresets",
    "abstract": "How can we train a statistical mixture model on a massive data set? In this work we show how to construct \\emph{coresets} for mixtures of Gaussians. A coreset is a weighted subset of the data, which guarantees that models fitting the coreset also provide a good fit for the original data set. We show that, perhaps surprisingly, Gaussian mixtures admit coresets of size polynomial in dimension and the number of mixture components, while being \\emph{independent} of the data set size. Hence, one can harness computationally intensive algorithms to compute a good approximation on a significantly smaller data set. More importantly, such coresets can be efficiently constructed both in distributed and streaming settings and do not impose restrictions on the data generating process. Our results rely on a novel reduction of statistical estimation to problems in computational geometry and new combinatorial complexity results for mixtures of Gaussians. Empirical evaluation on several real- world data sets suggests that our coreset-based approach enables significant reduction in training-time with negligible approximation error",
    "volume": "main",
    "checked": true,
    "id": "4baebac87436a44f6f06a84ffc54f89ed9d6b46b",
    "citation_count": 75
  },
  "https://jmlr.org/papers/v18/15-592.html": {
    "title": "Gradient Estimation with Simultaneous Perturbation and Compressive Sensing",
    "abstract": "We propose a scheme for finding a \"good\" estimator for the gradient of a function on a high-dimensional space with few function evaluations, for applications where function evaluations are expensive and the function under consideration is not sensitive in all coordinates locally, making its gradient almost sparse. Exploiting the latter aspect, our method combines ideas from Spall's Simultaneous Perturbation Stochastic Approximation with compressive sensing. We theoretically justify its computational advantages and illustrate them empirically by numerical experiments. In particular, applications to estimating gradient outer product matrix as well as standard optimization problems are illustrated via simulations",
    "volume": "main",
    "checked": true,
    "id": "c0bda50db615f1041f27b12df0e18ed4b050e0f0",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v18/15-595.html": {
    "title": "Principled Selection of Hyperparameters in the Latent Dirichlet Allocation Model",
    "abstract": "Latent Dirichlet Allocation (LDA) is a well known topic model that is often used to make inference regarding the properties of collections of text documents. LDA is a hierarchical Bayesian model, and involves a prior distribution on a set of latent topic variables. The prior is indexed by certain hyperparameters, and even though these have a large impact on inference, they are usually chosen either in an ad-hoc manner, or by applying an algorithm whose theoretical basis has not been firmly established. We present a method, based on a combination of Markov chain Monte Carlo and importance sampling, for estimating the maximum likelihood estimate of the hyperparameters. The method may be viewed as a computational scheme for implementation of an empirical Bayes analysis. It comes with theoretical guarantees, and a key feature of our approach is that we provide theoretically-valid error margins for our estimates. Experiments on both synthetic and real data show good performance of our methodology",
    "volume": "main",
    "checked": true,
    "id": "faf1ae7711186c3408ecef288cc04bcd94fd621b",
    "citation_count": 45
  },
  "https://jmlr.org/papers/v18/17-527.html": {
    "title": "Deep Learning the Ising Model Near Criticality",
    "abstract": "It is well established that neural networks with deep architectures perform better than shallow networks for many tasks in machine learning. In statistical physics, while there has been recent interest in representing physical data with generative modelling, the focus has been on shallow neural networks. A natural question to ask is whether deep neural networks hold any advantage over shallow networks in representing such data. We investigate this question by using unsupervised, generative graphical models to learn the probability distribution of a two-dimensional Ising system. Deep Boltzmann machines, deep belief networks, and deep restricted Boltzmann networks are trained on thermal spin configurations from this system, and compared to the shallow architecture of the restricted Boltzmann machine. We benchmark the models, focussing on the accuracy of generating energetic observables near the phase transition, where these quantities are most difficult to approximate. Interestingly, after training the generative networks, we observe that the accuracy essentially depends only on the number of neurons in the first hidden layer of the network, and not on other model details such as network depth or model type. This is evidence that shallow networks are more efficient than deep networks at representing physical probability distributions associated with Ising systems near criticality",
    "volume": "main",
    "checked": true,
    "id": "81fc34df88b2293981380454e7259629ff645320",
    "citation_count": 79
  },
  "https://jmlr.org/papers/v18/17-636.html": {
    "title": "pomegranate: Fast and Flexible Probabilistic Modeling in Python",
    "abstract": "We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with---or outperform---other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code. The code is available at \\url{https://github.com/jmschrei/pomegranate}",
    "volume": "main",
    "checked": true,
    "id": "65e1ada2360b42368a5f9f5e40ff436051c6fa84",
    "citation_count": 152
  },
  "https://jmlr.org/papers/v18/17-653.html": {
    "title": "Maximum Principle Based Algorithms for Deep Learning",
    "abstract": "The continuous dynamical system approach to deep learning is explored in order to devise alternative frameworks for training algorithms. Training is recast as a control problem and this allows us to formulate necessary optimality conditions in continuous time using the Pontryagin's maximum principle (PMP). A modification of the method of successive approximations is then used to solve the PMP, giving rise to an alternative training algorithm for deep learning. This approach has the advantage that rigorous error estimates and convergence results can be established. We also show that it may avoid some pitfalls of gradient-based methods, such as slow convergence on flat landscapes near saddle points. Furthermore, we demonstrate that it obtains favorable initial convergence rate per-iteration, provided Hamiltonian maximization can be efficiently carried out - a step which is still in need of improvement. Overall, the approach opens up new avenues to attack problems associated with deep learning, such as trapping in slow manifolds and inapplicability of gradient-based methods for discrete trainable variables",
    "volume": "main",
    "checked": true,
    "id": "5e3fd9e6e7bcfc37fa751385ea3c8c7c7ac80c43",
    "citation_count": 188
  },
  "https://jmlr.org/papers/v18/14-415.html": {
    "title": "Gradient Hard Thresholding Pursuit",
    "abstract": "Hard Thresholding Pursuit (HTP) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems. This method has been shown to have strong theoretical guarantee and impressive numerical performance. In this article, we generalize HTP from compressed sensing to a generic problem setup of sparsity-constrained convex optimization. The proposed algorithm iterates between a standard gradient descent step and a hard-thresholding step with or without debiasing. We analyze the parameter estimation and sparsity recovery performance of the proposed method. Extensive numerical results confirm our theoretical predictions and demonstrate the superiority of our method to the state-of-the-art greedy selection methods in sparse linear regression, sparse logistic regression and sparse precision matrix estimation problems.\\footnote{A conference version of this work appeared in ICML 2014 \\citep{Yuan- ICML-2014}.}",
    "volume": "main",
    "checked": true,
    "id": "346ad27591f1b65ad9b21ecb28def7763d832d61",
    "citation_count": 74
  },
  "https://jmlr.org/papers/v18/15-636.html": {
    "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
    "abstract": "In many sequential decision-making problems one is interested in minimizing an expected cumulative cost while taking into account risk, i.e., increased awareness of events of small probability and high consequences. Accordingly, the objective of this paper is to present efficient reinforcement learning algorithms for risk-constrained Markov decision processes (MDPs), where risk is represented via a chance constraint or a constraint on the conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer to such problems as percentile risk-constrained MDPs. Specifically, we first derive a formula for computing the gradient of the Lagrangian function for percentile risk-constrained MDPs. Then, we devise policy gradient and actor-critic algorithms that (1) estimate such gradient, (2) update the policy in the descent direction, and (3) update the Lagrange multiplier in the ascent direction. For these algorithms we prove convergence to locally optimal policies. Finally, we demonstrate the effectiveness of our algorithms in an optimal stopping problem and an online marketing application",
    "volume": "main",
    "checked": true,
    "id": "759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89",
    "citation_count": 327
  },
  "https://jmlr.org/papers/v18/16-119.html": {
    "title": "Local Identifiability of $\\ell_1$-minimization Dictionary Learning: a Sufficient and Almost Necessary Condition",
    "abstract": "We study the theoretical properties of learning a dictionary from $N$ signals $\\mathbf{x}_i\\in \\mathbb R^K$ for $i=1,\\ldots,N$ via $\\ell_1$-minimization. We assume that $\\mathbf{x}_i$'s are $i.i.d.$ random linear combinations of the $K$ columns from a complete (i.e., square and invertible) reference dictionary $\\mathbf{D}_0 \\in \\mathbb R^{K\\times K}$. Here, the random linear coefficients are generated from either the $s$-sparse Gaussian model or the Bernoulli-Gaussian model. First, for the population case, we establish a sufficient and almost necessary condition for the reference dictionary $\\mathbf{D}_0$ to be locally identifiable, i.e., a strict local minimum of the expected $\\ell_1$-norm objective function. Our condition covers both sparse and dense cases of the random linear coefficients and significantly improves the sufficient condition by Gribonval and Schnass (2010). In addition, we show that for a complete $\\mu$-coherent reference dictionary, i.e., a dictionary with absolute pairwise column inner-product at most $\\mu\\in[0,1)$, local identifiability holds even when the random linear coefficient vector has up to $O(\\mu^{-2})$ nonzero entries. Moreover, our local identifiability results also translate to the finite sample case with high probability provided that the number of signals $N$ scales as $O(K\\log K)$",
    "volume": "main",
    "checked": true,
    "id": "572b96d8ce584e75a96200c929ad03432ba88331",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v18/17-069.html": {
    "title": "In Search of Coherence and Consensus: Measuring the Interpretability of Statistical Topics",
    "abstract": "Topic modeling is an important tool in natural language processing. Topic models provide two forms of output. The first is a predictive model. This type of model has the ability to predict unseen documents (e.g., their categories). When topic models are used in this way, there are ample measures to assess their performance. The second output of these models is the topics themselves. Topics are lists of keywords that describe the top words pertaining to each topic. Often, these lists of keywords are presented to a human subject who then assesses the meaning of the topic, which is ultimately subjective. One of the fundamental problems of topic models lies in assessing the quality of the topics from the perspective of human interpretability. Naturally, human subjects need to be employed to evaluate interpretability of a topic. Lately, crowdsourcing approaches are widely used to serve the role of human subjects in evaluation. In this work we study measures of interpretability and propose to measure topic interpretability from two perspectives: topic coherence and topic consensus. We start with an existing measure for topic coherence---model precision. It evaluates coherence of a topic by introducing an intruded word and measuring how well a human subject or a crowdsourcing approach could identify the intruded word: if it is easy to identify, the topic is coherent. We then investigate how we can measure coherence comprehensively by examining dimensions of topic coherence. For the second perspective of topic interpretability, we suggest topic consensus that measures how well the results of a crowdsourcing approach matches those given categories of topics. Good topics should lead to good categories, thus, high topic consensus. Therefore, if there is low topic consensus in terms of categories, topics could be of low interpretability. We then further discuss how topic coherence and topic consensus assess different aspects of topic interpretability and hope that this work can pave way for comprehensive measures of topic interpretability",
    "volume": "main",
    "checked": true,
    "id": "f992072f620b559cc0c49eb0112c84bbf5cab44a",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v18/17-151.html": {
    "title": "On the Behavior of Intrinsically High-Dimensional Spaces: Distances, Direct and Reverse Nearest Neighbors, and Hubness",
    "abstract": "Over the years, different characterizations of the curse of dimensionality have been provided, usually stating the conditions under which, in the limit of the infinite dimensionality, distances become indistinguishable. However, these characterizations almost never address the form of associated distributions in the finite, although high- dimensional, case. This work aims to contribute in this respect by investigating the distribution of distances, and of direct and reverse nearest neighbors, in intrinsically high-dimensional spaces. Indeed, we derive a closed form for the distribution of distances from a given point, for the expected distance from a given point to its $k$th nearest neighbor, and for the expected size of the approximate set of neighbors of a given point in finite high-dimensional spaces. Additionally, the hubness problem is considered, which is related to the form of the function $N_k$ representing the number of points that have a given point as one of their $k$ nearest neighbors, which is also called the number of $k$-occurrences. Despite the extensive use of this function, the precise characterization of its form is a longstanding problem. We derive a closed form for the number of $k$-occurrences associated with a given point in finite high- dimensional spaces, together with the associated limiting probability distribution. By investigating the relationships with the hubness phenomenon emerging in network science, we find that the distribution of node (in-)degrees of some real-life, large-scale networks has connections with the distribution of $k$-occurrences described herein",
    "volume": "main",
    "checked": true,
    "id": "3646bd0f042ee386361426ff01f88b7f1a8d39f9",
    "citation_count": 21
  },
  "https://jmlr.org/papers/v18/17-457.html": {
    "title": "Convergence of Unregularized Online Learning Algorithms",
    "abstract": "In this paper we study the convergence of online gradient descent algorithms in reproducing kernel Hilbert spaces (RKHSs) without regularization. We establish a sufficient condition and a necessary condition for the convergence of excess generalization errors in expectation. A sufficient condition for the almost sure convergence is also given. With high probability, we provide explicit convergence rates of the excess generalization errors for both averaged iterates and the last iterate, which in turn also imply convergence rates with probability one. To our best knowledge, this is the first high- probability convergence rate for the last iterate of online gradient descent algorithms in the general convex setting. Without any boundedness assumptions on iterates, our results are derived by a novel use of two measures of the algorithm's one- step progress, respectively by generalization errors and by distances in RKHSs, where the variances of the involved martingales are cancelled out by the descent property of the algorithm",
    "volume": "main",
    "checked": true,
    "id": "4f7642c396ebfa115444ab5ba6819b5068398f1b",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v18/16-556.html": {
    "title": "Convergence Analysis of Distributed Inference with Vector-Valued Gaussian Belief Propagation",
    "abstract": "This paper considers inference over distributed linear Gaussian models using factor graphs and Gaussian belief propagation (BP). The distributed inference algorithm involves only local computation of the information matrix and of the mean vector, and message passing between neighbors. Under broad conditions, it is shown that the message information matrix converges to a unique positive definite limit matrix for arbitrary positive semidefinite initialization, and it approaches an arbitrarily small neighborhood of this limit matrix at an exponential rate. A necessary and sufficient convergence condition for the belief mean vector to converge to the optimal centralized estimator is provided under the assumption that the message information matrix is initialized as a positive semidefinite matrix. Further, it is shown that Gaussian BP always converges when the underlying factor graph is given by the union of a forest and a single loop. The proposed convergence condition in the setup of distributed linear Gaussian models is shown to be strictly weaker than other existing convergence conditions and requirements, including the Gaussian Markov random field based walk-summability condition, and applicable to a large class of scenarios",
    "volume": "main",
    "checked": true,
    "id": "0424813e783d90ec7c5b8994301991a991e912f1",
    "citation_count": 51
  },
  "https://jmlr.org/papers/v18/17-406.html": {
    "title": "auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks",
    "abstract": "auDeep is a Python toolkit for deep unsupervised representation learning from acoustic data. It is based on a recurrent sequence to sequence autoencoder approach which can learn representations of time series data by taking into account their temporal dynamics. We provide an extensive command line interface in addition to a Python API for users and developers, both of which are comprehensively documented and publicly available at https://github.com/auDeep/auDeep. Experimental results indicate that auDeep features are competitive with state-of-the art audio classification",
    "volume": "main",
    "checked": true,
    "id": "589b9bdabddfa3a8d03fcbcf9b45514b128cde1e",
    "citation_count": 122
  },
  "https://jmlr.org/papers/v18/17-514.html": {
    "title": "On the Stability of Feature Selection Algorithms",
    "abstract": "Feature Selection is central to modern data science, from exploratory data analysis to predictive model-building. The Ã¢ÂÂstabilityÃ¢ÂÂ of a feature selection algorithm refers to the robustness of its feature preferences, with respect to data sampling and to its stochastic nature. An algorithm is `unstable' if a small change in data leads to large changes in the chosen feature subset. Whilst the idea is simple, quantifying this has proven more challenging---we note numerous proposals in the literature, each with different motivation and justification. We present a rigorous statistical treatment for this issue. In particular, with this work we consolidate the literature and provide (1) a deeper understanding of existing work based on a small set of properties, and (2) a clearly justified statistical approach with several novel benefits. This approach serves to identify a stability measure obeying all desirable properties, and (for the first time in the literature) allowing confidence intervals and hypothesis tests on the stability, enabling rigorous experimental comparison of feature selection algorithms",
    "volume": "main",
    "checked": true,
    "id": "456f688d742ad3c086a9372c4e35251094904210",
    "citation_count": 20
  },
  "https://jmlr.org/papers/v18/16-657.html": {
    "title": "Maximum Likelihood Estimation for Mixtures of Spherical Gaussians is NP-hard",
    "abstract": "This paper presents NP-hardness and hardness of approximation results for maximum likelihood estimation of mixtures of spherical Gaussians",
    "volume": "main",
    "checked": true,
    "id": "0ce6a7f1b086bb06b5ae3b287db459721499d03e",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v18/16-532.html": {
    "title": "The DFS Fused Lasso: Linear-Time Denoising over General Graphs",
    "abstract": "The fused lasso, also known as (anisotropic) total variation denoising, is widely used for piecewise constant signal estimation with respect to a given undirected graph. The fused lasso estimate is highly nontrivial to compute when the underlying graph is large and has an arbitrary structure. But for a special graph structure, namely, the chain graph, the fused lasso---or simply, 1d fused lasso---can be computed in linear time. In this paper, we revisit a result recently established in the online classification literature (Herbster et al., 2009; Cesa-Bianchi et al., 2013) and show that it has important implications for signal denoising on graphs. The result can be translated to our setting as follows. Given a general graph, if we run the standard depth-first search (DFS) traversal algorithm, then the total variation of any signal over the chain graph induced by DFS is no more than twice its total variation over the original graph. This result leads to several interesting theoretical and computational conclusions. Letting $m$ and $n$ denote the number of edges and nodes, respectively, of the graph in consideration, it implies that for an underlying signal with total variation $t$ over the graph, the fused lasso (properly tuned) achieves a mean squared error rate of $t^{2/3} n^{-2/3}$. Moreover, precisely the same mean squared error rate is achieved by running the 1d fused lasso on the DFS-induced chain graph. Importantly, the latter estimator is simple and computationally cheap, requiring $O(m)$ operations to construct the DFS-induced chain and $O(n)$ operations to compute the 1d fused lasso solution over this chain. Further, for trees that have bounded maximum degree, the error rate of $t^{2/3} n^{-2/3}$ cannot be improved, in the sense that it is the minimax rate for signals that have total variation $t$ over the tree. Finally, several related results also hold---for example, the analogous result holds for a roughness measure defined by the $\\ell_0$ norm of differences across edges in place of the total variation metric",
    "volume": "main",
    "checked": true,
    "id": "4443a96f79cfbd5bc4170f84db4fd6f7df6edaab",
    "citation_count": 54
  },
  "https://jmlr.org/papers/v18/16-480.html": {
    "title": "Community Detection and Stochastic Block Models: Recent Developments",
    "abstract": "The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences. This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten- Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds. The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed",
    "volume": "main",
    "checked": true,
    "id": "41f6bbf03e9c2bf1bb2e3cea442a0f08b6d807d0",
    "citation_count": 887
  },
  "https://jmlr.org/papers/v18/16-587.html": {
    "title": "On $b$-bit Min-wise Hashing for Large-scale Regression and Classification with Sparse Data",
    "abstract": "Large-scale regression problems where both the number of variables, $p$, and the number of observations, $n$, may be large and in the order of millions or more, are becoming increasingly more common. Typically the data are sparse: only a fraction of a percent of the entries in the design matrix are non-zero. Nevertheless, often the only computationally feasible approach is to perform dimension reduction to obtain a new design matrix with far fewer columns and then work with this compressed data. $b$-bit min-wise hashing (Li and KÃÂ¶nig, 2011; Li et al., 2011) is a promising dimension reduction scheme for sparse matrices which produces a set of random features such that regression on the resulting design matrix approximates a kernel regression with the resemblance kernel. In this work, we derive bounds on the prediction error of such regressions. For both linear and logistic models, we show that the average prediction error vanishes asymptotically as long as $q \\|\\boldsymbol{\\beta}^*\\|_2^2 /n \\rightarrow 0$, where $q$ is the average number of non-zero entries in each row of the design matrix and $\\boldsymbol{\\beta}^*$ is the coefficient of the linear predictor. We also show that ordinary least squares or ridge regression applied to the reduced data can in fact allow us fit more flexible models. We obtain non-asymptotic prediction error bounds for interaction models and for models where an unknown row normalisation must be applied in order for the signal to be linear in the predictors",
    "volume": "main",
    "checked": true,
    "id": "7f3dfc6c1d6049714c4934369d7aecd06eea78ef",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/17-078.html": {
    "title": "Efficient Learning with a Family of Nonconvex Regularizers by Redistributing Nonconvexity",
    "abstract": "The use of convex regularizers allows for easy optimization, though they often produce biased estimation and inferior prediction performance. Recently, nonconvex regularizers have attracted a lot of attention and outperformed convex ones. However, the resultant optimization problem is much harder. In this paper, a popular subclass of $\\ell_1$-based nonconvex sparsity-inducing and low-rank regularizers is considered. This includes nonconvex variants of lasso, sparse group lasso, tree- structured lasso, nuclear norm and total variation regularizers. We propose to move the nonconvexity from the regularizer to the loss. The nonconvex regularizer is then transformed to a familiar convex one, while the resultant loss function can still be guaranteed to be smooth. Learning with the convexified regularizer can be performed by existing efficient algorithms originally designed for convex regularizers (such as the proximal algorithm, Frank-Wolfe algorithm, alternating direction method of multipliers and stochastic gradient descent). This is further extended to consider cases where the convexified regularizer does not have a closed-form proximal step, and when the loss function is nonconvex nonsmooth. Extensive experiments on a variety of machine learning application scenarios show that optimizing the transformed problem is much faster than running the state-of-the-art on the original problem",
    "volume": "main",
    "checked": true,
    "id": "9b71b0a5b177ded248381dba4430944886ebb3e3",
    "citation_count": 46
  },
  "https://jmlr.org/papers/v18/17-380.html": {
    "title": "Mode-Seeking Clustering and Density Ridge Estimation via Direct Estimation of Density-Derivative-Ratios",
    "abstract": "Modes and ridges of the probability density function behind observed data are useful geometric features. Mode-seeking clustering assigns cluster labels by associating data samples with the nearest modes, and estimation of density ridges enables us to find lower-dimensional structures hidden in data. A key technical challenge both in mode-seeking clustering and density ridge estimation is accurate estimation of the ratios of the first- and second-order density derivatives to the density. A naive approach takes a three-step approach of first estimating the data density, then computing its derivatives, and finally taking their ratios. However, this three-step approach can be unreliable because a good density estimator does not necessarily mean a good density derivative estimator, and division by the estimated density could significantly magnify the estimation error. To cope with these problems, we propose a novel estimator for the density-derivative-ratios. The proposed estimator does not involve density estimation, but rather directly approximates the ratios of density derivatives of any order. Moreover, we establish a convergence rate of the proposed estimator. Based on the proposed estimator, novel methods both for mode-seeking clustering and density ridge estimation are developed, and the respective convergence rates to the mode and ridge of the underlying density are also established. Finally, we experimentally demonstrate that the developed methods significantly outperform existing methods, particularly for relatively high-dimensional data",
    "volume": "main",
    "checked": true,
    "id": "738da70f6b1fb3657c7a0ae6e9742fe035be3a9c",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/17-269.html": {
    "title": "To Tune or Not to Tune the Number of Trees in Random Forest",
    "abstract": "The number of trees $T$ in the random forest (RF) algorithm for supervised learning has to be set by the user. It is unclear whether $T$ should simply be set to the largest computationally manageable value or whether a smaller $T$ may be sufficient or in some cases even better. While the principle underlying bagging is that more trees are better, in practice the classification error rate sometimes reaches a minimum before increasing again for increasing number of trees. The goal of this paper is four-fold: (i) providing theoretical results showing that the expected error rate may be a non-monotonous function of the number of trees and explaining under which circumstances this happens; (ii) providing theoretical results showing that such non-monotonous patterns cannot be observed for other performance measures such as the Brier score and the logarithmic loss (for classification) and the mean squared error (for regression); (iii) illustrating the extent of the problem through an application to a large number (n = 306) of datasets from the public database OpenML; (iv) finally arguing in favor of setting $T$ to a computationally feasible large number as long as classical error measures based on average loss are considered",
    "volume": "main",
    "checked": false,
    "id": "d25de110afec3131a87c2342af7bba50e5e2cc17",
    "citation_count": 259
  },
  "https://jmlr.org/papers/v18/17-343.html": {
    "title": "Divide-and-Conquer for Debiased $l_1$-norm Support Vector Machine in Ultra-high Dimensions",
    "abstract": "$1$-norm support vector machine (SVM) generally has competitive performance compared to standard $2$-norm support vector machine in classification problems, with the advantage of automatically selecting relevant features. We propose a divide-and-conquer approach in the large sample size and high-dimensional setting by splitting the data set across multiple machines, and then averaging the debiased estimators. Extension of existing theoretical studies to SVM is challenging in estimation of the inverse Hessian matrix that requires approximating the Dirac delta function via smoothing. We show that under appropriate conditions the aggregated estimator can obtain the same convergence rate as the central estimator utilizing all observations",
    "volume": "main",
    "checked": false,
    "id": "be50c4fe10616af1b39645eef8d7010c72957cea",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/17-364.html": {
    "title": "Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial Multi-armed Bandits",
    "abstract": "Recent work on follow the perturbed leader (FTPL) algorithms for the adversarial multi-armed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations. Assuming that the hazard rate is bounded, it is possible to provide regret analyses for a variety of FTPL algorithms for the multi-armed bandit problem. This paper pushes the inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate condition. There are good reasons to do so: natural distributions such as the uniform and Gaussian violate the condition. We give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition. We also disprove a conjecture that the Gaussian distribution cannot lead to a low-regret algorithm. In fact, it turns out that it leads to near optimal regret, up to logarithmic factors. A key ingredient in our approach is the introduction of a new notion called the generalized hazard rate",
    "volume": "main",
    "checked": true,
    "id": "bc56c74bc3c6111745dcf7542450393be2421c2e",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v18/17-157.html": {
    "title": "On Faster Convergence of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization",
    "abstract": "The cyclic block coordinate descent-type (CBCD-type) methods, which perform iterative updates for a few coordinates (a block) simultaneously throughout the procedure, have shown remarkable computational performance for solving strongly convex minimization problems. Typical applications include many popular statistical machine learning methods such as elastic-net regression, ridge penalized logistic regression, and sparse additive regression. Existing optimization literature has shown that for strongly convex minimization, the CBCD-type methods attain iteration complexity of $\\mathcal{O}(p\\log(1/\\epsilon))$, where $\\epsilon$ is a pre-specified accuracy of the objective value, and $p$ is the number of blocks. However, such iteration complexity explicitly depends on $p$, and therefore is at least $p$ times worse than the complexity $\\mathcal{O}(\\log(1/\\epsilon))$ of gradient descent (GD) methods. To bridge this theoretical gap, we propose an improved convergence analysis for the CBCD-type methods. In particular, we first show that for a family of quadratic minimization problems, the iteration complexity $\\mathcal{O}(\\log^2(p)\\cdot\\log(1/\\epsilon))$ of the CBCD-type methods matches that of the GD methods in term of dependency on $p$, up to a $\\log^2 p$ factor. Thus our complexity bounds are sharper than the existing bounds by at least a factor of $p/\\log^2(p)$. We also provide a lower bound to confirm that our improved complexity bounds are tight (up to a $\\log^2 (p)$ factor), under the assumption that the largest and smallest eigenvalues of the Hessian matrix do not scale with $p$. Finally, we generalize our analysis to other strongly convex minimization problems beyond quadratic ones",
    "volume": "main",
    "checked": true,
    "id": "486956c45fa19b2d0d794501ec296f607ed1eeb5",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v18/16-558.html": {
    "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
    "abstract": "Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, ÃÂ¸uralg , for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare ÃÂ¸uralg with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that ÃÂ¸uralg can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems",
    "volume": "main",
    "checked": true,
    "id": "892f9a2f69241feec647856cd26bed37e04fd747",
    "citation_count": 1602
  },
  "https://jmlr.org/papers/v18/17-297.html": {
    "title": "Submatrix localization via message passing",
    "abstract": "The principal submatrix localization problem deals with recovering a $K\\times K$ principal submatrix of elevated mean $\\mu$ in a large $n\\times n$ symmetric matrix subject to additive standard Gaussian noise, or more generally, mean zero, variance one, subgaussian noise. This problem serves as a prototypical example for community detection, in which the community corresponds to the support of the submatrix. The main result of this paper is that in the regime $\\Omega(\\sqrt{n}) \\leq K \\leq o(n)$, the support of the submatrix can be weakly recovered (with $o(K)$ misclassification errors on average) by an optimized message passing algorithm if $\\lambda = \\mu^2K^2/n$, the signal-to-noise ratio, exceeds $1/e$. This extends a result by Deshpande and Montanari previously obtained for $K=\\Theta(\\sqrt{n})$ and $\\mu=\\Theta(1).$ In addition, the algorithm can be combined with a voting procedure to achieve the information-theoretic limit of exact recovery with sharp constants for all $K \\geq \\frac{n}{\\log n} (\\frac{1}{8e} + o(1))$. The total running time of the algorithm is $O(n^2\\log n)$. Another version of the submatrix localization problem, known as noisy biclustering, aims to recover a $K_1\\times K_2$ submatrix of elevated mean $\\mu$ in a large $n_1\\times n_2$ Gaussian matrix. The optimized message passing algorithm and its analysis are adapted to the bicluster problem assuming $\\Omega(\\sqrt{n_i}) \\leq K_i \\leq o(n_i)$ and $K_1\\asymp K_2.$ A sharp information-theoretic condition for the weak recovery of both clusters is also identified",
    "volume": "main",
    "checked": true,
    "id": "168cf8023d7a934d2c264a755de668235373e16f",
    "citation_count": 22
  },
  "https://jmlr.org/papers/v18/16-456.html": {
    "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations",
    "abstract": "We introduce a method to train Quantized Neural Networks (QNNs) --- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At train-time the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves $51\\%$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online",
    "volume": "main",
    "checked": true,
    "id": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f",
    "citation_count": 1552
  },
  "https://jmlr.org/papers/v18/17-377.html": {
    "title": "Significance-based community detection in weighted networks",
    "abstract": "Community detection is the process of grouping strongly connected nodes in a network. Many community detection methods for un-weighted networks have a theoretical basis in a null model. Communities discovered by these methods therefore have interpretations in terms of statistical significance. In this paper, we introduce a null for weighted networks called the continuous configuration model. First, we propose a community extraction algorithm for weighted networks which incorporates iterative hypothesis testing under the null. We prove a central limit theorem for edge-weight sums and asymptotic consistency of the algorithm under a weighted stochastic block model. We then incorporate the algorithm in a community detection method called CCME. To benchmark the method, we provide a simulation framework involving the null to plant Ã¢ÂÂbackground\" nodes in weighted networks with communities. We show that the empirical performance of CCME on these simulations is competitive with existing methods, particularly when overlapping communities and background nodes are present. To further validate the method, we present two real-world networks with potential background nodes and analyze them with CCME, yielding results that reveal macro- features of the corresponding systems",
    "volume": "main",
    "checked": true,
    "id": "522ff723cfebc7c8153cb876f39d03be2746e43a",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/17-317.html": {
    "title": "Kernel Method for Persistence Diagrams via Kernel Embedding and Weight Factor",
    "abstract": "Topological data analysis (TDA) is an emerging mathematical concept for characterizing shapes in complicated data. In TDA, persistence diagrams are widely recognized as a useful descriptor of data, distinguishing robust and noisy topological properties. This paper introduces a kernel method for persistence diagrams to develop a statistical framework in TDA. The proposed kernel is stable under perturbation of data, enables one to explicitly control the effect of persistence by a weight function, and allows an efficient and accurate approximate computation. The method is applied into practical data on granular systems, oxide glasses and proteins, showing advantages of our method compared to other relevant methods for persistence diagrams",
    "volume": "main",
    "checked": true,
    "id": "87f3028a5cda86db2664b046f7aa587dbfda60cb",
    "citation_count": 70
  },
  "https://jmlr.org/papers/v18/17-228.html": {
    "title": "Pycobra: A Python Toolbox for Ensemble Learning and Visualisation",
    "abstract": "We introduce pycobra, a Python library devoted to ensemble learning (regression and classification) and visualisation. Its main assets are the implementation of several ensemble learning algorithms, a flexible and generic interface to compare and blend any existing machine learning algorithm available in Python libraries (as long as a predict method is given), and visualisation tools such as Voronoi tessellations. pycobra is fully scikit-learn compatible and is released under the MIT open-source license. pycobra can be downloaded from the Python Package Index (PyPi) and Machine Learning Open Source Software (MLOSS). The current version (along with Jupyter notebooks, extensive documentation, and continuous integration tests) is available at https://github.com/bhargavvader/pycobra and official documentation website is https://modal.lille.inria.fr/pycobra",
    "volume": "main",
    "checked": true,
    "id": "5deb9b967302c2541fe9322773615aa813cd8861",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/16-087.html": {
    "title": "KELP: a Kernel-based Learning Platform",
    "abstract": "KELP is a Java framework that enables fast and easy implementation of kernel functions over discrete data, such as strings, trees or graphs and their combination with standard vectorial kernels. Additionally, it provides several kernel- based algorithms, e.g., online and batch kernel machines for classification, regression and clustering, and a Java environment for easy implementation of new algorithms. KELP is a versatile toolkit, very appealing both to experts and practitioners of machine learning and Java language programming, who can find extensive documentation, tutorials and examples of increasing complexity on the accompanying website. Interestingly, KELP can be also used without any knowledge of Java programming through command line tools and JSON/XML interfaces enabling the declaration and instantiation of articulated learning models using simple templates. Finally, the extensive use of modularity and interfaces in KELP enables developers to easily extend it with their own kernels and algorithms",
    "volume": "main",
    "checked": true,
    "id": "023baa8fdcc3c5269226d328695907ad7ad35f12",
    "citation_count": 35
  },
  "https://jmlr.org/papers/v18/17-284.html": {
    "title": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants",
    "abstract": "We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data",
    "volume": "main",
    "checked": true,
    "id": "46325ef55d59f5b28fec01ffacdeca609158d0d1",
    "citation_count": 71
  },
  "https://jmlr.org/papers/v18/17-234.html": {
    "title": "Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research",
    "abstract": "This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature",
    "volume": "main",
    "checked": true,
    "id": "7e6ce5bd878f24839ba38fe8fb91f30aab3cd863",
    "citation_count": 124
  },
  "https://jmlr.org/papers/v18/17-563.html": {
    "title": "Enhancing Identification of Causal Effects by Pruning",
    "abstract": "Causal models communicate our assumptions about causes and effects in real-world phenomena. Often the interest lies in the identification of the effect of an action which means deriving an expression from the observed probability distribution for the interventional distribution resulting from the action. In many cases an identifiability algorithm may return a complicated expression that contains variables that are in fact unnecessary. In practice this can lead to additional computational burden and increased bias or inefficiency of estimates when dealing with measurement error or missing data. We present graphical criteria to detect variables which are redundant in identifying causal effects. We also provide an improved version of a well-known identifiability algorithm that implements these criteria",
    "volume": "main",
    "checked": true,
    "id": "463a035b6437ae0d572355b40e6edc2e4727008a",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v18/16-499.html": {
    "title": "Active Nearest-Neighbor Learning in Metric Spaces",
    "abstract": "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure",
    "volume": "main",
    "checked": true,
    "id": "f4902d60dd70aaf41862615a1ed97a8a36be56a2",
    "citation_count": 34
  },
  "https://jmlr.org/papers/v18/17-073.html": {
    "title": "From Predictive Methods to Missing Data Imputation: An Optimization Approach",
    "abstract": "Missing data is a common problem in real-world settings and for this reason has attracted significant attention in the statistical literature. We propose a flexible framework based on formal optimization to impute missing data with mixed continuous and categorical variables. This framework can readily incorporate various predictive models including $K$-nearest neighbors, support vector machines, and decision tree based methods, and can be adapted for multiple imputation. We derive fast first-order methods that obtain high quality solutions in seconds following a general imputation algorithm opt.impute presented in this paper. We demonstrate that our proposed method improves out-of-sample accuracy in large-scale computational experiments across a sample of 84 data sets taken from the UCI Machine Learning Repository. In all scenarios of missing at random mechanisms and various missing percentages, opt.impute produces the best overall imputation in most data sets benchmarked against five other methods: mean impute, $K$-nearest neighbors, iterative knn, Bayesian PCA, and predictive-mean matching, with an average reduction in mean absolute error of 8.3$\\%$ against the best cross-validated benchmark method. Moreover, opt.impute leads to improved out-of-sample performance of learning algorithms trained using the imputed data, demonstrated by computational experiments on 10 downstream tasks. For models trained using opt.impute single imputations with 50$\\%$ data missing, the average out-of- sample $R^2$ is 0.339 in the regression tasks and the average out-of-sample accuracy is 86.1$\\%$ in the classification tasks, compared to 0.315 and 84.4$\\%$ for the best cross-validated benchmark method. In the multiple imputation setting, downstream models trained using opt.impute obtain a statistically significant improvement over models trained using multivariate imputation by chained equations (mice) in 8/10 missing data scenarios considered",
    "volume": "main",
    "checked": true,
    "id": "b6f1be7f52d9f3dfaf5bb7ad726958e5d0ac92e9",
    "citation_count": 187
  },
  "https://jmlr.org/papers/v18/17-178.html": {
    "title": "Saturating Splines and Feature Selection",
    "abstract": "We extend the adaptive regression spline model by incorporating saturation, the natural requirement that a function extend as a constant outside a certain range. We fit saturating splines to data via a convex optimization problem over a space of measures, which we solve using an efficient algorithm based on the conditional gradient method. Unlike many existing approaches, our algorithm solves the original infinite- dimensional (for splines of degree at least two) optimization problem without pre-specified knot locations. We then adapt our algorithm to fit generalized additive models with saturating splines as coordinate functions and show that the saturation requirement allows our model to simultaneously perform feature selection and nonlinear function fitting. Finally, we briefly sketch how the method can be extended to higher order splines and to different requirements on the extension outside the data range",
    "volume": "main",
    "checked": true,
    "id": "f92ec07d952263fd5b105ff6d4c18c3d57e455a4",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v18/17-347.html": {
    "title": "Nonasymptotic convergence of stochastic proximal point methods for constrained convex optimization",
    "abstract": "A popular approach for solving stochastic optimization problems is the stochastic gradient descent (SGD) method. Although the SGD iteration is computationally cheap and its practical performance may be satisfactory under certain circumstances, there is recent evidence of its convergence difficulties and instability for unappropriate choice of parameters. To avoid some of the drawbacks of SGD, stochastic proximal point (SPP) algorithms have been recently considered. We introduce a new variant of the SPP method for solving stochastic convex problems subject to (in)finite intersection of constraints satisfying a linear regularity condition. For the newly introduced SPP scheme we prove new nonasymptotic convergence results. In particular, for convex Lipschitz continuous objective functions, we prove nonasymptotic convergence rates in terms of the expected value function gap of order $\\mathcal{O}\\left(\\frac{1}{k^{1/2}}\\right)$, where $k$ is the iteration counter. We also derive better nonasymptotic convergence rates in terms of expected quadratic distance from the iterates to the optimal solution for smooth strongly convex objective functions, which in the best case is of order $\\mathcal{O}\\left(\\frac{1}{k}\\right)$. Since these convergence rates can be attained by our SPP algorithm only under some natural restrictions on the stepsize, we also introduce a restarting variant of SPP that overcomes these difficulties and derive the corresponding nonasymptotic convergence rates. Numerical evidence supports the effectiveness of our methods in real problems",
    "volume": "main",
    "checked": true,
    "id": "5b9e83af8c74f834420e68406e3a378062a0c57f",
    "citation_count": 74
  },
  "https://jmlr.org/papers/v18/16-206.html": {
    "title": "Simple, Robust and Optimal Ranking from Pairwise Comparisons",
    "abstract": "We consider data in the form of pairwise comparisons of $n$ items, with the goal of identifying the top $k$ items for some value of $k < n$, or alternatively, recovering a ranking of all the items. We analyze the Borda counting algorithm that ranks the items in order of the number of pairwise comparisons won, and show it has three attractive features: (a) it is an optimal method achieving the information-theoretic limits up to constant factors; (b) it is robust in that its optimality holds without imposing conditions on the underlying matrix of pairwise-comparison probabilities, in contrast to some prior work that applies only to the BTL parametric model; and (c) its computational efficiency leads to speed-ups of several orders of magnitude. We address the problem of exact recovery, and for the top-$k$ recovery problem we also extend our results to obtain sharp guarantees for approximate recovery under the Hamming distortion metric, and more generally, to any arbitrary error requirement that satisfies a simple and natural monotonicity condition. In doing so, we introduce a general framework that allows us to treat a variety of problems in the literature in an unified manner",
    "volume": "main",
    "checked": true,
    "id": "f79361dda56ee755fc56ab83cf0d9f12d42b2d5e",
    "citation_count": 141
  },
  "https://jmlr.org/papers/v18/16-549.html": {
    "title": "Surprising properties of dropout in deep networks",
    "abstract": "We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress",
    "volume": "main",
    "checked": true,
    "id": "73b36c2a58ea9e2c156b08f09721414e573d22ab",
    "citation_count": 33
  },
  "https://jmlr.org/papers/v18/16-256.html": {
    "title": "Exact Learning of Lightweight Description Logic Ontologies",
    "abstract": "We study the problem of learning description logic (DL) ontologies in Angluin et al.'s framework of exact learning via queries. We admit membership queries (Ã¢ÂÂis a given subsumption entailed by the target ontology?Ã¢ÂÂ) and equivalence queries (Ã¢ÂÂis a given ontology equivalent to the target ontology?Ã¢ÂÂ). We present three main results: (1) ontologies formulated in (two relevant versions of) the description logic DL-Lite can be learned with polynomially many queries of polynomial size; (2) this is not the case for ontologies formulated in the description logic $\\mathcal{E}\\mathcal{L}$, even when only acyclic ontologies are admitted; and (3) ontologies formulated in a fragment of $\\mathcal{E}\\mathcal{L}$ related to the web ontology language OWL 2 RL can be learned in polynomial time. We also show that neither membership nor equivalence queries alone are sufficient in cases (1) and (3)",
    "volume": "main",
    "checked": true,
    "id": "293f3e37edf1174eec171c9240fccea3f45c405f",
    "citation_count": 39
  },
  "https://jmlr.org/papers/v18/17-159.html": {
    "title": "Sparse Concordance-assisted Learning for Optimal Treatment Decision",
    "abstract": "To find optimal decision rule, Fan et al. (2016) proposed an innovative concordance-assisted learning algorithm which is based on maximum rank correlation estimator. It makes better use of the available information through pairwise comparison. However the objective function is discontinuous and computationally hard to optimize. In this paper, we consider a convex surrogate loss function to solve this problem. In addition, our algorithm ensures sparsity of decision rule and renders easy interpretation. We derive the $L_2$ error bound of the estimated coefficients under ultra-high dimension. Simulation results of various settings and application to STAR*D both illustrate that the proposed method can still estimate optimal treatment regime successfully when the number of covariates is large",
    "volume": "main",
    "checked": true,
    "id": "94e2175d11108bca97212fcbebab3cf06f88942a",
    "citation_count": 26
  },
  "https://jmlr.org/papers/v18/17-145.html": {
    "title": "Post-Regularization Inference for Time-Varying Nonparanormal Graphical Models",
    "abstract": "We propose a novel class of time-varying nonparanormal graphical models, which allows us to model high dimensional heavy-tailed systems and the evolution of their latent network structures. Under this model we develop statistical tests for presence of edges both locally at a fixed index value and globally over a range of values. The tests are developed for a high-dimensional regime, are robust to model selection mistakes and do not require commonly assumed minimum signal strength. The testing procedures are based on a high dimensional, debiasing-free moment estimator, which uses a novel kernel smoothed Kendall's tau correlation matrix as an input statistic. The estimator consistently estimates the latent inverse Pearson correlation matrix uniformly in both the index variable and kernel bandwidth. Its rate of convergence is shown to be minimax optimal. Our method is supported by thorough numerical simulations and an application to a neural imaging data set",
    "volume": "main",
    "checked": true,
    "id": "3756325b07829a4b8957c07a0fb4c1af989e6727",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/17-409.html": {
    "title": "Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression",
    "abstract": "To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, increasing robustness, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and performance",
    "volume": "main",
    "checked": true,
    "id": "cbf3f558f1edd43853604491f6a0f9121d8d7d01",
    "citation_count": 9
  },
  "https://jmlr.org/papers/v18/17-416.html": {
    "title": "Steering Social Activity: A Stochastic Optimal Control Point Of View",
    "abstract": "User engagement in online social networking depends critically on the level of social activity in the corresponding platform---the number of online actions, such as posts, shares or replies, taken by their users. Can we design data-driven algorithms to increase social activity? At a user level, such algorithms may increase activity by helping users decide when to take an action to be more likely to be noticed by their peers. At a network level, they may increase activity by incentivizing a few influential users to take more actions, which in turn will trigger additional actions by other users. In this paper, we model social activity using the framework of marked temporal point processes, derive an alternate representation of these processes using stochastic differential equations (SDEs) with jumps and, exploiting this alternate representation, develop two efficient online algorithms with provable guarantees to steer social activity both at a user and at a network level. In doing so, we establish a previously unexplored connection between optimal control of jump SDEs and doubly stochastic marked temporal point processes, which is of independent interest. Finally, we experiment both with synthetic and real data gathered from Twitter and show that our algorithms consistently steer social activity more effectively than the state of the art",
    "volume": "main",
    "checked": true,
    "id": "e3db80d03afe62bd0d174c33492f08a49d23bc4e",
    "citation_count": 30
  },
  "https://jmlr.org/papers/v18/16-483.html": {
    "title": "The Search Problem in Mixture Models",
    "abstract": "We consider the task of learning the parameters of a  single component of a mixture model, for the case when we are given side information about that component; we call this the Ã¢ÂÂsearch problem\" in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components. Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain parameter estimates with greater accuracy, and also improved computation complexity than existing moment based mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real data sets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy",
    "volume": "main",
    "checked": true,
    "id": "6af4785e872029b349be27245d29fb027827fa0c",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v18/16-140.html": {
    "title": "An $\\ell_{\\infty}$ Eigenvector Perturbation Bound and Its Application",
    "abstract": "In statistics and machine learning, we are interested in the eigenvectors (or singular vectors) of certain matrices (e.g. covariance matrices, data matrices, etc). However, those matrices are usually perturbed by noises or statistical errors, either from random sampling or structural patterns. The Davis- Kahan $\\sin \\theta$ theorem is often used to bound the difference between the eigenvectors of a matrix $A$ and those of a perturbed matrix $\\widetilde{A} = A + E$, in terms of $\\ell_2$ norm. In this paper, we prove that when $A$ is a low-rank and incoherent matrix, the $\\ell_{\\infty}$ norm perturbation bound of singular vectors (or eigenvectors in the symmetric case) is smaller by a factor of $\\sqrt{d_1}$ or $\\sqrt{d_2}$ for left and right vectors, where $d_1$ and $d_2$ are the matrix dimensions. The power of this new perturbation result is shown in robust covariance estimation, particularly when random variables have heavy tails. There, we propose new robust covariance estimators and establish their asymptotic properties using the newly developed perturbation bound. Our theoretical results are verified through extensive numerical experiments",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v18/16-299.html": {
    "title": "A Tight Bound of Hard Thresholding",
    "abstract": "This paper is concerned with the hard thresholding operator which sets all but the $k$ largest absolute elements of a vector to zero. We establish a tight bound to quantitatively characterize the deviation of the thresholded solution from a given signal. Our theoretical result is universal in the sense that it holds for all choices of parameters, and the underlying analysis depends only on fundamental arguments in mathematical optimization. We discuss the implications for two domains: Compressed Sensing. On account of the crucial estimate, we bridge the connection between the restricted isometry property (RIP) and the sparsity parameter for a vast volume of hard thresholding based algorithms, which renders an improvement on the RIP condition especially when the true sparsity is unknown. This suggests that in essence, many more kinds of sensing matrices or fewer measurements are admissible for the data acquisition procedure. Machine Learning. In terms of large-scale machine learning, a significant yet challenging problem is learning accurate sparse models in an efficient manner. In stark contrast to prior work that attempted the $\\ell_1$-relaxation for promoting sparsity, we present a novel stochastic algorithm which performs hard thresholding in each iteration, hence ensuring such parsimonious solutions. Equipped with the developed bound, we prove the {\\em global linear convergence} for a number of prevalent statistical models under mild assumptions, even though the problem turns out to be non-convex",
    "volume": "main",
    "checked": true,
    "id": "8f029eada934e433a4f2bd6720e18f7d0930bc0a",
    "citation_count": 79
  },
  "https://jmlr.org/papers/v18/16-486.html": {
    "title": "Estimation of Graphical Models through Structured Norm Minimization",
    "abstract": "Estimation of Markov Random Field and covariance models from high-dimensional data represents a canonical problem that has received a lot of attention in the literature. A key assumption, widely employed, is that of sparsity of the underlying model. In this paper, we study the problem of estimating such models exhibiting a more intricate structure comprising simultaneously of sparse, structured sparse and dense components. Such structures naturally arise in several scientific fields, including molecular biology, finance and political science. We introduce a general framework based on a novel structured norm that enables us to estimate such complex structures from high-dimensional data. The resulting optimization problem is convex and we introduce a linearized multi-block alternating direction method of multipliers (ADMM) algorithm to solve it efficiently. We illustrate the superior performance of the proposed framework on a number of synthetic data sets generated from both random and structured networks. Further, we apply the method to a number of real data sets and discuss the results",
    "volume": "main",
    "checked": true,
    "id": "e53ef762bd887def2750aec863cf88e1d75a9a73",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/16-421.html": {
    "title": "Sparse Exchangeable Graphs and Their Limits via Graphon Processes",
    "abstract": "In a recent paper, Caron and Fox suggest a probabilistic model for sparse graphs which are exchangeable when associating each vertex with a time parameter in $\\mathbb{R}_+$. Here we show that by generalizing the classical definition of graphons as functions over probability spaces to functions over $\\sigma$-finite measure spaces, we can model a large family of exchangeable graphs, including the Caron-Fox graphs and the traditional exchangeable dense graphs as special cases. Explicitly, modelling the underlying space of features by a $\\sigma$-finite measure space $(S,\\mathcal{S},\\mu)$ and the connection probabilities by an integrable function $W\\colon S\\times S\\to [0,1]$, we construct a random family $(G_t)_{t\\geq 0}$ of growing graphs such that the vertices of $G_t$ are given by a Poisson point process on $S$ with intensity $t\\mu$, with two points $x,y$ of the point process connected with probability $W(x,y)$. We call such a random family a graphon process. We prove that a graphon process has convergent subgraph frequencies (with possibly infinite limits) and that, in the natural extension of the cut metric to our setting, the sequence converges to the generating graphon. We also show that the underlying graphon is identifiable only as an equivalence class over graphons with cut distance zero. More generally, we study metric convergence for arbitrary (not necessarily random) sequences of graphs, and show that a sequence of graphs has a convergent subsequence if and only if it has a subsequence satisfying a property we call uniform regularity of tails. Finally, we prove that every graphon is equivalent to a graphon on $\\mathbb{R}_+$ equipped with Lebesgue measure",
    "volume": "main",
    "checked": true,
    "id": "878afff9b207bba6b46968af5e65aa92c19ed083",
    "citation_count": 96
  },
  "https://jmlr.org/papers/v18/17-044.html": {
    "title": "Weighted SGD for $\\ell_p$ Regression with Randomized Preconditioning",
    "abstract": "In recent years, stochastic gradient descent (SGD) methods and randomized linear algebra (RLA) algorithms have been applied to many large-scale problems in machine learning and data analysis. SGD methods are easy to implement and applicable to a wide range of convex optimization problems. In contrast, RLA algorithms provide much stronger performance guarantees but are applicable to a narrower class of problems. We aim to bridge the gap between these two methods in solving constrained overdetermined linear regression problems---e.g., $\\ell_2$ and $\\ell_1$ regression problems",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v18/17-748.html": {
    "title": "Catalyst Acceleration for First-order Convex Optimization: from Theory to Practice",
    "abstract": "We introduce a generic scheme for accelerating gradient-based optimization methods in the sense of Nesterov. The approach, called Catalyst, builds upon the inexact accelerated proximal point algorithm for minimizing a convex objective function, and consists of approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. One of the keys to achieve acceleration in theory and in practice is to solve these sub-problems with appropriate accuracy by using the right stopping criterion and the right warm-start strategy. We give practical guidelines to use Catalyst and present a comprehensive analysis of its global complexity. We show that Catalyst applies to a large class of algorithms, including gradient descent, block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG, MISO/Finito, and their proximal variants. For all of these methods, we establish faster rates using the Catalyst acceleration, for strongly convex and non-strongly convex objectives. We conclude with extensive experiments showing that acceleration is useful in practice, especially for ill-conditioned problems",
    "volume": "main",
    "checked": true,
    "id": "1d7f90010b79c9d0ba1245c33b0122183d7c53a6",
    "citation_count": 119
  },
  "https://jmlr.org/papers/v18/17-398.html": {
    "title": "Gaussian Lower Bound for the Information Bottleneck Limit",
    "abstract": "The Information Bottleneck (IB) is a conceptual method for extracting the most compact, yet informative, representation of a set of variables, with respect to the target. It generalizes the notion of minimal sufficient statistics from classical parametric statistics to a broader information-theoretic sense. The IB curve defines the optimal trade-off between representation complexity and its predictive power. Specifically, it is achieved by minimizing the level of mutual information (MI) between the representation and the original variables, subject to a minimal level of MI between the representation and the target. This problem is shown to be in general NP hard. One important exception is the multivariate Gaussian case, for which the Gaussian IB (GIB) is known to obtain an analytical closed form solution, similar to Canonical Correlation Analysis (CCA). In this work we introduce a Gaussian lower bound to the IB curve; we find an embedding of the data which maximizes its Ã¢ÂÂGaussian part\", on which we apply the GIB. This embedding provides an efficient (and practical) representation of any arbitrary data-set (in the IB sense), which in addition holds the favorable properties of a Gaussian distribution. Importantly, we show that the optimal Gaussian embedding is bounded from above by non-linear CCA. This allows a fundamental limit for our ability to Gaussianize arbitrary data- sets and solve complex problems by linear methods",
    "volume": "main",
    "checked": true,
    "id": "5932781fedbdd113b9aa6b0f696880bc0fadaee7",
    "citation_count": 12
  },
  "https://jmlr.org/papers/v18/17-381.html": {
    "title": "tick: a Python Library for Statistical Learning, with an emphasis on Hawkes Processes and Time-Dependent Models",
    "abstract": "This paper introduces tick, is a statistical learning library for Python 3, with a particular emphasis on time-dependent models, such as point processes, tools for generalized linear models and survival analysis. The core of the library provides model computational classes, solvers and proximal operators for regularization. It relies on a C++ implementation and state- of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting. Source code and documentation can be downloaded from https://github.com/X-DataInitiative/tick",
    "volume": "main",
    "checked": true,
    "id": "93cf61011906392b8aba7efbdac83066d852d222",
    "citation_count": 91
  },
  "https://jmlr.org/papers/v18/17-632.html": {
    "title": "SGDLibrary: A MATLAB library for stochastic optimization algorithms",
    "abstract": "We consider the problem of finding the minimizer of a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ of the finite-sum form $\\min f(w) = 1/n\\sum_{i}^n f_i(w)$. This problem has been studied intensively in recent years in the field of machine learning (ML). One promising approach for large-scale data is to use a stochastic optimization algorithm to solve the problem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library of a collection of stochastic optimization algorithms. The purpose of the library is to provide researchers and implementers a comprehensive evaluation environment for the use of these algorithms on various ML problems",
    "volume": "main",
    "checked": true,
    "id": "af8f59a3e20277b8bc7c66f6b1f95a8a126c5382",
    "citation_count": 36
  },
  "https://jmlr.org/papers/v18/16-340.html": {
    "title": "Reward Maximization Under Uncertainty: Leveraging Side-Observations on Networks",
    "abstract": "We study the stochastic multi-armed bandit (MAB) problem in the presence of side-observations across actions that occur as a result of an underlying network structure. In our model, a bipartite graph captures the relationship between actions and a common set of unknowns such that choosing an action reveals observations for the unknowns that it is connected to. This models a common scenario in online social networks where users respond to their friends' activity, thus providing side information about each other's preferences. Our contributions are as follows: 1) We derive an asymptotic lower bound (with respect to time) as a function of the bi-partite network structure on the regret of any uniformly good policy that achieves the maximum long-term average reward. 2) We propose two policies - a randomized policy; and a policy based on the well- known upper confidence bound (UCB) policies - both of which explore each action at a rate that is a function of its network position. We show, under mild assumptions, that these policies achieve the asymptotic lower bound on the regret up to a multiplicative factor, independent of the network structure. Finally, we use numerical examples on a real-world social network and a routing example network to demonstrate the benefits obtained by our policies over other existing policies",
    "volume": "main",
    "checked": true,
    "id": "95797bfe76daebfe780db13c94643712a9d88e84",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v18/17-019.html": {
    "title": "Simultaneous Clustering and Estimation of Heterogeneous Graphical Models",
    "abstract": "We consider joint estimation of multiple graphical models arising from heterogeneous and high-dimensional observations. Unlike most previous approaches which assume that the cluster structure is given in advance, an appealing feature of our method is to learn cluster structure while estimating heterogeneous graphical models. This is achieved via a high dimensional version of Expectation Conditional Maximization (ECM) algorithm  (Meng and Rubin,  1993). A joint graphical lasso penalty is imposed on the conditional maximization step to extract both homogeneity and heterogeneity components across all clusters. Our algorithm is computationally efficient due to fast sparse learning routines and can be implemented without unsupervised learning knowledge. The superior performance of our method is demonstrated by extensive experiments and its application to a Glioblastoma cancer dataset reveals some new insights in understanding the Glioblastoma cancer. In theory, a non-asymptotic error bound is established for the output directly from our high dimensional ECM algorithm, and it consists of two quantities: statistical error (statistical accuracy) and optimization error (computational complexity). Such a result gives a theoretical guideline in terminating our ECM iterations",
    "volume": "main",
    "checked": true,
    "id": "1ff73da2c28dc2b028a4f0d53db66d2d35ba8a3b",
    "citation_count": 62
  },
  "https://jmlr.org/papers/v18/17-313.html": {
    "title": "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging",
    "abstract": "We address the statistical and optimization impacts of the classical sketch and Hessian sketch used to approximately solve the Matrix Ridge Regression (MRR) problem. Prior research has quantified the effects of classical sketch on the strictly simpler least squares regression (LSR) problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR: namely, it recovers nearly optimal solutions. By contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the Ã¢ÂÂmassÃ¢ÂÂ in the responses and the optimal objective value. For both types of approximation, the regularization in the sketched MRR problem results in significantly different statistical properties from those of the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the bias and variance of sketched MRR; these bounds show that classical sketch significantly increases the variance, while Hessian sketch significantly increases the bias. Empirically, sketched MRR solutions can have risks that are higher by an order-of- magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases the gap between the risks of the true and sketched solutions to the MRR problem. Thus, in parallel or distributed settings, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the increased statistical risk incurred by sketching",
    "volume": "main",
    "checked": true,
    "id": "bdc0063ad883585d072f8462a2a3730adf1ebe87",
    "citation_count": 79
  },
  "https://jmlr.org/papers/v18/16-147.html": {
    "title": "Compact Convex Projections",
    "abstract": "We study the usefulness of conditional gradient like methods for determining projections onto convex sets, in particular, projections onto naturally arising convex sets in reproducing kernel Hilbert spaces. Our work is motivated by the recently introduced kernel herding algorithm which is closely related to the Conditional Gradient Method (CGM). It is known that the herding algorithm converges with a rate of $1/t$, where $t$ counts the number of iterations, when a point in the interior of a convex set is approximated. We generalize this result and we provide a necessary and sufficient condition for the algorithm to approximate projections with a rate of $1/t$. The CGM, which is in general vastly superior to the herding algorithm, achieves only an inferior rate of $1/\\sqrt{t}$ in this setting. We study the usefulness of such projection algorithms further by exploring ways to use these for solving concrete machine learning problems. In particular, we derive non-parametric regression algorithms which use at their core a slightly modified kernel herding algorithm to determine projections. We derive bounds to control approximation errors of these methods and we demonstrate via experiments that the developed regressors are en-par with state-of-the-art regression algorithms for large scale problems",
    "volume": "main",
    "checked": true,
    "id": "b743bd84b236625fb7c9bb36aaf5a9f41b4ab269",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v18/16-319.html": {
    "title": "Complete Graphical Characterization and Construction of Adjustment Sets in Markov Equivalence Classes of Ancestral Graphs",
    "abstract": "We present a graphical criterion for covariate adjustment that is sound and complete for four different classes of causal graphical models: directed acyclic graphs (DAGs), maximal ancestral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs), and partial ancestral graphs (PAGs). Our criterion unifies covariate adjustment for a large set of graph classes. Moreover, we define an explicit set that satisfies our criterion, if there is any set that satisfies our criterion. We also give efficient algorithms for constructing all sets that fulfill our criterion, implemented in the R package dagitty. Finally, we discuss the relationship between our criterion and other criteria for adjustment, and we provide new soundness and completeness proofs for the adjustment criterion for DAGs",
    "volume": "main",
    "checked": true,
    "id": "7e8b21f4eae979e1e8642805674acdea81905978",
    "citation_count": 107
  },
  "https://jmlr.org/papers/v18/16-410.html": {
    "title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods",
    "abstract": "Nesterov's momentum trick is famously known for accelerating gradient descent, and has been proven useful in building fast iterative algorithms. However, in the stochastic setting, counterexamples exist and prevent Nesterov's momentum from providing similar acceleration, even if the underlying problem is convex and finite-sum. We introduce $\\mathtt{Katyusha}$, a direct, primal-only stochastic gradient method to fix this issue. In convex finite-sum stochastic optimization, $\\mathtt{Katyusha}$ has an optimal accelerated convergence rate, and enjoys an optimal parallel linear speedup in the mini-batch setting. The main ingredient is $\\textit{Katyusha momentum}$, a novel Ã¢ÂÂnegative momentumÃ¢ÂÂ on top of Nesterov's momentum. It can be incorporated into a variance-reduction based algorithm and speed it up, both in terms of $\\textit{sequential and parallel}$ performance. Since variance reduction has been successfully applied to a growing list of practical problems, our paper suggests that in each of such cases, one could potentially try to give Katyusha a hug",
    "volume": "main",
    "checked": true,
    "id": "06fc893d0a70249e6350df1495755b79f0870ad0",
    "citation_count": 438
  },
  "https://jmlr.org/papers/v18/16-503.html": {
    "title": "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization",
    "abstract": "We show that the average stability notion introduced by Kearns and Ron (1999); Bousquet and Elisseeff (2002) is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We complement the recent bounds of Hardt et al. (2015) on the stability rate of the Stochastic Gradient Descent algorithm",
    "volume": "main",
    "checked": true,
    "id": "727fa9a2fa05d2ce4d4baf7b27cf099147d51087",
    "citation_count": 21
  },
  "https://jmlr.org/papers/v18/16-595.html": {
    "title": "Parallelizing Stochastic Gradient Descent for Least Squares Regression: Mini-batching, Averaging, and Model Misspecification",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "6bb5c2180948902448388ac66e1d04010c3d9965",
    "citation_count": 121
  },
  "https://jmlr.org/papers/v18/17-243.html": {
    "title": "Learning Quadratic Variance Function (QVF) DAG Models via OverDispersion Scoring (ODS)",
    "abstract": "Learning DAG or Bayesian network models is an important problem in multi-variate causal inference. However, a number of challenges arises in learning large-scale DAG models including model identifiability and computational complexity since the space of directed graphs is huge. In this paper, we address these issues in a number of steps for a broad class of DAG models where the noise or variance is signal-dependent. Firstly we introduce a new class of identifiable DAG models, where each node has a distribution where the variance is a quadratic function of the mean (QVF DAG models). Our QVF DAG models include many interesting classes of distributions such as Poisson, Binomial, Geometric, Exponential, Gamma and many other distributions in which the noise variance depends on the mean. We prove that this class of QVF DAG models is identifiable, and introduce a new algorithm, the OverDispersion Scoring (ODS) algorithm, for learning large-scale QVF DAG models. Our algorithm is based on firstly learning the moralized or undirected graphical model representation of the DAG to reduce the DAG search-space, and then exploiting the quadratic variance property to learn the ordering. We show through theoretical results and simulations that our algorithm is statistically consistent in the high-dimensional $p>n$ setting provided that the degree of the moralized graph is bounded and performs well compared to state-of-the-art DAG-learning algorithms. We also demonstrate through a real data example involving multi-variate count data, that our ODS algorithm is well-suited to estimating DAG models for count data in comparison to other methods used for discrete data",
    "volume": "main",
    "checked": true,
    "id": "e5c2d4a715fb17b78b16d09cec6e08d521528dc8",
    "citation_count": 30
  },
  "https://jmlr.org/papers/v18/17-247.html": {
    "title": "Improved spectral community detection in large heterogeneous networks",
    "abstract": "In this article, we propose and study the performance of spectral community detection for a family of Ã¢ÂÂ$\\alpha$-normalizedÃ¢ÂÂ adjacency matrices $\\bf A$, of the type $ {\\bf D}^{-\\alpha}{\\bf A}{\\bf D}^{-\\alpha}$ with $\\bf D$ the degree matrix, in heterogeneous dense graph models. We show that the previously used normalization methods based on ${\\bf A}$ or $ {\\bf D}^{-1}{\\bf A}{\\bf D}^{-1} $ are in general suboptimal in terms of correct recovery rates and, relying on advanced random matrix methods, we prove instead the existence of an optimal value $ \\alpha_{\\rm opt} $ of the parameter $ \\alpha $ in our generic model; we further provide an online estimation of $ \\alpha_{\\rm opt} $ only based on the node degrees in the graph. Numerical simulations show that the proposed method outperforms state-of-the-art spectral approaches on moderately dense to dense heterogeneous graphs",
    "volume": "main",
    "checked": true,
    "id": "3efe2c23f90ec6988c359afc8669b6f94c821a68",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/17-448.html": {
    "title": "Statistical Inference on Random Dot Product Graphs: a Survey",
    "abstract": "The random dot product graph (RDPG) is an independent-edge random graph that is analytically tractable and, simultaneously, either encompasses or can successfully approximate a wide range of random graphs, from relatively simple stochastic block models to complex latent position graphs. In this survey paper, we describe a comprehensive paradigm for statistical inference on random dot product graphs, a paradigm centered on spectral embeddings of adjacency and Laplacian matrices. We examine the graph-inferential analogues of several canonical tenets of classical Euclidean inference. In particular, we summarize a body of existing results on the consistency and asymptotic normality of the adjacency and Laplacian spectral embeddings, and the role these spectral embeddings can play in the construction of single- and multi-sample hypothesis tests for graph data. We investigate several real-world applications, including community detection and classification in large social networks and the determination of functional and biologically relevant network properties from an exploratory data analysis of the Drosophila connectome. We outline requisite background and current open problems in spectral graph inference",
    "volume": "main",
    "checked": true,
    "id": "d4697ad12ea937b4306455eb053a0a0abd247b6d",
    "citation_count": 190
  },
  "https://jmlr.org/papers/v18/17-755.html": {
    "title": "Rate of Convergence of $k$-Nearest-Neighbor Classification Rule",
    "abstract": "A binary classification problem is considered. The excess error probability of the $k$-nearest-neighbor classification rule according to the error probability of the Bayes decision is revisited by a decomposition of the excess error probability into approximation and estimation errors. Under a weak margin condition and under a modified Lipschitz condition or a local Lipschitz condition, tight upper bounds are presented such that one avoids the condition that the feature vector is bounded. The concept of modified Lipschitz condition is applied for discrete distributions, too. As a consequence of both concepts, we present the rate of convergence of $L_2$ error for the corresponding nearest neighbor regression estimate",
    "volume": "main",
    "checked": true,
    "id": "45a1e7b6d91e3b9332e84f1f7a823df455630003",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v18/16-315.html": {
    "title": "A Theory of Learning with Corrupted Labels",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "90ba5eb1021fdf05edbbcb1424c477d86dd3a216",
    "citation_count": 67
  },
  "https://jmlr.org/papers/v18/16-424.html": {
    "title": "Interactive Algorithms: Pool, Stream and Precognitive Stream",
    "abstract": "We consider interactive algorithms in the pool-based setting, and in the stream-based setting. Interactive algorithms observe suggested elements (representing actions or queries), and interactively select some of them and receive responses. Pool- based algorithms can select elements at any order, while stream- based algorithms observe elements in sequence, and can only select elements immediately after observing them. We further consider an intermediate setting, which we term precognitive stream, in which the algorithm knows in advance the identity of all the elements in the sequence, but can select them only in the order of their appearance. For all settings, we assume that the suggested elements are generated independently from some source distribution, and ask what is the stream size required for emulating a pool algorithm with a given pool size, in the stream-based setting and in the precognitive stream setting. We provide algorithms and matching lower bounds for general pool algorithms, and for utility-based pool algorithms. We further derive nearly matching upper and lower bounds on the gap between the two settings for the special case of active learning for binary classification",
    "volume": "main",
    "checked": true,
    "id": "0d2b67a5e93d53748fbac783683e388585666a25",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v18/16-512.html": {
    "title": "CoCoA: A General Framework for Communication-Efficient Distributed Optimization",
    "abstract": "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for distributed computing environments, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly-convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly-convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the- art methods, as we illustrate with an extensive set of experiments on real distributed datasets",
    "volume": "main",
    "checked": true,
    "id": "1d720f25d5d2c6dcaebfe108ead3036b7fe06940",
    "citation_count": 234
  },
  "https://jmlr.org/papers/v18/17-012.html": {
    "title": "Concentration inequalities for empirical processes of linear time series",
    "abstract": "The paper considers suprema of empirical processes for linear time series indexed by functional classes. We derive an upper bound for the tail probability of the suprema under conditions on the size of the function class, the sample size, temporal dependence and the moment conditions of the underlying time series. Due to the dependence and heavy-tailness, our tail probability bound is substantially different from those classical exponential bounds obtained under the independence assumption in that it involves an extra polynomial decaying term. We allow both short- and long-range dependent processes. For empirical processes indexed by half intervals, our tail probability inequality is sharp up to a multiplicative constant",
    "volume": "main",
    "checked": true,
    "id": "98f9e6ff032d10a23d25f4795e36779f496fe775",
    "citation_count": 12
  },
  "https://jmlr.org/papers/v18/17-445.html": {
    "title": "A Cluster Elastic Net for Multivariate Regression",
    "abstract": "We propose a method for simultaneously estimating regression coefficients and clustering response variables in a multivariate regression model, to increase prediction accuracy and give insights into the relationship between response variables. The estimates of the regression coefficients and clusters are found by using a penalized likelihood estimator, which includes a cluster fusion penalty, to shrink the difference in fitted values from responses in the same cluster, and an $L_1$ penalty for simultaneous variable selection and estimation. We propose a two-step algorithm, that iterates between k-means clustering and solving the penalized likelihood function assuming the clusters are known, which has desirable parallel computational properties obtained by using the cluster fusion penalty. If the response variable clusters are known a priori then the algorithm reduces to just solving the penalized likelihood problem. Theoretical results are presented for the penalized least squares case, including asymptotic results allowing for $p \\gg n$. We extend our method to the setting where the responses are binomial variables. We propose a coordinate descent algorithm for the normal likelihood and a proximal gradient descent algorithm for the binomial likelihood, which can easily be extended to other generalized linear model (GLM) settings. Simulations and data examples from business operations and genomics are presented to show the merits of both the least squares and binomial methods",
    "volume": "main",
    "checked": true,
    "id": "a9efac350ca4e267480b95157a71a13444c7fa06",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v18/17-492.html": {
    "title": "Characteristic and Universal Tensor Product Kernels",
    "abstract": "Maximum mean discrepancy (MMD), also called energy distance or N-distance in statistics and Hilbert-Schmidt independence criterion (HSIC), specifically distance covariance in statistics, are among the most popular and successful approaches to quantify the difference and independence of random variables, respectively. Thanks to their kernel-based foundations, MMD and HSIC are applicable on a wide variety of domains. Despite their tremendous success, quite little is known about when HSIC characterizes independence and when MMD with tensor product kernel can discriminate probability distributions. In this paper, we answer these questions by studying various notions of characteristic property of the tensor product kernel",
    "volume": "main",
    "checked": true,
    "id": "ab2de69e9ea9e376fddd74137c51d7ebc36214c4",
    "citation_count": 54
  },
  "https://jmlr.org/papers/v18/17-716.html": {
    "title": "Learning Certifiably Optimal Rule Lists for Categorical Data",
    "abstract": "We present the design and implementation of a custom discrete optimization technique for building rule lists over a categorical feature space. Our algorithm produces rule lists with optimal training performance, according to the regularized empirical risk, with a certificate of optimality. By leveraging algorithmic bounds, efficient data structures, and computational reuse, we achieve several orders of magnitude speedup in time and a massive reduction of memory consumption. We demonstrate that our approach produces optimal rule lists on practical problems in seconds. Our results indicate that it is possible to construct optimal sparse rule lists that are approximately as accurate as the COMPAS proprietary risk prediction tool on data from Broward County, Florida, but that are completely interpretable. This framework is a novel alternative to CART and other decision tree methods for interpretable modeling",
    "volume": "main",
    "checked": true,
    "id": "36ea2ee3c4d6ff388dea3c7d685e7fe2019d8911",
    "citation_count": 192
  }
}