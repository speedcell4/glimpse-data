{
  "https://jmlr.org/papers/v18/14-249.html": {
    "title": "Averaged Collapsed Variational Bayes Inference",
    "abstract": "This paper presents the Averaged CVB (ACVB) inference and offers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence- guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non- expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence",
    "volume": "main",
    "checked": true,
    "id": "0f4cfa8fea4047d20217255964169ade91547fcc",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v18/14-400.html": {
    "title": "Scalable Influence Maximization for Multiple Products in Continuous-Time Diffusion Networks",
    "abstract": "A typical viral marketing model identifies influential users in a social network to maximize a single product adoption assuming unlimited user attention, campaign budgets, and time. In reality, multiple products need campaigns, users have limited attention, convincing users incurs costs, and advertisers have limited budgets and expect the adoptions to be maximized soon. Facing these user, monetary, and timing constraints, we formulate the problem as a submodular maximization task in a continuous-time diffusion model under the intersection of one matroid and multiple knapsack constraints. We propose a randomized algorithm estimating the user influence (Partial results in the paper on influence estimation have been published in a conference paper: Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha. Scalable influence estimation in continuous time diffusion networks. In Advances in Neural Information Processing Systems 26, 2013.) in a network ($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$ with $n=\\mathcal{O}(1/\\epsilon^2)$ randomizations and $\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By exploiting the influence estimation algorithm as a subroutine, we develop an adaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2 k)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active. Extensive experiments on networks of millions of nodes demonstrate that the proposed algorithms achieve the state-of- the-art in terms of effectiveness and scalability",
    "volume": "main",
    "checked": true,
    "id": "2689fcd90e0654306b51fd67eb8c4ecafc156293",
    "citation_count": 31
  },
  "https://jmlr.org/papers/v18/15-085.html": {
    "title": "Local algorithms for interactive clustering",
    "abstract": "We study the design of interactive clustering algorithms. The user supervision that we consider is in the form of cluster split/merge requests; such feedback is easy for users to provide because it only requires a high-level understanding of the clusters. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable properties in many applications. Local changes are desirable because in practice edits of other parts of the clustering are considered churn - changes that are perceived as quality-neutral or quality-negative. We show that in this framework we can still design provably correct algorithms given that our data satisfies natural separability properties. We also show that our framework works well in practice",
    "volume": "main",
    "checked": true,
    "id": "042b60abf09707305523b70d87312817654fe180",
    "citation_count": 94
  },
  "https://jmlr.org/papers/v18/15-492.html": {
    "title": "SnapVX: A Network-Based Convex Optimization Solver",
    "abstract": "SnapVX is a high-performance solver for convex optimization problems defined on networks. For problems of this form, SnapVX provides a fast and scalable solution with guaranteed global convergence. It combines the capabilities of two open source software packages: Snap.py and CVXPY. Snap.py is a large scale graph processing library, and CVXPY provides a general modeling framework for small-scale subproblems. SnapVX offers a customizable yet easy-to-use Python interface with out-of- the- box functionality. Based on the Alternating Direction Method of Multipliers (ADMM), it is able to efficiently store, analyze, parallelize, and solve large optimization problems from a variety of different applications. Documentation, examples, and more can be found on the SnapVX website at snap.stanford.edu/snapvx",
    "volume": "MLOSS",
    "checked": true,
    "id": "e6ffdba48af4e48a54cadd5fb27bc9244e04b027",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v18/16-002.html": {
    "title": "Communication-efficient Sparse Regression",
    "abstract": "We devise a communication-efficient approach to distributed sparse regression in the high-dimensional setting. The key idea is to average debiased or desparsified lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines, and consistently estimates the support under weaker conditions than the lasso. On the computational side, we propose a new parallel and computationally-efficient algorithm to compute the approximate inverse covariance required in the debiasing approach, when the dataset is split across samples. We further extend the approach to generalized linear models",
    "volume": "main",
    "checked": true,
    "id": "23a96514f84e452aaa921e8ea3b65b396cab52c8",
    "citation_count": 141
  },
  "https://jmlr.org/papers/v18/16-070.html": {
    "title": "Improving Variational Methods via Pairwise Linear Response Identities",
    "abstract": "Inference methods are often formulated as variational approximations: these approxima- tions allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima",
    "volume": "main",
    "checked": true,
    "id": "d5685cbae73b502f0912dcf0f32ccd3d7d84d152",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v18/16-270.html": {
    "title": "Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks",
    "abstract": "Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low- dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs",
    "volume": "main",
    "checked": true,
    "id": "589f5953378461f86d83c303451392bed9eab760",
    "citation_count": 20
  },
  "https://jmlr.org/papers/v18/16-337.html": {
    "title": "Persistence Images: A Stable Vector Representation of Persistent Homology",
    "abstract": "Many data sets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a data set. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite- dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs",
    "volume": "main",
    "checked": true,
    "id": "223841a71f5bce4cb03040e229d13e9a71b78ec3",
    "citation_count": 493
  },
  "https://jmlr.org/papers/v18/14-318.html": {
    "title": "Spectral Clustering Based on Local PCA",
    "abstract": "We propose a spectral clustering method based on local principal components analysis (PCA). After performing local PCA in selected neighborhoods, the algorithm builds a nearest neighbor graph weighted according to a discrepancy between the principal subspaces in the neighborhoods, and then applies spectral clustering. As opposed to standard spectral methods based solely on pairwise distances between points, our algorithm is able to resolve intersections. We establish theoretical guarantees for simpler variants within a prototypical mathematical framework for multi-manifold clustering, and evaluate our algorithm on various simulated data sets",
    "volume": "main",
    "checked": true,
    "id": "d3599be5b9a42da86748b3e87ffff42502413a9a",
    "citation_count": 94
  },
  "https://jmlr.org/papers/v18/15-038.html": {
    "title": "On Perturbed Proximal Gradient Algorithms",
    "abstract": "We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non- asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models",
    "volume": "main",
    "checked": true,
    "id": "aab2c7269ef68b9ef9ee5f8b8e90596b2fbae670",
    "citation_count": 93
  },
  "https://jmlr.org/papers/v18/15-257.html": {
    "title": "Differential Privacy for Bayesian Inference through Posterior Sampling",
    "abstract": "Differential privacy formalises privacy-preserving mechanisms that provide access to a database. Can Bayesian inference be used directly to provide private access to data? The answer is yes: under certain conditions on the prior, sampling from the posterior distribution can lead to a desired level of privacy and utility. For a uniform treatment, we define differential privacy over arbitrary data set metrics, outcome spaces and distribution families. This allows us to also deal with non-i.i.d or non-tabular data sets. We then prove bounds on the sensitivity of the posterior to the data, which delivers a measure of robustness. We also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. Finally, we provide bounds on the utility of answers to queries and on the ability of an adversary to distinguish between data sets. The latter are complemented by a novel use of Le Cam's method to obtain lower bounds on distinguishability. Our results hold for arbitrary metrics, including those for the common definition of differential privacy. For specific choices of the metric, we give a number of examples satisfying our assumptions",
    "volume": "main",
    "checked": true,
    "id": "80de56e417814d71b6b544aa592b2bec28efe1a4",
    "citation_count": 57
  },
  "https://jmlr.org/papers/v18/15-441.html": {
    "title": "Refinery: An Open Source Topic Modeling Web Platform",
    "abstract": "We introduce Refinery, an open source platform for exploring large text document collections with topic models. Refinery is a standalone web application driven by a graphical interface, so it is usable by those without machine learning or programming expertise. Users can interactively organize articles by topic and also refine this organization with phrase-level analysis. Under the hood, we train Bayesian nonparametric topic models that can adapt model complexity to the provided data with scalable learning algorithms.  The project website  contains Python code and further documentation",
    "volume": "MLOSS",
    "checked": true,
    "id": "ea3ec90c9c0a921f374f5d6697d60b1527b3eadc",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v18/15-449.html": {
    "title": "Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns",
    "abstract": "Biological brains can learn, recognize, organize, and re- generate large repertoires of temporal patterns. Here I propose a mechanism of neurodynamical pattern learning and representation, called conceptors, which offers an integrated account of a number of such phenomena and functionalities. It becomes possible to store a large number of temporal patterns in a single recurrent neural network. In the recall process, stored patterns can be morphed and focussed. Parametric families of patterns can be learnt from a very small number of examples. Stored temporal patterns can be content- addressed in ways that are analog to recalling static patterns in Hopfield networks",
    "volume": "main",
    "checked": true,
    "id": "0c497a5b95132d141dd7c310a20c7acbcd32c7c4",
    "citation_count": 42
  },
  "https://jmlr.org/papers/v18/16-107.html": {
    "title": "Automatic Differentiation Variational Inference",
    "abstract": "Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop ADVI. Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models ---no conjugacy assumptions are required. We study ADVI across ten modern probabilistic models and apply it to a dataset with millions of observations. We deploy ADVI as part of Stan, a probabilistic programming system",
    "volume": "main",
    "checked": true,
    "id": "30691d2a4eb1a6e88116c357e95b49f9573bcdae",
    "citation_count": 592
  },
  "https://jmlr.org/papers/v18/16-174.html": {
    "title": "Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters",
    "abstract": "Tuning the regularisation and kernel hyperparameters is a vital step in optimising the generalisation performance of kernel methods, such as the support vector machine (SVM). This is most often performed by minimising a resampling/cross-validation based model selection criterion, however there seems little practical guidance on the most suitable form of resampling. This paper presents the results of an extensive empirical evaluation of resampling procedures for SVM hyperparameter selection, designed to address this gap in the machine learning literature. We tested 15 different resampling procedures on 121 binary classification data sets in order to select the best SVM hyperparameters. We used three very different statistical procedures to analyse the results: the standard multi- classifier/multi-data set procedure proposed by Dem\\v{s}ar, the confidence intervals on the excess loss of each procedure in relation to 5-fold cross validation, and the Bayes factor analysis proposed by Barber. We conclude that a 2-fold procedure is appropriate to select the hyperparameters of an SVM for data sets for 1000 or more datapoints, while a 3-fold procedure is appropriate for smaller data sets",
    "volume": "main",
    "checked": true,
    "id": "79043b3bf656e7b3c475e07d3b9a473b37ce047c",
    "citation_count": 44
  },
  "https://jmlr.org/papers/v18/16-274.html": {
    "title": "A Unified Formulation and Fast Accelerated Proximal Gradient Method for Classification",
    "abstract": "Binary classification is the problem of predicting the class a given sample belongs to. To achieve a good prediction performance, it is important to find a suitable model for a given dataset. However, it is often time consuming and impractical for practitioners to try various classification models because each model employs a different formulation and algorithm. The difficulty can be mitigated if we have a unified formulation and an efficient universal algorithmic framework for various classification models to expedite the comparison of performance of different models for a given dataset. In this paper, we present a unified formulation of various classification models (including $C$-SVM, $\\ell_2$-SVM, $\\nu$-SVM, MM-FDA, MM-MPM, logistic regression, distance weighted discrimination) and develop a general optimization algorithm based on an accelerated proximal gradient (APG) method for the formulation. We design various techniques such as backtracking line search and adaptive restarting strategy in order to speed up the practical convergence of our method. We also give a theoretical convergence guarantee for the proposed fast APG algorithm. Numerical experiments show that our algorithm is stable and highly competitive to specialized algorithms designed for specific models (e.g., sequential minimal optimization (SMO) for SVM)",
    "volume": "main",
    "checked": true,
    "id": "810a874a6b9105f48287ca86da7c6ee0ce5300c6",
    "citation_count": 23
  },
  "https://jmlr.org/papers/v18/16-365.html": {
    "title": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning",
    "abstract": "imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the- art methods can be categorized into 4 groups: (i) under- sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn  and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from  github.com/scikit-learn-contrib/imbalanced-learn",
    "volume": "MLOSS",
    "checked": true,
    "id": "05c5b732fb92546c7d6eeabfadb5c14610d07373",
    "citation_count": 1457
  },
  "https://jmlr.org/papers/v18/14-467.html": {
    "title": "Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a866479d17163dff4a330cfa1682b6c86f74748b",
    "citation_count": 195
  },
  "https://jmlr.org/papers/v18/14-546.html": {
    "title": "Breaking the Curse of Dimensionality with Convex Neural Networks",
    "abstract": "We consider neural networks with a single hidden layer and non- decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity- inducing norms on the input weights, we show that high- dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non- convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non- convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomial-time algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities",
    "volume": "main",
    "checked": true,
    "id": "f9c2ece8262f9dcf4ec176799e88e51adb1fd052",
    "citation_count": 566
  },
  "https://jmlr.org/papers/v18/15-025.html": {
    "title": "Memory Efficient Kernel Approximation",
    "abstract": "Scaling kernel machines to massive data sets is a major challenge due to storage and computation issues in handling large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low-rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation framework -- Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the covtype dataset with half a million samples, MEKA takes around 70 seconds and uses less than 80 MB memory on a single machine to achieve 10% relative approximation error, while standard NystrÃÂ¶m approximation is about 6 times slower and uses more than 400MB memory to achieve similar approximation. We also present extensive experiments on applying MEKA to speed up kernel ridge regression",
    "volume": "main",
    "checked": true,
    "id": "58c85498e23c86f526223e661e250007794c8d67",
    "citation_count": 149
  },
  "https://jmlr.org/papers/v18/15-178.html": {
    "title": "On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions",
    "abstract": "We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in $L_2$- and $L_\\infty$-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses",
    "volume": "main",
    "checked": true,
    "id": "990f341846223e80a4c5fbd5c2be309eb5c8bec9",
    "citation_count": 251
  },
  "https://jmlr.org/papers/v18/15-486.html": {
    "title": "Analyzing Tensor Power Method Dynamics in Overcomplete Regime",
    "abstract": "We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multi-view mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components $k$ is much larger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models",
    "volume": "main",
    "checked": true,
    "id": "d7d0f0cfe67f84ef3adda30468bdd53fd96d571e",
    "citation_count": 46
  },
  "https://jmlr.org/papers/v18/16-131.html": {
    "title": "JSAT: Java Statistical Analysis Tool, a Library for Machine Learning",
    "abstract": "Java Statistical Analysis Tool (JSAT) is a Machine Learning library written in pure Java. It works to fill a void in the Java ecosystem for a general purpose library that is relatively high performance and flexible, which is not adequately fulfilled by Weka (Hall et al., 2009) and Java-ML (Abeel et al., 2009). Almost all of the algorithms are independently implemented using an Object- Oriented framework. JSAT is made available under the GNU GPL license here:  github.com/EdwardRaff/JSAT",
    "volume": "MLOSS",
    "checked": true,
    "id": "8347a5ae3a622ce0aaa7dd674020c18ea9c89fdf",
    "citation_count": 42
  },
  "https://jmlr.org/papers/v18/16-172.html": {
    "title": "Identifying a Minimal Class of Models for High--dimensional Data",
    "abstract": "Model selection consistency in the high--dimensional regression setting can be achieved only if strong assumptions are fulfilled. We therefore suggest to pursue a different goal, which we call a minimal class of models. The minimal class of models includes models that are similar in their prediction accuracy but not necessarily in their elements. We suggest a random search algorithm to reveal candidate models. The algorithm implements simulated annealing while using a score for each predictor that we suggest to derive using a combination of the lasso and the elastic net. The utility of using a minimal class of models is demonstrated in the analysis of two data sets",
    "volume": "main",
    "checked": false,
    "id": "3c7c2f42e87caf4d65a5a0e2d7dafc71a451aa7c",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v18/16-261.html": {
    "title": "Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA",
    "abstract": "WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA's learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm",
    "volume": "MLOSS",
    "checked": true,
    "id": "cef5f429a2ee5bad3a258c7baf7cc77e9af047a3",
    "citation_count": 614
  },
  "https://jmlr.org/papers/v18/16-300.html": {
    "title": "POMDPs.jl: A Framework for Sequential Decision Making under Uncertainty",
    "abstract": "POMDPs.jl is an open-source framework for solving Markov decision processes (MDPs) and partially observable MDPs (POMDPs). POMDPs.jl allows users to specify sequential decision making problems with minimal effort without sacrificing the expressive nature of POMDPs, making this framework viable for both educational and research purposes. It is written in the Julia language to allow flexible prototyping and large-scale computation that leverages the high-performance nature of the language. The associated JuliaPOMDP community also provides a number of state-of-the-art MDP and POMDP solvers and a rich library of support tools to help with implementing new solvers and evaluating the solution results. The most recent version of POMDPs.jl, the related packages, and documentation can be found at github.com/ JuliaPOMDP/POMDPs.jl",
    "volume": "MLOSS",
    "checked": true,
    "id": "5c84814b138a6c882ffed34d89bd8882077a5fed",
    "citation_count": 100
  },
  "https://jmlr.org/papers/v18/10-231.html": {
    "title": "Generalized P{\\'o}lya Urn for Time-Varying Pitman-Yor Processes",
    "abstract": "This article introduces a class of first-order stationary time- varying Pitman-Yor processes. Subsuming our construction of time-varying Dirichlet processes presented in (Caron et al., 2007), these models can be used for time-dynamic density estimation and clustering. Our intuitive and simple construction relies on a generalized PÃÂ³lya urn scheme. Significantly, this construction yields marginal distributions at each time point that can be explicitly characterized and easily controlled. Inference is performed using Markov chain Monte Carlo and sequential Monte Carlo methods. We demonstrate our models and algorithms on epidemiological and video tracking data",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v18/15-397.html": {
    "title": "Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture Models",
    "abstract": "This paper presents an original Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler introduced in Andrieu et al. (2009, 2010). The resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio and can be implemented using existing sequential Monte Carlo libraries. We investigate its performance experimentally on synthetic problems as well as on geolocation data. Our results show that for a given computational budget, the Particle Gibbs Split-Merge sampler empirically outperforms existing split merge methods. The code and instructions allowing to reproduce the experiments is available at github.com/aroth85/pgsm",
    "volume": "main",
    "checked": true,
    "id": "b9267c0214fcb6e5aacafe93eff1ad76f1fd56a6",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/15-613.html": {
    "title": "Certifiably Optimal Low Rank Factor Analysis",
    "abstract": "Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics, and econometrics. In this paper, we revisit the classical rank-constrained FA problem which seeks to approximate an observed covariance matrix ($\\B\\Sigma$) by the sum of a Positive Semidefinite (PSD) low-rank component ($\\B\\Theta$) and a diagonal matrix ($\\B\\Phi$) (with nonnegative entries) subject to $\\B\\Sigma - \\B\\Phi$ being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex, compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are certifiably optimal in many cases, even for problems with thousands of variables. Our techniques are general and make no assumption on the underlying problem data. The estimator proposed herein aids statistical interpretability and provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in global and discrete optimization",
    "volume": "main",
    "checked": true,
    "id": "2523b796d50720685258144eb66585855c3eb018",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v18/15-651.html": {
    "title": "Group Sparse Optimization via lp,q Regularization",
    "abstract": "In this paper, we investigate a group sparse optimization problem via $\\ell_{p,q}$ regularization in three aspects: theory, algorithm and application. In the theoretical aspect, by introducing a notion of group restricted eigenvalue condition, we establish an oracle property and a global recovery bound of order $\\mathcal{O}(\\lambda^\\frac{2}{2-q})$ for any point in a level set of the $\\ell_{p,q}$ regularization problem, and by virtue of modern variational analysis techniques, we also provide a local analysis of recovery bound of order $\\mathcal{O}(\\lambda^2)$ for a path of local minima. In the algorithmic aspect, we apply the well-known proximal gradient method to solve the $\\ell_{p,q}$ regularization problems, either by analytically solving some specific $\\ell_{p,q}$ regularization subproblems, or by using the Newton method to solve general $\\ell_{p,q}$ regularization subproblems. In particular, we establish a local linear convergence rate of the proximal gradient method for solving the $\\ell_{1,q}$ regularization problem under some mild conditions and by first proving a second-order growth condition. As a consequence, the local linear convergence rate of proximal gradient method for solving the usual $\\ell_{q}$ regularization problem ($0<q<1$) is obtained. Finally in the aspect of application, we present some numerical results on both the simulated data and the real data in gene transcriptional regulation",
    "volume": "main",
    "checked": true,
    "id": "1db7e65053bb5a2e11659c0d851609e1ec8d6f0a",
    "citation_count": 57
  },
  "https://jmlr.org/papers/v18/16-460.html": {
    "title": "Preference-based Teaching",
    "abstract": "We introduce a new model of teaching named preference-based teaching and a corresponding complexity parameter---the preference-based teaching dimension (PBTD)---representing the worst-case number of examples needed to teach any concept in a given concept class. Although the PBTD coincides with the well- known recursive teaching dimension (RTD) on finite classes, it is radically different on infinite ones: the RTD becomes infinite already for trivial infinite classes (such as half- intervals) whereas the PBTD evaluates to reasonably small values for a wide collection of infinite classes including classes consisting of so-called closed sets w.r.t. a given closure operator, including various classes related to linear sets over $\\mathbb{N}_0$ (whose RTD had been studied quite recently) and including the class of Euclidean half-spaces. On top of presenting these concrete results, we provide the reader with a theoretical framework (of a combinatorial flavor) which helps to derive bounds on the PBTD",
    "volume": "main",
    "checked": true,
    "id": "39043c793428b36b53451f89bc885d91bdf5cf29",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/13-336.html": {
    "title": "Nonparametric Risk Bounds for Time-Series Forecasting",
    "abstract": "We derive generalization error bounds for traditional time- series forecasting models. Our results hold for many standard forecasting tools including autoregressive models, moving average models, and, more generally, linear state-space models. These non-asymptotic bounds need only weak assumptions on the data-generating process, yet allow forecasters to select among competing models and to guarantee, with high probability, that their chosen model will perform well. We motivate our techniques with and apply them to standard economic and financial forecasting tools---a GARCH model for predicting equity volatility and a dynamic stochastic general equilibrium model (DSGE), the standard tool in macroeconomic forecasting. We demonstrate in particular how our techniques can aid forecasters and policy makers in choosing models which behave well under uncertainty and mis-specification",
    "volume": "main",
    "checked": true,
    "id": "11699a49dfe5c469267cfa63ae31e7fe176c7d52",
    "citation_count": 26
  },
  "https://jmlr.org/papers/v18/14-188.html": {
    "title": "Online Bayesian Passive-Aggressive Learning",
    "abstract": "We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive- Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms",
    "volume": "main",
    "checked": true,
    "id": "3e4d9583480e21dee3cdd8b5c587ca63af194a7b",
    "citation_count": 37
  },
  "https://jmlr.org/papers/v18/15-104.html": {
    "title": "Asymptotic Analysis of Objectives Based on Fisher Information in Active Learning",
    "abstract": "Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for a more efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective",
    "volume": "main",
    "checked": true,
    "id": "4cdcbc5f8ff509904b432eed361af1f1be714a7c",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v18/15-468.html": {
    "title": "A Spectral Algorithm for Inference in Hidden semi-Markov Models",
    "abstract": "Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large data sets",
    "volume": "main",
    "checked": true,
    "id": "aa3899d2c1d13aaf9670e89cd478a84c8e467274",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/16-166.html": {
    "title": "Simplifying Probabilistic Expressions in Causal Inference",
    "abstract": "Obtaining a non-parametric expression for an interventional distribution is one of the most fundamental tasks in causal inference. Such an expression can be obtained for an identifiable causal effect by an algorithm or by manual application of do-calculus. Often we are left with a complicated expression which can lead to biased or inefficient estimates when missing data or measurement errors are involved. We present an automatic simplification algorithm that seeks to eliminate symbolically unnecessary variables from these expressions by taking advantage of the structure of the underlying graphical model. Our method is applicable to all causal effect formulas and is readily available in the R package causaleffect",
    "volume": "main",
    "checked": true,
    "id": "191d99fb17e4b0e70893fae9f51e1a8d246e4904",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v18/16-217.html": {
    "title": "Nearly optimal classification for semimetrics",
    "abstract": "We initiate the rigorous study of classification in semimetric spaces, which are point sets with a distance function that is non-negative and symmetric, but need not satisfy the triangle inequality. We define the density dimension dens and discover that it plays a central role in the statistical and algorithmic feasibility of learning in semimetric spaces. We compute this quantity for several widely used semimetrics and present nearly optimal sample compression algorithms, which are then used to obtain generalization guarantees, including fast rates. Our claim of near-optimality holds in both computational and statistical senses. When the sample has radius $R$ and margin $\\gamma$, we show that it can be compressed down to roughly $d=(R/\\gamma)^{\\text{dens}}$ points, and further that finding a significantly better compression is algorithmically intractable unless P=NP. This compression implies generalization via standard Occam-type arguments, to which we provide a nearly matching lower bound",
    "volume": "main",
    "checked": true,
    "id": "a96c125f91def81a16ab0c76fd9dceaf7849fa5e",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/16-223.html": {
    "title": "Bridging Supervised Learning and Test-Based Co-optimization",
    "abstract": "This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of co-optimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches",
    "volume": "main",
    "checked": true,
    "id": "cf90f4d719742c9f8974f1603e5329d8b0c65db5",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/16-509.html": {
    "title": "GFA: Exploratory Analysis of Multiple Data Sources with Group Factor Analysis",
    "abstract": "The R package GFA provides a full pipeline for factor analysis of multiple data sources that are represented as matrices with co-occurring samples. It allows learning dependencies between subsets of the data sources, decomposed into latent factors. The package also implements sparse priors for the factorization, providing interpretable biclusters of the multi-source data",
    "volume": "MLOSS",
    "checked": true,
    "id": "626b1a48e0ee7f5714693c5dea3fe9f5eab07cd7",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v18/16-537.html": {
    "title": "GPflow: A Gaussian Process Library using TensorFlow",
    "abstract": "GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware",
    "volume": "MLOSS",
    "checked": true,
    "id": "c4be5b937c819d1432c5343d1ea07a5269a0380d",
    "citation_count": 534
  },
  "https://jmlr.org/papers/v18/16-132.html": {
    "title": "COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Evolution",
    "abstract": "",
    "volume": "main",
    "checked": false,
    "id": "d4872aebf6f8e8644d93f55a6aef042de18d61a6",
    "citation_count": 209
  },
  "https://jmlr.org/papers/v18/16-198.html": {
    "title": "Learning Local Dependence In Ordered Data",
    "abstract": "In many applications, data come with a natural ordering. This ordering can often induce local dependence among nearby variables. However, in complex data, the width of this dependence may vary, making simple assumptions such as a constant neighborhood size unrealistic. We propose a framework for learning this local dependence based on estimating the inverse of the Cholesky factor of the covariance matrix. Penalized maximum likelihood estimation of this matrix yields a simple regression interpretation for local dependence in which variables are predicted by their neighbors. Our proposed method involves solving a convex, penalized Gaussian likelihood problem with a hierarchical group lasso penalty. The problem decomposes into independent subproblems which can be solved efficiently in parallel using first-order methods. Our method yields a sparse, symmetric, positive definite estimator of the precision matrix, encoding a Gaussian graphical model. We derive theoretical results not found in existing methods attaining this structure. In particular, our conditions for signed support recovery and estimation consistency rates in multiple norms are as mild as those in a regression problem. Empirical results show our method performing favorably compared to existing methods. We apply our method to genomic data to flexibly model linkage disequilibrium. Our method is also applied to improve the performance of discriminant analysis in sound recording classification",
    "volume": "main",
    "checked": true,
    "id": "29365331f6a56938bdcd2abd8e186d127a709af0",
    "citation_count": 29
  },
  "https://jmlr.org/papers/v18/16-391.html": {
    "title": "Bayesian Learning of Dynamic Multilayer Networks",
    "abstract": "A plethora of networks is being collected in a growing number of fields, including disease transmission, international relations, social interactions, and others. As data streams continue to grow, the complexity associated with these highly multidimensional connectivity data presents novel challenges. In this paper, we focus on the time-varying interconnections among a set of actors in multiple contexts, called layers. Current literature lacks flexible statistical models for dynamic multilayer networks, which can enhance quality in inference and prediction by efficiently borrowing information within each network, across time, and between layers. Motivated by this gap, we develop a Bayesian nonparametric model leveraging latent space representations. Our formulation characterizes the edge probabilities as a function of shared and layer-specific actors positions in a latent space, with these positions changing in time via Gaussian processes. This representation facilitates dimensionality reduction and incorporates different sources of information in the observed data. In addition, we obtain tractable procedures for posterior computation, inference, and prediction. We provide theoretical results on the flexibility of our model. Our methods are tested on simulations and infection studies monitoring dynamic face-to-face contacts among individuals in multiple days, where we perform better than current methods in inference and prediction",
    "volume": "main",
    "checked": true,
    "id": "727898f95fbdc494e7e02b0406e33cecda39dc7d",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/16-538.html": {
    "title": "Time-Accuracy Tradeoffs in Kernel Prediction: Controlling Prediction Quality",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "f68f682f9bf329dbba8c228989162ac7b4855e2c",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v18/16-564.html": {
    "title": "Asymptotic behavior of Support Vector Machine for spiked population model",
    "abstract": "For spiked population model, we investigate the large dimension $N$ and large sample size $M$ asymptotic behavior of the Support Vector Machine (SVM) classification method in the limit of $N,M\\rightarrow\\infty$ at fixed $\\alpha=M/N$. We focus on the generalization performance by analytically evaluating the angle between the normal direction vectors of SVM separating hyperplane and corresponding Bayes optimal separating hyperplane. This is an analogous result to the one shown in Paul (2007) and Nadler (2008) for the angle between the sample eigenvector and the population eigenvector in random matrix theorem. We provide not just bound, but sharp prediction of the asymptotic behavior of SVM that can be determined by a set of nonlinear equations. Based on the analytical results, we propose a new method of selecting tuning parameter which significantly reduces the computational cost. A surprising finding is that SVM achieves its best performance at small value of the tuning parameter under spiked population model. These results are confirmed to be correct by comparing with those of numerical simulations on finite-size systems. We also apply our formulas to an actual dataset of breast cancer and find agreement between analytical derivations and numerical computations based on cross validation",
    "volume": "main",
    "checked": true,
    "id": "6eef85cd80123e770b8304ae08957f59ff22e05a",
    "citation_count": 30
  },
  "https://jmlr.org/papers/v18/16-601.html": {
    "title": "Distributed Semi-supervised Learning with Kernel Ridge Regression",
    "abstract": "This paper provides error analysis for distributed semi- supervised learning with kernel ridge regression (DSKRR) based on a divide-and-conquer strategy. DSKRR applies kernel ridge regression (KRR) to data subsets that are distributively stored on multiple servers to produce individual output functions, and then takes a weighted average of the individual output functions as a final estimator. Using a novel error decomposition which divides the generalization error of DSKRR into the approximation error, sample error and distributed error, we find that the sample error and distributed error reflect the power and limitation of DSKRR, compared with KRR processing the whole data. Thus a small distributed error provides a large range of the number of data subsets to guarantee a small generalization error. Our results show that unlabeled data play important roles in reducing the distributed error and enlarging the number of data subsets in DSKRR. Our analysis also applies to the case when the regression function is out of the reproducing kernel Hilbert space. Numerical experiments including toy simulations and a music-prediction task are employed to demonstrate our theoretical statements and show the power of unlabeled data in distributed learning",
    "volume": "main",
    "checked": true,
    "id": "d2e0340b2949eacfe89e3c899cb8da0815b61ed0",
    "citation_count": 83
  },
  "https://jmlr.org/papers/v18/15-205.html": {
    "title": "On Markov chain Monte Carlo methods for tall data",
    "abstract": "Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number $n$ of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis- Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach relying on a control variate method which samples under regularity conditions from a distribution provably close to the posterior distribution of interest, yet can require less than $O(n)$ data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we emphasize that we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor",
    "volume": "main",
    "checked": true,
    "id": "e01a2082464b3598962ed697e7b6942a0f8477f0",
    "citation_count": 239
  },
  "https://jmlr.org/papers/v18/15-240.html": {
    "title": "Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers",
    "abstract": "There is a large literature explaining why AdaBoost is a successful classifier. The literature on AdaBoost focuses on classifier margins and boosting's interpretation as the optimization of an exponential likelihood function. These existing explanations, however, have been pointed out to be incomplete. A random forest is another popular ensemble method for which there is substantially less explanation in the literature. We introduce a novel perspective on AdaBoost and random forests that proposes that the two algorithms work for similar reasons. While both classifiers achieve similar predictive accuracy, random forests cannot be conceived as a direct optimization procedure. Rather, random forests is a self- averaging, interpolating algorithm which creates what we denote as a spiked-smooth classifier, and we view AdaBoost in the same light. We conjecture that both AdaBoost and random forests succeed because of this mechanism. We provide a number of examples to support this explanation. In the process, we question the conventional wisdom that suggests that boosting algorithms for classification require regularization or early stopping and should be limited to low complexity classes of learners, such as decision stumps. We conclude that boosting should be used like random forests: with large decision trees, without regularization or early stopping",
    "volume": "main",
    "checked": true,
    "id": "ce6879fbd6cab2688cad1f530a75992ec184d0b9",
    "citation_count": 212
  },
  "https://jmlr.org/papers/v18/15-659.html": {
    "title": "Clustering from General Pairwise Observations with Applications to Time-varying Graphs",
    "abstract": "We present a general framework for graph clustering and bi- clustering where we are given a general observation (called a label) between each pair of nodes. This framework allows a rich encoding of various types of pairwise interactions between nodes. We propose a new tractable and robust approach to this problem based on convex optimization and maximum likelihood estimators. We analyze our algorithms under a general statistical model extending the planted partition and stochastic block models. Both sufficient and necessary conditions are provided for successful recovery of the underlying clusters. Our theoretical results subsume many existing graph clustering results for a wide range of settings, including planted partition, weighted clustering, submatrix localization and partially observed graphs. Furthermore, our results are applicable to novel settings including time-varying graphs, providing new insights to solutions of these problems. We provide empirical results on both synthetic and real data that corroborate with our theoretical findings",
    "volume": "main",
    "checked": true,
    "id": "53ecda4c0acae30d780358e52748030149f34822",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v18/16-100.html": {
    "title": "Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling Techniques",
    "abstract": "In a series of recent works, we have generalised the consistency results in the stochastic block model literature to the case of uniform and non-uniform hypergraphs. The present paper continues the same line of study, where we focus on partitioning weighted uniform hypergraphs---a problem often encountered in computer vision. This work is motivated by two issues that arise when a hypergraph partitioning approach is used to tackle computer vision problems: (i) The uniform hypergraphs constructed for higher-order learning contain all edges, but most have negligible weights. Thus, the adjacency tensor is nearly sparse, and yet, not binary. (ii) A more serious concern is that standard partitioning algorithms need to compute all edge weights, which is computationally expensive for hypergraphs. This is usually resolved in practice by merging the clustering algorithm with a tensor sampling strategy---an approach that is yet to be analysed rigorously. We build on our earlier work on partitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati, ICML, 2015), and address the aforementioned issues by proposing provable and efficient partitioning algorithms. Our analysis justifies the empirical success of practical sampling techniques. We also complement our theoretical findings by elaborate empirical comparison of various hypergraph partitioning schemes",
    "volume": "main",
    "checked": true,
    "id": "be849fbd09b263d2c5db0ab9a113002b2ef1d5d8",
    "citation_count": 41
  },
  "https://jmlr.org/papers/v18/16-146.html": {
    "title": "Reconstructing Undirected Graphs from Eigenspaces",
    "abstract": "We aim at recovering the weighted adjacency matrix $\\mathsf{W}$ of an undirected graph from a perturbed version of its eigenspaces. This situation arises for instance when working with stationary signals on graphs or Markov chains observed at random times. Our approach relies on minimizing a cost function based on the Frobenius norm of the commutator $\\mathsf{A} \\mathsf{B}-\\mathsf{B} \\mathsf{A}$ between symmetric matrices $\\mathsf{A}$ and $\\mathsf{B}$. We describe a particular framework in which we have access to an estimation of the eigenspaces and provide support selection procedures from theoretical and practical points of view. In the ErdÃÂs-RÃÂ©nyi model on $N$ vertices with no self-loops, we show that identifiability (i.e., the ability to reconstruct $\\mathsf{W}$ from the knowledge of its eigenspaces) follows a sharp phase transition on the expected number of edges with threshold function $N\\log N/2$. Simulated and real life numerical experiments assert our methodology",
    "volume": "main",
    "checked": true,
    "id": "89b3458006f056a840ea4f4960c6eab0968999dd",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/16-632.html": {
    "title": "An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback",
    "abstract": "We consider the closely related problems of bandit convex optimization with two-point feedback, and zero-order stochastic convex optimization with two function evaluations per round. We provide a simple algorithm and analysis which is optimal for convex Lipschitz functions. This improves on Duchi et al. (2015), which only provides an optimal result for smooth functions; Moreover, the algorithm and analysis are simpler, and readily extend to non-Euclidean problems. The algorithm is based on a small but surprisingly powerful modification of the gradient estimator",
    "volume": "main",
    "checked": true,
    "id": "436445b4cf3fe25e5778b5838eb623accad428e5",
    "citation_count": 183
  },
  "https://jmlr.org/papers/v18/17-061.html": {
    "title": "Perishability of Data: Dynamic Pricing under Varying-Coefficient Models",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "403594304795a58284fcb3c22e125bc8cae8236b",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/14-453.html": {
    "title": "Two New Approaches to Compressed Sensing Exhibiting Both Robust Sparse Recovery and the Grouping Effect",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "10ad60495b140ccb0b2c6ed39f8cfd8b8a25a94d",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/15-495.html": {
    "title": "On the Consistency of Ordinal Regression Methods",
    "abstract": "Many of the ordinal regression models that have been proposed in the literature can be seen as methods that minimize a convex surrogate of the zero-one, absolute, or squared loss functions. A key property that allows to study the statistical implications of such approximations is that of Fisher consistency. Fisher consistency is a desirable property for surrogate loss functions and implies that in the population setting, i.e., if the probability distribution that generates the data were available, then optimization of the surrogate would yield the best possible model. In this paper we will characterize the Fisher consistency of a rich family of surrogate loss functions used in the context of ordinal regression, including support vector ordinal regression, ORBoosting and least absolute deviation. We will see that, for a family of surrogate loss functions that subsumes support vector ordinal regression and ORBoosting, consistency can be fully characterized by the derivative of a real-valued function at zero, as happens for convex margin-based surrogates in binary classification. We also derive excess risk bounds for a surrogate of the absolute error that generalize existing risk bounds for binary classification. Finally, our analysis suggests a novel surrogate of the squared error loss. We compare this novel surrogate with competing approaches on 9 different datasets. Our method shows to be highly competitive in practice, outperforming the least squares loss on 7 out of 9 datasets",
    "volume": "main",
    "checked": true,
    "id": "24def5b14643ab40ba1789e86c24516dd6d8a8fa",
    "citation_count": 52
  },
  "https://jmlr.org/papers/v18/15-596.html": {
    "title": "Statistical Inference with Unnormalized Discrete Models and Localized Homogeneous Divergences",
    "abstract": "In this paper, we focus on parameters estimation of probabilistic models in discrete space. A naive calculation of the normalization constant of the probabilistic model on discrete space is often infeasible and statistical inference based on such probabilistic models has difficulty. In this paper, we propose a novel estimator for probabilistic models on discrete space, which is derived from an empirically localized homogeneous divergence. The idea of the empirical localization makes it possible to ignore an unobserved domain on sample space, and the homogeneous divergence is a discrepancy measure between two positive measures and has a weak coincidence axiom. The proposed estimator can be constructed without calculating the normalization constant and is asymptotically consistent and Fisher efficient. We investigate statistical properties of the proposed estimator and reveal a relationship between the empirically localized homogeneous divergence and a mixture of the $\\alpha$-divergence. The $\\alpha$-divergence is a non- homogeneous discrepancy measure that is frequently discussed in the context of information geometry. Using the relationship, we also propose an asymptotically consistent estimator of the normalization constant. Experiments showed that the proposed estimator comparably performs to the maximum likelihood estimator but with drastically lower computational cost",
    "volume": "main",
    "checked": true,
    "id": "1e2215a0eb75dc38b0126838f3234175090bdc78",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/16-011.html": {
    "title": "Density Estimation in Infinite Dimensional Exponential Families",
    "abstract": "In this paper, we consider an infinite dimensional exponential family $\\mathcal{P}$ of probability densities, which are parametrized by functions in a reproducing kernel Hilbert space $\\mathcal{H}$, and show it to be quite rich in the sense that a broad class of densities on $\\mathbb{R}^d$ can be approximated arbitrarily well in Kullback-Leibler (KL) divergence by elements in $\\mathcal{P}$. Motivated by this approximation property, the paper addresses the question of estimating an unknown density $p_0$ through an element in $\\mathcal{P}$. Standard techniques like maximum likelihood estimation (MLE) or pseudo MLE (based on the method of sieves), which are based on minimizing the KL divergence between $p_0$ and $\\mathcal{P}$, do not yield practically useful estimators because of their inability to efficiently handle the log-partition function. We propose an estimator $\\hat{p}_n$ based on minimizing the Fisher divergence, $J(p_0\\Vert p)$ between $p_0$ and $p\\in \\mathcal{P}$, which involves solving a simple finite-dimensional linear system. When $p_0\\in\\mathcal{P}$, we show that the proposed estimator is consistent, and provide a convergence rate of $n^{-\\min\\left\\{\\frac{2}{3},\\frac{2\\beta+1}{2\\beta+2}\\right\\}}$ in Fisher divergence under the smoothness assumption that $\\log p_0\\in\\mathcal{R}(C^\\beta)$ for some $\\beta\\ge 0$, where $C$ is a certain Hilbert-Schmidt operator on $\\mathcal{H}$ and $\\mathcal{R}(C^\\beta)$ denotes the image of $C^\\beta$. We also investigate the misspecified case of $p_0\\notin\\mathcal{P}$ and show that $J(p_0\\Vert\\hat{p}_n)\\rightarrow \\inf_{p\\in\\mathcal{P}}J(p_0\\Vert p)$ as $n\\rightarrow \\infty$, and provide a rate for this convergence under a similar smoothness condition as above. Through numerical simulations we demonstrate that the proposed estimator outperforms the non- parametric kernel density estimator, and that the advantage of the proposed estimator grows as $d$ increases",
    "volume": "main",
    "checked": true,
    "id": "21fafb9789b4c9ddfc324044868109b17419c914",
    "citation_count": 113
  },
  "https://jmlr.org/papers/v18/16-061.html": {
    "title": "Lens Depth Function and k-Relative Neighborhood Graph: Versatile Tools for Ordinal Data Analysis",
    "abstract": "In recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as $d(A,B)<d(C,D)$. For many problems in machine learning and statistics it is unclear how to solve them in such a scenario. Up to now, the main approach is to explicitly construct an ordinal embedding of the data points in the Euclidean space, an approach that has a number of drawbacks. In this paper, we propose algorithms for the problems of medoid estimation, outlier identification, classification, and clustering when given only ordinal data. They are based on estimating the lens depth function and the $k$-relative neighborhood graph on a data set. Our algorithms are simple, are much faster than an ordinal embedding approach and avoid some of its drawbacks, and can easily be parallelized",
    "volume": "main",
    "checked": true,
    "id": "5cb510ea5ab90fc34fb11d74cc23b248dd16515d",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v18/16-214.html": {
    "title": "Joint Label Inference in Networks",
    "abstract": "We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers for people connected by a social network; by predicting these user profile fields, the network can provide a better experience to its users. Existing approaches such as Label Propagation (Zhu et al., 2003) fail to consider interactions between the label types. Our proposed method, called EDGEEXPLAIN explicitly models these interactions, while still allowing scalable inference under a distributed message- passing architecture. On a large subset of the Facebook social network, collected in a previous study (Chakrabarti et al., 2014), EDGEEXPLAIN outperforms label propagation for several label types, with lifts of up to $120\\%$ for recall@1 and $60\\%$ for recall@3",
    "volume": "main",
    "checked": true,
    "id": "1b85d4eaf476a69ca838967bebacfd433d5d5be3",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/16-245.html": {
    "title": "Achieving Optimal Misclassification Proportion in Stochastic Block Models",
    "abstract": "Community detection is a fundamental statistical problem in network data analysis.  In this paper, we present a polynomial time two-stage method that provably achieves optimal statistical performance in misclassification proportion for stochastic block model under weak regularity conditions. Our two-stage procedure consists of a refinement stage motivated by penalized local maximum likelihood estimation. This stage can take a wide range of weakly consistent community detection procedures as its initializer, to which it applies and outputs a community assignment that achieves optimal misclassification proportion with high probability. The theoretical property is confirmed by simulated examples",
    "volume": "main",
    "checked": true,
    "id": "60845819aeedbadda293b6098a8d712f86860981",
    "citation_count": 220
  },
  "https://jmlr.org/papers/v18/16-497.html": {
    "title": "On the Propagation of Low-Rate Measurement Error to Subgraph Counts in Large Networks",
    "abstract": "Our work in this paper is inspired by a statistical observation that is both elementary and broadly relevant to network analysis in practice---that the uncertainty in approximating some true graph $G=(V,E)$ by some estimated graph $\\hat{G}=(V,\\hat{E})$ manifests as errors in our knowledge of the presence/absence of edges between vertex pairs, which must necessarily propagate to any estimates of network summaries $\\eta(G)$ we seek. Motivated by the common practice of using plug-in estimates $\\eta(\\hat{G})$ as proxies for $\\eta(G)$, our focus is on the problem of characterizing the distribution of the discrepancy $D=\\eta(\\hat{G}) - \\eta(G)$, in the case where $\\eta(\\cdot)$ is a subgraph count. Specifically, we study the fundamental case where the statistic of interest is $|E|$, the number of edges in $G$. Our primary contribution in this paper is to show that in the empirically relevant setting of large graphs with low-rate measurement errors, the distribution of $D_E=|\\hat{E}| - |E|$ is well-characterized by a Skellam distribution, when the errors are independent or weakly dependent. Under an assumption of independent errors, we are able to further show conditions under which this characterization is strictly better than that of an appropriate normal distribution. These results derive from our formulation of a general result, quantifying the accuracy with which the difference of two sums of dependent Bernoulli random variables may be approximated by the difference of two independent Poisson random variables, i.e., by a Skellam distribution. This general result is developed through the use of Stein's method, and may be of some general interest. We finish with a discussion of possible extension of our work to subgraph counts $\\eta(G)$ of higher order",
    "volume": "main",
    "checked": true,
    "id": "7dbe30a7e7dad2551c751fefb12d734856684ee5",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v18/16-526.html": {
    "title": "Dense Distributions from Sparse Samples: Improved Gibbs Sampling Parameter Estimators for LDA",
    "abstract": "We introduce a novel approach for estimating Latent Dirichlet Allocation (LDA) parameters from collapsed Gibbs samples (CGS), by leveraging the full conditional distributions over the latent variable assignments to efficiently average over multiple samples, for little more computational cost than drawing a single additional collapsed Gibbs sample. Our approach can be understood as adapting the soft clustering methodology of Collapsed Variational Bayes (CVB0) to CGS parameter estimation, in order to get the best of both techniques. Our estimators can straightforwardly be applied to the output of any existing implementation of CGS, including modern accelerated variants. We perform extensive empirical comparisons of our estimators with those of standard collapsed inference algorithms on real-world data for both unsupervised LDA and Prior-LDA, a supervised variant of LDA for multi-label classification. Our results show a consistent advantage of our approach over traditional CGS under all experimental conditions, and over CVB0 inference in the majority of conditions. More broadly, our results highlight the importance of averaging over multiple samples in LDA parameter estimation, and the use of efficient computational techniques to do so",
    "volume": "main",
    "checked": true,
    "id": "e77cbb9c8ae3a35bcef724b16bc43d5d561550aa",
    "citation_count": 31
  },
  "https://jmlr.org/papers/v18/17-189.html": {
    "title": "Fundamental Conditions for Low-CP-Rank Tensor Completion",
    "abstract": "We consider the problem of low canonical polyadic (CP) rank tensor completion. A completion is a tensor whose entries agree with the observed entries and its rank matches the given CP rank. We analyze the manifold structure corresponding to the tensors with the given rank and define a set of polynomials based on the sampling pattern and CP decomposition. Then, we show that finite completability of the sampled tensor is equivalent to having a certain number of algebraically independent polynomials among the defined polynomials. Our proposed approach results in characterizing the maximum number of algebraically independent polynomials in terms of a simple geometric structure of the sampling pattern, and therefore we obtain the deterministic necessary and sufficient condition on the sampling pattern for finite completability of the sampled tensor. Moreover, assuming that the entries of the tensor are sampled independently with probability $p$ and using the mentioned deterministic analysis, we propose a combinatorial method to derive a lower bound on the sampling probability $p$, or equivalently, the number of sampled entries that guarantees finite completability with high probability. We also show that the existing result for the matrix completion problem can be used to obtain a loose lower bound on the sampling probability $p$. In addition, we obtain deterministic and probabilistic conditions for unique completability. It is seen that the number of samples required for finite or unique completability obtained by the proposed analysis on the CP manifold is orders-of- magnitude lower than that is obtained by the existing analysis on the Grassmannian manifold",
    "volume": "main",
    "checked": true,
    "id": "c897cee9b439e66408faa514d3c69b88051d3de6",
    "citation_count": 43
  },
  "https://jmlr.org/papers/v18/14-317.html": {
    "title": "Parallel Symmetric Class Expression Learning",
    "abstract": "In machine learning, one often encounters data sets where a general pattern is violated by a relatively small number of exceptions (for example, a rule that says that all birds can fly is violated by examples such as penguins). This complicates the concept learning process and may lead to the rejection of some simple and expressive rules that cover many cases. In this paper we present an approach to this problem in description logic learning by computing partial descriptions (which are not necessarily entirely complete) of both positive and negative examples and combining them. Our Symmetric Parallel Class Expression Learning approach enables the generation of general rules with exception patterns included. We demonstrate that this algorithm provides significantly better results (in terms of metrics such as accuracy, search space covered, and learning time) than standard approaches on some typical data sets. Further, the approach has the added benefit that it can be parallelised relatively simply, leading to much faster exploration of the search tree on modern computers",
    "volume": "main",
    "checked": true,
    "id": "9f2e72969b0a795d349c90ba6542217afa7202b6",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/15-251.html": {
    "title": "Learning Partial Policies to Speedup MDP Tree Search via Reduction to I.I.D. Learning",
    "abstract": "A popular approach for online decision-making in large MDPs is time-bounded tree search. The effectiveness of tree search, however, is largely influenced by the action branching factor, which limits the search depth given a time bound. An obvious way to reduce action branching is to consider only a subset of potentially good actions at each state as specified by a provided partial policy. In this work, we consider offline learning of such partial policies with the goal of speeding up search without significantly reducing decision-making quality. Our first contribution consists of reducing the learning problem to set learning. We give a reduction-style analysis of three such algorithms, each making different assumptions, which relates the set learning objectives to the sub-optimality of search using the learned partial policies. Our second contribution is to describe concrete implementations of the algorithms within the popular framework of Monte-Carlo tree search. Finally, the third contribution is to evaluate the learning algorithms on two challenging MDPs with large action branching factors. The results show that the learned partial policies can significantly improve the anytime performance of Monte-Carlo tree search",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v18/15-376.html": {
    "title": "Hierarchically Compositional Kernels for Scalable Nonparametric Learning",
    "abstract": "We propose a novel class of kernels to alleviate the high computational cost of large-scale nonparametric learning with kernel methods. The proposed kernel is defined based on a hierarchical partitioning of the underlying data domain, where the NystrÃÂ¶m method (a globally low-rank approximation) is married with a locally lossless approximation in a hierarchical fashion. The kernel maintains (strict) positive-definiteness. The corresponding kernel matrix admits a recursively off- diagonal low-rank structure, which allows for fast linear algebra computations. Suppressing the factor of data dimension, the memory and arithmetic complexities for training a regression or a classifier are reduced from $O(n^2)$ and $O(n^3)$ to $O(nr)$ and $O(nr^2)$, respectively, where $n$ is the number of training examples and $r$ is the rank on each level of the hierarchy. Although other randomized approximate kernels entail a similar complexity, empirical results show that the proposed kernel achieves a matching performance with a smaller $r$. We demonstrate comprehensive experiments to show the effective use of the proposed kernel on data sizes up to the order of millions",
    "volume": "main",
    "checked": true,
    "id": "581ace3120b3ef3ae67496723f773117bf781260",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/15-482.html": {
    "title": "Sharp Oracle Inequalities for Square Root Regularization",
    "abstract": "We study a set of regularization methods for high-dimensional linear regression models. These penalized estimators have the square root of the residual sum of squared errors as loss function, and any weakly decomposable norm as penalty function. This fit measure is chosen because of its property that the estimator does not depend on the unknown standard deviation of the noise. On the other hand, a generalized weakly decomposable norm penalty is very useful in being able to deal with different underlying sparsity structures. We can choose a different sparsity inducing norm depending on how we want to interpret the unknown parameter vector $\\beta$. Structured sparsity norms, as defined in Micchelli et al. (2010), are special cases of weakly decomposable norms, therefore we also include the square root LASSO (Belloni et al., 2011), the group square root LASSO (Bunea et al., 2014) and a new method called the square root SLOPE (in a similar fashion to the SLOPE from Bogdan et al. 2015). For this collection of estimators our results provide sharp oracle inequalities with the Karush-Kuhn-Tucker conditions. We discuss some examples of estimators. Based on a simulation we illustrate some advantages of the square root SLOPE",
    "volume": "main",
    "checked": true,
    "id": "c0b7cf9b918ffbf7a990610235b0f5f82e7800df",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/15-566.html": {
    "title": "Soft Margin Support Vector Classification as Buffered Probability Minimization",
    "abstract": "In this paper, we show that the popular C-SVM, soft-margin support vector classifier is equivalent to minimization of Buffered Probability of Exceedance (bPOE), a recently introduced characterization of uncertainty. To show this, we introduce a new SVM formulation, called the EC-SVM, which is derived from a simple bPOE minimization problem that is easy to interpret with a meaningful free parameter, optimal objective value, and probabilistic derivation. Over the range of its free parameter, the EC-SVM has both a convex and non-convex case which we connect to existing SVM formulations. We first show that the C-SVM, formulated with any regularization norm, is equivalent to the convex EC-SVM. Similarly, we show that the E$\\nu$-SVM is equivalent to the EC-SVM over its entire parameter range, which includes both the convex and non-convex case. These equivalences, coupled with the interpretability of the EC-SVM, allow us to gain surprising new insights into the C-SVM and fully connect soft margin support vector classification with superquantile and bPOE concepts. We also show that the EC-SVM can easily be cast as a robust optimization problem, where bPOE is minimized with data lying in a fixed uncertainty set. This reformulation allows us to clearly differentiate between the convex and non-convex case, with convexity associated with pessimistic views of uncertainty and non-convexity associated with optimistic views of uncertainty. Finally, we address some practical considerations. First, we show that these new insights can assist in making parameter selection more efficient. Second, we discuss optimization approaches for solving the EC-SVM. Third, we address the issue of generalization, providing generalization bounds for both bPOE and misclassification rate",
    "volume": "main",
    "checked": true,
    "id": "308467c72d9e3b812d76d9bec16edcaca5586231",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v18/15-615.html": {
    "title": "Variational Particle Approximations",
    "abstract": "Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search- based techniques. DPVI is based on a novel family of particle- based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well- defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper defines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can offer appealing time/accuracy trade-offs as compared to multiple alternatives",
    "volume": "main",
    "checked": true,
    "id": "c3048b130ed9a362e40bba0a66971663db1fec73",
    "citation_count": 54
  },
  "https://jmlr.org/papers/v18/16-003.html": {
    "title": "A Bayesian Framework for Learning Rule Sets for Interpretable Classification",
    "abstract": "We present a machine learning algorithm for building classifiers that are comprised of a small number of short rules. These are restricted disjunctive normal form models. An example of a classifier of this form is as follows: If $X$ satisfies (condition $A$ AND condition $B$) OR (condition $C$) OR $\\cdots$, then $Y=1$. Models of this form have the advantage of being interpretable to human experts since they produce a set of rules that concisely describe a specific class. We present two probabilistic models with prior parameters that the user can set to encourage the model to have a desired size and shape, to conform with a domain-specific definition of interpretability. We provide a scalable MAP inference approach and develop theoretical bounds to reduce computation by iteratively pruning the search space. We apply our method (Bayesian Rule Sets -- BRS) to characterize and predict user behavior with respect to in-vehicle context-aware personalized recommender systems. Our method has a major advantage over classical associative classification methods and decision trees in that it does not greedily grow the model",
    "volume": "main",
    "checked": true,
    "id": "8bd4850cc2ac1a328deaeb97bcc76fb43284d50a",
    "citation_count": 169
  },
  "https://jmlr.org/papers/v18/16-079.html": {
    "title": "A Robust-Equitable Measure for Feature Ranking and Selection",
    "abstract": "In many applications, not all the features used to represent data samples are important. Often only a few features are relevant for the prediction task. The choice of dependence measures often affect the final result of many feature selection methods. To select features that have complex nonlinear relationships with the response variable, the dependence measure should be equitable, a concept proposed by Reshef et al. (2011); that is, the dependence measure treats linear and nonlinear relationships equally. Recently, Kinney and Atwal (2014) gave a mathematical definition of self- equitability. In this paper, we introduce a new concept of robust-equitability and identify a robust- equitable copula dependence measure, the robust copula dependence (RCD) measure. RCD is based on the $L_1$-distance of the copula density from uniform and we show that it is equitable under both equitability definitions. We also prove theoretically that RCD is much easier to estimate than mutual information. Because of these theoretical properties, the RCD measure has the following advantages compared to existing dependence measures: it is robust to different relationship forms and robust to unequal sample sizes of different features. Experiments on both synthetic and real-world data sets confirm the theoretical analysis, and illustrate the advantage of using the dependence measure RCD for feature selection",
    "volume": "main",
    "checked": true,
    "id": "2d9072ed612c1cecbf7acae29342ad7dc0a6f7cc",
    "citation_count": 44
  },
  "https://jmlr.org/papers/v18/16-108.html": {
    "title": "Multiscale Strategies for Computing Optimal Transport",
    "abstract": "This paper presents a multiscale approach to efficiently compute approximate optimal transport plans between point sets. It is particularly well-suited for point sets that are in high- dimensions, but are close to being intrinsically low- dimensional. The approach is based on an adaptive multiscale decomposition of the point sets. The multiscale decomposition yields a sequence of optimal transport problems, that are solved in a top-to-bottom fashion from the coarsest to the finest scale. We provide numerical evidence that this multiscale approach scales approximately linearly, in time and memory, in the number of nodes, instead of quadratically or worse for a direct solution. Empirically, the multiscale approach results in less than one percent relative error in the objective function. Furthermore, the multiscale plans constructed are of interest by themselves as they may be used to introduce novel features and notions of distances between point sets. An analysis of sets of brain MRI based on optimal transport distances illustrates the effectiveness of the proposed method on a real world data set. The application demonstrates that multiscale optimal transport distances have the potential to improve on state-of-the-art metrics currently used in computational anatomy",
    "volume": "main",
    "checked": true,
    "id": "a01d9d9c5915bcfd3458b19120f01dbd44633e0d",
    "citation_count": 31
  },
  "https://jmlr.org/papers/v18/16-142.html": {
    "title": "Non-parametric Policy Search with Limited Information Loss",
    "abstract": "Learning complex control policies from non-linear and redundant sensory input is an important challenge for reinforcement learning algorithms. Non-parametric methods that approximate values functions or transition models can address this problem, by adapting to the complexity of the data set. Yet, many current non-parametric approaches rely on unstable greedy maximization of approximate value functions, which might lead to poor convergence or oscillations in the policy update. A more robust policy update can be obtained by limiting the information loss between successive state-action distributions. In this paper, we develop a policy search algorithm with policy updates that are both robust and non-parametric. Our method can learn non- parametric control policies for infinite horizon continuous Markov decision processes with non-linear and redundant sensory representations. We investigate how we can use approximations of the kernel function to reduce the time requirements of the demanding non-parametric computations. In our experiments, we show the strong performance of the proposed method, and how it can be approximated efficiently. Finally, we show that our algorithm can learn a real-robot under-powered swing-up task directly from image data",
    "volume": "main",
    "checked": true,
    "id": "f325b087b0a1a3995ea559aac42cd87c2a796b9a",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v18/16-184.html": {
    "title": "Tests of Mutual or Serial Independence of Random Vectors with Applications",
    "abstract": "The problem of testing mutual independence between many random vectors is addressed. The closely related problem of testing serial independence of a multivariate stationary sequence is also considered. The MÃÂ¶bius transformation of characteristic functions is used to characterize independence. A generalization to $p$ vectors of distance covariance and Hilbert-Schmidt independence criterion ($HSIC$) tests with the translation invariant kernel of a stable probability distribution is proposed. Both test statistics can be expressed in a simple form as a sum over all elements of a componentwise product of $p$ doubly-centered matrices. It is shown that an $HSIC$ statistic with sufficiently small scale parameters is equivalent to a distance covariance statistic. Consistency and weak convergence of both types of statistics are established. Approximation of $p$-values is made by randomization tests without recomputing interpoint distances for each randomized sample. The dependogram is adapted to the proposed tests for the graphical identification of sources of dependencies. Empirical rejection rates obtained through extensive simulations confirm both the applicability of the testing procedures in small samples and the high level of competitiveness in terms of power. Applications to meteorological and financial data provide some interesting interpretations of dependencies revealed by dependograms",
    "volume": "main",
    "checked": true,
    "id": "f681220b3a123be2f27423e3aec5490bbe269d2d",
    "citation_count": 13
  },
  "https://jmlr.org/papers/v18/16-258.html": {
    "title": "Recovering PCA and Sparse PCA via Hybrid-(l1,l2) Sparse Sampling of Data Elements",
    "abstract": "This paper addresses how well we can recover a data matrix when only given a few of its elements. We present a randomized algorithm that element-wise sparsifies the data, retaining only a few of its entries. Our new algorithm independently samples the data using probabilities that depend on both squares ($\\ell_2$ sampling) and absolute values ($\\ell_1$ sampling) of the entries. We prove that this hybrid algorithm ($i$) achieves a near-PCA reconstruction of the data, and ($ii$) recovers sparse principal components of the data, from a sketch formed by a sublinear sample size. Hybrid-($\\ell_1,\\ell_2$) inherits the $\\ell_2$-ability to sample the important elements, as well as the regularization properties of $\\ell_1$ sampling, and maintains strictly better quality than either $\\ell_1$ or $\\ell_2$ on their own. Extensive experimental results on synthetic, image, text, biological, and financial data show that not only are we able to recover PCA and sparse PCA from incomplete data, but we can speed up such computations significantly using our sparse sketch",
    "volume": "main",
    "checked": false,
    "id": "1c435d538ebcaf2361a89c3f77c568c5ec4eff1c",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v18/16-296.html": {
    "title": "Quantifying the Informativeness of Similarity Measurements",
    "abstract": "In this paper, we describe an unsupervised measure for quantifying the 'informativeness' of correlation matrices formed from the pairwise similarities or relationships among data instances. The measure quantifies the heterogeneity of the correlations and is defined as the distance between a correlation matrix and the nearest correlation matrix with constant off-diagonal entries. This non-parametric notion generalizes existing test statistics for equality of correlation coefficients by allowing for alternative distance metrics, such as the Bures and other distances from quantum information theory. For several distance and dissimilarity metrics, we derive closed-form expressions of informativeness, which can be applied as objective functions for machine learning applications. Empirically, we demonstrate that informativeness is a useful criterion for selecting kernel parameters, choosing the dimension for kernel-based nonlinear dimensionality reduction, and identifying structured graphs. We also consider the problem of finding a maximally informative correlation matrix around a target matrix, and explore parameterizing the optimization in terms of the coordinates of the sample or through a lower-dimensional embedding. In the latter case, we find that maximizing the Bures-based informativeness measure, which is maximal for centered rank-1 correlation matrices, is equivalent to minimizing a specific matrix norm, and present an algorithm to solve the minimization problem using the norm's proximal operator. The proposed correlation denoising algorithm consistently improves spectral clustering. Overall, we find informativeness to be a novel and useful criterion for identifying non-trivial correlation structure",
    "volume": "main",
    "checked": true,
    "id": "be6f7ff9724de39139a9a0bcb1cf73874fc7b5c0",
    "citation_count": 13
  },
  "https://jmlr.org/papers/v18/16-305.html": {
    "title": "Time for a Change: a Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis",
    "abstract": "The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better---more sound and useful--- alternatives for it",
    "volume": "main",
    "checked": true,
    "id": "8ce2c4a374e8b37e3eef080c956f22cfc6ea25d6",
    "citation_count": 304
  },
  "https://jmlr.org/papers/v18/16-326.html": {
    "title": "Relational Reinforcement Learning for Planning with Exogenous Effects",
    "abstract": "Probabilistic planners have improved recently to the point that they can solve difficult tasks with complex and expressive models. In contrast, learners cannot tackle yet the expressive models that planners do, which forces complex models to be mostly handcrafted. We propose a new learning approach that can learn relational probabilistic models with both action effects and exogenous effects. The proposed learning approach combines a multi-valued variant of inductive logic programming for the generation of candidate models, with an optimization method to select the best set of planning operators to model a problem. We also show how to combine this learner with reinforcement learning algorithms to solve complete problems. Finally, experimental validation is provided that shows improvements over previous work in both simulation and a robotic task. The robotic task involves a dynamic scenario with several agents where a manipulator robot has to clear the tableware on a table. We show that the exogenous effects learned by our approach allowed the robot to clear the table in a more efficient way",
    "volume": "main",
    "checked": true,
    "id": "9f06375a01ffbdf1244e5217fdc8fb3d7b41cffb",
    "citation_count": 70
  },
  "https://jmlr.org/papers/v18/16-362.html": {
    "title": "Bayesian Tensor Regression",
    "abstract": "We propose a Bayesian approach to regression with a scalar response on vector and tensor covariates. Vectorization of the tensor prior to analysis fails to exploit the structure, often leading to poor estimation and predictive performance. We introduce a novel class of multiway shrinkage priors for tensor coefficients in the regression setting and present posterior consistency results under mild conditions. A computationally efficient Markov chain Monte Carlo algorithm is developed for posterior computation. Simulation studies illustrate substantial gains over existing tensor regression methods in terms of estimation and parameter inference. Our approach is further illustrated in a neuroimaging application",
    "volume": "main",
    "checked": true,
    "id": "03e2ba1bc02d230661adfcda9ed2953fb2d7f741",
    "citation_count": 87
  },
  "https://jmlr.org/papers/v18/16-429.html": {
    "title": "Robust Discriminative Clustering with Sparse Regularizers",
    "abstract": "Clustering high-dimensional data often requires some form of dimensionality reduction, where clustered variables are separated from noise-looking variables. We cast this problem as finding a low-dimensional projection of the data which is well-clustered. This yields a one-dimensional projection in the simplest situation with two clusters, and extends naturally to a multi-label scenario for more than two clusters. In this paper, (a) we first show that this joint clustering and dimension reduction formulation is equivalent to previously proposed discriminative clustering frameworks, thus leading to convex relaxations of the problem; (b) we propose a novel sparse extension, which is still cast as a convex relaxation and allows estimation in higher dimensions; (c) we propose a natural extension for the multi-label scenario; (d) we provide a new theoretical analysis of the performance of these formulations with a simple probabilistic model, leading to scalings over the form $d=O(\\sqrt{n})$ for the affine invariant case and $d=O(n)$ for the sparse case, where $n$ is the number of examples and $d$ the ambient dimension; and finally, (e) we propose an efficient iterative algorithm with running-time complexity proportional to $O(nd^2)$, improving on earlier algorithms for discriminative clustering with the square loss, which had quadratic complexity in the number of examples",
    "volume": "main",
    "checked": true,
    "id": "df66473d58b0ab0aa5decea55e684fd94438b502",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/16-466.html": {
    "title": "Making Decision Trees Feasible in Ultrahigh Feature and Label Dimensions",
    "abstract": "Due to the non-linear but highly interpretable representations, decision tree (DT) models have significantly attracted a lot of attention of researchers. However, it is difficult to understand and interpret DT models in ultrahigh dimensions and DT models usually suffer from the curse of dimensionality and achieve degenerated performance when there are many noisy features. To address these issues, this paper first presents a novel data- dependent generalization error bound for the perceptron decision tree (PDT), which provides the theoretical justification to learn a sparse linear hyperplane in each decision node and to prune the tree. Following our analysis, we introduce the notion of budget-aware classifier (BAC) with a budget constraint on the weight coefficients, and propose a supervised budgeted tree (SBT) algorithm to achieve non-linear prediction performance. To avoid generating an unstable and complicated decision tree and improve the generalization of the SBT, we present a pruning strategy by learning classifiers to minimize cross-validation errors on each BAC. To deal with ultrahigh label dimensions, based on three important phenomena of real-world data sets from a variety of application domains, we develop a sparse coding tree framework for multi-label annotation problems and provide the theoretical analysis. Extensive empirical studies verify that 1) SBT is easy to understand and interpret in ultrahigh dimensions and is more resilient to noisy features. 2) Compared with state-of-the-art algorithms, our proposed sparse coding tree framework is more efficient, yet accurate in ultrahigh label and feature dimensions",
    "volume": "main",
    "checked": true,
    "id": "70c2701361ff5e39ca5250069a1ac8dda4d4123f",
    "citation_count": 65
  },
  "https://jmlr.org/papers/v18/16-498.html": {
    "title": "Learning Scalable Deep Kernels with Recurrent Structure",
    "abstract": "Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure, and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP- LSTM are uniquely valuable",
    "volume": "main",
    "checked": true,
    "id": "0f71a2b6e26a0b97364d221d55ccfd1956fcf0f2",
    "citation_count": 98
  },
  "https://jmlr.org/papers/v18/16-505.html": {
    "title": "Convolutional Neural Networks Analyzed via Convolutional Sparse Coding",
    "abstract": "Convolutional neural networks (CNN) have led to many state-of- the-art results spanning through various fields. However, a clear and profound theoretical understanding of the forward pass, the core algorithm of CNN, is still lacking. In parallel, within the wide field of sparse approximation, Convolutional Sparse Coding (CSC) has gained increasing attention in recent years. A theoretical study of this model was recently conducted, establishing it as a reliable and stable alternative to the commonly practiced patch-based processing. Herein, we propose a novel multi-layer model, ML-CSC, in which signals are assumed to emerge from a cascade of CSC layers. This is shown to be tightly connected to CNN, so much so that the forward pass of the CNN is in fact the thresholding pursuit serving the ML-CSC model. This connection brings a fresh view to CNN, as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network, and their stable estimation, all guaranteed under simple local sparsity conditions. Lastly, identifying the weaknesses in the above pursuit scheme, we propose an alternative to the forward pass, which is connected to deconvolutional and recurrent networks, and also has better theoretical guarantees",
    "volume": "main",
    "checked": true,
    "id": "2ccdebc1ebfd103f6c97cd31059936e2e7c48dc1",
    "citation_count": 247
  },
  "https://jmlr.org/papers/v18/16-568.html": {
    "title": "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization",
    "abstract": "We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate (SPDC) method, which alternates between maximizing over a randomly chosen dual variable and minimizing over the primal variables. An extrapolation step on the primal variables is performed to obtain accelerated convergence rate. We also develop a mini-batch version of the SPDC method which facilitates parallel computing, and an extension with weighted sampling probabilities on the dual variables, which has a better complexity than uniform sampling on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods",
    "volume": "main",
    "checked": true,
    "id": "75359c49a6abdd5fba642f46ec44812ed8e8a648",
    "citation_count": 252
  },
  "https://jmlr.org/papers/v18/17-003.html": {
    "title": "Angle-based Multicategory Distance-weighted SVM",
    "abstract": "Classification is an important supervised learning technique with numerous applications. We develop an angle-based multicategory distance-weighted support vector machine (MDWSVM) classification method that is motivated from the binary distance-weighted support vector machine (DWSVM) classification method. The new method has the merits of both support vector machine (SVM) and distance-weighted discrimination (DWD) but also alleviates both the data piling issue of SVM and the imbalanced data issue of DWD. Theoretical and numerical studies demonstrate the advantages of MDWSVM method over existing angle-based methods",
    "volume": "main",
    "checked": true,
    "id": "a877b837fbb0942dd3eccc3c1a00d3bc2af16e52",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/17-032.html": {
    "title": "Minimax Estimation of Kernel Mean Embeddings",
    "abstract": "In this paper, we study the minimax estimation of the Bochner integral \\[ \\mu_k(P) := \\int_\\mathcal{X} k(\\cdot,x)\\, dP(x), \\] also called the kernel mean embedding, based on random samples drawn i.i.d. from $P$, where $k:\\mathcal{X}\\times\\mathcal{X}\\rightarrow \\mathbb{R}$ is a positive definite kernel.  Various estimators (including the empirical estimator), $\\hat{\\theta}_n$ of $\\mu_k(P)$ are studied in the literature wherein all of them satisfy $\\|\\hat{\\theta}_n-\\mu_k(P)\\|_{\\mathcal{H}_k}=O_P(n^{-1/2})$ with $\\mathcal{H}_k$ being the reproducing kernel Hilbert space induced by $k$.  The main contribution of the paper is in showing that the above mentioned rate of $n^{-1/2}$ is minimax in $\\|\\cdot\\|_{\\mathcal{H}_k}$ and $\\|\\cdot\\|_{L^2(\\mathbb{R}^d)}$-norms over the class of discrete measures and the class of measures that has an infinitely differentiable density, with $k$ being a continuous translation- invariant kernel on $\\mathbb{R}^d$. The interesting aspect of this result is that the minimax rate is independent of the smoothness of the kernel and the density of $P$ (if it exists)",
    "volume": "main",
    "checked": true,
    "id": "1911e2685d9b471203f53da4d437c9adcf406cac",
    "citation_count": 67
  },
  "https://jmlr.org/papers/v18/17-039.html": {
    "title": "The Impact of Random Models on Clustering Similarity",
    "abstract": "Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, multiple runs of K-means clustering reurns clusterings with a fixed number of clusters, while the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on the ranking of similar clustering pairs, and the evaluation of a clustering method with respect to a random baseline; thus, the choice of random clustering model should be carefully justified",
    "volume": "main",
    "checked": true,
    "id": "9e115c09286b51b20a40e59bffd5d7c7f063ecfe",
    "citation_count": 93
  },
  "https://jmlr.org/papers/v18/17-081.html": {
    "title": "Hierarchical Clustering via Spreading Metrics",
    "abstract": "We study the cost function for hierarchical clusterings introduced by (Dasgupta, 2016) where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in (Dasgupta, 2016) that a top-down algorithm based on the uniform Sparsest Cut problem returns a hierarchical clustering of cost at most $O\\left(\\alpha_n \\log n\\right)$ times the cost of the optimal hierarchical clustering, where $\\alpha_n$ is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao- Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most $O\\left(\\log^{3/2} n\\right)$ times the cost of the optimal solution. We improve this by giving an $O(\\log{n})$-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an $O(\\log{n})$- approximate hierarchical clustering for a generalization of this cost function also studied in (Dasgupta, 2016). Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the Small Set Expansion hypothesis",
    "volume": "main",
    "checked": true,
    "id": "24bdbbfa99c2d951e7d684100cd5b2f70a7a8b56",
    "citation_count": 66
  },
  "https://jmlr.org/papers/v18/17-156.html": {
    "title": "The MADP Toolbox: An Open Source Library for Planning and Learning in (Multi-)Agent Systems",
    "abstract": "This article describes the Multiagent Decision Process (MADP) Toolbox, a software library to support planning and learning for intelligent agents and multiagent systems in uncertain environments. Key features are that it supports partially observable environments and stochastic transition models; has unified support for single- and multiagent systems; provides a large number of models for decision-theoretic decision making, including one-shot and sequential decision making under various assumptions of observability and cooperation, such as Dec-POMDPs and POSGs; provides tools and parsers to quickly prototype new problems; provides an extensive range of planning and learning algorithms for single- and multiagent systems; is released under the GNU GPL v3 license; and is written in C++ and designed to be extensible via the object-oriented paradigm",
    "volume": "MLOSS",
    "checked": true,
    "id": "6801be1bcc57e454f0109008d674f081361cfe6c",
    "citation_count": 49
  },
  "https://jmlr.org/papers/v18/14-428.html": {
    "title": "A survey of Algorithms and Analysis for Adaptive Online Learning",
    "abstract": "We present tools for the analysis of Follow-The-Regularized- Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, prox-function or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens previously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between adaptive Mirror Descent algorithms and a corresponding FTRL update, which allows us to analyze Mirror Descent algorithms in the same framework. The key to bridging the gap between Dural Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non- smooth regularizers with time-varying weight",
    "volume": "main",
    "checked": true,
    "id": "b86524dd0e2eba0f1b6e56bd2b1c0b0fcd28d60b",
    "citation_count": 143
  },
  "https://jmlr.org/papers/v18/14-484.html": {
    "title": "A distributed block coordinate descent method for training l1 regularized linear classifiers",
    "abstract": "Distributed training of $l_1$ regularized classifiers has received great attention recently. Most existing methods approach this problem by taking steps obtained from approximating the objective by a quadratic approximation that is decoupled at the individual variable level. These methods are designed for multicore systems where communication costs are low. They are inefficient on systems such as Hadoop running on a cluster of commodity machines where communication costs are substantial. In this paper we design a distributed algorithm for $l_1$ regularization that is much better suited for such systems than existing algorithms. A careful cost analysis is used to support these points and motivate our method. The main idea of our algorithm is to do block optimization of many variables on the actual objective function within each computing node; this increases the computational cost per step that is matched with the communication cost, and decreases the number of outer iterations, thus yielding a faster overall method. Distributed Gauss-Seidel and Gauss-Southwell greedy schemes are used for choosing variables to update in each step. We establish global convergence theory for our algorithm, including Q-linear rate of convergence. Experiments on two benchmark problems show our method to be much faster than existing methods",
    "volume": "main",
    "checked": false,
    "id": "e6a37d32a48faf943d485c3da05f428c90eb1e4d",
    "citation_count": 33
  },
  "https://jmlr.org/papers/v18/15-586.html": {
    "title": "Distributed Learning with Regularized Least Squares",
    "abstract": "We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds and learning rates in expectation in both the $L^2$-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our derived learning rates in expectation are optimal and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in expectation in the literature",
    "volume": "main",
    "checked": true,
    "id": "c1a46d9a0972fb0ec0977d24a191f612e7401369",
    "citation_count": 157
  }
}