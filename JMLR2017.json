{
  "https://jmlr.org/papers/v18/14-249.html": {
    "title": "Averaged Collapsed Variational Bayes Inference",
    "abstract": "This paper presents the Averaged CVB (ACVB) inference and offers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence- guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non- expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence",
    "volume": "main",
    "checked": true,
    "id": "0f4cfa8fea4047d20217255964169ade91547fcc",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v18/14-400.html": {
    "title": "Scalable Influence Maximization for Multiple Products in Continuous-Time Diffusion Networks",
    "abstract": "A typical viral marketing model identifies influential users in a social network to maximize a single product adoption assuming unlimited user attention, campaign budgets, and time. In reality, multiple products need campaigns, users have limited attention, convincing users incurs costs, and advertisers have limited budgets and expect the adoptions to be maximized soon. Facing these user, monetary, and timing constraints, we formulate the problem as a submodular maximization task in a continuous-time diffusion model under the intersection of one matroid and multiple knapsack constraints. We propose a randomized algorithm estimating the user influence (Partial results in the paper on influence estimation have been published in a conference paper: Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha. Scalable influence estimation in continuous time diffusion networks. In Advances in Neural Information Processing Systems 26, 2013.) in a network ($|\\mathcal{V}|$ nodes, $|\\mathcal{E}|$ edges) to an accuracy of $\\epsilon$ with $n=\\mathcal{O}(1/\\epsilon^2)$ randomizations and $\\tilde{\\mathcal{O}}(n|\\mathcal{E}|+n|\\mathcal{V}|)$ computations. By exploiting the influence estimation algorithm as a subroutine, we develop an adaptive threshold greedy algorithm achieving an approximation factor $k_a/(2+2 k)$ of the optimal when $k_a$ out of the $k$ knapsack constraints are active. Extensive experiments on networks of millions of nodes demonstrate that the proposed algorithms achieve the state-of- the-art in terms of effectiveness and scalability",
    "volume": "main",
    "checked": true,
    "id": "2689fcd90e0654306b51fd67eb8c4ecafc156293",
    "citation_count": 31
  },
  "https://jmlr.org/papers/v18/15-085.html": {
    "title": "Local algorithms for interactive clustering",
    "abstract": "We study the design of interactive clustering algorithms. The user supervision that we consider is in the form of cluster split/merge requests; such feedback is easy for users to provide because it only requires a high-level understanding of the clusters. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable properties in many applications. Local changes are desirable because in practice edits of other parts of the clustering are considered churn - changes that are perceived as quality-neutral or quality-negative. We show that in this framework we can still design provably correct algorithms given that our data satisfies natural separability properties. We also show that our framework works well in practice",
    "volume": "main",
    "checked": true,
    "id": "042b60abf09707305523b70d87312817654fe180",
    "citation_count": 94
  },
  "https://jmlr.org/papers/v18/15-492.html": {
    "title": "SnapVX: A Network-Based Convex Optimization Solver",
    "abstract": "SnapVX is a high-performance solver for convex optimization problems defined on networks. For problems of this form, SnapVX provides a fast and scalable solution with guaranteed global convergence. It combines the capabilities of two open source software packages: Snap.py and CVXPY. Snap.py is a large scale graph processing library, and CVXPY provides a general modeling framework for small-scale subproblems. SnapVX offers a customizable yet easy-to-use Python interface with out-of- the- box functionality. Based on the Alternating Direction Method of Multipliers (ADMM), it is able to efficiently store, analyze, parallelize, and solve large optimization problems from a variety of different applications. Documentation, examples, and more can be found on the SnapVX website at snap.stanford.edu/snapvx",
    "volume": "main",
    "checked": true,
    "id": "e6ffdba48af4e48a54cadd5fb27bc9244e04b027",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v18/16-002.html": {
    "title": "Communication-efficient Sparse Regression",
    "abstract": "We devise a communication-efficient approach to distributed sparse regression in the high-dimensional setting. The key idea is to average debiased or desparsified lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines, and consistently estimates the support under weaker conditions than the lasso. On the computational side, we propose a new parallel and computationally-efficient algorithm to compute the approximate inverse covariance required in the debiasing approach, when the dataset is split across samples. We further extend the approach to generalized linear models",
    "volume": "main",
    "checked": true,
    "id": "23a96514f84e452aaa921e8ea3b65b396cab52c8",
    "citation_count": 140
  },
  "https://jmlr.org/papers/v18/16-070.html": {
    "title": "Improving Variational Methods via Pairwise Linear Response Identities",
    "abstract": "Inference methods are often formulated as variational approximations: these approxima- tions allow easy evaluation of statistics by marginalization or linear response, but these estimates can be inconsistent. We show that by introducing constraints on covariance, one can ensure consistency of linear response with the variational parameters, and in so doing inference of marginal probability distributions is improved. For the Bethe approximation and its generalizations, improvements are achieved with simple choices of the constraints. The approximations are presented as variational frameworks; iterative procedures related to message passing are provided for finding the minima",
    "volume": "main",
    "checked": true,
    "id": "d5685cbae73b502f0912dcf0f32ccd3d7d84d152",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v18/16-270.html": {
    "title": "Distributed Sequence Memory of Multidimensional Inputs in Recurrent Networks",
    "abstract": "Recurrent neural networks (RNNs) have drawn interest from machine learning researchers because of their effectiveness at preserving past inputs for time-varying data processing tasks. To understand the success and limitations of RNNs, it is critical that we advance our analysis of their fundamental memory properties. We focus on echo state networks (ESNs), which are RNNs with simple memoryless nodes and random connectivity. In most existing analyses, the short-term memory (STM) capacity results conclude that the ESN network size must scale linearly with the input size for unstructured inputs. The main contribution of this paper is to provide general results characterizing the STM capacity for linear ESNs with multidimensional input streams when the inputs have common low- dimensional structure: sparsity in a basis or significant statistical dependence between inputs. In both cases, we show that the number of nodes in the network must scale linearly with the information rate and poly-logarithmically with the input dimension. The analysis relies on advanced applications of random matrix theory and results in explicit non-asymptotic bounds on the recovery error. Taken together, this analysis provides a significant step forward in our understanding of the STM properties in RNNs",
    "volume": "main",
    "checked": true,
    "id": "589f5953378461f86d83c303451392bed9eab760",
    "citation_count": 20
  },
  "https://jmlr.org/papers/v18/16-337.html": {
    "title": "Persistence Images: A Stable Vector Representation of Persistent Homology",
    "abstract": "Many data sets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a data set. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite- dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs",
    "volume": "main",
    "checked": true,
    "id": "223841a71f5bce4cb03040e229d13e9a71b78ec3",
    "citation_count": 492
  },
  "https://jmlr.org/papers/v18/14-318.html": {
    "title": "Spectral Clustering Based on Local PCA",
    "abstract": "We propose a spectral clustering method based on local principal components analysis (PCA). After performing local PCA in selected neighborhoods, the algorithm builds a nearest neighbor graph weighted according to a discrepancy between the principal subspaces in the neighborhoods, and then applies spectral clustering. As opposed to standard spectral methods based solely on pairwise distances between points, our algorithm is able to resolve intersections. We establish theoretical guarantees for simpler variants within a prototypical mathematical framework for multi-manifold clustering, and evaluate our algorithm on various simulated data sets",
    "volume": "main",
    "checked": true,
    "id": "d3599be5b9a42da86748b3e87ffff42502413a9a",
    "citation_count": 94
  },
  "https://jmlr.org/papers/v18/15-038.html": {
    "title": "On Perturbed Proximal Gradient Algorithms",
    "abstract": "We study a version of the proximal gradient algorithm for which the gradient is intractable and is approximated by Monte Carlo methods (and in particular Markov Chain Monte Carlo). We derive conditions on the step size and the Monte Carlo batch size under which convergence is guaranteed: both increasing batch size and constant batch size are considered. We also derive non- asymptotic bounds for an averaged version. Our results cover both the cases of biased and unbiased Monte Carlo approximation. To support our findings, we discuss the inference of a sparse generalized linear model with random effect and the problem of learning the edge structure and parameters of sparse undirected graphical models",
    "volume": "main",
    "checked": true,
    "id": "aab2c7269ef68b9ef9ee5f8b8e90596b2fbae670",
    "citation_count": 93
  },
  "https://jmlr.org/papers/v18/15-257.html": {
    "title": "Differential Privacy for Bayesian Inference through Posterior Sampling",
    "abstract": "Differential privacy formalises privacy-preserving mechanisms that provide access to a database. Can Bayesian inference be used directly to provide private access to data? The answer is yes: under certain conditions on the prior, sampling from the posterior distribution can lead to a desired level of privacy and utility. For a uniform treatment, we define differential privacy over arbitrary data set metrics, outcome spaces and distribution families. This allows us to also deal with non-i.i.d or non-tabular data sets. We then prove bounds on the sensitivity of the posterior to the data, which delivers a measure of robustness. We also show how to use posterior sampling to provide differentially private responses to queries, within a decision-theoretic framework. Finally, we provide bounds on the utility of answers to queries and on the ability of an adversary to distinguish between data sets. The latter are complemented by a novel use of Le Cam's method to obtain lower bounds on distinguishability. Our results hold for arbitrary metrics, including those for the common definition of differential privacy. For specific choices of the metric, we give a number of examples satisfying our assumptions",
    "volume": "main",
    "checked": true,
    "id": "80de56e417814d71b6b544aa592b2bec28efe1a4",
    "citation_count": 57
  },
  "https://jmlr.org/papers/v18/15-441.html": {
    "title": "Refinery: An Open Source Topic Modeling Web Platform",
    "abstract": "We introduce Refinery, an open source platform for exploring large text document collections with topic models. Refinery is a standalone web application driven by a graphical interface, so it is usable by those without machine learning or programming expertise. Users can interactively organize articles by topic and also refine this organization with phrase-level analysis. Under the hood, we train Bayesian nonparametric topic models that can adapt model complexity to the provided data with scalable learning algorithms.  The project website  contains Python code and further documentation",
    "volume": "main",
    "checked": true,
    "id": "ea3ec90c9c0a921f374f5d6697d60b1527b3eadc",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v18/15-449.html": {
    "title": "Using Conceptors to Manage Neural Long-Term Memories for Temporal Patterns",
    "abstract": "Biological brains can learn, recognize, organize, and re- generate large repertoires of temporal patterns. Here I propose a mechanism of neurodynamical pattern learning and representation, called conceptors, which offers an integrated account of a number of such phenomena and functionalities. It becomes possible to store a large number of temporal patterns in a single recurrent neural network. In the recall process, stored patterns can be morphed and focussed. Parametric families of patterns can be learnt from a very small number of examples. Stored temporal patterns can be content- addressed in ways that are analog to recalling static patterns in Hopfield networks",
    "volume": "main",
    "checked": true,
    "id": "0c497a5b95132d141dd7c310a20c7acbcd32c7c4",
    "citation_count": 42
  },
  "https://jmlr.org/papers/v18/16-107.html": {
    "title": "Automatic Differentiation Variational Inference",
    "abstract": "Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop ADVI. Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models ---no conjugacy assumptions are required. We study ADVI across ten modern probabilistic models and apply it to a dataset with millions of observations. We deploy ADVI as part of Stan, a probabilistic programming system",
    "volume": "main",
    "checked": true,
    "id": "30691d2a4eb1a6e88116c357e95b49f9573bcdae",
    "citation_count": 592
  },
  "https://jmlr.org/papers/v18/16-174.html": {
    "title": "Empirical Evaluation of Resampling Procedures for Optimising SVM Hyperparameters",
    "abstract": "Tuning the regularisation and kernel hyperparameters is a vital step in optimising the generalisation performance of kernel methods, such as the support vector machine (SVM). This is most often performed by minimising a resampling/cross-validation based model selection criterion, however there seems little practical guidance on the most suitable form of resampling. This paper presents the results of an extensive empirical evaluation of resampling procedures for SVM hyperparameter selection, designed to address this gap in the machine learning literature. We tested 15 different resampling procedures on 121 binary classification data sets in order to select the best SVM hyperparameters. We used three very different statistical procedures to analyse the results: the standard multi- classifier/multi-data set procedure proposed by Dem\\v{s}ar, the confidence intervals on the excess loss of each procedure in relation to 5-fold cross validation, and the Bayes factor analysis proposed by Barber. We conclude that a 2-fold procedure is appropriate to select the hyperparameters of an SVM for data sets for 1000 or more datapoints, while a 3-fold procedure is appropriate for smaller data sets",
    "volume": "main",
    "checked": true,
    "id": "79043b3bf656e7b3c475e07d3b9a473b37ce047c",
    "citation_count": 44
  },
  "https://jmlr.org/papers/v18/16-274.html": {
    "title": "A Unified Formulation and Fast Accelerated Proximal Gradient Method for Classification",
    "abstract": "Binary classification is the problem of predicting the class a given sample belongs to. To achieve a good prediction performance, it is important to find a suitable model for a given dataset. However, it is often time consuming and impractical for practitioners to try various classification models because each model employs a different formulation and algorithm. The difficulty can be mitigated if we have a unified formulation and an efficient universal algorithmic framework for various classification models to expedite the comparison of performance of different models for a given dataset. In this paper, we present a unified formulation of various classification models (including $C$-SVM, $\\ell_2$-SVM, $\\nu$-SVM, MM-FDA, MM-MPM, logistic regression, distance weighted discrimination) and develop a general optimization algorithm based on an accelerated proximal gradient (APG) method for the formulation. We design various techniques such as backtracking line search and adaptive restarting strategy in order to speed up the practical convergence of our method. We also give a theoretical convergence guarantee for the proposed fast APG algorithm. Numerical experiments show that our algorithm is stable and highly competitive to specialized algorithms designed for specific models (e.g., sequential minimal optimization (SMO) for SVM)",
    "volume": "main",
    "checked": true,
    "id": "810a874a6b9105f48287ca86da7c6ee0ce5300c6",
    "citation_count": 23
  },
  "https://jmlr.org/papers/v18/16-365.html": {
    "title": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning",
    "abstract": "imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the- art methods can be categorized into 4 groups: (i) under- sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox depends only on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn  and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. Source code, binaries, and documentation can be downloaded from  github.com/scikit-learn-contrib/imbalanced-learn",
    "volume": "main",
    "checked": true,
    "id": "05c5b732fb92546c7d6eeabfadb5c14610d07373",
    "citation_count": 1457
  },
  "https://jmlr.org/papers/v18/14-467.html": {
    "title": "Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles",
    "abstract": "",
    "volume": "main",
    "checked": true,
    "id": "a866479d17163dff4a330cfa1682b6c86f74748b",
    "citation_count": 195
  },
  "https://jmlr.org/papers/v18/14-546.html": {
    "title": "Breaking the Curse of Dimensionality with Convex Neural Networks",
    "abstract": "We consider neural networks with a single hidden layer and non- decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity- inducing norms on the input weights, we show that high- dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non- convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non- convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomial-time algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities",
    "volume": "main",
    "checked": true,
    "id": "f9c2ece8262f9dcf4ec176799e88e51adb1fd052",
    "citation_count": 566
  },
  "https://jmlr.org/papers/v18/15-025.html": {
    "title": "Memory Efficient Kernel Approximation",
    "abstract": "Scaling kernel machines to massive data sets is a major challenge due to storage and computation issues in handling large kernel matrices, that are usually dense. Recently, many papers have suggested tackling this problem by using a low-rank approximation of the kernel matrix. In this paper, we first make the observation that the structure of shift-invariant kernels changes from low-rank to block-diagonal (without any low-rank structure) when varying the scale parameter. Based on this observation, we propose a new kernel approximation framework -- Memory Efficient Kernel Approximation (MEKA), which considers both low-rank and clustering structure of the kernel matrix. We show that the resulting algorithm outperforms state-of-the-art low-rank kernel approximation methods in terms of speed, approximation error, and memory usage. As an example, on the covtype dataset with half a million samples, MEKA takes around 70 seconds and uses less than 80 MB memory on a single machine to achieve 10% relative approximation error, while standard NystrÃÂ¶m approximation is about 6 times slower and uses more than 400MB memory to achieve similar approximation. We also present extensive experiments on applying MEKA to speed up kernel ridge regression",
    "volume": "main",
    "checked": true,
    "id": "58c85498e23c86f526223e661e250007794c8d67",
    "citation_count": 149
  },
  "https://jmlr.org/papers/v18/15-178.html": {
    "title": "On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions",
    "abstract": "We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in $L_2$- and $L_\\infty$-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipshitz-continuous losses",
    "volume": "main",
    "checked": true,
    "id": "990f341846223e80a4c5fbd5c2be309eb5c8bec9",
    "citation_count": 251
  },
  "https://jmlr.org/papers/v18/15-486.html": {
    "title": "Analyzing Tensor Power Method Dynamics in Overcomplete Regime",
    "abstract": "We present a novel analysis of the dynamics of tensor power iterations in the overcomplete regime where the tensor CP rank is larger than the input dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in general. We consider the case where the tensor components are randomly drawn, and show that the simple power iteration recovers the components with bounded error under mild initialization conditions. We apply our analysis to unsupervised learning of latent variable models, such as multi-view mixture models and spherical Gaussian mixtures. Given the third order moment tensor, we learn the parameters using tensor power iterations. We prove it can correctly learn the model parameters when the number of hidden components $k$ is much larger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize the power iterations with data samples and prove its success under mild conditions on the signal-to-noise ratio of the samples. Our analysis significantly expands the class of latent variable models where spectral methods are applicable. Our analysis also deals with noise in the input tensor leading to sample complexity result in the application to learning latent variable models",
    "volume": "main",
    "checked": true,
    "id": "d7d0f0cfe67f84ef3adda30468bdd53fd96d571e",
    "citation_count": 46
  },
  "https://jmlr.org/papers/v18/16-131.html": {
    "title": "JSAT: Java Statistical Analysis Tool, a Library for Machine Learning",
    "abstract": "Java Statistical Analysis Tool (JSAT) is a Machine Learning library written in pure Java. It works to fill a void in the Java ecosystem for a general purpose library that is relatively high performance and flexible, which is not adequately fulfilled by Weka (Hall et al., 2009) and Java-ML (Abeel et al., 2009). Almost all of the algorithms are independently implemented using an Object- Oriented framework. JSAT is made available under the GNU GPL license here:  github.com/EdwardRaff/JSAT",
    "volume": "main",
    "checked": true,
    "id": "8347a5ae3a622ce0aaa7dd674020c18ea9c89fdf",
    "citation_count": 42
  },
  "https://jmlr.org/papers/v18/16-172.html": {
    "title": "Identifying a Minimal Class of Models for High--dimensional Data",
    "abstract": "Model selection consistency in the high--dimensional regression setting can be achieved only if strong assumptions are fulfilled. We therefore suggest to pursue a different goal, which we call a minimal class of models. The minimal class of models includes models that are similar in their prediction accuracy but not necessarily in their elements. We suggest a random search algorithm to reveal candidate models. The algorithm implements simulated annealing while using a score for each predictor that we suggest to derive using a combination of the lasso and the elastic net. The utility of using a minimal class of models is demonstrated in the analysis of two data sets",
    "volume": "main",
    "checked": false,
    "id": "3c7c2f42e87caf4d65a5a0e2d7dafc71a451aa7c",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v18/16-261.html": {
    "title": "Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA",
    "abstract": "WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA's learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm",
    "volume": "main",
    "checked": true,
    "id": "cef5f429a2ee5bad3a258c7baf7cc77e9af047a3",
    "citation_count": 613
  },
  "https://jmlr.org/papers/v18/16-300.html": {
    "title": "POMDPs.jl: A Framework for Sequential Decision Making under Uncertainty",
    "abstract": "POMDPs.jl is an open-source framework for solving Markov decision processes (MDPs) and partially observable MDPs (POMDPs). POMDPs.jl allows users to specify sequential decision making problems with minimal effort without sacrificing the expressive nature of POMDPs, making this framework viable for both educational and research purposes. It is written in the Julia language to allow flexible prototyping and large-scale computation that leverages the high-performance nature of the language. The associated JuliaPOMDP community also provides a number of state-of-the-art MDP and POMDP solvers and a rich library of support tools to help with implementing new solvers and evaluating the solution results. The most recent version of POMDPs.jl, the related packages, and documentation can be found at github.com/ JuliaPOMDP/POMDPs.jl",
    "volume": "main",
    "checked": true,
    "id": "5c84814b138a6c882ffed34d89bd8882077a5fed",
    "citation_count": 100
  },
  "https://jmlr.org/papers/v18/10-231.html": {
    "title": "Generalized P{\\'o}lya Urn for Time-Varying Pitman-Yor Processes",
    "abstract": "This article introduces a class of first-order stationary time- varying Pitman-Yor processes. Subsuming our construction of time-varying Dirichlet processes presented in (Caron et al., 2007), these models can be used for time-dynamic density estimation and clustering. Our intuitive and simple construction relies on a generalized PÃÂ³lya urn scheme. Significantly, this construction yields marginal distributions at each time point that can be explicitly characterized and easily controlled. Inference is performed using Markov chain Monte Carlo and sequential Monte Carlo methods. We demonstrate our models and algorithms on epidemiological and video tracking data",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v18/15-397.html": {
    "title": "Particle Gibbs Split-Merge Sampling for Bayesian Inference in Mixture Models",
    "abstract": "This paper presents an original Markov chain Monte Carlo method to sample from the posterior distribution of conjugate mixture models. This algorithm relies on a flexible split-merge procedure built using the particle Gibbs sampler introduced in Andrieu et al. (2009, 2010). The resulting so-called Particle Gibbs Split-Merge sampler does not require the computation of a complex acceptance ratio and can be implemented using existing sequential Monte Carlo libraries. We investigate its performance experimentally on synthetic problems as well as on geolocation data. Our results show that for a given computational budget, the Particle Gibbs Split-Merge sampler empirically outperforms existing split merge methods. The code and instructions allowing to reproduce the experiments is available at github.com/aroth85/pgsm",
    "volume": "main",
    "checked": true,
    "id": "b9267c0214fcb6e5aacafe93eff1ad76f1fd56a6",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v18/15-613.html": {
    "title": "Certifiably Optimal Low Rank Factor Analysis",
    "abstract": "Factor Analysis (FA) is a technique of fundamental importance that is widely used in classical and modern multivariate statistics, psychometrics, and econometrics. In this paper, we revisit the classical rank-constrained FA problem which seeks to approximate an observed covariance matrix ($\\B\\Sigma$) by the sum of a Positive Semidefinite (PSD) low-rank component ($\\B\\Theta$) and a diagonal matrix ($\\B\\Phi$) (with nonnegative entries) subject to $\\B\\Sigma - \\B\\Phi$ being PSD. We propose a flexible family of rank-constrained, nonlinear Semidefinite Optimization based formulations for this task. We introduce a reformulation of the problem as a smooth optimization problem with convex, compact constraints and propose a unified algorithmic framework, utilizing state of the art techniques in nonlinear optimization to obtain high-quality feasible solutions for our proposed formulation. At the same time, by using a variety of techniques from discrete and global optimization, we show that these solutions are certifiably optimal in many cases, even for problems with thousands of variables. Our techniques are general and make no assumption on the underlying problem data. The estimator proposed herein aids statistical interpretability and provides computational scalability and significantly improved accuracy when compared to current, publicly available popular methods for rank-constrained FA. We demonstrate the effectiveness of our proposal on an array of synthetic and real-life datasets. To our knowledge, this is the first paper that demonstrates how a previously intractable rank-constrained optimization problem can be solved to provable optimality by coupling developments in convex analysis and in global and discrete optimization",
    "volume": "main",
    "checked": true,
    "id": "2523b796d50720685258144eb66585855c3eb018",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v18/15-651.html": {
    "title": "Group Sparse Optimization via lp,q Regularization",
    "abstract": "In this paper, we investigate a group sparse optimization problem via $\\ell_{p,q}$ regularization in three aspects: theory, algorithm and application. In the theoretical aspect, by introducing a notion of group restricted eigenvalue condition, we establish an oracle property and a global recovery bound of order $\\mathcal{O}(\\lambda^\\frac{2}{2-q})$ for any point in a level set of the $\\ell_{p,q}$ regularization problem, and by virtue of modern variational analysis techniques, we also provide a local analysis of recovery bound of order $\\mathcal{O}(\\lambda^2)$ for a path of local minima. In the algorithmic aspect, we apply the well-known proximal gradient method to solve the $\\ell_{p,q}$ regularization problems, either by analytically solving some specific $\\ell_{p,q}$ regularization subproblems, or by using the Newton method to solve general $\\ell_{p,q}$ regularization subproblems. In particular, we establish a local linear convergence rate of the proximal gradient method for solving the $\\ell_{1,q}$ regularization problem under some mild conditions and by first proving a second-order growth condition. As a consequence, the local linear convergence rate of proximal gradient method for solving the usual $\\ell_{q}$ regularization problem ($0<q<1$) is obtained. Finally in the aspect of application, we present some numerical results on both the simulated data and the real data in gene transcriptional regulation",
    "volume": "main",
    "checked": true,
    "id": "1db7e65053bb5a2e11659c0d851609e1ec8d6f0a",
    "citation_count": 57
  },
  "https://jmlr.org/papers/v18/16-460.html": {
    "title": "Preference-based Teaching",
    "abstract": "We introduce a new model of teaching named preference-based teaching and a corresponding complexity parameter---the preference-based teaching dimension (PBTD)---representing the worst-case number of examples needed to teach any concept in a given concept class. Although the PBTD coincides with the well- known recursive teaching dimension (RTD) on finite classes, it is radically different on infinite ones: the RTD becomes infinite already for trivial infinite classes (such as half- intervals) whereas the PBTD evaluates to reasonably small values for a wide collection of infinite classes including classes consisting of so-called closed sets w.r.t. a given closure operator, including various classes related to linear sets over $\\mathbb{N}_0$ (whose RTD had been studied quite recently) and including the class of Euclidean half-spaces. On top of presenting these concrete results, we provide the reader with a theoretical framework (of a combinatorial flavor) which helps to derive bounds on the PBTD",
    "volume": "main",
    "checked": true,
    "id": "39043c793428b36b53451f89bc885d91bdf5cf29",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v18/13-336.html": {
    "title": "Nonparametric Risk Bounds for Time-Series Forecasting",
    "abstract": "We derive generalization error bounds for traditional time- series forecasting models. Our results hold for many standard forecasting tools including autoregressive models, moving average models, and, more generally, linear state-space models. These non-asymptotic bounds need only weak assumptions on the data-generating process, yet allow forecasters to select among competing models and to guarantee, with high probability, that their chosen model will perform well. We motivate our techniques with and apply them to standard economic and financial forecasting tools---a GARCH model for predicting equity volatility and a dynamic stochastic general equilibrium model (DSGE), the standard tool in macroeconomic forecasting. We demonstrate in particular how our techniques can aid forecasters and policy makers in choosing models which behave well under uncertainty and mis-specification",
    "volume": "main",
    "checked": true,
    "id": "11699a49dfe5c469267cfa63ae31e7fe176c7d52",
    "citation_count": 26
  },
  "https://jmlr.org/papers/v18/14-188.html": {
    "title": "Online Bayesian Passive-Aggressive Learning",
    "abstract": "We present online Bayesian Passive-Aggressive (BayesPA) learning, a generic online learning framework for hierarchical Bayesian models with max-margin posterior regularization. We show that BayesPA subsumes the standard online Passive- Aggressive (PA) learning and extends naturally to incorporate latent variables for both parametric and nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. As an important example, we apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric BayesPA topic models to infer the unknown number of topics in an online manner. Experimental results on 20newsgroups and a large Wikipedia multi-label dataset (with 1.1 millions of training documents and 0.9 million of unique terms in the vocabulary) show that our approaches significantly improve time efficiency while achieving comparable accuracy with the corresponding batch algorithms",
    "volume": "main",
    "checked": true,
    "id": "3e4d9583480e21dee3cdd8b5c587ca63af194a7b",
    "citation_count": 37
  },
  "https://jmlr.org/papers/v18/15-104.html": {
    "title": "Asymptotic Analysis of Objectives Based on Fisher Information in Active Learning",
    "abstract": "Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for a more efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also allows us to give insight into the development of new active learning approaches based on this objective",
    "volume": "main",
    "checked": true,
    "id": "4cdcbc5f8ff509904b432eed361af1f1be714a7c",
    "citation_count": 24
  },
  "https://jmlr.org/papers/v18/15-468.html": {
    "title": "A Spectral Algorithm for Inference in Hidden semi-Markov Models",
    "abstract": "Hidden semi-Markov models (HSMMs) are latent variable models which allow latent state persistence and can be viewed as a generalization of the popular hidden Markov models (HMMs). In this paper, we introduce a novel spectral algorithm to perform inference in HSMMs. Unlike expectation maximization (EM), our approach correctly estimates the probability of given observation sequence based on a set of training sequences. Our approach is based on estimating moments from the sample, whose number of dimensions depends only logarithmically on the maximum length of the hidden state persistence. Moreover, the algorithm requires only a few matrix inversions and is therefore computationally efficient. Empirical evaluations on synthetic and real data demonstrate the advantage of the algorithm over EM in terms of speed and accuracy, especially for large data sets",
    "volume": "main",
    "checked": true,
    "id": "aa3899d2c1d13aaf9670e89cd478a84c8e467274",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v18/16-166.html": {
    "title": "Simplifying Probabilistic Expressions in Causal Inference",
    "abstract": "Obtaining a non-parametric expression for an interventional distribution is one of the most fundamental tasks in causal inference. Such an expression can be obtained for an identifiable causal effect by an algorithm or by manual application of do-calculus. Often we are left with a complicated expression which can lead to biased or inefficient estimates when missing data or measurement errors are involved. We present an automatic simplification algorithm that seeks to eliminate symbolically unnecessary variables from these expressions by taking advantage of the structure of the underlying graphical model. Our method is applicable to all causal effect formulas and is readily available in the R package causaleffect",
    "volume": "main",
    "checked": true,
    "id": "191d99fb17e4b0e70893fae9f51e1a8d246e4904",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v18/16-217.html": {
    "title": "Nearly optimal classification for semimetrics",
    "abstract": "We initiate the rigorous study of classification in semimetric spaces, which are point sets with a distance function that is non-negative and symmetric, but need not satisfy the triangle inequality. We define the density dimension dens and discover that it plays a central role in the statistical and algorithmic feasibility of learning in semimetric spaces. We compute this quantity for several widely used semimetrics and present nearly optimal sample compression algorithms, which are then used to obtain generalization guarantees, including fast rates. Our claim of near-optimality holds in both computational and statistical senses. When the sample has radius $R$ and margin $\\gamma$, we show that it can be compressed down to roughly $d=(R/\\gamma)^{\\text{dens}}$ points, and further that finding a significantly better compression is algorithmically intractable unless P=NP. This compression implies generalization via standard Occam-type arguments, to which we provide a nearly matching lower bound",
    "volume": "main",
    "checked": true,
    "id": "a96c125f91def81a16ab0c76fd9dceaf7849fa5e",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v18/16-223.html": {
    "title": "Bridging Supervised Learning and Test-Based Co-optimization",
    "abstract": "This paper takes a close look at the important commonalities and subtle differences between the well-established field of supervised learning and the much younger one of co-optimization. It explains the relationships between the problems, algorithms and views on cost and performance of the two fields, all throughout providing a two-way dictionary for the respective terminologies used to describe these concepts. The intent is to facilitate advancement of both fields through transfer and cross-pollination of ideas, techniques and results. As a proof of concept, a theoretical study is presented on the connection between existence / lack of free lunch in the two fields, showcasing a few ideas for improving computational complexity of certain supervised learning approaches",
    "volume": "main",
    "checked": true,
    "id": "cf90f4d719742c9f8974f1603e5329d8b0c65db5",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v18/16-509.html": {
    "title": "GFA: Exploratory Analysis of Multiple Data Sources with Group Factor Analysis",
    "abstract": "The R package GFA provides a full pipeline for factor analysis of multiple data sources that are represented as matrices with co-occurring samples. It allows learning dependencies between subsets of the data sources, decomposed into latent factors. The package also implements sparse priors for the factorization, providing interpretable biclusters of the multi-source data",
    "volume": "main",
    "checked": true,
    "id": "626b1a48e0ee7f5714693c5dea3fe9f5eab07cd7",
    "citation_count": 16
  }
}