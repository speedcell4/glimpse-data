{
  "https://www.isca-speech.org/archive/interspeech_2022/cho22_interspeech.html": {
    "title": "SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech",
    "volume": "main",
    "abstract": "In this paper, we present SANE-TTS, a stable and natural end-to-end multilingual TTS model. By the difficulty of obtaining multilingual corpus for given speaker, training multilingual TTS model with monolingual corpora is unavoidable. We introduce speaker regularization loss that improves speech naturalness during cross-lingual synthesis as well as domain adversarial training, which is applied in other multilingual TTS models. Furthermore, by adding speaker regularization loss, replacing speaker embedding with zero vector in duration predictor stabilizes cross-lingual inference. With this replacement, our model generates speeches with moderate rhythm regardless of source speaker in cross-lingual synthesis. In MOS evaluation, SANE-TTS achieves naturalness score above 3.80 both in cross-lingual and intralingual synthesis, where the ground truth score is 3.99. Also, SANE-TTS maintains speaker similarity close to that of ground truth even in cross-lingual inference. Audio samples are available on our web page",
    "checked": true,
    "id": "3ac674d9ea88fe1fb9dcd4031e4f50c3ecaa3ab9",
    "semantic_title": "sane-tts: stable and natural end-to-end multilingual text-to-speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bae22_interspeech.html": {
    "title": "Enhancement of Pitch Controllability using Timbre-Preserving Pitch Augmentation in FastPitch",
    "volume": "main",
    "abstract": "The recently developed pitch-controllable text-to-speech (TTS) model, i.e. FastPitch, was conditioned for the pitch contours. However, the quality of the synthesized speech degraded considerably for pitch values that deviated significantly from the average pitch; i.e. the ability to control vocal pitch was limited. To address this issue, we propose two algorithms to improve the robustness of FastPitch. First, we propose a novel timbre-preserving pitch-shifting algorithm for natural pitch augmentation. Pitch-shifted speech samples sound more natural when using the proposed algorithm because the speaker's vocal timbre is maintained. Moreover, we propose a training algorithm that defines FastPitch using pitch-augmented speech datasets with different pitch ranges for the same sentence. The experimental results demonstrate that the proposed algorithms improve the pitch controllability of FastPitch",
    "checked": true,
    "id": "c1a6a70774b675f55a830b72137c4cbac9b91f11",
    "semantic_title": "enhancement of pitch controllability using timbre-preserving pitch augmentation in fastpitch",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lenglet22_interspeech.html": {
    "title": "Speaking Rate Control of end-to-end TTS Models by Direct Manipulation of the Encoder's Output Embeddings",
    "volume": "main",
    "abstract": "Since neural Text-To-Speech models have achieved such high standards in terms of naturalness, the main focus of the field has gradually shifted to gaining more control over the expressiveness of the synthetic voices. One of these leverages is the control of the speaking rate that has become harder for a human operator to control since the introduction of neural attention networks to model speech dynamics. While numerous models have reintroduced an explicit duration control (ex: FastSpeech2), these models generally rely on additional tasks to complete during their training. In this paper, we show how an acoustic analysis of the internal embeddings delivered by the encoder of an unsupervised end-to-end TTS Tacotron2 model is enough to identify and control some acoustic parameters of interest. Specifically, we compare this speaking rate control with the duration control offered by a supervised FastSpeech2 model. Experimental results show that the control provided by embeddings reproduces a behaviour closer to natural speech data",
    "checked": true,
    "id": "bfc440698d636f521bb693d69526bcc922375135",
    "semantic_title": "speaking rate control of end-to-end tts models by direct manipulation of the encoder's output embeddings",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ju22_interspeech.html": {
    "title": "TriniTTS: Pitch-controllable End-to-end TTS without External Aligner",
    "volume": "main",
    "abstract": "Three research directions that have recently advanced the text-to-speech (TTS) field are end-to-end architecture, prosody control modeling, and on-the-fly duration alignment of non-auto-regressive models. However, these three agendas have yet to be tackled at once in a single solution. Current studies are limited either by a lack of control over prosody modeling or by the inefficient training inherent in building a two-stage TTS pipeline. We propose TriniTTS, a pitch-controllable end-to-end TTS without an external aligner that generates natural speech by addressing the issues mentioned above at once. It eliminates the training inefficiency in the two-stage TTS pipeline by the end-to-end architecture. Moreover, it manages to learn the latent vector representing the data distribution of the speeches through performing tasks (alignment search, pitch estimation, waveform generation) simultaneously. Experimental results demonstrate that TriniTTS enables prosody modeling with user input parameters to generate deterministic speech, while synthesizing comparable speech to the state-of-the-art VITS. Furthermore, eliminating normalizing flow modules used in VITS increases the inference speed by 28.84% in CPU environment and by 29.16% in GPU environment",
    "checked": true,
    "id": "a501ec0d907a508ef7caf84801f70af287b125a9",
    "semantic_title": "trinitts: pitch-controllable end-to-end tts without external aligner",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lim22_interspeech.html": {
    "title": "JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech",
    "volume": "main",
    "abstract": "In neural text-to-speech (TTS), two-stage system or a cascade of separately learned models have shown synthesis quality close to human speech. For example, FastSpeech2 transforms an input text to a mel-spectrogram and then HiFi-GAN generates a raw waveform from a mel-spectogram where they are called an acoustic feature generator and a neural vocoder respectively. However, their training pipeline is somewhat cumbersome in that it requires a fine-tuning and an accurate speech-text alignment for optimal performance. In this work, we present end-to-end text-to-speech (E2E-TTS) model which has a simplified training pipeline and outperforms a cascade of separately learned models. Specifically, our proposed model is jointly trained FastSpeech2 and HiFi-GAN with an alignment module. Since there is no acoustic feature mismatch between training and inference, it does not requires fine-tuning. Furthermore, we remove dependency on an external speech-text alignment tool by adopting an alignment learning objective in our joint training framework. Experiments on LJSpeech corpus shows that the proposed model outperforms publicly available, state-of-the-art implementations of ESPNet2-TTS on subjective evaluation (MOS) and some objective evaluations",
    "checked": true,
    "id": "3f0e0b556a04660a2333cfc768953e44ad76986e",
    "semantic_title": "jets: jointly training fastspeech2 and hifi-gan for end to end text to speech",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2022/turrisi22_interspeech.html": {
    "title": "Interpretable dysarthric speaker adaptation based on optimal-transport",
    "volume": "main",
    "abstract": "This work addresses the mismatch problem between the distribution of training data (source) and testing data (target), in the challenging context of dysarthric speech recognition. We focus on Speaker Adaptation (SA) in command speech recognition, where data from multiple sources (i.e., multiple speakers) are available. Specifically, we propose an unsupervised Multi-Source Domain Adaptation (MSDA) algorithm based on optimal-transport, called MSDA via Weighted Joint Optimal Transport (MSDA-WJDOT). We achieve a Command Error Rate relative reduction of 16% and 7% over the speaker-independent model and the best competitor method, respectively. The strength of the proposed approach is that, differently from any other existing SA method, it offers an interpretable model that can also be exploited, in this context, to diagnose dysarthria without any specific training. Indeed, it provides a closeness measure between the target and the source speakers, reflecting their similarity in terms of speech characteristics. Based on the similarity between the target speaker and the healthy/dysarthric source speakers, we then define the healthy/dysarthric score of the target speaker that we leverage to perform dysarthria detection. This approach does not require any additional training and achieves a 95% accuracy in the dysarthria diagnosis",
    "checked": true,
    "id": "e4274ce9a61feac852051f7b9dd0caadebf32c6f",
    "semantic_title": "interpretable dysarthric speaker adaptation based on optimal-transport",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yue22_interspeech.html": {
    "title": "Dysarthric Speech Recognition From Raw Waveform with Parametric CNNs",
    "volume": "main",
    "abstract": "Raw waveform acoustic modelling has recently received increasing attention. Compared with the task-blind hand-crafted features which may discard useful information, representations directly learned from the raw waveform are task-specific and potentially include all task-relevant information. In the context of automatic dysarthric speech recognition (ADSR), raw waveform acoustic modelling is under-explored owing to data scarcity. Parametric convolutional neural networks (CNNs) can compensate for this problem due to having notably fewer parameters and requiring less training data in comparison with conventional non-parametric CNNs. In this paper, we explore the usefulness of raw waveform acoustic modelling using various parametric CNNs for ADSR. We investigate the properties of the learned filters and monitor the training dynamics of various models. Furthermore, we study the effectiveness of data augmentation and multi-stream acoustic modelling through combining the non-parametric and parametric CNNs fed by hand-crafted and raw waveform features. Experimental results on the TORGO dysarthric database show that the parametric CNNs significantly outperform the non-parametric CNNs, reaching up to 36.2% and 12.6% WERs (up to 3.4% and 1.1% absolute error reduction) for dysarthric and typical speech, respectively. Multi-stream acoustic modelling further improves the performance resulting in up to 33.2% and 10.3% WERs for dysarthric and typical speech, respectively",
    "checked": true,
    "id": "b1920c26f9af25455711372e61dce209078e8400",
    "semantic_title": "dysarthric speech recognition from raw waveform with parametric cnns",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/prananta22_interspeech.html": {
    "title": "The Effectiveness of Time Stretching for Enhancing Dysarthric Speech for Improved Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we investigate several existing and a new state-of-the-art generative adversarial network-based (GAN) voice conversion method for enhancing dysarthric speech for improved dysarthric speech recognition. We compare key components of existing methods as part of a rigorous ablation study to find the most effective solution to improve dysarthric speech recognition. We find that straightforward signal processing methods such as stationary noise removal and vocoder-based time stretching lead to dysarthric speech recognition results comparable to those obtained when using state-of-the-art GAN-based voice conversion methods as measured using a phoneme recognition task. Additionally, our proposed solution of a combination of MaskCycleGAN-VC and time stretching is able to improve the phoneme recognition results for certain dysarthric speakers compared to our time stretched baseline",
    "checked": true,
    "id": "d56dce24a8e32aaed720dab7a685315db7a472b8",
    "semantic_title": "the effectiveness of time stretching for enhancing dysarthric speech for improved dysarthric speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/violeta22_interspeech.html": {
    "title": "Investigating Self-supervised Pretraining Frameworks for Pathological Speech Recognition",
    "volume": "main",
    "abstract": "We investigate the performance of self-supervised pretraining frameworks on pathological speech datasets used for automatic speech recognition (ASR). Modern end-to-end models require thousands of hours of data to train well, but only a small number of pathological speech datasets are publicly available. A proven solution to this problem is by first pretraining the model on a huge number of healthy speech datasets and then fine-tuning it on the pathological speech datasets. One new pretraining framework called self-supervised learning (SSL) trains a network using only speech data, providing more flexibility in training data requirements and allowing more speech data to be used in pretraining. We investigate SSL frameworks such as the wav2vec 2.0 and WavLM models using different setups and compare their performance with different supervised pretraining setups, using two types of pathological speech, namely, Japanese electrolaryngeal and English dysarthric. Our results show that although SSL has shown success with minimally resourced healthy speech, we do not find this to be the case with pathological speech. The best supervised setup outperforms the best SSL setup by 13.9% character error rate in electrolaryngeal speech and 16.8% word error rate in dysarthric speech",
    "checked": true,
    "id": "519fe656d67f6d3d154a7b5b81d798b03a3ac5a4",
    "semantic_title": "investigating self-supervised pretraining frameworks for pathological speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bhat22_interspeech.html": {
    "title": "Improved ASR Performance for Dysarthric Speech Using Two-stage DataAugmentation",
    "volume": "main",
    "abstract": "Machine learning (ML) and Deep Neural Networks (DNN) have greatly aided the problem of Automatic Speech Recognition (ASR). However, accurate ASR for dysarthric speech remains a serious challenge. Dearth of usable data remains a problem in applying ML and DNN techniques for dysarthric speech recognition. In the current research, we address this challenge using a novel two-stage data augmentation scheme, a combination of static and dynamic data augmentation techniques that are designed by leveraging an understanding of the characteristics of dysarthric speech. Deep Autoencoder (DAE)-based healthy speech modification and various perturbations comprise static augmentations, whereas SpecAugment techniques modified to specifically augment dysarthric speech comprise the dynamic data augmentation. The objective of this work is to improve the ASR performance for dysarthric speech using the two-stage data augmentation scheme. An end-to-end ASR using a Transformer acoustic model is used to evaluate the data augmentation scheme on speech from the UA dysarthric speech corpus. We achieve an absolute improvement of 16% in word error rate (WER) over a baseline with no augmentation, with a final WER of 20.6%",
    "checked": true,
    "id": "4cc550a6ac45b9e2643f70ab3a0c71ed42c621cf",
    "semantic_title": "improved asr performance for dysarthric speech using two-stage dataaugmentation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hernandez22_interspeech.html": {
    "title": "Cross-lingual Self-Supervised Speech Representations for Improved Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "State-of-the-art automatic speech recognition (ASR) systems perform well on healthy speech. However, the performance on impaired speech still remains an issue. The current study explores the usefulness of using Wav2Vec self-supervised speech representations as features for training an ASR system for dysarthric speech. Dysarthric speech recognition is particularly difficult as several aspects of speech such as articulation, prosody and phonation can be impaired. Specifically, we train an acoustic model with features extracted from Wav2Vec, Hubert, and the cross-lingual XLSR model. Results suggest that speech representations pretrained on large unlabelled data can improve word error rate (WER) performance. In particular, features from the multilingual model led to lower WERs than Fbanks or models trained on a single language. Improvements were seen in English speakers with cerebral palsy caused dysarthria (UASpeech corpus), Spanish speakers with Parkinsonian dysarthria (PC-GITA corpus) and Italian speakers with paralysis-based dysarthria (EasyCall corpus). Compared to using Fbank features, XLSR-based features reduced WERs by 6.8%, 22.0%, and 7.0% for the UASpeech, PC-GITA, and EasyCall corpus, respectively",
    "checked": true,
    "id": "e7b7066324c2f24a8a64b0e2783bd7596c568609",
    "semantic_title": "cross-lingual self-supervised speech representations for improved dysarthric speech recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22b_interspeech.html": {
    "title": "Regularizing Transformer-based Acoustic Models by Penalizing Attention Weights",
    "volume": "main",
    "abstract": "The application of deep learning has significantly advanced the performance of automatic speech recognition (ASR) systems. Various components make up an ASR system, such as the acoustic model (AM), language model, and lexicon. Generally, the AM has benefited the most from deep learning. Numerous types of neural network-based AMs have been studied, but the structure that has received the most attention in recent years is the Transformer. In this study, we demonstrate that the Transformer model is more vulnerable to input sparsity compared to the convolutional neural network (CNN) and analyze the cause of performance degradation through structural characteristics of the Transformer. Moreover, we also propose a novel regularization method that makes the transformer model robust against input sparsity. The proposed sparsity regularization method directly regulates attention weights using silence label information in forced-alignment and has the advantage of not requiring additional module training and excessive computation. We tested the proposed method on five benchmarks and observed an average relative error rate reduction (RERR) of 4.7%",
    "checked": true,
    "id": "bf8d0c637369bce9d39f23b41a805b699a6d8f6d",
    "semantic_title": "regularizing transformer-based acoustic models by penalizing attention weights",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chan22_interspeech.html": {
    "title": "Content-Context Factorized Representations for Automated Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural networks have largely demonstrated their ability to perform automated speech recognition (ASR) by extracting meaningful features from input audio frames. Such features, however, may consist not only of information about the spoken language content, but also may contain information about unnecessary contexts such as background noise and sounds or speaker identity, accent, or protected attributes. Such information can directly harm generalization performance, by introducing spurious correlations between the spoken words and the context in which such words were spoken. In this work, we introduce an unsupervised, encoder-agnostic method for factoring speech-encoder representations into explicit content-encoding representations and spurious context-encoding representations. By doing so, we demonstrate improved performance on standard ASR benchmarks, as well as improved performance in both real-world and artificially noisy ASR scenarios",
    "checked": true,
    "id": "8e565cc901b25e3ccafb758178f925b98b849299",
    "semantic_title": "content-context factorized representations for automated speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/karakasidis22_interspeech.html": {
    "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
    "volume": "main",
    "abstract": "It is common knowledge that the quantity and quality of the training data play a significant role in the creation of a good machine learning model. In this paper, we take it one step further and demonstrate that the way the training examples are arranged is also of crucial importance. Curriculum Learning is built on the observation that organized and structured assimilation of knowledge has the ability to enable faster training and better comprehension. When humans learn to speak, they first try to utter basic phones and then gradually move towards more complex structures such as words and sentences. This methodology is known as Curriculum Learning, and we employ it in the context of Automatic Speech Recognition. We hypothesize that end-to-end models can achieve better performance when provided with an organized training set consisting of examples that exhibit an increasing level of difficulty (i.e. a curriculum). To impose structure on the training set and to define the notion of an easy example, we explored multiple scoring functions that either use feedback from an external neural network or incorporate feedback from the model itself. Empirical results show that with different curriculums we can balance the training times and the network's performance",
    "checked": true,
    "id": "be3c2b892608a9e2da36f4d6f02e8ef0306fc461",
    "semantic_title": "comparison and analysis of new curriculum criteria for end-to-end asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baby22_interspeech.html": {
    "title": "Incremental learning for RNN-Transducer based speech recognition models",
    "volume": "main",
    "abstract": "This paper investigates an incremental learning framework for a real-world voice assistant employing RNN-Transducer based automatic speech recognition (ASR) model. Such a model needs to be regularly updated to keep up with changing distribution of customer requests. We demonstrate that a simple fine-tuning approach with a combination of old and new training data can be used to incrementally update the model spending only several hours of training time and without any degradation on old data. This paper explores multiple rounds of incremental updates on the ASR model with monthly training data. Results show that the proposed approach achieves 5-6\\% relative WER improvement over the models trained from scratch on the monthly evaluation datasets. In addition, we explore if it is possible to improve recognition of specific new words. We simulate multiple rounds of incremental updates with handful of training utterances per word (both real and synthetic) and show that the recognition of the new words improves dramatically but with a minor degradation on general data. Finally, we demonstrate that the observed degradation on general data can be mitigated by interleaving monthly updates with updates targeting specific words",
    "checked": true,
    "id": "1333b81fb6fef9b3b1b6a09d35f7eb6fc4910ff9",
    "semantic_title": "incremental learning for rnn-transducer based speech recognition models",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hard22_interspeech.html": {
    "title": "Production federated keyword spotting via distillation, filtering, and joint federated-centralized training",
    "volume": "main",
    "abstract": "We trained a keyword spotting model using federated learning on real user devices and observed significant improvements when the model was deployed for inference on phones. To compensate for data domains that are missing from on-device training caches, we employed joint federated-centralized training. And to learn in the absence of curated labels on-device, we formulated a confidence filtering strategy based on user-feedback signals for federated distillation. These techniques created models that significantly improved quality metrics in offline evaluations and user-experience metrics in live A/B experiments",
    "checked": true,
    "id": "7e4db4b20a9c2299109849c84179f8dfb788ee17",
    "semantic_title": "production federated keyword spotting via distillation, filtering, and joint federated-centralized training",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22b_interspeech.html": {
    "title": "Use of prosodic and lexical cues for disambiguating wh-words in Korean",
    "volume": "main",
    "abstract": "Previous research has shown that the ambiguity of wh-words in Korean can be resolved by prosody. The present study investigated the interplay between prosody and lexical cues in disambiguation. Our written survey results showed that the use of certain adverbs (e.g., a little, once) with a wh-word increases the likelihood of a yes-no question interpretation. The results of our speech production experiment found an interaction of lexical and prosodic cues in the disambiguation. In particular, the presence of a lexical cue affected speakers' phrasing choice, but not the type of Intonational Phrase (IP) boundary tones or acoustic prominence. The finding supports the proposal that speech production is affected by the amount of linguistic information available for speakers. We further suggest how the phrasing structure could affect speakers' choice of the IP boundary tone in Korean",
    "checked": true,
    "id": "7efca97ec18ed3b00a75fc8a2e5279cc2d755c05",
    "semantic_title": "use of prosodic and lexical cues for disambiguating wh-words in korean",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ribeiro22_interspeech.html": {
    "title": "Autoencoder-Based Tongue Shape Estimation During Continuous Speech",
    "volume": "main",
    "abstract": "Vocal tract shape estimation is a necessary step for articulatory speech synthesis. However, the literature on the topic is scarce, and most current methods lack adequacy to many physical constraints related to speech production. This study proposes an alternative approach to the task to solve specific issues faced in the previous work, especially those related to critical articulators. We present an autoencoder-based method for tongue shape estimation during continuous speech. An autoencoder is trained to learn the data's encoding and serves as an auxiliary network for the principal one, which maps phonemes to the shapes. Instead of predicting the exact points in the target curve, the neural network learns how to predict the curve's main components, i.e., the autoencoder's representation. We show how this approach allows imposing critical articulators' constraints, controlling the tongue shape through the latent space, and generating a smooth output without relying on any postprocessing method",
    "checked": true,
    "id": "a1e7cfe8f7f5ec38c9c009ec78e585163550182f",
    "semantic_title": "autoencoder-based tongue shape estimation during continuous speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/magistro22_interspeech.html": {
    "title": "Phonetic erosion and information structure in function words: the case of mia",
    "volume": "main",
    "abstract": "The purpose of this paper is to examine the prosodic correlates of a grammaticalisation process that leads to the formation of a function word. In particular, our case study will tackle the pattern of negation renewal known as Jespersen's Cycle (JC). In JC, a negative reinforcer carrying contrastive meaning grammaticalises to a function word denoting polar negation. We want to show that this change fits in with prosodic change: specifically, the grammaticalised item undergoes prosodic reduction. We test the latter hypothesis on the peculiar Italo-Romance dialect Gazzolese, where mia, the particle undergoing JC, can be used both as the erstwhile contrastive function and as a function word denoting negation (it can appear, for example, in Broad Focus statements). The results confirm that when mia is used as a function word, it displays a shorter duration, a reduced intensity excursion, and does not associate with a pitch accent, in comparison to the original contrastive context. These results show that the change in function word can be appreciated on different phonetic/phonological levels: the metrical one and the intonational one, mediated through the role of the lexical item within information structure",
    "checked": true,
    "id": "c728c743ca132aafc4f66109eb584c13fab33fd9",
    "semantic_title": "phonetic erosion and information structure in function words: the case of mia",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/oh22_interspeech.html": {
    "title": "Dynamic Vertical Larynx Actions Under Prosodic Focus",
    "volume": "main",
    "abstract": "Recently, Lee (2018) observes that one vertical larynx movement (VLM) is associated with an Accentual Phrase (AP) in Seoul Korean. The current study builds on these findings by investigating the effect of prosodic focus on vertical larynx actions. Target sentences were designed to produce four APs (e.g., Joohyun sold six yards of shabby garden field; AP[Joohyun-SUBJ] AP[shabby garden field] AP[six yards-OBJ] AP[sold-DECL], presented in Korean) and were used to elicit focus on the initial word of the object phrase (e.g., six). Articulatory data on VLM is obtained from five Seoul Korean speakers using real-time MRI. Results indicate that quantifiable VLMs observed for each sentence range from 3 to 6 movements, with 4 movements per sentence being the most frequent. Sentences with focus have more instances of VLM per sentence than those without. Focused sentences exhibit significantly greater vertical larynx displacement around the region of focus than the control. Our findings have implications for prosodic planning and pitch resetting, and ongoing analyses examine how VLMs align with Accentual Phrases in Seoul Korean and correlate with fundamental frequency",
    "checked": true,
    "id": "c54e9cabf0d8cd1d88fb234ee5bc1e190554e00d",
    "semantic_title": "dynamic vertical larynx actions under prosodic focus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bradshaw22_interspeech.html": {
    "title": "Fundamental Frequency Variability over Time in Telephone Interactions",
    "volume": "main",
    "abstract": "Speech signals contain substantial fundamental frequency (f0) variability. Even within a single utterance, speakers modify f0 to create different intonational patterns. Previous studies have identified markers of increased f0 variability, such as the introduction of a new topic or greetings, but these are limited in the scope of their analyses. In the present study, we investigate f0 variability over the course of a telephone conversation, with a focus on the initial and medial utterances within the exchange. We examined f0 standard deviation of each utterance in over 2000 telephone conversations from 509 American English speakers from the Switchboard corpus. Findings showed that on average, speakers exhibit more f0 variability in the opening compared to mid-conversation utterances. Further, findings suggest that the inclusion of a greeting word in an initial turn, e.g., \"hello\" or \"hi\", corresponds to an increase in f0 standard deviation. These results suggest that speakers employed more variable f0 in the initial few turns of a telephone conversation. The interpretation of this finding is multifaceted and may be linked to several communicative goals, including the placement of identity markers in conversation or the attraction of attention, or the role of openings as boundary markers",
    "checked": true,
    "id": "ca01ae7023dc37cef393de12603af5e3b22d911b",
    "semantic_title": "fundamental frequency variability over time in telephone interactions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tsiamas22_interspeech.html": {
    "title": "SHAS: Approaching optimal Segmentation for End-to-End Speech Translation",
    "volume": "main",
    "abstract": "Speech translation models are unable to directly process long audios, like TED talks, which have to be split into shorter segments. Speech translation datasets provide manual segmentations of the audios, which are not available in real-world scenarios, and existing segmentation methods usually significantly reduce translation quality at inference time. To bridge the gap between the manual segmentation of training and the automatic one at inference, we propose Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively learn the optimal segmentation from any manually segmented speech corpus. First, we train a classifier to identify the included frames in a segmentation, using speech representations from a pre-trained wav2vec 2.0. The optimal splitting points are then found by a probabilistic Divide-and-Conquer algorithm that progressively splits at the frame of lowest probability until all segments are below a pre-specified length. Experiments on MuST-C and mTEDx show that the translation of the segments produced by our method approaches the quality of the manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of the manual segmentation's BLEU score, compared to the 87-93% of the best existing methods. Our method is additionally generalizable to different domains and achieves high zero-shot performance in unseen languages",
    "checked": true,
    "id": "aba3812ffaf26833f5e92919eaa42e0bcc8d8c7e",
    "semantic_title": "shas: approaching optimal segmentation for end-to-end speech translation",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22g_interspeech.html": {
    "title": "M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation",
    "volume": "main",
    "abstract": "End-to-end speech-to-text translation models are often initialized with pre-trained speech encoder and pre-trained text decoder. This leads to a significant training gap between pre-training and fine-tuning, largely due to the modality differences between speech outputs from the encoder and text inputs to the decoder. In this work, we aim to bridge the modality gap between speech and text to improve translation quality. We propose M-Adapter, a novel Transformer-based module, to adapt speech representations to text. While shrinking the speech sequence, M-Adapter produces features desired for speech-to-text translation via modelling global and local dependencies of a speech sequence. Our experimental results show that our model outperforms a strong baseline by up to 1 BLEU score on the Must-C En$\\rightarrow$DE dataset.\\footnote{Our code is available at https://github.com/mingzi151/w2v2-st.}",
    "checked": true,
    "id": "795e211298e125938f6ee2efec673b3f6025f4ca",
    "semantic_title": "m-adapter: modality adaptation for end-to-end speech-to-text translation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zaidi22_interspeech.html": {
    "title": "Cross-Modal Decision Regularization for Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "Simultaneous translation systems start producing the output while processing the partial source sentence in the incoming input stream. These systems need to decide when to read more input and when to write the output. The decisions taken by the model depend on the structure of source/target language and the information contained in the partial input sequence. Hence, read/write decision policy remains the same across different input modalities, i.e., speech and text. This motivates us to leverage the text transcripts corresponding to the speech input for improving simultaneous speech-to-text translation (SimulST). We propose Cross-Modal Decision Regularization (CMDR) to improve the decision policy of SimulST systems by using the simultaneous text-to-text translation (SimulMT) task. We also extend several techniques from the offline speech translation domain to explore the role of SimulMT task in improving SimulST performance. Overall, we achieve 34.66% / 4.5 BLEU improvement over the baseline model across different latency regimes for the MuST-C English-German (EnDe) SimulST task",
    "checked": true,
    "id": "4412334747a7491315e049de555179bc2f724309",
    "semantic_title": "cross-modal decision regularization for simultaneous speech translation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fukuda22b_interspeech.html": {
    "title": "Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation",
    "volume": "main",
    "abstract": "Speech segmentation, which splits long speech into short segments, is essential for speech translation (ST). Popular VAD tools like WebRTC VAD have generally relied on pause-based segmentation. Unfortunately, pauses in speech do not necessarily match sentence boundaries, and sentences can be connected by a very short pause that is difficult to detect by VAD. In this study, we propose a speech segmentation method using a binary classification model trained using a segmented bilingual speech corpus. We also propose a hybrid method that combines VAD and the above speech segmentation method. Experimental results revealed that the proposed method is more suitable for cascade and end-to-end ST systems than conventional segmentation methods. The hybrid approach further improved the translation performance",
    "checked": true,
    "id": "5b88709ebaf4a3666d73c2f272bcb909e0925ea1",
    "semantic_title": "speech segmentation optimization using segmented bilingual speech corpus for end-to-end speech translation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/r22_interspeech.html": {
    "title": "Generalized Keyword Spotting using ASR embeddings",
    "volume": "main",
    "abstract": "Keyword Spotting (KWS) detects a set of pre-defined spoken keywords. Building a KWS system for an arbitrary set requires massive training datasets. We propose to use the text transcripts from an Automatic Speech Recognition (ASR) system alongside triplets for KWS training. The intermediate representation from the ASR system trained on a speech corpus is used as acoustic word embeddings for keywords. Triplet loss is added to the Connectionist Temporal Classification (CTC) loss in the ASR while training. This method achieves an Average Precision (AP) of 0.843 over 344 words unseen by the model trained on the TIMIT dataset. In contrast, the Multi-View recurrent method that learns jointly on the text and acoustic embeddings achieves only 0.218 for out-of-vocabulary words. This method is also applied to low-resource languages such as Tamil by converting Tamil characters to English using transliteration. This is a very challenging novel task for which we provide a dataset of transcripts for the keywords. Despite our model not generalizing well, we achieve a benchmark AP of 0.321 on over 38 words unseen by the model on the MSWC Tamil keyword set. The model also produces an accuracy of 96.2% for classification tasks on the Google Speech Commands dataset",
    "checked": true,
    "id": "4e59b71da152b76049164826bcdce3038a251e90",
    "semantic_title": "generalized keyword spotting using asr embeddings",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ahn22_interspeech.html": {
    "title": "Multi-Corpus Speech Emotion Recognition for Unseen Corpus Using Corpus-Wise Weights in Classification Loss",
    "volume": "main",
    "abstract": "Since each of the currently available emotional speech corpora is rather small to deal with personal or cultural diversity, multiple emotional speech corpora can be jointly used to train a speech emotion recognition (SER) model robust to unseen corpora. Each corpus has different characteristics, including whether acted or spontaneous, in which environment it was recorded, and what lexical contents it contains. Depending on the characteristics, the emotion recognition accuracy and time required to train a model for it are different. If we train the SER model utilizing multiple corpora equally, the classification performance for each training corpus would be different. The performance for unseen corpora may be enhanced if the model is trained to show similar recognition accuracy for each training corpus that covers different characteristics. In this study, we propose to adopt corpus-wise weights in the classification loss, which are functions of the recognition accuracy for each of the training corpus. We also adopt pseudo-emotion labels for the unlabeled speech corpus to further enhance the performance. Experimental results showed that the proposed method outperformed previously proposed approaches in the out-of-corpus SER using three emotional corpora for training and one corpus for evaluation",
    "checked": true,
    "id": "ee43187689de86a55faec99d4ed4996c6a63c6fa",
    "semantic_title": "multi-corpus speech emotion recognition for unseen corpus using corpus-wise weights in classification loss",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22d_interspeech.html": {
    "title": "Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms",
    "volume": "main",
    "abstract": "Attention has become one of the most commonly used mechanisms in deep learning approaches. The attention mechanism can help the system focus more on the feature space's critical regions. For example, high amplitude regions can play an important role for Speech Emotion Recognition (SER). In this paper, we identify misalignments between the attention and the signal amplitude in the existing multi-head self-attention. To improve the attention area, we propose to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA) mechanism in combination with the multi-head self-attention. Through the FA mechanism, the network can detect the largest amplitude part in the segment. By employing the CA mechanism, the network can modulate the information flow by assigning different weights to each attention head and improve the utilization of surrounding contexts. To evaluate the proposed method, experiments are performed with the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed framework significantly outperforms the state-of-the-art approaches on both datasets",
    "checked": true,
    "id": "c6da42178e907c18c3af763f274ffda285e86042",
    "semantic_title": "improving speech emotion recognition through focus and calibration attention mechanisms",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22e_interspeech.html": {
    "title": "The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation",
    "volume": "main",
    "abstract": "In emotion recognition in conversation (ERC), the emotion of the current utterance is predicted by considering the previous context, which can be utilized in many natural language processing tasks. Although multiple emotions can coexist in a given sentence, most previous approaches take the perspective of a classification task to predict only a given label. However, it is expensive and difficult to label the emotion of a sentence with confidence or multi-label. In this paper, we automatically construct a grayscale label considering the correlation between emotions and use it for learning. That is, instead of using a given label as a one-hot encoding, we construct a grayscale label by measuring scores for different emotions. We introduce several methods for constructing grayscale labels and confirm that each method improves the emotion recognition performance. Our method is simple, effective, and universally applicable to previous systems. The experiments show a significant improvement in the performance of baselines",
    "checked": true,
    "id": "b30cde7df8d004fb3dd029b07619a739bc3e2c59",
    "semantic_title": "the emotion is not one-hot encoding: learning with grayscale label for emotion recognition in conversation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/triantafyllopoulos22b_interspeech.html": {
    "title": "Probing speech emotion recognition transformers for linguistic knowledge",
    "volume": "main",
    "abstract": "Large, pre-trained neural networks consisting of self-attention layers (transformers) have recently achieved state-of-the-art results on several speech emotion recognition (SER) datasets. These models are typically pre-trained in self-supervised manner with the goal to improve automatic speech recognition performance -- and thus, to understand linguistic information. In this work, we investigate the extent in which this information is exploited during SER fine-tuning. Using a reproducible methodology based on open-source tools, we synthesise prosodically neutral speech utterances while varying the sentiment of the text. Valence predictions of the transformer model are very reactive to positive and negative sentiment content, as well as negations, but not to intensifiers or reducers, while none of those linguistic features impact arousal or dominance. These findings show that transformers can successfully leverage linguistic information to improve their valence predictions, and that linguistic analysis should be included in their testing",
    "checked": true,
    "id": "6d9395e99fddcbd1bc83328b3898e67e76bd657c",
    "semantic_title": "probing speech emotion recognition transformers for linguistic knowledge",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2022/prabhu22_interspeech.html": {
    "title": "End-To-End Label Uncertainty Modeling for Speech-based Arousal Recognition Using Bayesian Neural Networks",
    "volume": "main",
    "abstract": "Emotions are subjective constructs. Recent end-to-end speech emotion recognition systems are typically agnostic to the subjective nature of emotions, despite their state-of-the-art performance. In this work, we introduce an end-to-end Bayesian neural network architecture to capture the inherent subjectivity in the arousal dimension of emotional expressions. To the best of our knowledge, this work is the first to use Bayesian neural networks for speech emotion recognition. At training, the network learns a distribution of weights to capture the inherent uncertainty related to subjective arousal annotations. To this end, we introduce a loss term that enables the model to be explicitly trained on a distribution of annotations, rather than training them exclusively on mean or gold-standard labels. We evaluate the proposed approach on the AVEC'16 dataset. Qualitative and quantitative analysis of the results reveals that the proposed model can aptly capture the distribution of subjective arousal annotations, with state-of-the-art results in mean and standard deviation estimations for uncertainty modeling",
    "checked": true,
    "id": "90fc3e08850601fb43749b3d3bc77012f33cc9b7",
    "semantic_title": "end-to-end label uncertainty modeling for speech-based arousal recognition using bayesian neural networks",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/perez22_interspeech.html": {
    "title": "Mind the gap: On the value of silence representations to lexical-based speech emotion recognition",
    "volume": "main",
    "abstract": "Speech timing and non-speech regions (here referred to as ``silence\"), often play a critical role in the perception of spoken language. Silence represents an important paralinguistic component in communication. For example, some of its functions include conveying emphasis, dramatization, or even sarcasm. In speech emotion recognition (SER), there has been relatively little work on investigating the utility of silence and no work regarding the effect of silence on linguistics. In this work, we present a novel framework which investigates fusing linguistic and silence representations for emotion recognition in naturalistic speech using the MSP-Podcast dataset. We investigate two methods to represent silence in SER models; the first approach uses utterance-level statistics, while the second learns a silence token embedding within a transformer language model. Our results show that modeling silence does improve SER performance and that modeling silence as a token in a transformer language model significantly improves performance on MSP-Podcast achieving a concordance correlation coefficient of .191 and .453 for activation and valence respectively. In addition, we perform analyses on the attention of silence and find that silence emphasizes the attention of its surrounding words",
    "checked": true,
    "id": "8049c5bb4e95af939b7ab650bc0017bf588f0f3f",
    "semantic_title": "mind the gap: on the value of silence representations to lexical-based speech emotion recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chou22_interspeech.html": {
    "title": "Exploiting Co-occurrence Frequency of Emotions in Perceptual Evaluations To Train A Speech Emotion Classifier",
    "volume": "main",
    "abstract": "Previous studies on speech emotion recognition (SER) with categorical emotions have often formulated the task as a single-label classification problem, where the emotions are considered orthogonal to each other. However, previous studies have indicated that emotions can co-occur, especially for more ambiguous emotional sentences (e.g., a mixture of happiness and surprise). Some studies have regarded SER problems as a multi-label task, predicting multiple emotional classes. However, this formulation does not leverage the relation between emotions during training, since emotions are assumed to be independent. This study explores the idea that emotional classes are not necessarily independent and its implications on training SER models. In particular, we calculate the frequency of co-occurring emotions from perceptual evaluations in the train set to generate a matrix with class-dependent penalties, punishing more mistakes between distant emotional classes. We integrate the penalization matrix into three existing label-learning approaches (hard-label, multi-label, and distribution-label learning) using the proposed modified loss. We train SER models using the penalty loss and commonly used cost functions for SER tasks. The evaluation of our proposed penalization matrix on the MSP-Podcast corpus shows important relative improvements in macro F1-score for hard-label learning (17.12%), multi-label learning (12.79%), and distribution-label learning (25.8%)",
    "checked": true,
    "id": "da3c26775aeafaae8e82e71012bc9b731b7c2766",
    "semantic_title": "exploiting co-occurrence frequency of emotions in perceptual evaluations to train a speech emotion classifier",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dhamyal22_interspeech.html": {
    "title": "Positional Encoding for Capturing Modality Specific Cadence for Emotion Detection",
    "volume": "main",
    "abstract": "Emotion detection from a single modality, such as an audio or text stream, has been known to be a challenging task. While encouraging results have been obtained by using joint evidence from multiple streams, combining such evidence in optimal ways is an open challenge. In this paper, we claim that although the multi-modalities like audio, phoneme sequence ids and word sequence ids are related to each other, they also have their individual local 'cadence', which is important to be modelled for the task of emotion recognition. We model the local cadence by using separate `positional encodings' for each modality in a transformer architecture. Our results show that emotion detection based on this strategy is better than when the modality specific cadence is ignored or normalized out by using a shared positional encoding. We also find that capturing the modality interdependence is not as important as is capturing of the local cadence of individual modalities. We conduct our experiments on the IEMOCAP and CMU-MOSI datasets to demonstrate the effectiveness of the proposed methodology for combining multi-modal evidence",
    "checked": true,
    "id": "d0bac683b9ee598d7deeb5d21c26cdc471002631",
    "semantic_title": "positional encoding for capturing modality specific cadence for emotion detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vuho22_interspeech.html": {
    "title": "Speak Like a Professional: Increasing Speech Intelligibility by Mimicking Professional Announcer Voice with Voice Conversion",
    "volume": "main",
    "abstract": "In most of practical scenarios, the announcement system must deliver speech messages in a noisy environment, in which the background noise cannot be cancelled out. The local noise reduces speech intelligibility and increases listening effort of the listener, hence hamper the effectiveness of announcement system. There has been reported that voices of professional announcers are clearer and more comprehensive than that of non-expert speakers in noisy environment. This finding suggests that the speech intelligibility might be related to the speaking style of professional announcer, which can be adapted using voice conversion method. Motivated by this idea, this paper proposes a speech intelligibility enhancement in noisy environment by applying voice conversion method on non-professional voice. We discovered that the professional announcers and non-professional speakers are clusterized into different clusters on the speaker embedding plane. This implies that the speech intelligibility can be controlled as an independent feature of speaker individuality. To examine the advantage of converted voice in noisy environment, we experimented using test words masked in pink noise at different SNR levels. The results of objective and subjective evaluations confirm that the speech intelligibility of converted voice is higher than that of original voice in low SNR conditions",
    "checked": true,
    "id": "d472dc50e2310e4a4a07c0c727bd88ec1a03a2f7",
    "semantic_title": "speak like a professional: increasing speech intelligibility by mimicking professional announcer voice with voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ho22_interspeech.html": {
    "title": "Vector-quantized Variational Autoencoder for Phase-aware Speech Enhancement",
    "volume": "main",
    "abstract": "Recent speech enhancement methods based on the complex ideal ratio mask (cIRM) have achieved promising results. These methods often deploy a deep neural network to jointly estimate the real and imaginary components of the cIRM defined in the complex domain. However, the unbounded property of cIRM poses difficulties when it comes to effectively training a neural network. To alleviate this problem, this paper proposes a phase-aware speech enhancement method by estimating the magnitude and phase of a complex adaptive Wiener filter. In this method, a noise-robust vector-quantized variational autoencoder is utilized for estimating the magnitude Wiener filter by using the Itakura-Saito divergence on time-frequency domain, while the phase of the Wiener filter is estimated by a convolutional recurrent network using the scale-invariant signal-to-noise ratio constraint in the time domain. The proposed method was evaluated on the open Voice Bank+DEMAND dataset to provide a direct comparison with other speech enhancement studies and achieved the PESQ score of 2.85 and STOI score of 0.94, which is better than the state-of-art method based on cIRM estimation in the 2020 Deep Noise Challenge",
    "checked": true,
    "id": "53778ad96a6fb92779ebcf7ae1a197c7c47ca5de",
    "semantic_title": "vector-quantized variational autoencoder for phase-aware speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22i_interspeech.html": {
    "title": "iDeepMMSE: An improved deep learning approach to MMSE speech and noise power spectrum estimation for speech enhancement",
    "volume": "main",
    "abstract": "Deep learning approaches have been successfully applied to single channel speech enhancement exhibiting significant performance improvement. Recently, approaches unifying deep learning techniques into a statistical speech enhancement framework were proposed, including Deep Xi and DeepMMSE in which a priori signal-to-noise ratios (SNRs) were estimated by deep neural networks (DNNs) and noise power spectral density (PSD) and spectral gain functions were computed with estimated parameters. In this paper, we propose an improved DeepMMSE (iDeepMMSE) which estimates the speech PSD and speech presence probability as well as the a priori SNR using a DNN for MMSE estimation of the speech and noise PSDs. The a priori and a posteriori SNRs are refined with the estimated PSDs, which in turn are used to compute spectral gain function. We also replaced the DNN architecture with the Conformer which efficiently captures the local and global sequential information. Experimental results on the Voice Bank-DEMAND dataset and Deep Xi dataset showed the proposed iDeepMMSE outperformed the DeepMMSE in terms of the perceptual evaluation of speech quality (PESQ) scores and composite objective measures",
    "checked": true,
    "id": "a2139b2f32b00fe4ef4c64571f5583496d8c019b",
    "semantic_title": "ideepmmse: an improved deep learning approach to mmse speech and noise power spectrum estimation for speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hung22_interspeech.html": {
    "title": "Boosting Self-Supervised Embeddings for Speech Enhancement",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) representation for speech has achieved state-of-the-art (SOTA) performance on several downstream tasks. However, there remains room for improvement in speech enhancement (SE) tasks. In this study, we used a cross-domain feature to solve the problem that SSL embeddings may lack fine-grained information to regenerate speech signals. By integrating the SSL representation and spectrogram, the result can be significantly boosted. We further study the relationship between the noise robustness of SSL representation via clean-noisy distance (CN distance) and the layer importance for SE. Consequently, we found that SSL representations with lower noise robustness are more important. Furthermore, our experiments on the VCTK-DEMAND dataset demonstrated that fine-tuning an SSL representation with an SE model can outperform the SOTA SSL-based SE methods in PESQ, CSIG and COVL without invoking complicated network architectures. In later experiments, the CN distance in SSL embeddings was observed to increase after fine-tuning. These results verify our expectations and may help design SE-related SSL training in the future",
    "checked": true,
    "id": "bc89597321a15c8568658809e77fbf21ef663c63",
    "semantic_title": "boosting self-supervised embeddings for speech enhancement",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hwang22b_interspeech.html": {
    "title": "Monoaural Speech Enhancement Using a Nested U-Net with Two-Level Skip Connections",
    "volume": "main",
    "abstract": "Capturing the contextual information in multi-scale is known to be beneficial for improving the performance of DNN-based speech enhancement (SE) models. This paper proposes a new SE model, called NUNet-TLS, having two-level skip connections between the residual U-Blocks nested in each layer of a large U-Net structure. The proposed model also has a causal time-frequency attention (CFTA) at the output of the residual U-Block to boost dynamic representation of the speech context in multi-scale. Even having the two-level skip connections, the proposed model slightly increases the network parameters, but the performance improvement is significant. Experimental results show that the proposed NUNet-TLS has superior performance in various objective evaluation metrics to other state-of-the-art models. The code of our model is available at https://github.com/seorim0/NUNet-TLS",
    "checked": true,
    "id": "e0a8a466098f902a1b53cf9200808e401bf9d3bc",
    "semantic_title": "monoaural speech enhancement using a nested u-net with two-level skip connections",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/muckenhirn22_interspeech.html": {
    "title": "CycleGAN-based Unpaired Speech Dereverberation",
    "volume": "main",
    "abstract": "Typically, neural network-based speech dereverberation models are trained on paired data, composed of a dry utterance and its corresponding reverberant utterance. The main limitation of this approach is that such models can only be trained on large amounts of data and a variety of room impulse responses when the data is synthetically reverberated, since acquiring real paired data is costly. In this paper we propose a CycleGAN-based approach that enables dereverberation models to be trained on unpaired data. We quantify the impact of using unpaired data by comparing the proposed unpaired model to a paired model with the same architecture and trained on the paired version of the same dataset. We show that the performance of the unpaired model is comparable to the performance of the paired model on two different datasets, according to objective evaluation metrics. Furthermore, we run two subjective evaluations and show that both models achieve comparable subjective quality on the AMI dataset, which was not seen during training",
    "checked": true,
    "id": "013406ed228229b710a2e7c2a043b531ebf20489",
    "semantic_title": "cyclegan-based unpaired speech dereverberation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pandey22_interspeech.html": {
    "title": "Attentive Training: A New Training Framework for Talker-independent Speaker Extraction",
    "volume": "main",
    "abstract": "Listening in a multitalker scenario, we typically attend to a single talker through auditory selective attention. Inspired by human selective attention, we propose attentive training: a new training framework for talker-independent speaker extraction with an intrinsic selection mechanism. In the real world, multiple talkers very unlikely start speaking at the same time. Based on this observation, we train a deep neural network to create a representation for the first speaker and utilize it to extract or track that speaker from a multitalker noisy mixture. Experimental results demonstrate the superiority of attentive training over widely used permutation invariant training for talker-independent speaker extraction, especially in mismatched conditions in terms of the number of speakers, speaker interaction patterns, and the amount of speaker overlaps",
    "checked": true,
    "id": "e394c3de6dbfaa24c12ab168bc6ad439fee0fffa",
    "semantic_title": "attentive training: a new training framework for talker-independent speaker extraction",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vuong22_interspeech.html": {
    "title": "Improved Modulation-Domain Loss for Neural-Network-based Speech Enhancement",
    "volume": "main",
    "abstract": "We describe an improved modulation-domain loss for deeplearning- based speech enhancement systems (SE). We utilized a simple self-supervised speech reconstruction task to learn a set of spectro-temporal receptive fields (STRFs). Similar to the recently developed spectro-temporal modulation error, the learned STRFs are used to calculate a weighted mean-squared error in the modulation domain for training a speech enhancement system. Experiments show that training the SE systems using the improved modulation-domain loss consistently improves the objective prediction of speech quality and intelligibility. Additionally, we show that the SE systems improve the word error rate of a state-of-the-art automatic speech recognition system at low SNRs",
    "checked": true,
    "id": "21300ed24cf2c01ce06c46361f216713299e8104",
    "semantic_title": "improved modulation-domain loss for neural-network-based speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22d_interspeech.html": {
    "title": "Perceptual Characteristics Based Multi-objective Model for Speech Enhancement",
    "volume": "main",
    "abstract": "Deep learning has been widely adopted for speech applications. Many studies have shown that using the multiple objective framework and learned deep features is effective for improving system performance. In this paper, we propose a perceptual characteristics based multi-objective speech enhancement (SE) algorithm that combines the conventional loss and objective losses of pitch and timbre related features. Timbre related features include frequency modulation (encoded by the pitch contour), amplitude modulation (encoded by the energy contour), and speaker identity. For the speaker identity loss, we consider the deep features derived in a speaker identification system. The proposed algorithm consists of two parts, a LSTM based SE model and CNN based multi-objective models. The objective losses are derived between speech enhanced by the SE model and clean speech and combined with the SE loss for updating the SE model. The proposed algorithm is evaluated using the corpus of Taiwan Mandarin hearing in noise test (TMHINT). Experimental results show the proposed algorithm evidently outperforms the original SE model in all objective scores, including speech quality, speech intelligibility and signal distortion",
    "checked": true,
    "id": "a43ef077e3f16ea0d38ba02b05039075609e0d03",
    "semantic_title": "perceptual characteristics based multi-objective model for speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/delcroix22_interspeech.html": {
    "title": "Listen only to me! How well can target speech extraction handle false alarms?",
    "volume": "main",
    "abstract": "Target speech extraction (TSE) extracts the speech of a target speaker in a mixture given auxiliary clues characterizing the speaker, such as an enrollment utterance. TSE addresses thus the challenging problem of simultaneously performing separation and speaker identification. There has been much progress in extraction performance following the recent development of neural networks for speech enhancement and separation. Most studies have focused on processing mixtures where the target speaker is actively speaking. However, the target speaker is sometimes silent in practice, i.e., inactive speaker (IS). A typical TSE system will tend to output a signal in IS cases, causing false alarms. This is a severe problem for the practical deployment of TSE systems. This paper aims at understanding better how well TSE systems can handle IS cases. We consider two approaches to deal with IS, (1) training a system to directly output zero signals or (2) detecting IS with an extra speaker verification module. We perform an extensive experimental comparison of these schemes in terms of extraction performance and IS detection using the LibriMix dataset and reveal their pros and cons",
    "checked": true,
    "id": "19e4a756cd760323969721131cb2cfd61fb05b02",
    "semantic_title": "listen only to me! how well can target speech extraction handle false alarms?",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22e_interspeech.html": {
    "title": "Monaural Speech Enhancement Based on Spectrogram Decomposition for Convolutional Neural Network-sensitive Feature Extraction",
    "volume": "main",
    "abstract": "Many state-of-the-art speech enhancement (SE) systems have recently used convolutional neural networks (CNNs) to extract multi-scale feature maps. However, CNN relies more on local texture than global shape, which is more susceptible to degraded spectrogram and may fail to capture the detailed structure of speech. Although some two-stage systems feed the first-stage enhanced and original noisy spectrograms to the second stage simultaneously, this does not guarantee sufficient guidance for the second stage since the first-stage spectrogram can not provide precise spectral details. In order to allow CNNs to perceive clear speech component boundary information, we compose feature maps with spectrograms containing evident speech components according to the mask value from the first stage. The positions corresponding to the mask greater than certain thresholds are extracted as feature maps. These feature maps make the boundary information of speech components obvious by ignoring others, thus making CNNs sensitive to input features. Experiments on the VB dataset show that with a proper decomposition numbers, the proposed method can enhance SE performance, which can provide 0.15 PESQ improvement. Besides, the proposed method is more effective for spectral detail recovery",
    "checked": true,
    "id": "a344e3c08264f177485a0deb7239bc2befdcf768",
    "semantic_title": "monaural speech enhancement based on spectrogram decomposition for convolutional neural network-sensitive feature extraction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lemercier22_interspeech.html": {
    "title": "Neural Network-augmented Kalman Filtering for Robust Online Speech Dereverberation in Noisy Reverberant Environments",
    "volume": "main",
    "abstract": "In this paper, a neural network-augmented algorithm for noise-robust online dereverberation with a Kalman filtering variant of the weighted prediction error (WPE) method is proposed. The filter stochastic variations are predicted by a deep neural network (DNN) trained end-to-end using the filter residual error and signal characteristics. The presented framework allows for robust dereverberation on a single-channel noisy reverberant dataset similar to WHAMR!. The Kalman filtering WPE introduces distortions in the enhanced signal when predicting the filter variations from the residual error only, if the target speech power spectral density is not perfectly known and the observation is noisy. The proposed approach avoids these distortions by correcting the filter variations estimation in a data-driven way, increasing the robustness of the method to noisy scenarios. Furthermore, it yields a strong dereverberation and denoising performance compared to a DNN-supported recursive least squares variant of WPE, especially for highly noisy inputs",
    "checked": true,
    "id": "45ffaa49410a2855d1150e2c6a8568a6a07ef2c7",
    "semantic_title": "neural network-augmented kalman filtering for robust online speech dereverberation in noisy reverberant environments",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schmidt22_interspeech.html": {
    "title": "PodcastMix: A dataset for separating music and speech in podcasts",
    "volume": "main",
    "abstract": "We introduce PodcastMix, a dataset formalizing the task of separating background music and foreground speech in podcasts. We aim at defining a benchmark suitable for training and evaluating (deep learning) source separation models. To that end, we release a large and diverse training dataset based on programatically generated podcasts. However, current (deep learning) models can incur into generalization issues, specially when trained on synthetic data. To target potential generalization issues, we release an evaluation set based on real podcasts for which we design objective and subjective tests. Out of our experiments with real podcasts, we find that current (deep learning) models may have generalization issues. Yet, these can perform competently, e.g., our best baseline separates speech with a mean opinion score of 3.84 (rating ``overall separation quality\" from 1 to 5). The dataset and baselines are accessible online",
    "checked": true,
    "id": "714a7e6548f08bd9050e830f6c8faf840406b40d",
    "semantic_title": "podcastmix: a dataset for separating music and speech in podcasts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saijo22_interspeech.html": {
    "title": "Independence-based Joint Dereverberation and Separation with Neural Source Model",
    "volume": "main",
    "abstract": "We propose an independence-based joint dereverberation and separation method with a neural source model. We introduce a neural network in the framework of time-decorrelation iterative source steering, which is an extension of independent vector analysis to joint dereverberation and separation. The network is trained in an end-to-end manner with a permutation invariant loss on the time-domain separation output signals. Our proposed method can be applied in any situation with at least as many microphones as sources, regardless of their number. In experiments, we demonstrate that our method results in high performance in terms of both speech quality metrics and word error rate (WER), even for mixtures with a different number of speakers than training. Furthermore, the model, trained on synthetic mixtures, without any modifications, greatly reduces the WER on the recorded dataset LibriCSS",
    "checked": true,
    "id": "f51c2a32e9540e7e508fa36ecbc17cc4701985d4",
    "semantic_title": "independence-based joint dereverberation and separation with neural source model",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saijo22b_interspeech.html": {
    "title": "Spatial Loss for Unsupervised Multi-channel Source Separation",
    "volume": "main",
    "abstract": "We propose a spatial loss for unsupervised multi-channel source separation. The proposed loss exploits the duality of direction of arrival (DOA) and beamforming: the steering and beamforming vectors should be aligned for the target source, but orthogonal for interfering ones. The spatial loss encourages consistency between the mixing and demixing systems from a classic DOA estimator and a neural separator, respectively. With the proposed loss, we train the neural separators based on minimum variance distortionless response (MVDR) beamforming and independent vector analysis (IVA). We also investigate the effectiveness of combining our spatial loss and a signal loss, which uses the outputs of blind source separation as the references. We evaluate our proposed method on synthetic and recorded (LibriCSS) mixtures. We find that the spatial loss is most effective to train IVA-based separators. For the neural MVDR beamformer, it performs best when combined with a signal loss. On synthetic mixtures, the proposed unsupervised loss leads to the same performance as a supervised loss in terms of word error rate. On LibriCSS, we obtain close to state-of-the-art performance without any labeled training data",
    "checked": true,
    "id": "2c372ad41c123e1fc83d27672952849f54ea8449",
    "semantic_title": "spatial loss for unsupervised multi-channel source separation",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bellows22_interspeech.html": {
    "title": "Effect of Head Orientation on Speech Directivity",
    "volume": "main",
    "abstract": "The directional characteristics of human speech have many applications in speech acoustics, audio, telecommunications, room acoustical design, and other areas. However, professionals in these fields require carefully conducted, high-resolution, spherical speech directivity measurements taken under distinct circumstances to gain additional insights for their work. Because head orientation and human-body diffraction influence speech radiation, this work explores such effects under various controlled conditions through the changing directivity patterns of a head and torso simulator. The results show that head orientation and body diffraction at low frequencies impact directivities only slightly. However, the effects are more substantial at higher frequencies, particularly above 1 kHz",
    "checked": true,
    "id": "bb329e0996657c7e645cf98f942719923fc3fe8e",
    "semantic_title": "effect of head orientation on speech directivity",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saijo22c_interspeech.html": {
    "title": "Unsupervised Training of Sequential Neural Beamformer Using Coarsely-separated and Non-separated Signals",
    "volume": "main",
    "abstract": "We present an unsupervised training method of the sequential neural beamformer (Seq-BF) using coarsely-separated and non-separated supervisory signals. The signal coarsely separated by blind source separation (BSS) has been used for training neural separators in an unsupervised manner. However, the performance is limited due to distortions in the supervision. In contrast, remix-cycle-consistent learning (RCCL) enables a separator to be trained on distortion-free observed mixtures by making the remixed mixtures obtained by repeatedly separating and remixing the two different mixtures closer to the original mixtures. Still, training with RCCL from scratch often falls into a trivial solution, i.e., not separating signals. The present study provides a novel unsupervised learning algorithm for the Seq-BF with two stacked neural separators, in which the separators are pre-trained using the BSS outputs and then fine-tuned with RCCL. Such configuration compensates for the shortcomings of both approaches: the guiding mechanism in Seq-BF accelerates separation to exceed BSS performance, thereby stabilizing RCCL. Experimental comparisons demonstrated that the proposed unsupervised learning achieved performance comparable to supervised learning (0.4 point difference in word error rate)",
    "checked": true,
    "id": "9b2d664a6bfb8ac82111bd27e38bc11cebc1612a",
    "semantic_title": "unsupervised training of sequential neural beamformer using coarsely-separated and non-separated signals",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/borsdorf22_interspeech.html": {
    "title": "Blind Language Separation: Disentangling Multilingual Cocktail Party Voices by Language",
    "volume": "main",
    "abstract": "We introduce blind language separation (BLS) as novel research task, in which we seek to disentangle overlapping voices of multiple languages by language. BLS is expected to separate seen as well as unseen languages, which is different from the target language extraction task that works for one seen target language at a time. To develop a BLS model, we simulate a multilingual cocktail party database, of which each scene consists of two randomly selected languages, each represented by two randomly selected speakers. The database follows the recently proposed GlobalPhoneMCP database design concept that uses the audio data of the GlobalPhone 2000 Speaker Package. We show that a BLS model is able to learn the language characteristics so as to disentangle overlapping voices by language. We achieve a mean SI-SDR improvement of 12.63 dB over 231 test sets. The performance on the individual test sets varies depending on the language combination. Finally, we show that BLS can generalize well to unseen speakers and languages in the mixture",
    "checked": true,
    "id": "60842dfaa6f3f88d1d05b1c0e1f52453eb85f5b4",
    "semantic_title": "blind language separation: disentangling multilingual cocktail party voices by language",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guzik22_interspeech.html": {
    "title": "NTF of Spectral and Spatial Features for Tracking and Separation of Moving Sound Sources in Spherical Harmonic Domain",
    "volume": "main",
    "abstract": "This paper presents a novel Non-negative Tensor Factorization (NTF) based approach to tracking and separation of moving sound sources, formulated in the Spherical Harmonic Domain (SHD). In particular, at first, we redefine an already existing Ambisonic NTF by introducing time-dependence into the Spatial Covariance Matrix (SCM) model. Next, we further extend the time-dependent SCM by incorporating a newly proposed NTF model of the spatial features, thereby introducing spatial components. To exploit the relationship between the positions of sound sources in adjacent time frames, resulting from the naturally occurring continuity of the movement itself, we impose local smoothness on time-dependent components of the spatial features. To this end, we propose a suitable posterior probability with Gibbs prior, and finally we derive the corresponding update rules. The experimental evaluation is based on first-order Ambisonic recordings of speech utterances and musical instruments in several scenarios with moving sources",
    "checked": true,
    "id": "782c2d176f265f016cc4bdd6ae5c101d897d54a4",
    "semantic_title": "ntf of spectral and spatial features for tracking and separation of moving sound sources in spherical harmonic domain",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deadman22_interspeech.html": {
    "title": "Modelling Turn-taking in Multispeaker Parties for Realistic Data Simulation",
    "volume": "main",
    "abstract": "Simulation plays a crucial role in developing components of automatic speech recognition systems such as enhancement and diarization. In source separation and target-speaker extraction, datasets with high degrees of temporal overlap are used both in training and evaluation. However, this contrasts with the fact that people tend to avoid such overlap in real conversations. It is well known that artifacts introduced from pre-processing with no overlapping speech can be detrimental to recognition performance. This work proposes a finite-state based generative method trained on timing information in speech corpora, which leads to two main contributions. First, a method for generating arbitrary large datasets which follow desired statistics of real parties. Second, features extracted from the models are shown to have a correlation with speaker extraction performance. This leads to the contribution of quantifying how much difficulty in a mixture is due to turn-taking, factoring out other complexities in the signal. Models which treat speakers as independent produce poor generation and representation results. We improve upon this by proposing models which have states conditioned on whether another person is speaking",
    "checked": true,
    "id": "5b70fa432f4dcdbdd25432edff67204db2aaa897",
    "semantic_title": "modelling turn-taking in multispeaker parties for realistic data simulation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/boeddeker22_interspeech.html": {
    "title": "An Initialization Scheme for Meeting Separation with Spatial Mixture Models",
    "volume": "main",
    "abstract": "Spatial mixture model (SMM) supported acoustic beamforming has been extensively used for the separation of simultaneously active speakers. However, it has hardly been considered for the separation of meeting data, that are characterized by long recordings and only partially overlapping speech. In this contribution, we show that the fact that often only a single speaker is active can be utilized for a clever initialization of an SMM that employs time-varying class priors. In experiments on LibriCSS we show that the proposed initialization scheme achieves a significantly lower Word Error Rate (WER) on a downstream speech recognition task than a random initialization of the class probabilities by drawing from a Dirichlet distribution. With the only requirement that the number of speakers has to be known, we obtain a WER of 5.9 %, which is comparable to the best reported WER on this data set. Furthermore, the estimated speaker activity from the mixture model serves as a diarization based on spatial information",
    "checked": true,
    "id": "69c3b02a5a2883ad38cca70dcceb1a04bfcd63a7",
    "semantic_title": "an initialization scheme for meeting separation with spatial mixture models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mun22_interspeech.html": {
    "title": "Prototypical speaker-interference loss for target voice separation using non-parallel audio samples",
    "volume": "main",
    "abstract": "In this paper, we propose a new prototypical loss function for training neural network models for target voice separation. Conventional methods use paired parallel audio samples of the target speaker with and without an interfering speaker or noise, and minimize the spectrographic mean squared error (MSE) between the clean and enhanced target speaker audio. Motivated by the use of contrastive loss in speaker recognition task, we had earlier proposed a speaker representation loss that uses representative samples from the target speaker in addition to the conventional MSE loss. In this work, we propose a prototypical speaker-interference (PSI) loss, that makes use of representative samples from the target speaker, interfering speaker as well as the interfering noise to better utilize any non-parallel data that may be available. The performance of the proposed loss function is evaluated using VoiceFilter, a popular framework for target voice separation. Experimental results show that the proposed PSI loss significantly improves the PESQ scores of the enhanced target speaker audio",
    "checked": true,
    "id": "ed3b6e3f0dba366e8ac798a0d8fbadef18537703",
    "semantic_title": "prototypical speaker-interference loss for target voice separation using non-parallel audio samples",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bousquet22_interspeech.html": {
    "title": "Reliability criterion based on learning-phase entropy for speaker recognition with neural network",
    "volume": "main",
    "abstract": "The reliability of Automatic Speaker Recognition (SR) is of the utmost importance for real-world applications. Even if SR systems obtain spectacular performance during evaluation campaigns, several studies have shown the limits and shortcomings of these systems. Reliability first means knowing where and when a system is performing as expected and a research effort is devoted to building confidence measures, by scanning input signals, representations or output scores. Here, a new reliability criterion is presented, dedicated to the latest SR systems based on deep neural network (DNN). The proposed approach uses the set of anchor speakers that controls the learning phase and takes advantage of the structure of the network itself, in order to derive a criterion making it possible to better assess the reliability of the decision based on the extracted speaker embeddings. The relevance and effectiveness of the proposed confidence measure are tested and demonstrated on widely used datasets",
    "checked": true,
    "id": "043c324c3566e44932ab171bc304edf37fa2351d",
    "semantic_title": "reliability criterion based on learning-phase entropy for speaker recognition with neural network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22f_interspeech.html": {
    "title": "Attentive Feature Fusion for Robust Speaker Verification",
    "volume": "main",
    "abstract": "As the most widely used technique, deep speaker embedding learning has become predominant in speaker verification task recently. This approach utilizes deep neural networks to extract fixed dimension embedding vectors which represent different speaker identities. Two network architectures such as ResNet and ECAPA-TDNN have been commonly adopted in prior studies and achieved the state-of-the-art performance. One omnipresent part, feature fusion, plays an important role in both of them. For example, shortcut connections are designed to fuse the identity mapping of inputs and outputs of residual blocks in ResNet. ECAPA-TDNN employs the multi-layer feature aggregation to integrate shallow feature maps with deep ones. Traditional feature fusion is often implemented via simple operations, such as element-wise addition or concatenation. In this paper, we propose a more effective feature fusion scheme, namely Attentive Feature Fusion (AFF), to render dynamic weighted fusion of different features. It utilizes attention modules to learn fusion weights based on the feature contents. Additionally, two fusion strategies are designed: sequential fusion and parallel fusion. Experiments on Voxceleb dataset show that our proposed attentive feature fusion scheme can result in up to 40% relative improvement over the baseline systems",
    "checked": true,
    "id": "2b6f6c62898923b4f66638425ed5cfba78443a14",
    "semantic_title": "attentive feature fusion for robust speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22g_interspeech.html": {
    "title": "Dual Path Embedding Learning for Speaker Verification with Triplet Attention",
    "volume": "main",
    "abstract": "Currently, many different network architectures have been explored in speaker verification, including time-delay neural network (TDNN), convolutional neural network (CNN), transformer and multi-layer perceptrons (MLP). However, hybrid networks with diverse structures are rarely investigated. In this paper, we present a novel and effective dual path embedding learning framework, named Dual Path Network (DPNet), for speaker verification with triplet attention. A new topology of integrating CNN with a separate recurrent layer connection path internally is designed, which introduces the sequential structure along depth into CNN. This new architecture inherits both advantages of residual and recurrent networks, enabling better feature re-usage and re-exploitation. Additionally, an efficient triplet attention module is utilized to capture cross-dimension interactions between features. The experimental results conducted on Voxceleb dataset show that our proposed hybrid network with triplet attention can outperform the corresponding ResNet by a significant margin",
    "checked": true,
    "id": "c4fb36db0131e175a31ace4264c3a928e1811b65",
    "semantic_title": "dual path embedding learning for speaker verification with triplet attention",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22h_interspeech.html": {
    "title": "DF-ResNet: Boosting Speaker Verification Performance with Depth-First Design",
    "volume": "main",
    "abstract": "Embeddings extracted by deep neural networks have become the state-of-the-art utterance representation in speaker verification (SV). Despite the various network architectures that have been investigated in previous works, how to design and scale up networks to achieve a better trade-off on performance and complexity in a principled manner has been rarely discussed in the SV field. In this paper, we first systematically study model scaling from the perspective of the depth and width of networks and empirically discover that depth is more important than the width of networks for speaker verification task. Based on this observation, we design a new backbone constructed entirely from standard convolutional network modules by significantly increasing the number of layers while maintaining the network complexity following the depth-first rule and scale it up to obtain a family of much deeper models dubbed DF-ResNets. Comprehensive comparisons with other state-of-the-art systems on the Voxceleb dataset demonstrate that DF-ResNets achieve a much better trade-off than previous SV systems in terms of performance and complexity",
    "checked": true,
    "id": "08fa9dbc0827ead5700bdf6d3e15d70ceaf65794",
    "semantic_title": "df-resnet: boosting speaker verification performance with depth-first design",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ruida22_interspeech.html": {
    "title": "Adaptive Rectangle Loss for Speaker Verification",
    "volume": "main",
    "abstract": "From the perspective of pair similarity optimization, speaker verification is expected to satisfy the criterion that each intraclass similarity is higher than the maximal inter-class similarity. However, we find that most softmax-based losses are suboptimal which encourages each sample to have a higher target similarity score only than its corresponding non-target similarity scores but not all the non-target ones. To this end, we propose a batch-wise maximum softmax loss, in which the non-target logits are replaced by the ones derived from the whole batch. To further emphasize the minority hard non-target pairs, an adaptive margin mechanism is introduced at the same time. The proposed loss is named Adaptive Rectangle loss due to its rectangle decision boundary. In addition, an annealing strategy is introduced to improve the stability of the training process and boost the convergence. Experimentally, we demonstrate the superiority of adaptive rectangle loss on speaker verification tasks. Results on VoxCeleb show that our proposed loss outperforms state-of-the-art by 10.11% in EER",
    "checked": true,
    "id": "43403366cb69f3f2422a08a159d8b42b7a82890d",
    "semantic_title": "adaptive rectangle loss for speaker verification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22h_interspeech.html": {
    "title": "MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we present Multi-scale Feature Aggregation Conformer (MFA-Conformer), an easy-to-implement, simple but effective backbone for automatic speaker verification based on the Convolution-augmented Transformer (Conformer). The architecture of the MFA-Conformer is inspired by recent state-of-the-art models in speech recognition and speaker verification. Firstly, we introduce a convolution subsampling layer to decrease the computational cost of the model. Secondly, we adopt Conformer blocks which combine Transformers and convolution neural networks (CNNs) to capture global and local features effectively. Finally, the output feature maps from all Conformer blocks are concatenated to aggregate multi-scale representations before final pooling. We evaluate the MFA-Conformer on the widely used benchmarks. The best system obtains 0.64%, 1.29% and 1.63% EER on VoxCeleb1-O, SITW.Dev, and SITW.Eval set, respectively. MFA-Conformer significantly outperforms the popular ECAPA-TDNN systems in both recognition performance and inference speed. Last but not the least, the ablation studies clearly demonstrate that the combination of global and local feature learning can lead to robust and accurate speaker embedding extraction. We have also released the code for future comparison",
    "checked": true,
    "id": "47942bfbc4fdde742137f70aac8028a46b1809d9",
    "semantic_title": "mfa-conformer: multi-scale feature aggregation conformer for automatic speaker verification",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22j_interspeech.html": {
    "title": "Enroll-Aware Attentive Statistics Pooling for Target Speaker Verification",
    "volume": "main",
    "abstract": "The well-developed robust speaker verification system can remove the environment noise and retain speaker information automatically. However, when the uttering voice is disturbed by another interfering speaker's voice, the speaker verification system usually cannot selectively extract only the target speaker's information. Some works have been done by introducing a speech separation network to separate the target speaker's speech in advance. However, adding a speech separation network for speaker verification task could be redundant. Here, we proposed enroll-aware attentive statistic pooling (EA-ASP) layer to help the speaker verification system extract specific speaker's information. To evaluate the system, we simulate the multi-speaker evaluation data based on Voxceleb1 data. The results show that our proposed EA-ASP can outperform the baseline system by a large margin and achieved 50% relative Equal Error Rate (EER) reduction",
    "checked": true,
    "id": "e91d09f889ddb807263d63fc58a5eb886f958d94",
    "semantic_title": "enroll-aware attentive statistics pooling for target speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22b_interspeech.html": {
    "title": "Transport-Oriented Feature Aggregation for Speaker Embedding Learning",
    "volume": "main",
    "abstract": "Pooling is needed to aggregate frame-level features into utterance-level representations for speaker modeling. Given the success of statistics-based pooling methods, we hypothesize that speaker characteristics are well represented in the statistical distribution over the pre-aggregation layer's output, and propose to use transport-oriented feature aggregation for deriving speaker embeddings. The aggregated representation encodes the geometric structure of the underlying feature distribution, which is expected to contain valuable speaker-specific information that may not be represented by the commonly used statistical measures like mean and variance. The original transport-oriented feature aggregation is also extended to a weighted-frame version to incorporate the attention mechanism. Experiments on speaker verification with the Voxceleb dataset show improvement over statistics pooling and its attentive variant",
    "checked": true,
    "id": "365b03585694d22d10fafe2e366702cdc519ce63",
    "semantic_title": "transport-oriented feature aggregation for speaker embedding learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sang22_interspeech.html": {
    "title": "Multi-Frequency Information Enhanced Channel Attention Module for Speaker Representation Learning",
    "volume": "main",
    "abstract": "Recently, attention mechanisms have been applied successfully in neural network-based speaker verification systems. Incorporating the Squeeze-and-Excitation block into convolutional neural networks has achieved remarkable performance. However, it uses global average pooling (GAP) to simply average the features along time and frequency dimensions, which is incapable of preserving sufficient speaker information in the feature maps. In this study, we show that GAP is a special case of a discrete cosine transform (DCT) on time-frequency domain mathematically using only the lowest frequency component in frequency decomposition. To strengthen the speaker information extraction ability, we propose to utilize multi-frequency information and design two novel and effective attention modules, called Single-Frequency Single-Channel (SFSC) attention module and Multi-Frequency Single-Channel (MFSC) attention module. The proposed attention modules can effectively capture more speaker information from multiple frequency components on the basis of DCT. We conduct comprehensive experiments on the VoxCeleb datasets and a probe evaluation on the 1st 48-UTD forensic corpus. Experimental results demonstrate that our proposed SFSC and MFSC attention modules can efficiently generate more discriminative speaker representations and outperform ResNet34-SE and ECAPA-TDNN systems with relative 20.9% and 20.2% reduction in EER, without adding extra network parameters",
    "checked": true,
    "id": "4d052febd3625bc0fd09c81d51ec1c4d58bad95d",
    "semantic_title": "multi-frequency information enhanced channel attention module for speaker representation learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cai22_interspeech.html": {
    "title": "CS-CTCSCONV1D: Small footprint speaker verification with channel split time-channel-time separable 1-dimensional convolution",
    "volume": "main",
    "abstract": "We present an efficient small-footprint network for speaker verification. We start by introducing the bottleneck to the QuartzNet model. Then we proposed a Channel Split Time Channel-Time Separable 1-dimensional Convolution (CS-CTCSConv1d) module, yielding stronger performance over the State-Of-The-Art small footprint speaker verification system. We apply knowledge distillation to further improve performance to learn better speaker embedding from the large model. We evaluate the proposed approach on Voxceleb dataset, obtaining better performances concerning the baseline method. The proposed model takes only 238.9K parameters to outperform the baseline system by 10% relatively in equal error rate (EER)",
    "checked": true,
    "id": "185c5fdc6604d49249388742f3b9e08dfa1d873f",
    "semantic_title": "cs-ctcsconv1d: small footprint speaker verification with channel split time-channel-time separable 1-dimensional convolution",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22l_interspeech.html": {
    "title": "Reliable Visualization for Deep Speaker Recognition",
    "volume": "main",
    "abstract": "In spite of the impressive success of convolutional neural networks (CNNs) in speaker recognition, our understanding to CNNs' internal functions is still limited. A major obstacle is that some popular visualization tools are difficult to apply, for example those producing saliency maps. The reason is that speaker information does not show clear spatial patterns in the temporal-frequency space, which makes it hard to interpret the visualization results, and hence hard to confirm the reliability of a visualization tool. In this paper, we conduct an extensive analysis on three popular visualization methods based on class activation map(CAM): Grad-CAM, Score-CAM and Layer-CAM, to investigate their reliability for speaker recognition tasks. Experiments conducted on a state-of-the-art ResNet34SE model show that the Layer-CAM algorithm can produce reliable visualization, and thus can be used as a promising tool to explain CNN-based speaker models. The source code and examples are available in our project page: http://project.cslt.org/",
    "checked": true,
    "id": "9b532f3979e24db8fc0f621ef278e641474e71c9",
    "semantic_title": "reliable visualization for deep speaker recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22b_interspeech.html": {
    "title": "Unifying Cosine and PLDA Back-ends for Speaker Verification",
    "volume": "main",
    "abstract": "State-of-art speaker verification (SV) systems use a back-end model to score the similarity of speaker embeddings extracted from a neural network. The commonly used back-ends are the cosine scoring and the probabilistic linear discriminant analysis (PLDA) scoring. With the recently developed neural embeddings, the theoretically more appealing PLDA approach is found to have no advantage against or even be inferior to the simple cosine scoring in terms of verification performance. This paper presents an investigation on the relation between the two back-ends, aiming to explain the above counter-intuitive observation. It is shown that the cosine scoring is essentially a special case of PLDA scoring. In other words, by properly setting the parameters of PLDA, the two back-ends become equivalent. As a consequence, the cosine scoring not only inherits the basic assumptions for the PLDA but also introduces additional assumptions on speaker embeddings. Experiments show that the dimensional independence assumption required by the cosine scoring contributes most to the performance gap between the two methods under the domain-matched condition. When there is severe domain mismatch, the dimensional independence assumption does not hold and the PLDA would perform better than the cosine for domain adaptation",
    "checked": true,
    "id": "8642361b68a7d1490face31e098fc95ebd448b0e",
    "semantic_title": "unifying cosine and plda back-ends for speaker verification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22d_interspeech.html": {
    "title": "CTFALite: Lightweight Channel-specific Temporal and Frequency Attention Mechanism for Enhancing the Speaker Embedding Extractor",
    "volume": "main",
    "abstract": "Attention mechanism provides an effective and plug-and-play feature enhancement module for speaker embedding extractors. Attention-based pooling layers have been widely used to aggregate a sequence of frame-level feature vectors into an utterance-level speaker embedding. Besides, convolution attention mechanisms are introduced into convolution blocks to improve the sensibility of speaker embedding extractors to those features with more discriminative speaker characteristics. However, it is still a challenging problem to make a good trade off between performance and model complexity for convolution attention models, especially for speaker recognition systems on low-resource edge computing nodes (smartphone, embedded devices, etc.). In this paper, we propose a lightweight convolution attention model named as CTFALite, which learns channel-specific temporal attention and frequency attention by leveraging both of the global context information and the local cross-channel dependencies. Experiment results demonstrate the effectiveness of CTFALite for improving performance. The further analysis about computational resource consumption shows that CTFALite achieves a better trade-off between performance and computational complexity, compared to other competing lightweight convolution attention mechanisms",
    "checked": true,
    "id": "5a7ad4b9b94d5bdc0087532c1689dc55bb9acec5",
    "semantic_title": "ctfalite: lightweight channel-specific temporal and frequency attention mechanism for enhancing the speaker embedding extractor",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22_interspeech.html": {
    "title": "SpeechFormer: A Hierarchical Efficient Framework Incorporating the Characteristics of Speech",
    "volume": "main",
    "abstract": "Transformer has obtained promising results on cognitive speech signal processing field, which is of interest in various applications ranging from emotion to neurocognitive disorder analysis. However, most works treat speech signal as a whole, leading to the neglect of the pronunciation structure that is unique to speech and reflects the cognitive process. Meanwhile, Transformer has heavy computational burden due to its full attention operation. In this paper, a hierarchical efficient framework, called SpeechFormer, which considers the structural characteristics of speech, is proposed and can be served as a general-purpose backbone for cognitive speech signal processing. The proposed SpeechFormer consists of frame, phoneme, word and utterance stages in succession, each performing a neighboring attention according to the structural pattern of speech with high computational efficiency. SpeechFormer is evaluated on speech emotion recognition (IEMOCAP & MELD) and neurocognitive disorder detection (Pitt & DAIC-WOZ) tasks, and the results show that SpeechFormer outperforms the standard Transformer-based framework while greatly reducing the computational cost. Furthermore, our SpeechFormer achieves comparable results to the state-of-the-art approaches",
    "checked": true,
    "id": "40eca3f99c70f50501fdae4a0e2dd9a239d5d70d",
    "semantic_title": "speechformer: a hierarchical efficient framework incorporating the characteristics of speech",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2022/feinberg22_interspeech.html": {
    "title": "VoiceLab: Software for Fully Reproducible Automated Voice Analysis",
    "volume": "main",
    "abstract": "There's a problem with acoustic analyses because you often need to hand adjust parameters meaning you can only process them individually or in small batches. This creates two key problems. First, it compromises the reproducibility of measurements because setting parameters by hand requires specialist knowledge and is often poorly documented. Second, it means that you can't easily process large samples of voices, which creates bottlenecks in workflows. This issue is compounded by researchers looking to use increasingly large and diverse samples. To address these issues, VoiceLab software offers automated acoustical analysis and automatically logs analysis parameters. VoiceLab analyses are fully reproducible and require little to no knowledge about acoustical analysis from the user. Analysis parameters can also be manually adjusted by experts. VoiceLab is used primarily by researchers studying person perception, creating reproducible voice manipulations, developing voices for conversational agents, and creating feature sets for machine learning",
    "checked": true,
    "id": "4a7e4b4e19217734423ad1f6d8753745582807af",
    "semantic_title": "voicelab: software for fully reproducible automated voice analysis",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shor22_interspeech.html": {
    "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
    "volume": "main",
    "abstract": "Recent advances in self-supervision have dramatically improved the quality of speech representations. However, deployment of state-of-the-art embedding models on devices has been restricted due to their limited public availability and large resource footprint. Our work addresses these issues by publicly releasing a collection of paralinguistic speech models that are small and near state-of-the-art performance. Our approach is based on knowledge distillation, and our models are distilled on public data only. We explore different architectures and thoroughly evaluate our models on the NonSemantic Speech (NOSS) benchmark. Our largest distilled model achieves over 96% the accuracy on 6 of 7 tasks, is less than 15% the size of the original model (314MB vs 2.2GB), and is trained on 6.5% the data. The smallest model achieves over 90% the accuracy on 6 of 7 tasks and is 1% in size (22MB). Our models outperform the 1.2GB open source Wav2Vec 2.0 model on 5 of 7 tasks despite being less than a third the size, and one of our models outperforms Wav2Vec 2.0 on both emotion recognition tasks despite being less than 4% the size",
    "checked": true,
    "id": "4d35845f81c5afe83c9b101507d2a187ef4d8c3c",
    "semantic_title": "trillsson: distilled universal paralinguistic speech representations",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22b_interspeech.html": {
    "title": "Global Signal-to-noise Ratio Estimation Based on Multi-subband Processing Using Convolutional Neural Network",
    "volume": "main",
    "abstract": "The global signal-to-noise ratio (gSNR) is defined as the ratio of speech energy to noise energy in whole noisy audio. However, due to the increase in noise interference, the generalization ability declines when the traditional features (e.g., raw waveforms and MFCCs) are fed directly to the statistical model to estimate a single fullband gSNR. In this paper, we propose a multi-subband-based gSNR estimation network (MSGNet). Specifically, we split the noisy speech waveforms into Bark-scale subbands to obtain higher resolution signals to the middle and low frequencies. Then, convolutional neural networks (CNNs) are used to learn a non-linear function to estimate the speech and noise energy ratio of each subband from the input muti-subband features. Finally, by integrating subbands with different speech and noise energies, gSNR in the fullband is calculated. Extensive experimental results on the AURORA-2J dataset demonstrate that the proposed MSGNet significantly reduces the mean absolute error compared to other baseline gSNR estimation methods",
    "checked": true,
    "id": "4c159b24ee30dece185cc167e2ed3bce1ee73405",
    "semantic_title": "global signal-to-noise ratio estimation based on multi-subband processing using convolutional neural network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sadeghi22_interspeech.html": {
    "title": "A Sparsity-promoting Dictionary Model for Variational Autoencoders",
    "volume": "main",
    "abstract": "Structuring the latent space in probabilistic deep generative models, e.g., variational autoencoders (VAEs), is important to yield more expressive models and interpretable representations, and to avoid overfitting. One way to achieve this objective is to impose a sparsity constraint on the latent variables, e.g., via a Laplace prior. However, such approaches usually complicate the training phase, and they sacrifice the reconstruction quality to promote sparsity. In this paper, we propose a simple yet effective methodology to structure the latent space via a sparsity-promoting dictionary model, which assumes that each latent code can be written as a sparse linear combination of a dictionary's columns. In particular, we leverage a computationally efficient and tuning-free method, which relies on a zero-mean Gaussian latent prior with learnable variances. We derive a variational inference scheme to train the model. Experiments on speech generative modeling demonstrate the advantage of the proposed approach over competing techniques, since it promotes sparsity while not deteriorating the output speech quality",
    "checked": true,
    "id": "6aed8f70d922527d172da51aa35c2ff6fd99cb9d",
    "semantic_title": "a sparsity-promoting dictionary model for variational autoencoders",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22h_interspeech.html": {
    "title": "Deep Transductive Transfer Regression Network for Cross-Corpus Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this paper, we focus on the research of cross-corpus speech emotion recognition (SER), in which the training (source) and testing (target) speech samples come from different corpora leading to a feature distribution gap between them. To solve this problem, we propose a simple yet effective method called deep transductive transfer regression network (DTTRN). The basic idea of DTTRN is to learn a corpus invariant deep neural network to bridge the source and target speech samples and their label information. Following this idea, we make use of a transductive learning way to enforce a deep regressor to build the relationship between the features and emotional labels jointly in both speech corpora. Meanwhile, we also design an emotion guided regularization term for learning DTTRN by aligning source and target speech samples feature distributions from three different scales. Thus, the DTTRN only absorbing the label information provided by source speech samples is able to correctly predict the emotions of the target ones. To evaluate DTTRN, we conduct extensive cross-corpus SER experiments on EmoDB, CASIA, and eNTERFACE corpora. Experimental results show the superior performance of our DTTRN over recent state-of-the-art deep transfer learning methods in dealing with the cross-corpus SER tasks",
    "checked": true,
    "id": "ea6cbe4b1163207601c5bb0932630a6acd2b8ed3",
    "semantic_title": "deep transductive transfer regression network for cross-corpus speech emotion recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hansen22_interspeech.html": {
    "title": "Audio Anti-spoofing Using Simple Attention Module and Joint Optimization Based on Additive Angular Margin Loss and Meta-learning",
    "volume": "main",
    "abstract": "Automatic speaker verification systems are vulnerable to a variety of access threats, prompting research into the formulation of effective spoofing detection systems to act as a gate to filter out such spoofing attacks. This study introduces a simple attention module to infer 3-dim attention weights for the feature map in a convolutional layer, which then optimizes an energy function to determine each neuron's importance. With the advancement of both voice conversion and speech synthesis technologies, un-seen spoofing attacks are constantly emerging to limit spoofing detection system performance. Here, we propose a joint optimization approach based on the weighted additive angular margin loss for binary classification, with a meta-learning training framework to develop an efficient system that is robust to a wide range of spoofing attacks for model generalization en- enhancement. As a result, when compared to current state-of-the-art systems, our proposed approach delivers a competitive result with a pooled EER of 0.99% and min t-DCF of 0.0289",
    "checked": true,
    "id": "e871fa38d82b20d21dc925548802c27718346185",
    "semantic_title": "audio anti-spoofing using simple attention module and joint optimization based on additive angular margin loss and meta-learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bergsma22_interspeech.html": {
    "title": "PEAF: Learnable Power Efficient Analog Acoustic Features for Audio Recognition",
    "volume": "main",
    "abstract": "At the end of Moore's law, new computing paradigms are required to prolong the battery life of wearable and IoT smart audio devices. Theoretical analysis and physical validation have shown that analog signal processing (ASP) can be more power-efficient than its digital counterpart in the realm of low-to-medium signal-to-noise ratio applications. In addition, ASP allows a direct interface with an analog microphone without a power-hungry analog-to-digital converter. Here, we present power-efficient analog acoustic features (PEAF) that are validated by fabricated CMOS chips for running audio recognition. Linear, non-linear, and learnable PEAF variants are evaluated on two speech processing tasks that are demanded in many battery-operated devices: wake word detection (WWD) and keyword spotting (KWS). Compared to digital acoustic features, higher power efficiency with competitive classification accuracy can be obtained. A novel theoretical framework based on information theory is established to analyze the information flow in each individual stage of the feature extraction pipeline. The analysis identifies the information bottleneck and helps improve the KWS accuracy by up to 7%. This work may pave the way to building more power-efficient smart audio devices with best-in-class inference performance",
    "checked": true,
    "id": "4abd6fc03c16ea39c3285db8ffca6bf0de87dc48",
    "semantic_title": "peaf: learnable power efficient analog acoustic features for audio recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/elbanna22_interspeech.html": {
    "title": "Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load",
    "volume": "main",
    "abstract": "As a neurophysiological response to threat or adverse conditions, stress can affect cognition, emotion and behaviour with potentially detrimental effects on health in the case of sustained exposure. Since the affective content of speech is inherently modulated by an individual's physical and mental state, a substantial body of research has been devoted to the study of paralinguistic correlates of stress-inducing task load. Historically, voice stress analysis (VSA) has been conducted using conventional digital signal processing (DSP) techniques. Despite the development of modern methods based on deep neural networks (DNNs), accurately detecting stress in speech remains difficult due to the wide variety of stressors and considerable variability in the individual stress perception. To that end, we introduce a set of five datasets for task load detection in speech. The voice recordings were collected as either cognitive or physical stress was induced in the cohort of volunteers, with a cumulative number of more than a hundred speakers. We used the datasets to design and evaluate a novel self-supervised audio representation that leverages the effectiveness of handcrafted features (DSP-based) and the complexity of data-driven DNN representations. Notably, the proposed approach outperformed both extensive handcrafted feature sets and novel DNN-based audio representation learning approaches",
    "checked": true,
    "id": "f9fc66bdecafcc90d6dbc3991f91975a1da6e09e",
    "semantic_title": "hybrid handcrafted and learnable audio representation for analysis of speech under cognitive and physical load",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22w_interspeech.html": {
    "title": "Generative Data Augmentation Guided by Triplet Loss for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) is crucial for human-computer interaction but still remains a challenging problem because of two major obstacles: data scarcity and imbalance. Many datasets for SER are substantially imbalanced, where data utterances of one class (most often Neutral) are much more frequent than those of other classes. Furthermore, only a few data resources are available for many existing spoken languages. To address these problems, we exploit a GAN-based augmentation model guided by a triplet network, to improve SER performance given imbalanced and insufficient training data. We conduct experiments and demonstrate: 1) With a highly imbalanced dataset, our augmentation strategy significantly improves the SER performance (+8\\% recall score compared with the baseline). 2) Moreover, in a cross-lingual benchmark, where we train a model with enough source language utterances but very few target language utterances (around 50 in our experiments), our augmentation strategy brings benefits for the SER performance of all three target languages",
    "checked": true,
    "id": "8c470b97f8ae3e9e9f623e38be01f6ad6d69d79c",
    "semantic_title": "generative data augmentation guided by triplet loss for speech emotion recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yadav22_interspeech.html": {
    "title": "Learning neural audio features without supervision",
    "volume": "main",
    "abstract": "Deep audio classification, traditionally cast as training a deep neural network on top of mel-filterbanks in a supervised fashion, has recently benefited from two independent lines of work. The first one explores \"learnable frontends'', i.e., neural modules that produce a learnable time-frequency representation, to overcome limitations of fixed features. The second one uses self-supervised learning to leverage unprecedented scales of pre-training data. In this work, we study the feasibility of combining both approaches, i.e., pre-training learnable frontend jointly with the main architecture for downstream classification. First, we show that pretraining two previously proposed frontends (SincNet and LEAF) on Audioset drastically improves linear-probe performance over fixed mel-filterbanks, suggesting that learnable time-frequency representations can benefit self-supervised pre-training even more than supervised training. Surprisingly, randomly initialized learnable filterbanks outperform mel-scaled initialization in the self-supervised setting, a counter-intuitive result that questions the appropriateness of strong priors when designing learnable filters. Through exploratory analysis of the learned frontend components, we uncover crucial differences in properties of these frontends when used in a supervised and self-supervised setting, especially the affinity of self-supervised filters to diverge significantly from the mel-scale to model a broader range of frequencies",
    "checked": true,
    "id": "f67d1a19823dec6eeb32339018f89a6176d38477",
    "semantic_title": "learning neural audio features without supervision",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22ca_interspeech.html": {
    "title": "Densely-connected Convolutional Recurrent Network for Fundamental Frequency Estimation in Noisy Speech",
    "volume": "main",
    "abstract": "Estimating fundamental frequency (F0) from an audio signal is a necessary step in many tasks such as speech synthesis and speech analysis. Although high estimation accuracy has been achieved for clean speech, it is still challenging for F0 estimation to handle noisy speech, mainly because of the corruption of harmonic structure caused by noise. In this paper, we view F0 estimation as a multi-class classification problem and train a frequency-domain densely-connected convolutional neural network (DC-CRN) to estimate F0 from noisy speech. The proposed model significantly outperforms baseline methods in terms of detection rate. We find that using complex short-time Fourier transform (STFT) as input produces better performance compared to using magnitude STFT as input. Furthermore, we explore improving F0 estimation with speech enhancement. Although the F0 estimation model trained on clean speech performs well on enhanced speech, the distortion introduced by the speech enhancement model limits the estimation performance. We propose a cascade model which consists of two modules that optimize enhanced speech and estimated F0 in turn. Experimental results show that the cascade model brings further improvements to the DC-CRN model, especially in low signal-to-noise ratio (SNR) conditions",
    "checked": true,
    "id": "b2b0e3b8d6600fd82e33a0aac512f64d54d6988d",
    "semantic_title": "densely-connected convolutional recurrent network for fundamental frequency estimation in noisy speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/faridee22_interspeech.html": {
    "title": "Predicting label distribution improves non-intrusive speech quality estimation",
    "volume": "main",
    "abstract": "Deep noise suppressors (DNS) have become an attractive solution to remove background noise, reverberation, and distortions from speech and are widely used in telephony/voice applications. They are also occasionally prone to introducing artifacts and lowering the perceptual quality of the speech. Subjective listening tests that use multiple human judges to derive a mean opinion score (MOS) are a popular way to measure these models' performance. Deep neural network based non-intrusive MOS estimation models have recently emerged as a popular cost-efficient alternative to these tests. These models are trained with only the MOS labels, often discarding the secondary statistics of the opinion scores. In this paper, we investigate several ways to integrate the distribution of opinion scores (e.g. variance, histogram information) to improve the MOS estimation performance. Our model is trained on a corpus of 419K denoised samples by 320 different DNS models and model variations and evaluated on 18K test samples from DNSMOS. We show that with very minor modification of a single task MOS estimation pipeline, these freely available labels can provide up to a 0.016 RMSE and 1% SRCC improvement",
    "checked": true,
    "id": "2be30cd0b7657718282804391e2039337af2229a",
    "semantic_title": "predicting label distribution improves non-intrusive speech quality estimation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ashihara22_interspeech.html": {
    "title": "Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) is seen as a very promising approach with high performance for several speech downstream tasks. Since the parameters of SSL models are generally so large that training and inference require a lot of memory and computational cost, it is desirable to produce compact SSL models without a significant performance degradation by applying compression methods such as knowledge distillation (KD). Although the KD approach is able to shrink the depth and/or width of SSL model structures, there has been little research on how varying the depth and width impacts the internal representation of the small-footprint model. This paper provides an empirical study that addresses the question. We investigate the performance on SUPERB while varying the structure and KD methods so as to keep the number of parameters constant; this allows us to analyze the contribution of the representation introduced by varying the model architecture. Experiments demonstrate that a certain depth is essential for solving content-oriented tasks (e.g. automatic speech recognition) accurately, whereas a certain width is necessary for achieving high performance on several speaker-oriented tasks (e.g. speaker identification). Based on these observations, we identify, for SUPERB, a more compressed model with better performance than previous studies",
    "checked": true,
    "id": "1d91bc3979e87380c31ac8aa152919228a639a04",
    "semantic_title": "deep versus wide: an analysis of student architectures for task-agnostic knowledge distillation of self-supervised speech models",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/azeemi22_interspeech.html": {
    "title": "Dataset Pruning for Resource-constrained Spoofed Audio Detection",
    "volume": "main",
    "abstract": "The performance of neural anti-spoofing models has rapidly improved in recent years due to larger network architectures and better training methodologies. However, these systems require considerable training data for achieving high performance, which makes it challenging to train them in compute-restricted environments. To make these systems accessible in resource-constrained environments, we consider the task of training neural anti-spoofing models with limited training data. We apply multiple dataset pruning techniques to the ASVspoof 2019 dataset for selecting the most informative training examples and pruning a significant chunk of the data with minimal decrease in performance. We find that the existing pruning metrics are not simultaneously granular and stable. To address this problem and further improve the performance of anti-spoofing models on pruned data, we propose a new metric, Forgetting Norm, to score individual training examples with higher granularity. Extensive experiments on two anti-spoofing models, AASIST-L and RawNet2, and several pruning settings demonstrate up to 23% relative improvement with forgetting norm over other baseline pruning heuristics. We also demonstrate the desirable properties of the proposed metric by analyzing the training landscape of the neural anti-spoofing models",
    "checked": true,
    "id": "1d9200f3eec099abbab3a62d3ab5a9e251a526b5",
    "semantic_title": "dataset pruning for resource-constrained spoofed audio detection",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tae22_interspeech.html": {
    "title": "EdiTTS: Score-based Editing for Controllable Text-to-Speech",
    "volume": "main",
    "abstract": "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements",
    "checked": true,
    "id": "4ecf116b9a8fdee90030a9c8c45d9c20b77bc703",
    "semantic_title": "editts: score-based editing for controllable text-to-speech",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22b_interspeech.html": {
    "title": "Improving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information",
    "volume": "main",
    "abstract": "For text-to-speech (TTS) synthesis, prosodic structure prediction (PSP) plays an important role in producing natural and intelligible speech. Although inter-utterance linguistic information can influence the speech interpretation of the target utterance, previous works on PSP mainly focus on utilizing intrautterance linguistic information of the current utterance only. This work proposes to use inter-utterance linguistic information to improve the performance of PSP. Multi-level contextual information, which includes both inter-utterance and intrautterance linguistic information, is extracted by a hierarchical encoder from character level, utterance level and discourse level of the input text. Then a multi-task learning (MTL) decoder predicts prosodic boundaries from multi-level contextual information. Objective evaluation results on two datasets show that our method achieves better F1 scores in predicting prosodic word (PW), prosodic phrase (PPH) and intonational phrase (IPH). It demonstrates the effectiveness of using multi-level contextual information for PSP. Subjective preference tests also indicate the naturalness of synthesized speeches are improved",
    "checked": true,
    "id": "86ff8b31cf636ece970a6a8bd56e2611848845db",
    "semantic_title": "improving mandarin prosodic structure prediction with multi-level contextual information",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/borsos22_interspeech.html": {
    "title": "SpeechPainter: Text-conditioned Speech Inpainting",
    "volume": "main",
    "abstract": "We propose SpeechPainter, a model for filling in gaps of up to one second in speech samples by leveraging an auxiliary textual input. We demonstrate that the model performs speech inpainting with the appropriate content, while maintaining speaker identity, prosody and recording environment conditions, and generalizing to unseen speakers. Our approach significantly outperforms baselines constructed using adaptive TTS, as judged by human raters in side-by-side preference and MOS tests",
    "checked": true,
    "id": "0a33c8d2e83b2d8d611698b52805232567724817",
    "semantic_title": "speechpainter: text-conditioned speech inpainting",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22b_interspeech.html": {
    "title": "A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese",
    "volume": "main",
    "abstract": "Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese Mandarin text-to-speech (TTS) system, and the core of G2P conversion is to solve the problem of polyphone disambiguation, which is to pick up the correct pronunciation for several candidates for a Chinese polyphonic character. In this paper, we propose a Chinese polyphone BERT model to predict the pronunciations of Chinese polyphonic characters. Firstly, we create 741 new Chinese monophonic characters from 354 source Chinese polyphonic characters by pronunciation. Then we get a Chinese polyphone BERT by extending a pre-trained Chinese BERT with 741 new Chinese monophonic characters and adding a corresponding embedding layer for new tokens, which is initialized by the embeddings of source Chinese polyphonic characters. In this way, we can turn the polyphone disambiguation task into a pre-training task of the Chinese polyphone BERT. Experimental results demonstrate the effectiveness of the proposed model, and the polyphone BERT model obtain 2% (from 92.1% to 94.1%) improvement of average accuracy compared with the BERT-based classifier model, which is the prior state-of-the-art in polyphone disambiguation",
    "checked": true,
    "id": "83b5ea4e7ac8dba943c2e720e46aa4d4fca9e190",
    "semantic_title": "a polyphone bert for polyphone disambiguation in mandarin chinese",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/he22b_interspeech.html": {
    "title": "Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge",
    "volume": "main",
    "abstract": "End-to-end TTS requires a large amount of speech/text paired data to cover all necessary knowledge, particularly how to pronounce different words in diverse contexts, so that a neural model may learn such knowledge accordingly. But in real applications, such high demand of training data is hard to be satisfied and additional knowledge often needs to be injected manually. For example, to capture pronunciation knowledge on languages without regular orthography, a complicated grapheme-to-phoneme pipeline needs to be built based on a large structured pronunciation lexicon, leading to extra, sometimes high, costs to extend neural TTS to such languages. In this paper, we propose a framework to learn to automatically extract knowledge from unstructured external resources using a novel Token2Knowledge attention module. The framework is applied to build a TTS model named Neural Lexicon Reader that extracts pronunciations from raw lexicon texts in an end-to-end manner. Experiments show the proposed model significantly reduces pronunciation errors in low-resource, end-to-end Chinese TTS, and the lexicon-reading capability can be transferred to other languages with a smaller amount of data",
    "checked": true,
    "id": "d2b4a3a3fe9915b7d9455fe177f76a8884a05673",
    "semantic_title": "neural lexicon reader: reduce pronunciation errors in end-to-end tts by leveraging external textual knowledge",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22_interspeech.html": {
    "title": "ByT5 model for massively multilingual grapheme-to-phoneme conversion",
    "volume": "main",
    "abstract": "In this study, we tackle massively multilingual grapheme-to-phoneme conversion through implementing G2P models based on ByT5. We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. We found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P. Pairwise comparison with monolingual models in these languages suggests that multilingual ByT5 models generally lower the phone error rate by jointly learning from a variety of languages. The pretrained model can further benefit low resource G2P through zero-shot prediction on unseen languages or provides pretrained weights for finetuning, which helps the model converge to a lower phone error rate than randomly initialized weights. To facilitate future research on multilingual G2P, we make available our code and pretrained multilingual G2P models at: https://github.com/lingjzhu/CharsiuG2P",
    "checked": true,
    "id": "880c8973ea1376c6bff23226b3f64792657aa666",
    "semantic_title": "byt5 model for massively multilingual grapheme-to-phoneme conversion",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mathur22_interspeech.html": {
    "title": "DocLayoutTTS: Dataset and Baselines for Layout-informed Document-level Neural Speech Synthesis",
    "volume": "main",
    "abstract": "We propose a new task of synthesizing speech directly from semi-structured documents where the extracted text tokens from OCR systems may not be in the correct reading order due to the complex document layout. We refer to this task as layout-informed document-level TTS and present the DocSpeech dataset which consists of 10K audio clips of a single-speaker reading layout-enriched Word document. For each document, we provide the natural reading order of text tokens, their corresponding bounding boxes, and the audio clips synthesized in the correct reading order. We also introduce DocLayoutTTS, a Transformer encoder-decoder architecture that generates speech in an end-to-end manner given a document image with OCR extracted text. Our architecture simultaneously learns text reordering and mel-spectrogram prediction in a multi-task setup. Moreover, we take advantage of curriculum learning to progressively learn longer, more challenging document-level text utilizing both \\texttt{DocSpeech} and LJSpeech datasets. Our empirical results show that the underlying task is challenging. Our proposed architecture performs slightly better than competitive baseline TTS models with a pre-trained model providing reading order priors. We release samples of the DocSpeech dataset",
    "checked": true,
    "id": "43e9409c847cefd7ec00e65635cd995cbb017862",
    "semantic_title": "doclayouttts: dataset and baselines for layout-informed document-level neural speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22i_interspeech.html": {
    "title": "Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech",
    "volume": "main",
    "abstract": "Recently, leveraging BERT pre-training to improve the phoneme encoder in text to speech (TTS) has drawn increasing attention. However, the works apply pre-training with character-based units to enhance the TTS phoneme encoder, which is inconsistent with the TTS fine-tuning that takes phonemes as input. Pre-training only with phonemes as input can alleviate the input mismatch but lack the ability to model rich representations and sematic information due to limited phoneme vocabulary. In this paper, we propose Mixed-Phoneme BERT, a novel variant of the BERT model that uses mixed phoneme and sup-phoneme representations to enhance the learning capability. Specifically, we merge the adjacent phonemes into sup-phonemes and combine the phoneme sequence and the merged sup-phoneme sequence as the model input, which can enhance the model capacity to learn rich contextual representations. Experiment results demonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS performance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The Mixed-Phoneme BERT achieves $3\\times$ inference speedup and similar voice quality to the previous TTS pre-trained model PnG BERT",
    "checked": true,
    "id": "b3b1659c992cbbd233038522ddd170887c033fe7",
    "semantic_title": "mixed-phoneme bert: improving bert with mixed phoneme and sup-phoneme representations for text to speech",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ni22_interspeech.html": {
    "title": "Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition",
    "volume": "main",
    "abstract": "An unsupervised text-to-speech synthesis (TTS) system learns to generate speech waveforms corresponding to any written sentence in a language by observing: 1) a collection of untranscribed speech waveforms in that language; 2) a collection of texts written in that language without access to any transcribed speech. Developing such a system can significantly improve the availability of speech technology to languages without a large amount of parallel speech and text data. This paper proposes an unsupervised TTS system based on an alignment module that outputs pseudo-text and another synthesis module that uses pseudo-text for training and real text for inference. Our unsupervised system can achieve comparable performance to the supervised system in seven languages with about 10-20 hours of speech each. A careful study on the effect of text units and vocoders has also been conducted to better understand what factors may affect unsupervised TTS performance. The samples generated by our models can be found at https://cactuswiththoughts.github.io/UnsupTTS-Demo, and our code can be found at https://github.com/lwang114/UnsupTTS",
    "checked": true,
    "id": "ed8fccab99d018f478899d3b9014b8f98a9af38f",
    "semantic_title": "unsupervised text-to-speech synthesis by unsupervised automatic speech recognition",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tran22_interspeech.html": {
    "title": "An Efficient and High Fidelity Vietnamese Streaming End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "In recent years, parallel end-to-end speech synthesis systems have outperformed the 2-stage TTS approaches in audio quality and latency. A parallel end-to-end speech like VITS can generate the audio with high MOS comparable to ground truth and achieve low latency on GPU. However, the VITS still has high latency when synthesizing long utterances on CPUs. Therefore, in this paper, we propose a streaming method for the parallel speech synthesis model like VITS to synthesize with the long texts effectively on CPU. Our system has achieved human-like speech quality in both the non-streaming and streaming mode on the in-house Vietnamese evaluation set, while the synthesis speed of our system is approximately four times faster than that of the VITS in the non-streaming mode. Furthermore, the customer perceived latency of our system in streaming mode is 25 times faster than the VITS on computer CPU. Our system in non-streaming mode achieves a MOS of 4.43 compared to ground-truth with MOS 4.56; it also has high-quality speech with a MOS of 4.35 in streaming mode. Finally, we release a Vietnamese single accent dataset used in our experiments",
    "checked": true,
    "id": "be2ee91e2e9b746d4a3a1aa5707d071dc072fca7",
    "semantic_title": "an efficient and high fidelity vietnamese streaming end-to-end speech synthesis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/valentinibotinhao22_interspeech.html": {
    "title": "Predicting pairwise preferences between TTS audio stimuli using parallel ratings data and anti-symmetric twin neural networks",
    "volume": "main",
    "abstract": "Automatically predicting the outcome of subjective listening tests is a challenging task. Ratings may vary from person to person even if preferences are consistent across listeners. While previous work has focused on predicting listeners' ratings (mean opinion scores) of individual stimuli, we focus on the simpler task of predicting subjective preference given two speech stimuli for the same text. We propose a model based on anti-symmetric twin neural networks, trained on pairs of waveforms and their corresponding preference scores. We explore both attention and recurrent neural nets to account for the fact that stimuli in a pair are not time aligned. To obtain a large training set we convert listeners' ratings from MUSHRA tests to values that reflect how often one stimulus in the pair was rated higher than the other. Specifically, we evaluate performance on data obtained from twelve MUSHRA evaluations conducted over five years, containing different TTS systems, built from data of different speakers. Our results compare favourably to a state-of-the-art model trained to predict MOS scores",
    "checked": true,
    "id": "8b0850810f852fe7c307aa0afd66e61a53ede988",
    "semantic_title": "predicting pairwise preferences between tts audio stimuli using parallel ratings data and anti-symmetric twin neural networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22j_interspeech.html": {
    "title": "An Automatic Soundtracking System for Text-to-Speech Audiobooks",
    "volume": "main",
    "abstract": "Background music (BGM) plays an essential role in audiobooks, which can enhance the immersive experience of audiences and help them better understand the story. However, well-designed BGM still requires human effort in the text-to-speech (TTS) audiobook production, which is quite time-consuming and costly. In this paper, we introduce an automatic soundtracking system for TTS-based audiobooks. The proposed system divides the soundtracking process into three tasks: plot partition, plot classification, and music selection. The experiments shows that both our plot partition module and plot classification module outperform baselines by a large margin. Furthermore, TTS-based audiobooks produced with our proposed automatic soundtracking system achieves comparable performance to that produced with the human soundtracking system. To our best of knowledge, this is the first work of automatic soundtracking system for audiobooks. Demos are available on https://acst1223.github.io/interspeech2022/main",
    "checked": true,
    "id": "b9853557e0362976c8205b68b011bfcf0d5b4161",
    "semantic_title": "an automatic soundtracking system for text-to-speech audiobooks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tan22_interspeech.html": {
    "title": "Environment Aware Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "This study aims at designing an environment-aware text-to-speech (TTS) system that can generate speech to suit specific acoustic environments. It is also motivated by the desire to leverage massive data of speech audio from heterogeneous sources in TTS system development. The key idea is to model the acoustic environment in speech audio as a factor of data variability and incorporate it as a condition in the process of neural network based speech synthesis. Two embedding extractors are trained with two purposely constructed datasets for characterization and disentanglement of speaker and environment factors in speech. A neural network model is trained to generate speech from extracted speaker and environment embeddings. Objective and subjective evaluation results demonstrate that the proposed TTS system is able to effectively disentangle speaker and environment factors and synthesize speech audio that carries designated speaker characteristics and environment attribute. Audio samples are available online for demonstration",
    "checked": true,
    "id": "9a18d05858d3a08c193de32737b1d5917c86c8cf",
    "semantic_title": "environment aware text-to-speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ploujnikov22_interspeech.html": {
    "title": "SoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation",
    "volume": "main",
    "abstract": "End-to-end speech synthesis models directly convert the input characters into an audio representation (e.g., spectrograms). Despite their impressive performance, such models have difficulty disambiguating the pronunciations of identically spelled words. To mitigate this issue, a separate Grapheme-to-Phoneme (G2P) model can be employed to convert the characters into phonemes before synthesizing the audio. This paper proposes SoundChoice, a novel G2P architecture that processes entire sentences rather than operating at the word level. The proposed architecture takes advantage of a weighted homograph loss (that improves disambiguation), exploits curriculum learning (that gradually switches from wordlevel to sentence-level G2P), and integrates word embeddings from BERT (for further performance improvement). Moreover, the model inherits the best practices in speech recognition, including multi-task learning with Connectionist Temporal Classification (CTC) and beam search with an embedded language model. As a result, SoundChoice achieves a Phoneme Error Rate (PER) of 2.65% on whole-sentence transcription using data from LibriSpeech and Wikipedia",
    "checked": true,
    "id": "bb3d5919ac730fc0e59b325cac048b17a651f003",
    "semantic_title": "soundchoice: grapheme-to-phoneme models with semantic disambiguation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bakhturina22_interspeech.html": {
    "title": "Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization",
    "volume": "main",
    "abstract": "Text normalization (TN) systems in production are largely rule-based using weighted finite-state transducers (WFST). However, WFST-based systems struggle with ambiguous input when the normalized form is context-dependent. On the other hand, neural text normalization systems can take context into account but they suffer from unrecoverable errors and require labeled normalization datasets, which are hard to collect. We propose a new hybrid approach that combines the benefits of rule-based and neural systems. First, a non-deterministic WFST outputs all normalization candidates, and then a neural language model picks the best one -- similar to shallow fusion for automatic speech recognition. While the WFST prevents unrecoverable errors, the language model resolves contextual ambiguity. We show for English that the approach is effective and easy to extend. It achieves comparable or better results than existing state-of-the-art TN models",
    "checked": true,
    "id": "ccf6c9fdda939f9e62c4fdf4aaca3b6c43ea692f",
    "semantic_title": "shallow fusion of weighted finite-state transducer and language model for text normalization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/virkar22_interspeech.html": {
    "title": "Prosodic alignment for off-screen automatic dubbing",
    "volume": "main",
    "abstract": "The goal of automatic dubbing is to perform speech-to-speech translation while achieving audiovisual coherence. This entails isochrony, i.e., translating the original speech by also matching its prosodic structure into phrases and pauses, especially when the speaker's mouth is visible. In previous work, we introduced a prosodic alignment model to address isochrone or on-screen dubbing. In this work, we extend the prosodic alignment model to also address off-screen dubbing that requires less stringent synchronization constraints. We conduct experiments on four dubbing directions  English to French, Italian, German and Spanish  on a publicly available collection of TED Talks and on publicly available YouTube videos. Empirical results show that compared to our previous work the extended prosodic alignment model provides significantly better subjective viewing experience on videos in which on-screen and off-screen automatic dubbing is applied for sentences with speakers mouth visible and not visible, respectively",
    "checked": true,
    "id": "d6137685be671bcf1e3a062f8ee3c76d8c651b88",
    "semantic_title": "prosodic alignment for off-screen automatic dubbing",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bai22c_interspeech.html": {
    "title": "A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis",
    "volume": "main",
    "abstract": "In human speech, the attitude of a speaker cannot be fully expressed only by the textual content. It has to come along with the intonation. Declarative questions are commonly used in daily Cantonese conversations, and they are usually uttered with rising intonation. Vanilla neural text-to-speech (TTS) systems are not capable of synthesizing rising intonation for these sentences due to the loss of semantic information. Though it has become more common to complement the systems with extra language models, their performance in modeling rising intonation is not well studied. In this paper, we propose to complement the Cantonese TTS model with a BERT-based statement/question classifier. We design different training strategies and compare their performance. We conduct our experiments on a Cantonese corpus named CanTTS. Empirical results show that the separate training approach obtains the best generalization performance and feasibility",
    "checked": true,
    "id": "88c5fb33a01c57803102d3b80b4664668c5d725f",
    "semantic_title": "a study of modeling rising intonation in cantonese neural speech synthesis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kameoka22_interspeech.html": {
    "title": "CAUSE: Crossmodal Action Unit Sequence Estimation from Speech",
    "volume": "main",
    "abstract": "This paper proposes a task and method for estimating a sequence of facial action units (AUs) solely from speech. AUs were introduced in the facial action coding system to objectively describe facial muscle activations. Our motivation is that AUs can be useful continuous quantities for representing speaker's subtle emotional states, attitudes, and moods in a variety of applications such as expressive speech synthesis and emotional voice conversion. We hypothesize that the information about the speaker's facial muscle movements is expressed in the generated speech and can somehow be predicted from speech alone. To verify this, we devise a neural network model that predicts an AU sequence from the mel-spectrogram of input speech and train it using a large-scale audio-visual dataset consisting of many speaking face-tracks. We call our method and model ``crossmodal AU sequence estimation/estimator (CAUSE)''. We implemented several of the most basic architectures for CAUSE, and quantitatively confirmed that the fully convolutional architecture performed best. Furthermore, by combining CAUSE with an AU-conditioned image-to-image translation method, we implemented a system that animates a given still face image from speech. Using this system, we confirmed the potential usefulness of AUs as a representation of non-linguistic features via subjective evaluations",
    "checked": true,
    "id": "eabb83f97f7ae4a7ea7125906cf66d2dcce31aca",
    "semantic_title": "cause: crossmodal action unit sequence estimation from speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/abeysinghe22_interspeech.html": {
    "title": "Visualising Model Training via Vowel Space for Text-To-Speech Systems",
    "volume": "main",
    "abstract": "With the recent developments in speech synthesis via machine learning, this study explores incorporating linguistics knowledge to visualise and evaluate synthetic speech model training. If changes to the first and second formant (in turn, the vowel space) can be seen and heard in synthetic speech, this knowledge can inform speech synthesis technology developers. A speech synthesis model trained on a large General American English database was fine-tuned into a New Zealand English voice to identify if the changes in the vowel space of synthetic speech could be seen and heard. The vowel spaces at different intervals during the fine-tuning were analysed to determine if the model learned the New Zealand English vowel space. Our findings based on vowel space analysis show that we can visualise how a speech synthesis model learns the vowel space of the database it is trained on. Perception tests confirmed that humans could perceive when a speech synthesis model has learned characteristics of the speech database it is training on. Using the vowel space as an intermediary evaluation helps understand what sounds are to be added to the training database and build speech synthesis models based on linguistics knowledge",
    "checked": true,
    "id": "7f77e0a9a08114ac150d11c180b1c260dcd5d14b",
    "semantic_title": "visualising model training via vowel space for text-to-speech systems",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeed22_interspeech.html": {
    "title": "Binary Early-Exit Network for Adaptive Inference on Low-Resource Devices",
    "volume": "main",
    "abstract": "Deep neural networks have significantly improved performance on a range of tasks with the increasing demand for computational resources, leaving deployment on low-resource devices (with limited memory and battery power) infeasible. Binary neural networks (BNNs) tackle the issue to an extent with extreme compression and speed-up gains compared to real-valued models. We propose a simple but effective method to accelerate inference through unifying BNNs with an early-exiting strategy. Our approach allows simple instances to exit early based on a decision threshold and utilizes output layers added to different intermediate layers to avoid executing the entire binary model. We extensively evaluate our method on three audio classification tasks and across four BNNs architectures. Our method demonstrates favorable quality-efficiency trade-offs while being controllable with an entropy-based threshold specified by the system user. It also results in better speed-ups (latency less than 6ms) with a single model based on existing BNN architectures without retraining for different efficiency levels. It also provides a straightforward way to estimate sample difficulty and better understanding of uncertainty around certain classes within the dataset",
    "checked": true,
    "id": "3fd99a934a2151d2c2744f0f9895723312e6f8cf",
    "semantic_title": "binary early-exit network for adaptive inference on low-resource devices",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kanda22b_interspeech.html": {
    "title": "Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings",
    "volume": "main",
    "abstract": "This paper presents a streaming speaker-attributed automatic speech recognition (SA-ASR) model that can recognize ``who spoke what'' with low latency even when multiple people are speaking simultaneously. Our model is based on token-level serialized output training (t-SOT) which was recently proposed to transcribe multi-talker speech in a streaming fashion. To further recognize speaker identities, we propose an encoder-decoder based speaker embedding extractor that can estimate a speaker representation for each recognized token not only from non-overlapping speech but also from overlapping speech. The proposed speaker embedding, named t-vector, is extracted synchronously with the t-SOT ASR model, enabling joint execution of speaker identification (SID) or speaker diarization (SD) with the multi-talker transcription with low latency. We evaluate the proposed model for a joint task of ASR and SID/SD by using LibriSpeechMix and LibriCSS corpora. The proposed model achieves substantially better accuracy than a prior streaming model and shows comparable or sometimes even superior results to the state-of-the-art offline SA-ASR model",
    "checked": true,
    "id": "f5345ca39fd5b86a438d856683bda819a37718b3",
    "semantic_title": "streaming speaker-attributed asr with token-level speaker embeddings",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/makishima22_interspeech.html": {
    "title": "Speaker consistency loss and step-wise optimization for semi-supervised joint training of TTS and ASR using unpaired text data",
    "volume": "main",
    "abstract": "In this paper, we investigate the semi-supervised joint training of text to speech (TTS) and automatic speech recognition (ASR), where a small amount of paired data and a large amount of unpaired text data are available. Conventional studies form a cycle called the TTS-ASR pipeline, where the multi-speaker TTS model synthesizes speech from text with a reference speech and the ASR model reconstructs the text from the synthesized speech, after which both models are trained with a cycle-consistency loss. However, the synthesized speech does not reflect the speaker characteristics of the reference speech and the synthesized speech becomes overly easy for the ASR model to recognize after training. This not only decreases the TTS model quality but also limits the ASR model improvement. To solve this problem, we propose improving the cycle-consistency-based training with a speaker consistency loss and step-wise optimization. The speaker consistency loss brings the speaker characteristics of the synthesized speech closer to that of the reference speech. In the step-wise optimization, we first freeze the parameter of the TTS model before both models are trained to avoid over-adaptation of the TTS model to the ASR model. Experimental results demonstrate the efficacy of the proposed method",
    "checked": true,
    "id": "bf261b2a17ccb296a56966bc333d0951b0c613f9",
    "semantic_title": "speaker consistency loss and step-wise optimization for semi-supervised joint training of tts and asr using unpaired text data",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22k_interspeech.html": {
    "title": "Audio-Visual Generalized Few-Shot Learning with Prototype-Based Co-Adaptation",
    "volume": "main",
    "abstract": "Although deep learning-based audio-visual speech recognition (AVSR) systems recognize base closed-set categories well, extending their discerning ability to additional novel categories with limited labeled training data is challenging since the model easily over-fits. In this paper, we propose Prototype-based Co-Adaptation with Transformer (Proto-CAT), a multi-modal generalized few-shot learning (GFSL) method for AVSR systems. In other words, Proto-CAT learns to recognize a novel class multi-modal object with few-shot training data, while maintaining its ability on those base closed-set categories. The main idea is to transform the prototypes (i.e., class centers) by incorporating cross-modality complementary information and calibrating cross-category semantic differences. In particular, Proto-CAT co-adapts the embeddings from audio-visual and category levels, so that it generalizes its predictions on all categories dynamically. Proto-CAT achieves state-of-the-art performance on various AVSR-GFSL benchmarks. The code is available at https://github.com/ZhangYikaii/Proto-CAT",
    "checked": true,
    "id": "b5ec154fdf4ce07ed96a6a1b513ae081029dedf6",
    "semantic_title": "audio-visual generalized few-shot learning with prototype-based co-adaptation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jia22_interspeech.html": {
    "title": "Federated Domain Adaptation for ASR with Full Self-Supervision",
    "volume": "main",
    "abstract": "Cross-device federated learning (FL) protects user privacy by collaboratively training a model on user devices, therefore eliminating the need for collecting, storing, and manually labeling user data. Previous works have considered cross-device FL for automatic speech recognition (ASR), however, there are a few important challenges that havenot been fully addressed. These include the lack of ground-truth ASR transcriptions, and the scarcity of compute resource and network bandwidth on edge devices. In this paper, we address these two challenges. First, we propose a federated learning system to support on-device ASR adaptation with full self-supervision, which uses self-labeling together with data augmentation and filtering techniques. The proposed system can improve a strong Emformer-Transducer based ASR model pretrained on out-of-domain data, using in-domain audios without any ground-truth transcriptions. Second, to reduce the training cost, we propose a self-restricted RNN Transducer (SR-RNN-T) loss, a new variant of alignment-restricted RNN-T that uses Viterbi forced-alignment from self-supervision. To further reduce the compute and network cost, we systematically explore adapting only a subset of weights in the Emformer-Transducer. Our best training recipe achieves a 12.9% relative WER reduction over the strong out-of-domain baseline, which equals 70% of the reduction achievable with full human supervision and centralized training",
    "checked": true,
    "id": "36f64815402971b40ce82e7c59f08d0eb8d274cd",
    "semantic_title": "federated domain adaptation for asr with full self-supervision",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22k_interspeech.html": {
    "title": "Augmented Adversarial Self-Supervised Learning for Early-Stage Alzheimer's Speech Detection",
    "volume": "main",
    "abstract": "The early-stage detection of Alzheimer's disease has been considered an important field of medical studies. While speech-based automatic detection methods have raised attention in the community, traditional machine learning methods suffer from data shortage because Alzheimer's record data is very difficult to get from medical institutions. To address this problem, this study proposes an augmented adversarial self-supervised learning method for Alzheimer's disease detection using limited speech data. In our approach, Alzheimer-like patterns are captured through an augmented adversarial self-supervised framework, which is trained in an adversarial manner using limited Alzheimer's data with a large scale of easily-collected normal speech data and an augmented set of Alzheimer's data. Experimental results show that our model can effectively handle the data sparsity problems and outperform the several baselines by a large margin. The performance for the ``AD\" class has been improved significantly, which is very important to actual AD detection applications",
    "checked": true,
    "id": "c52463cf1f92f22a149f99144efac7cb28b26de5",
    "semantic_title": "augmented adversarial self-supervised learning for early-stage alzheimer's speech detection",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kons22_interspeech.html": {
    "title": "Extending RNN-T-based speech recognition systems with emotion and language classification",
    "volume": "main",
    "abstract": "Speech transcription, emotion recognition, and language identification are usually considered to be three different tasks. Each one requires a different model with a different architecture and training process. We propose using a recurrent neural network transducer (RNN-T)-based speech-to-text (STT) system as a common component that can be used for emotion recognition and language identification as well as for speech recognition. Our work extends the STT system for emotion classification through minimal changes, and shows successful results on the IEMOCAP and MELD datasets. In addition, we demonstrate that by adding a lightweight component to the RNN-T module, it can also be used for language identification. In our evaluations, this new classifier demonstrates state-of-the-art accuracy for the NIST-LRE-07 dataset",
    "checked": true,
    "id": "6761cdf6e845a4b66e3fd33f082b9b97076a11c2",
    "semantic_title": "extending rnn-t-based speech recognition systems with emotion and language classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/antonova22_interspeech.html": {
    "title": "Thutmose Tagger: Single-pass neural model for Inverse Text Normalization",
    "volume": "main",
    "abstract": "Inverse text normalization (ITN) is an essential post-processing step in automatic speech recognition (ASR). It converts numbers, dates, abbreviations, and other semiotic classes from the spoken form generated by ASR to their written forms. One can consider ITN as a Machine Translation task and use neural sequence-to-sequence models to solve it. Unfortunately, such neural models are prone to hallucinations that could lead to unacceptable errors. To mitigate this issue, we propose a single-pass token classifier model that regards ITN as a tagging task. The model assigns a replacement fragment to every input token or marks it for deletion or copying without changes. We present a method of dataset preparation, based on granular alignment of ITN examples. The proposed model is less prone to hallucination errors. The model is trained on the Google Text Normalization dataset and achieves state-of-the-art sentence accuracy on both English and Russian test sets. One-to-one correspondence between tags and input words improves the interpretability of the model's predictions, simplifies debugging, and allows for post-processing corrections. The model is simpler than sequence-to-sequence models and easier to optimize in production settings. The model and the code to prepare the dataset is published as part of NeMo project",
    "checked": true,
    "id": "03ec7f75a2c7cb644ec88949e98ba55316fe79c3",
    "semantic_title": "thutmose tagger: single-pass neural model for inverse text normalization",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cho22b_interspeech.html": {
    "title": "Leveraging Prosody for Punctuation Prediction of Spontaneous Speech",
    "volume": "main",
    "abstract": "This paper introduces a new neural model for punctuation prediction that incorporates prosodic features to improve automatic punctuation prediction in transcriptions of spontaneous speech. We explore the benefit of intonation and energy features over simply using pauses. In addition, the work poses the question of how to represent interruption points associated with disfluencies in spontaneous speech. In experiments on the Switchboard corpus, we find that prosodic information improved punctuation prediction fidelity for both hand transcripts and ASR output. Explicit modeling of interruption points can benefit prediction of standard punctuation, particularly if the convention associates interruptions with commas",
    "checked": true,
    "id": "1bb965584a1e9ab0754c1e3a487ee6c7b636c734",
    "semantic_title": "leveraging prosody for punctuation prediction of spontaneous speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yu22b_interspeech.html": {
    "title": "A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings",
    "volume": "main",
    "abstract": "In this paper, we conduct a comparative study on speaker-attributed automatic speech recognition (SA-ASR) in the multi-party meeting scenario, a topic with increasing attention in meeting rich transcription. Specifically, three approaches are evaluated in this study. The first approach, FD-SOT, consists of a frame-level diarization model to identify speakers and a multi-talker ASR to recognize utterances. The speaker-attributed transcriptions are obtained by aligning the diarization results and recognized hypotheses. However, such an alignment strategy may suffer from erroneous timestamps due to the modular independence, severely hindering the model performance. Therefore, we propose the second approach, WD-SOT, to address alignment errors by introducing a word-level diarization model, which can get rid of such timestamp alignment dependency. To further mitigate the alignment issues, we propose the third approach, TS-ASR, which trains a target-speaker separation module and an ASR module jointly. By comparing various strategies for each SA-ASR approach, experimental results on a real meeting scenario corpus, AliMeeting, reveal that the WD-SOT approach achieves 10.7% relative reduction on averaged speaker-dependent character error rate (SD-CER), compared with the FD-SOT approach. In addition, the TS-ASR approach also outperforms the FD-SOT approach and brings 16.5% relative average SD-CER reduction",
    "checked": true,
    "id": "29ea9c4243563dc1a04b82ead8fc8ea0c35dd207",
    "semantic_title": "a comparative study on speaker-attributed automatic speech recognition in multi-party meetings",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guan22_interspeech.html": {
    "title": "TMGAN-PLC: Audio Packet Loss Concealment using Temporal Memory Generative Adversarial Network",
    "volume": "main",
    "abstract": "Real-time communications in packet-switched networks have become widely used in daily communication, while they inevitably suffer from network delays and data losses in constrained real-time conditions. To solve these problems, audio packet loss concealment (PLC) algorithms have been developed to mitigate voice transmission failures by reconstructing the lost information. Limited by the transmission latency and device memory, it is still intractable for PLC to accomplish high-quality voice reconstruction using a relatively small packet buffer. In this paper, we propose a temporal memory generative adversarial network for audio PLC, dubbed TMGAN-PLC, which is comprised of a novel nested-UNet generator and the time-domain/frequency-domain discriminators. Specifically, a combination of the nested-UNet and temporal feature-wise linear modulation is elaborately devised in the generator to finely adjust the intra-frame information and establish inter-frame temporal dependencies. To complement the missing speech content caused by longer loss bursts, we employ multi-stage gated vector quantizers to capture the correct content and reconstruct the near-real smooth audio. Extensive experiments on the PLC Challenge dataset demonstrate that the proposed method yields promising performance in terms of speech quality, intelligibility, and PLCMOS",
    "checked": true,
    "id": "df07a6a366bc83ae4b8b46150c9ef2305e578701",
    "semantic_title": "tmgan-plc: audio packet loss concealment using temporal memory generative adversarial network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/valin22_interspeech.html": {
    "title": "Real-Time Packet Loss Concealment With Mixed Generative and Predictive Model",
    "volume": "main",
    "abstract": "As deep speech enhancement algorithms have recently demonstrated capabilities greatly surpassing their traditional counterparts for suppressing noise, reverberation and echo, attention is turning to the problem of packet loss concealment (PLC). PLC is a challenging task because it not only involves real-time speech synthesis, but also frequent transitions between the received audio and the synthesized concealment. We propose a hybrid neural PLC architecture where the missing speech is synthesized using a generative model conditioned using a predictive model. The resulting algorithm achieves natural concealment that surpasses the quality of existing conventional PLC algorithms and ranked second in the Interspeech 2022 PLC Challenge. We show that our solution not only works for uncompressed audio, but is also applicable to a modern speech codec",
    "checked": true,
    "id": "81ce23f3fe1f54e1f283f8f96b7efaaf59899134",
    "semantic_title": "real-time packet loss concealment with mixed generative and predictive model",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22s_interspeech.html": {
    "title": "PLCNet: Real-time Packet Loss Concealment with Semi-supervised Generative Adversarial Network",
    "volume": "main",
    "abstract": "Packet loss is one of the main reasons for speech quality degradation in voice over internet phone (VOIP) calls. However, the existing packet loss concealment (PLC) algorithms are hard to generate high-quality speech signal while maintaining low computational complexity. In this paper, a causal wave-to-wave non-autoregressive lightweight PLC model (PLCNet) is proposed, which can do real-time streaming process with low latency. In addition, we introduce multiple multi-resolution discriminators and semi-supervised training strategy to improve the ability of the encoder part to extract global features while enabling the decoder part to accurately reconstruct waveforms where packets are lost. Contrary to autoregressive model, PLCNet can guarantee the smoothness and continuity of the speech phase before and after packet loss without any smoothing operations. Experimental results show that PLCNet achieves significant improvements in perceptual quality and intelligibility over three classical PLC methods and three state-of-the-art deep PLC methods. In the INTERSPEECH 2022 PLC Challenge, our approach has ranked the 3rd place on PLCMOS (3.829) and the 3rd place on the final score (0.798)",
    "checked": true,
    "id": "036617b6a60faa2f62ee3c6ec78658ddf016dc45",
    "semantic_title": "plcnet: real-time packet loss concealment with semi-supervised generative adversarial network",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/diener22_interspeech.html": {
    "title": "INTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge",
    "volume": "main",
    "abstract": "Audio Packet Loss Concealment (PLC) is the hiding of gaps in audio streams caused by data transmission failures in packet switched networks. This is a common problem, and of increasing importance as end-to-end VoIP telephony and teleconference systems become the default and ever more widely used form of communication in business as well as in personal usage. This paper presents the INTERSPEECH 2022 Audio Deep Packet Loss Concealment challenge. We first give an overview of the PLC problem, and introduce some classical approaches to PLC as well as recent work. We then present the open source dataset released as part of this challenge as well as the evaluation methods and metrics used to determine the winner. We also briefly introduce PLCMOS, a novel data-driven metric that can be used to quickly evaluate the performance PLC systems. Finally, we present the results of the INTERSPEECH 2022 Audio Deep PLC Challenge, and provide a summary of important takeaways",
    "checked": true,
    "id": "626a2abde4ce59eb1a39812a167ac7c15afe73ff",
    "semantic_title": "interspeech 2022 audio deep packet loss concealment challenge",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22ea_interspeech.html": {
    "title": "End-to-End Multi-Loss Training for Low Delay Packet Loss Concealment",
    "volume": "main",
    "abstract": "Real-time teleconferencing has become one of the essential parts in our daily life. While packet loss during real-time data transmission is unavoidable, traditional signal processing based Packet Loss Concealment (PLC) techniques have been developed in recent decades. In recent years, deep learning based approaches have also proposed and achieved state-of-the-art PLC performance. This work presents a low-delay multi-loss based neural PLC system. The multi-loss is consisted by a signal loss, a perceptual loss and an ASR loss ensuring good speech quality and automatic speech recognition compatibility. The proposed system was ranked 1st place in INTERSPEECH 2022's Audio Deep Packet Loss Concealment Challenge",
    "checked": true,
    "id": "88a2d947f8539c72bd5d2d200e9bd073ab6c7b86",
    "semantic_title": "end-to-end multi-loss training for low delay packet loss concealment",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22b_interspeech.html": {
    "title": "Extended U-Net for Speaker Verification in Noisy Environments",
    "volume": "main",
    "abstract": "Background noise is a well-known factor that deteriorates the accuracy and reliability of speaker verification (SV) systems by blurring speech intelligibility. Various studies have used separate pretrained enhancement models as the front-end module of the SV system in noisy environments, and these methods effectively remove noises. However, the denoising process of independent enhancement models not tailored to the SV task can also distort the speaker information included in utterances. We argue that the enhancement network and speaker embedding extractor should be fully jointly trained for SV tasks under noisy conditions to alleviate this issue. Therefore, we proposed a U-Net-based integrated framework that simultaneously optimizes speaker identification and feature enhancement losses. Moreover, we analyzed the structural limitations of using U-Net directly for noise SV tasks and further proposed Extended U-Net to reduce these drawbacks. We evaluated the models on the noise-synthesized VoxCeleb1 test set and VOiCES development set recorded in various noisy scenarios. The experimental results demonstrate that the U-Net-based fully joint training framework is more effective than the baseline, and the extended U-Net exhibited state-of-the-art performance versus the recently proposed compensation systems",
    "checked": true,
    "id": "455a8c1a9890243455f7a087c4cf45120bea5108",
    "semantic_title": "extended u-net for speaker verification in noisy environments",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22j_interspeech.html": {
    "title": "Domain Agnostic Few-shot Learning for Speaker Verification",
    "volume": "main",
    "abstract": "Deep learning models for verification systems often fail to generalize to new users and new environments, even though they learn highly discriminative features. To address this problem, we propose a few-shot domain generalization framework that learns to tackle distribution shift for new users and new domains. Our framework consists of domain-specific and domain-aggregation networks, which are the experts on specific and combined domains, respectively. By using these networks, we generate episodes that mimic the presence of both novel users and novel domains in the training phase to eventually produce better generalization. To save memory, we reduce the number of domain-specific networks by clustering similar domains together. Upon extensive evaluation on artificially generated noise domains, we can explicitly show generalization ability of our framework. In addition, we apply our proposed methods to the existing competitive architecture on the standard benchmark, which shows further performance improvements",
    "checked": true,
    "id": "bc2ceeea0ca9a5042d2c7da3d156bd20ee51b71d",
    "semantic_title": "domain agnostic few-shot learning for speaker verification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22r_interspeech.html": {
    "title": "Scoring of Large-Margin Embeddings for Speaker Verification: Cosine or PLDA?",
    "volume": "main",
    "abstract": "The emergence of large-margin softmax cross-entropy losses in training deep speaker embedding neural networks has triggered a gradual shift from parametric back-ends to a simpler cosine similarity measure for speaker verification. Popular parametric back-ends include the probabilistic linear discriminant analysis (PLDA) and its variants. This paper investigates the properties of margin-based cross-entropy losses leading to such a shift and aims to find scoring back-ends best suited for speaker verification. In addition, we revisit the pre-processing techniques which have been widely used in the past and assess their effectiveness on large-margin embeddings. Experiments on the state-of-the-art ECAPA-TDNN networks trained with various large-margin softmax cross-entropy losses show a substantial increment in intra-speaker compactness making the conventional PLDA superfluous. In this regard, we found that constraining the within-speaker covariance matrix could improve the performance of the PLDA. It is demonstrated through a series of experiments on the VoxCeleb-1 and SITW core-core test sets with 40.8% equal error rate (EER) reduction and 35.1% minimum detection cost (minDCF) reduction. It also outperforms cosine scoring consistently with reductions in EER and minDCF by 10.9% and 4.9%, respectively",
    "checked": true,
    "id": "a653a2fa51a8847511be8e5aa07d2774826a0f1c",
    "semantic_title": "scoring of large-margin embeddings for speaker verification: cosine or plda?",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/stafylakis22_interspeech.html": {
    "title": "Training speaker embedding extractors using multi-speaker audio with unknown speaker boundaries",
    "volume": "main",
    "abstract": "In this paper, we demonstrate a method for training speaker embedding extractors using weak annotation. More specifically, we are using the full VoxCeleb recordings and the name of the celebrities appearing on each video without knowledge of the time intervals the celebrities appear in the video. We show that by combining a baseline speaker diarization algorithm that requires no training or parameter tuning, a modified loss with aggregation over segments, and a two-stage training approach, we are able to train a competitive ResNet-based embedding extractor. Finally, we experiment with two different aggregation functions and analyze their behaviour in terms of their gradients",
    "checked": true,
    "id": "227088f40b8c5c08f4c6518718366f11f5d4f718",
    "semantic_title": "training speaker embedding extractors using multi-speaker audio with unknown speaker boundaries",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luu22_interspeech.html": {
    "title": "Investigating the contribution of speaker attributes to speaker separability using disentangled speaker representations",
    "volume": "main",
    "abstract": "Deep speaker embeddings have been shown to encode a wide variety of attributes relating to a speaker. The aim of this work is to separate out some of these attributes in the embedding space, disentangling these sources of speaker variation into subsets of the embedding dimensions. This is achieved modifying the training procedure of a typical speaker embedding network, which is typically only trained to classify speakers. This work instead adds pairs of attribute specific task heads to operate on complementary subsets of the speaker embedding dimensions. While specific dimensions are encouraged to encode an attribute, for example gender, the other dimensions are penalized for containing this information using an adversarial loss. We show that this method is effective in factorizing out multiple attributes in the embedding space, successfully disentangling gender, nationality and age. Using the disentangled representations, we investigate how much removing this information impacts speaker verification and diarization performance, showing that gender is a significant source of separation in the deep speaker embedding space, with nationality and age also contributing to a lesser degree",
    "checked": true,
    "id": "8236cbb75a50f1bad05cb7d50d156526046b6b4d",
    "semantic_title": "investigating the contribution of speaker attributes to speaker separability using disentangled speaker representations",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kataria22_interspeech.html": {
    "title": "Joint domain adaptation and speech bandwidth extension using time-domain GANs for speaker verification",
    "volume": "main",
    "abstract": "Speech systems developed for a particular choice of acoustic domain and sampling frequency do not translate easily to others. The usual practice is to learn domain adaptation and bandwidth extension models independently. Contrary to this, we propose to learn both tasks together. Particularly, we learn to map narrowband conversational telephone speech to wideband microphone speech. We developed parallel and non-parallel learning solutions which utilize both paired and unpaired data. We first discuss joint and disjoint training of multiple generative models for our tasks. Then, we propose a two-stage learning solution using a pre-trained domain adaptation system for pre-processing in bandwidth extension training. We evaluated our schemes on a Speaker Verification downstream task. We used the JHU-MIT experimental setup for NIST SRE21, which comprises SRE16, SRE-CTS Superset, and SRE21. Our results prove that learning both tasks is better than learning just one. On SRE16, our best system achieves 22% relative improvement in Equal Error Rate w.r.t. a direct learning baseline and 8% w.r.t. a strong bandwidth expansion system",
    "checked": true,
    "id": "d58ebbc34e8ea987da5dda1bb132823b3e9105d3",
    "semantic_title": "joint domain adaptation and speech bandwidth extension using time-domain gans for speaker verification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoshinaga22_interspeech.html": {
    "title": "Variability in Production of Non-Sibilant Fricative [] in /hi/",
    "volume": "main",
    "abstract": "The alveolo-palatal sibilant fricative [] in /si/ and palatal non-sibilant fricative [] in /hi/ are known to be distinguished by the geometrical change of the vocal tract only in the coronal direction and acoustically unstable in some Japanese speakers. In this study, we reconstructed the vocal tract geometry with three repetitive magnetic resonance imaging (MRI) on the same subject and investigated the effects of coronal vocal tract shapes on the airflow and sound characteristics in []. The computational grids were constructed on each vocal tract and a numerical airflow simulation was conducted to predict the turbulent airflow and aeroacoustic sound generation. The predicted sound properties were in good agreement with the measurement of Japanese subjects. The comparison of the airflow and acoustic characteristics among three vocal tracts showed that the slight changes of the constriction area and flow channel downstream from the constriction influenced the sound source generation and peak amplitudes at around 3 kHz, indicating that the characteristic peak of [] was variable due to the constriction shape at the middle part of the hard palate",
    "checked": true,
    "id": "803fa3bd9d58b49f939b5499dbec456f8156c2f1",
    "semantic_title": "variability in production of non-sibilant fricative [] in /hi/",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/udupa22_interspeech.html": {
    "title": "Streaming model for Acoustic to Articulatory Inversion with transformer networks",
    "volume": "main",
    "abstract": "Estimating speech articulatory movements from speech acoustics is known as Acoustic to Articulatory Inversion (AAI). Recently, transformer-based AAI models have been shown to achieve state-of-art performance. However, in transformer networks, the attention is applied over the whole utterance, thereby needing to obtain the full utterance before the inference, which leads to high latency and is impractical for streaming AAI. To enable streaming during inference, evaluation could be performed on non-overlapping chucks instead of full utterance. However, due to a mismatch of attention receptive field during training and evaluation, there could be a drop in AAI performance. To overcome this scenario, in this work we perform experiments with different attention masks and use context from previous predictions during training. Experiments results revealed that using the random start mask attention with the context from previous predictions of transformer encoder performs better than the baseline results",
    "checked": true,
    "id": "4f892f88ba61212c10501d00fcda98b3a30ac7ef",
    "semantic_title": "streaming model for acoustic to articulatory inversion with transformer networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rakotomalala22_interspeech.html": {
    "title": "Trajectories predicted by optimal speech motor control using LSTM networks",
    "volume": "main",
    "abstract": "The question of optimality and its role in trajectory formation is at the core of important debates in motor control research. We present the first speech control model that associates Optimal Feedback Control (OFC) for planning and execution of movements with a biomechanical model of the vocal tract. Simulated trajectories in the VCV sequences are compared with trajectories generated using the GEPPETO model that drives the same 2D biomechanical model; in GEPPETO, the scope of optimality is limited to movement planning and to phoneme-related target motor commands. In our OFC model commands are estimated via the minimisation of a cost that combines neuromuscular effort, and a penalty on accuracy of the auditory patterns reached for the phonemes. The biomechanics of the plant are implemented by an LSTM trained on simulations of a finite element model of the tongue. The comparison of the OFC model with GEPPETO relies on the time variation of the motor commands, the shape of the articulatory trajectories, and on auditory trajectories in the F1-F2 planes",
    "checked": true,
    "id": "354329f7f8f298476ce4f04632fbbcc8ab97f25d",
    "semantic_title": "trajectories predicted by optimal speech motor control using lstm networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vanniekerk22_interspeech.html": {
    "title": "Exploration strategies for articulatory synthesis of complex syllable onsets",
    "volume": "main",
    "abstract": "High-quality articulatory speech synthesis has many potential applications in speech science and technology. However, developing appropriate mappings from linguistic specification to articulatory gestures is difficult and time consuming. In this paper we construct an optimisation-based framework as a first step towards learning these mappings without manual intervention. We demonstrate the production of CCV syllables and discuss the quality of the articulatory gestures with reference to coarticulation",
    "checked": true,
    "id": "2962f72c44cabe81a1438f5184d52e11abc57794",
    "semantic_title": "exploration strategies for articulatory synthesis of complex syllable onsets",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22m_interspeech.html": {
    "title": "Linguistic versus biological factors governing acoustic voice variation",
    "volume": "main",
    "abstract": "This study presents a cross-linguistic investigation of acoustic voice spaces in English, Seoul Korean, and White Hmong, which differ in whether they phonologically contrast phonation type and/or tone. The overarching hypothesis is that acoustic variability in voice will be shaped by biological factors, linguistic factors, and individual idiosyncrasies. By employing principal component analysis on speakers' read speech productions, we identify how individual and population voice spaces are acoustically structured for speakers of these three languages. Results revealed several factors that consistently account for acoustic variability across speakers and languages, but also factors that vary with language-specific phonology",
    "checked": true,
    "id": "a697d86f057261d827c3548eb13174947fc8cc3c",
    "semantic_title": "linguistic versus biological factors governing acoustic voice variation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nagamine22_interspeech.html": {
    "title": "Acquisition of allophonic variation in second language speech: An acoustic and articulatory study of English laterals by Japanese speakers",
    "volume": "main",
    "abstract": "Acquisition of positional allophonic variation is seen as the foundation of a successful L2 speech learning. However, previous research has mostly focused on the phonemic contrast between English /l/ and /r/, providing little evidence in the acquisition of positional allophones, such as those in English /l/. The current study investigates the acoustics and articulation of allophonic variations in English laterals produced by Japanese speakers, focusing on the effects of syllabic positions and flanking vowels. Acoustic and articulatory data were obtained from five Japanese speakers in a simultaneous audio and high-speed ultrasound tongue imaging recording set-up while they read sentences containing syllable-initial and -final tokens of English /l/ in four different vowel contexts. Acoustic analysis was conducted on 500 tokens using linear-mixed effects modelling and the articulatory data were analysed using generalised additive mixed modelling. Syllable position and vowel context had significant effects on acoustics, while midsagittal tongue shape was more influenced by vowel context, with fewer positional effects. The results demonstrate that differences in acoustics not always be mirrored exactly by midsagittal tongue shape, suggesting multidimensionality of articulation in second language speech",
    "checked": true,
    "id": "95b265cd0add05966907267bb3d891f6db42b183",
    "semantic_title": "acquisition of allophonic variation in second language speech: an acoustic and articulatory study of english laterals by japanese speakers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/manocha22b_interspeech.html": {
    "title": "SAQAM: Spatial Audio Quality Assessment Metric",
    "volume": "main",
    "abstract": "Audio quality assessment is critical for assessing the perceptual realism of sounds. However, the time and expense of obtaining \"gold standard\" human judgments limit the availability of such data. For AR&VR, good perceived sound quality and localizability of sources are among the key elements to ensure complete immersion of the user. Our work introduces SAQAM which uses a multi-task learning framework to assess listening quality (LQ) and spatialization quality (SQ) between any given pair of binaural signals without using any subjective data. We model LQ by training on a simulated dataset of triplet human judgments, and SQ by utilizing activation-level distances from networks trained for direction of arrival (DOA) estimation. We show that SAQAM correlates well with human responses across four diverse datasets. Since it is a deep network, the metric is differentiable, making it suitable as a loss function for other tasks. For example, simply replacing an existing loss with our metric yields improvement in a speech-enhancement network",
    "checked": true,
    "id": "d0b08bf812ff3bfca6b99509f0be44db75f06592",
    "semantic_title": "saqam: spatial audio quality assessment metric",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/manocha22c_interspeech.html": {
    "title": "Speech Quality Assessment through MOS using Non-Matching References",
    "volume": "main",
    "abstract": "Human judgments obtained through Mean Opinion Scores (MOS) are the most reliable way to assess the quality of speech signals. However, several recent attempts to automatically estimate MOS using deep learning approaches lack robustness and generalization capabilities, limiting their use in real-world applications. In this work, we present a novel framework, NORESQA-MOS, for estimating the MOS of a speech signal. Unlike prior works, our approach uses non-matching references as a form of conditioning to ground the MOS estimation by neural networks. We show that NORESQA-MOS provides better generalization and more robust MOS estimation than previous state-of-the-art methods such as DNSMOS and NISQA, even though we use a smaller training set. Moreover, we also show that our generic framework can be combined with other learning methods such as self-supervised learning and can further supplement the benefits from these methods",
    "checked": true,
    "id": "65c06ccd1157c42252aac4ff2a913ed7457ada66",
    "semantic_title": "speech quality assessment through mos using non-matching references",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kawahara22_interspeech.html": {
    "title": "An objective test tool for pitch extractors' response attributes",
    "volume": "main",
    "abstract": "We propose an objective measurement method for pitch extractors' responses to frequency-modulated signals. It enables us to evaluate different pitch extractors with unified criteria. The method uses extended time-stretched pulses combined by binary orthogonal sequences. It provides simultaneous measurement results consisting of the linear and the non-linear time-invariant responses and random and time-varying responses. We tested representative pitch extractors using fundamental frequencies spanning 80~Hz to 800~Hz with 1/48 octave steps and produced more than 2000 modulation frequency response plots. We found that making scientific visualization by animating these plots enables us to understand different pitch extractors' behavior at once. Such efficient and effortless inspection is impossible by inspecting all individual plots. The proposed measurement method with visualization leads to further improvement of the performance of one of the extractors mentioned above. In other words, our procedure turns the specific pitch extractor into the best reliable measuring equipment that is crucial for scientific research. We open-sourced MATLAB codes of the proposed objective measurement method and visualization procedure",
    "checked": true,
    "id": "63b89ed948b530045852d34e2b4a85e079b0588f",
    "semantic_title": "an objective test tool for pitch extractors' response attributes",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22o_interspeech.html": {
    "title": "Data Augmentation Using McAdams-Coefficient-Based Speaker Anonymization for Fake Audio Detection",
    "volume": "main",
    "abstract": "Fake audio detection (FAD) is a technique to distinguish synthetic speech from natural speech. In most FAD systems, removing irrelevant features from acoustic speech while keeping only robust discriminative features is essential. Intuitively, speaker information entangled in acoustic speech should be suppressed for the FAD task. Particularly in a deep neural network (DNN)-based FAD system, the learning system may learn speaker information from a training dataset and cannot generalize well on a testing dataset. In this paper, we propose to use the speaker anonymization (SA) technique to suppress speaker information from acoustic speech before inputting it into a DNN-based FAD system. We adopted the McAdams-coefficient-based SA (MC-SA) algorithm, and this is expected that the entangled speaker information will not be involved in the DNN-based FAD learning. Based on this idea, we implemented a light convolutional neural network bidirectional long short-term memory (LCNN-BLSTM)-based FAD system and conducted experiments on the Audio Deep Synthesis Detection Challenge (ADD2022) datasets. The results showed that removing the speaker information from acoustic speech improved the relative performance in the first track of ADD2022 by 17.66%",
    "checked": true,
    "id": "e148912c4b4ed28f594d859a74d8a55591f61863",
    "semantic_title": "data augmentation using mcadams-coefficient-based speaker anonymization for fake audio detection",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zaiem22_interspeech.html": {
    "title": "Automatic Data Augmentation Selection and Parametrization in Contrastive Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Contrastive learning enables learning useful audio and speech representations without ground-truth labels by maximizing the similarity between latent representations of similar signal segments. In this framework various data augmentation techniques are usually exploited to help enforce desired invariances within the learned representations, improving performance on various audio tasks thanks to more robust embeddings. Now, selecting the most relevant augmentations has proven crucial for better downstream performances. Thus, this work introduces a conditional independance-based method which allows for automatically selecting a suitable distribution on the choice of augmentations and their parametrization from a set of predefined ones, for contrastive self-supervised pre-training. This is performed with respect to a downstream task of interest, hence saving a costly hyper-parameter search. Experiments performed on two different downstream tasks validate the proposed approach showing better results than experimenting without augmentation or with baseline augmentations. We furthermore conduct a qualitative analysis of the automatically selected augmentations and their variation according to the considered final downstream dataset",
    "checked": true,
    "id": "7bcd50b75b84552b9e5d806efe376651132d78c1",
    "semantic_title": "automatic data augmentation selection and parametrization in contrastive self-supervised speech representation learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mumtaz22_interspeech.html": {
    "title": "Transformer-based quality assessment model for generalized user-generated multimedia audio content",
    "volume": "main",
    "abstract": "In this paper, we propose a computational measure for the quality of audio in user-generated multimedia (UGM) in accordance with the human perceptual system. To this end, we first extend the previously proposed IIT-JMU-UGM Audio dataset by including samples with more diverse context, content, distortion types, and intensities, along with implicitly distorted audio that reflect realistic scenarios. We conduct subjective testing on the extended database containing 2075 audio clips to obtain the mean opinion scores for each sample. We then introduce transformer-based learning to the domain of audio quality assessment, which is trained on three vital audio features: Mel-frequency cepstral coefficients, chroma, and Mel-scaled spectrogram. The proposed non-intrusive transformer-based model is compared against state-of-the-art methods and found to outperform Simple RNN, LSTM, and GRU models by over 4%. The database and the source code will be made public upon acceptance",
    "checked": true,
    "id": "4dc20f41528f406dbbf077f8943ec0e4adb09cde",
    "semantic_title": "transformer-based quality assessment model for generalized user-generated multimedia audio content",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vangysel22_interspeech.html": {
    "title": "Space-Efficient Representation of Entity-centric Query Language Models",
    "volume": "main",
    "abstract": "Virtual assistants make use of automatic speech recognition (ASR) to help users answer entity-centric queries. However, spoken entity recognition is a difficult problem, due to the large number of frequently-changing named entities. In addition, resources available for recognition are constrained when ASR is performed on-device. In this work, we investigate the use of probabilistic grammars as language models within the finite-state transducer (FST) framework. We introduce a deterministic approximation to probabilistic grammars that avoids the explicit expansion of non-terminals at model creation time, integrates directly with the FST framework, and is complementary to n-gram models. We obtain a 10% relative word error rate improvement on long tail entity queries compared to when a similarly-sized n-gram model is used without our method",
    "checked": true,
    "id": "6ab7125907039eaeb24090bc6d97140f1953c66a",
    "semantic_title": "space-efficient representation of entity-centric query language models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dingliwal22_interspeech.html": {
    "title": "Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) systems have found their use in numerous industrial applications in very diverse domains creating a need to adapt to new domains with small memory and deployment overhead. In this work, we introduce domain-prompts, a methodology that involves training a small number of domain embedding parameters to prime a Transformer-based Language Model (LM) to a particular domain. Using this domain-adapted LM for rescoring ASR hypotheses can achieve 7-13% WER reduction for a new domain with just 1000 unlabeled textual domain-specific sentences. This improvement is comparable or even better than fully fine-tuned models even though just 0.02% of the parameters of the base LM are updated. Additionally, our method is deployment-friendly as the learnt domain embeddings are prefixed to the input to the model rather than changing the base model architecture. Therefore, our method is an ideal choice for on-the-fly adaptation of LMs used in ASR systems to progressively scale it to new domains",
    "checked": true,
    "id": "f7f3c01b8ccc0841ea1009198aa6d274c68c9b8b",
    "semantic_title": "domain prompts: towards memory and compute efficient domain adaptation of asr systems",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22j_interspeech.html": {
    "title": "Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition",
    "volume": "main",
    "abstract": "Language model fusion helps smart assistants recognize words which are rare in acoustic data but abundant in text-only corpora (typed search logs). However, such corpora have properties that hinder downstream performance, including being (1) too large, (2) beset with domain-mismatched content, and (3) heavy-headed rather than heavy-tailed (excessively many duplicate search queries such as \"weather\"). We show that three simple strategies for selecting language modeling data can dramatically improve rare-word recognition without harming overall performance. First, to address the heavy-headedness, we downsample the data according to a soft log function, which tunably reduces high frequency (head) sentences. Second, to encourage rare-word exposure, we explicitly filter for words rare in the acoustic data. Finally, we tackle domain-mismatch via perplexity-based contrastive selection, filtering for examples matched to the target domain. We down-select a large corpus of web search queries by a factor of 53x and achieve better LM perplexities than without down-selection. When shallow-fused with a state-of-the-art, production speech engine, our LM achieves WER reductions of up to 24\\% relative on rare-word sentences (without changing overall WER) compared to a baseline LM trained on the raw corpus. These gains are further validated through favorable side-by-side evaluations on live voice search traffic",
    "checked": true,
    "id": "3f7aa16227677d6d76893b6d4c64276addfa1b8c",
    "semantic_title": "sentence-select: large-scale language model data selection for rare-word speech recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/breiner22_interspeech.html": {
    "title": "UserLibri: A Dataset for ASR Personalization Using Only Text",
    "volume": "main",
    "abstract": "Personalization of speech models on mobile devices (on-device personalization) is an active area of research, but more often than not, mobile devices have more text-only data than paired audio-text data. We explore training a personalized language model on text-only data, used during inference to improve speech recognition performance for that user. We experiment on a user-clustered LibriSpeech corpus, supplemented with personalized text-only data for each user from Project Gutenberg. We release this User-Specific LibriSpeech (UserLibri) dataset to aid future personalization research. LibriSpeech audio-transcript pairs are grouped into 55 users from the test-clean dataset and 52 users from test-other. We are able to lower the average word error rate per user across both sets in streaming and nonstreaming models, including an improvement of 2.5 for the harder set of test-other users when streaming",
    "checked": true,
    "id": "0beccd1692536d1aa7bb572bc32040a9d409e141",
    "semantic_title": "userlibri: a dataset for asr personalization using only text",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chien22b_interspeech.html": {
    "title": "A BERT-based Language Modeling Framework",
    "volume": "main",
    "abstract": "Deep learning has brought considerable changes and created a new paradigm in many research areas, including computer vision, speech processing, and natural language processing. In the context of language modeling, recurrent-based language models and word embedding methods have been pivotal studies in the past decade. Recently, pre-trained language models have attracted widespread attention due to their enormous success in many challenging tasks. However, only a dearth of works concentrates on creating novel language models based on the pre-trained models. In order to bridge the research gap, we take the bidirectional encoder representations from Transformers (BERT) model as an example to explore novel uses of a pre-trained model for language modeling. More formally, this paper proposes a set of BERT-based language models, and a neural-based dynamic adaptation method is also introduced to combine these language models systematically and methodically. We conduct comprehensive studies on three datasets for the perplexity evaluation. Experiments show that the proposed framework achieves 11%, 39%, and 5% relative improvements over the baseline model for Penn Treebank, Wikitext 2, and Tedlium Release 2 corpora, respectively. Besides, when applied to rerank n-best lists from a speech recognizer, our framework also yields promising results compared with baseline systems",
    "checked": true,
    "id": "8ccd6ea80e3fb165213b95d5ff7e71b497befc0a",
    "semantic_title": "a bert-based language modeling framework",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/masuyama22_interspeech.html": {
    "title": "Joint Optimization of Sampling Rate Offsets Based on Entire Signal Relationship Among Distributed Microphones",
    "volume": "main",
    "abstract": "In this paper, we propose to simultaneously estimate all the sampling rate offsets (SROs) of multiple devices. In a distributed microphone array, the SRO is inevitable, which deteriorates the performance of array signal processing. Most of the existing SRO estimation methods focused on synchronizing two microphones. When synchronizing more than two microphones, we select one reference microphone and estimate the SRO of each non-reference microphone independently. Hence, the relationship among signals observed by non-reference microphones is not considered. To address this problem, the proposed method jointly optimizes all SROs based on a probabilistic model of a multichannel signal. The SROs and model parameters are alternately updated to increase the log-likelihood based on an auxiliary function. The effectiveness of the proposed method is validated on mixtures of various numbers of speakers",
    "checked": true,
    "id": "99c460aa54dedea706420358084dad300ec617b4",
    "semantic_title": "joint optimization of sampling rate offsets based on entire signal relationship among distributed microphones",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ciccarelli22_interspeech.html": {
    "title": "Challenges and Opportunities in Multi-device Speech Processing",
    "volume": "main",
    "abstract": "We review current solutions and technical challenges for automatic speech recognition, keyword spotting, device arbitration, speech enhancement, and source localization in multi-device home environments to provide context for the INTERSPEECH 2022 special session, \"Challenges and opportunities for signal processing and machine learning for multiple smart devices''. We also identify the datasets needed to support these research areas. Based on the review and our research experience in the multi-device domain, we conclude with an outlook on the future evolution of multiple device signal processing and machine learning",
    "checked": true,
    "id": "b178a77420cb07e17b1ecb70585c1c76b55b1ee7",
    "semantic_title": "challenges and opportunities in multi-device speech processing",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/agaskar22_interspeech.html": {
    "title": "Practical Over-the-air Perceptual AcousticWatermarking",
    "volume": "main",
    "abstract": "In this work, we demonstrate a novel technique for automatically scaling over-the-air acoustic watermarks to maximize amplitude while remaining imperceptible to human listeners. These watermarks have been demonstrated in prior work to be robust to the indoor acoustic channel. However, they require careful calibration to ensure that they are (a) detectable by the device and (b) imperceptible to humans. While previously this was done using listening tests, we show that psychoacoustic masking curves can be used to automatically scale each watermark frame's amplitude to be as high as possible while remaining below the masking level. This maximizes watermark detectability by the self-correlation decoder described in earlier work, while ensuring that the watermark is not heard",
    "checked": true,
    "id": "19e546410c6f394e3f108af8beded9d3f1c2ecca",
    "semantic_title": "practical over-the-air perceptual acousticwatermarking",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koppelmann22_interspeech.html": {
    "title": "Clustering-based Wake Word Detection in Privacy-aware Acoustic Sensor Networks",
    "volume": "main",
    "abstract": "This work investigates privacy-aware collaborative wake word detection (WWD) in acoustic sensor networks. To meet state-of-the-art privacy constraints, the proposed WWD scheme is based on privacy-aware unsupervised clustered federated learning that groups microphone nodes w.r.t. active sound sources and on a privacy-preserving high-level feature representation. Using the partition of microphone nodes into clusters, we apply intra- and inter-cluster feature enhancement strategies directly in the privacy-preserving feature domain and thus circumvent the need for communicating privacy-sensitive information between nodes. The approach is demonstrated for an acoustic sensor network deployed in a smart-home environment. We show that the proposed collaborative WWD system clearly outperforms independent decisions of individual microphone nodes. Index Terms: privacy, wake word detection, clustering, federated learning, unsupervised clustered federated learning",
    "checked": true,
    "id": "4ba811b2dc1462cfc3f1a0ad5f7af702079f89d1",
    "semantic_title": "clustering-based wake word detection in privacy-aware acoustic sensor networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nespoli22_interspeech.html": {
    "title": "Relative Acoustic Features for Distance Estimation in Smart-Homes",
    "volume": "main",
    "abstract": "Any audio recording encapsulates the unique fingerprint of the associated acoustic environment, namely the background noise and reverberation. Considering the scenario of a room equipped with a fixed smart speaker device with one or more microphones and a wearable smart device (watch, glasses or smartphone), we employed the improved proportionate normalized least mean square adaptive filter to estimate the relative room impulse response mapping the audio recordings of the two devices. We performed inter-device distance estimation by exploiting a new set of features obtained extending the definition of some acoustic attributes of the room impulse response to its relative version. In combination with the sparseness measure of the estimated relative room impulse response, the relative features allow precise inter-device distance estimation which can be exploited for tasks such as best microphone selection or acoustic scene analysis. Experimental results from simulated rooms of different dimensions and reverberation times demonstrate the effectiveness of this computationally lightweight approach for smart home acoustic ranging applications",
    "checked": true,
    "id": "c2716a8f434fee715511eacdc1f2ffab7b21754c",
    "semantic_title": "relative acoustic features for distance estimation in smart-homes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pandey22c_interspeech.html": {
    "title": "Time-domain Ad-hoc Array Speech Enhancement Using a Triple-path Network",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) are very effective for multichannel speech enhancement with fixed array geometries. However, it is not trivial to use DNNs for ad-hoc arrays with unknown order and placement of microphones. We propose a novel triple-path network for ad-hoc array processing in the time domain. The key idea in the network design is to divide the overall processing into spatial processing and temporal processing and use self-attention for spatial processing. Using self-attention for spatial processing makes the network invariant to the order and the number of microphones. The temporal processing is done independently for all channels using a recently proposed dual-path attentive recurrent network. The proposed network is a multiple-input multiple-output architecture that can simultaneously enhance signals at all microphones. Experimental results demonstrate the excellent performance of the proposed approach. Further, we present analysis to demonstrate the effectiveness of the proposed network in utilizing multichannel information even from microphones at far locations",
    "checked": true,
    "id": "6bb3aeaa2d1641c7d86a3cccf86143a68617b4f9",
    "semantic_title": "time-domain ad-hoc array speech enhancement using a triple-path network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fietkau22_interspeech.html": {
    "title": "Relationship between the acoustic time intervals and tongue movements of German diphthongs",
    "volume": "main",
    "abstract": "This study investigated the relationship between tongue movements during the production of German diphthongs and their acoustic time intervals. To this end, five subjects produced a set of logatomes that contained German primary, secondary, and peripheral diphthongs in the context of bilabial and labiodental consonants at three different speaking rates. During the utterances, tongue movements were measured by means of optical palatography (OPG), i.e. by optical distance sensing in the oral cavity, along with the acoustic speech signal. The analysis of the movement signals revealed that the diphthongs have s-shaped tongue trajectories that strongly resemble half cosine periods. In addition, acoustic and articulatory diphthong durations have a linear, but not proportional, relationship. Finally, the peak velocity and midpoint between the two targets of a diphthong are reached in the middle of both the acoustic and articulatory diphthong time intervals, regardless of the duration and type of diphthong. These results can help to model realistic tongue movements for diphthongs in articulatory speech synthesis",
    "checked": true,
    "id": "0634ab237d3ea6ee6da371fc9c03e29ab20841c9",
    "semantic_title": "relationship between the acoustic time intervals and tongue movements of german diphthongs",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/matsui22_interspeech.html": {
    "title": "Development of allophonic realization until adolescence: A production study of the affricate-fricative variation of /z/ among Japanese children",
    "volume": "main",
    "abstract": "The development of allophonic variants of phonemes is poorly understood. This study aimed to examine when the allophonic variants of a phoneme are realized like adults. Japanese children aged 513 years and adults participated in an elicited production task. We analyzed developmental changes in allophonic variation of the phoneme /z/, which is realized variably either as an affricate or a fricative. The results revealed that children aged nine years or younger realized /z/ as affricate significantly more than 13-year-old and adult speakers. Once the children reached 11 years of age, the difference compared to adults was not statistically significant, which denotes a similar developmental pattern as that of speech motor control (e.g., lip and jaw) and cognitive-linguistic skill. Moreover, we examined whether the developmental changes of allophonic realization of /z/ are due to speech rate and the time to articulate /z/. The results showed that the allophonic realization of /z/ is not affected by those factors, different from that of adults. We also found that the effects of speech rate and the time to articulate /z/ on the allophonic realization become adult-like at around 11 years of age",
    "checked": true,
    "id": "aa75e7b14a52e3a2b758348cbf6b9bb04b8bf1d9",
    "semantic_title": "development of allophonic realization until adolescence: a production study of the affricate-fricative variation of /z/ among japanese children",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ahn22b_interspeech.html": {
    "title": "Recurrent multi-head attention fusion network for combining audio and text for speech emotion recognition",
    "volume": "main",
    "abstract": "To infer emotions accurately from speech, fusion of audio and text is essential as words carry most information about semantics and emotions. Attention mechanism is essential component in multimodal fusion architecture as it dynamically pairs different regions within multimodal sequences. However, existing architecture lacks explicit structure to model dynamics between fused representations. Thus we propose recurrent multi-head attention in a fusion architecture, which selects salient fused representations and learns dynamics between them. Multiple 2-D attention layers select salient pairs among all possible pairs of audio and text representations, which are combined with fusion operation. Lastly, multiple fused representations are fed into recurrent unit to learn dynamics between fused representations. Our method outperforms existing approaches for fusion of audio and text for speech emotion recognition and achieves state-of-the-art accuracies on benchmark IEMOCAP dataset",
    "checked": true,
    "id": "24130b932d17a53ec50932a074e74b18f989d3e7",
    "semantic_title": "recurrent multi-head attention fusion network for combining audio and text for speech emotion recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/coppietersdegibson22_interspeech.html": {
    "title": "Low-Level Physiological Implications of End-to-End Learning for Speech Recognition",
    "volume": "main",
    "abstract": "Current speech recognition architectures perform very well from the point of view of machine learning, hence user interaction. This suggests that they are emulating the human biological system well. We investigate whether the inference can be inverted to provide insights into that biological system; in particular the hearing mechanism. Using SincNet, we confirm that end-to-end systems do learn well known filterbank structures. However, we also show that wider band-width filters are important in the learned structure. Whilst some benefits can be gained by initialising both narrow and wide-band filters, physiological constraints suggest that such filters arise in mid-brain rather than the cochlea. We show that standard machine learning architectures must be modified to allow this process to be emulated neurally",
    "checked": false,
    "id": "f2cb19ab1c2046b318c24bfb85968b33e6f8d6f9",
    "semantic_title": "low-level physiological implications of end-to-end learning of speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/machado22_interspeech.html": {
    "title": "Idiosyncratic lingual articulation of American English // and // using network analysis",
    "volume": "main",
    "abstract": "Formant dynamics are believed to reflect the characteristic articulatory behavior of a speaker. The present study aims to explore individual articulatory behaviors when producing American English // and //. The two vowels differ in the degree of inherent spectral change, a property believed to carry information about vowel-phoneme identity, which may be reflected in the articulatory movements. We measured first and second formants together with tongue blade and dorsum trajectories from 20 speakers producing 330 words in citation forms. Using the network analysis, the relationships between acoustic and kinematic variables were revealed. In particular, between-speaker articulatory behaviors were most dissimilar in // which requires less inherent spectral change. Moreover, when networks of speakers with similar formant patterns were compared, it was revealed that their articulatory behaviors also shared similarities, although they seemed to be organized in characteristic ways. These findings contribute to our understanding of the complex interaction between articulatory variables and the acoustic outcome",
    "checked": true,
    "id": "f0cb58052f86d1bfc1cddd2ab86024c43d498e15",
    "semantic_title": "idiosyncratic lingual articulation of american english // and // using network analysis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/toya22_interspeech.html": {
    "title": "Method for improving the word intelligibility of presented speech using bone-conduction headphones",
    "volume": "main",
    "abstract": "Bone-conduction (BC) headphones enable listeners to hear sounds through BC while leaving the ear canal (EC) open to enable surrounding air-conducted (AC) sound to pass through at the same time. However, the intelligibility of presented speech using BC headphones is degraded by BC transmission, especially in noisy environments. This paper proposes a method for improving the word intelligibility of presented BC speech under noisy conditions. The method consists of two types of emphasis: higher-frequency emphasis and consonant emphasis. In the higher-frequency emphasis, frequency components attenuated due to BC transmission were compensated by the inverse-filtering of the transfer function obtained from the regio-temporalis (RT) vibration or the EC radiated sound. In the consonant emphasis, consonant sections with 20-ms short-formant trajectories of subsequent vowels in speech signals were locally amplified by a constant gain. The results of word intelligibility tests showed that both types of emphasis had significant improvements in comparison with no-emphasis. Moreover, we found that the proposed method had the best improvements under all conditions",
    "checked": true,
    "id": "3e32efbdcc7432de3923a5aa715f40db385a28a2",
    "semantic_title": "method for improving the word intelligibility of presented speech using bone-conduction headphones",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mohapatra22_interspeech.html": {
    "title": "Three-dimensional finite-difference time-domain acoustic analysis of simplified vocal tract shapes",
    "volume": "main",
    "abstract": "The finite-difference time-domain (FDTD) method has been widely used for vocal tract acoustic modelling due to its simplicity and low computational cost. Nevertheless, the method suffers from high discretization error while approximating realistic vocal tract geometries using orthogonal grid elements. Alternatively, simplified vocal tract shapes having regular contours can be used for articulatory models. These geometries can be generated from one-dimensional (1D) area functions, which approximate vocal tracts as concatenated tubes with different cross-sections. To this aim, we modify an existing 3D FDTD model for faster acoustic simulation and synthesize five English vowels with various simplified vocal tract shapes. We implement six geometrical shapes for each vowel, consisting of circular, elliptical and square cross-sections with centric and eccentric tube segment configurations. Vowel transfer functions obtained from these FDTD simulations are compared with a highly accurate finite element (FE) scheme. The acoustic formants of the FDTD model agree well with the corresponding FEM approach for most vowels. The influence of vocal tracts with different geometry approximations remains insignificant for frequencies below $5$~kHz. However, vocal tracts with elliptical or eccentric configurations have produced higher-order acoustic modes. This paper characterizes the acoustic properties of simplified vocal tract shapes using the 3D FDTD scheme",
    "checked": true,
    "id": "8c6d89a4e25acb5bdca4afcb65676fe8723df1cc",
    "semantic_title": "three-dimensional finite-difference time-domain acoustic analysis of simplified vocal tract shapes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dejong22_interspeech.html": {
    "title": "Speech imitation skills predict automatic phonetic convergence: a GMM-UBM study on L2",
    "volume": "main",
    "abstract": "Phonetic convergence is the observation that two interlocutors adapt their speech towards one another on an acoustic-phonetic level. It happens automatically and unconsciously, but people can also deliberately imitate others when asked to do so. Here, we investigate to what degree people converge to their interlocutor in a scripted dialogue when they are and when they are not explicitly requested to imitate their interlocutor. More specifically, we collected two separate data sets, where Italian- and French-native participants read English sentences aloud in alternating speaking turns. The results of both groups with different language backgrounds were compared against each other. We used a Gaussian mixture model  universal background model (GMM-UBM) to assess phonetic convergence on the sentence level. The GMM-UBM configuration was optimized to make the best distinction between speakers on validation data. We found that people start to converge to one another while interacting compared to the baseline and even more substantially when explicitly asked to do so. Results are robust across data sets. More importantly, the degree of implicit convergence people display is related to how good of an explicit imitator they are, supporting the claim that the two phenomena are based on the same neurocognitive process",
    "checked": true,
    "id": "03affadb0ff26037ce30ed7ceccc482ba944e715",
    "semantic_title": "speech imitation skills predict automatic phonetic convergence: a gmm-ubm study on l2",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/georges22_interspeech.html": {
    "title": "Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE",
    "volume": "main",
    "abstract": "The human perception system is often assumed to recruit motor knowledge when processing auditory speech inputs. Using articulatory modeling and deep learning, this study examines how this articulatory information can be used for discovering speech units in a self-supervised setting. We used vector-quantized variational autoencoders (VQ-VAE) to learn discrete representations from articulatory and acoustic speech data. In line with the zero-resource paradigm, an ABX test was then used to investigate how the extracted representations encode phonetically relevant properties. Experiments were conducted on three different corpora in English and French. We found that articulatory information rather organises the latent representations in terms of place of articulation whereas the speech acoustics mainly structure the latent space in terms of manner of articulation. We show that an optimal fusion of the two modalities can lead to a joint representation of these phonetic dimensions more accurate than each modality considered individually. Since articulatory information is usually not available in a practical situation, we finally investigate the benefit it provides when inferred from the speech acoustics in a self-supervised manner",
    "checked": true,
    "id": "372f6831903dab5bdb15b583f8f4890a7be6161e",
    "semantic_title": "self-supervised speech unit discovery from articulatory and acoustic features using vq-vae",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22i_interspeech.html": {
    "title": "Deep Speech Synthesis from Articulatory Representations",
    "volume": "main",
    "abstract": "In the articulatory synthesis task, speech is synthesized from input features containing information about the physical behavior of the human vocal tract. This task provides a promising direction for speech synthesis research, as the articulatory space is compact, smooth, and interpretable. Current works have highlighted the potential for deep learning models to perform articulatory synthesis. However, it remains unclear whether these models can achieve the efficiency and fidelity of the human speech production system. To help bridge this gap, we propose a time-domain articulatory synthesis methodology and demonstrate its efficacy with both electromagnetic articulography (EMA) and synthetic articulatory feature inputs. Our model is computationally efficient and achieves a transcription word error rate (WER) of 18.5% for the EMA-to-speech task, yielding an improvement of 11.6% compared to prior work. Through interpolation experiments, we also highlight the generalizability and interpretability of our approach",
    "checked": true,
    "id": "94796c7fceed1e6517575963e1b3f86e151aeb61",
    "semantic_title": "deep speech synthesis from articulatory representations",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ashokumar22_interspeech.html": {
    "title": "Orofacial somatosensory inputs in speech perceptual training modulate speech production",
    "volume": "main",
    "abstract": "Somatosensory inputs are important to acquire or learn precise control of movement [1]. In the case of speech, receiving somatosensory inputs together with corresponding speech sounds may be a key to formulate or calibrate the speech production system [2]. We here examined whether speech production can be modulated by perceptual training with repetitive exposure to paired auditory-somatosensory stimulation in the absence of actual production of the sound. We carried out a perceptual training using a vowel identification task with /e/-/eu/ continuum. The speech sounds were accompanied with somatosensory stimulation, in which a facial skin-stretch was applied in the backward direction. The vowels /e/ and /eu/ were recorded prior to and following the training and the first three formants were compared. Results showed that the third formant of /e/ was increased following the training, and the rest of formant was not changed. Since the current somatosensory stimulation was related to the articulatory movement for the production of /e/ (lip-spreading), repetitive exposure to somatosensory stimulation in addition to the sound may specifically change the articulatory behavior for the production of /e/. The results suggest that perceptual training with specific pairs of auditory-somatosensory inputs can be important to formulate production mechanisms",
    "checked": true,
    "id": "097cab1652391eedee8652105610b1ff764c3f70",
    "semantic_title": "orofacial somatosensory inputs in speech perceptual training modulate speech production",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22c_interspeech.html": {
    "title": "Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus",
    "volume": "main",
    "abstract": "Training a text-to-speech (TTS) model requires a large-scale text labeled speech corpus, which is troublesome to collect. In this paper, we propose a transfer learning framework for TTS that utilizes a large amount of unlabeled speech dataset for pre-training. By leveraging wav2vec2.0 representation, unlabeled speech can highly improve performance, especially in the lack of labeled speech. We also extend the proposed method to zero-shot multi-speaker TTS (ZS-TTS). The experimental results verify the effectiveness of the proposed method in terms of naturalness, intelligibility, and speaker generalization. We highlight that the single speaker TTS model fine-tuned on only 10 minutes of labeled dataset outperforms the other baselines, and the ZS-TTS model fine-tuned on only 30 minutes of single speaker dataset can generate the voice of the arbitrary speaker, by pre-training on an unlabeled multi-speaker speech corpus",
    "checked": true,
    "id": "f962c52c92097da18965a170b322e3dea63a3e47",
    "semantic_title": "transfer learning framework for low-resource text-to-speech using a large-scale unlabeled speech corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeki22_interspeech.html": {
    "title": "DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning",
    "volume": "main",
    "abstract": "Most text-to-speech (TTS) methods use high-quality speech corpora recorded in a well-designed environment, incurring a high cost for data collection. To solve this problem, existing noise-robust TTS methods are intended to use noisy speech corpora as training data. However, they only address either time-invariant or time-variant noises. We propose a degradation-robust TTS method, which can be trained on speech corpora that contain both additive noises and environmental distortions. It jointly represents the time-variant additive noises with a frame-level encoder and the time-invariant environmental distortions with an utterance-level encoder. We also propose a regularization method to attain clean environmental embedding that is disentangled from the utterance-dependent information such as linguistic contents and speaker characteristics. Evaluation results show that our method achieved significantly higher-quality synthetic speech than previous methods in the condition including both additive noise and reverberation",
    "checked": true,
    "id": "d3b5d9c635bf14b9bfd563df9fa52a22e653cfae",
    "semantic_title": "drspeech: degradation-robust text-to-speech synthesis with frame-level and utterance-level acoustic representation learning",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mitsui22b_interspeech.html": {
    "title": "MSR-NV: Neural Vocoder Using Multiple Sampling Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koizumi22_interspeech.html": {
    "title": "SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22_interspeech.html": {
    "title": "Bunched LPCNet2: Efficient Neural Vocoders Covering Devices from Cloud to Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bae22b_interspeech.html": {
    "title": "Hierarchical and Multi-Scale Variational Autoencoder for Diverse and Natural Non-Autoregressive Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/subramani22_interspeech.html": {
    "title": "End-to-end LPCNet: A Neural Vocoder With Fully-Differentiable LPC Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lam22_interspeech.html": {
    "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nikitaras22_interspeech.html": {
    "title": "Fine-grained Noise Control for Multispeaker Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/siuzdak22_interspeech.html": {
    "title": "WavThruVec: Latent speech representation as intermediate features for neural speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vovk22_interspeech.html": {
    "title": "Fast Grad-TTS: Towards Efficient Diffusion-Based Speech Generation on CPU",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22z_interspeech.html": {
    "title": "Simple and Effective Unsupervised Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoneyama22_interspeech.html": {
    "title": "Unified Source-Filter GAN with Harmonic-plus-Noise Source Excitation Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22e_interspeech.html": {
    "title": "NeMo Open Source Speaker Diarization System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22e_interspeech.html": {
    "title": "Voice2Alliance: Automatic Speaker Diarization and Quality Assurance of Conversational Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kumar22d_interspeech.html": {
    "title": "VAgyojaka: An Annotating and Post-Editing Tool for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/badi22_interspeech.html": {
    "title": "SKYE: More than a conversational AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/munakata22_interspeech.html": {
    "title": "Training Data Generation with DOA-based Selecting and Remixing for Unsupervised Training of Deep Separation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22e_interspeech.html": {
    "title": "Beam-Guided TasNet: An Iterative Speech Separation Framework with Multi-Channel Output",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xiong22b_interspeech.html": {
    "title": "Joint Estimation of Direction-of-Arrival and Distance for Arrays with Directional Sensors based on Sparse Bayesian Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22d_interspeech.html": {
    "title": "How to Listen? Rethinking Visual Sound Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ouyang22_interspeech.html": {
    "title": "Small Footprint Neural Networks for Acoustic Direction of Arrival Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22s_interspeech.html": {
    "title": "Multi-Modal Multi-Correlation Learning for Audio-Visual Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yin22b_interspeech.html": {
    "title": "MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with Unknown Number of Sound Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fu22c_interspeech.html": {
    "title": "Iterative Sound Source Localization for Unknown Number of Sources",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/patterson22_interspeech.html": {
    "title": "Distance-Based Sound Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22ba_interspeech.html": {
    "title": "VCSE: Time-Domain Visual-Contextual Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/aroudi22_interspeech.html": {
    "title": "TRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant Sound Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ge22_interspeech.html": {
    "title": "PercepNet+: A Phase and SNR Aware PercepNet for Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22c_interspeech.html": {
    "title": "Lightweight Full-band and Sub-band Fusion Network for Real Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cheng22_interspeech.html": {
    "title": "Cross-Layer Similarity Knowledge Distillation for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xiong22_interspeech.html": {
    "title": "Spectro-Temporal SubNet for Real-Time Monaural Speech Denoising and Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cao22_interspeech.html": {
    "title": "CMGAN: Conformer-based Metric GAN for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22_interspeech.html": {
    "title": "Model Compression by Iterative Pruning with Knowledge Distillation and Its Application to Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22l_interspeech.html": {
    "title": "Single-channel speech enhancement using Graph Fourier Transform",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22c_interspeech.html": {
    "title": "Joint Optimization of the Module and Sign of the Spectral Real Part Based on CRN for Speech Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22m_interspeech.html": {
    "title": "Attentive Recurrent Network for Low-Latency Active Noise Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22e_interspeech.html": {
    "title": "Memory-Efficient Multi-Step Speech Enhancement with Neural ODE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22f_interspeech.html": {
    "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22g_interspeech.html": {
    "title": "Improving Visual Speech Enhancement Network by Learning Audio-visual Affinity with Multi-head Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22k_interspeech.html": {
    "title": "Speech Enhancement with Fullband-Subband Cross-Attention Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yu22_interspeech.html": {
    "title": "OSSEM: one-shot speaker adaptive speech enhancement using meta learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jiang22b_interspeech.html": {
    "title": "Efficient Speech Enhancement with Neural Homomorphic Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/thakker22_interspeech.html": {
    "title": "Fast Real-time Personalized Speech Enhancement: End-to-End Enhancement Network (E3Net) and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sato22b_interspeech.html": {
    "title": "Strategies to Improve Robustness of Target Speech Extraction to Enrollment Variations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mehmood22_interspeech.html": {
    "title": "FedNST: Federated Noisy Student Training for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fu22_interspeech.html": {
    "title": "SCaLa: Supervised Contrastive Learning for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22k_interspeech.html": {
    "title": "NAS-SCAE: Searching Compact Attention-based Encoders For End-to-end Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22e_interspeech.html": {
    "title": "Leveraging Acoustic Contextual Representation by Audio-textual Cross-modal Learning for Conversational ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ma22_interspeech.html": {
    "title": "PM-MMUT: Boosted Phone-mask Data Augmentation using Multi-Modeling Unit Training for Phonetic-Reduction-Robust E2E Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/audhkhasi22_interspeech.html": {
    "title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weiran22_interspeech.html": {
    "title": "Improving Rare Word Recognition with LM-aware MWER Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zeineldeen22_interspeech.html": {
    "title": "Improving the Training Recipe for a Robust Conformer-based Hybrid Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/laptev22_interspeech.html": {
    "title": "CTC Variations Through New WFST Topologies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sustek22_interspeech.html": {
    "title": "Dealing with Unknowns in Continual Learning for End-to-end Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/miao22c_interspeech.html": {
    "title": "Towards Efficiently Learning Monotonic Alignments for Attention-based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22fa_interspeech.html": {
    "title": "On monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/handekabil22_interspeech.html": {
    "title": "From Undercomplete to Sparse Overcomplete Autoencoders to Improve LF-MMI based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tanaka22_interspeech.html": {
    "title": "Domain Adversarial Self-Supervised Speech Representation Learning for Improving Unknown Domain Downstream Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/maekaku22_interspeech.html": {
    "title": "Attention Weight Smoothing Using Prior Distributions for Transformer-Based End-to-End ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/uchida22_interspeech.html": {
    "title": "Reducing Offensive Replies in Open Domain Dialogue Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22b_interspeech.html": {
    "title": "Induce Spoken Dialog Intents via Deep Unsupervised Context Contrastive Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nihei22_interspeech.html": {
    "title": "Dialogue Acts Aided Important Utterance Detection Based on Multiparty and Multimodal Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bekal22_interspeech.html": {
    "title": "Contextual Acoustic Barge-In Classification for Spoken Dialog Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22b_interspeech.html": {
    "title": "Calibrate and Refine! A Novel and Agile Framework for ASR Error Robust Intent Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/feng22c_interspeech.html": {
    "title": "ASR-Robust Natural Language Understanding on ASR-GLUE dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dao22_interspeech.html": {
    "title": "From Disfluency Detection to Intent Detection and Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22g_interspeech.html": {
    "title": "Audio-Visual Wake Word Spotting in MISP2021 Challenge: Dataset Release and Deep Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sartzetaki22_interspeech.html": {
    "title": "Extending Compositional Attention Networks for Social Reasoning in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22da_interspeech.html": {
    "title": "TopicKS: Topic-driven Knowledge Selection for Knowledge-grounded Dialogue Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liesenfeld22_interspeech.html": {
    "title": "Bottom-up discovery of structure and variation in response tokens (backchannels') across diverse languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22f_interspeech.html": {
    "title": "Cross-modal Transfer Learning via Multi-grained Alignment for End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ochi22_interspeech.html": {
    "title": "Use of Nods Less Synchronized with Turn-Taking and Prosody During Conversations in Adults with Autism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ivanko22_interspeech.html": {
    "title": "DAVIS: Driver's Audio-Visual Speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vaaras22_interspeech.html": {
    "title": "Analysis of Self-Supervised Learning and Dimensionality Reduction Methods in Clustering-Based Active Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22n_interspeech.html": {
    "title": "Emotion-Shift Aware CRF for Decoding Emotion Sequence in Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/su22_interspeech.html": {
    "title": "Vaccinating SER to Neutralize Adversarial Attacks with Self-Supervised Augmentation Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/parry22_interspeech.html": {
    "title": "Speech Emotion Recognition in the Wild using Multi-task and Adversarial Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gudmalwar22_interspeech.html": {
    "title": "The Magnitude and Phase based Speech Representation Learning using Autoencoder for Classifying Speech Emotions using Deep Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/goncalves22_interspeech.html": {
    "title": "Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koizumi22b_interspeech.html": {
    "title": "SNRi Target Training for Joint Speech Enhancement and Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sanada22_interspeech.html": {
    "title": "Deep Self-Supervised Learning of Speech Denoising from Noisy Speeches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22d_interspeech.html": {
    "title": "NASTAR: Noise Adaptive Speech Enhancement with Target-Conditional Resampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shchekotov22_interspeech.html": {
    "title": "FFC-SE: Fast Fourier Convolution for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tal22_interspeech.html": {
    "title": "A Systematic Comparison of Phonetic Aware Techniques for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shin22c_interspeech.html": {
    "title": "Multi-View Attention Transfer for Efficient Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/goswami22_interspeech.html": {
    "title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bensimon22_interspeech.html": {
    "title": "Correcting Mispronunciations in Speech using Spectrogram Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fong22_interspeech.html": {
    "title": "Speech Audio Corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22i_interspeech.html": {
    "title": "End-to-End Binaural Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koch22_interspeech.html": {
    "title": "PoeticTTS - Controllable Poetry Reading for Literary Studies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/krug22_interspeech.html": {
    "title": "Articulatory Synthesis for Data Augmentation in Phoneme Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22c_interspeech.html": {
    "title": "SF-DST: Few-Shot Self-Feeding Reading Comprehension Dialogue State Tracking with Auxiliary Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cattan22_interspeech.html": {
    "title": "Benchmarking Transformers-based models on French Spoken Language Understanding tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/heo22b_interspeech.html": {
    "title": "mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22y_interspeech.html": {
    "title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/raju22_interspeech.html": {
    "title": "On joint training with interfaces for spoken language understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/garg22_interspeech.html": {
    "title": "Device-Directed Speech Detection: Regularization via Distillation for Weakly-Supervised Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ogayo22_interspeech.html": {
    "title": "Building African Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dheram22_interspeech.html": {
    "title": "Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chan22b_interspeech.html": {
    "title": "Training and typological bias in ASR performance for world Englishes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zanonboito22_interspeech.html": {
    "title": "A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/johnson22_interspeech.html": {
    "title": "Automatic Dialect Density Estimation for African American English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kukk22_interspeech.html": {
    "title": "Improving Language Identification of Accented Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/toussaint22_interspeech.html": {
    "title": "Design Guidelines for Inclusive Speaker Verification Evaluation Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/trinh22_interspeech.html": {
    "title": "Reducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kunihara22_interspeech.html": {
    "title": "Gradual Improvements Observed in Learners' Perception and Production of L2 Sounds Through Continuing Shadowing Practices on a Daily Basis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kirchhubel22_interspeech.html": {
    "title": "Spoofed speech from the perspective of a forensic phonetician",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jeon22_interspeech.html": {
    "title": "Investigating Prosodic Variation in British English Varieties using ProPer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hwang22_interspeech.html": {
    "title": "Perceived prominence and downstep in Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/alicehajic22_interspeech.html": {
    "title": "The discrimination of [zi]-[di] by Japanese listeners and the prospective phonologization of /zi/",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/langheinrich22_interspeech.html": {
    "title": "Glottal inverse filtering based on articulatory synthesis and deep learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ludusan22_interspeech.html": {
    "title": "Investigating phonetic convergence of laughter in conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/delvaux22_interspeech.html": {
    "title": "Telling self-defining memories: An acoustic study of natural emotional speech productions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/spinu22_interspeech.html": {
    "title": "Voicing neutralization in Romanian fricatives across different speech styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liao22_interspeech.html": {
    "title": "Nasal Coda Loss in the Chengdu Dialect of Mandarin: Evidence from RT-MRI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/buech22_interspeech.html": {
    "title": "ema2wav: doing articulation by Praat",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rumberg22b_interspeech.html": {
    "title": "Improving Phonetic Transcriptions of Children's Speech by Pronunciation Modelling with Constrained CTC-Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/soky22_interspeech.html": {
    "title": "Leveraging Simultaneous Translation for Enhancing Transcription of Low-resource Language via Cross Attention Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mussakhojayeva22_interspeech.html": {
    "title": "KSC2: An Industrial-Scale Open-Source Kazakh Speech Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/szalay22_interspeech.html": {
    "title": "Knowledge of accent differences can be used to predict speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/scharf22_interspeech.html": {
    "title": "Lombard Effect for Bilingual Speakers in Cantonese and English: importance of spectro-temporal features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/flechl22_interspeech.html": {
    "title": "End-to-end speech recognition modeling from de-identified data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yadavalli22_interspeech.html": {
    "title": "Multi-Task End-to-End Model for Telugu Dialect and Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xie22b_interspeech.html": {
    "title": "DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22_interspeech.html": {
    "title": "Keyword Spotting with Synthetic Data using Heterogeneous Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deseyssel22_interspeech.html": {
    "title": "Probing phoneme, language and speaker information in unsupervised speech representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/birladeanu22_interspeech.html": {
    "title": "Automatic Detection of Reactive Attachment Disorder Through Turn-Taking Analysis in Clinical Child-Caregiver Sessions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22k_interspeech.html": {
    "title": "Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/miller22_interspeech.html": {
    "title": "Exploring Few-Shot Fine-Tuning Strategies for Models of Visually Grounded Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hwang22c_interspeech.html": {
    "title": "Pseudo Label Is Better Than Human Label",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vandermerwe22_interspeech.html": {
    "title": "A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22_interspeech.html": {
    "title": "PRISM: Pre-trained Indeterminate Speaker Representation Model for Speaker Diarization and Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/qin22_interspeech.html": {
    "title": "Cross-Age Speaker Verification: Learning Age-Invariant Speaker Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22j_interspeech.html": {
    "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/brummer22_interspeech.html": {
    "title": "Probabilistic Spherical Discriminant Analysis: An Alternative to PLDA for length-normalized embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gu22_interspeech.html": {
    "title": "Deep speaker embedding with frame-constrained training strategy for speaker verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22f_interspeech.html": {
    "title": "Interrelate Training and Searching: A Unified Online Clustering Framework for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/he22c_interspeech.html": {
    "title": "End-to-End Audio-Visual Neural Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yue22b_interspeech.html": {
    "title": "Online Speaker Diarization with Core Samples Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22r_interspeech.html": {
    "title": "Robust End-to-end Speaker Diarization with Generic Neural Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22t_interspeech.html": {
    "title": "MSDWild: Multi-modal Speaker Diarization Dataset in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tanveer22_interspeech.html": {
    "title": "Unsupervised Speaker Diarization that is Agnostic to Language, Overlap-Aware, and Tuning Free",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kinoshita22_interspeech.html": {
    "title": "Utterance-by-utterance overlap-aware neural diarization with Graph-PIT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ha_interspeech.html": {
    "title": "Spatial-aware Speaker Diarizaiton for Multi-channel Multi-party Meeting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liang22_interspeech.html": {
    "title": "Selective Pseudo-labeling and Class-wise Discriminative Fusion for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22d_interspeech.html": {
    "title": "An End-to-End Macaque Voiceprint Verification Method Based on Channel Fusion Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22c_interspeech.html": {
    "title": "Human Sound Classification based on Feature Fusion Method with Air and Bone Conducted Signal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22e_interspeech.html": {
    "title": "RaDur: A Reference-aware and Duration-robust Network for Target Sound Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tripathi22_interspeech.html": {
    "title": "Temporal Self Attention-Based Residual Network for Environmental Sound Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22f_interspeech.html": {
    "title": "AudioTagging Done Right: 2nd comparison of deep learning methods for environmental sound classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22i_interspeech.html": {
    "title": "Improving Target Sound Extraction with Timestamp Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22d_interspeech.html": {
    "title": "A Multi-grained based Attention Network for Semi-supervised Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22c_interspeech.html": {
    "title": "Temporal coding with magnitude-phase regularization for sound event detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shao22_interspeech.html": {
    "title": "RCT: Random consistency training for semi-supervised sound event detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xin22_interspeech.html": {
    "title": "Audio Pyramid Transformer with Domain Adaption for Weakly Supervised Sound Event Detection and Audio Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22aa_interspeech.html": {
    "title": "Active Few-Shot Learning for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ye22b_interspeech.html": {
    "title": "Uncertainty Calibration for Deep Audio Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hou22c_interspeech.html": {
    "title": "Event-related data conditioning for acoustic event classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22_interspeech.html": {
    "title": "A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yin22_interspeech.html": {
    "title": "RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luong22_interspeech.html": {
    "title": "FlowVocoder: A small Footprint Neural Vocoder based Normalizing Flow for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22c_interspeech.html": {
    "title": "DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yuan22_interspeech.html": {
    "title": "AdaVocoder: Adaptive Vocoder for Custom Voice",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22d_interspeech.html": {
    "title": "RefineGAN: Universally Generating Waveform Better than Ground Truth with Highly Accurate Pitch and Intensity Responses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/du22b_interspeech.html": {
    "title": "VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mengnan22_interspeech.html": {
    "title": "Improving GAN-based vocoder for fast and high-quality speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yi22_interspeech.html": {
    "title": "SoftSpeech: Unsupervised Duration Model in FastSpeech 2",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22d_interspeech.html": {
    "title": "A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22q_interspeech.html": {
    "title": "SiD-WaveFlow: A Low-Resource Vocoder Independent of Prior Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gorai22_interspeech.html": {
    "title": "Text-to-speech synthesis using spectral modeling based on non-negative autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kanagawa22_interspeech.html": {
    "title": "Joint Modeling of Multi-Sample and Subband Signals for Fast Neural Vocoding on CPU",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kaneko22_interspeech.html": {
    "title": "MISRNet: Lightweight Neural Vocoder Using Multi-Input Single Shared Residual Blocks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/miao22b_interspeech.html": {
    "title": "A compact transformer-based GAN vocoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tachibana22_interspeech.html": {
    "title": "Diffusion Generative Vocoder for Fullband Speech Synthesis Based on Weak Third-order SDE Solver",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/variani22_interspeech.html": {
    "title": "On Adaptive Weight Interpolation of the Hybrid Autoregressive Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22_interspeech.html": {
    "title": "Learning to rank with BERT-based confidence models in ASR rescoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22b_interspeech.html": {
    "title": "VQ-T: RNN Transducers using Vector-Quantized Prediction Network States",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22g_interspeech.html": {
    "title": "WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22j_interspeech.html": {
    "title": "Internal Language Model Estimation Through Explicit Context Vector Learning for Attention-based Encoder-decoder ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22i_interspeech.html": {
    "title": "Improving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bai22_interspeech.html": {
    "title": "Parameter-Efficient Conformers via Sharing Sparsely-Gated Experts for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22n_interspeech.html": {
    "title": "CaTT-KWS: A Multi-stage Customized Keyword Spotting Framework based on Cascaded Transducer-Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22t_interspeech.html": {
    "title": "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rathod22_interspeech.html": {
    "title": "Multi-stage Progressive Compression of Conformer Transducer for On-device Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weiran22b_interspeech.html": {
    "title": "Streaming Align-Refine for Non-autoregressive Deliberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22d_interspeech.html": {
    "title": "Federated Pruning: Improving Neural Network Efficiency with Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ding22b_interspeech.html": {
    "title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ding22c_interspeech.html": {
    "title": "4-bit Conformer with Native Quantization Aware Training for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22i_interspeech.html": {
    "title": "Self-Distillation Based on High-level Information Supervision for Compressing End-to-End ASR Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jia22b_interspeech.html": {
    "title": "Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22_interspeech.html": {
    "title": "A High-Quality and Large-Scale Dataset for English-Vietnamese Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22f_interspeech.html": {
    "title": "Investigating Parameter Sharing in Multilingual Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22h_interspeech.html": {
    "title": "Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational(RAMC) Speech Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22j_interspeech.html": {
    "title": "TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deng22b_interspeech.html": {
    "title": "Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tran22b_interspeech.html": {
    "title": "BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/markitantov22_interspeech.html": {
    "title": "Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS) Corpus: Multimodal Mask Type Recognition Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chien22_interspeech.html": {
    "title": "Bayesian Transformer Using Disentangled Mask Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22o_interspeech.html": {
    "title": "Audio-Visual Speech Recognition in MISP2021 Challenge: Dataset Release and Deep Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22u_interspeech.html": {
    "title": "From Start to Finish: Latency Reduction Strategies for Incremental Speech Synthesis in Simultaneous Speech-to-Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tam22_interspeech.html": {
    "title": "Isochrony-Aware Neural Machine Translation for Automatic Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dong22b_interspeech.html": {
    "title": "Leveraging Pseudo-labeled Data to Improve Direct Speech-to-Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pan22b_interspeech.html": {
    "title": "A Hybrid Continuity Loss to Reduce Over-Suppression for Time-domain Target Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/berg22_interspeech.html": {
    "title": "Extending GCC-PHAT using Shift Equivariant Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tzinis22_interspeech.html": {
    "title": "Heterogeneous Target Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22w_interspeech.html": {
    "title": "Separate What You Describe: Language-Queried Audio Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/markovic22_interspeech.html": {
    "title": "Implicit Neural Spatial Filtering for Multichannel Source Separation in the Waveform Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nozaki22_interspeech.html": {
    "title": "End-to-end Speech-to-Punctuated-Text Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pupier22_interspeech.html": {
    "title": "End-to-End Dependency Parsing of Spoken French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22_interspeech.html": {
    "title": "Turn-Taking Prediction for Natural Conversational Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22b_interspeech.html": {
    "title": "Streaming Intended Query Detection using E2E Modeling for Continued Conversation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lehecka22_interspeech.html": {
    "title": "Exploring Capabilities of Monolingual Audio Transformers using Large Datasets in Automatic Speech Recognition of Czech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schoburgcarrillodemira22_interspeech.html": {
    "title": "SVTS: Scalable Video-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kishiyama22_interspeech.html": {
    "title": "One-step models in pitch perception: Experimental evidence from Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/perezramon22_interspeech.html": {
    "title": "Generating iso-accented stimuli for second language research: methodology and a dataset for Spanish-accented English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/leemann22_interspeech.html": {
    "title": "Factors affecting the percept of Yanny v. Laurel (or mixed): Insights from a large-scale study on Swiss German listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22ba_interspeech.html": {
    "title": "Effects of laryngeal manipulations on voice gender perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22n_interspeech.html": {
    "title": "Why is Korean lenis stop difficult to perceive for L2 Korean learners?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zurita22_interspeech.html": {
    "title": "Lexical stress in Spanish word segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shin22_interspeech.html": {
    "title": "Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/abdullah22_interspeech.html": {
    "title": "Integrating Form and Meaning: A Multi-Task Learning Model for Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22l_interspeech.html": {
    "title": "Personalized Keyword Spotting through Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/svec22_interspeech.html": {
    "title": "Deep LSTM Spoken Term Detection using Wav2Vec 2.0 Recognizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jose22_interspeech.html": {
    "title": "Latency Control for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nayak22_interspeech.html": {
    "title": "Improving Voice Trigger Detection with Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/soltau22_interspeech.html": {
    "title": "RNN Transducers for Named Entity Recognition with constraints on alignment for understanding medical conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22c_interspeech.html": {
    "title": "Towards Automated Counselling Decision-Making: Remarks on Therapist Action Forecasting on the AnnoMI Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fara22_interspeech.html": {
    "title": "Speech and the n-Back task as a lens into depression. How combining both may allow us to isolate different core symptoms of depression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/romana22_interspeech.html": {
    "title": "Enabling Off-the-Shelf Disfluency Detection and Categorization for Pathological Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/botelho22_interspeech.html": {
    "title": "Challenges of using longitudinal and cross-domain corpora on studies of pathological speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22d_interspeech.html": {
    "title": "g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22b_interspeech.html": {
    "title": "A Unified Accent Estimation Method Based on Multi-Task Learning for Japanese Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/raitio22_interspeech.html": {
    "title": "Vocal effort modeling in neural TTS for improving the intelligibility of synthetic speech in noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22d_interspeech.html": {
    "title": "TTS-by-TTS 2: Data-Selective Augmentation for Neural Speech Synthesis Using Ranking Support Vector Machine with Variational Autoencoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/comini22_interspeech.html": {
    "title": "Low-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ingle22_interspeech.html": {
    "title": "Real-Time Monitoring of Silences in Contact Center Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zielinski22_interspeech.html": {
    "title": "Humanizing bionic voice: interactive demonstration of aesthetic design and control factors influencing the devices assembly and waveshape engineering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ronssin22_interspeech.html": {
    "title": "Application for Real-time Personalized Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bhattacharya22b_interspeech.html": {
    "title": "Coswara: A website application enabling COVID-19 screening by analysing respiratory sound samples and health symptoms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schafer22_interspeech.html": {
    "title": "CoachLea: an Android Application to Evaluate the Speech Production and Perception of Children with Hearing Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/haider22_interspeech.html": {
    "title": "An Automated Mood Diary for Older User's using Ambient Assisted Living Recorded Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22_interspeech.html": {
    "title": "Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer Based Infant Cry Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fernau22_interspeech.html": {
    "title": "Towards Automated Dialog Personalization using MBTI Personality Indicators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/qian22_interspeech.html": {
    "title": "Word-wise Sparse Attention for Multimodal Sentiment Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gupta22_interspeech.html": {
    "title": "Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22b_interspeech.html": {
    "title": "Exploring Multi-task Learning Based Gender Recognition and Age Estimation for Class-imbalanced Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22b_interspeech.html": {
    "title": "Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22r_interspeech.html": {
    "title": "Impact of Background Noise and Contribution of Visual Information in Emotion Identification by Native Mandarin Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22q_interspeech.html": {
    "title": "Exploiting Fine-tuning of Self-supervised Learning Models for Improving Bi-modal Sentiment Analysis and Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tao22_interspeech.html": {
    "title": "Characterizing Therapist's Speaking Style in Relation to Empathy in Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tao22b_interspeech.html": {
    "title": "Hierarchical Attention Network for Evaluating Therapist Empathy in Counseling Session",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22v_interspeech.html": {
    "title": "Context-aware Multimodal Fusion for Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22z_interspeech.html": {
    "title": "Unsupervised Instance Discriminative Learning for Depression Detection from Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sardhaei22_interspeech.html": {
    "title": "How do our eyebrows respond to masks and whispering? The case of Persians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baird22_interspeech.html": {
    "title": "State & Trait Measurement from Nonverbal Vocalizations: A Multi-Task Joint Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saraf22_interspeech.html": {
    "title": "Confidence Measure for Automatic Age Estimation From Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fasoli22_interspeech.html": {
    "title": "Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sun22_interspeech.html": {
    "title": "Tree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hou22b_interspeech.html": {
    "title": "Bring dialogue-context into RNN-T for streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weninger22_interspeech.html": {
    "title": "Conformer with dual-mode chunked attention for joint online and offline ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22c_interspeech.html": {
    "title": "Efficient Training of Neural Transducer for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22b_interspeech.html": {
    "title": "Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kuang22_interspeech.html": {
    "title": "Pruned RNN-T for fast, memory-eicient ASR training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22h_interspeech.html": {
    "title": "Deep Sparse Conformer for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22l_interspeech.html": {
    "title": "Chain-based Discriminative Autoencoders for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mahadeokar22_interspeech.html": {
    "title": "Streaming parallel transducer beam search with fast slow cascaded encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22y_interspeech.html": {
    "title": "Self-regularised Minimum Latency Training for Streaming Transformer-based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/albesano22_interspeech.html": {
    "title": "On the Prediction Network Architecture in RNN-T for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shinohara22_interspeech.html": {
    "title": "Minimum latency training of sequence transducers for streaming end-to-end speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/an22_interspeech.html": {
    "title": "CUSIDE: Chunking, Simulating Future Context and Decoding for Streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22j_interspeech.html": {
    "title": "Attention Enhanced Citrinet for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22b_interspeech.html": {
    "title": "Simple and Effective Zero-shot Cross-lingual Phoneme Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22_interspeech.html": {
    "title": "Robust Self-Supervised Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/algayres22_interspeech.html": {
    "title": "Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22e_interspeech.html": {
    "title": "Towards Green ASR: Lossless 4-bit Quantization of a Hybrid TDNN System on the 300-hr Swithboard Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/qin22b_interspeech.html": {
    "title": "Finer-grained Modeling units-based Meta-Learning for Low-resource Tibetan Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/janbakhshi22_interspeech.html": {
    "title": "Adversarial-Free Speaker Identity-Invariant Representation Learning for Automatic Dysarthric Speech Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22o_interspeech.html": {
    "title": "Automated Detection of Wilson's Disease Based on Improved Mel-frequency Cepstral Coefficients with Signal Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fan22b_interspeech.html": {
    "title": "The effect of backward noise on lexical tone discrimination in Mandarin-speaking amusics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ke22_interspeech.html": {
    "title": "Automatic Selection of Discriminative Features for Dementia Detection in Cantonese-Speaking People",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22q_interspeech.html": {
    "title": "Automated Voice Pathology Discrimination from Continuous Speech Benefits from Analysis by Phonetic Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mallolragolta22_interspeech.html": {
    "title": "Multi-Type Outer Product-Based Fusion of Respiratory Sounds for Detecting COVID-19",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22z_interspeech.html": {
    "title": "Robust Cough Feature Extraction and Classification Method for COVID-19 Cough Detection Based on Vocalization Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/javanmardi22_interspeech.html": {
    "title": "Comparing 1-dimensional and 2-dimensional spectral feature representations in voice pathology detection using machine learning and deep learning classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chatzoudis22_interspeech.html": {
    "title": "Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22d_interspeech.html": {
    "title": "Domain-aware Intermediate Pretraining for Dementia Detection with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fougeron22_interspeech.html": {
    "title": "Comparison of 5 methods for the evaluation of intelligibility in mild to moderate French dysarthric speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22b_interspeech.html": {
    "title": "Improving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22b_interspeech.html": {
    "title": "Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22d_interspeech.html": {
    "title": "Distilling a Pretrained Language Model to a Multilingual ASR Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sato22_interspeech.html": {
    "title": "Text-Only Domain Adaptation Based on Intermediate CTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/thienpondt22_interspeech.html": {
    "title": "Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takashima22_interspeech.html": {
    "title": "Updating Only Encoders Prevents Catastrophic Forgetting of End-to-End ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22_interspeech.html": {
    "title": "Improved CNN-Transformer using Broadcasted Residual Learning for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jung22_interspeech.html": {
    "title": "Pushing the limits of raw waveform speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22e_interspeech.html": {
    "title": "PHO-LID: A Unified Model Incorporating Acoustic-Phonetic and Phonotactic Information for Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tzudir22_interspeech.html": {
    "title": "Prosodic Information in Dialect Identification of a Tonal Language: The case of Ao",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22o_interspeech.html": {
    "title": "A Multimodal Strategy for Singing Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/daoudi22_interspeech.html": {
    "title": "A comparative study on vowel articulation in Parkinson's disease and multiple system atrophy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ardaillon22_interspeech.html": {
    "title": "Voicing decision based on phonemes classification and spectral moments for whisper-to-speech conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/talkar22_interspeech.html": {
    "title": "Speech Acoustics in Mild Cognitive Impairment and Parkinson's Disease With and Without Concurrent Drawing Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tran22c_interspeech.html": {
    "title": "Investigating the Impact of Speech Compression on the Acoustics of Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/brueggeman22_interspeech.html": {
    "title": "Speaker Trait Enhancement for Cochlear Implant Users: A Case Study for Speaker Emotion Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/reddy22b_interspeech.html": {
    "title": "Optimal thyroplasty implant shape and stiffness for treatment of acute unilateral vocal fold paralysis: Evidence from a canine in vivo phonation model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/babu22_interspeech.html": {
    "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rugayan22_interspeech.html": {
    "title": "Semantically Meaningful Metrics for Norwegian ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/klejch22_interspeech.html": {
    "title": "Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kumar22c_interspeech.html": {
    "title": "Linguistically Informed Post-processing for ASR Error correction in Sanskrit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/morshed22_interspeech.html": {
    "title": "Cross-lingual articulatory feature information transfer for speech recognition using recurrent progressive neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/aguirre22_interspeech.html": {
    "title": "Comparison of Models for Detecting Off-Putting Speaking Styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kawano22_interspeech.html": {
    "title": "Multimodal Persuasive Dialogue Corpus using Teleoperated Android",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shin22b_interspeech.html": {
    "title": "Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/adigwe22_interspeech.html": {
    "title": "Strategies for developing a Conversational Speech Dataset for Text-To-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22f_interspeech.html": {
    "title": "Deep CNN-based Inductive Transfer Learning for Sarcasm Detection in Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mitsui22_interspeech.html": {
    "title": "End-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/afshan22_interspeech.html": {
    "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/afshan22b_interspeech.html": {
    "title": "Learning from human perception to improve automatic speaker verification in style-mismatched conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/martikainen22_interspeech.html": {
    "title": "Exploring audio-based stylistic variation in podcasts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deja22_interspeech.html": {
    "title": "Automatic Evaluation of Speaker Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22c_interspeech.html": {
    "title": "Mix and Match: An Empirical Study on Training Corpus Composition for Polyglot Text-To-Speech (TTS)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takamichi22_interspeech.html": {
    "title": "J-MAC: Japanese multi-speaker audiobook corpus for speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/webber22_interspeech.html": {
    "title": "REYD  The First Yiddish Text-to-Speech Dataset and System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dekorte22_interspeech.html": {
    "title": "Data-augmented cross-lingual synthesis in a teacher-student framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pandey22b_interspeech.html": {
    "title": "Production characteristics of obstruents in WaveNet and older TTS systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lemaguer22_interspeech.html": {
    "title": "Back to the Future: Extending the Blizzard Challenge 2013",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meyer22c_interspeech.html": {
    "title": "BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/maniati22_interspeech.html": {
    "title": "SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22_interspeech.html": {
    "title": "Domain Generalization with Relaxed Instance Frequency-wise Normalization for Multi-device Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rui22_interspeech.html": {
    "title": "Couple learning for semi-supervised sound event detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rajan22_interspeech.html": {
    "title": "Oktoechos Classification in Liturgical Music Using SBU-LSTM/GRU",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/he22_interspeech.html": {
    "title": "SoundDoA: Learn Sound Source Direction of Arrival and Semantics from Sound Raw Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bergler22_interspeech.html": {
    "title": "ORCA-WHISPER: An Automatic Killer Whale Sound Type Generation Toolkit Using Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22d_interspeech.html": {
    "title": "Convolutional Recurrent Neural Network with Auxiliary Stream for Robust Variable-Length Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bassan22_interspeech.html": {
    "title": "Unsupervised Symbolic Music Segmentation using Ensemble Temporal Prediction Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shirian22_interspeech.html": {
    "title": "Visually-aware Acoustic Event Detection using Heterogeneous Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/singh22_interspeech.html": {
    "title": "A Passive Similarity based CNN Filter Pruning for Efficient Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baade22_interspeech.html": {
    "title": "MAE-AST: Masked Autoencoding Audio Spectrogram Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bayerl22_interspeech.html": {
    "title": "What can Speech and Language Tell us About the Working Alliance in Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/frost22_interspeech.html": {
    "title": "TB or not TB? Acoustic cough analysis for tuberculosis classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/berisha22_interspeech.html": {
    "title": "Are reported accuracies in the clinical speech machine learning literature overoptimistic?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mirheidari22_interspeech.html": {
    "title": "Automatic Detection of Expressed Emotion from Five-Minute Speech Samples: Challenges and Opportunities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mirheidari22b_interspeech.html": {
    "title": "Automatic cognitive assessment: Combining sparse datasets with disparate cognitive scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dang22_interspeech.html": {
    "title": "Exploring Semi-supervised Learning for Audio-based COVID-19 Detection using FixMatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bhattacharya22_interspeech.html": {
    "title": "Analyzing the impact of SARS-CoV-2 variants on respiratory sound signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/braun22_interspeech.html": {
    "title": "Automated Evaluation of Standardized Dementia Screening Tests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pereztoro22_interspeech.html": {
    "title": "Alzheimer's Detection from English to Spanish Using Acoustic and Linguistic Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/su22b_interspeech.html": {
    "title": "Extract and Abstract with BART for Clinical Notes from Doctor-Patient Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lamichhane22_interspeech.html": {
    "title": "Dyadic Interaction Assessment from Free-living Audio for Depression Severity Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nallanthighal22_interspeech.html": {
    "title": "COVID-19 detection based on respiratory sensing from speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luo22c_interspeech.html": {
    "title": "Bifurcation and Reunion: A Loss-Guided Two-Stage Approach for Monaural Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cheng22b_interspeech.html": {
    "title": "A deep complex multi-frame filtering network for stereophonic acoustic echo cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/han22c_interspeech.html": {
    "title": "Speaker- and Phone-aware Convolutional Transformer Network for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22t_interspeech.html": {
    "title": "Personalized Acoustic Echo Cancellation for Full-duplex Communications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22v_interspeech.html": {
    "title": "LCSM: A Lightweight Complex Spectral Mapping Framework for Stereophonic Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kothapally22_interspeech.html": {
    "title": "Joint Neural AEC and Beamforming with Double-Talk Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/helwani22_interspeech.html": {
    "title": "Clock Skew Robust Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/panchapagesan22_interspeech.html": {
    "title": "A Conformer-based Waveform-domain Neural Acoustic Echo Canceller Optimized for ASR Accuracy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kothapally22b_interspeech.html": {
    "title": "Complex-Valued Time-Frequency Self-Attention for Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xue22_interspeech.html": {
    "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22f_interspeech.html": {
    "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22c_interspeech.html": {
    "title": "FlowCPCVC: A Contrastive Predictive Coding Supervised Flow Framework for Any-to-Any Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lei22_interspeech.html": {
    "title": "Glow-WaveGAN 2: High-quality Zero-shot Text-to-speech Synthesis and Any-to-any Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22f_interspeech.html": {
    "title": "AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22d_interspeech.html": {
    "title": "Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22p_interspeech.html": {
    "title": "Streamable Speech Representation Disentanglement and Multi-Level Prosody Modeling for Live One-Shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22d_interspeech.html": {
    "title": "Accent Conversion using Pre-trained Model and Synthesized Data from Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vanrijn22_interspeech.html": {
    "title": "VoiceMe: Personalized voice generation in TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yuan22b_interspeech.html": {
    "title": "DeID-VC: Speaker De-identification via Zero-shot Pseudo Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lian22_interspeech.html": {
    "title": "Towards Improved Zero-shot Voice Conversion with Conditional DSVAE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/du22c_interspeech.html": {
    "title": "Disentanglement of Emotional Style and Speaker Identity for Expressive Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meng22_interspeech.html": {
    "title": "Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/du22_interspeech.html": {
    "title": "A Complementary Joint Training Approach Using Unpaired Speech and Text A Complementary Joint Training Approach Using Unpaired Speech and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gong22_interspeech.html": {
    "title": "Knowledge Transfer and Distillation from Autoregressive to Non-Autoregessive Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deng22_interspeech.html": {
    "title": "Confidence Score Based Conformer Speaker Adaptation for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22b_interspeech.html": {
    "title": "Decoupled Federated Learning for ASR with Non-IID Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22_interspeech.html": {
    "title": "Knowledge Distillation For CTC-based Speech Recognition Via Consistent Acoustic Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cui22b_interspeech.html": {
    "title": "Improving Generalization of Deep Neural Network Acoustic Models with Length Perturbation and N-best Based Label Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22m_interspeech.html": {
    "title": "Supervision-Guided Codebooks for Masked Prediction in Speech Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ren22_interspeech.html": {
    "title": "Speech Pre-training with Acoustic Piece",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22u_interspeech.html": {
    "title": "Censer: Curriculum Semi-supervised Learning for Speech Recognition Based on Self-supervised Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ao22_interspeech.html": {
    "title": "Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sawhney22_interspeech.html": {
    "title": "PISA: PoIncar Saliency-Aware Interpolative Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22w_interspeech.html": {
    "title": "Online Continual Learning of End-to-End Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/moriya22_interspeech.html": {
    "title": "Streaming Target-Speaker ASR with Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jain22_interspeech.html": {
    "title": "SPLICEOUT: A Simple and Efficient Audio Augmentation Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sunder22_interspeech.html": {
    "title": "Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in End-to-End Speech-to-Intent Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ohsugi22_interspeech.html": {
    "title": "Japanese ASR-Robust Pre-trained Language Model with Pseudo-Error Sentences Generated by Grapheme-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dong22_interspeech.html": {
    "title": "Improving Spoken Language Understanding with Cross-Modal Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/avila22_interspeech.html": {
    "title": "Low-bit Shift Network for End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22_interspeech.html": {
    "title": "Meta Auxiliary Learning for Low-resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22o_interspeech.html": {
    "title": "Adversarial Knowledge Distillation For Robust Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ou22_interspeech.html": {
    "title": "Incorporating Dual-Aware with Hierarchical Interactive Memory Networks for Task-Oriented Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22w_interspeech.html": {
    "title": "Pay More Attention to History: A Context Modeling Strategy for Conversational Text-to-SQL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22x_interspeech.html": {
    "title": "Small Changes Make Big Differences: Improving Multi-turn Response Selection in Dialogue Systems via Fine-Grained Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dinarelli22_interspeech.html": {
    "title": "Toward Low-Cost End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kapelonis22_interspeech.html": {
    "title": "A Multi-Task BERT Model for Schema-Guided Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22e_interspeech.html": {
    "title": "WavPrompt: Towards Few-Shot Spoken Language Understanding with Frozen Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ogushi22_interspeech.html": {
    "title": "Analysis of praising skills focusing on utterance contents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22fa_interspeech.html": {
    "title": "Speech2Slot: A Limited Generation Framework with Boundary Detection for Slot Filling from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koutini22_interspeech.html": {
    "title": "Efficient Training of Audio Transformers with Patchout",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sharma22_interspeech.html": {
    "title": "CNN-based Audio Event Recognition for Automated Violence Classification and Rating for Prime Video Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nam22_interspeech.html": {
    "title": "Frequency Dynamic Convolution: Frequency-Adaptive Pattern Recognition for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mostaani22_interspeech.html": {
    "title": "On Breathing Pattern Information in Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22p_interspeech.html": {
    "title": "Interactive Auido-text Representation for Automated Audio Captioning with Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yamamoto22_interspeech.html": {
    "title": "Deformable CNN and Imbalance-Aware Feature Learning for Singing Technique Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/muller22_interspeech.html": {
    "title": "Does Audio Deepfake Detection Generalize?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/muller22b_interspeech.html": {
    "title": "Attacker Attribution of Audio Deepfakes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pierre22_interspeech.html": {
    "title": "Are disentangled representations all you need to build speaker anonymization systems?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/teixeira22_interspeech.html": {
    "title": "Towards End-to-End Private Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/amid22_interspeech.html": {
    "title": "Extracting Targeted Training Data from ASR Models, and How to Mitigate It",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22k_interspeech.html": {
    "title": "Detecting Unintended Memorization in Language-Model-Fused ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/taniguchi22_interspeech.html": {
    "title": "Transformer-Based Automatic Speech Recognition with Auxiliary Input of Source Language Text Toward Transcribing Simultaneous Interpretation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gabeur22_interspeech.html": {
    "title": "AVATAR: Unconstrained Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22c_interspeech.html": {
    "title": "Word Discovery in Visually Grounded, Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rose22_interspeech.html": {
    "title": "End-to-End multi-talker audio-visual ASR using an active speaker attention module",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/serdyuk22_interspeech.html": {
    "title": "Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Muti-Person Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hong22_interspeech.html": {
    "title": "Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/harvill22_interspeech.html": {
    "title": "Frame-Level Stutter Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/priyasad22_interspeech.html": {
    "title": "Detecting Heart Failure Through Voice Analysis using Self-Supervised Mode-Based Memory Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ng22_interspeech.html": {
    "title": "Automatic Detection of Speech Sound Disorder in Child Speech Using Posterior-based Speaker Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/woszczyk22_interspeech.html": {
    "title": "Data Augmentation for Dementia Detection in Spoken Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dutta22b_interspeech.html": {
    "title": "Acoustic Representation Learning on Breathing and Speech Signals for COVID-19 Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bayerl22b_interspeech.html": {
    "title": "Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22b_interspeech.html": {
    "title": "HYU Submission for the SASV Challenge 2022: Reforming Speaker Embeddings with Spoofing-Aware Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/heo22_interspeech.html": {
    "title": "Two Methods for Spoofing-Aware Speaker Verification: Multi-Layer Perceptron Score Fusion Model and Integrated Embedding Projector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zeng22_interspeech.html": {
    "title": "Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/alenin22_interspeech.html": {
    "title": "A Subnetwork Approach for Spoofing Aware Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jung22c_interspeech.html": {
    "title": "SASV 2022: The First Spoofing-Aware Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22q_interspeech.html": {
    "title": "Representation Selective Self-distillation and wav2vec 2.0 Feature Exploration for Spoof-aware Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/westhausen22_interspeech.html": {
    "title": "tPLCnet: Real-time Deep Packet Loss Concealment in the Time Domain Using a Short Temporal Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tesch22_interspeech.html": {
    "title": "On the Role of Spatial, Spectral, and Temporal Processing for DNN-based Non-linear Multi-channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22e_interspeech.html": {
    "title": "DDS: A new device-degraded speech dataset for speech enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/du22d_interspeech.html": {
    "title": "Direction-Aware Joint Adaptation of Neural Speech Enhancement and Recognition in Real Multiparty Conversational Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bartolewska22_interspeech.html": {
    "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/welker22_interspeech.html": {
    "title": "Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ali22_interspeech.html": {
    "title": "Enhancing Embeddings for Speech Classification in Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/turetzky22_interspeech.html": {
    "title": "Deep Audio Waveform Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fras22_interspeech.html": {
    "title": "Convolutive Weighted Multichannel Wiener Filter Front-end for Distant Automatic Speech Recognition in Reverberant Multispeaker Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deoliveira22_interspeech.html": {
    "title": "Efficient Transformer-based Speech Enhancement Using Long Frames and STFT Magnitudes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22x_interspeech.html": {
    "title": "Improving Speech Enhancement through Fine-Grained Speech Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bilinski22_interspeech.html": {
    "title": "Creating New Voices using Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sanchez22_interspeech.html": {
    "title": "Unify and Conquer: How Phonetic Feature Representation Affects Polyglot Text-To-Speech (TTS)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/udagawa22_interspeech.html": {
    "title": "Human-in-the-loop Speaker Adaptation for DNN-based Multi-speaker TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/proszewska22_interspeech.html": {
    "title": "GlowVC: Mel-spectrogram space disentangling model for language-independent text-free voice conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22h_interspeech.html": {
    "title": "One-Shot Speaker Adaptation Based on Initialization by Generative Adversarial Networks for TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/levkovitch22_interspeech.html": {
    "title": "Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22j_interspeech.html": {
    "title": "Advanced Speaker Embedding with Predictive Variance of Gaussian Distribution for Speaker Adaptation in TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kakoulidis22_interspeech.html": {
    "title": "Karaoker: Alignment-free singing voice synthesis with speech training data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/um22_interspeech.html": {
    "title": "ACNN-VC: Utilizing Adaptive Convolution Neural Network for One-Shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sadekova22_interspeech.html": {
    "title": "A Unified System for Voice Cloning and Voice Conversion through Diffusion Probabilistic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22o_interspeech.html": {
    "title": "Adversarial Multi-Task Learning for Disentangling Timbre and Pitch in Singing Voice Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/agarwal22_interspeech.html": {
    "title": "Leveraging Symmetrical Convolutional Transformer Networks for Speech to Singing Voice Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/terashima22_interspeech.html": {
    "title": "Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22_interspeech.html": {
    "title": "Deep residual spiking neural network for keyword spotting in low-resource settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baskar22_interspeech.html": {
    "title": "Reducing Domain mismatch in Self-supervised speech pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhen22_interspeech.html": {
    "title": "Sub-8-Bit Quantization Aware Training for 8-Bit Neural Network Accelerator with On-Device Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22l_interspeech.html": {
    "title": "W2V2-Light: A Lightweight Version of Wav2vec 2.0 for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xie22_interspeech.html": {
    "title": "Compute Cost Amortized Transformer for Streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vyas22_interspeech.html": {
    "title": "On-demand compute reduction with stochastic wav2vec 2.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vanderreydt22_interspeech.html": {
    "title": "Transfer Learning from Multi-Lingual Speech Translation Benefits Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22q_interspeech.html": {
    "title": "FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kitamura22_interspeech.html": {
    "title": "Perceptual Evaluation of Penetrating Voices through a Semantic Differential Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tsukada22_interspeech.html": {
    "title": "Non-native Perception of Japanese Singleton/Geminate Contrasts: Comparison of Mandarin and Mongolian Speakers Differing in Japanese Experience",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/obrien22_interspeech.html": {
    "title": "Evaluating the effects of modified speech on perceptual speaker identification performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22i_interspeech.html": {
    "title": "Mandarin Lombard Grid: a Lombard-grid-like corpus of Standard Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arai22_interspeech.html": {
    "title": "Syllable sequence of /a/+/ta/ can be heard as /atta/ in Japanese with visual or tactile cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22i_interspeech.html": {
    "title": "InQSS: a speech intelligibility and quality assessment model using a multi-task learning network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weise22b_interspeech.html": {
    "title": "Investigating the influence of personality on acoustic-prosodic entrainment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22t_interspeech.html": {
    "title": "Common and differential acoustic representation of interpersonal and tactile iconic perception of Mandarin vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/eranovic22_interspeech.html": {
    "title": "Effects of Noise on Speech Perception and Spoken Word Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22aa_interspeech.html": {
    "title": "Acquisition of Two Consecutive Neutral Tones in Mandarin-Speaking Preschoolers: Phonological Representation and Phonetic Realization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/roy22_interspeech.html": {
    "title": "Air tissue boundary segmentation using regional loss in real-time Magnetic Resonance Imaging video for speech production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gibson22_interspeech.html": {
    "title": "Language-specific interactions of vowel discrimination in noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/antony22_interspeech.html": {
    "title": "An Improved Transformer Transducer Architecture for Hindi-English Code Switched Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kadandale22_interspeech.html": {
    "title": "VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dalhouse22_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning Approach to Phoneme Error Detection via Latent Phonetic Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fukuda22_interspeech.html": {
    "title": "Global RNN Transducer Models For Multi-dialect Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bernhard22_interspeech.html": {
    "title": "Acoustic Stress Detection in Isolated English Words for Computer-Assisted Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pundak22_interspeech.html": {
    "title": "On-the-fly ASR Corrections with Audio Exemplars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/quan22_interspeech.html": {
    "title": "FFM: A Frame Filtering Mechanism To Accelerate Inference Speed For Conformer In Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cui22_interspeech.html": {
    "title": "Two-pass Decoding and Cross-adaptation Based System Combination of End-to-end Conformer and Hybrid TDNN ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ye22_interspeech.html": {
    "title": "Improving Recognition of Out-of-vocabulary Words in E2E Code-switching ASR by Fusing Speech Generation Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22n_interspeech.html": {
    "title": "Mitigating bias against non-native accents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22k_interspeech.html": {
    "title": "A Multi-level Acoustic Feature Extraction Framework for Transformer Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22c_interspeech.html": {
    "title": "LAE: Language-Aware Encoder for Monolingual and Multilingual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pattanayak22_interspeech.html": {
    "title": "Significance of single frequency filter for the development of children's KWS system",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22n_interspeech.html": {
    "title": "A Language Agnostic Multilingual Streaming On-Device ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22m_interspeech.html": {
    "title": "Minimizing Sequential Confusion Error in Speech Command Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schuppler22_interspeech.html": {
    "title": "Homophone Disambiguation Profits from Durational Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zuo22b_interspeech.html": {
    "title": "Speaker-Specific Utterance Ensemble based Transfer Attack on Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sadhu22_interspeech.html": {
    "title": "Complex Frequency Domain Linear Prediction: A Tool to Compute Modulation Spectrum of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/singh22b_interspeech.html": {
    "title": "Spectral Modification Based Data Augmentation For Improving End-to-End ASR For Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/masumura22_interspeech.html": {
    "title": "End-to-End Joint Modeling of Conversation History-Dependent and Independent ASR Systems with Multi-History Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22da_interspeech.html": {
    "title": "Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22d_interspeech.html": {
    "title": "An Anchor-Free Detector for Continuous Speech Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22g_interspeech.html": {
    "title": "Low-complex and Highly-performed Binary Residual Neural Network for Small-footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dinkel22_interspeech.html": {
    "title": "UniKW-AT: Unified Keyword Spotting and Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22n_interspeech.html": {
    "title": "ESSumm: Extractive Speech Summarization from Untranscribed Meeting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/conneau22_interspeech.html": {
    "title": "XTREME-S: Evaluating Cross-lingual Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22r_interspeech.html": {
    "title": "Negative Guided Abstractive Dialogue Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cui22c_interspeech.html": {
    "title": "Exploring representation learning for small-footprint keyword spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xue22d_interspeech.html": {
    "title": "Large-Scale Streaming End-to-End Speech Translation with Neural Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22h_interspeech.html": {
    "title": "Phonetic Embedding for ASR Robustness in Entity Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22f_interspeech.html": {
    "title": "Hierarchical Tagger with Multi-task Learning for Cross-domain Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22h_interspeech.html": {
    "title": "Multi-class AUC Optimization for Robust Small-footprint Keyword Spotting with Limited Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/martnek22_interspeech.html": {
    "title": "Weak supervision for Question Type Detection with large language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22n_interspeech.html": {
    "title": "BIT-MI Deep Learning-based Model to Non-intrusive Speech Quality Assessment Challenge in Online Conferencing Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22o_interspeech.html": {
    "title": "MOS Prediction Network for Non-intrusive Speech Quality Assessment in Online Conferencing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shu22_interspeech.html": {
    "title": "Non-intrusive Speech Quality Assessment with a Multi-Task Learning based Subband Adaptive Attention Temporal Convolutional Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hao22_interspeech.html": {
    "title": "Soft-label Learn for No-Intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yi22b_interspeech.html": {
    "title": "ConferencingSpeech 2022 Challenge: Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/elhajal22_interspeech.html": {
    "title": "MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22v_interspeech.html": {
    "title": "CCATMos: Convolutional Context-aware Transformer Network for Non-intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22t_interspeech.html": {
    "title": "Impairment Representation Learning for Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22l_interspeech.html": {
    "title": "Exploring linguistic feature and model combination for speech recognition based automatic AD detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22q_interspeech.html": {
    "title": "ECAPA-TDNN Based Depression Detection from Clinical Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ravi22_interspeech.html": {
    "title": "A Step Towards Preserving Speakers' Identity While Detecting Depression Via Speaker Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rutowski22_interspeech.html": {
    "title": "Toward Corpus Size Requirements for Training and Evaluating Depression Risk Models Using Spoken Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ablimit22_interspeech.html": {
    "title": "Deep Learning Approaches for Detecting Alzheimer's Dementia from Conversational Speech of ILSE Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/seneviratne22_interspeech.html": {
    "title": "Multimodal Depression Severity Score Prediction Using Articulatory Coordination Features and Hierarchical Attention Based Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meripo22_interspeech.html": {
    "title": "ASR Error Detection via Audio-Transcript entailment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/karlapati22_interspeech.html": {
    "title": "CopyCat2: A Single Model for Multi-Speaker TTS and Many-to-Many Fine-Grained Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/makarov22_interspeech.html": {
    "title": "Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nishimura22_interspeech.html": {
    "title": "Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/seshadri22_interspeech.html": {
    "title": "Emphasis Control for Parallel Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/stephenson22_interspeech.html": {
    "title": "BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/omahony22_interspeech.html": {
    "title": "Combining conversational speech with read speech to improve prosody in Text-to-Speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lu22_interspeech.html": {
    "title": "Unsupervised Data Selection via Discrete Speech Representation for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22i_interspeech.html": {
    "title": "CTRL: Continual Representation Learning to Transfer Information of Pre-trained for WAV2VEC 2.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baskar22b_interspeech.html": {
    "title": "Speaker adaptation for Wav2vec2 based dysarthric ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ba_interspeech.html": {
    "title": "Non-Parallel Voice Conversion for ASR Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/qi22_interspeech.html": {
    "title": "Improved Consistency Training for Semi-Supervised Sequence-to-Sequence ASR via Speech Chain Reconstruction and Self-Transcribing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arunkumar22_interspeech.html": {
    "title": "Joint Encoder-Decoder Self-Supervised Pre-training for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zellers22_interspeech.html": {
    "title": "An overview of discourse clicks in Central Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/noguchi22_interspeech.html": {
    "title": "VOT and F0 perturbations for the realization of voicing contrast in Tohoku Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ridouane22_interspeech.html": {
    "title": "Complex sounds and cross-language influence: The case of ejectives in Omani Mehri",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hutin22_interspeech.html": {
    "title": "When Phonetics Meets Morphology: Intervocalic Voicing Within and Across Words in Romance Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kuang22b_interspeech.html": {
    "title": "The mapping between syntactic and prosodic phrasing in English and Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/buech22b_interspeech.html": {
    "title": "Pharyngealization in Amazigh: Acoustic and articulatory marking over time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pelloin22_interspeech.html": {
    "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22c_interspeech.html": {
    "title": "Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kumar22_interspeech.html": {
    "title": "Learning Under Label Noise for Robust Spoken Language Understanding systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/le22_interspeech.html": {
    "title": "Deliberation Model for On-Device Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yadav22b_interspeech.html": {
    "title": "Intent classification using pre-trained language agnostic embeddings for low resource languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arora22_interspeech.html": {
    "title": "Two-Pass Low Latency End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/close22_interspeech.html": {
    "title": "Non-intrusive Speech Intelligibility Metric Prediction for Hearing Impaired Individuals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tu22_interspeech.html": {
    "title": "Exploiting Hidden Representations from a DNN-based Speech Recogniser for Speech Intelligibility Prediction in Hearing-impaired Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tu22b_interspeech.html": {
    "title": "Unsupervised Uncertainty Measures of Automatic Speech Recognition for Non-intrusive Speech Intelligibility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/robach22_interspeech.html": {
    "title": "Speech Intelligibility Prediction for Hearing-Impaired Listeners with the LEAP Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cardinale22_interspeech.html": {
    "title": "Predicting Speech Intelligibility using the Spike Acativity Mutual Information Index",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/barker22_interspeech.html": {
    "title": "The 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baas22_interspeech.html": {
    "title": "Voice Conversion Can Improve ASR in Very Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zevallos22_interspeech.html": {
    "title": "Data Augmentation for Low-Resource Quechua ASR Improvement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fatehi22_interspeech.html": {
    "title": "ScoutWav: Two-Step Fine-Tuning on Self-Supervised Automatic Speech Recognition for Low-Resource Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bandarupalli22_interspeech.html": {
    "title": "Semi-supervised Acoustic and Language Modeling for Hindi ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/berrebbi22_interspeech.html": {
    "title": "Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/robinson22_interspeech.html": {
    "title": "When Is TTS Augmentation Through a Pivot Language Useful?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rouhe22_interspeech.html": {
    "title": "Low Resource Comparison of Attention-based and Hybrid ASR Exploiting wav2vec 2.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bhanushali22_interspeech.html": {
    "title": "Gram Vaani ASR Challenge on spontaneous telephone speech recordings in regional variations of Hindi",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/manocha22_interspeech.html": {
    "title": "Audio Similarity is Unreliable as a Proxy for Audio Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22c_interspeech.html": {
    "title": "Overlapped Frequency-Distributed Network: Frequency-Aware Voice Spoofing Countermeasure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shrem22_interspeech.html": {
    "title": "Formant Estimation and Tracking using Probabilistic Heat-Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/eom22_interspeech.html": {
    "title": "Anti-Spoofing Using Transfer Learning with Variational Information Bottleneck",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/batra22_interspeech.html": {
    "title": "Robust Pitch Estimation Using Multi-Branch CNN-LSTM and 1-Norm LP Residual",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chernyak22_interspeech.html": {
    "title": "DeepFry: Identifying Vocal Fry Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wells22_interspeech.html": {
    "title": "Phonetic Analysis of Self-supervised Representations of English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22p_interspeech.html": {
    "title": "FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dumpala22_interspeech.html": {
    "title": "On Combining Global and Localized Self-Supervised Models of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dissanayake22_interspeech.html": {
    "title": "Self-supervised Representation Fusion for Speech and Wearable Based Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peyser22_interspeech.html": {
    "title": "Towards Disentangled Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/quintas22_interspeech.html": {
    "title": "Automatic Assessment of Speech Intelligibility using Consonant Similarity for Head and Neck Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/neijman22_interspeech.html": {
    "title": "Compensation in Verbal and Nonverbal Communication after Total Laryngectomy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/getman22_interspeech.html": {
    "title": "wav2vec2-based Speech Rating System for Children with Speech Sound Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/triantafyllopoulos22_interspeech.html": {
    "title": "Distinguishing between pre- and post-treatment in the speech of patients with chronic obstructive pulmonary disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22k_interspeech.html": {
    "title": "A Study on the Phonetic Inventory Development of Children with Cochlear Implants for 5 Years after Implantation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vitormenezes22_interspeech.html": {
    "title": "Evaluation of different antenna types and positions in a stepped frequency continuous-wave radar-based silent speech interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/abderrazek22_interspeech.html": {
    "title": "Validation of the Neuro-Concept Detector framework for the characterization of speech disorders: A comparative study including Dysarthria and Dysphonia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baumann22_interspeech.html": {
    "title": "Nonwords Pronunciation Classification in Language Development Tests for Preschool Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/benway22_interspeech.html": {
    "title": "PERCEPT-R: An Open-Access American English Child/Clinical Speech Corpus Specialized for the Audio Classification of //",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cao22b_interspeech.html": {
    "title": "Data Augmentation for End-to-end Silent Speech Recognition for Laryngectomees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kothare22_interspeech.html": {
    "title": "Statistical and clinical utility of multimodal dialogue-based speech and facial metrics for Parkinson's disease assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arco22_interspeech.html": {
    "title": "Evaluation of call centre conversations based on a high-level symbolic representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22j_interspeech.html": {
    "title": "Evoc-Learn  High quality simulation of early vocal learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/siddarth22_interspeech.html": {
    "title": "Watch Me Speak: 2D Visualization of Human Mouth during Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lesnichaia22_interspeech.html": {
    "title": "Classification of Accented English Using CNN Model Trained on Amplitude Mel-Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kang22c_interspeech.html": {
    "title": "MIM-DG: Mutual information minimization-based domain generalization for speaker verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liang22b_interspeech.html": {
    "title": "Multi-Channel Far-Field Speaker Verification with Large-Scale Ad-hoc Microphone Arrays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lyu22_interspeech.html": {
    "title": "Ant Multilingual Recognition System for OLR 2021 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22b_interspeech.html": {
    "title": "Class-Aware Distribution Alignment based Unsupervised Domain Adaptation for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22m_interspeech.html": {
    "title": "EDITnet: A Lightweight Network for Unsupervised Domain Adaptation in Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22g_interspeech.html": {
    "title": "Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22j_interspeech.html": {
    "title": "Audio Visual Multi-Speaker Tracking with Improved GCF and PMBM Filter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22s_interspeech.html": {
    "title": "The HCCL System for the NIST SRE21",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22c_interspeech.html": {
    "title": "UNet-DenseNet for Robust Far-Field Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shao22b_interspeech.html": {
    "title": "Linguistic-Acoustic Similarity Based Accent Shift for Accent Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shen22b_interspeech.html": {
    "title": "Transducer-based language embedding for spoken language identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ga_interspeech.html": {
    "title": "Oriental Language Recognition (OLR) 2021: Summary and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kang22b_interspeech.html": {
    "title": "Mixup regularization strategies for spoofing countermeasure system",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ghosh22_interspeech.html": {
    "title": "Low-resource Low-footprint Wake-word Detection using Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ding22_interspeech.html": {
    "title": "Personal VAD 2.0: Optimizing Personal Voice Activity Detection for On-Device Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fan22_interspeech.html": {
    "title": "Token-level Speaker Change Detection Using Speaker Difference and Speech Content via Continuous Integrate-and-fire",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rho22_interspeech.html": {
    "title": "NAS-VAD: Neural Architecture Search for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/larsen22_interspeech.html": {
    "title": "Adversarial Multi-Task Deep Learning for Noise-Robust Voice Activity Detection with Low Algorithmic Delay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xiao22_interspeech.html": {
    "title": "Rainbow Keywords: Efficient Incremental Learning for Online Spoken Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22e_interspeech.html": {
    "title": "Filler Word Detection and Classification: A Dataset and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kanda22_interspeech.html": {
    "title": "Streaming Multi-Talker ASR with Token-Level Serialized Output Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pesoparada22_interspeech.html": {
    "title": "pMCT: Patched Multi-Condition Training for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/novitasari22_interspeech.html": {
    "title": "Improving ASR Robustness in Noisy Condition Through VAD Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takeda22_interspeech.html": {
    "title": "Empirical Sampling from Latent Utterance-wise Evidence Model for Missing Data ASR based on Neural Encoder-Decoder Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhuang22_interspeech.html": {
    "title": "Coarse-Grained Attention Fusion With Joint Training Framework for Complex Speech Enhancement and End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22b_interspeech.html": {
    "title": "DENT-DDSP: Data-efficient noisy speech generator using differentiable digital signal processors for explicit distortion modelling and noise-robust speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22c_interspeech.html": {
    "title": "Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22d_interspeech.html": {
    "title": "Federated Self-supervised Speech Representations: Are We There Yet?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22x_interspeech.html": {
    "title": "Leveraging Real Conversational Data for Multi-Channel Continuous Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22g_interspeech.html": {
    "title": "End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bando22_interspeech.html": {
    "title": "Weakly-Supervised Neural Full-Rank Spatial Covariance Analysis for a Front-End System of Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/omalley22_interspeech.html": {
    "title": "A universally-deployable ASR frontend for joint acoustic echo cancellation, speech enhancement, and voice separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rajchetupalli22_interspeech.html": {
    "title": "Speaker conditioned acoustic modeling for multi-speaker conversational ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/das22_interspeech.html": {
    "title": "Hear No Evil: Towards Adversarial Robustness of Automatic Speech Recognition via Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22f_interspeech.html": {
    "title": "Tandem Multitask Training of Speaker Diarisation and Speech Recognition for Meeting Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/farooq22_interspeech.html": {
    "title": "Investigating the Impact of Crosslingual Acoustic-Phonetic Similarities on Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shen22_interspeech.html": {
    "title": "An Improved Deliberation Network with Text Pre-training for Code-Switching Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22e_interspeech.html": {
    "title": "CyclicAugment: Speech Data Random Augmentation with Cosine Annealing Scheduler for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nie22_interspeech.html": {
    "title": "Prompt-based Re-ranking Language Model for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22g_interspeech.html": {
    "title": "Avoid Overfitting User Specific Information in Federated Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22g_interspeech.html": {
    "title": "ASR Error Correction with Constrained Decoding on Operation Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pham22_interspeech.html": {
    "title": "Adaptive multilingual speech recognition with pretrained models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/thithuuyen22_interspeech.html": {
    "title": "Vietnamese Capitalization and Punctuation Recovery Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/futami22_interspeech.html": {
    "title": "Non-autoregressive Error Correction for CTC-based ASR with Phone-conditioned Masked LM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22x_interspeech.html": {
    "title": "reducing multilingual context confusion for end-to-end code-switching automatic speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tsunoo22_interspeech.html": {
    "title": "Residual Language Model for End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22c_interspeech.html": {
    "title": "An Empirical Study of Language Model Integration for Transducer based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22s_interspeech.html": {
    "title": "Self-Normalized Importance Sampling for Neural Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fox22_interspeech.html": {
    "title": "Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/udagawa22b_interspeech.html": {
    "title": "Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22e_interspeech.html": {
    "title": "Language-specific Characteristic Assistance for Code-switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/irino22_interspeech.html": {
    "title": "Speech intelligibility of simulated hearing loss sounds and its prediction using the Gammachirp Envelope Similarity Index (GESI)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huckvale22_interspeech.html": {
    "title": "ELO-SPHERES intelligibility prediction model for the Clarity Prediction Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22u_interspeech.html": {
    "title": "Listening with Googlears: Low-Latency Neural Multiframe Beamforming and Equalization for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/edozezario22_interspeech.html": {
    "title": "MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tan22c_interspeech.html": {
    "title": "A Deep Learning Platform for Language Education Research and Development",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jin22b_interspeech.html": {
    "title": "A VR Interactive 3D Mandarin Pronunciation Teaching Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/strom22_interspeech.html": {
    "title": "Squashed Weight Distribution for Low Bit Quantization of Deep Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hollands22_interspeech.html": {
    "title": "Evaluating the Performance of State-of-the-Art ASR Systems on Non-Native English using Corpora with Extensive Language Background Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/singla22_interspeech.html": {
    "title": "Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/roux22_interspeech.html": {
    "title": "Qualitative Evaluation of Language Model Rescoring in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/faria22_interspeech.html": {
    "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22p_interspeech.html": {
    "title": "Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoon22_interspeech.html": {
    "title": "Predicting Emotional Intensity in Political Debates via Non-verbal Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeki22d_interspeech.html": {
    "title": "Confusion Detection for Adaptive Conversational Strategies of An Oral Proficiency Assessment Interview Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gent22_interspeech.html": {
    "title": "Deep Learning for Prosody-Based Irony Classification in Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ghosh22c_interspeech.html": {
    "title": "Span Classification with Structured Information for Disfluency Detection in Spoken Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22h_interspeech.html": {
    "title": "Example-based Explanations with Adversarial Attacks for Respiratory Sound Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rennie22_interspeech.html": {
    "title": "Which Model is Best: Comparing Methods and Metrics for Automatic Laughter Detection in a Naturalistic Conversational Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dissen22_interspeech.html": {
    "title": "Self-supervised Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lepage22_interspeech.html": {
    "title": "Label-Efficient Self-Supervised Speaker Verification With Information Maximization and Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kawa22_interspeech.html": {
    "title": "Attack Agnostic Dataset: Towards Generalization and Stabilization of Audio DeepFake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cho22c_interspeech.html": {
    "title": "Non-contrastive self-supervised learning of utterance-level speech representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mohammadamini22_interspeech.html": {
    "title": "Barlow Twins self-supervised learning for robust speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/puffay22_interspeech.html": {
    "title": "Relating the fundamental frequency of speech with EEG using a dilated convolutional network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/defino22_interspeech.html": {
    "title": "Prediction of L2 speech proficiency based on multi-level linguistic features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rana22_interspeech.html": {
    "title": "The effect of increasing acoustic and linguistic complexity on auditory processing: an EEG study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22z_interspeech.html": {
    "title": "Recording and timing vocal responses in online experimentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cordero22_interspeech.html": {
    "title": "Neural correlates of acoustic and semantic cues during speech segmentation in French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meng22d_interspeech.html": {
    "title": "Evidence of Onset and Sustained Neural Responses to Isolated Phonemes from Intracranial Recordings in a Voice-based Cursor Control Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mdhaffar22_interspeech.html": {
    "title": "End-to-end model for named entity recognition from speech without paired training data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meeus22_interspeech.html": {
    "title": "Multitask Learning for Low Resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jayesh22_interspeech.html": {
    "title": "Transformer Networks for Non-Intrusive Speech Quality Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tamm22_interspeech.html": {
    "title": "Pre-trained Speech Representations as Feature Extractors for Speech Quality Assessment in Online Conferencing Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/becerra22_interspeech.html": {
    "title": "Exploring the influence of fine-tuning data on wav2vec 2.0 model for blind speech quality prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22r_interspeech.html": {
    "title": "MAESTRO: Matched Speech Text Representations through Modality Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22b_interspeech.html": {
    "title": "FiLM Conditioning with Enhanced Feature to the Transformer-based End-to-End Noisy Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ristea22_interspeech.html": {
    "title": "SepTr: Separable Transformer for Audio Spectrogram Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/horii22_interspeech.html": {
    "title": "End-to-End Spontaneous Speech Recognition Using Disfluency Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/olivier22_interspeech.html": {
    "title": "Recent improvements of ASR models in the face of adversarial attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shim22_interspeech.html": {
    "title": "Similarity and Content-based Phonetic Self Attention for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22f_interspeech.html": {
    "title": "Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22c_interspeech.html": {
    "title": "Knowledge distillation for In-memory keyword spotting model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meyer22_interspeech.html": {
    "title": "Automatic Learning of Subword Dependent Model Scales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bittar22_interspeech.html": {
    "title": "Bayesian Recurrent Units and the Forward-Backward Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mei22_interspeech.html": {
    "title": "On Metric Learning for Audio-Text Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hou22_interspeech.html": {
    "title": "CT-SAT: Contextual Transformer for Sequential Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22d_interspeech.html": {
    "title": "ADFF: Attention Based Deep Feature Fusion Approach for Music Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lei22b_interspeech.html": {
    "title": "Audio-Visual Scene Classification Based on Multi-modal Graph Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/reddy22_interspeech.html": {
    "title": "MusicNet: Compact Convolutional Neural Network for Real-time Background Music Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22h_interspeech.html": {
    "title": "iCNN-Transformer: An improved CNN-Transformer with Channel-spatial Attention and Keyword Prediction for Automated Audio Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22p_interspeech.html": {
    "title": "ATST: Audio Representation Learning with Teacher-Student Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22v_interspeech.html": {
    "title": "Deep Segment Model for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sonowal22_interspeech.html": {
    "title": "Novel Augmentation Schemes for Device Robust Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22f_interspeech.html": {
    "title": "WideResNet with Joint Representation Learning and Data Augmentation for Cover Song Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/parikh22_interspeech.html": {
    "title": "Impact of Acoustic Event Tagging on Scene Classification in a Multi-Task Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takeuchi22_interspeech.html": {
    "title": "Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pan22_interspeech.html": {
    "title": "Speaker recognition-assisted robust audio deepfake detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22_interspeech.html": {
    "title": "Preventing sensitive-word recognition using self-supervised learning to preserve user-privacy for automatic speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pia22_interspeech.html": {
    "title": "NESC: Robust Neural End-2-End Speech Coding with GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xue22b_interspeech.html": {
    "title": "Towards Error-Resilient Neural Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jiang22_interspeech.html": {
    "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22x_interspeech.html": {
    "title": "Neural Vocoder is All You Need for Speech Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22y_interspeech.html": {
    "title": "VoiceFixer: A Unified Framework for High-Fidelity Speech Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/stoidis22_interspeech.html": {
    "title": "Generating gender-ambiguous voices for privacy-preserving speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22b_interspeech.html": {
    "title": "Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhan22_interspeech.html": {
    "title": "Exploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22e_interspeech.html": {
    "title": "WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22_interspeech.html": {
    "title": "Decoupled Pronunciation and Prosody Modeling in Meta-Learning-based Multilingual Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhuang22b_interspeech.html": {
    "title": "KaraTuner: Towards End-to-End Natural Pitch Correction for Singing Voice in Karaoke",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xue22c_interspeech.html": {
    "title": "Learn2Sing 2.0: Diffusion and Mutual Information-Based Target Speaker SVS by Learning from Singing Teacher",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22e_interspeech.html": {
    "title": "SingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22d_interspeech.html": {
    "title": "Muskits: an End-to-end Music Processing Toolkit for Singing Voice Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22p_interspeech.html": {
    "title": "Pronunciation Dictionary-Free Multilingual Speech Synthesis by Combining Unsupervised and Supervised Phonetic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22u_interspeech.html": {
    "title": "Towards high-fidelity singing voice conversion with acoustic reference and contrastive predictive coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22f_interspeech.html": {
    "title": "Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/manghat22_interspeech.html": {
    "title": "Normalization of code-switched text for speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chung22_interspeech.html": {
    "title": "Synthesizing Near Native-accented Speech for a Non-native Speaker by Imitating the Pronunciation and Prosody of a Native Speaker",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22da_interspeech.html": {
    "title": "A Hierarchical Speaker Representation Framework for One-shot Singing Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22c_interspeech.html": {
    "title": "Self-Supervised Learning with Multi-Target Contrastive Coding for Non-Native Acoustic Modeling of Mispronunciation Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22_interspeech.html": {
    "title": "L2-GEN: A Neural Phoneme Paraphrasing Approach to L2 Speech Synthesis for Mispronunciation Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dutta22_interspeech.html": {
    "title": "Challenges remain in Building ASR for Spontaneous Preschool Children Speech in Naturalistic Educational Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22p_interspeech.html": {
    "title": "End-to-end Mispronunciation Detection with Simulated Error Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22q_interspeech.html": {
    "title": "BiCAPT: Bidirectional Computer-Assisted Pronunciation Training with Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fu22b_interspeech.html": {
    "title": "Using Fluency Representation Learned from Sequential Raw Features for Improving Non-native Fluency Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22l_interspeech.html": {
    "title": "An Alignment Method Leveraging Articulatory Features for Mispronunciation Detection and Diagnosis in L2 English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nidadavolu22_interspeech.html": {
    "title": "RefTextLAS: Reference Text Biased Listen, Attend, and Spell Model For Accurate Reading Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22e_interspeech.html": {
    "title": "CoCA-MDD: A Coupled Cross-Attention based Framework for Streaming Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22g_interspeech.html": {
    "title": "Spoofing-Aware Speaker Verification by Multi-Level Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kang22_interspeech.html": {
    "title": "End-to-end framework for spoof-aware speaker verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22_interspeech.html": {
    "title": "The CLIPS System for 2022 Spoofing-Aware Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22f_interspeech.html": {
    "title": "Norm-constrained Score-level Ensemble for Spoofing Aware Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22s_interspeech.html": {
    "title": "SASV Based on Pre-trained ASV System and Integrated Scoring Module",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22w_interspeech.html": {
    "title": "Backend Ensemble for Speaker Verification and Spoofing Countermeasure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tan22b_interspeech.html": {
    "title": "NRI-FGSM: An Efficient Transferable Adversarial Attack for Speaker Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/teng22_interspeech.html": {
    "title": "SA-SASV: An End-to-End Spoof-Aggregated Spoofing-Aware Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ea_interspeech.html": {
    "title": "The DKU-OPPO System for the 2022 Spoofing-Aware Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/han22_interspeech.html": {
    "title": "NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeki22b_interspeech.html": {
    "title": "SelfRemaster: Self-Supervised Speech Restoration with Analysis-by-Synthesis Approach Using Channel Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/byun22_interspeech.html": {
    "title": "Optimization of Deep Neural Network (DNN) Speech Coder Using a Multi Time Scale Perceptual Loss Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22m_interspeech.html": {
    "title": "Phase Vocoder For Time Stretch Based On Center Frequency Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/siahkoohi22_interspeech.html": {
    "title": "Ultra-Low-Bitrate Speech Coding with Pretrained Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/miao22_interspeech.html": {
    "title": "Analyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/radfar22_interspeech.html": {
    "title": "ConvRNN-T: Convolutional Augmented Recurrent Neural Network Transducers for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22f_interspeech.html": {
    "title": "Knowledge Distillation via Module Replacing for Automatic Speech Recognition with Recurrent Neural Network Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22g_interspeech.html": {
    "title": "Memory-Efficient Training of RNN-Transducer with Sampled Softmax",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/do22_interspeech.html": {
    "title": "Multiple-hypothesis RNN-T Loss for Unsupervised Fine-tuning and Self-training of Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sklyar22_interspeech.html": {
    "title": "Separator-Transducer-Segmenter: Streaming Recognition and Segmentation of Multi-party Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wong22_interspeech.html": {
    "title": "Variations of multi-task learning for spoken language assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kunihara22b_interspeech.html": {
    "title": "Detection of Learners' Listening Breakdown with Oral Dictation and Its Use to Model Listening Skill Improvement Exclusively Through Shadowing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/suzuki22b_interspeech.html": {
    "title": "Automatic Prosody Evaluation of L2 English Read Speech in Reference to Accent Dictionary with Transformer Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/banno22_interspeech.html": {
    "title": "View-Specific Assessment of L2 Spoken English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bai22b_interspeech.html": {
    "title": "The Effects of Implicit and Explicit Feedback in an ASR-based Reading Tutor for Dutch First-graders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22v_interspeech.html": {
    "title": "Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sakuma22_interspeech.html": {
    "title": "Response Timing Estimation for Spoken Dialog System using Dialog Act Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jabeen22_interspeech.html": {
    "title": "Hesitations in Urdu/Hindi: Distribution and Properties of Fillers & Silences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/girish22_interspeech.html": {
    "title": "Interpretabilty of Speech Emotion Recognition modelled using Self-Supervised Speech and Text Pre-Trained Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kumar22b_interspeech.html": {
    "title": "Does Utterance entails Intent?: Evaluating Natural Language Inference Based Setup for Few-Shot Intent Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wallbridge22_interspeech.html": {
    "title": "Investigating perception of spoken dialogue acceptability through surprisal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hori22_interspeech.html": {
    "title": "Low-Latency Online Streaming VideoQA Using Audio-Visual Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/stan22_interspeech.html": {
    "title": "The ZevoMOS entry to VoiceMOS Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeki22c_interspeech.html": {
    "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22b_interspeech.html": {
    "title": "Automatic Mean Opinion Score Estimation with Temporal Modulation Features on Gammatone Filterbank for Speech Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chinen22_interspeech.html": {
    "title": "Using Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22f_interspeech.html": {
    "title": "The VoiceMOS Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tseng22b_interspeech.html": {
    "title": "DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ammarabbas22_interspeech.html": {
    "title": "Expressive, Variable, and Controllable Duration Modelling in TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nakata22_interspeech.html": {
    "title": "Predicting VQVAE-based Character Acting Style from Quotation-Annotated Text for Audiobook Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22g_interspeech.html": {
    "title": "Adversarial and Sequential Training for Cross-lingual Prosody Transfer TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22j_interspeech.html": {
    "title": "FluentTTS: Text-dependent Fine-grained Style Control for Multi-style TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22g_interspeech.html": {
    "title": "Few Shot Cross-Lingual TTS Using Transferable Phoneme Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/finkelstein22_interspeech.html": {
    "title": "Training Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoshioka22_interspeech.html": {
    "title": "Spoken-Text-Style Transfer with Conditional Variational Autoencoder and Content Word Storage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kulkarni22_interspeech.html": {
    "title": "Analysis of expressivity transfer in non-autoregressive end-to-end multispeaker TTS systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rattcliffe22_interspeech.html": {
    "title": "Cross-lingual Style Transfer with Conditional Prior VAE and Style Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zaidi22b_interspeech.html": {
    "title": "Daft-Exprt: Cross-Speaker Prosody Transfer on Any Text for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoon22b_interspeech.html": {
    "title": "Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mukherjee22_interspeech.html": {
    "title": "Text aware Emotional Text-to-speech with BERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mateju22_interspeech.html": {
    "title": "Overlapped Speech Detection in Broadcast Streams Using X-vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/segal22_interspeech.html": {
    "title": "DDKtor: Automatic Diadochokinetic Speech Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meneses22_interspeech.html": {
    "title": "SiDi KWS: A Large-Scale Multilingual Dataset for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22h_interspeech.html": {
    "title": "Dummy Prototypical Networks for Few-Shot Open-Set Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sarkar22_interspeech.html": {
    "title": "Unsupervised Voice Activity Detection by Modeling Source and System Information using Zero Frequency Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sharon22_interspeech.html": {
    "title": "Multilingual and Multimodal Abuse Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mariotte22_interspeech.html": {
    "title": "Microphone Array Channel Combination Algorithms for Overlapped Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sudo22_interspeech.html": {
    "title": "Streaming Automatic Speech Recognition with Re-blocking Processing Based on Integrated Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fuchs22_interspeech.html": {
    "title": "Unsupervised Word Segmentation using K Nearest Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22d_interspeech.html": {
    "title": "Investigation on the Band Importance of Phase-aware Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sun22b_interspeech.html": {
    "title": "Unsupervised Acoustic-to-Articulatory Inversion with Variable Vocal Tract Anatomy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sun22c_interspeech.html": {
    "title": "Unsupervised Inference of Physiologically Meaningful Articulatory Trajectories with VocalTractLab",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22i_interspeech.html": {
    "title": "Radio2Speech: High Quality Speech Recovery from Radio Frequency Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nabe22_interspeech.html": {
    "title": "Isochronous is beautiful? Syllabic event detection in a neuro-inspired oscillatory model is facilitated by isochrony in speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liebig22_interspeech.html": {
    "title": "An investigation of regression-based prediction of the femininity or masculinity in speech of transgender people",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/parikh22b_interspeech.html": {
    "title": "Acoustic To Articulatory Speech Inversion Using Multi-Resolution Spectro-Temporal Representations Of Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lian22b_interspeech.html": {
    "title": "Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22ea_interspeech.html": {
    "title": "Vocal-Tract Area Functions with Articulatory Reality for Tract Opening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22_interspeech.html": {
    "title": "Coupled Discriminant Subspace Alignment for Cross-database Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/santoso22_interspeech.html": {
    "title": "Performance Improvement of Speech Emotion Recognition by Neutral Speech Detection Using Autoencoder and Intermediate Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22c_interspeech.html": {
    "title": "A Graph Isomorphism Network with Weighted Multiple Aggregators for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baruah22_interspeech.html": {
    "title": "Speech Emotion Recognition via Generation using an Attention-based Variational Recurrent Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mitra22_interspeech.html": {
    "title": "Speech Emotion: Investigating Model Representations, Multi-Task Learning and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22e_interspeech.html": {
    "title": "Multiple Enhancements to LSTM for Learning Emotion-Salient Features in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22k_interspeech.html": {
    "title": "Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22m_interspeech.html": {
    "title": "CTA-RNN: Channel and Temporal-wise Attention RNN leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/velichko22_interspeech.html": {
    "title": "Complex Paralinguistic Analysis of Speech: Predicting Gender, Emotions and Deception in a Hierarchical Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takashima22b_interspeech.html": {
    "title": "Interactive Co-Learning with Cross-Modal Transformer for Audio-Visual Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kang22d_interspeech.html": {
    "title": "SpeechEQ: Speech Emotion Recognition based on Multi-scale Unified Datasets and Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22aa_interspeech.html": {
    "title": "Discriminative Feature Representation Based on Cascaded Attention Network with Adversarial Joint Loss for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/audibert22_interspeech.html": {
    "title": "Intra-speaker phonetic variation in read speech: comparison with inter-speaker variability in a controlled population",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vaessen22_interspeech.html": {
    "title": "Training speaker recognition systems with limited data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lou22_interspeech.html": {
    "title": "A Deep One-Class Learning Method for Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22e_interspeech.html": {
    "title": "A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22h_interspeech.html": {
    "title": "A Novel Phoneme-based Modeling for Text-independent Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/han22b_interspeech.html": {
    "title": "Self-Supervised Speaker Verification Using Dynamic Loss-Gate and Label Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22c_interspeech.html": {
    "title": "Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22r_interspeech.html": {
    "title": "Acoustic Feature Shuffling Network for Text-independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wen22_interspeech.html": {
    "title": "Multi-Path GMM-MobileNet Based on Attack Algorithms and Codecs for Synthetic Speech and Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jin22_interspeech.html": {
    "title": "Adversarial Reweighting for Speaker Verification Fairness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22s_interspeech.html": {
    "title": "Graph-based Multi-View Fusion and Local Adaptation: Mitigating Within-Household Confusability for Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zuo22_interspeech.html": {
    "title": "Local Context-aware Self-attention for Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weise22_interspeech.html": {
    "title": "Disentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22_interspeech.html": {
    "title": "Improving Hypernasality Estimation with Automatic Speech Recognition in Cleft Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22k_interspeech.html": {
    "title": "Conformer Based Elderly Speech Recognition System for Alzheimer's Disease Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fan22c_interspeech.html": {
    "title": "Revisiting visuo-spatial processing in individuals with congenital amusia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/aminidigehsara22_interspeech.html": {
    "title": "A user-friendly headset for radar-based silent speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cheng22c_interspeech.html": {
    "title": "A study of production error analysis for Mandarin-speaking Children with Hearing Impairment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huo22_interspeech.html": {
    "title": "Incremental Layer-Wise Self-Supervised Learning for Efficient Unsupervised Speech Domain Adaptation On Device",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/farooq22b_interspeech.html": {
    "title": "Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22c_interspeech.html": {
    "title": "The THUEE System Description for the IARPA OpenASR21 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhong22_interspeech.html": {
    "title": "External Text Based Data Augmentation for Low-Resource Speech Recognition in the Constrained Condition of OpenASR21 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lonergan22_interspeech.html": {
    "title": "Cross-dialect lexicon optimisation for an endangered language ASR system: the case of Irish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22c_interspeech.html": {
    "title": "Wav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schraner22_interspeech.html": {
    "title": "Comparison of Unsupervised Learning and Supervised Learning with Noisy Labels for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/patel22_interspeech.html": {
    "title": "Using cross-model learnings for the Gram Vaani ASR Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22aa_interspeech.html": {
    "title": "ASR2K: Speech Recognition for Around 2000 Languages without Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/damania22_interspeech.html": {
    "title": "Combining Simple but Novel Data Augmentation Methods for Improving Conformer ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peterson22_interspeech.html": {
    "title": "OpenASR21: The Second Open Challenge for Automatic Speech Recognition of Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fan22d_interspeech.html": {
    "title": "DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised Learning and Its Application to Children's ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guillaume22_interspeech.html": {
    "title": "Plugging a neural phoneme recognizer into a simple language model: a workflow for low-resource setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22e_interspeech.html": {
    "title": "An Evaluation of Three-Stage Voice Conversion Framework for Noisy and Reverberant Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22t_interspeech.html": {
    "title": "An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/quamer22_interspeech.html": {
    "title": "Zero-Shot Foreign Accent Conversion without a Native Reference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meyer22b_interspeech.html": {
    "title": "Speaker Anonymization with Phonetic Intermediate Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kuhlmann22_interspeech.html": {
    "title": "Investigation into Target Speaking Rate Adaptation for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/klapsas22_interspeech.html": {
    "title": "Self supervised learning for robust voice cloning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22_interspeech.html": {
    "title": "Improving Deliberation by Text-Only and Semi-Supervised Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22e_interspeech.html": {
    "title": "K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of Graphemes and Syllables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sriram22_interspeech.html": {
    "title": "Wav2Vec-Aug: Improved self-supervised training with limited data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kocour22_interspeech.html": {
    "title": "Revisiting joint decoding based multi-talker speech recognition with DNN acoustic model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/novak22_interspeech.html": {
    "title": "RNN-T lattice enhancement by grafting of pruned paths",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/komatsu22_interspeech.html": {
    "title": "Better Intermediates Improve CTC Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gessinger22_interspeech.html": {
    "title": "Cross-Cultural Comparison of Gradient Emotion Perception: Human vs. Alexa TTS Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kasun22_interspeech.html": {
    "title": "Discriminative Adversarial Learning for Speaker Independent Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/suzuki22_interspeech.html": {
    "title": "Representing 'how you say' with 'what you say': English corpus of focused speech and text reflecting corresponding implications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/salais22_interspeech.html": {
    "title": "Production Strategies of Vocal Attitudes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kirkland22_interspeech.html": {
    "title": "Where's the uh, hesitation? The interplay between filled pause location, speech rate and fundamental frequency in perception of confidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22_interspeech.html": {
    "title": "E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yeh22_interspeech.html": {
    "title": "Autoregressive Co-Training for Learning Discrete Speech Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22e_interspeech.html": {
    "title": "An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lebourdais22_interspeech.html": {
    "title": "Overlapped speech and gender detection with WavLM pre-trained features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/teytaut22_interspeech.html": {
    "title": "A study on constraining Connectionist Temporal Classification for temporal audio alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/siriwardena22_interspeech.html": {
    "title": "Acoustic-to-articulatory Speech Inversion with Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/maouche22_interspeech.html": {
    "title": "Enhancing Speech Privacy with Slicing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22h_interspeech.html": {
    "title": "An Attention-Based Method for Guiding Attribute-Aligned Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/joshi22_interspeech.html": {
    "title": "Defense against Adversarial Attacks on Hybrid Speech Recognition System using Adversarial Fine-tuning with Denoiser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tseng22_interspeech.html": {
    "title": "Membership Inference Attacks Against Self-supervised Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shao22c_interspeech.html": {
    "title": "Chunking Defense for Adversarial Attacks on ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/feng22_interspeech.html": {
    "title": "Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/feng22b_interspeech.html": {
    "title": "User-Level Differential Privacy against Attribute Inference Attack of Speech Emotion Recognition on Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/joshi22b_interspeech.html": {
    "title": "AdvEst: Adversarial Perturbation Estimation to Classify and Detect Adversarial Attacks against Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoo22_interspeech.html": {
    "title": "Online Learning of Open-set Speaker Identification by Active User-registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/salim22_interspeech.html": {
    "title": "Automatic Speaker Verification System for Dysarthria Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/flemotomos22_interspeech.html": {
    "title": "Multimodal Clustering with Role Induced Constraints for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22d_interspeech.html": {
    "title": "Multi-scale Speaker Diarization with Dynamic Scale Weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chaubey22_interspeech.html": {
    "title": "Improved Relation Networks for End-to-End Speaker Verification and Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rybicka22_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with an Iterative Refinement of Non-Autoregressive Attention-based Attractors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/landini22_interspeech.html": {
    "title": "From Simulated Mixtures to Simulated Conversations as Training Data for End-to-End Neural Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ide22_interspeech.html": {
    "title": "Can Humans Correct Errors From System? Investigating Error Tendencies in Speaker Identification Using Crowdsourcing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22n_interspeech.html": {
    "title": "Light-Weight Speaker Verification with Global Context Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22e_interspeech.html": {
    "title": "Learnable Sparse Filterbank for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sapru22_interspeech.html": {
    "title": "Using Data Augmentation and Consistency Regularization to Improve Semi-supervised Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mai22_interspeech.html": {
    "title": "Unsupervised domain adaptation for speech recognition with unsupervised error correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/biadsy22_interspeech.html": {
    "title": "A Scalable Model Specialization Framework for Training and Inference using Submodels and its Application to Speech Model Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dieck22_interspeech.html": {
    "title": "Wav2vec behind the Scenes: How end2end Models learn Phonetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22d_interspeech.html": {
    "title": "Scaling ASR Improves Zero and Few Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nakagome22_interspeech.html": {
    "title": "InterAug: Augmenting Noisy Intermediate Predictions for CTC-based ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arunkumar22b_interspeech.html": {
    "title": "Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22b_interspeech.html": {
    "title": "Dynamic Sliding Window Modeling for Abstractive Meeting Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saito22_interspeech.html": {
    "title": "STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rumberg22_interspeech.html": {
    "title": "kidsTALC: A Corpus of 3- to 11-year-old German Children's Connected Natural Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22c_interspeech.html": {
    "title": "DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jung22b_interspeech.html": {
    "title": "Asymmetric Proxy Loss for Multi-View Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22f_interspeech.html": {
    "title": "Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22c_interspeech.html": {
    "title": "Building Vietnamese Conversational Smart Home Dataset and Natural Language Understanding Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ghosh22b_interspeech.html": {
    "title": "DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ekstedt22_interspeech.html": {
    "title": "Voice Activity Projection: Self-supervised Learning of Turn-taking Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/popuri22_interspeech.html": {
    "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22l_interspeech.html": {
    "title": "QbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using MLPMixer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gharbieh22_interspeech.html": {
    "title": "DyConvMixer: Dynamic Convolution Mixer Architecture for Open-Vocabulary Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/belitz22_interspeech.html": {
    "title": "Challenges in Metadata Creation for Massive Naturalistic Team-Based Audio Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nicmanis22_interspeech.html": {
    "title": "Spoken Dialogue System for Call Centers with Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/draxler22_interspeech.html": {
    "title": "OCTRA  An Innovative Approach to Orthographic Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vandevreken22_interspeech.html": {
    "title": "Voice Puppetry with FastPitch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/paul22_interspeech.html": {
    "title": "Improving Data Driven Inverse Text Normalization using Data Augmentation and Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22_interspeech.html": {
    "title": "Native phonotactic interference in L2 vowel processing: Mouse-tracking reveals cognitive conflicts during identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luo22b_interspeech.html": {
    "title": "Mandarin nasal place assimilation revisited: an acoustic study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kaland22_interspeech.html": {
    "title": "Bending the string: intonation contour length as a correlate of macro-rhythm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hughes22_interspeech.html": {
    "title": "Eliciting and evaluating likelihood ratios for speaker recognition by human listeners under forensically realistic channel-mismatched conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22d_interspeech.html": {
    "title": "Reducing uncertainty at the score-to-LR stage in likelihood ratio-based forensic voice comparison using automatic speaker recognition systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22f_interspeech.html": {
    "title": "Durational Patterning at Discourse Boundaries in Relation to Therapist Empathy in Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kadiri22_interspeech.html": {
    "title": "Convolutional Neural Networks for Classification of Voice Qualities from Speech and Neck Surface Accelerometer Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/furukawa22_interspeech.html": {
    "title": "Applying SyntaxProsody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22u_interspeech.html": {
    "title": "Effects of Language Contact on Vowel Nasalization in Wenzhou and Rugao Dialects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/young22_interspeech.html": {
    "title": "A blueprint for using deepfakes in sociolinguistic matched-guise experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22e_interspeech.html": {
    "title": "Mandarin Tone Sandhi Realization: Evidence from Large Speech Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/le22b_interspeech.html": {
    "title": "A Laryngographic Study on the Voice Quality of Northern Vietnamese Tones under the Lombard Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zygis22_interspeech.html": {
    "title": "The Prosody of Cheering in Sport Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ca_interspeech.html": {
    "title": "Contribution of the glottal flow residual in affect-related voice transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/carne22_interspeech.html": {
    "title": "High level feature fusion in forensic voice comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/begus22_interspeech.html": {
    "title": "Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jun22_interspeech.html": {
    "title": "Paraguayan Guarani: Tritonal pitch accent and Accentual Phrase",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zeng22b_interspeech.html": {
    "title": "Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luo22_interspeech.html": {
    "title": "Tiny-Sepformer: A Tiny Time-Domain Transformer Network For Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22_interspeech.html": {
    "title": "Speaker-Aware Mixture of Mixtures Training for Weakly Supervised Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lutati22_interspeech.html": {
    "title": "SepIt: Approaching a Single Channel Speech Separation Bound",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22d_interspeech.html": {
    "title": "On the Use of Deep Mask Estimation Module for Neural Source Separation Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22b_interspeech.html": {
    "title": "Target Confusion in End-to-end Speaker Extraction: Analysis and Approaches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22d_interspeech.html": {
    "title": "Embedding Recurrent Layers with Dual-Path Strategy in a Variant of Convolutional Network for Speaker-Independent Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22c_interspeech.html": {
    "title": "Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ivry22_interspeech.html": {
    "title": "Objective Metrics to Evaluate Residual-Echo Suppression During Double-Talk in the Stereophonic Case",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rixen22_interspeech.html": {
    "title": "QDPN - Quasi-dual-path Network for single-channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lu22b_interspeech.html": {
    "title": "Conformer Space Neural Architecture Search for Multi-Task Audio Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kopuklu22_interspeech.html": {
    "title": "ResectNet: An Efficient Architecture for Voice Activity Detection on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22l_interspeech.html": {
    "title": "Gated Convolutional Fusion for Time-Domain Target Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22p_interspeech.html": {
    "title": "WA-Transformer: Window Attention-based Transformer with Two-stage Strategy for Multi-task Audio Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/quan22b_interspeech.html": {
    "title": "Multichannel Speech Separation with Narrow-band Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22y_interspeech.html": {
    "title": "Separating Long-Form Speech with Group-wise Permutation Invariant Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/paturi22_interspeech.html": {
    "title": "Directed speech separation for automatic speech recognition of long form conversational speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chetupalli22_interspeech.html": {
    "title": "Speech Separation for an Unknown Number of Speakers Using Transformers With Encoder-Decoder Attractors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/corey22_interspeech.html": {
    "title": "Cooperative Speech Separation With a Microphone Array and Asynchronous Wearable Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kilgour22_interspeech.html": {
    "title": "Text-Driven Separation of Arbitrary Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/parikh22c_interspeech.html": {
    "title": "An Empirical Analysis on the Vulnerabilities of End-to-End Speech Segregation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22c_interspeech.html": {
    "title": "TaylorBeamformer: Learning All-Neural Beamformer for Multi-Channel Speech Enhancement from Taylor's Approximation Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/iwamoto22_interspeech.html": {
    "title": "How bad are artifacts?: Analyzing the impact of speech enhancement errors on ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22_interspeech.html": {
    "title": "Multi-source wideband DOA estimation method by frequency focusing and error weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nossier22_interspeech.html": {
    "title": "Convolutional Recurrent Smart Speech Enhancement Architecture for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meng22b_interspeech.html": {
    "title": "Fully Automatic Balance between Directivity Factor and White Noise Gain for Large-scale Microphone Arrays in Diffuse Noise Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22d_interspeech.html": {
    "title": "A Transfer and Multi-Task Learning based Approach for MOS Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22o_interspeech.html": {
    "title": "Fusion of Self-supervised Learned Models for MOS Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chao22_interspeech.html": {
    "title": "Perceptual Contrast Stretching on Target Feature for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/geng22_interspeech.html": {
    "title": "A speech enhancement method for long-range speech acquisition task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lu22c_interspeech.html": {
    "title": "ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zezario22_interspeech.html": {
    "title": "MTI-Net: A Multi-Target Speech Intelligibility Prediction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bu22_interspeech.html": {
    "title": "Steering vector correction in MVDR beamformer for speech enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saba22_interspeech.html": {
    "title": "Speech Modification for Intelligibility in Cochlear Implant Listeners: Individual Effects of Vowel- and Consonant-Boosting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jigang22_interspeech.html": {
    "title": "DCTCN:Deep Complex Temporal Convolutional Network for Long Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22l_interspeech.html": {
    "title": "Improve Speech Enhancement using Perception-High-Related Time-Frequency Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fernandez22_interspeech.html": {
    "title": "Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22i_interspeech.html": {
    "title": "Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22h_interspeech.html": {
    "title": "Cross-speaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22e_interspeech.html": {
    "title": "Self-supervised Context-aware Style Representation for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22m_interspeech.html": {
    "title": "Integrating Discrete Word-Level Style Variations into Non-Autoregressive Acoustic Models for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dai22_interspeech.html": {
    "title": "Automatic Prosody Annotation with Pre-Trained Text-Speech Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22e_interspeech.html": {
    "title": "Enhancing Word-Level Semantic Representation via Dependency Structure for Expressive Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lei22c_interspeech.html": {
    "title": "Towards Multi-Scale Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22ca_interspeech.html": {
    "title": "Towards Cross-speaker Reading Style Transfer on Audiobook Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meng22c_interspeech.html": {
    "title": "CALM: Constrastive Cross-modal Speaking Style Modeling for Expressive Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/he22d_interspeech.html": {
    "title": "Improve emotional speech synthesis quality by learning explicit and implicit representations with semi-supervised training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22e_interspeech.html": {
    "title": "A Vietnamese-English Neural Machine Translation System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}