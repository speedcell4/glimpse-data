{
  "https://www.isca-speech.org/archive/interspeech_2022/cho22_interspeech.html": {
    "title": "SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech",
    "volume": "main",
    "abstract": "In this paper, we present SANE-TTS, a stable and natural end-to-end multilingual TTS model. By the difficulty of obtaining multilingual corpus for given speaker, training multilingual TTS model with monolingual corpora is unavoidable. We introduce speaker regularization loss that improves speech naturalness during cross-lingual synthesis as well as domain adversarial training, which is applied in other multilingual TTS models. Furthermore, by adding speaker regularization loss, replacing speaker embedding with zero vector in duration predictor stabilizes cross-lingual inference. With this replacement, our model generates speeches with moderate rhythm regardless of source speaker in cross-lingual synthesis. In MOS evaluation, SANE-TTS achieves naturalness score above 3.80 both in cross-lingual and intralingual synthesis, where the ground truth score is 3.99. Also, SANE-TTS maintains speaker similarity close to that of ground truth even in cross-lingual inference. Audio samples are available on our web page",
    "checked": true,
    "id": "3ac674d9ea88fe1fb9dcd4031e4f50c3ecaa3ab9",
    "semantic_title": "sane-tts: stable and natural end-to-end multilingual text-to-speech",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bae22_interspeech.html": {
    "title": "Enhancement of Pitch Controllability using Timbre-Preserving Pitch Augmentation in FastPitch",
    "volume": "main",
    "abstract": "The recently developed pitch-controllable text-to-speech (TTS) model, i.e. FastPitch, was conditioned for the pitch contours. However, the quality of the synthesized speech degraded considerably for pitch values that deviated significantly from the average pitch; i.e. the ability to control vocal pitch was limited. To address this issue, we propose two algorithms to improve the robustness of FastPitch. First, we propose a novel timbre-preserving pitch-shifting algorithm for natural pitch augmentation. Pitch-shifted speech samples sound more natural when using the proposed algorithm because the speaker's vocal timbre is maintained. Moreover, we propose a training algorithm that defines FastPitch using pitch-augmented speech datasets with different pitch ranges for the same sentence. The experimental results demonstrate that the proposed algorithms improve the pitch controllability of FastPitch",
    "checked": true,
    "id": "c1a6a70774b675f55a830b72137c4cbac9b91f11",
    "semantic_title": "enhancement of pitch controllability using timbre-preserving pitch augmentation in fastpitch",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lenglet22_interspeech.html": {
    "title": "Speaking Rate Control of end-to-end TTS Models by Direct Manipulation of the Encoder's Output Embeddings",
    "volume": "main",
    "abstract": "Since neural Text-To-Speech models have achieved such high standards in terms of naturalness, the main focus of the field has gradually shifted to gaining more control over the expressiveness of the synthetic voices. One of these leverages is the control of the speaking rate that has become harder for a human operator to control since the introduction of neural attention networks to model speech dynamics. While numerous models have reintroduced an explicit duration control (ex: FastSpeech2), these models generally rely on additional tasks to complete during their training. In this paper, we show how an acoustic analysis of the internal embeddings delivered by the encoder of an unsupervised end-to-end TTS Tacotron2 model is enough to identify and control some acoustic parameters of interest. Specifically, we compare this speaking rate control with the duration control offered by a supervised FastSpeech2 model. Experimental results show that the control provided by embeddings reproduces a behaviour closer to natural speech data",
    "checked": true,
    "id": "bfc440698d636f521bb693d69526bcc922375135",
    "semantic_title": "speaking rate control of end-to-end tts models by direct manipulation of the encoder's output embeddings",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ju22_interspeech.html": {
    "title": "TriniTTS: Pitch-controllable End-to-end TTS without External Aligner",
    "volume": "main",
    "abstract": "Three research directions that have recently advanced the text-to-speech (TTS) field are end-to-end architecture, prosody control modeling, and on-the-fly duration alignment of non-auto-regressive models. However, these three agendas have yet to be tackled at once in a single solution. Current studies are limited either by a lack of control over prosody modeling or by the inefficient training inherent in building a two-stage TTS pipeline. We propose TriniTTS, a pitch-controllable end-to-end TTS without an external aligner that generates natural speech by addressing the issues mentioned above at once. It eliminates the training inefficiency in the two-stage TTS pipeline by the end-to-end architecture. Moreover, it manages to learn the latent vector representing the data distribution of the speeches through performing tasks (alignment search, pitch estimation, waveform generation) simultaneously. Experimental results demonstrate that TriniTTS enables prosody modeling with user input parameters to generate deterministic speech, while synthesizing comparable speech to the state-of-the-art VITS. Furthermore, eliminating normalizing flow modules used in VITS increases the inference speed by 28.84% in CPU environment and by 29.16% in GPU environment",
    "checked": true,
    "id": "a501ec0d907a508ef7caf84801f70af287b125a9",
    "semantic_title": "trinitts: pitch-controllable end-to-end tts without external aligner",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lim22_interspeech.html": {
    "title": "JETS: Jointly Training FastSpeech2 and HiFi-GAN for End to End Text to Speech",
    "volume": "main",
    "abstract": "In neural text-to-speech (TTS), two-stage system or a cascade of separately learned models have shown synthesis quality close to human speech. For example, FastSpeech2 transforms an input text to a mel-spectrogram and then HiFi-GAN generates a raw waveform from a mel-spectogram where they are called an acoustic feature generator and a neural vocoder respectively. However, their training pipeline is somewhat cumbersome in that it requires a fine-tuning and an accurate speech-text alignment for optimal performance. In this work, we present end-to-end text-to-speech (E2E-TTS) model which has a simplified training pipeline and outperforms a cascade of separately learned models. Specifically, our proposed model is jointly trained FastSpeech2 and HiFi-GAN with an alignment module. Since there is no acoustic feature mismatch between training and inference, it does not requires fine-tuning. Furthermore, we remove dependency on an external speech-text alignment tool by adopting an alignment learning objective in our joint training framework. Experiments on LJSpeech corpus shows that the proposed model outperforms publicly available, state-of-the-art implementations of ESPNet2-TTS on subjective evaluation (MOS) and some objective evaluations",
    "checked": true,
    "id": "3f0e0b556a04660a2333cfc768953e44ad76986e",
    "semantic_title": "jets: jointly training fastspeech2 and hifi-gan for end to end text to speech",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2022/turrisi22_interspeech.html": {
    "title": "Interpretable dysarthric speaker adaptation based on optimal-transport",
    "volume": "main",
    "abstract": "This work addresses the mismatch problem between the distribution of training data (source) and testing data (target), in the challenging context of dysarthric speech recognition. We focus on Speaker Adaptation (SA) in command speech recognition, where data from multiple sources (i.e., multiple speakers) are available. Specifically, we propose an unsupervised Multi-Source Domain Adaptation (MSDA) algorithm based on optimal-transport, called MSDA via Weighted Joint Optimal Transport (MSDA-WJDOT). We achieve a Command Error Rate relative reduction of 16% and 7% over the speaker-independent model and the best competitor method, respectively. The strength of the proposed approach is that, differently from any other existing SA method, it offers an interpretable model that can also be exploited, in this context, to diagnose dysarthria without any specific training. Indeed, it provides a closeness measure between the target and the source speakers, reflecting their similarity in terms of speech characteristics. Based on the similarity between the target speaker and the healthy/dysarthric source speakers, we then define the healthy/dysarthric score of the target speaker that we leverage to perform dysarthria detection. This approach does not require any additional training and achieves a 95% accuracy in the dysarthria diagnosis",
    "checked": true,
    "id": "e4274ce9a61feac852051f7b9dd0caadebf32c6f",
    "semantic_title": "interpretable dysarthric speaker adaptation based on optimal-transport",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yue22_interspeech.html": {
    "title": "Dysarthric Speech Recognition From Raw Waveform with Parametric CNNs",
    "volume": "main",
    "abstract": "Raw waveform acoustic modelling has recently received increasing attention. Compared with the task-blind hand-crafted features which may discard useful information, representations directly learned from the raw waveform are task-specific and potentially include all task-relevant information. In the context of automatic dysarthric speech recognition (ADSR), raw waveform acoustic modelling is under-explored owing to data scarcity. Parametric convolutional neural networks (CNNs) can compensate for this problem due to having notably fewer parameters and requiring less training data in comparison with conventional non-parametric CNNs. In this paper, we explore the usefulness of raw waveform acoustic modelling using various parametric CNNs for ADSR. We investigate the properties of the learned filters and monitor the training dynamics of various models. Furthermore, we study the effectiveness of data augmentation and multi-stream acoustic modelling through combining the non-parametric and parametric CNNs fed by hand-crafted and raw waveform features. Experimental results on the TORGO dysarthric database show that the parametric CNNs significantly outperform the non-parametric CNNs, reaching up to 36.2% and 12.6% WERs (up to 3.4% and 1.1% absolute error reduction) for dysarthric and typical speech, respectively. Multi-stream acoustic modelling further improves the performance resulting in up to 33.2% and 10.3% WERs for dysarthric and typical speech, respectively",
    "checked": true,
    "id": "b1920c26f9af25455711372e61dce209078e8400",
    "semantic_title": "dysarthric speech recognition from raw waveform with parametric cnns",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/prananta22_interspeech.html": {
    "title": "The Effectiveness of Time Stretching for Enhancing Dysarthric Speech for Improved Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "In this paper, we investigate several existing and a new state-of-the-art generative adversarial network-based (GAN) voice conversion method for enhancing dysarthric speech for improved dysarthric speech recognition. We compare key components of existing methods as part of a rigorous ablation study to find the most effective solution to improve dysarthric speech recognition. We find that straightforward signal processing methods such as stationary noise removal and vocoder-based time stretching lead to dysarthric speech recognition results comparable to those obtained when using state-of-the-art GAN-based voice conversion methods as measured using a phoneme recognition task. Additionally, our proposed solution of a combination of MaskCycleGAN-VC and time stretching is able to improve the phoneme recognition results for certain dysarthric speakers compared to our time stretched baseline",
    "checked": true,
    "id": "d56dce24a8e32aaed720dab7a685315db7a472b8",
    "semantic_title": "the effectiveness of time stretching for enhancing dysarthric speech for improved dysarthric speech recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/violeta22_interspeech.html": {
    "title": "Investigating Self-supervised Pretraining Frameworks for Pathological Speech Recognition",
    "volume": "main",
    "abstract": "We investigate the performance of self-supervised pretraining frameworks on pathological speech datasets used for automatic speech recognition (ASR). Modern end-to-end models require thousands of hours of data to train well, but only a small number of pathological speech datasets are publicly available. A proven solution to this problem is by first pretraining the model on a huge number of healthy speech datasets and then fine-tuning it on the pathological speech datasets. One new pretraining framework called self-supervised learning (SSL) trains a network using only speech data, providing more flexibility in training data requirements and allowing more speech data to be used in pretraining. We investigate SSL frameworks such as the wav2vec 2.0 and WavLM models using different setups and compare their performance with different supervised pretraining setups, using two types of pathological speech, namely, Japanese electrolaryngeal and English dysarthric. Our results show that although SSL has shown success with minimally resourced healthy speech, we do not find this to be the case with pathological speech. The best supervised setup outperforms the best SSL setup by 13.9% character error rate in electrolaryngeal speech and 16.8% word error rate in dysarthric speech",
    "checked": true,
    "id": "519fe656d67f6d3d154a7b5b81d798b03a3ac5a4",
    "semantic_title": "investigating self-supervised pretraining frameworks for pathological speech recognition",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bhat22_interspeech.html": {
    "title": "Improved ASR Performance for Dysarthric Speech Using Two-stage DataAugmentation",
    "volume": "main",
    "abstract": "Machine learning (ML) and Deep Neural Networks (DNN) have greatly aided the problem of Automatic Speech Recognition (ASR). However, accurate ASR for dysarthric speech remains a serious challenge. Dearth of usable data remains a problem in applying ML and DNN techniques for dysarthric speech recognition. In the current research, we address this challenge using a novel two-stage data augmentation scheme, a combination of static and dynamic data augmentation techniques that are designed by leveraging an understanding of the characteristics of dysarthric speech. Deep Autoencoder (DAE)-based healthy speech modification and various perturbations comprise static augmentations, whereas SpecAugment techniques modified to specifically augment dysarthric speech comprise the dynamic data augmentation. The objective of this work is to improve the ASR performance for dysarthric speech using the two-stage data augmentation scheme. An end-to-end ASR using a Transformer acoustic model is used to evaluate the data augmentation scheme on speech from the UA dysarthric speech corpus. We achieve an absolute improvement of 16% in word error rate (WER) over a baseline with no augmentation, with a final WER of 20.6%",
    "checked": true,
    "id": "4cc550a6ac45b9e2643f70ab3a0c71ed42c621cf",
    "semantic_title": "improved asr performance for dysarthric speech using two-stage dataaugmentation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hernandez22_interspeech.html": {
    "title": "Cross-lingual Self-Supervised Speech Representations for Improved Dysarthric Speech Recognition",
    "volume": "main",
    "abstract": "State-of-the-art automatic speech recognition (ASR) systems perform well on healthy speech. However, the performance on impaired speech still remains an issue. The current study explores the usefulness of using Wav2Vec self-supervised speech representations as features for training an ASR system for dysarthric speech. Dysarthric speech recognition is particularly difficult as several aspects of speech such as articulation, prosody and phonation can be impaired. Specifically, we train an acoustic model with features extracted from Wav2Vec, Hubert, and the cross-lingual XLSR model. Results suggest that speech representations pretrained on large unlabelled data can improve word error rate (WER) performance. In particular, features from the multilingual model led to lower WERs than Fbanks or models trained on a single language. Improvements were seen in English speakers with cerebral palsy caused dysarthria (UASpeech corpus), Spanish speakers with Parkinsonian dysarthria (PC-GITA corpus) and Italian speakers with paralysis-based dysarthria (EasyCall corpus). Compared to using Fbank features, XLSR-based features reduced WERs by 6.8%, 22.0%, and 7.0% for the UASpeech, PC-GITA, and EasyCall corpus, respectively",
    "checked": true,
    "id": "e7b7066324c2f24a8a64b0e2783bd7596c568609",
    "semantic_title": "cross-lingual self-supervised speech representations for improved dysarthric speech recognition",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22b_interspeech.html": {
    "title": "Regularizing Transformer-based Acoustic Models by Penalizing Attention Weights",
    "volume": "main",
    "abstract": "The application of deep learning has significantly advanced the performance of automatic speech recognition (ASR) systems. Various components make up an ASR system, such as the acoustic model (AM), language model, and lexicon. Generally, the AM has benefited the most from deep learning. Numerous types of neural network-based AMs have been studied, but the structure that has received the most attention in recent years is the Transformer. In this study, we demonstrate that the Transformer model is more vulnerable to input sparsity compared to the convolutional neural network (CNN) and analyze the cause of performance degradation through structural characteristics of the Transformer. Moreover, we also propose a novel regularization method that makes the transformer model robust against input sparsity. The proposed sparsity regularization method directly regulates attention weights using silence label information in forced-alignment and has the advantage of not requiring additional module training and excessive computation. We tested the proposed method on five benchmarks and observed an average relative error rate reduction (RERR) of 4.7%",
    "checked": true,
    "id": "bf8d0c637369bce9d39f23b41a805b699a6d8f6d",
    "semantic_title": "regularizing transformer-based acoustic models by penalizing attention weights",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chan22_interspeech.html": {
    "title": "Content-Context Factorized Representations for Automated Speech Recognition",
    "volume": "main",
    "abstract": "Deep neural networks have largely demonstrated their ability to perform automated speech recognition (ASR) by extracting meaningful features from input audio frames. Such features, however, may consist not only of information about the spoken language content, but also may contain information about unnecessary contexts such as background noise and sounds or speaker identity, accent, or protected attributes. Such information can directly harm generalization performance, by introducing spurious correlations between the spoken words and the context in which such words were spoken. In this work, we introduce an unsupervised, encoder-agnostic method for factoring speech-encoder representations into explicit content-encoding representations and spurious context-encoding representations. By doing so, we demonstrate improved performance on standard ASR benchmarks, as well as improved performance in both real-world and artificially noisy ASR scenarios",
    "checked": true,
    "id": "8e565cc901b25e3ccafb758178f925b98b849299",
    "semantic_title": "content-context factorized representations for automated speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/karakasidis22_interspeech.html": {
    "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR",
    "volume": "main",
    "abstract": "It is common knowledge that the quantity and quality of the training data play a significant role in the creation of a good machine learning model. In this paper, we take it one step further and demonstrate that the way the training examples are arranged is also of crucial importance. Curriculum Learning is built on the observation that organized and structured assimilation of knowledge has the ability to enable faster training and better comprehension. When humans learn to speak, they first try to utter basic phones and then gradually move towards more complex structures such as words and sentences. This methodology is known as Curriculum Learning, and we employ it in the context of Automatic Speech Recognition. We hypothesize that end-to-end models can achieve better performance when provided with an organized training set consisting of examples that exhibit an increasing level of difficulty (i.e. a curriculum). To impose structure on the training set and to define the notion of an easy example, we explored multiple scoring functions that either use feedback from an external neural network or incorporate feedback from the model itself. Empirical results show that with different curriculums we can balance the training times and the network's performance",
    "checked": true,
    "id": "be3c2b892608a9e2da36f4d6f02e8ef0306fc461",
    "semantic_title": "comparison and analysis of new curriculum criteria for end-to-end asr",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baby22_interspeech.html": {
    "title": "Incremental learning for RNN-Transducer based speech recognition models",
    "volume": "main",
    "abstract": "This paper investigates an incremental learning framework for a real-world voice assistant employing RNN-Transducer based automatic speech recognition (ASR) model. Such a model needs to be regularly updated to keep up with changing distribution of customer requests. We demonstrate that a simple fine-tuning approach with a combination of old and new training data can be used to incrementally update the model spending only several hours of training time and without any degradation on old data. This paper explores multiple rounds of incremental updates on the ASR model with monthly training data. Results show that the proposed approach achieves 5-6\\% relative WER improvement over the models trained from scratch on the monthly evaluation datasets. In addition, we explore if it is possible to improve recognition of specific new words. We simulate multiple rounds of incremental updates with handful of training utterances per word (both real and synthetic) and show that the recognition of the new words improves dramatically but with a minor degradation on general data. Finally, we demonstrate that the observed degradation on general data can be mitigated by interleaving monthly updates with updates targeting specific words",
    "checked": true,
    "id": "1333b81fb6fef9b3b1b6a09d35f7eb6fc4910ff9",
    "semantic_title": "incremental learning for rnn-transducer based speech recognition models",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hard22_interspeech.html": {
    "title": "Production federated keyword spotting via distillation, filtering, and joint federated-centralized training",
    "volume": "main",
    "abstract": "We trained a keyword spotting model using federated learning on real user devices and observed significant improvements when the model was deployed for inference on phones. To compensate for data domains that are missing from on-device training caches, we employed joint federated-centralized training. And to learn in the absence of curated labels on-device, we formulated a confidence filtering strategy based on user-feedback signals for federated distillation. These techniques created models that significantly improved quality metrics in offline evaluations and user-experience metrics in live A/B experiments",
    "checked": true,
    "id": "7e4db4b20a9c2299109849c84179f8dfb788ee17",
    "semantic_title": "production federated keyword spotting via distillation, filtering, and joint federated-centralized training",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22b_interspeech.html": {
    "title": "Use of prosodic and lexical cues for disambiguating wh-words in Korean",
    "volume": "main",
    "abstract": "Previous research has shown that the ambiguity of wh-words in Korean can be resolved by prosody. The present study investigated the interplay between prosody and lexical cues in disambiguation. Our written survey results showed that the use of certain adverbs (e.g., a little, once) with a wh-word increases the likelihood of a yes-no question interpretation. The results of our speech production experiment found an interaction of lexical and prosodic cues in the disambiguation. In particular, the presence of a lexical cue affected speakers' phrasing choice, but not the type of Intonational Phrase (IP) boundary tones or acoustic prominence. The finding supports the proposal that speech production is affected by the amount of linguistic information available for speakers. We further suggest how the phrasing structure could affect speakers' choice of the IP boundary tone in Korean",
    "checked": true,
    "id": "7efca97ec18ed3b00a75fc8a2e5279cc2d755c05",
    "semantic_title": "use of prosodic and lexical cues for disambiguating wh-words in korean",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ribeiro22_interspeech.html": {
    "title": "Autoencoder-Based Tongue Shape Estimation During Continuous Speech",
    "volume": "main",
    "abstract": "Vocal tract shape estimation is a necessary step for articulatory speech synthesis. However, the literature on the topic is scarce, and most current methods lack adequacy to many physical constraints related to speech production. This study proposes an alternative approach to the task to solve specific issues faced in the previous work, especially those related to critical articulators. We present an autoencoder-based method for tongue shape estimation during continuous speech. An autoencoder is trained to learn the data's encoding and serves as an auxiliary network for the principal one, which maps phonemes to the shapes. Instead of predicting the exact points in the target curve, the neural network learns how to predict the curve's main components, i.e., the autoencoder's representation. We show how this approach allows imposing critical articulators' constraints, controlling the tongue shape through the latent space, and generating a smooth output without relying on any postprocessing method",
    "checked": true,
    "id": "a1e7cfe8f7f5ec38c9c009ec78e585163550182f",
    "semantic_title": "autoencoder-based tongue shape estimation during continuous speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/magistro22_interspeech.html": {
    "title": "Phonetic erosion and information structure in function words: the case of mia",
    "volume": "main",
    "abstract": "The purpose of this paper is to examine the prosodic correlates of a grammaticalisation process that leads to the formation of a function word. In particular, our case study will tackle the pattern of negation renewal known as Jespersen's Cycle (JC). In JC, a negative reinforcer carrying contrastive meaning grammaticalises to a function word denoting polar negation. We want to show that this change fits in with prosodic change: specifically, the grammaticalised item undergoes prosodic reduction. We test the latter hypothesis on the peculiar Italo-Romance dialect Gazzolese, where mia, the particle undergoing JC, can be used both as the erstwhile contrastive function and as a function word denoting negation (it can appear, for example, in Broad Focus statements). The results confirm that when mia is used as a function word, it displays a shorter duration, a reduced intensity excursion, and does not associate with a pitch accent, in comparison to the original contrastive context. These results show that the change in function word can be appreciated on different phonetic/phonological levels: the metrical one and the intonational one, mediated through the role of the lexical item within information structure",
    "checked": true,
    "id": "c728c743ca132aafc4f66109eb584c13fab33fd9",
    "semantic_title": "phonetic erosion and information structure in function words: the case of mia",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/oh22_interspeech.html": {
    "title": "Dynamic Vertical Larynx Actions Under Prosodic Focus",
    "volume": "main",
    "abstract": "Recently, Lee (2018) observes that one vertical larynx movement (VLM) is associated with an Accentual Phrase (AP) in Seoul Korean. The current study builds on these findings by investigating the effect of prosodic focus on vertical larynx actions. Target sentences were designed to produce four APs (e.g., Joohyun sold six yards of shabby garden field; AP[Joohyun-SUBJ] AP[shabby garden field] AP[six yards-OBJ] AP[sold-DECL], presented in Korean) and were used to elicit focus on the initial word of the object phrase (e.g., six). Articulatory data on VLM is obtained from five Seoul Korean speakers using real-time MRI. Results indicate that quantifiable VLMs observed for each sentence range from 3 to 6 movements, with 4 movements per sentence being the most frequent. Sentences with focus have more instances of VLM per sentence than those without. Focused sentences exhibit significantly greater vertical larynx displacement around the region of focus than the control. Our findings have implications for prosodic planning and pitch resetting, and ongoing analyses examine how VLMs align with Accentual Phrases in Seoul Korean and correlate with fundamental frequency",
    "checked": true,
    "id": "c54e9cabf0d8cd1d88fb234ee5bc1e190554e00d",
    "semantic_title": "dynamic vertical larynx actions under prosodic focus",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bradshaw22_interspeech.html": {
    "title": "Fundamental Frequency Variability over Time in Telephone Interactions",
    "volume": "main",
    "abstract": "Speech signals contain substantial fundamental frequency (f0) variability. Even within a single utterance, speakers modify f0 to create different intonational patterns. Previous studies have identified markers of increased f0 variability, such as the introduction of a new topic or greetings, but these are limited in the scope of their analyses. In the present study, we investigate f0 variability over the course of a telephone conversation, with a focus on the initial and medial utterances within the exchange. We examined f0 standard deviation of each utterance in over 2000 telephone conversations from 509 American English speakers from the Switchboard corpus. Findings showed that on average, speakers exhibit more f0 variability in the opening compared to mid-conversation utterances. Further, findings suggest that the inclusion of a greeting word in an initial turn, e.g., \"hello\" or \"hi\", corresponds to an increase in f0 standard deviation. These results suggest that speakers employed more variable f0 in the initial few turns of a telephone conversation. The interpretation of this finding is multifaceted and may be linked to several communicative goals, including the placement of identity markers in conversation or the attraction of attention, or the role of openings as boundary markers",
    "checked": true,
    "id": "ca01ae7023dc37cef393de12603af5e3b22d911b",
    "semantic_title": "fundamental frequency variability over time in telephone interactions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tsiamas22_interspeech.html": {
    "title": "SHAS: Approaching optimal Segmentation for End-to-End Speech Translation",
    "volume": "main",
    "abstract": "Speech translation models are unable to directly process long audios, like TED talks, which have to be split into shorter segments. Speech translation datasets provide manual segmentations of the audios, which are not available in real-world scenarios, and existing segmentation methods usually significantly reduce translation quality at inference time. To bridge the gap between the manual segmentation of training and the automatic one at inference, we propose Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively learn the optimal segmentation from any manually segmented speech corpus. First, we train a classifier to identify the included frames in a segmentation, using speech representations from a pre-trained wav2vec 2.0. The optimal splitting points are then found by a probabilistic Divide-and-Conquer algorithm that progressively splits at the frame of lowest probability until all segments are below a pre-specified length. Experiments on MuST-C and mTEDx show that the translation of the segments produced by our method approaches the quality of the manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of the manual segmentation's BLEU score, compared to the 87-93% of the best existing methods. Our method is additionally generalizable to different domains and achieves high zero-shot performance in unseen languages",
    "checked": true,
    "id": "aba3812ffaf26833f5e92919eaa42e0bcc8d8c7e",
    "semantic_title": "shas: approaching optimal segmentation for end-to-end speech translation",
    "citation_count": 26
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22g_interspeech.html": {
    "title": "M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation",
    "volume": "main",
    "abstract": "End-to-end speech-to-text translation models are often initialized with pre-trained speech encoder and pre-trained text decoder. This leads to a significant training gap between pre-training and fine-tuning, largely due to the modality differences between speech outputs from the encoder and text inputs to the decoder. In this work, we aim to bridge the modality gap between speech and text to improve translation quality. We propose M-Adapter, a novel Transformer-based module, to adapt speech representations to text. While shrinking the speech sequence, M-Adapter produces features desired for speech-to-text translation via modelling global and local dependencies of a speech sequence. Our experimental results show that our model outperforms a strong baseline by up to 1 BLEU score on the Must-C En$\\rightarrow$DE dataset.\\footnote{Our code is available at https://github.com/mingzi151/w2v2-st.}",
    "checked": true,
    "id": "795e211298e125938f6ee2efec673b3f6025f4ca",
    "semantic_title": "m-adapter: modality adaptation for end-to-end speech-to-text translation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zaidi22_interspeech.html": {
    "title": "Cross-Modal Decision Regularization for Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "Simultaneous translation systems start producing the output while processing the partial source sentence in the incoming input stream. These systems need to decide when to read more input and when to write the output. The decisions taken by the model depend on the structure of source/target language and the information contained in the partial input sequence. Hence, read/write decision policy remains the same across different input modalities, i.e., speech and text. This motivates us to leverage the text transcripts corresponding to the speech input for improving simultaneous speech-to-text translation (SimulST). We propose Cross-Modal Decision Regularization (CMDR) to improve the decision policy of SimulST systems by using the simultaneous text-to-text translation (SimulMT) task. We also extend several techniques from the offline speech translation domain to explore the role of SimulMT task in improving SimulST performance. Overall, we achieve 34.66% / 4.5 BLEU improvement over the baseline model across different latency regimes for the MuST-C English-German (EnDe) SimulST task",
    "checked": true,
    "id": "4412334747a7491315e049de555179bc2f724309",
    "semantic_title": "cross-modal decision regularization for simultaneous speech translation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fukuda22b_interspeech.html": {
    "title": "Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation",
    "volume": "main",
    "abstract": "Speech segmentation, which splits long speech into short segments, is essential for speech translation (ST). Popular VAD tools like WebRTC VAD have generally relied on pause-based segmentation. Unfortunately, pauses in speech do not necessarily match sentence boundaries, and sentences can be connected by a very short pause that is difficult to detect by VAD. In this study, we propose a speech segmentation method using a binary classification model trained using a segmented bilingual speech corpus. We also propose a hybrid method that combines VAD and the above speech segmentation method. Experimental results revealed that the proposed method is more suitable for cascade and end-to-end ST systems than conventional segmentation methods. The hybrid approach further improved the translation performance",
    "checked": true,
    "id": "5b88709ebaf4a3666d73c2f272bcb909e0925ea1",
    "semantic_title": "speech segmentation optimization using segmented bilingual speech corpus for end-to-end speech translation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/r22_interspeech.html": {
    "title": "Generalized Keyword Spotting using ASR embeddings",
    "volume": "main",
    "abstract": "Keyword Spotting (KWS) detects a set of pre-defined spoken keywords. Building a KWS system for an arbitrary set requires massive training datasets. We propose to use the text transcripts from an Automatic Speech Recognition (ASR) system alongside triplets for KWS training. The intermediate representation from the ASR system trained on a speech corpus is used as acoustic word embeddings for keywords. Triplet loss is added to the Connectionist Temporal Classification (CTC) loss in the ASR while training. This method achieves an Average Precision (AP) of 0.843 over 344 words unseen by the model trained on the TIMIT dataset. In contrast, the Multi-View recurrent method that learns jointly on the text and acoustic embeddings achieves only 0.218 for out-of-vocabulary words. This method is also applied to low-resource languages such as Tamil by converting Tamil characters to English using transliteration. This is a very challenging novel task for which we provide a dataset of transcripts for the keywords. Despite our model not generalizing well, we achieve a benchmark AP of 0.321 on over 38 words unseen by the model on the MSWC Tamil keyword set. The model also produces an accuracy of 96.2% for classification tasks on the Google Speech Commands dataset",
    "checked": true,
    "id": "4e59b71da152b76049164826bcdce3038a251e90",
    "semantic_title": "generalized keyword spotting using asr embeddings",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ahn22_interspeech.html": {
    "title": "Multi-Corpus Speech Emotion Recognition for Unseen Corpus Using Corpus-Wise Weights in Classification Loss",
    "volume": "main",
    "abstract": "Since each of the currently available emotional speech corpora is rather small to deal with personal or cultural diversity, multiple emotional speech corpora can be jointly used to train a speech emotion recognition (SER) model robust to unseen corpora. Each corpus has different characteristics, including whether acted or spontaneous, in which environment it was recorded, and what lexical contents it contains. Depending on the characteristics, the emotion recognition accuracy and time required to train a model for it are different. If we train the SER model utilizing multiple corpora equally, the classification performance for each training corpus would be different. The performance for unseen corpora may be enhanced if the model is trained to show similar recognition accuracy for each training corpus that covers different characteristics. In this study, we propose to adopt corpus-wise weights in the classification loss, which are functions of the recognition accuracy for each of the training corpus. We also adopt pseudo-emotion labels for the unlabeled speech corpus to further enhance the performance. Experimental results showed that the proposed method outperformed previously proposed approaches in the out-of-corpus SER using three emotional corpora for training and one corpus for evaluation",
    "checked": true,
    "id": "ee43187689de86a55faec99d4ed4996c6a63c6fa",
    "semantic_title": "multi-corpus speech emotion recognition for unseen corpus using corpus-wise weights in classification loss",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22d_interspeech.html": {
    "title": "Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms",
    "volume": "main",
    "abstract": "Attention has become one of the most commonly used mechanisms in deep learning approaches. The attention mechanism can help the system focus more on the feature space's critical regions. For example, high amplitude regions can play an important role for Speech Emotion Recognition (SER). In this paper, we identify misalignments between the attention and the signal amplitude in the existing multi-head self-attention. To improve the attention area, we propose to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA) mechanism in combination with the multi-head self-attention. Through the FA mechanism, the network can detect the largest amplitude part in the segment. By employing the CA mechanism, the network can modulate the information flow by assigning different weights to each attention head and improve the utilization of surrounding contexts. To evaluate the proposed method, experiments are performed with the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed framework significantly outperforms the state-of-the-art approaches on both datasets",
    "checked": true,
    "id": "c6da42178e907c18c3af763f274ffda285e86042",
    "semantic_title": "improving speech emotion recognition through focus and calibration attention mechanisms",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22e_interspeech.html": {
    "title": "The Emotion is Not One-hot Encoding: Learning with Grayscale Label for Emotion Recognition in Conversation",
    "volume": "main",
    "abstract": "In emotion recognition in conversation (ERC), the emotion of the current utterance is predicted by considering the previous context, which can be utilized in many natural language processing tasks. Although multiple emotions can coexist in a given sentence, most previous approaches take the perspective of a classification task to predict only a given label. However, it is expensive and difficult to label the emotion of a sentence with confidence or multi-label. In this paper, we automatically construct a grayscale label considering the correlation between emotions and use it for learning. That is, instead of using a given label as a one-hot encoding, we construct a grayscale label by measuring scores for different emotions. We introduce several methods for constructing grayscale labels and confirm that each method improves the emotion recognition performance. Our method is simple, effective, and universally applicable to previous systems. The experiments show a significant improvement in the performance of baselines",
    "checked": true,
    "id": "b30cde7df8d004fb3dd029b07619a739bc3e2c59",
    "semantic_title": "the emotion is not one-hot encoding: learning with grayscale label for emotion recognition in conversation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/triantafyllopoulos22b_interspeech.html": {
    "title": "Probing speech emotion recognition transformers for linguistic knowledge",
    "volume": "main",
    "abstract": "Large, pre-trained neural networks consisting of self-attention layers (transformers) have recently achieved state-of-the-art results on several speech emotion recognition (SER) datasets. These models are typically pre-trained in self-supervised manner with the goal to improve automatic speech recognition performance -- and thus, to understand linguistic information. In this work, we investigate the extent in which this information is exploited during SER fine-tuning. Using a reproducible methodology based on open-source tools, we synthesise prosodically neutral speech utterances while varying the sentiment of the text. Valence predictions of the transformer model are very reactive to positive and negative sentiment content, as well as negations, but not to intensifiers or reducers, while none of those linguistic features impact arousal or dominance. These findings show that transformers can successfully leverage linguistic information to improve their valence predictions, and that linguistic analysis should be included in their testing",
    "checked": true,
    "id": "6d9395e99fddcbd1bc83328b3898e67e76bd657c",
    "semantic_title": "probing speech emotion recognition transformers for linguistic knowledge",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2022/prabhu22_interspeech.html": {
    "title": "End-To-End Label Uncertainty Modeling for Speech-based Arousal Recognition Using Bayesian Neural Networks",
    "volume": "main",
    "abstract": "Emotions are subjective constructs. Recent end-to-end speech emotion recognition systems are typically agnostic to the subjective nature of emotions, despite their state-of-the-art performance. In this work, we introduce an end-to-end Bayesian neural network architecture to capture the inherent subjectivity in the arousal dimension of emotional expressions. To the best of our knowledge, this work is the first to use Bayesian neural networks for speech emotion recognition. At training, the network learns a distribution of weights to capture the inherent uncertainty related to subjective arousal annotations. To this end, we introduce a loss term that enables the model to be explicitly trained on a distribution of annotations, rather than training them exclusively on mean or gold-standard labels. We evaluate the proposed approach on the AVEC'16 dataset. Qualitative and quantitative analysis of the results reveals that the proposed model can aptly capture the distribution of subjective arousal annotations, with state-of-the-art results in mean and standard deviation estimations for uncertainty modeling",
    "checked": true,
    "id": "90fc3e08850601fb43749b3d3bc77012f33cc9b7",
    "semantic_title": "end-to-end label uncertainty modeling for speech-based arousal recognition using bayesian neural networks",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/perez22_interspeech.html": {
    "title": "Mind the gap: On the value of silence representations to lexical-based speech emotion recognition",
    "volume": "main",
    "abstract": "Speech timing and non-speech regions (here referred to as ``silence\"), often play a critical role in the perception of spoken language. Silence represents an important paralinguistic component in communication. For example, some of its functions include conveying emphasis, dramatization, or even sarcasm. In speech emotion recognition (SER), there has been relatively little work on investigating the utility of silence and no work regarding the effect of silence on linguistics. In this work, we present a novel framework which investigates fusing linguistic and silence representations for emotion recognition in naturalistic speech using the MSP-Podcast dataset. We investigate two methods to represent silence in SER models; the first approach uses utterance-level statistics, while the second learns a silence token embedding within a transformer language model. Our results show that modeling silence does improve SER performance and that modeling silence as a token in a transformer language model significantly improves performance on MSP-Podcast achieving a concordance correlation coefficient of .191 and .453 for activation and valence respectively. In addition, we perform analyses on the attention of silence and find that silence emphasizes the attention of its surrounding words",
    "checked": true,
    "id": "8049c5bb4e95af939b7ab650bc0017bf588f0f3f",
    "semantic_title": "mind the gap: on the value of silence representations to lexical-based speech emotion recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chou22_interspeech.html": {
    "title": "Exploiting Co-occurrence Frequency of Emotions in Perceptual Evaluations To Train A Speech Emotion Classifier",
    "volume": "main",
    "abstract": "Previous studies on speech emotion recognition (SER) with categorical emotions have often formulated the task as a single-label classification problem, where the emotions are considered orthogonal to each other. However, previous studies have indicated that emotions can co-occur, especially for more ambiguous emotional sentences (e.g., a mixture of happiness and surprise). Some studies have regarded SER problems as a multi-label task, predicting multiple emotional classes. However, this formulation does not leverage the relation between emotions during training, since emotions are assumed to be independent. This study explores the idea that emotional classes are not necessarily independent and its implications on training SER models. In particular, we calculate the frequency of co-occurring emotions from perceptual evaluations in the train set to generate a matrix with class-dependent penalties, punishing more mistakes between distant emotional classes. We integrate the penalization matrix into three existing label-learning approaches (hard-label, multi-label, and distribution-label learning) using the proposed modified loss. We train SER models using the penalty loss and commonly used cost functions for SER tasks. The evaluation of our proposed penalization matrix on the MSP-Podcast corpus shows important relative improvements in macro F1-score for hard-label learning (17.12%), multi-label learning (12.79%), and distribution-label learning (25.8%)",
    "checked": true,
    "id": "da3c26775aeafaae8e82e71012bc9b731b7c2766",
    "semantic_title": "exploiting co-occurrence frequency of emotions in perceptual evaluations to train a speech emotion classifier",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dhamyal22_interspeech.html": {
    "title": "Positional Encoding for Capturing Modality Specific Cadence for Emotion Detection",
    "volume": "main",
    "abstract": "Emotion detection from a single modality, such as an audio or text stream, has been known to be a challenging task. While encouraging results have been obtained by using joint evidence from multiple streams, combining such evidence in optimal ways is an open challenge. In this paper, we claim that although the multi-modalities like audio, phoneme sequence ids and word sequence ids are related to each other, they also have their individual local 'cadence', which is important to be modelled for the task of emotion recognition. We model the local cadence by using separate `positional encodings' for each modality in a transformer architecture. Our results show that emotion detection based on this strategy is better than when the modality specific cadence is ignored or normalized out by using a shared positional encoding. We also find that capturing the modality interdependence is not as important as is capturing of the local cadence of individual modalities. We conduct our experiments on the IEMOCAP and CMU-MOSI datasets to demonstrate the effectiveness of the proposed methodology for combining multi-modal evidence",
    "checked": true,
    "id": "d0bac683b9ee598d7deeb5d21c26cdc471002631",
    "semantic_title": "positional encoding for capturing modality specific cadence for emotion detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vuho22_interspeech.html": {
    "title": "Speak Like a Professional: Increasing Speech Intelligibility by Mimicking Professional Announcer Voice with Voice Conversion",
    "volume": "main",
    "abstract": "In most of practical scenarios, the announcement system must deliver speech messages in a noisy environment, in which the background noise cannot be cancelled out. The local noise reduces speech intelligibility and increases listening effort of the listener, hence hamper the effectiveness of announcement system. There has been reported that voices of professional announcers are clearer and more comprehensive than that of non-expert speakers in noisy environment. This finding suggests that the speech intelligibility might be related to the speaking style of professional announcer, which can be adapted using voice conversion method. Motivated by this idea, this paper proposes a speech intelligibility enhancement in noisy environment by applying voice conversion method on non-professional voice. We discovered that the professional announcers and non-professional speakers are clusterized into different clusters on the speaker embedding plane. This implies that the speech intelligibility can be controlled as an independent feature of speaker individuality. To examine the advantage of converted voice in noisy environment, we experimented using test words masked in pink noise at different SNR levels. The results of objective and subjective evaluations confirm that the speech intelligibility of converted voice is higher than that of original voice in low SNR conditions",
    "checked": true,
    "id": "d472dc50e2310e4a4a07c0c727bd88ec1a03a2f7",
    "semantic_title": "speak like a professional: increasing speech intelligibility by mimicking professional announcer voice with voice conversion",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ho22_interspeech.html": {
    "title": "Vector-quantized Variational Autoencoder for Phase-aware Speech Enhancement",
    "volume": "main",
    "abstract": "Recent speech enhancement methods based on the complex ideal ratio mask (cIRM) have achieved promising results. These methods often deploy a deep neural network to jointly estimate the real and imaginary components of the cIRM defined in the complex domain. However, the unbounded property of cIRM poses difficulties when it comes to effectively training a neural network. To alleviate this problem, this paper proposes a phase-aware speech enhancement method by estimating the magnitude and phase of a complex adaptive Wiener filter. In this method, a noise-robust vector-quantized variational autoencoder is utilized for estimating the magnitude Wiener filter by using the Itakura-Saito divergence on time-frequency domain, while the phase of the Wiener filter is estimated by a convolutional recurrent network using the scale-invariant signal-to-noise ratio constraint in the time domain. The proposed method was evaluated on the open Voice Bank+DEMAND dataset to provide a direct comparison with other speech enhancement studies and achieved the PESQ score of 2.85 and STOI score of 0.94, which is better than the state-of-art method based on cIRM estimation in the 2020 Deep Noise Challenge",
    "checked": true,
    "id": "53778ad96a6fb92779ebcf7ae1a197c7c47ca5de",
    "semantic_title": "vector-quantized variational autoencoder for phase-aware speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22i_interspeech.html": {
    "title": "iDeepMMSE: An improved deep learning approach to MMSE speech and noise power spectrum estimation for speech enhancement",
    "volume": "main",
    "abstract": "Deep learning approaches have been successfully applied to single channel speech enhancement exhibiting significant performance improvement. Recently, approaches unifying deep learning techniques into a statistical speech enhancement framework were proposed, including Deep Xi and DeepMMSE in which a priori signal-to-noise ratios (SNRs) were estimated by deep neural networks (DNNs) and noise power spectral density (PSD) and spectral gain functions were computed with estimated parameters. In this paper, we propose an improved DeepMMSE (iDeepMMSE) which estimates the speech PSD and speech presence probability as well as the a priori SNR using a DNN for MMSE estimation of the speech and noise PSDs. The a priori and a posteriori SNRs are refined with the estimated PSDs, which in turn are used to compute spectral gain function. We also replaced the DNN architecture with the Conformer which efficiently captures the local and global sequential information. Experimental results on the Voice Bank-DEMAND dataset and Deep Xi dataset showed the proposed iDeepMMSE outperformed the DeepMMSE in terms of the perceptual evaluation of speech quality (PESQ) scores and composite objective measures",
    "checked": true,
    "id": "a2139b2f32b00fe4ef4c64571f5583496d8c019b",
    "semantic_title": "ideepmmse: an improved deep learning approach to mmse speech and noise power spectrum estimation for speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hung22_interspeech.html": {
    "title": "Boosting Self-Supervised Embeddings for Speech Enhancement",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) representation for speech has achieved state-of-the-art (SOTA) performance on several downstream tasks. However, there remains room for improvement in speech enhancement (SE) tasks. In this study, we used a cross-domain feature to solve the problem that SSL embeddings may lack fine-grained information to regenerate speech signals. By integrating the SSL representation and spectrogram, the result can be significantly boosted. We further study the relationship between the noise robustness of SSL representation via clean-noisy distance (CN distance) and the layer importance for SE. Consequently, we found that SSL representations with lower noise robustness are more important. Furthermore, our experiments on the VCTK-DEMAND dataset demonstrated that fine-tuning an SSL representation with an SE model can outperform the SOTA SSL-based SE methods in PESQ, CSIG and COVL without invoking complicated network architectures. In later experiments, the CN distance in SSL embeddings was observed to increase after fine-tuning. These results verify our expectations and may help design SE-related SSL training in the future",
    "checked": true,
    "id": "bc89597321a15c8568658809e77fbf21ef663c63",
    "semantic_title": "boosting self-supervised embeddings for speech enhancement",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hwang22b_interspeech.html": {
    "title": "Monoaural Speech Enhancement Using a Nested U-Net with Two-Level Skip Connections",
    "volume": "main",
    "abstract": "Capturing the contextual information in multi-scale is known to be beneficial for improving the performance of DNN-based speech enhancement (SE) models. This paper proposes a new SE model, called NUNet-TLS, having two-level skip connections between the residual U-Blocks nested in each layer of a large U-Net structure. The proposed model also has a causal time-frequency attention (CFTA) at the output of the residual U-Block to boost dynamic representation of the speech context in multi-scale. Even having the two-level skip connections, the proposed model slightly increases the network parameters, but the performance improvement is significant. Experimental results show that the proposed NUNet-TLS has superior performance in various objective evaluation metrics to other state-of-the-art models. The code of our model is available at https://github.com/seorim0/NUNet-TLS",
    "checked": true,
    "id": "e0a8a466098f902a1b53cf9200808e401bf9d3bc",
    "semantic_title": "monoaural speech enhancement using a nested u-net with two-level skip connections",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/muckenhirn22_interspeech.html": {
    "title": "CycleGAN-based Unpaired Speech Dereverberation",
    "volume": "main",
    "abstract": "Typically, neural network-based speech dereverberation models are trained on paired data, composed of a dry utterance and its corresponding reverberant utterance. The main limitation of this approach is that such models can only be trained on large amounts of data and a variety of room impulse responses when the data is synthetically reverberated, since acquiring real paired data is costly. In this paper we propose a CycleGAN-based approach that enables dereverberation models to be trained on unpaired data. We quantify the impact of using unpaired data by comparing the proposed unpaired model to a paired model with the same architecture and trained on the paired version of the same dataset. We show that the performance of the unpaired model is comparable to the performance of the paired model on two different datasets, according to objective evaluation metrics. Furthermore, we run two subjective evaluations and show that both models achieve comparable subjective quality on the AMI dataset, which was not seen during training",
    "checked": true,
    "id": "013406ed228229b710a2e7c2a043b531ebf20489",
    "semantic_title": "cyclegan-based unpaired speech dereverberation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pandey22_interspeech.html": {
    "title": "Attentive Training: A New Training Framework for Talker-independent Speaker Extraction",
    "volume": "main",
    "abstract": "Listening in a multitalker scenario, we typically attend to a single talker through auditory selective attention. Inspired by human selective attention, we propose attentive training: a new training framework for talker-independent speaker extraction with an intrinsic selection mechanism. In the real world, multiple talkers very unlikely start speaking at the same time. Based on this observation, we train a deep neural network to create a representation for the first speaker and utilize it to extract or track that speaker from a multitalker noisy mixture. Experimental results demonstrate the superiority of attentive training over widely used permutation invariant training for talker-independent speaker extraction, especially in mismatched conditions in terms of the number of speakers, speaker interaction patterns, and the amount of speaker overlaps",
    "checked": true,
    "id": "e394c3de6dbfaa24c12ab168bc6ad439fee0fffa",
    "semantic_title": "attentive training: a new training framework for talker-independent speaker extraction",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vuong22_interspeech.html": {
    "title": "Improved Modulation-Domain Loss for Neural-Network-based Speech Enhancement",
    "volume": "main",
    "abstract": "We describe an improved modulation-domain loss for deeplearning- based speech enhancement systems (SE). We utilized a simple self-supervised speech reconstruction task to learn a set of spectro-temporal receptive fields (STRFs). Similar to the recently developed spectro-temporal modulation error, the learned STRFs are used to calculate a weighted mean-squared error in the modulation domain for training a speech enhancement system. Experiments show that training the SE systems using the improved modulation-domain loss consistently improves the objective prediction of speech quality and intelligibility. Additionally, we show that the SE systems improve the word error rate of a state-of-the-art automatic speech recognition system at low SNRs",
    "checked": true,
    "id": "21300ed24cf2c01ce06c46361f216713299e8104",
    "semantic_title": "improved modulation-domain loss for neural-network-based speech enhancement",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22d_interspeech.html": {
    "title": "Perceptual Characteristics Based Multi-objective Model for Speech Enhancement",
    "volume": "main",
    "abstract": "Deep learning has been widely adopted for speech applications. Many studies have shown that using the multiple objective framework and learned deep features is effective for improving system performance. In this paper, we propose a perceptual characteristics based multi-objective speech enhancement (SE) algorithm that combines the conventional loss and objective losses of pitch and timbre related features. Timbre related features include frequency modulation (encoded by the pitch contour), amplitude modulation (encoded by the energy contour), and speaker identity. For the speaker identity loss, we consider the deep features derived in a speaker identification system. The proposed algorithm consists of two parts, a LSTM based SE model and CNN based multi-objective models. The objective losses are derived between speech enhanced by the SE model and clean speech and combined with the SE loss for updating the SE model. The proposed algorithm is evaluated using the corpus of Taiwan Mandarin hearing in noise test (TMHINT). Experimental results show the proposed algorithm evidently outperforms the original SE model in all objective scores, including speech quality, speech intelligibility and signal distortion",
    "checked": true,
    "id": "a43ef077e3f16ea0d38ba02b05039075609e0d03",
    "semantic_title": "perceptual characteristics based multi-objective model for speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/delcroix22_interspeech.html": {
    "title": "Listen only to me! How well can target speech extraction handle false alarms?",
    "volume": "main",
    "abstract": "Target speech extraction (TSE) extracts the speech of a target speaker in a mixture given auxiliary clues characterizing the speaker, such as an enrollment utterance. TSE addresses thus the challenging problem of simultaneously performing separation and speaker identification. There has been much progress in extraction performance following the recent development of neural networks for speech enhancement and separation. Most studies have focused on processing mixtures where the target speaker is actively speaking. However, the target speaker is sometimes silent in practice, i.e., inactive speaker (IS). A typical TSE system will tend to output a signal in IS cases, causing false alarms. This is a severe problem for the practical deployment of TSE systems. This paper aims at understanding better how well TSE systems can handle IS cases. We consider two approaches to deal with IS, (1) training a system to directly output zero signals or (2) detecting IS with an extra speaker verification module. We perform an extensive experimental comparison of these schemes in terms of extraction performance and IS detection using the LibriMix dataset and reveal their pros and cons",
    "checked": true,
    "id": "19e4a756cd760323969721131cb2cfd61fb05b02",
    "semantic_title": "listen only to me! how well can target speech extraction handle false alarms?",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22e_interspeech.html": {
    "title": "Monaural Speech Enhancement Based on Spectrogram Decomposition for Convolutional Neural Network-sensitive Feature Extraction",
    "volume": "main",
    "abstract": "Many state-of-the-art speech enhancement (SE) systems have recently used convolutional neural networks (CNNs) to extract multi-scale feature maps. However, CNN relies more on local texture than global shape, which is more susceptible to degraded spectrogram and may fail to capture the detailed structure of speech. Although some two-stage systems feed the first-stage enhanced and original noisy spectrograms to the second stage simultaneously, this does not guarantee sufficient guidance for the second stage since the first-stage spectrogram can not provide precise spectral details. In order to allow CNNs to perceive clear speech component boundary information, we compose feature maps with spectrograms containing evident speech components according to the mask value from the first stage. The positions corresponding to the mask greater than certain thresholds are extracted as feature maps. These feature maps make the boundary information of speech components obvious by ignoring others, thus making CNNs sensitive to input features. Experiments on the VB dataset show that with a proper decomposition numbers, the proposed method can enhance SE performance, which can provide 0.15 PESQ improvement. Besides, the proposed method is more effective for spectral detail recovery",
    "checked": true,
    "id": "a344e3c08264f177485a0deb7239bc2befdcf768",
    "semantic_title": "monaural speech enhancement based on spectrogram decomposition for convolutional neural network-sensitive feature extraction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lemercier22_interspeech.html": {
    "title": "Neural Network-augmented Kalman Filtering for Robust Online Speech Dereverberation in Noisy Reverberant Environments",
    "volume": "main",
    "abstract": "In this paper, a neural network-augmented algorithm for noise-robust online dereverberation with a Kalman filtering variant of the weighted prediction error (WPE) method is proposed. The filter stochastic variations are predicted by a deep neural network (DNN) trained end-to-end using the filter residual error and signal characteristics. The presented framework allows for robust dereverberation on a single-channel noisy reverberant dataset similar to WHAMR!. The Kalman filtering WPE introduces distortions in the enhanced signal when predicting the filter variations from the residual error only, if the target speech power spectral density is not perfectly known and the observation is noisy. The proposed approach avoids these distortions by correcting the filter variations estimation in a data-driven way, increasing the robustness of the method to noisy scenarios. Furthermore, it yields a strong dereverberation and denoising performance compared to a DNN-supported recursive least squares variant of WPE, especially for highly noisy inputs",
    "checked": true,
    "id": "45ffaa49410a2855d1150e2c6a8568a6a07ef2c7",
    "semantic_title": "neural network-augmented kalman filtering for robust online speech dereverberation in noisy reverberant environments",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schmidt22_interspeech.html": {
    "title": "PodcastMix: A dataset for separating music and speech in podcasts",
    "volume": "main",
    "abstract": "We introduce PodcastMix, a dataset formalizing the task of separating background music and foreground speech in podcasts. We aim at defining a benchmark suitable for training and evaluating (deep learning) source separation models. To that end, we release a large and diverse training dataset based on programatically generated podcasts. However, current (deep learning) models can incur into generalization issues, specially when trained on synthetic data. To target potential generalization issues, we release an evaluation set based on real podcasts for which we design objective and subjective tests. Out of our experiments with real podcasts, we find that current (deep learning) models may have generalization issues. Yet, these can perform competently, e.g., our best baseline separates speech with a mean opinion score of 3.84 (rating ``overall separation quality\" from 1 to 5). The dataset and baselines are accessible online",
    "checked": true,
    "id": "714a7e6548f08bd9050e830f6c8faf840406b40d",
    "semantic_title": "podcastmix: a dataset for separating music and speech in podcasts",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saijo22_interspeech.html": {
    "title": "Independence-based Joint Dereverberation and Separation with Neural Source Model",
    "volume": "main",
    "abstract": "We propose an independence-based joint dereverberation and separation method with a neural source model. We introduce a neural network in the framework of time-decorrelation iterative source steering, which is an extension of independent vector analysis to joint dereverberation and separation. The network is trained in an end-to-end manner with a permutation invariant loss on the time-domain separation output signals. Our proposed method can be applied in any situation with at least as many microphones as sources, regardless of their number. In experiments, we demonstrate that our method results in high performance in terms of both speech quality metrics and word error rate (WER), even for mixtures with a different number of speakers than training. Furthermore, the model, trained on synthetic mixtures, without any modifications, greatly reduces the WER on the recorded dataset LibriCSS",
    "checked": true,
    "id": "f51c2a32e9540e7e508fa36ecbc17cc4701985d4",
    "semantic_title": "independence-based joint dereverberation and separation with neural source model",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saijo22b_interspeech.html": {
    "title": "Spatial Loss for Unsupervised Multi-channel Source Separation",
    "volume": "main",
    "abstract": "We propose a spatial loss for unsupervised multi-channel source separation. The proposed loss exploits the duality of direction of arrival (DOA) and beamforming: the steering and beamforming vectors should be aligned for the target source, but orthogonal for interfering ones. The spatial loss encourages consistency between the mixing and demixing systems from a classic DOA estimator and a neural separator, respectively. With the proposed loss, we train the neural separators based on minimum variance distortionless response (MVDR) beamforming and independent vector analysis (IVA). We also investigate the effectiveness of combining our spatial loss and a signal loss, which uses the outputs of blind source separation as the references. We evaluate our proposed method on synthetic and recorded (LibriCSS) mixtures. We find that the spatial loss is most effective to train IVA-based separators. For the neural MVDR beamformer, it performs best when combined with a signal loss. On synthetic mixtures, the proposed unsupervised loss leads to the same performance as a supervised loss in terms of word error rate. On LibriCSS, we obtain close to state-of-the-art performance without any labeled training data",
    "checked": true,
    "id": "2c372ad41c123e1fc83d27672952849f54ea8449",
    "semantic_title": "spatial loss for unsupervised multi-channel source separation",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bellows22_interspeech.html": {
    "title": "Effect of Head Orientation on Speech Directivity",
    "volume": "main",
    "abstract": "The directional characteristics of human speech have many applications in speech acoustics, audio, telecommunications, room acoustical design, and other areas. However, professionals in these fields require carefully conducted, high-resolution, spherical speech directivity measurements taken under distinct circumstances to gain additional insights for their work. Because head orientation and human-body diffraction influence speech radiation, this work explores such effects under various controlled conditions through the changing directivity patterns of a head and torso simulator. The results show that head orientation and body diffraction at low frequencies impact directivities only slightly. However, the effects are more substantial at higher frequencies, particularly above 1 kHz",
    "checked": true,
    "id": "bb329e0996657c7e645cf98f942719923fc3fe8e",
    "semantic_title": "effect of head orientation on speech directivity",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saijo22c_interspeech.html": {
    "title": "Unsupervised Training of Sequential Neural Beamformer Using Coarsely-separated and Non-separated Signals",
    "volume": "main",
    "abstract": "We present an unsupervised training method of the sequential neural beamformer (Seq-BF) using coarsely-separated and non-separated supervisory signals. The signal coarsely separated by blind source separation (BSS) has been used for training neural separators in an unsupervised manner. However, the performance is limited due to distortions in the supervision. In contrast, remix-cycle-consistent learning (RCCL) enables a separator to be trained on distortion-free observed mixtures by making the remixed mixtures obtained by repeatedly separating and remixing the two different mixtures closer to the original mixtures. Still, training with RCCL from scratch often falls into a trivial solution, i.e., not separating signals. The present study provides a novel unsupervised learning algorithm for the Seq-BF with two stacked neural separators, in which the separators are pre-trained using the BSS outputs and then fine-tuned with RCCL. Such configuration compensates for the shortcomings of both approaches: the guiding mechanism in Seq-BF accelerates separation to exceed BSS performance, thereby stabilizing RCCL. Experimental comparisons demonstrated that the proposed unsupervised learning achieved performance comparable to supervised learning (0.4 point difference in word error rate)",
    "checked": true,
    "id": "9b2d664a6bfb8ac82111bd27e38bc11cebc1612a",
    "semantic_title": "unsupervised training of sequential neural beamformer using coarsely-separated and non-separated signals",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/borsdorf22_interspeech.html": {
    "title": "Blind Language Separation: Disentangling Multilingual Cocktail Party Voices by Language",
    "volume": "main",
    "abstract": "We introduce blind language separation (BLS) as novel research task, in which we seek to disentangle overlapping voices of multiple languages by language. BLS is expected to separate seen as well as unseen languages, which is different from the target language extraction task that works for one seen target language at a time. To develop a BLS model, we simulate a multilingual cocktail party database, of which each scene consists of two randomly selected languages, each represented by two randomly selected speakers. The database follows the recently proposed GlobalPhoneMCP database design concept that uses the audio data of the GlobalPhone 2000 Speaker Package. We show that a BLS model is able to learn the language characteristics so as to disentangle overlapping voices by language. We achieve a mean SI-SDR improvement of 12.63 dB over 231 test sets. The performance on the individual test sets varies depending on the language combination. Finally, we show that BLS can generalize well to unseen speakers and languages in the mixture",
    "checked": true,
    "id": "60842dfaa6f3f88d1d05b1c0e1f52453eb85f5b4",
    "semantic_title": "blind language separation: disentangling multilingual cocktail party voices by language",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guzik22_interspeech.html": {
    "title": "NTF of Spectral and Spatial Features for Tracking and Separation of Moving Sound Sources in Spherical Harmonic Domain",
    "volume": "main",
    "abstract": "This paper presents a novel Non-negative Tensor Factorization (NTF) based approach to tracking and separation of moving sound sources, formulated in the Spherical Harmonic Domain (SHD). In particular, at first, we redefine an already existing Ambisonic NTF by introducing time-dependence into the Spatial Covariance Matrix (SCM) model. Next, we further extend the time-dependent SCM by incorporating a newly proposed NTF model of the spatial features, thereby introducing spatial components. To exploit the relationship between the positions of sound sources in adjacent time frames, resulting from the naturally occurring continuity of the movement itself, we impose local smoothness on time-dependent components of the spatial features. To this end, we propose a suitable posterior probability with Gibbs prior, and finally we derive the corresponding update rules. The experimental evaluation is based on first-order Ambisonic recordings of speech utterances and musical instruments in several scenarios with moving sources",
    "checked": true,
    "id": "782c2d176f265f016cc4bdd6ae5c101d897d54a4",
    "semantic_title": "ntf of spectral and spatial features for tracking and separation of moving sound sources in spherical harmonic domain",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deadman22_interspeech.html": {
    "title": "Modelling Turn-taking in Multispeaker Parties for Realistic Data Simulation",
    "volume": "main",
    "abstract": "Simulation plays a crucial role in developing components of automatic speech recognition systems such as enhancement and diarization. In source separation and target-speaker extraction, datasets with high degrees of temporal overlap are used both in training and evaluation. However, this contrasts with the fact that people tend to avoid such overlap in real conversations. It is well known that artifacts introduced from pre-processing with no overlapping speech can be detrimental to recognition performance. This work proposes a finite-state based generative method trained on timing information in speech corpora, which leads to two main contributions. First, a method for generating arbitrary large datasets which follow desired statistics of real parties. Second, features extracted from the models are shown to have a correlation with speaker extraction performance. This leads to the contribution of quantifying how much difficulty in a mixture is due to turn-taking, factoring out other complexities in the signal. Models which treat speakers as independent produce poor generation and representation results. We improve upon this by proposing models which have states conditioned on whether another person is speaking",
    "checked": true,
    "id": "5b70fa432f4dcdbdd25432edff67204db2aaa897",
    "semantic_title": "modelling turn-taking in multispeaker parties for realistic data simulation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/boeddeker22_interspeech.html": {
    "title": "An Initialization Scheme for Meeting Separation with Spatial Mixture Models",
    "volume": "main",
    "abstract": "Spatial mixture model (SMM) supported acoustic beamforming has been extensively used for the separation of simultaneously active speakers. However, it has hardly been considered for the separation of meeting data, that are characterized by long recordings and only partially overlapping speech. In this contribution, we show that the fact that often only a single speaker is active can be utilized for a clever initialization of an SMM that employs time-varying class priors. In experiments on LibriCSS we show that the proposed initialization scheme achieves a significantly lower Word Error Rate (WER) on a downstream speech recognition task than a random initialization of the class probabilities by drawing from a Dirichlet distribution. With the only requirement that the number of speakers has to be known, we obtain a WER of 5.9 %, which is comparable to the best reported WER on this data set. Furthermore, the estimated speaker activity from the mixture model serves as a diarization based on spatial information",
    "checked": true,
    "id": "69c3b02a5a2883ad38cca70dcceb1a04bfcd63a7",
    "semantic_title": "an initialization scheme for meeting separation with spatial mixture models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mun22_interspeech.html": {
    "title": "Prototypical speaker-interference loss for target voice separation using non-parallel audio samples",
    "volume": "main",
    "abstract": "In this paper, we propose a new prototypical loss function for training neural network models for target voice separation. Conventional methods use paired parallel audio samples of the target speaker with and without an interfering speaker or noise, and minimize the spectrographic mean squared error (MSE) between the clean and enhanced target speaker audio. Motivated by the use of contrastive loss in speaker recognition task, we had earlier proposed a speaker representation loss that uses representative samples from the target speaker in addition to the conventional MSE loss. In this work, we propose a prototypical speaker-interference (PSI) loss, that makes use of representative samples from the target speaker, interfering speaker as well as the interfering noise to better utilize any non-parallel data that may be available. The performance of the proposed loss function is evaluated using VoiceFilter, a popular framework for target voice separation. Experimental results show that the proposed PSI loss significantly improves the PESQ scores of the enhanced target speaker audio",
    "checked": true,
    "id": "ed3b6e3f0dba366e8ac798a0d8fbadef18537703",
    "semantic_title": "prototypical speaker-interference loss for target voice separation using non-parallel audio samples",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bousquet22_interspeech.html": {
    "title": "Reliability criterion based on learning-phase entropy for speaker recognition with neural network",
    "volume": "main",
    "abstract": "The reliability of Automatic Speaker Recognition (SR) is of the utmost importance for real-world applications. Even if SR systems obtain spectacular performance during evaluation campaigns, several studies have shown the limits and shortcomings of these systems. Reliability first means knowing where and when a system is performing as expected and a research effort is devoted to building confidence measures, by scanning input signals, representations or output scores. Here, a new reliability criterion is presented, dedicated to the latest SR systems based on deep neural network (DNN). The proposed approach uses the set of anchor speakers that controls the learning phase and takes advantage of the structure of the network itself, in order to derive a criterion making it possible to better assess the reliability of the decision based on the extracted speaker embeddings. The relevance and effectiveness of the proposed confidence measure are tested and demonstrated on widely used datasets",
    "checked": true,
    "id": "043c324c3566e44932ab171bc304edf37fa2351d",
    "semantic_title": "reliability criterion based on learning-phase entropy for speaker recognition with neural network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22f_interspeech.html": {
    "title": "Attentive Feature Fusion for Robust Speaker Verification",
    "volume": "main",
    "abstract": "As the most widely used technique, deep speaker embedding learning has become predominant in speaker verification task recently. This approach utilizes deep neural networks to extract fixed dimension embedding vectors which represent different speaker identities. Two network architectures such as ResNet and ECAPA-TDNN have been commonly adopted in prior studies and achieved the state-of-the-art performance. One omnipresent part, feature fusion, plays an important role in both of them. For example, shortcut connections are designed to fuse the identity mapping of inputs and outputs of residual blocks in ResNet. ECAPA-TDNN employs the multi-layer feature aggregation to integrate shallow feature maps with deep ones. Traditional feature fusion is often implemented via simple operations, such as element-wise addition or concatenation. In this paper, we propose a more effective feature fusion scheme, namely Attentive Feature Fusion (AFF), to render dynamic weighted fusion of different features. It utilizes attention modules to learn fusion weights based on the feature contents. Additionally, two fusion strategies are designed: sequential fusion and parallel fusion. Experiments on Voxceleb dataset show that our proposed attentive feature fusion scheme can result in up to 40% relative improvement over the baseline systems",
    "checked": true,
    "id": "2b6f6c62898923b4f66638425ed5cfba78443a14",
    "semantic_title": "attentive feature fusion for robust speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22g_interspeech.html": {
    "title": "Dual Path Embedding Learning for Speaker Verification with Triplet Attention",
    "volume": "main",
    "abstract": "Currently, many different network architectures have been explored in speaker verification, including time-delay neural network (TDNN), convolutional neural network (CNN), transformer and multi-layer perceptrons (MLP). However, hybrid networks with diverse structures are rarely investigated. In this paper, we present a novel and effective dual path embedding learning framework, named Dual Path Network (DPNet), for speaker verification with triplet attention. A new topology of integrating CNN with a separate recurrent layer connection path internally is designed, which introduces the sequential structure along depth into CNN. This new architecture inherits both advantages of residual and recurrent networks, enabling better feature re-usage and re-exploitation. Additionally, an efficient triplet attention module is utilized to capture cross-dimension interactions between features. The experimental results conducted on Voxceleb dataset show that our proposed hybrid network with triplet attention can outperform the corresponding ResNet by a significant margin",
    "checked": true,
    "id": "c4fb36db0131e175a31ace4264c3a928e1811b65",
    "semantic_title": "dual path embedding learning for speaker verification with triplet attention",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22h_interspeech.html": {
    "title": "DF-ResNet: Boosting Speaker Verification Performance with Depth-First Design",
    "volume": "main",
    "abstract": "Embeddings extracted by deep neural networks have become the state-of-the-art utterance representation in speaker verification (SV). Despite the various network architectures that have been investigated in previous works, how to design and scale up networks to achieve a better trade-off on performance and complexity in a principled manner has been rarely discussed in the SV field. In this paper, we first systematically study model scaling from the perspective of the depth and width of networks and empirically discover that depth is more important than the width of networks for speaker verification task. Based on this observation, we design a new backbone constructed entirely from standard convolutional network modules by significantly increasing the number of layers while maintaining the network complexity following the depth-first rule and scale it up to obtain a family of much deeper models dubbed DF-ResNets. Comprehensive comparisons with other state-of-the-art systems on the Voxceleb dataset demonstrate that DF-ResNets achieve a much better trade-off than previous SV systems in terms of performance and complexity",
    "checked": true,
    "id": "08fa9dbc0827ead5700bdf6d3e15d70ceaf65794",
    "semantic_title": "df-resnet: boosting speaker verification performance with depth-first design",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ruida22_interspeech.html": {
    "title": "Adaptive Rectangle Loss for Speaker Verification",
    "volume": "main",
    "abstract": "From the perspective of pair similarity optimization, speaker verification is expected to satisfy the criterion that each intraclass similarity is higher than the maximal inter-class similarity. However, we find that most softmax-based losses are suboptimal which encourages each sample to have a higher target similarity score only than its corresponding non-target similarity scores but not all the non-target ones. To this end, we propose a batch-wise maximum softmax loss, in which the non-target logits are replaced by the ones derived from the whole batch. To further emphasize the minority hard non-target pairs, an adaptive margin mechanism is introduced at the same time. The proposed loss is named Adaptive Rectangle loss due to its rectangle decision boundary. In addition, an annealing strategy is introduced to improve the stability of the training process and boost the convergence. Experimentally, we demonstrate the superiority of adaptive rectangle loss on speaker verification tasks. Results on VoxCeleb show that our proposed loss outperforms state-of-the-art by 10.11% in EER",
    "checked": true,
    "id": "43403366cb69f3f2422a08a159d8b42b7a82890d",
    "semantic_title": "adaptive rectangle loss for speaker verification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22h_interspeech.html": {
    "title": "MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification",
    "volume": "main",
    "abstract": "In this paper, we present Multi-scale Feature Aggregation Conformer (MFA-Conformer), an easy-to-implement, simple but effective backbone for automatic speaker verification based on the Convolution-augmented Transformer (Conformer). The architecture of the MFA-Conformer is inspired by recent state-of-the-art models in speech recognition and speaker verification. Firstly, we introduce a convolution subsampling layer to decrease the computational cost of the model. Secondly, we adopt Conformer blocks which combine Transformers and convolution neural networks (CNNs) to capture global and local features effectively. Finally, the output feature maps from all Conformer blocks are concatenated to aggregate multi-scale representations before final pooling. We evaluate the MFA-Conformer on the widely used benchmarks. The best system obtains 0.64%, 1.29% and 1.63% EER on VoxCeleb1-O, SITW.Dev, and SITW.Eval set, respectively. MFA-Conformer significantly outperforms the popular ECAPA-TDNN systems in both recognition performance and inference speed. Last but not the least, the ablation studies clearly demonstrate that the combination of global and local feature learning can lead to robust and accurate speaker embedding extraction. We have also released the code for future comparison",
    "checked": true,
    "id": "47942bfbc4fdde742137f70aac8028a46b1809d9",
    "semantic_title": "mfa-conformer: multi-scale feature aggregation conformer for automatic speaker verification",
    "citation_count": 40
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22j_interspeech.html": {
    "title": "Enroll-Aware Attentive Statistics Pooling for Target Speaker Verification",
    "volume": "main",
    "abstract": "The well-developed robust speaker verification system can remove the environment noise and retain speaker information automatically. However, when the uttering voice is disturbed by another interfering speaker's voice, the speaker verification system usually cannot selectively extract only the target speaker's information. Some works have been done by introducing a speech separation network to separate the target speaker's speech in advance. However, adding a speech separation network for speaker verification task could be redundant. Here, we proposed enroll-aware attentive statistic pooling (EA-ASP) layer to help the speaker verification system extract specific speaker's information. To evaluate the system, we simulate the multi-speaker evaluation data based on Voxceleb1 data. The results show that our proposed EA-ASP can outperform the baseline system by a large margin and achieved 50% relative Equal Error Rate (EER) reduction",
    "checked": true,
    "id": "e91d09f889ddb807263d63fc58a5eb886f958d94",
    "semantic_title": "enroll-aware attentive statistics pooling for target speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22b_interspeech.html": {
    "title": "Transport-Oriented Feature Aggregation for Speaker Embedding Learning",
    "volume": "main",
    "abstract": "Pooling is needed to aggregate frame-level features into utterance-level representations for speaker modeling. Given the success of statistics-based pooling methods, we hypothesize that speaker characteristics are well represented in the statistical distribution over the pre-aggregation layer's output, and propose to use transport-oriented feature aggregation for deriving speaker embeddings. The aggregated representation encodes the geometric structure of the underlying feature distribution, which is expected to contain valuable speaker-specific information that may not be represented by the commonly used statistical measures like mean and variance. The original transport-oriented feature aggregation is also extended to a weighted-frame version to incorporate the attention mechanism. Experiments on speaker verification with the Voxceleb dataset show improvement over statistics pooling and its attentive variant",
    "checked": true,
    "id": "365b03585694d22d10fafe2e366702cdc519ce63",
    "semantic_title": "transport-oriented feature aggregation for speaker embedding learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sang22_interspeech.html": {
    "title": "Multi-Frequency Information Enhanced Channel Attention Module for Speaker Representation Learning",
    "volume": "main",
    "abstract": "Recently, attention mechanisms have been applied successfully in neural network-based speaker verification systems. Incorporating the Squeeze-and-Excitation block into convolutional neural networks has achieved remarkable performance. However, it uses global average pooling (GAP) to simply average the features along time and frequency dimensions, which is incapable of preserving sufficient speaker information in the feature maps. In this study, we show that GAP is a special case of a discrete cosine transform (DCT) on time-frequency domain mathematically using only the lowest frequency component in frequency decomposition. To strengthen the speaker information extraction ability, we propose to utilize multi-frequency information and design two novel and effective attention modules, called Single-Frequency Single-Channel (SFSC) attention module and Multi-Frequency Single-Channel (MFSC) attention module. The proposed attention modules can effectively capture more speaker information from multiple frequency components on the basis of DCT. We conduct comprehensive experiments on the VoxCeleb datasets and a probe evaluation on the 1st 48-UTD forensic corpus. Experimental results demonstrate that our proposed SFSC and MFSC attention modules can efficiently generate more discriminative speaker representations and outperform ResNet34-SE and ECAPA-TDNN systems with relative 20.9% and 20.2% reduction in EER, without adding extra network parameters",
    "checked": true,
    "id": "4d052febd3625bc0fd09c81d51ec1c4d58bad95d",
    "semantic_title": "multi-frequency information enhanced channel attention module for speaker representation learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cai22_interspeech.html": {
    "title": "CS-CTCSCONV1D: Small footprint speaker verification with channel split time-channel-time separable 1-dimensional convolution",
    "volume": "main",
    "abstract": "We present an efficient small-footprint network for speaker verification. We start by introducing the bottleneck to the QuartzNet model. Then we proposed a Channel Split Time Channel-Time Separable 1-dimensional Convolution (CS-CTCSConv1d) module, yielding stronger performance over the State-Of-The-Art small footprint speaker verification system. We apply knowledge distillation to further improve performance to learn better speaker embedding from the large model. We evaluate the proposed approach on Voxceleb dataset, obtaining better performances concerning the baseline method. The proposed model takes only 238.9K parameters to outperform the baseline system by 10% relatively in equal error rate (EER)",
    "checked": true,
    "id": "185c5fdc6604d49249388742f3b9e08dfa1d873f",
    "semantic_title": "cs-ctcsconv1d: small footprint speaker verification with channel split time-channel-time separable 1-dimensional convolution",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22l_interspeech.html": {
    "title": "Reliable Visualization for Deep Speaker Recognition",
    "volume": "main",
    "abstract": "In spite of the impressive success of convolutional neural networks (CNNs) in speaker recognition, our understanding to CNNs' internal functions is still limited. A major obstacle is that some popular visualization tools are difficult to apply, for example those producing saliency maps. The reason is that speaker information does not show clear spatial patterns in the temporal-frequency space, which makes it hard to interpret the visualization results, and hence hard to confirm the reliability of a visualization tool. In this paper, we conduct an extensive analysis on three popular visualization methods based on class activation map(CAM): Grad-CAM, Score-CAM and Layer-CAM, to investigate their reliability for speaker recognition tasks. Experiments conducted on a state-of-the-art ResNet34SE model show that the Layer-CAM algorithm can produce reliable visualization, and thus can be used as a promising tool to explain CNN-based speaker models. The source code and examples are available in our project page: http://project.cslt.org/",
    "checked": true,
    "id": "9b532f3979e24db8fc0f621ef278e641474e71c9",
    "semantic_title": "reliable visualization for deep speaker recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22b_interspeech.html": {
    "title": "Unifying Cosine and PLDA Back-ends for Speaker Verification",
    "volume": "main",
    "abstract": "State-of-art speaker verification (SV) systems use a back-end model to score the similarity of speaker embeddings extracted from a neural network. The commonly used back-ends are the cosine scoring and the probabilistic linear discriminant analysis (PLDA) scoring. With the recently developed neural embeddings, the theoretically more appealing PLDA approach is found to have no advantage against or even be inferior to the simple cosine scoring in terms of verification performance. This paper presents an investigation on the relation between the two back-ends, aiming to explain the above counter-intuitive observation. It is shown that the cosine scoring is essentially a special case of PLDA scoring. In other words, by properly setting the parameters of PLDA, the two back-ends become equivalent. As a consequence, the cosine scoring not only inherits the basic assumptions for the PLDA but also introduces additional assumptions on speaker embeddings. Experiments show that the dimensional independence assumption required by the cosine scoring contributes most to the performance gap between the two methods under the domain-matched condition. When there is severe domain mismatch, the dimensional independence assumption does not hold and the PLDA would perform better than the cosine for domain adaptation",
    "checked": true,
    "id": "8642361b68a7d1490face31e098fc95ebd448b0e",
    "semantic_title": "unifying cosine and plda back-ends for speaker verification",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22d_interspeech.html": {
    "title": "CTFALite: Lightweight Channel-specific Temporal and Frequency Attention Mechanism for Enhancing the Speaker Embedding Extractor",
    "volume": "main",
    "abstract": "Attention mechanism provides an effective and plug-and-play feature enhancement module for speaker embedding extractors. Attention-based pooling layers have been widely used to aggregate a sequence of frame-level feature vectors into an utterance-level speaker embedding. Besides, convolution attention mechanisms are introduced into convolution blocks to improve the sensibility of speaker embedding extractors to those features with more discriminative speaker characteristics. However, it is still a challenging problem to make a good trade off between performance and model complexity for convolution attention models, especially for speaker recognition systems on low-resource edge computing nodes (smartphone, embedded devices, etc.). In this paper, we propose a lightweight convolution attention model named as CTFALite, which learns channel-specific temporal attention and frequency attention by leveraging both of the global context information and the local cross-channel dependencies. Experiment results demonstrate the effectiveness of CTFALite for improving performance. The further analysis about computational resource consumption shows that CTFALite achieves a better trade-off between performance and computational complexity, compared to other competing lightweight convolution attention mechanisms",
    "checked": true,
    "id": "5a7ad4b9b94d5bdc0087532c1689dc55bb9acec5",
    "semantic_title": "ctfalite: lightweight channel-specific temporal and frequency attention mechanism for enhancing the speaker embedding extractor",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22_interspeech.html": {
    "title": "SpeechFormer: A Hierarchical Efficient Framework Incorporating the Characteristics of Speech",
    "volume": "main",
    "abstract": "Transformer has obtained promising results on cognitive speech signal processing field, which is of interest in various applications ranging from emotion to neurocognitive disorder analysis. However, most works treat speech signal as a whole, leading to the neglect of the pronunciation structure that is unique to speech and reflects the cognitive process. Meanwhile, Transformer has heavy computational burden due to its full attention operation. In this paper, a hierarchical efficient framework, called SpeechFormer, which considers the structural characteristics of speech, is proposed and can be served as a general-purpose backbone for cognitive speech signal processing. The proposed SpeechFormer consists of frame, phoneme, word and utterance stages in succession, each performing a neighboring attention according to the structural pattern of speech with high computational efficiency. SpeechFormer is evaluated on speech emotion recognition (IEMOCAP & MELD) and neurocognitive disorder detection (Pitt & DAIC-WOZ) tasks, and the results show that SpeechFormer outperforms the standard Transformer-based framework while greatly reducing the computational cost. Furthermore, our SpeechFormer achieves comparable results to the state-of-the-art approaches",
    "checked": true,
    "id": "40eca3f99c70f50501fdae4a0e2dd9a239d5d70d",
    "semantic_title": "speechformer: a hierarchical efficient framework incorporating the characteristics of speech",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2022/feinberg22_interspeech.html": {
    "title": "VoiceLab: Software for Fully Reproducible Automated Voice Analysis",
    "volume": "main",
    "abstract": "There's a problem with acoustic analyses because you often need to hand adjust parameters meaning you can only process them individually or in small batches. This creates two key problems. First, it compromises the reproducibility of measurements because setting parameters by hand requires specialist knowledge and is often poorly documented. Second, it means that you can't easily process large samples of voices, which creates bottlenecks in workflows. This issue is compounded by researchers looking to use increasingly large and diverse samples. To address these issues, VoiceLab software offers automated acoustical analysis and automatically logs analysis parameters. VoiceLab analyses are fully reproducible and require little to no knowledge about acoustical analysis from the user. Analysis parameters can also be manually adjusted by experts. VoiceLab is used primarily by researchers studying person perception, creating reproducible voice manipulations, developing voices for conversational agents, and creating feature sets for machine learning",
    "checked": true,
    "id": "4a7e4b4e19217734423ad1f6d8753745582807af",
    "semantic_title": "voicelab: software for fully reproducible automated voice analysis",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shor22_interspeech.html": {
    "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
    "volume": "main",
    "abstract": "Recent advances in self-supervision have dramatically improved the quality of speech representations. However, deployment of state-of-the-art embedding models on devices has been restricted due to their limited public availability and large resource footprint. Our work addresses these issues by publicly releasing a collection of paralinguistic speech models that are small and near state-of-the-art performance. Our approach is based on knowledge distillation, and our models are distilled on public data only. We explore different architectures and thoroughly evaluate our models on the NonSemantic Speech (NOSS) benchmark. Our largest distilled model achieves over 96% the accuracy on 6 of 7 tasks, is less than 15% the size of the original model (314MB vs 2.2GB), and is trained on 6.5% the data. The smallest model achieves over 90% the accuracy on 6 of 7 tasks and is 1% in size (22MB). Our models outperform the 1.2GB open source Wav2Vec 2.0 model on 5 of 7 tasks despite being less than a third the size, and one of our models outperforms Wav2Vec 2.0 on both emotion recognition tasks despite being less than 4% the size",
    "checked": true,
    "id": "4d35845f81c5afe83c9b101507d2a187ef4d8c3c",
    "semantic_title": "trillsson: distilled universal paralinguistic speech representations",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22b_interspeech.html": {
    "title": "Global Signal-to-noise Ratio Estimation Based on Multi-subband Processing Using Convolutional Neural Network",
    "volume": "main",
    "abstract": "The global signal-to-noise ratio (gSNR) is defined as the ratio of speech energy to noise energy in whole noisy audio. However, due to the increase in noise interference, the generalization ability declines when the traditional features (e.g., raw waveforms and MFCCs) are fed directly to the statistical model to estimate a single fullband gSNR. In this paper, we propose a multi-subband-based gSNR estimation network (MSGNet). Specifically, we split the noisy speech waveforms into Bark-scale subbands to obtain higher resolution signals to the middle and low frequencies. Then, convolutional neural networks (CNNs) are used to learn a non-linear function to estimate the speech and noise energy ratio of each subband from the input muti-subband features. Finally, by integrating subbands with different speech and noise energies, gSNR in the fullband is calculated. Extensive experimental results on the AURORA-2J dataset demonstrate that the proposed MSGNet significantly reduces the mean absolute error compared to other baseline gSNR estimation methods",
    "checked": true,
    "id": "4c159b24ee30dece185cc167e2ed3bce1ee73405",
    "semantic_title": "global signal-to-noise ratio estimation based on multi-subband processing using convolutional neural network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sadeghi22_interspeech.html": {
    "title": "A Sparsity-promoting Dictionary Model for Variational Autoencoders",
    "volume": "main",
    "abstract": "Structuring the latent space in probabilistic deep generative models, e.g., variational autoencoders (VAEs), is important to yield more expressive models and interpretable representations, and to avoid overfitting. One way to achieve this objective is to impose a sparsity constraint on the latent variables, e.g., via a Laplace prior. However, such approaches usually complicate the training phase, and they sacrifice the reconstruction quality to promote sparsity. In this paper, we propose a simple yet effective methodology to structure the latent space via a sparsity-promoting dictionary model, which assumes that each latent code can be written as a sparse linear combination of a dictionary's columns. In particular, we leverage a computationally efficient and tuning-free method, which relies on a zero-mean Gaussian latent prior with learnable variances. We derive a variational inference scheme to train the model. Experiments on speech generative modeling demonstrate the advantage of the proposed approach over competing techniques, since it promotes sparsity while not deteriorating the output speech quality",
    "checked": true,
    "id": "6aed8f70d922527d172da51aa35c2ff6fd99cb9d",
    "semantic_title": "a sparsity-promoting dictionary model for variational autoencoders",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22h_interspeech.html": {
    "title": "Deep Transductive Transfer Regression Network for Cross-Corpus Speech Emotion Recognition",
    "volume": "main",
    "abstract": "In this paper, we focus on the research of cross-corpus speech emotion recognition (SER), in which the training (source) and testing (target) speech samples come from different corpora leading to a feature distribution gap between them. To solve this problem, we propose a simple yet effective method called deep transductive transfer regression network (DTTRN). The basic idea of DTTRN is to learn a corpus invariant deep neural network to bridge the source and target speech samples and their label information. Following this idea, we make use of a transductive learning way to enforce a deep regressor to build the relationship between the features and emotional labels jointly in both speech corpora. Meanwhile, we also design an emotion guided regularization term for learning DTTRN by aligning source and target speech samples feature distributions from three different scales. Thus, the DTTRN only absorbing the label information provided by source speech samples is able to correctly predict the emotions of the target ones. To evaluate DTTRN, we conduct extensive cross-corpus SER experiments on EmoDB, CASIA, and eNTERFACE corpora. Experimental results show the superior performance of our DTTRN over recent state-of-the-art deep transfer learning methods in dealing with the cross-corpus SER tasks",
    "checked": true,
    "id": "ea6cbe4b1163207601c5bb0932630a6acd2b8ed3",
    "semantic_title": "deep transductive transfer regression network for cross-corpus speech emotion recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hansen22_interspeech.html": {
    "title": "Audio Anti-spoofing Using Simple Attention Module and Joint Optimization Based on Additive Angular Margin Loss and Meta-learning",
    "volume": "main",
    "abstract": "Automatic speaker verification systems are vulnerable to a variety of access threats, prompting research into the formulation of effective spoofing detection systems to act as a gate to filter out such spoofing attacks. This study introduces a simple attention module to infer 3-dim attention weights for the feature map in a convolutional layer, which then optimizes an energy function to determine each neuron's importance. With the advancement of both voice conversion and speech synthesis technologies, un-seen spoofing attacks are constantly emerging to limit spoofing detection system performance. Here, we propose a joint optimization approach based on the weighted additive angular margin loss for binary classification, with a meta-learning training framework to develop an efficient system that is robust to a wide range of spoofing attacks for model generalization en- enhancement. As a result, when compared to current state-of-the-art systems, our proposed approach delivers a competitive result with a pooled EER of 0.99% and min t-DCF of 0.0289",
    "checked": true,
    "id": "e871fa38d82b20d21dc925548802c27718346185",
    "semantic_title": "audio anti-spoofing using simple attention module and joint optimization based on additive angular margin loss and meta-learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bergsma22_interspeech.html": {
    "title": "PEAF: Learnable Power Efficient Analog Acoustic Features for Audio Recognition",
    "volume": "main",
    "abstract": "At the end of Moore's law, new computing paradigms are required to prolong the battery life of wearable and IoT smart audio devices. Theoretical analysis and physical validation have shown that analog signal processing (ASP) can be more power-efficient than its digital counterpart in the realm of low-to-medium signal-to-noise ratio applications. In addition, ASP allows a direct interface with an analog microphone without a power-hungry analog-to-digital converter. Here, we present power-efficient analog acoustic features (PEAF) that are validated by fabricated CMOS chips for running audio recognition. Linear, non-linear, and learnable PEAF variants are evaluated on two speech processing tasks that are demanded in many battery-operated devices: wake word detection (WWD) and keyword spotting (KWS). Compared to digital acoustic features, higher power efficiency with competitive classification accuracy can be obtained. A novel theoretical framework based on information theory is established to analyze the information flow in each individual stage of the feature extraction pipeline. The analysis identifies the information bottleneck and helps improve the KWS accuracy by up to 7%. This work may pave the way to building more power-efficient smart audio devices with best-in-class inference performance",
    "checked": true,
    "id": "4abd6fc03c16ea39c3285db8ffca6bf0de87dc48",
    "semantic_title": "peaf: learnable power efficient analog acoustic features for audio recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/elbanna22_interspeech.html": {
    "title": "Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load",
    "volume": "main",
    "abstract": "As a neurophysiological response to threat or adverse conditions, stress can affect cognition, emotion and behaviour with potentially detrimental effects on health in the case of sustained exposure. Since the affective content of speech is inherently modulated by an individual's physical and mental state, a substantial body of research has been devoted to the study of paralinguistic correlates of stress-inducing task load. Historically, voice stress analysis (VSA) has been conducted using conventional digital signal processing (DSP) techniques. Despite the development of modern methods based on deep neural networks (DNNs), accurately detecting stress in speech remains difficult due to the wide variety of stressors and considerable variability in the individual stress perception. To that end, we introduce a set of five datasets for task load detection in speech. The voice recordings were collected as either cognitive or physical stress was induced in the cohort of volunteers, with a cumulative number of more than a hundred speakers. We used the datasets to design and evaluate a novel self-supervised audio representation that leverages the effectiveness of handcrafted features (DSP-based) and the complexity of data-driven DNN representations. Notably, the proposed approach outperformed both extensive handcrafted feature sets and novel DNN-based audio representation learning approaches",
    "checked": true,
    "id": "f9fc66bdecafcc90d6dbc3991f91975a1da6e09e",
    "semantic_title": "hybrid handcrafted and learnable audio representation for analysis of speech under cognitive and physical load",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22w_interspeech.html": {
    "title": "Generative Data Augmentation Guided by Triplet Loss for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) is crucial for human-computer interaction but still remains a challenging problem because of two major obstacles: data scarcity and imbalance. Many datasets for SER are substantially imbalanced, where data utterances of one class (most often Neutral) are much more frequent than those of other classes. Furthermore, only a few data resources are available for many existing spoken languages. To address these problems, we exploit a GAN-based augmentation model guided by a triplet network, to improve SER performance given imbalanced and insufficient training data. We conduct experiments and demonstrate: 1) With a highly imbalanced dataset, our augmentation strategy significantly improves the SER performance (+8\\% recall score compared with the baseline). 2) Moreover, in a cross-lingual benchmark, where we train a model with enough source language utterances but very few target language utterances (around 50 in our experiments), our augmentation strategy brings benefits for the SER performance of all three target languages",
    "checked": true,
    "id": "8c470b97f8ae3e9e9f623e38be01f6ad6d69d79c",
    "semantic_title": "generative data augmentation guided by triplet loss for speech emotion recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yadav22_interspeech.html": {
    "title": "Learning neural audio features without supervision",
    "volume": "main",
    "abstract": "Deep audio classification, traditionally cast as training a deep neural network on top of mel-filterbanks in a supervised fashion, has recently benefited from two independent lines of work. The first one explores \"learnable frontends'', i.e., neural modules that produce a learnable time-frequency representation, to overcome limitations of fixed features. The second one uses self-supervised learning to leverage unprecedented scales of pre-training data. In this work, we study the feasibility of combining both approaches, i.e., pre-training learnable frontend jointly with the main architecture for downstream classification. First, we show that pretraining two previously proposed frontends (SincNet and LEAF) on Audioset drastically improves linear-probe performance over fixed mel-filterbanks, suggesting that learnable time-frequency representations can benefit self-supervised pre-training even more than supervised training. Surprisingly, randomly initialized learnable filterbanks outperform mel-scaled initialization in the self-supervised setting, a counter-intuitive result that questions the appropriateness of strong priors when designing learnable filters. Through exploratory analysis of the learned frontend components, we uncover crucial differences in properties of these frontends when used in a supervised and self-supervised setting, especially the affinity of self-supervised filters to diverge significantly from the mel-scale to model a broader range of frequencies",
    "checked": true,
    "id": "f67d1a19823dec6eeb32339018f89a6176d38477",
    "semantic_title": "learning neural audio features without supervision",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22ca_interspeech.html": {
    "title": "Densely-connected Convolutional Recurrent Network for Fundamental Frequency Estimation in Noisy Speech",
    "volume": "main",
    "abstract": "Estimating fundamental frequency (F0) from an audio signal is a necessary step in many tasks such as speech synthesis and speech analysis. Although high estimation accuracy has been achieved for clean speech, it is still challenging for F0 estimation to handle noisy speech, mainly because of the corruption of harmonic structure caused by noise. In this paper, we view F0 estimation as a multi-class classification problem and train a frequency-domain densely-connected convolutional neural network (DC-CRN) to estimate F0 from noisy speech. The proposed model significantly outperforms baseline methods in terms of detection rate. We find that using complex short-time Fourier transform (STFT) as input produces better performance compared to using magnitude STFT as input. Furthermore, we explore improving F0 estimation with speech enhancement. Although the F0 estimation model trained on clean speech performs well on enhanced speech, the distortion introduced by the speech enhancement model limits the estimation performance. We propose a cascade model which consists of two modules that optimize enhanced speech and estimated F0 in turn. Experimental results show that the cascade model brings further improvements to the DC-CRN model, especially in low signal-to-noise ratio (SNR) conditions",
    "checked": true,
    "id": "b2b0e3b8d6600fd82e33a0aac512f64d54d6988d",
    "semantic_title": "densely-connected convolutional recurrent network for fundamental frequency estimation in noisy speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/faridee22_interspeech.html": {
    "title": "Predicting label distribution improves non-intrusive speech quality estimation",
    "volume": "main",
    "abstract": "Deep noise suppressors (DNS) have become an attractive solution to remove background noise, reverberation, and distortions from speech and are widely used in telephony/voice applications. They are also occasionally prone to introducing artifacts and lowering the perceptual quality of the speech. Subjective listening tests that use multiple human judges to derive a mean opinion score (MOS) are a popular way to measure these models' performance. Deep neural network based non-intrusive MOS estimation models have recently emerged as a popular cost-efficient alternative to these tests. These models are trained with only the MOS labels, often discarding the secondary statistics of the opinion scores. In this paper, we investigate several ways to integrate the distribution of opinion scores (e.g. variance, histogram information) to improve the MOS estimation performance. Our model is trained on a corpus of 419K denoised samples by 320 different DNS models and model variations and evaluated on 18K test samples from DNSMOS. We show that with very minor modification of a single task MOS estimation pipeline, these freely available labels can provide up to a 0.016 RMSE and 1% SRCC improvement",
    "checked": true,
    "id": "2be30cd0b7657718282804391e2039337af2229a",
    "semantic_title": "predicting label distribution improves non-intrusive speech quality estimation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ashihara22_interspeech.html": {
    "title": "Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) is seen as a very promising approach with high performance for several speech downstream tasks. Since the parameters of SSL models are generally so large that training and inference require a lot of memory and computational cost, it is desirable to produce compact SSL models without a significant performance degradation by applying compression methods such as knowledge distillation (KD). Although the KD approach is able to shrink the depth and/or width of SSL model structures, there has been little research on how varying the depth and width impacts the internal representation of the small-footprint model. This paper provides an empirical study that addresses the question. We investigate the performance on SUPERB while varying the structure and KD methods so as to keep the number of parameters constant; this allows us to analyze the contribution of the representation introduced by varying the model architecture. Experiments demonstrate that a certain depth is essential for solving content-oriented tasks (e.g. automatic speech recognition) accurately, whereas a certain width is necessary for achieving high performance on several speaker-oriented tasks (e.g. speaker identification). Based on these observations, we identify, for SUPERB, a more compressed model with better performance than previous studies",
    "checked": true,
    "id": "1d91bc3979e87380c31ac8aa152919228a639a04",
    "semantic_title": "deep versus wide: an analysis of student architectures for task-agnostic knowledge distillation of self-supervised speech models",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/azeemi22_interspeech.html": {
    "title": "Dataset Pruning for Resource-constrained Spoofed Audio Detection",
    "volume": "main",
    "abstract": "The performance of neural anti-spoofing models has rapidly improved in recent years due to larger network architectures and better training methodologies. However, these systems require considerable training data for achieving high performance, which makes it challenging to train them in compute-restricted environments. To make these systems accessible in resource-constrained environments, we consider the task of training neural anti-spoofing models with limited training data. We apply multiple dataset pruning techniques to the ASVspoof 2019 dataset for selecting the most informative training examples and pruning a significant chunk of the data with minimal decrease in performance. We find that the existing pruning metrics are not simultaneously granular and stable. To address this problem and further improve the performance of anti-spoofing models on pruned data, we propose a new metric, Forgetting Norm, to score individual training examples with higher granularity. Extensive experiments on two anti-spoofing models, AASIST-L and RawNet2, and several pruning settings demonstrate up to 23% relative improvement with forgetting norm over other baseline pruning heuristics. We also demonstrate the desirable properties of the proposed metric by analyzing the training landscape of the neural anti-spoofing models",
    "checked": true,
    "id": "1d9200f3eec099abbab3a62d3ab5a9e251a526b5",
    "semantic_title": "dataset pruning for resource-constrained spoofed audio detection",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tae22_interspeech.html": {
    "title": "EdiTTS: Score-based Editing for Controllable Text-to-Speech",
    "volume": "main",
    "abstract": "We present EdiTTS, an off-the-shelf speech editing methodology based on score-based generative modeling for text-to-speech synthesis. EdiTTS allows for targeted, granular editing of audio, both in terms of content and pitch, without the need for any additional training, task-specific optimization, or architectural modifications to the score-based model backbone. Specifically, we apply coarse yet deliberate perturbations in the Gaussian prior space to induce desired behavior from the diffusion model while applying masks and softening kernels to ensure that iterative edits are applied only to the target region. Through listening tests and speech-to-text back transcription, we show that EdiTTS outperforms existing baselines and produces robust samples that satisfy user-imposed requirements",
    "checked": true,
    "id": "4ecf116b9a8fdee90030a9c8c45d9c20b77bc703",
    "semantic_title": "editts: score-based editing for controllable text-to-speech",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22b_interspeech.html": {
    "title": "Improving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information",
    "volume": "main",
    "abstract": "For text-to-speech (TTS) synthesis, prosodic structure prediction (PSP) plays an important role in producing natural and intelligible speech. Although inter-utterance linguistic information can influence the speech interpretation of the target utterance, previous works on PSP mainly focus on utilizing intrautterance linguistic information of the current utterance only. This work proposes to use inter-utterance linguistic information to improve the performance of PSP. Multi-level contextual information, which includes both inter-utterance and intrautterance linguistic information, is extracted by a hierarchical encoder from character level, utterance level and discourse level of the input text. Then a multi-task learning (MTL) decoder predicts prosodic boundaries from multi-level contextual information. Objective evaluation results on two datasets show that our method achieves better F1 scores in predicting prosodic word (PW), prosodic phrase (PPH) and intonational phrase (IPH). It demonstrates the effectiveness of using multi-level contextual information for PSP. Subjective preference tests also indicate the naturalness of synthesized speeches are improved",
    "checked": true,
    "id": "86ff8b31cf636ece970a6a8bd56e2611848845db",
    "semantic_title": "improving mandarin prosodic structure prediction with multi-level contextual information",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/borsos22_interspeech.html": {
    "title": "SpeechPainter: Text-conditioned Speech Inpainting",
    "volume": "main",
    "abstract": "We propose SpeechPainter, a model for filling in gaps of up to one second in speech samples by leveraging an auxiliary textual input. We demonstrate that the model performs speech inpainting with the appropriate content, while maintaining speaker identity, prosody and recording environment conditions, and generalizing to unseen speakers. Our approach significantly outperforms baselines constructed using adaptive TTS, as judged by human raters in side-by-side preference and MOS tests",
    "checked": true,
    "id": "0a33c8d2e83b2d8d611698b52805232567724817",
    "semantic_title": "speechpainter: text-conditioned speech inpainting",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22b_interspeech.html": {
    "title": "A polyphone BERT for Polyphone Disambiguation in Mandarin Chinese",
    "volume": "main",
    "abstract": "Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese Mandarin text-to-speech (TTS) system, and the core of G2P conversion is to solve the problem of polyphone disambiguation, which is to pick up the correct pronunciation for several candidates for a Chinese polyphonic character. In this paper, we propose a Chinese polyphone BERT model to predict the pronunciations of Chinese polyphonic characters. Firstly, we create 741 new Chinese monophonic characters from 354 source Chinese polyphonic characters by pronunciation. Then we get a Chinese polyphone BERT by extending a pre-trained Chinese BERT with 741 new Chinese monophonic characters and adding a corresponding embedding layer for new tokens, which is initialized by the embeddings of source Chinese polyphonic characters. In this way, we can turn the polyphone disambiguation task into a pre-training task of the Chinese polyphone BERT. Experimental results demonstrate the effectiveness of the proposed model, and the polyphone BERT model obtain 2% (from 92.1% to 94.1%) improvement of average accuracy compared with the BERT-based classifier model, which is the prior state-of-the-art in polyphone disambiguation",
    "checked": true,
    "id": "83b5ea4e7ac8dba943c2e720e46aa4d4fca9e190",
    "semantic_title": "a polyphone bert for polyphone disambiguation in mandarin chinese",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/he22b_interspeech.html": {
    "title": "Neural Lexicon Reader: Reduce Pronunciation Errors in End-to-end TTS by Leveraging External Textual Knowledge",
    "volume": "main",
    "abstract": "End-to-end TTS requires a large amount of speech/text paired data to cover all necessary knowledge, particularly how to pronounce different words in diverse contexts, so that a neural model may learn such knowledge accordingly. But in real applications, such high demand of training data is hard to be satisfied and additional knowledge often needs to be injected manually. For example, to capture pronunciation knowledge on languages without regular orthography, a complicated grapheme-to-phoneme pipeline needs to be built based on a large structured pronunciation lexicon, leading to extra, sometimes high, costs to extend neural TTS to such languages. In this paper, we propose a framework to learn to automatically extract knowledge from unstructured external resources using a novel Token2Knowledge attention module. The framework is applied to build a TTS model named Neural Lexicon Reader that extracts pronunciations from raw lexicon texts in an end-to-end manner. Experiments show the proposed model significantly reduces pronunciation errors in low-resource, end-to-end Chinese TTS, and the lexicon-reading capability can be transferred to other languages with a smaller amount of data",
    "checked": true,
    "id": "d2b4a3a3fe9915b7d9455fe177f76a8884a05673",
    "semantic_title": "neural lexicon reader: reduce pronunciation errors in end-to-end tts by leveraging external textual knowledge",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22_interspeech.html": {
    "title": "ByT5 model for massively multilingual grapheme-to-phoneme conversion",
    "volume": "main",
    "abstract": "In this study, we tackle massively multilingual grapheme-to-phoneme conversion through implementing G2P models based on ByT5. We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. We found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P. Pairwise comparison with monolingual models in these languages suggests that multilingual ByT5 models generally lower the phone error rate by jointly learning from a variety of languages. The pretrained model can further benefit low resource G2P through zero-shot prediction on unseen languages or provides pretrained weights for finetuning, which helps the model converge to a lower phone error rate than randomly initialized weights. To facilitate future research on multilingual G2P, we make available our code and pretrained multilingual G2P models at: https://github.com/lingjzhu/CharsiuG2P",
    "checked": true,
    "id": "880c8973ea1376c6bff23226b3f64792657aa666",
    "semantic_title": "byt5 model for massively multilingual grapheme-to-phoneme conversion",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mathur22_interspeech.html": {
    "title": "DocLayoutTTS: Dataset and Baselines for Layout-informed Document-level Neural Speech Synthesis",
    "volume": "main",
    "abstract": "We propose a new task of synthesizing speech directly from semi-structured documents where the extracted text tokens from OCR systems may not be in the correct reading order due to the complex document layout. We refer to this task as layout-informed document-level TTS and present the DocSpeech dataset which consists of 10K audio clips of a single-speaker reading layout-enriched Word document. For each document, we provide the natural reading order of text tokens, their corresponding bounding boxes, and the audio clips synthesized in the correct reading order. We also introduce DocLayoutTTS, a Transformer encoder-decoder architecture that generates speech in an end-to-end manner given a document image with OCR extracted text. Our architecture simultaneously learns text reordering and mel-spectrogram prediction in a multi-task setup. Moreover, we take advantage of curriculum learning to progressively learn longer, more challenging document-level text utilizing both \\texttt{DocSpeech} and LJSpeech datasets. Our empirical results show that the underlying task is challenging. Our proposed architecture performs slightly better than competitive baseline TTS models with a pre-trained model providing reading order priors. We release samples of the DocSpeech dataset",
    "checked": true,
    "id": "43e9409c847cefd7ec00e65635cd995cbb017862",
    "semantic_title": "doclayouttts: dataset and baselines for layout-informed document-level neural speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22i_interspeech.html": {
    "title": "Mixed-Phoneme BERT: Improving BERT with Mixed Phoneme and Sup-Phoneme Representations for Text to Speech",
    "volume": "main",
    "abstract": "Recently, leveraging BERT pre-training to improve the phoneme encoder in text to speech (TTS) has drawn increasing attention. However, the works apply pre-training with character-based units to enhance the TTS phoneme encoder, which is inconsistent with the TTS fine-tuning that takes phonemes as input. Pre-training only with phonemes as input can alleviate the input mismatch but lack the ability to model rich representations and sematic information due to limited phoneme vocabulary. In this paper, we propose Mixed-Phoneme BERT, a novel variant of the BERT model that uses mixed phoneme and sup-phoneme representations to enhance the learning capability. Specifically, we merge the adjacent phonemes into sup-phonemes and combine the phoneme sequence and the merged sup-phoneme sequence as the model input, which can enhance the model capacity to learn rich contextual representations. Experiment results demonstrate that our proposed Mixed-Phoneme BERT significantly improves the TTS performance with 0.30 CMOS gain compared with the FastSpeech 2 baseline. The Mixed-Phoneme BERT achieves $3\\times$ inference speedup and similar voice quality to the previous TTS pre-trained model PnG BERT",
    "checked": true,
    "id": "b3b1659c992cbbd233038522ddd170887c033fe7",
    "semantic_title": "mixed-phoneme bert: improving bert with mixed phoneme and sup-phoneme representations for text to speech",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ni22_interspeech.html": {
    "title": "Unsupervised Text-to-Speech Synthesis by Unsupervised Automatic Speech Recognition",
    "volume": "main",
    "abstract": "An unsupervised text-to-speech synthesis (TTS) system learns to generate speech waveforms corresponding to any written sentence in a language by observing: 1) a collection of untranscribed speech waveforms in that language; 2) a collection of texts written in that language without access to any transcribed speech. Developing such a system can significantly improve the availability of speech technology to languages without a large amount of parallel speech and text data. This paper proposes an unsupervised TTS system based on an alignment module that outputs pseudo-text and another synthesis module that uses pseudo-text for training and real text for inference. Our unsupervised system can achieve comparable performance to the supervised system in seven languages with about 10-20 hours of speech each. A careful study on the effect of text units and vocoders has also been conducted to better understand what factors may affect unsupervised TTS performance. The samples generated by our models can be found at https://cactuswiththoughts.github.io/UnsupTTS-Demo, and our code can be found at https://github.com/lwang114/UnsupTTS",
    "checked": true,
    "id": "ed8fccab99d018f478899d3b9014b8f98a9af38f",
    "semantic_title": "unsupervised text-to-speech synthesis by unsupervised automatic speech recognition",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tran22_interspeech.html": {
    "title": "An Efficient and High Fidelity Vietnamese Streaming End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "In recent years, parallel end-to-end speech synthesis systems have outperformed the 2-stage TTS approaches in audio quality and latency. A parallel end-to-end speech like VITS can generate the audio with high MOS comparable to ground truth and achieve low latency on GPU. However, the VITS still has high latency when synthesizing long utterances on CPUs. Therefore, in this paper, we propose a streaming method for the parallel speech synthesis model like VITS to synthesize with the long texts effectively on CPU. Our system has achieved human-like speech quality in both the non-streaming and streaming mode on the in-house Vietnamese evaluation set, while the synthesis speed of our system is approximately four times faster than that of the VITS in the non-streaming mode. Furthermore, the customer perceived latency of our system in streaming mode is 25 times faster than the VITS on computer CPU. Our system in non-streaming mode achieves a MOS of 4.43 compared to ground-truth with MOS 4.56; it also has high-quality speech with a MOS of 4.35 in streaming mode. Finally, we release a Vietnamese single accent dataset used in our experiments",
    "checked": true,
    "id": "be2ee91e2e9b746d4a3a1aa5707d071dc072fca7",
    "semantic_title": "an efficient and high fidelity vietnamese streaming end-to-end speech synthesis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/valentinibotinhao22_interspeech.html": {
    "title": "Predicting pairwise preferences between TTS audio stimuli using parallel ratings data and anti-symmetric twin neural networks",
    "volume": "main",
    "abstract": "Automatically predicting the outcome of subjective listening tests is a challenging task. Ratings may vary from person to person even if preferences are consistent across listeners. While previous work has focused on predicting listeners' ratings (mean opinion scores) of individual stimuli, we focus on the simpler task of predicting subjective preference given two speech stimuli for the same text. We propose a model based on anti-symmetric twin neural networks, trained on pairs of waveforms and their corresponding preference scores. We explore both attention and recurrent neural nets to account for the fact that stimuli in a pair are not time aligned. To obtain a large training set we convert listeners' ratings from MUSHRA tests to values that reflect how often one stimulus in the pair was rated higher than the other. Specifically, we evaluate performance on data obtained from twelve MUSHRA evaluations conducted over five years, containing different TTS systems, built from data of different speakers. Our results compare favourably to a state-of-the-art model trained to predict MOS scores",
    "checked": true,
    "id": "8b0850810f852fe7c307aa0afd66e61a53ede988",
    "semantic_title": "predicting pairwise preferences between tts audio stimuli using parallel ratings data and anti-symmetric twin neural networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22j_interspeech.html": {
    "title": "An Automatic Soundtracking System for Text-to-Speech Audiobooks",
    "volume": "main",
    "abstract": "Background music (BGM) plays an essential role in audiobooks, which can enhance the immersive experience of audiences and help them better understand the story. However, well-designed BGM still requires human effort in the text-to-speech (TTS) audiobook production, which is quite time-consuming and costly. In this paper, we introduce an automatic soundtracking system for TTS-based audiobooks. The proposed system divides the soundtracking process into three tasks: plot partition, plot classification, and music selection. The experiments shows that both our plot partition module and plot classification module outperform baselines by a large margin. Furthermore, TTS-based audiobooks produced with our proposed automatic soundtracking system achieves comparable performance to that produced with the human soundtracking system. To our best of knowledge, this is the first work of automatic soundtracking system for audiobooks. Demos are available on https://acst1223.github.io/interspeech2022/main",
    "checked": true,
    "id": "b9853557e0362976c8205b68b011bfcf0d5b4161",
    "semantic_title": "an automatic soundtracking system for text-to-speech audiobooks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tan22_interspeech.html": {
    "title": "Environment Aware Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "This study aims at designing an environment-aware text-to-speech (TTS) system that can generate speech to suit specific acoustic environments. It is also motivated by the desire to leverage massive data of speech audio from heterogeneous sources in TTS system development. The key idea is to model the acoustic environment in speech audio as a factor of data variability and incorporate it as a condition in the process of neural network based speech synthesis. Two embedding extractors are trained with two purposely constructed datasets for characterization and disentanglement of speaker and environment factors in speech. A neural network model is trained to generate speech from extracted speaker and environment embeddings. Objective and subjective evaluation results demonstrate that the proposed TTS system is able to effectively disentangle speaker and environment factors and synthesize speech audio that carries designated speaker characteristics and environment attribute. Audio samples are available online for demonstration",
    "checked": true,
    "id": "9a18d05858d3a08c193de32737b1d5917c86c8cf",
    "semantic_title": "environment aware text-to-speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ploujnikov22_interspeech.html": {
    "title": "SoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation",
    "volume": "main",
    "abstract": "End-to-end speech synthesis models directly convert the input characters into an audio representation (e.g., spectrograms). Despite their impressive performance, such models have difficulty disambiguating the pronunciations of identically spelled words. To mitigate this issue, a separate Grapheme-to-Phoneme (G2P) model can be employed to convert the characters into phonemes before synthesizing the audio. This paper proposes SoundChoice, a novel G2P architecture that processes entire sentences rather than operating at the word level. The proposed architecture takes advantage of a weighted homograph loss (that improves disambiguation), exploits curriculum learning (that gradually switches from wordlevel to sentence-level G2P), and integrates word embeddings from BERT (for further performance improvement). Moreover, the model inherits the best practices in speech recognition, including multi-task learning with Connectionist Temporal Classification (CTC) and beam search with an embedded language model. As a result, SoundChoice achieves a Phoneme Error Rate (PER) of 2.65% on whole-sentence transcription using data from LibriSpeech and Wikipedia",
    "checked": true,
    "id": "bb3d5919ac730fc0e59b325cac048b17a651f003",
    "semantic_title": "soundchoice: grapheme-to-phoneme models with semantic disambiguation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bakhturina22_interspeech.html": {
    "title": "Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization",
    "volume": "main",
    "abstract": "Text normalization (TN) systems in production are largely rule-based using weighted finite-state transducers (WFST). However, WFST-based systems struggle with ambiguous input when the normalized form is context-dependent. On the other hand, neural text normalization systems can take context into account but they suffer from unrecoverable errors and require labeled normalization datasets, which are hard to collect. We propose a new hybrid approach that combines the benefits of rule-based and neural systems. First, a non-deterministic WFST outputs all normalization candidates, and then a neural language model picks the best one -- similar to shallow fusion for automatic speech recognition. While the WFST prevents unrecoverable errors, the language model resolves contextual ambiguity. We show for English that the approach is effective and easy to extend. It achieves comparable or better results than existing state-of-the-art TN models",
    "checked": true,
    "id": "ccf6c9fdda939f9e62c4fdf4aaca3b6c43ea692f",
    "semantic_title": "shallow fusion of weighted finite-state transducer and language model for text normalization",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/virkar22_interspeech.html": {
    "title": "Prosodic alignment for off-screen automatic dubbing",
    "volume": "main",
    "abstract": "The goal of automatic dubbing is to perform speech-to-speech translation while achieving audiovisual coherence. This entails isochrony, i.e., translating the original speech by also matching its prosodic structure into phrases and pauses, especially when the speaker's mouth is visible. In previous work, we introduced a prosodic alignment model to address isochrone or on-screen dubbing. In this work, we extend the prosodic alignment model to also address off-screen dubbing that requires less stringent synchronization constraints. We conduct experiments on four dubbing directions  English to French, Italian, German and Spanish  on a publicly available collection of TED Talks and on publicly available YouTube videos. Empirical results show that compared to our previous work the extended prosodic alignment model provides significantly better subjective viewing experience on videos in which on-screen and off-screen automatic dubbing is applied for sentences with speakers mouth visible and not visible, respectively",
    "checked": true,
    "id": "d6137685be671bcf1e3a062f8ee3c76d8c651b88",
    "semantic_title": "prosodic alignment for off-screen automatic dubbing",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bai22c_interspeech.html": {
    "title": "A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis",
    "volume": "main",
    "abstract": "In human speech, the attitude of a speaker cannot be fully expressed only by the textual content. It has to come along with the intonation. Declarative questions are commonly used in daily Cantonese conversations, and they are usually uttered with rising intonation. Vanilla neural text-to-speech (TTS) systems are not capable of synthesizing rising intonation for these sentences due to the loss of semantic information. Though it has become more common to complement the systems with extra language models, their performance in modeling rising intonation is not well studied. In this paper, we propose to complement the Cantonese TTS model with a BERT-based statement/question classifier. We design different training strategies and compare their performance. We conduct our experiments on a Cantonese corpus named CanTTS. Empirical results show that the separate training approach obtains the best generalization performance and feasibility",
    "checked": true,
    "id": "88c5fb33a01c57803102d3b80b4664668c5d725f",
    "semantic_title": "a study of modeling rising intonation in cantonese neural speech synthesis",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kameoka22_interspeech.html": {
    "title": "CAUSE: Crossmodal Action Unit Sequence Estimation from Speech",
    "volume": "main",
    "abstract": "This paper proposes a task and method for estimating a sequence of facial action units (AUs) solely from speech. AUs were introduced in the facial action coding system to objectively describe facial muscle activations. Our motivation is that AUs can be useful continuous quantities for representing speaker's subtle emotional states, attitudes, and moods in a variety of applications such as expressive speech synthesis and emotional voice conversion. We hypothesize that the information about the speaker's facial muscle movements is expressed in the generated speech and can somehow be predicted from speech alone. To verify this, we devise a neural network model that predicts an AU sequence from the mel-spectrogram of input speech and train it using a large-scale audio-visual dataset consisting of many speaking face-tracks. We call our method and model ``crossmodal AU sequence estimation/estimator (CAUSE)''. We implemented several of the most basic architectures for CAUSE, and quantitatively confirmed that the fully convolutional architecture performed best. Furthermore, by combining CAUSE with an AU-conditioned image-to-image translation method, we implemented a system that animates a given still face image from speech. Using this system, we confirmed the potential usefulness of AUs as a representation of non-linguistic features via subjective evaluations",
    "checked": true,
    "id": "eabb83f97f7ae4a7ea7125906cf66d2dcce31aca",
    "semantic_title": "cause: crossmodal action unit sequence estimation from speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/abeysinghe22_interspeech.html": {
    "title": "Visualising Model Training via Vowel Space for Text-To-Speech Systems",
    "volume": "main",
    "abstract": "With the recent developments in speech synthesis via machine learning, this study explores incorporating linguistics knowledge to visualise and evaluate synthetic speech model training. If changes to the first and second formant (in turn, the vowel space) can be seen and heard in synthetic speech, this knowledge can inform speech synthesis technology developers. A speech synthesis model trained on a large General American English database was fine-tuned into a New Zealand English voice to identify if the changes in the vowel space of synthetic speech could be seen and heard. The vowel spaces at different intervals during the fine-tuning were analysed to determine if the model learned the New Zealand English vowel space. Our findings based on vowel space analysis show that we can visualise how a speech synthesis model learns the vowel space of the database it is trained on. Perception tests confirmed that humans could perceive when a speech synthesis model has learned characteristics of the speech database it is training on. Using the vowel space as an intermediary evaluation helps understand what sounds are to be added to the training database and build speech synthesis models based on linguistics knowledge",
    "checked": true,
    "id": "7f77e0a9a08114ac150d11c180b1c260dcd5d14b",
    "semantic_title": "visualising model training via vowel space for text-to-speech systems",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeed22_interspeech.html": {
    "title": "Binary Early-Exit Network for Adaptive Inference on Low-Resource Devices",
    "volume": "main",
    "abstract": "Deep neural networks have significantly improved performance on a range of tasks with the increasing demand for computational resources, leaving deployment on low-resource devices (with limited memory and battery power) infeasible. Binary neural networks (BNNs) tackle the issue to an extent with extreme compression and speed-up gains compared to real-valued models. We propose a simple but effective method to accelerate inference through unifying BNNs with an early-exiting strategy. Our approach allows simple instances to exit early based on a decision threshold and utilizes output layers added to different intermediate layers to avoid executing the entire binary model. We extensively evaluate our method on three audio classification tasks and across four BNNs architectures. Our method demonstrates favorable quality-efficiency trade-offs while being controllable with an entropy-based threshold specified by the system user. It also results in better speed-ups (latency less than 6ms) with a single model based on existing BNN architectures without retraining for different efficiency levels. It also provides a straightforward way to estimate sample difficulty and better understanding of uncertainty around certain classes within the dataset",
    "checked": true,
    "id": "3fd99a934a2151d2c2744f0f9895723312e6f8cf",
    "semantic_title": "binary early-exit network for adaptive inference on low-resource devices",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kanda22b_interspeech.html": {
    "title": "Streaming Speaker-Attributed ASR with Token-Level Speaker Embeddings",
    "volume": "main",
    "abstract": "This paper presents a streaming speaker-attributed automatic speech recognition (SA-ASR) model that can recognize ``who spoke what'' with low latency even when multiple people are speaking simultaneously. Our model is based on token-level serialized output training (t-SOT) which was recently proposed to transcribe multi-talker speech in a streaming fashion. To further recognize speaker identities, we propose an encoder-decoder based speaker embedding extractor that can estimate a speaker representation for each recognized token not only from non-overlapping speech but also from overlapping speech. The proposed speaker embedding, named t-vector, is extracted synchronously with the t-SOT ASR model, enabling joint execution of speaker identification (SID) or speaker diarization (SD) with the multi-talker transcription with low latency. We evaluate the proposed model for a joint task of ASR and SID/SD by using LibriSpeechMix and LibriCSS corpora. The proposed model achieves substantially better accuracy than a prior streaming model and shows comparable or sometimes even superior results to the state-of-the-art offline SA-ASR model",
    "checked": true,
    "id": "f5345ca39fd5b86a438d856683bda819a37718b3",
    "semantic_title": "streaming speaker-attributed asr with token-level speaker embeddings",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/makishima22_interspeech.html": {
    "title": "Speaker consistency loss and step-wise optimization for semi-supervised joint training of TTS and ASR using unpaired text data",
    "volume": "main",
    "abstract": "In this paper, we investigate the semi-supervised joint training of text to speech (TTS) and automatic speech recognition (ASR), where a small amount of paired data and a large amount of unpaired text data are available. Conventional studies form a cycle called the TTS-ASR pipeline, where the multi-speaker TTS model synthesizes speech from text with a reference speech and the ASR model reconstructs the text from the synthesized speech, after which both models are trained with a cycle-consistency loss. However, the synthesized speech does not reflect the speaker characteristics of the reference speech and the synthesized speech becomes overly easy for the ASR model to recognize after training. This not only decreases the TTS model quality but also limits the ASR model improvement. To solve this problem, we propose improving the cycle-consistency-based training with a speaker consistency loss and step-wise optimization. The speaker consistency loss brings the speaker characteristics of the synthesized speech closer to that of the reference speech. In the step-wise optimization, we first freeze the parameter of the TTS model before both models are trained to avoid over-adaptation of the TTS model to the ASR model. Experimental results demonstrate the efficacy of the proposed method",
    "checked": true,
    "id": "bf261b2a17ccb296a56966bc333d0951b0c613f9",
    "semantic_title": "speaker consistency loss and step-wise optimization for semi-supervised joint training of tts and asr using unpaired text data",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22k_interspeech.html": {
    "title": "Audio-Visual Generalized Few-Shot Learning with Prototype-Based Co-Adaptation",
    "volume": "main",
    "abstract": "Although deep learning-based audio-visual speech recognition (AVSR) systems recognize base closed-set categories well, extending their discerning ability to additional novel categories with limited labeled training data is challenging since the model easily over-fits. In this paper, we propose Prototype-based Co-Adaptation with Transformer (Proto-CAT), a multi-modal generalized few-shot learning (GFSL) method for AVSR systems. In other words, Proto-CAT learns to recognize a novel class multi-modal object with few-shot training data, while maintaining its ability on those base closed-set categories. The main idea is to transform the prototypes (i.e., class centers) by incorporating cross-modality complementary information and calibrating cross-category semantic differences. In particular, Proto-CAT co-adapts the embeddings from audio-visual and category levels, so that it generalizes its predictions on all categories dynamically. Proto-CAT achieves state-of-the-art performance on various AVSR-GFSL benchmarks. The code is available at https://github.com/ZhangYikaii/Proto-CAT",
    "checked": true,
    "id": "b5ec154fdf4ce07ed96a6a1b513ae081029dedf6",
    "semantic_title": "audio-visual generalized few-shot learning with prototype-based co-adaptation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jia22_interspeech.html": {
    "title": "Federated Domain Adaptation for ASR with Full Self-Supervision",
    "volume": "main",
    "abstract": "Cross-device federated learning (FL) protects user privacy by collaboratively training a model on user devices, therefore eliminating the need for collecting, storing, and manually labeling user data. Previous works have considered cross-device FL for automatic speech recognition (ASR), however, there are a few important challenges that havenot been fully addressed. These include the lack of ground-truth ASR transcriptions, and the scarcity of compute resource and network bandwidth on edge devices. In this paper, we address these two challenges. First, we propose a federated learning system to support on-device ASR adaptation with full self-supervision, which uses self-labeling together with data augmentation and filtering techniques. The proposed system can improve a strong Emformer-Transducer based ASR model pretrained on out-of-domain data, using in-domain audios without any ground-truth transcriptions. Second, to reduce the training cost, we propose a self-restricted RNN Transducer (SR-RNN-T) loss, a new variant of alignment-restricted RNN-T that uses Viterbi forced-alignment from self-supervision. To further reduce the compute and network cost, we systematically explore adapting only a subset of weights in the Emformer-Transducer. Our best training recipe achieves a 12.9% relative WER reduction over the strong out-of-domain baseline, which equals 70% of the reduction achievable with full human supervision and centralized training",
    "checked": true,
    "id": "36f64815402971b40ce82e7c59f08d0eb8d274cd",
    "semantic_title": "federated domain adaptation for asr with full self-supervision",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22k_interspeech.html": {
    "title": "Augmented Adversarial Self-Supervised Learning for Early-Stage Alzheimer's Speech Detection",
    "volume": "main",
    "abstract": "The early-stage detection of Alzheimer's disease has been considered an important field of medical studies. While speech-based automatic detection methods have raised attention in the community, traditional machine learning methods suffer from data shortage because Alzheimer's record data is very difficult to get from medical institutions. To address this problem, this study proposes an augmented adversarial self-supervised learning method for Alzheimer's disease detection using limited speech data. In our approach, Alzheimer-like patterns are captured through an augmented adversarial self-supervised framework, which is trained in an adversarial manner using limited Alzheimer's data with a large scale of easily-collected normal speech data and an augmented set of Alzheimer's data. Experimental results show that our model can effectively handle the data sparsity problems and outperform the several baselines by a large margin. The performance for the ``AD\" class has been improved significantly, which is very important to actual AD detection applications",
    "checked": true,
    "id": "c52463cf1f92f22a149f99144efac7cb28b26de5",
    "semantic_title": "augmented adversarial self-supervised learning for early-stage alzheimer's speech detection",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kons22_interspeech.html": {
    "title": "Extending RNN-T-based speech recognition systems with emotion and language classification",
    "volume": "main",
    "abstract": "Speech transcription, emotion recognition, and language identification are usually considered to be three different tasks. Each one requires a different model with a different architecture and training process. We propose using a recurrent neural network transducer (RNN-T)-based speech-to-text (STT) system as a common component that can be used for emotion recognition and language identification as well as for speech recognition. Our work extends the STT system for emotion classification through minimal changes, and shows successful results on the IEMOCAP and MELD datasets. In addition, we demonstrate that by adding a lightweight component to the RNN-T module, it can also be used for language identification. In our evaluations, this new classifier demonstrates state-of-the-art accuracy for the NIST-LRE-07 dataset",
    "checked": true,
    "id": "6761cdf6e845a4b66e3fd33f082b9b97076a11c2",
    "semantic_title": "extending rnn-t-based speech recognition systems with emotion and language classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/antonova22_interspeech.html": {
    "title": "Thutmose Tagger: Single-pass neural model for Inverse Text Normalization",
    "volume": "main",
    "abstract": "Inverse text normalization (ITN) is an essential post-processing step in automatic speech recognition (ASR). It converts numbers, dates, abbreviations, and other semiotic classes from the spoken form generated by ASR to their written forms. One can consider ITN as a Machine Translation task and use neural sequence-to-sequence models to solve it. Unfortunately, such neural models are prone to hallucinations that could lead to unacceptable errors. To mitigate this issue, we propose a single-pass token classifier model that regards ITN as a tagging task. The model assigns a replacement fragment to every input token or marks it for deletion or copying without changes. We present a method of dataset preparation, based on granular alignment of ITN examples. The proposed model is less prone to hallucination errors. The model is trained on the Google Text Normalization dataset and achieves state-of-the-art sentence accuracy on both English and Russian test sets. One-to-one correspondence between tags and input words improves the interpretability of the model's predictions, simplifies debugging, and allows for post-processing corrections. The model is simpler than sequence-to-sequence models and easier to optimize in production settings. The model and the code to prepare the dataset is published as part of NeMo project",
    "checked": true,
    "id": "03ec7f75a2c7cb644ec88949e98ba55316fe79c3",
    "semantic_title": "thutmose tagger: single-pass neural model for inverse text normalization",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cho22b_interspeech.html": {
    "title": "Leveraging Prosody for Punctuation Prediction of Spontaneous Speech",
    "volume": "main",
    "abstract": "This paper introduces a new neural model for punctuation prediction that incorporates prosodic features to improve automatic punctuation prediction in transcriptions of spontaneous speech. We explore the benefit of intonation and energy features over simply using pauses. In addition, the work poses the question of how to represent interruption points associated with disfluencies in spontaneous speech. In experiments on the Switchboard corpus, we find that prosodic information improved punctuation prediction fidelity for both hand transcripts and ASR output. Explicit modeling of interruption points can benefit prediction of standard punctuation, particularly if the convention associates interruptions with commas",
    "checked": true,
    "id": "1bb965584a1e9ab0754c1e3a487ee6c7b636c734",
    "semantic_title": "leveraging prosody for punctuation prediction of spontaneous speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yu22b_interspeech.html": {
    "title": "A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings",
    "volume": "main",
    "abstract": "In this paper, we conduct a comparative study on speaker-attributed automatic speech recognition (SA-ASR) in the multi-party meeting scenario, a topic with increasing attention in meeting rich transcription. Specifically, three approaches are evaluated in this study. The first approach, FD-SOT, consists of a frame-level diarization model to identify speakers and a multi-talker ASR to recognize utterances. The speaker-attributed transcriptions are obtained by aligning the diarization results and recognized hypotheses. However, such an alignment strategy may suffer from erroneous timestamps due to the modular independence, severely hindering the model performance. Therefore, we propose the second approach, WD-SOT, to address alignment errors by introducing a word-level diarization model, which can get rid of such timestamp alignment dependency. To further mitigate the alignment issues, we propose the third approach, TS-ASR, which trains a target-speaker separation module and an ASR module jointly. By comparing various strategies for each SA-ASR approach, experimental results on a real meeting scenario corpus, AliMeeting, reveal that the WD-SOT approach achieves 10.7% relative reduction on averaged speaker-dependent character error rate (SD-CER), compared with the FD-SOT approach. In addition, the TS-ASR approach also outperforms the FD-SOT approach and brings 16.5% relative average SD-CER reduction",
    "checked": true,
    "id": "29ea9c4243563dc1a04b82ead8fc8ea0c35dd207",
    "semantic_title": "a comparative study on speaker-attributed automatic speech recognition in multi-party meetings",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guan22_interspeech.html": {
    "title": "TMGAN-PLC: Audio Packet Loss Concealment using Temporal Memory Generative Adversarial Network",
    "volume": "main",
    "abstract": "Real-time communications in packet-switched networks have become widely used in daily communication, while they inevitably suffer from network delays and data losses in constrained real-time conditions. To solve these problems, audio packet loss concealment (PLC) algorithms have been developed to mitigate voice transmission failures by reconstructing the lost information. Limited by the transmission latency and device memory, it is still intractable for PLC to accomplish high-quality voice reconstruction using a relatively small packet buffer. In this paper, we propose a temporal memory generative adversarial network for audio PLC, dubbed TMGAN-PLC, which is comprised of a novel nested-UNet generator and the time-domain/frequency-domain discriminators. Specifically, a combination of the nested-UNet and temporal feature-wise linear modulation is elaborately devised in the generator to finely adjust the intra-frame information and establish inter-frame temporal dependencies. To complement the missing speech content caused by longer loss bursts, we employ multi-stage gated vector quantizers to capture the correct content and reconstruct the near-real smooth audio. Extensive experiments on the PLC Challenge dataset demonstrate that the proposed method yields promising performance in terms of speech quality, intelligibility, and PLCMOS",
    "checked": true,
    "id": "df07a6a366bc83ae4b8b46150c9ef2305e578701",
    "semantic_title": "tmgan-plc: audio packet loss concealment using temporal memory generative adversarial network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/valin22_interspeech.html": {
    "title": "Real-Time Packet Loss Concealment With Mixed Generative and Predictive Model",
    "volume": "main",
    "abstract": "As deep speech enhancement algorithms have recently demonstrated capabilities greatly surpassing their traditional counterparts for suppressing noise, reverberation and echo, attention is turning to the problem of packet loss concealment (PLC). PLC is a challenging task because it not only involves real-time speech synthesis, but also frequent transitions between the received audio and the synthesized concealment. We propose a hybrid neural PLC architecture where the missing speech is synthesized using a generative model conditioned using a predictive model. The resulting algorithm achieves natural concealment that surpasses the quality of existing conventional PLC algorithms and ranked second in the Interspeech 2022 PLC Challenge. We show that our solution not only works for uncompressed audio, but is also applicable to a modern speech codec",
    "checked": true,
    "id": "81ce23f3fe1f54e1f283f8f96b7efaaf59899134",
    "semantic_title": "real-time packet loss concealment with mixed generative and predictive model",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22s_interspeech.html": {
    "title": "PLCNet: Real-time Packet Loss Concealment with Semi-supervised Generative Adversarial Network",
    "volume": "main",
    "abstract": "Packet loss is one of the main reasons for speech quality degradation in voice over internet phone (VOIP) calls. However, the existing packet loss concealment (PLC) algorithms are hard to generate high-quality speech signal while maintaining low computational complexity. In this paper, a causal wave-to-wave non-autoregressive lightweight PLC model (PLCNet) is proposed, which can do real-time streaming process with low latency. In addition, we introduce multiple multi-resolution discriminators and semi-supervised training strategy to improve the ability of the encoder part to extract global features while enabling the decoder part to accurately reconstruct waveforms where packets are lost. Contrary to autoregressive model, PLCNet can guarantee the smoothness and continuity of the speech phase before and after packet loss without any smoothing operations. Experimental results show that PLCNet achieves significant improvements in perceptual quality and intelligibility over three classical PLC methods and three state-of-the-art deep PLC methods. In the INTERSPEECH 2022 PLC Challenge, our approach has ranked the 3rd place on PLCMOS (3.829) and the 3rd place on the final score (0.798)",
    "checked": true,
    "id": "036617b6a60faa2f62ee3c6ec78658ddf016dc45",
    "semantic_title": "plcnet: real-time packet loss concealment with semi-supervised generative adversarial network",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/diener22_interspeech.html": {
    "title": "INTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge",
    "volume": "main",
    "abstract": "Audio Packet Loss Concealment (PLC) is the hiding of gaps in audio streams caused by data transmission failures in packet switched networks. This is a common problem, and of increasing importance as end-to-end VoIP telephony and teleconference systems become the default and ever more widely used form of communication in business as well as in personal usage. This paper presents the INTERSPEECH 2022 Audio Deep Packet Loss Concealment challenge. We first give an overview of the PLC problem, and introduce some classical approaches to PLC as well as recent work. We then present the open source dataset released as part of this challenge as well as the evaluation methods and metrics used to determine the winner. We also briefly introduce PLCMOS, a novel data-driven metric that can be used to quickly evaluate the performance PLC systems. Finally, we present the results of the INTERSPEECH 2022 Audio Deep PLC Challenge, and provide a summary of important takeaways",
    "checked": true,
    "id": "626a2abde4ce59eb1a39812a167ac7c15afe73ff",
    "semantic_title": "interspeech 2022 audio deep packet loss concealment challenge",
    "citation_count": 16
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22ea_interspeech.html": {
    "title": "End-to-End Multi-Loss Training for Low Delay Packet Loss Concealment",
    "volume": "main",
    "abstract": "Real-time teleconferencing has become one of the essential parts in our daily life. While packet loss during real-time data transmission is unavoidable, traditional signal processing based Packet Loss Concealment (PLC) techniques have been developed in recent decades. In recent years, deep learning based approaches have also proposed and achieved state-of-the-art PLC performance. This work presents a low-delay multi-loss based neural PLC system. The multi-loss is consisted by a signal loss, a perceptual loss and an ASR loss ensuring good speech quality and automatic speech recognition compatibility. The proposed system was ranked 1st place in INTERSPEECH 2022's Audio Deep Packet Loss Concealment Challenge",
    "checked": true,
    "id": "88a2d947f8539c72bd5d2d200e9bd073ab6c7b86",
    "semantic_title": "end-to-end multi-loss training for low delay packet loss concealment",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22b_interspeech.html": {
    "title": "Extended U-Net for Speaker Verification in Noisy Environments",
    "volume": "main",
    "abstract": "Background noise is a well-known factor that deteriorates the accuracy and reliability of speaker verification (SV) systems by blurring speech intelligibility. Various studies have used separate pretrained enhancement models as the front-end module of the SV system in noisy environments, and these methods effectively remove noises. However, the denoising process of independent enhancement models not tailored to the SV task can also distort the speaker information included in utterances. We argue that the enhancement network and speaker embedding extractor should be fully jointly trained for SV tasks under noisy conditions to alleviate this issue. Therefore, we proposed a U-Net-based integrated framework that simultaneously optimizes speaker identification and feature enhancement losses. Moreover, we analyzed the structural limitations of using U-Net directly for noise SV tasks and further proposed Extended U-Net to reduce these drawbacks. We evaluated the models on the noise-synthesized VoxCeleb1 test set and VOiCES development set recorded in various noisy scenarios. The experimental results demonstrate that the U-Net-based fully joint training framework is more effective than the baseline, and the extended U-Net exhibited state-of-the-art performance versus the recently proposed compensation systems",
    "checked": true,
    "id": "455a8c1a9890243455f7a087c4cf45120bea5108",
    "semantic_title": "extended u-net for speaker verification in noisy environments",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22j_interspeech.html": {
    "title": "Domain Agnostic Few-shot Learning for Speaker Verification",
    "volume": "main",
    "abstract": "Deep learning models for verification systems often fail to generalize to new users and new environments, even though they learn highly discriminative features. To address this problem, we propose a few-shot domain generalization framework that learns to tackle distribution shift for new users and new domains. Our framework consists of domain-specific and domain-aggregation networks, which are the experts on specific and combined domains, respectively. By using these networks, we generate episodes that mimic the presence of both novel users and novel domains in the training phase to eventually produce better generalization. To save memory, we reduce the number of domain-specific networks by clustering similar domains together. Upon extensive evaluation on artificially generated noise domains, we can explicitly show generalization ability of our framework. In addition, we apply our proposed methods to the existing competitive architecture on the standard benchmark, which shows further performance improvements",
    "checked": true,
    "id": "bc2ceeea0ca9a5042d2c7da3d156bd20ee51b71d",
    "semantic_title": "domain agnostic few-shot learning for speaker verification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22r_interspeech.html": {
    "title": "Scoring of Large-Margin Embeddings for Speaker Verification: Cosine or PLDA?",
    "volume": "main",
    "abstract": "The emergence of large-margin softmax cross-entropy losses in training deep speaker embedding neural networks has triggered a gradual shift from parametric back-ends to a simpler cosine similarity measure for speaker verification. Popular parametric back-ends include the probabilistic linear discriminant analysis (PLDA) and its variants. This paper investigates the properties of margin-based cross-entropy losses leading to such a shift and aims to find scoring back-ends best suited for speaker verification. In addition, we revisit the pre-processing techniques which have been widely used in the past and assess their effectiveness on large-margin embeddings. Experiments on the state-of-the-art ECAPA-TDNN networks trained with various large-margin softmax cross-entropy losses show a substantial increment in intra-speaker compactness making the conventional PLDA superfluous. In this regard, we found that constraining the within-speaker covariance matrix could improve the performance of the PLDA. It is demonstrated through a series of experiments on the VoxCeleb-1 and SITW core-core test sets with 40.8% equal error rate (EER) reduction and 35.1% minimum detection cost (minDCF) reduction. It also outperforms cosine scoring consistently with reductions in EER and minDCF by 10.9% and 4.9%, respectively",
    "checked": true,
    "id": "a653a2fa51a8847511be8e5aa07d2774826a0f1c",
    "semantic_title": "scoring of large-margin embeddings for speaker verification: cosine or plda?",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/stafylakis22_interspeech.html": {
    "title": "Training speaker embedding extractors using multi-speaker audio with unknown speaker boundaries",
    "volume": "main",
    "abstract": "In this paper, we demonstrate a method for training speaker embedding extractors using weak annotation. More specifically, we are using the full VoxCeleb recordings and the name of the celebrities appearing on each video without knowledge of the time intervals the celebrities appear in the video. We show that by combining a baseline speaker diarization algorithm that requires no training or parameter tuning, a modified loss with aggregation over segments, and a two-stage training approach, we are able to train a competitive ResNet-based embedding extractor. Finally, we experiment with two different aggregation functions and analyze their behaviour in terms of their gradients",
    "checked": true,
    "id": "227088f40b8c5c08f4c6518718366f11f5d4f718",
    "semantic_title": "training speaker embedding extractors using multi-speaker audio with unknown speaker boundaries",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luu22_interspeech.html": {
    "title": "Investigating the contribution of speaker attributes to speaker separability using disentangled speaker representations",
    "volume": "main",
    "abstract": "Deep speaker embeddings have been shown to encode a wide variety of attributes relating to a speaker. The aim of this work is to separate out some of these attributes in the embedding space, disentangling these sources of speaker variation into subsets of the embedding dimensions. This is achieved modifying the training procedure of a typical speaker embedding network, which is typically only trained to classify speakers. This work instead adds pairs of attribute specific task heads to operate on complementary subsets of the speaker embedding dimensions. While specific dimensions are encouraged to encode an attribute, for example gender, the other dimensions are penalized for containing this information using an adversarial loss. We show that this method is effective in factorizing out multiple attributes in the embedding space, successfully disentangling gender, nationality and age. Using the disentangled representations, we investigate how much removing this information impacts speaker verification and diarization performance, showing that gender is a significant source of separation in the deep speaker embedding space, with nationality and age also contributing to a lesser degree",
    "checked": true,
    "id": "8236cbb75a50f1bad05cb7d50d156526046b6b4d",
    "semantic_title": "investigating the contribution of speaker attributes to speaker separability using disentangled speaker representations",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kataria22_interspeech.html": {
    "title": "Joint domain adaptation and speech bandwidth extension using time-domain GANs for speaker verification",
    "volume": "main",
    "abstract": "Speech systems developed for a particular choice of acoustic domain and sampling frequency do not translate easily to others. The usual practice is to learn domain adaptation and bandwidth extension models independently. Contrary to this, we propose to learn both tasks together. Particularly, we learn to map narrowband conversational telephone speech to wideband microphone speech. We developed parallel and non-parallel learning solutions which utilize both paired and unpaired data. We first discuss joint and disjoint training of multiple generative models for our tasks. Then, we propose a two-stage learning solution using a pre-trained domain adaptation system for pre-processing in bandwidth extension training. We evaluated our schemes on a Speaker Verification downstream task. We used the JHU-MIT experimental setup for NIST SRE21, which comprises SRE16, SRE-CTS Superset, and SRE21. Our results prove that learning both tasks is better than learning just one. On SRE16, our best system achieves 22% relative improvement in Equal Error Rate w.r.t. a direct learning baseline and 8% w.r.t. a strong bandwidth expansion system",
    "checked": true,
    "id": "d58ebbc34e8ea987da5dda1bb132823b3e9105d3",
    "semantic_title": "joint domain adaptation and speech bandwidth extension using time-domain gans for speaker verification",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoshinaga22_interspeech.html": {
    "title": "Variability in Production of Non-Sibilant Fricative [] in /hi/",
    "volume": "main",
    "abstract": "The alveolo-palatal sibilant fricative [] in /si/ and palatal non-sibilant fricative [] in /hi/ are known to be distinguished by the geometrical change of the vocal tract only in the coronal direction and acoustically unstable in some Japanese speakers. In this study, we reconstructed the vocal tract geometry with three repetitive magnetic resonance imaging (MRI) on the same subject and investigated the effects of coronal vocal tract shapes on the airflow and sound characteristics in []. The computational grids were constructed on each vocal tract and a numerical airflow simulation was conducted to predict the turbulent airflow and aeroacoustic sound generation. The predicted sound properties were in good agreement with the measurement of Japanese subjects. The comparison of the airflow and acoustic characteristics among three vocal tracts showed that the slight changes of the constriction area and flow channel downstream from the constriction influenced the sound source generation and peak amplitudes at around 3 kHz, indicating that the characteristic peak of [] was variable due to the constriction shape at the middle part of the hard palate",
    "checked": true,
    "id": "803fa3bd9d58b49f939b5499dbec456f8156c2f1",
    "semantic_title": "variability in production of non-sibilant fricative [] in /hi/",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/udupa22_interspeech.html": {
    "title": "Streaming model for Acoustic to Articulatory Inversion with transformer networks",
    "volume": "main",
    "abstract": "Estimating speech articulatory movements from speech acoustics is known as Acoustic to Articulatory Inversion (AAI). Recently, transformer-based AAI models have been shown to achieve state-of-art performance. However, in transformer networks, the attention is applied over the whole utterance, thereby needing to obtain the full utterance before the inference, which leads to high latency and is impractical for streaming AAI. To enable streaming during inference, evaluation could be performed on non-overlapping chucks instead of full utterance. However, due to a mismatch of attention receptive field during training and evaluation, there could be a drop in AAI performance. To overcome this scenario, in this work we perform experiments with different attention masks and use context from previous predictions during training. Experiments results revealed that using the random start mask attention with the context from previous predictions of transformer encoder performs better than the baseline results",
    "checked": true,
    "id": "4f892f88ba61212c10501d00fcda98b3a30ac7ef",
    "semantic_title": "streaming model for acoustic to articulatory inversion with transformer networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rakotomalala22_interspeech.html": {
    "title": "Trajectories predicted by optimal speech motor control using LSTM networks",
    "volume": "main",
    "abstract": "The question of optimality and its role in trajectory formation is at the core of important debates in motor control research. We present the first speech control model that associates Optimal Feedback Control (OFC) for planning and execution of movements with a biomechanical model of the vocal tract. Simulated trajectories in the VCV sequences are compared with trajectories generated using the GEPPETO model that drives the same 2D biomechanical model; in GEPPETO, the scope of optimality is limited to movement planning and to phoneme-related target motor commands. In our OFC model commands are estimated via the minimisation of a cost that combines neuromuscular effort, and a penalty on accuracy of the auditory patterns reached for the phonemes. The biomechanics of the plant are implemented by an LSTM trained on simulations of a finite element model of the tongue. The comparison of the OFC model with GEPPETO relies on the time variation of the motor commands, the shape of the articulatory trajectories, and on auditory trajectories in the F1-F2 planes",
    "checked": true,
    "id": "354329f7f8f298476ce4f04632fbbcc8ab97f25d",
    "semantic_title": "trajectories predicted by optimal speech motor control using lstm networks",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vanniekerk22_interspeech.html": {
    "title": "Exploration strategies for articulatory synthesis of complex syllable onsets",
    "volume": "main",
    "abstract": "High-quality articulatory speech synthesis has many potential applications in speech science and technology. However, developing appropriate mappings from linguistic specification to articulatory gestures is difficult and time consuming. In this paper we construct an optimisation-based framework as a first step towards learning these mappings without manual intervention. We demonstrate the production of CCV syllables and discuss the quality of the articulatory gestures with reference to coarticulation",
    "checked": true,
    "id": "2962f72c44cabe81a1438f5184d52e11abc57794",
    "semantic_title": "exploration strategies for articulatory synthesis of complex syllable onsets",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22m_interspeech.html": {
    "title": "Linguistic versus biological factors governing acoustic voice variation",
    "volume": "main",
    "abstract": "This study presents a cross-linguistic investigation of acoustic voice spaces in English, Seoul Korean, and White Hmong, which differ in whether they phonologically contrast phonation type and/or tone. The overarching hypothesis is that acoustic variability in voice will be shaped by biological factors, linguistic factors, and individual idiosyncrasies. By employing principal component analysis on speakers' read speech productions, we identify how individual and population voice spaces are acoustically structured for speakers of these three languages. Results revealed several factors that consistently account for acoustic variability across speakers and languages, but also factors that vary with language-specific phonology",
    "checked": true,
    "id": "a697d86f057261d827c3548eb13174947fc8cc3c",
    "semantic_title": "linguistic versus biological factors governing acoustic voice variation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nagamine22_interspeech.html": {
    "title": "Acquisition of allophonic variation in second language speech: An acoustic and articulatory study of English laterals by Japanese speakers",
    "volume": "main",
    "abstract": "Acquisition of positional allophonic variation is seen as the foundation of a successful L2 speech learning. However, previous research has mostly focused on the phonemic contrast between English /l/ and /r/, providing little evidence in the acquisition of positional allophones, such as those in English /l/. The current study investigates the acoustics and articulation of allophonic variations in English laterals produced by Japanese speakers, focusing on the effects of syllabic positions and flanking vowels. Acoustic and articulatory data were obtained from five Japanese speakers in a simultaneous audio and high-speed ultrasound tongue imaging recording set-up while they read sentences containing syllable-initial and -final tokens of English /l/ in four different vowel contexts. Acoustic analysis was conducted on 500 tokens using linear-mixed effects modelling and the articulatory data were analysed using generalised additive mixed modelling. Syllable position and vowel context had significant effects on acoustics, while midsagittal tongue shape was more influenced by vowel context, with fewer positional effects. The results demonstrate that differences in acoustics not always be mirrored exactly by midsagittal tongue shape, suggesting multidimensionality of articulation in second language speech",
    "checked": true,
    "id": "95b265cd0add05966907267bb3d891f6db42b183",
    "semantic_title": "acquisition of allophonic variation in second language speech: an acoustic and articulatory study of english laterals by japanese speakers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/manocha22b_interspeech.html": {
    "title": "SAQAM: Spatial Audio Quality Assessment Metric",
    "volume": "main",
    "abstract": "Audio quality assessment is critical for assessing the perceptual realism of sounds. However, the time and expense of obtaining \"gold standard\" human judgments limit the availability of such data. For AR&VR, good perceived sound quality and localizability of sources are among the key elements to ensure complete immersion of the user. Our work introduces SAQAM which uses a multi-task learning framework to assess listening quality (LQ) and spatialization quality (SQ) between any given pair of binaural signals without using any subjective data. We model LQ by training on a simulated dataset of triplet human judgments, and SQ by utilizing activation-level distances from networks trained for direction of arrival (DOA) estimation. We show that SAQAM correlates well with human responses across four diverse datasets. Since it is a deep network, the metric is differentiable, making it suitable as a loss function for other tasks. For example, simply replacing an existing loss with our metric yields improvement in a speech-enhancement network",
    "checked": true,
    "id": "d0b08bf812ff3bfca6b99509f0be44db75f06592",
    "semantic_title": "saqam: spatial audio quality assessment metric",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/manocha22c_interspeech.html": {
    "title": "Speech Quality Assessment through MOS using Non-Matching References",
    "volume": "main",
    "abstract": "Human judgments obtained through Mean Opinion Scores (MOS) are the most reliable way to assess the quality of speech signals. However, several recent attempts to automatically estimate MOS using deep learning approaches lack robustness and generalization capabilities, limiting their use in real-world applications. In this work, we present a novel framework, NORESQA-MOS, for estimating the MOS of a speech signal. Unlike prior works, our approach uses non-matching references as a form of conditioning to ground the MOS estimation by neural networks. We show that NORESQA-MOS provides better generalization and more robust MOS estimation than previous state-of-the-art methods such as DNSMOS and NISQA, even though we use a smaller training set. Moreover, we also show that our generic framework can be combined with other learning methods such as self-supervised learning and can further supplement the benefits from these methods",
    "checked": true,
    "id": "65c06ccd1157c42252aac4ff2a913ed7457ada66",
    "semantic_title": "speech quality assessment through mos using non-matching references",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kawahara22_interspeech.html": {
    "title": "An objective test tool for pitch extractors' response attributes",
    "volume": "main",
    "abstract": "We propose an objective measurement method for pitch extractors' responses to frequency-modulated signals. It enables us to evaluate different pitch extractors with unified criteria. The method uses extended time-stretched pulses combined by binary orthogonal sequences. It provides simultaneous measurement results consisting of the linear and the non-linear time-invariant responses and random and time-varying responses. We tested representative pitch extractors using fundamental frequencies spanning 80~Hz to 800~Hz with 1/48 octave steps and produced more than 2000 modulation frequency response plots. We found that making scientific visualization by animating these plots enables us to understand different pitch extractors' behavior at once. Such efficient and effortless inspection is impossible by inspecting all individual plots. The proposed measurement method with visualization leads to further improvement of the performance of one of the extractors mentioned above. In other words, our procedure turns the specific pitch extractor into the best reliable measuring equipment that is crucial for scientific research. We open-sourced MATLAB codes of the proposed objective measurement method and visualization procedure",
    "checked": true,
    "id": "63b89ed948b530045852d34e2b4a85e079b0588f",
    "semantic_title": "an objective test tool for pitch extractors' response attributes",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22o_interspeech.html": {
    "title": "Data Augmentation Using McAdams-Coefficient-Based Speaker Anonymization for Fake Audio Detection",
    "volume": "main",
    "abstract": "Fake audio detection (FAD) is a technique to distinguish synthetic speech from natural speech. In most FAD systems, removing irrelevant features from acoustic speech while keeping only robust discriminative features is essential. Intuitively, speaker information entangled in acoustic speech should be suppressed for the FAD task. Particularly in a deep neural network (DNN)-based FAD system, the learning system may learn speaker information from a training dataset and cannot generalize well on a testing dataset. In this paper, we propose to use the speaker anonymization (SA) technique to suppress speaker information from acoustic speech before inputting it into a DNN-based FAD system. We adopted the McAdams-coefficient-based SA (MC-SA) algorithm, and this is expected that the entangled speaker information will not be involved in the DNN-based FAD learning. Based on this idea, we implemented a light convolutional neural network bidirectional long short-term memory (LCNN-BLSTM)-based FAD system and conducted experiments on the Audio Deep Synthesis Detection Challenge (ADD2022) datasets. The results showed that removing the speaker information from acoustic speech improved the relative performance in the first track of ADD2022 by 17.66%",
    "checked": true,
    "id": "e148912c4b4ed28f594d859a74d8a55591f61863",
    "semantic_title": "data augmentation using mcadams-coefficient-based speaker anonymization for fake audio detection",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zaiem22_interspeech.html": {
    "title": "Automatic Data Augmentation Selection and Parametrization in Contrastive Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Contrastive learning enables learning useful audio and speech representations without ground-truth labels by maximizing the similarity between latent representations of similar signal segments. In this framework various data augmentation techniques are usually exploited to help enforce desired invariances within the learned representations, improving performance on various audio tasks thanks to more robust embeddings. Now, selecting the most relevant augmentations has proven crucial for better downstream performances. Thus, this work introduces a conditional independance-based method which allows for automatically selecting a suitable distribution on the choice of augmentations and their parametrization from a set of predefined ones, for contrastive self-supervised pre-training. This is performed with respect to a downstream task of interest, hence saving a costly hyper-parameter search. Experiments performed on two different downstream tasks validate the proposed approach showing better results than experimenting without augmentation or with baseline augmentations. We furthermore conduct a qualitative analysis of the automatically selected augmentations and their variation according to the considered final downstream dataset",
    "checked": true,
    "id": "7bcd50b75b84552b9e5d806efe376651132d78c1",
    "semantic_title": "automatic data augmentation selection and parametrization in contrastive self-supervised speech representation learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mumtaz22_interspeech.html": {
    "title": "Transformer-based quality assessment model for generalized user-generated multimedia audio content",
    "volume": "main",
    "abstract": "In this paper, we propose a computational measure for the quality of audio in user-generated multimedia (UGM) in accordance with the human perceptual system. To this end, we first extend the previously proposed IIT-JMU-UGM Audio dataset by including samples with more diverse context, content, distortion types, and intensities, along with implicitly distorted audio that reflect realistic scenarios. We conduct subjective testing on the extended database containing 2075 audio clips to obtain the mean opinion scores for each sample. We then introduce transformer-based learning to the domain of audio quality assessment, which is trained on three vital audio features: Mel-frequency cepstral coefficients, chroma, and Mel-scaled spectrogram. The proposed non-intrusive transformer-based model is compared against state-of-the-art methods and found to outperform Simple RNN, LSTM, and GRU models by over 4%. The database and the source code will be made public upon acceptance",
    "checked": true,
    "id": "4dc20f41528f406dbbf077f8943ec0e4adb09cde",
    "semantic_title": "transformer-based quality assessment model for generalized user-generated multimedia audio content",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vangysel22_interspeech.html": {
    "title": "Space-Efficient Representation of Entity-centric Query Language Models",
    "volume": "main",
    "abstract": "Virtual assistants make use of automatic speech recognition (ASR) to help users answer entity-centric queries. However, spoken entity recognition is a difficult problem, due to the large number of frequently-changing named entities. In addition, resources available for recognition are constrained when ASR is performed on-device. In this work, we investigate the use of probabilistic grammars as language models within the finite-state transducer (FST) framework. We introduce a deterministic approximation to probabilistic grammars that avoids the explicit expansion of non-terminals at model creation time, integrates directly with the FST framework, and is complementary to n-gram models. We obtain a 10% relative word error rate improvement on long tail entity queries compared to when a similarly-sized n-gram model is used without our method",
    "checked": true,
    "id": "6ab7125907039eaeb24090bc6d97140f1953c66a",
    "semantic_title": "space-efficient representation of entity-centric query language models",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dingliwal22_interspeech.html": {
    "title": "Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems",
    "volume": "main",
    "abstract": "Automatic Speech Recognition (ASR) systems have found their use in numerous industrial applications in very diverse domains creating a need to adapt to new domains with small memory and deployment overhead. In this work, we introduce domain-prompts, a methodology that involves training a small number of domain embedding parameters to prime a Transformer-based Language Model (LM) to a particular domain. Using this domain-adapted LM for rescoring ASR hypotheses can achieve 7-13% WER reduction for a new domain with just 1000 unlabeled textual domain-specific sentences. This improvement is comparable or even better than fully fine-tuned models even though just 0.02% of the parameters of the base LM are updated. Additionally, our method is deployment-friendly as the learnt domain embeddings are prefixed to the input to the model rather than changing the base model architecture. Therefore, our method is an ideal choice for on-the-fly adaptation of LMs used in ASR systems to progressively scale it to new domains",
    "checked": true,
    "id": "f7f3c01b8ccc0841ea1009198aa6d274c68c9b8b",
    "semantic_title": "domain prompts: towards memory and compute efficient domain adaptation of asr systems",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22j_interspeech.html": {
    "title": "Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition",
    "volume": "main",
    "abstract": "Language model fusion helps smart assistants recognize words which are rare in acoustic data but abundant in text-only corpora (typed search logs). However, such corpora have properties that hinder downstream performance, including being (1) too large, (2) beset with domain-mismatched content, and (3) heavy-headed rather than heavy-tailed (excessively many duplicate search queries such as \"weather\"). We show that three simple strategies for selecting language modeling data can dramatically improve rare-word recognition without harming overall performance. First, to address the heavy-headedness, we downsample the data according to a soft log function, which tunably reduces high frequency (head) sentences. Second, to encourage rare-word exposure, we explicitly filter for words rare in the acoustic data. Finally, we tackle domain-mismatch via perplexity-based contrastive selection, filtering for examples matched to the target domain. We down-select a large corpus of web search queries by a factor of 53x and achieve better LM perplexities than without down-selection. When shallow-fused with a state-of-the-art, production speech engine, our LM achieves WER reductions of up to 24\\% relative on rare-word sentences (without changing overall WER) compared to a baseline LM trained on the raw corpus. These gains are further validated through favorable side-by-side evaluations on live voice search traffic",
    "checked": true,
    "id": "3f7aa16227677d6d76893b6d4c64276addfa1b8c",
    "semantic_title": "sentence-select: large-scale language model data selection for rare-word speech recognition",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/breiner22_interspeech.html": {
    "title": "UserLibri: A Dataset for ASR Personalization Using Only Text",
    "volume": "main",
    "abstract": "Personalization of speech models on mobile devices (on-device personalization) is an active area of research, but more often than not, mobile devices have more text-only data than paired audio-text data. We explore training a personalized language model on text-only data, used during inference to improve speech recognition performance for that user. We experiment on a user-clustered LibriSpeech corpus, supplemented with personalized text-only data for each user from Project Gutenberg. We release this User-Specific LibriSpeech (UserLibri) dataset to aid future personalization research. LibriSpeech audio-transcript pairs are grouped into 55 users from the test-clean dataset and 52 users from test-other. We are able to lower the average word error rate per user across both sets in streaming and nonstreaming models, including an improvement of 2.5 for the harder set of test-other users when streaming",
    "checked": true,
    "id": "0beccd1692536d1aa7bb572bc32040a9d409e141",
    "semantic_title": "userlibri: a dataset for asr personalization using only text",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chien22b_interspeech.html": {
    "title": "A BERT-based Language Modeling Framework",
    "volume": "main",
    "abstract": "Deep learning has brought considerable changes and created a new paradigm in many research areas, including computer vision, speech processing, and natural language processing. In the context of language modeling, recurrent-based language models and word embedding methods have been pivotal studies in the past decade. Recently, pre-trained language models have attracted widespread attention due to their enormous success in many challenging tasks. However, only a dearth of works concentrates on creating novel language models based on the pre-trained models. In order to bridge the research gap, we take the bidirectional encoder representations from Transformers (BERT) model as an example to explore novel uses of a pre-trained model for language modeling. More formally, this paper proposes a set of BERT-based language models, and a neural-based dynamic adaptation method is also introduced to combine these language models systematically and methodically. We conduct comprehensive studies on three datasets for the perplexity evaluation. Experiments show that the proposed framework achieves 11%, 39%, and 5% relative improvements over the baseline model for Penn Treebank, Wikitext 2, and Tedlium Release 2 corpora, respectively. Besides, when applied to rerank n-best lists from a speech recognizer, our framework also yields promising results compared with baseline systems",
    "checked": true,
    "id": "8ccd6ea80e3fb165213b95d5ff7e71b497befc0a",
    "semantic_title": "a bert-based language modeling framework",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/masuyama22_interspeech.html": {
    "title": "Joint Optimization of Sampling Rate Offsets Based on Entire Signal Relationship Among Distributed Microphones",
    "volume": "main",
    "abstract": "In this paper, we propose to simultaneously estimate all the sampling rate offsets (SROs) of multiple devices. In a distributed microphone array, the SRO is inevitable, which deteriorates the performance of array signal processing. Most of the existing SRO estimation methods focused on synchronizing two microphones. When synchronizing more than two microphones, we select one reference microphone and estimate the SRO of each non-reference microphone independently. Hence, the relationship among signals observed by non-reference microphones is not considered. To address this problem, the proposed method jointly optimizes all SROs based on a probabilistic model of a multichannel signal. The SROs and model parameters are alternately updated to increase the log-likelihood based on an auxiliary function. The effectiveness of the proposed method is validated on mixtures of various numbers of speakers",
    "checked": true,
    "id": "99c460aa54dedea706420358084dad300ec617b4",
    "semantic_title": "joint optimization of sampling rate offsets based on entire signal relationship among distributed microphones",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ciccarelli22_interspeech.html": {
    "title": "Challenges and Opportunities in Multi-device Speech Processing",
    "volume": "main",
    "abstract": "We review current solutions and technical challenges for automatic speech recognition, keyword spotting, device arbitration, speech enhancement, and source localization in multi-device home environments to provide context for the INTERSPEECH 2022 special session, \"Challenges and opportunities for signal processing and machine learning for multiple smart devices''. We also identify the datasets needed to support these research areas. Based on the review and our research experience in the multi-device domain, we conclude with an outlook on the future evolution of multiple device signal processing and machine learning",
    "checked": true,
    "id": "b178a77420cb07e17b1ecb70585c1c76b55b1ee7",
    "semantic_title": "challenges and opportunities in multi-device speech processing",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/agaskar22_interspeech.html": {
    "title": "Practical Over-the-air Perceptual AcousticWatermarking",
    "volume": "main",
    "abstract": "In this work, we demonstrate a novel technique for automatically scaling over-the-air acoustic watermarks to maximize amplitude while remaining imperceptible to human listeners. These watermarks have been demonstrated in prior work to be robust to the indoor acoustic channel. However, they require careful calibration to ensure that they are (a) detectable by the device and (b) imperceptible to humans. While previously this was done using listening tests, we show that psychoacoustic masking curves can be used to automatically scale each watermark frame's amplitude to be as high as possible while remaining below the masking level. This maximizes watermark detectability by the self-correlation decoder described in earlier work, while ensuring that the watermark is not heard",
    "checked": true,
    "id": "19e546410c6f394e3f108af8beded9d3f1c2ecca",
    "semantic_title": "practical over-the-air perceptual acousticwatermarking",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koppelmann22_interspeech.html": {
    "title": "Clustering-based Wake Word Detection in Privacy-aware Acoustic Sensor Networks",
    "volume": "main",
    "abstract": "This work investigates privacy-aware collaborative wake word detection (WWD) in acoustic sensor networks. To meet state-of-the-art privacy constraints, the proposed WWD scheme is based on privacy-aware unsupervised clustered federated learning that groups microphone nodes w.r.t. active sound sources and on a privacy-preserving high-level feature representation. Using the partition of microphone nodes into clusters, we apply intra- and inter-cluster feature enhancement strategies directly in the privacy-preserving feature domain and thus circumvent the need for communicating privacy-sensitive information between nodes. The approach is demonstrated for an acoustic sensor network deployed in a smart-home environment. We show that the proposed collaborative WWD system clearly outperforms independent decisions of individual microphone nodes. Index Terms: privacy, wake word detection, clustering, federated learning, unsupervised clustered federated learning",
    "checked": true,
    "id": "4ba811b2dc1462cfc3f1a0ad5f7af702079f89d1",
    "semantic_title": "clustering-based wake word detection in privacy-aware acoustic sensor networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nespoli22_interspeech.html": {
    "title": "Relative Acoustic Features for Distance Estimation in Smart-Homes",
    "volume": "main",
    "abstract": "Any audio recording encapsulates the unique fingerprint of the associated acoustic environment, namely the background noise and reverberation. Considering the scenario of a room equipped with a fixed smart speaker device with one or more microphones and a wearable smart device (watch, glasses or smartphone), we employed the improved proportionate normalized least mean square adaptive filter to estimate the relative room impulse response mapping the audio recordings of the two devices. We performed inter-device distance estimation by exploiting a new set of features obtained extending the definition of some acoustic attributes of the room impulse response to its relative version. In combination with the sparseness measure of the estimated relative room impulse response, the relative features allow precise inter-device distance estimation which can be exploited for tasks such as best microphone selection or acoustic scene analysis. Experimental results from simulated rooms of different dimensions and reverberation times demonstrate the effectiveness of this computationally lightweight approach for smart home acoustic ranging applications",
    "checked": true,
    "id": "c2716a8f434fee715511eacdc1f2ffab7b21754c",
    "semantic_title": "relative acoustic features for distance estimation in smart-homes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pandey22c_interspeech.html": {
    "title": "Time-domain Ad-hoc Array Speech Enhancement Using a Triple-path Network",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) are very effective for multichannel speech enhancement with fixed array geometries. However, it is not trivial to use DNNs for ad-hoc arrays with unknown order and placement of microphones. We propose a novel triple-path network for ad-hoc array processing in the time domain. The key idea in the network design is to divide the overall processing into spatial processing and temporal processing and use self-attention for spatial processing. Using self-attention for spatial processing makes the network invariant to the order and the number of microphones. The temporal processing is done independently for all channels using a recently proposed dual-path attentive recurrent network. The proposed network is a multiple-input multiple-output architecture that can simultaneously enhance signals at all microphones. Experimental results demonstrate the excellent performance of the proposed approach. Further, we present analysis to demonstrate the effectiveness of the proposed network in utilizing multichannel information even from microphones at far locations",
    "checked": true,
    "id": "6bb3aeaa2d1641c7d86a3cccf86143a68617b4f9",
    "semantic_title": "time-domain ad-hoc array speech enhancement using a triple-path network",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fietkau22_interspeech.html": {
    "title": "Relationship between the acoustic time intervals and tongue movements of German diphthongs",
    "volume": "main",
    "abstract": "This study investigated the relationship between tongue movements during the production of German diphthongs and their acoustic time intervals. To this end, five subjects produced a set of logatomes that contained German primary, secondary, and peripheral diphthongs in the context of bilabial and labiodental consonants at three different speaking rates. During the utterances, tongue movements were measured by means of optical palatography (OPG), i.e. by optical distance sensing in the oral cavity, along with the acoustic speech signal. The analysis of the movement signals revealed that the diphthongs have s-shaped tongue trajectories that strongly resemble half cosine periods. In addition, acoustic and articulatory diphthong durations have a linear, but not proportional, relationship. Finally, the peak velocity and midpoint between the two targets of a diphthong are reached in the middle of both the acoustic and articulatory diphthong time intervals, regardless of the duration and type of diphthong. These results can help to model realistic tongue movements for diphthongs in articulatory speech synthesis",
    "checked": true,
    "id": "0634ab237d3ea6ee6da371fc9c03e29ab20841c9",
    "semantic_title": "relationship between the acoustic time intervals and tongue movements of german diphthongs",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/matsui22_interspeech.html": {
    "title": "Development of allophonic realization until adolescence: A production study of the affricate-fricative variation of /z/ among Japanese children",
    "volume": "main",
    "abstract": "The development of allophonic variants of phonemes is poorly understood. This study aimed to examine when the allophonic variants of a phoneme are realized like adults. Japanese children aged 513 years and adults participated in an elicited production task. We analyzed developmental changes in allophonic variation of the phoneme /z/, which is realized variably either as an affricate or a fricative. The results revealed that children aged nine years or younger realized /z/ as affricate significantly more than 13-year-old and adult speakers. Once the children reached 11 years of age, the difference compared to adults was not statistically significant, which denotes a similar developmental pattern as that of speech motor control (e.g., lip and jaw) and cognitive-linguistic skill. Moreover, we examined whether the developmental changes of allophonic realization of /z/ are due to speech rate and the time to articulate /z/. The results showed that the allophonic realization of /z/ is not affected by those factors, different from that of adults. We also found that the effects of speech rate and the time to articulate /z/ on the allophonic realization become adult-like at around 11 years of age",
    "checked": true,
    "id": "aa75e7b14a52e3a2b758348cbf6b9bb04b8bf1d9",
    "semantic_title": "development of allophonic realization until adolescence: a production study of the affricate-fricative variation of /z/ among japanese children",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ahn22b_interspeech.html": {
    "title": "Recurrent multi-head attention fusion network for combining audio and text for speech emotion recognition",
    "volume": "main",
    "abstract": "To infer emotions accurately from speech, fusion of audio and text is essential as words carry most information about semantics and emotions. Attention mechanism is essential component in multimodal fusion architecture as it dynamically pairs different regions within multimodal sequences. However, existing architecture lacks explicit structure to model dynamics between fused representations. Thus we propose recurrent multi-head attention in a fusion architecture, which selects salient fused representations and learns dynamics between them. Multiple 2-D attention layers select salient pairs among all possible pairs of audio and text representations, which are combined with fusion operation. Lastly, multiple fused representations are fed into recurrent unit to learn dynamics between fused representations. Our method outperforms existing approaches for fusion of audio and text for speech emotion recognition and achieves state-of-the-art accuracies on benchmark IEMOCAP dataset",
    "checked": true,
    "id": "24130b932d17a53ec50932a074e74b18f989d3e7",
    "semantic_title": "recurrent multi-head attention fusion network for combining audio and text for speech emotion recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/coppietersdegibson22_interspeech.html": {
    "title": "Low-Level Physiological Implications of End-to-End Learning for Speech Recognition",
    "volume": "main",
    "abstract": "Current speech recognition architectures perform very well from the point of view of machine learning, hence user interaction. This suggests that they are emulating the human biological system well. We investigate whether the inference can be inverted to provide insights into that biological system; in particular the hearing mechanism. Using SincNet, we confirm that end-to-end systems do learn well known filterbank structures. However, we also show that wider band-width filters are important in the learned structure. Whilst some benefits can be gained by initialising both narrow and wide-band filters, physiological constraints suggest that such filters arise in mid-brain rather than the cochlea. We show that standard machine learning architectures must be modified to allow this process to be emulated neurally",
    "checked": false,
    "id": "f2cb19ab1c2046b318c24bfb85968b33e6f8d6f9",
    "semantic_title": "low-level physiological implications of end-to-end learning of speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/machado22_interspeech.html": {
    "title": "Idiosyncratic lingual articulation of American English // and // using network analysis",
    "volume": "main",
    "abstract": "Formant dynamics are believed to reflect the characteristic articulatory behavior of a speaker. The present study aims to explore individual articulatory behaviors when producing American English // and //. The two vowels differ in the degree of inherent spectral change, a property believed to carry information about vowel-phoneme identity, which may be reflected in the articulatory movements. We measured first and second formants together with tongue blade and dorsum trajectories from 20 speakers producing 330 words in citation forms. Using the network analysis, the relationships between acoustic and kinematic variables were revealed. In particular, between-speaker articulatory behaviors were most dissimilar in // which requires less inherent spectral change. Moreover, when networks of speakers with similar formant patterns were compared, it was revealed that their articulatory behaviors also shared similarities, although they seemed to be organized in characteristic ways. These findings contribute to our understanding of the complex interaction between articulatory variables and the acoustic outcome",
    "checked": true,
    "id": "f0cb58052f86d1bfc1cddd2ab86024c43d498e15",
    "semantic_title": "idiosyncratic lingual articulation of american english // and // using network analysis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/toya22_interspeech.html": {
    "title": "Method for improving the word intelligibility of presented speech using bone-conduction headphones",
    "volume": "main",
    "abstract": "Bone-conduction (BC) headphones enable listeners to hear sounds through BC while leaving the ear canal (EC) open to enable surrounding air-conducted (AC) sound to pass through at the same time. However, the intelligibility of presented speech using BC headphones is degraded by BC transmission, especially in noisy environments. This paper proposes a method for improving the word intelligibility of presented BC speech under noisy conditions. The method consists of two types of emphasis: higher-frequency emphasis and consonant emphasis. In the higher-frequency emphasis, frequency components attenuated due to BC transmission were compensated by the inverse-filtering of the transfer function obtained from the regio-temporalis (RT) vibration or the EC radiated sound. In the consonant emphasis, consonant sections with 20-ms short-formant trajectories of subsequent vowels in speech signals were locally amplified by a constant gain. The results of word intelligibility tests showed that both types of emphasis had significant improvements in comparison with no-emphasis. Moreover, we found that the proposed method had the best improvements under all conditions",
    "checked": true,
    "id": "3e32efbdcc7432de3923a5aa715f40db385a28a2",
    "semantic_title": "method for improving the word intelligibility of presented speech using bone-conduction headphones",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mohapatra22_interspeech.html": {
    "title": "Three-dimensional finite-difference time-domain acoustic analysis of simplified vocal tract shapes",
    "volume": "main",
    "abstract": "The finite-difference time-domain (FDTD) method has been widely used for vocal tract acoustic modelling due to its simplicity and low computational cost. Nevertheless, the method suffers from high discretization error while approximating realistic vocal tract geometries using orthogonal grid elements. Alternatively, simplified vocal tract shapes having regular contours can be used for articulatory models. These geometries can be generated from one-dimensional (1D) area functions, which approximate vocal tracts as concatenated tubes with different cross-sections. To this aim, we modify an existing 3D FDTD model for faster acoustic simulation and synthesize five English vowels with various simplified vocal tract shapes. We implement six geometrical shapes for each vowel, consisting of circular, elliptical and square cross-sections with centric and eccentric tube segment configurations. Vowel transfer functions obtained from these FDTD simulations are compared with a highly accurate finite element (FE) scheme. The acoustic formants of the FDTD model agree well with the corresponding FEM approach for most vowels. The influence of vocal tracts with different geometry approximations remains insignificant for frequencies below $5$~kHz. However, vocal tracts with elliptical or eccentric configurations have produced higher-order acoustic modes. This paper characterizes the acoustic properties of simplified vocal tract shapes using the 3D FDTD scheme",
    "checked": true,
    "id": "8c6d89a4e25acb5bdca4afcb65676fe8723df1cc",
    "semantic_title": "three-dimensional finite-difference time-domain acoustic analysis of simplified vocal tract shapes",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dejong22_interspeech.html": {
    "title": "Speech imitation skills predict automatic phonetic convergence: a GMM-UBM study on L2",
    "volume": "main",
    "abstract": "Phonetic convergence is the observation that two interlocutors adapt their speech towards one another on an acoustic-phonetic level. It happens automatically and unconsciously, but people can also deliberately imitate others when asked to do so. Here, we investigate to what degree people converge to their interlocutor in a scripted dialogue when they are and when they are not explicitly requested to imitate their interlocutor. More specifically, we collected two separate data sets, where Italian- and French-native participants read English sentences aloud in alternating speaking turns. The results of both groups with different language backgrounds were compared against each other. We used a Gaussian mixture model  universal background model (GMM-UBM) to assess phonetic convergence on the sentence level. The GMM-UBM configuration was optimized to make the best distinction between speakers on validation data. We found that people start to converge to one another while interacting compared to the baseline and even more substantially when explicitly asked to do so. Results are robust across data sets. More importantly, the degree of implicit convergence people display is related to how good of an explicit imitator they are, supporting the claim that the two phenomena are based on the same neurocognitive process",
    "checked": true,
    "id": "03affadb0ff26037ce30ed7ceccc482ba944e715",
    "semantic_title": "speech imitation skills predict automatic phonetic convergence: a gmm-ubm study on l2",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/georges22_interspeech.html": {
    "title": "Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE",
    "volume": "main",
    "abstract": "The human perception system is often assumed to recruit motor knowledge when processing auditory speech inputs. Using articulatory modeling and deep learning, this study examines how this articulatory information can be used for discovering speech units in a self-supervised setting. We used vector-quantized variational autoencoders (VQ-VAE) to learn discrete representations from articulatory and acoustic speech data. In line with the zero-resource paradigm, an ABX test was then used to investigate how the extracted representations encode phonetically relevant properties. Experiments were conducted on three different corpora in English and French. We found that articulatory information rather organises the latent representations in terms of place of articulation whereas the speech acoustics mainly structure the latent space in terms of manner of articulation. We show that an optimal fusion of the two modalities can lead to a joint representation of these phonetic dimensions more accurate than each modality considered individually. Since articulatory information is usually not available in a practical situation, we finally investigate the benefit it provides when inferred from the speech acoustics in a self-supervised manner",
    "checked": true,
    "id": "372f6831903dab5bdb15b583f8f4890a7be6161e",
    "semantic_title": "self-supervised speech unit discovery from articulatory and acoustic features using vq-vae",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22i_interspeech.html": {
    "title": "Deep Speech Synthesis from Articulatory Representations",
    "volume": "main",
    "abstract": "In the articulatory synthesis task, speech is synthesized from input features containing information about the physical behavior of the human vocal tract. This task provides a promising direction for speech synthesis research, as the articulatory space is compact, smooth, and interpretable. Current works have highlighted the potential for deep learning models to perform articulatory synthesis. However, it remains unclear whether these models can achieve the efficiency and fidelity of the human speech production system. To help bridge this gap, we propose a time-domain articulatory synthesis methodology and demonstrate its efficacy with both electromagnetic articulography (EMA) and synthetic articulatory feature inputs. Our model is computationally efficient and achieves a transcription word error rate (WER) of 18.5% for the EMA-to-speech task, yielding an improvement of 11.6% compared to prior work. Through interpolation experiments, we also highlight the generalizability and interpretability of our approach",
    "checked": true,
    "id": "94796c7fceed1e6517575963e1b3f86e151aeb61",
    "semantic_title": "deep speech synthesis from articulatory representations",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ashokumar22_interspeech.html": {
    "title": "Orofacial somatosensory inputs in speech perceptual training modulate speech production",
    "volume": "main",
    "abstract": "Somatosensory inputs are important to acquire or learn precise control of movement [1]. In the case of speech, receiving somatosensory inputs together with corresponding speech sounds may be a key to formulate or calibrate the speech production system [2]. We here examined whether speech production can be modulated by perceptual training with repetitive exposure to paired auditory-somatosensory stimulation in the absence of actual production of the sound. We carried out a perceptual training using a vowel identification task with /e/-/eu/ continuum. The speech sounds were accompanied with somatosensory stimulation, in which a facial skin-stretch was applied in the backward direction. The vowels /e/ and /eu/ were recorded prior to and following the training and the first three formants were compared. Results showed that the third formant of /e/ was increased following the training, and the rest of formant was not changed. Since the current somatosensory stimulation was related to the articulatory movement for the production of /e/ (lip-spreading), repetitive exposure to somatosensory stimulation in addition to the sound may specifically change the articulatory behavior for the production of /e/. The results suggest that perceptual training with specific pairs of auditory-somatosensory inputs can be important to formulate production mechanisms",
    "checked": true,
    "id": "097cab1652391eedee8652105610b1ff764c3f70",
    "semantic_title": "orofacial somatosensory inputs in speech perceptual training modulate speech production",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22c_interspeech.html": {
    "title": "Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus",
    "volume": "main",
    "abstract": "Training a text-to-speech (TTS) model requires a large-scale text labeled speech corpus, which is troublesome to collect. In this paper, we propose a transfer learning framework for TTS that utilizes a large amount of unlabeled speech dataset for pre-training. By leveraging wav2vec2.0 representation, unlabeled speech can highly improve performance, especially in the lack of labeled speech. We also extend the proposed method to zero-shot multi-speaker TTS (ZS-TTS). The experimental results verify the effectiveness of the proposed method in terms of naturalness, intelligibility, and speaker generalization. We highlight that the single speaker TTS model fine-tuned on only 10 minutes of labeled dataset outperforms the other baselines, and the ZS-TTS model fine-tuned on only 30 minutes of single speaker dataset can generate the voice of the arbitrary speaker, by pre-training on an unlabeled multi-speaker speech corpus",
    "checked": true,
    "id": "f962c52c92097da18965a170b322e3dea63a3e47",
    "semantic_title": "transfer learning framework for low-resource text-to-speech using a large-scale unlabeled speech corpus",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeki22_interspeech.html": {
    "title": "DRSpeech: Degradation-Robust Text-to-Speech Synthesis with Frame-Level and Utterance-Level Acoustic Representation Learning",
    "volume": "main",
    "abstract": "Most text-to-speech (TTS) methods use high-quality speech corpora recorded in a well-designed environment, incurring a high cost for data collection. To solve this problem, existing noise-robust TTS methods are intended to use noisy speech corpora as training data. However, they only address either time-invariant or time-variant noises. We propose a degradation-robust TTS method, which can be trained on speech corpora that contain both additive noises and environmental distortions. It jointly represents the time-variant additive noises with a frame-level encoder and the time-invariant environmental distortions with an utterance-level encoder. We also propose a regularization method to attain clean environmental embedding that is disentangled from the utterance-dependent information such as linguistic contents and speaker characteristics. Evaluation results show that our method achieved significantly higher-quality synthetic speech than previous methods in the condition including both additive noise and reverberation",
    "checked": true,
    "id": "d3b5d9c635bf14b9bfd563df9fa52a22e653cfae",
    "semantic_title": "drspeech: degradation-robust text-to-speech synthesis with frame-level and utterance-level acoustic representation learning",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mitsui22b_interspeech.html": {
    "title": "MSR-NV: Neural Vocoder Using Multiple Sampling Rates",
    "volume": "main",
    "abstract": "The development of neural vocoders (NVs) has resulted in the high-quality and fast generation of waveforms. However, conventional NVs target a single sampling rate and require re-training when applied to different sampling rates. A suitable sampling rate varies from application to application due to the trade-off between speech quality and generation speed. In this study, we propose a method to handle multiple sampling rates in a single NV, called the MSR-NV. By generating waveforms step-by-step starting from a low sampling rate, MSR-NV can efficiently learn the characteristics of each frequency band and synthesize high-quality speech at multiple sampling rates. It can be regarded as an extension of the previously proposed NVs, and in this study, we extend the structure of Parallel WaveGAN (PWG). Experimental evaluation results demonstrate that the proposed method achieves remarkably higher subjective quality than the original PWG trained separately at 16, 24, and 48 kHz, without increasing the inference time. We also show that MSR-NV can leverage speech with lower sampling rates to further improve the quality of the synthetic speech",
    "checked": true,
    "id": "4893e0c886fedcb8e662dbc95f5b122f95f0a7fc",
    "semantic_title": "msr-nv: neural vocoder using multiple sampling rates",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koizumi22_interspeech.html": {
    "title": "SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping",
    "volume": "main",
    "abstract": "Neural vocoder using denoising diffusion probabilistic model (DDPM) has been improved by adaptation of the diffusion noise distribution to given acoustic features. In this study, we propose SpecGrad that adapts the diffusion noise so that its time-varying spectral envelope becomes close to the conditioning log-mel spectrogram. This adaptation by time-varying filtering improves the sound quality especially in the high-frequency bands. It is processed in the time-frequency domain to keep the computational cost almost the same as the conventional DDPM-based neural vocoders. Experimental results showed that SpecGrad generates higher-fidelity speech waveform than conventional DDPM-based neural vocoders in both analysis-synthesis and speech enhancement scenarios. Audio demos are available at wavegrad.github.io/specgrad/",
    "checked": true,
    "id": "3990b97b9cdb46ecb0cd2d62394865d58b223675",
    "semantic_title": "specgrad: diffusion probabilistic model based neural vocoder with adaptive noise spectral shaping",
    "citation_count": 22
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22_interspeech.html": {
    "title": "Bunched LPCNet2: Efficient Neural Vocoders Covering Devices from Cloud to Edge",
    "volume": "main",
    "abstract": "Text-to-Speech (TTS) services that run on edge devices have many advantages compared to cloud TTS, e.g., latency and privacy issues. However, neural vocoders with a low complexity and small model footprint inevitably generate annoying sounds. This study proposes a Bunched LPCNet2, an improved LPCNet architecture that provides highly efficient performance in high-quality for cloud servers and in a low-complexity for low-resource edge devices. Single logistic distribution achieves computational efficiency, and insightful tricks reduce the model footprint while maintaining speech quality. A DualRate architecture, which generates a lower sampling rate from a prosody model, is also proposed to reduce maintenance costs. The experiments demonstrate that Bunched LPCNet2 generates satisfactory speech quality with a model footprint of 1.1MB while operating faster than real-time on a RPi 3B. Our audio samples are available at https://srtts.github.io/bunchedLPCNet2",
    "checked": true,
    "id": "897696d4556d883f326423a0464ee3215d6e0af8",
    "semantic_title": "bunched lpcnet2: efficient neural vocoders covering devices from cloud to edge",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bae22b_interspeech.html": {
    "title": "Hierarchical and Multi-Scale Variational Autoencoder for Diverse and Natural Non-Autoregressive Text-to-Speech",
    "volume": "main",
    "abstract": "This paper proposes a hierarchical and multi-scale variational autoencoder-based non-autoregressive text-to-speech model (HiMuV-TTS) to generate natural speech with diverse speaking styles. Recent advances in non-autoregressive TTS (NAR-TTS) models have significantly improved the inference speed and robustness of synthesized speech. However, the diversity of speaking styles and naturalness are needed to be improved. To solve this problem, we propose the HiMuV-TTS model that first determines the global-scale prosody and then determines the local-scale prosody via conditioning on the global-scale prosody and the learned text representation. In addition, we improve the quality of speech by adopting the adversarial training technique. Experimental results verify that the proposed HiMuV-TTS model can generate more diverse and natural speech as compared to TTS models with single-scale variational autoencoders, and can represent different prosody information in each scale",
    "checked": true,
    "id": "0987e5b56c119c3d562b1e80fc8979ebcd90dce9",
    "semantic_title": "hierarchical and multi-scale variational autoencoder for diverse and natural non-autoregressive text-to-speech",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/subramani22_interspeech.html": {
    "title": "End-to-end LPCNet: A Neural Vocoder With Fully-Differentiable LPC Estimation",
    "volume": "main",
    "abstract": "Neural vocoders have recently demonstrated high quality speech synthesis, but typically require a high computational complexity. LPCNet was proposed as a way to reduce the complexity of neural synthesis by using linear prediction (LP) to assist an autoregressive model. At inference time, LPCNet relies on the LP coefficients being explicitly computed from the input acoustic features. That makes the design of LPCNet-based systems more complicated, while adding the constraint that the input features must represent a clean speech spectrum. We propose an end-to-end version of LPCNet that lifts these limitations by learning to infer the LP coefficients from the input features in the frame rate network. Results show that the proposed end-to-end approach equals or exceeds the quality of the original LPCNet model, but without explicit LP analysis. Our open-source end-to-end model still benefits from LPCNet's low complexity, while allowing for any type of conditioning features",
    "checked": true,
    "id": "48c6d19c7719891e27d2e7d2f53ea3cdfc0d1c78",
    "semantic_title": "end-to-end lpcnet: a neural vocoder with fully-differentiable lpc estimation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lam22_interspeech.html": {
    "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
    "volume": "main",
    "abstract": "Neural models are known to be over-parameterized, and recent work has shown that sparse text-to-speech (TTS) models can outperform dense models. Although a plethora of sparse methods has been proposed for other domains, such methods have rarely been applied in TTS. In this work, we seek to answer the question: what are the characteristics of selected sparse techniques on the performance and model complexity? We compare a Tacotron2 baseline and the results of applying five techniques. We then evaluate the performance via the factors of naturalness, intelligibility and prosody, while reporting model size and training time. Complementary to prior research, we find that pruning before or during training can achieve similar performance to pruning after training and can be trained much faster, while removing entire neurons degrades performance much more than removing parameters. To our best knowledge, this is the first work that compares sparsity paradigms in text-to-speech synthesis",
    "checked": true,
    "id": "688d5bd53be1c02fe1d313ebc2d49eb382b175bb",
    "semantic_title": "epic tts models: empirical pruning investigations characterizing text-to-speech models",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nikitaras22_interspeech.html": {
    "title": "Fine-grained Noise Control for Multispeaker Speech Synthesis",
    "volume": "main",
    "abstract": "A text-to-speech (TTS) model typically factorizes speech attributes such as content, speaker and prosody into disentangled representations. Recent works aim to additionally model the acoustic conditions explicitly, in order to disentangle the primary speech factors, i.e. linguistic content, prosody and timbre from any residual factors, such as recording conditions and background noise. This paper proposes unsupervised, interpretable and fine-grained noise and prosody modeling. We incorporate adversarial training, representation bottleneck and utterance-to-frame modeling in order to learn frame-level noise representations. To the same end, we perform fine-grained prosody modeling via a Fully Hierarchical Variational AutoEncoder (FVAE) which additionally results in more expressive speech synthesis. Experimental results support our claims and ablation studies verify the importance of each proposed component. Audio samples are available in our demo page",
    "checked": true,
    "id": "09e76c243759c2233648e9f27e60bbba481011c8",
    "semantic_title": "fine-grained noise control for multispeaker speech synthesis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/siuzdak22_interspeech.html": {
    "title": "WavThruVec: Latent speech representation as intermediate features for neural speech synthesis",
    "volume": "main",
    "abstract": "Recent advances in neural text-to-speech research have been dominated by two-stage pipelines utilizing low-level intermediate speech representation such as mel-spectrograms. However, such predetermined features are fundamentally limited, because they do not allow to exploit the full potential of a data-driven approach through learning hidden representations. For this reason, several end-to-end methods have been proposed. However, such models are harder to train and require a large number of high-quality recordings with transcriptions. Here, we propose WavThruVec - a two-stage architecture that resolves the bottleneck by using high-dimensional wav2vec 2.0 embeddings as intermediate speech representation. Since these hidden activations provide high-level linguistic features, they are more robust to noise. That allows us to utilize annotated speech datasets of a lower quality to train the first-stage module. At the same time, the second-stage component can be trained on large-scale untranscribed audio corpora, as wav2vec 2.0 embeddings are already time-aligned. This results in an increased generalization capability to out-of-vocabulary words, as well as to a better generalization to unseen speakers. We show that the proposed model not only matches the quality of state-of-the-art neural models, but also presents useful properties enabling tasks like voice conversion or zero-shot synthesis",
    "checked": true,
    "id": "0d8d119f378b9c560eba40d95095d67e10f604f9",
    "semantic_title": "wavthruvec: latent speech representation as intermediate features for neural speech synthesis",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vovk22_interspeech.html": {
    "title": "Fast Grad-TTS: Towards Efficient Diffusion-Based Speech Generation on CPU",
    "volume": "main",
    "abstract": "Recently, score-based diffusion probabilistic modeling has shown encouraging results in various tasks outperforming other popular generative modeling frameworks in terms of quality. However, to unlock its potential and make diffusion models feasible from the practical point of view, special efforts should be made to enable more efficient iterative sampling procedure on CPU devices. In this paper, we focus on applying the most promising techniques from recent literature on diffusion modeling to Grad-TTS, a diffusion-based text-to-speech system, in order to accelerate it. We compare various reverse diffusion sampling schemes, the technique of progressive distillation, GAN-based diffusion modeling and score-based generative modeling in latent space. Experimental results demonstrate that it is possible to speed Grad-TTS up to 4.5 times compared to vanilla Grad-TTS and achieve real time factor 0.15 on CPU while keeping synthesis quality competitive with that of conventional text-to-speech baselines",
    "checked": true,
    "id": "e19c63a7a87e6a75267b552debb90905050e8ff9",
    "semantic_title": "fast grad-tts: towards efficient diffusion-based speech generation on cpu",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22z_interspeech.html": {
    "title": "Simple and Effective Unsupervised Speech Synthesis",
    "volume": "main",
    "abstract": "We introduce the first unsupervised speech synthesis system based on a simple, yet effective recipe. The framework leverages recent work in unsupervised speech recognition as well as existing neural-based speech synthesis. Using only unlabeled speech audio and unlabeled text as well as a lexicon, our method enables speech synthesis without the need for a human-labeled corpus. Experiments demonstrate the unsupervised system can synthesize speech similar to a supervised counterpart in terms of naturalness and intelligibility measured by human evaluation",
    "checked": true,
    "id": "2fd1811b9ad335aea526eb6d80046101ee8a3703",
    "semantic_title": "simple and effective unsupervised speech synthesis",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoneyama22_interspeech.html": {
    "title": "Unified Source-Filter GAN with Harmonic-plus-Noise Source Excitation Generation",
    "volume": "main",
    "abstract": "This paper introduces a unified source-filter network with a harmonic-plus-noise source excitation generation mechanism. In our previous work, we proposed unified Source-Filter GAN (uSFGAN) for developing a high-fidelity neural vocoder with flexible voice controllability using a unified source-filter neural network architecture. However, the capability of uSFGAN to model the aperiodic source excitation signal is insufficient, and there is still a gap in sound quality between the natural and generated speech. To improve the source excitation modeling and generated sound quality, a new source excitation generation network separately generating periodic and aperiodic components is proposed. The advanced adversarial training procedure of HiFiGAN is also adopted to replace that of Parallel WaveGAN used in the original uSFGAN. Both objective and subjective evaluation results show that the modified uSFGAN significantly improves the sound quality of the basic uSFGAN while maintaining the voice controllability",
    "checked": true,
    "id": "7187a10fddecf7a667c6dcafd3ed6d2006ec5a82",
    "semantic_title": "unified source-filter gan with harmonic-plus-noise source excitation generation",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22e_interspeech.html": {
    "title": "NeMo Open Source Speaker Diarization System",
    "volume": "main",
    "abstract": "We introduce an open-source speaker diarization system which is part of the NeMo conversational AI toolkit. During the Show and Tell session, we will present an interactive system which demonstrates both online and offline speaker diarization. The audience would be able to test the speaker diarization system by recording their voice. We believe that our demo session would be an excellent opportunity to learn and experience how a speaker diarization system can be implemented for real-life applications using an open source toolkit",
    "checked": true,
    "id": "4fc8ee1ab88bd3fd78205eb4479eef03800cfadb",
    "semantic_title": "nemo open source speaker diarization system",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22e_interspeech.html": {
    "title": "Voice2Alliance: Automatic Speaker Diarization and Quality Assurance of Conversational Alignment",
    "volume": "main",
    "abstract": "We propose a real-time AI system to conduct sentence-level quality assurance of conversational alignment based on speakerdiarized dialogues transcribed from automatic speech recognition of continuous audio stream. This system utilizes two new interactive engine: (1) an online registration-free speaker diarization component to perform separation of speech utterances of multiple speakers in the conversations that learns from user feedback; (2) a turn-level scoring mechanism that infers the conversation quality by computing a similarity score between the deep embeddings of a user-specified scoring inventory of interest, and the current sentence that the user is speaking. These real-time scores are known to be predictive to successful conversational outcome (such as relating to the therapeutic working alliance, which is an important indicator of clinical psychotherapy outcome). Other than evaluating the empirical advantages of the core components on existing dataset, we demonstrate the effectiveness of this system in a web-based application at https://www.baihan.nyc/viz/Voice2Alliance/",
    "checked": true,
    "id": "e9e2c291d17bff03f062fd3eb733ae85c206a198",
    "semantic_title": "voice2alliance: automatic speaker diarization and quality assurance of conversational alignment",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kumar22d_interspeech.html": {
    "title": "VAgyojaka: An Annotating and Post-Editing Tool for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Vagyojaka is an open-source post-editing and annotation tool  for automatic speech recognition (ASR) that aims to reduce the human effort required to correct the ASR results. We adopt a dictionary-based lookup method to highlight the incorrect words in the ASR transcript and give suggestions by generat ing the closest valid words. For curating the speech corpus, we provide a rich list of tagset that captures various spoken audio features. Further, we conducted a user study to evaluate the ef fectiveness of our tool and observed that post-editing requires 1/3 lesser time than editing without using our tool. The user study can be found on our website 1",
    "checked": true,
    "id": "a61f2d2cf7b7f5fab1c06f0f0338c1391ed8be53",
    "semantic_title": "vagyojaka: an annotating and post-editing tool for automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/badi22_interspeech.html": {
    "title": "SKYE: More than a conversational AI",
    "volume": "main",
    "abstract": "This paper presents how a virtual character was created through a conversational AI system that not only has an open-domain dialog system but also takes visual and audio inputs to create an interactive and interpersonal responses to users. The virtual character has its 3D figure with a particular voice and personality for the immersive AI experience. Multiple AI modules were integrated into the system to make human-like interaction including automatic speech recognition, Text-to-speech model, intelligent open-domain dialog system, and 3D game engine",
    "checked": true,
    "id": "cb69c14e0733f0ed2b8035fe86e791fcb8e14821",
    "semantic_title": "skye: more than a conversational ai",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/munakata22_interspeech.html": {
    "title": "Training Data Generation with DOA-based Selecting and Remixing for Unsupervised Training of Deep Separation Models",
    "volume": "main",
    "abstract": "We propose a simple and easy-to-apply unsupervised training method for multi-channel deep separation models used in sound source separation. Such models require a large amount of training data, i.e., source signals and their mixtures. A previous method uses pseudo-target source signals, which can be obtained as the outputs of blind source separation (BSS) based on statistical models in place of ground-truth source signals. However, the model performance of the previous method is degraded by some pseudo-targets that are inadequately separated by BSS. To exploit the reliable part of BSS, we select and remix well-separated signals included in the BSS result. In the selection step, we choose well-separated signals using the direction of arrival (DOA). As a criterion that addresses the quality of the separated signals, we adopted the minimum angular difference of DOA between source signals. In the remixing step, we introduce resampling of the DOA, which generates mixtures composed of source signals with both wide and narrow angular differences. These mixtures are not simply given by BSS and allow the deep separation model to learn both spectral and spatial information. In our experiment, our method's model performance was improved for mixture signals composed of the sources from various angles",
    "checked": true,
    "id": "a8b2c2190611da0322bd0fedfd7070eb241a175c",
    "semantic_title": "training data generation with doa-based selecting and remixing for unsupervised training of deep separation models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22e_interspeech.html": {
    "title": "Beam-Guided TasNet: An Iterative Speech Separation Framework with Multi-Channel Output",
    "volume": "main",
    "abstract": "Time-domain audio separation network (TasNet) has achieved remarkable performance in blind source separation (BSS). Classic multi-channel speech processing framework employs signal estimation and beamforming. For example, Beam-TasNet links multi-channel convolutional TasNet (MC-Conv-TasNet) with minimum variance distortionless response (MVDR) beamforming, which leverages the strong modeling ability of data-driven network and boosts the performance of beamforming with an accurate estimation of speech statistics. Such integration can be viewed as a directed acyclic graph by accepting multi-channel input and generating multi-source output. In this paper, we design a \"multi-channel input, multi-channel multi-source output'' (MIMMO) speech separation system entitled \"Beam-Guided TasNet'', where MC-Conv-TasNet and MVDR can interact and promote each other more compactly under a directed cyclic flow. Specifically, the first stage uses Beam-TasNet to generate estimated single-speaker signals, which favors the separation in the second stage. The proposed framework facilitates iterative signal refinement with the guide of beamforming and seeks to reach the upper bound of the MVDR-based methods. Experimental results on the spatialized WSJ0-2MIX demonstrate that the Beam-Guided TasNet has achieved an SDR of 21.5 dB, exceeding the baseline Beam-TasNet by 4.1 dB under the same model size and narrowing the gap with the oracle signal-based MVDR to 2 dB",
    "checked": true,
    "id": "5f08db77a9b5b1e7e0d2ea9301e494722abc14fa",
    "semantic_title": "beam-guided tasnet: an iterative speech separation framework with multi-channel output",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xiong22b_interspeech.html": {
    "title": "Joint Estimation of Direction-of-Arrival and Distance for Arrays with Directional Sensors based on Sparse Bayesian Learning",
    "volume": "main",
    "abstract": "Source localization with sensor arrays is an active research topic in many areas, such as speaker localization and communication. The existing estimators usually focus on arrays with omnidirectional sensors, but struggle on arrays with directional sensors. In this work, a new method is proposed for locating the near-field sources based on sparse Bayesian learning (SBL), which is capable of integrating the near-field signal model to jointly estimate direction-of-arrival (DOA) and distance. By further considering the directionality of sensors in the signal model which takes full advantage of the magnitude information, the proposed method can handle arrays with both omnidirectional and directional sensors. Simulation results show that the proposed method yields a sharp spatial spectrum, and performs more accurately than traditional near-field Multiple Signal Classification (MUSIC) and Steered-Response Power Phase Transform (SRP-PHAT) for arrays covering heterogeneous directional sensors",
    "checked": true,
    "id": "0cd98e63a07fcc3b25d13e91dbadc47d21aa6838",
    "semantic_title": "joint estimation of direction-of-arrival and distance for arrays with directional sensors based on sparse bayesian learning",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22d_interspeech.html": {
    "title": "How to Listen? Rethinking Visual Sound Localization",
    "volume": "main",
    "abstract": "Localizing visual sounds consists of locating the position of objects that emit sound within an image. It is a growing research area with potential applications in monitoring natural and urban environments, such as wildlife migration and urban traffic. Previous works were usually evaluated with datasets having mostly a single dominant visible object, and their proposed models usually require the introduction of localization modules during training or dedicated sampling strategies, but it remains unclear how these design choices play a role in the adaptability of these methods in more challenging scenarios. In this work, we analyze various model choices for visual sound localization and discuss how their different components affect the model's performance, namely the encoders' architecture, the loss function and the localization strategy. Furthermore, we study the interaction between these decisions, the model performance, and the data, by digging into different evaluation datasets spanning different difficulties and characteristics, and discuss the implications of such decisions in the context of real-world applications. Our code and model weights are open-sourced and made available for further applications",
    "checked": true,
    "id": "3bcf3b0eb24204ba7b57a8f9b60d69c1501b63a8",
    "semantic_title": "how to listen? rethinking visual sound localization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ouyang22_interspeech.html": {
    "title": "Small Footprint Neural Networks for Acoustic Direction of Arrival Estimation",
    "volume": "main",
    "abstract": "In this paper we propose acoustic direction of arrival (DOA) estimation with neural networks. Conventional signal processing tasks such as DOA estimation have benefited from recent advancements in deep learning, which leads to a data-driven approach that allows neural networks to be employed in a black-box manner. From traditional aspects, modern network models often lack interpretability when directly employed in signal processing realm. As an alternative, we introduce a learnable network from spatial acoustical DOA estimation. Convolutional variants on feature projection can be derived while maintaining the explainability in both acoustical and neural network aspects. We introduce factorized spatial-temporal-spectral filtering which can significantly reduce computational cost and memory footprint. Experiments show the proposed networks perform well in harsh acoustic conditions with reduced requirement for hardware resources",
    "checked": true,
    "id": "f1c991331b25c0f9b5e2993462a17c2593a7a63b",
    "semantic_title": "small footprint neural networks for acoustic direction of arrival estimation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22s_interspeech.html": {
    "title": "Multi-Modal Multi-Correlation Learning for Audio-Visual Speech Separation",
    "volume": "main",
    "abstract": "In this paper we propose a multi-modal multi-correlation learning framework targeting at the task of audio-visual speech separation. Although previous efforts have been extensively put on combining audio and visual modalities, most of them solely adopt a straightforward concatenation of audio and visual features. To exploit the real useful information behind these two modalities, we define two key correlations which are: (1) identity correlation (between timbre and facial attributes); (2) phonetic correlation (between phoneme and lip motion). These two correlations together comprise the complete information, which shows a certain superiority in separating target speaker's voice especially in some hard cases, such as the same gender or similar content. For implementation, contrastive learning or adversarial training approach is applied to maximize these two correlations. Both of them work well, while adversarial training shows its advantage by avoiding some limitations of contrastive learning. Compared with previous research, our solution demonstrates clear improvement on experimental metrics without additional complexity. Further analysis reveals the validity of the proposed architecture and its good potential for future extension",
    "checked": true,
    "id": "fd03dc876a404305cb3ce480e19e1df111f098aa",
    "semantic_title": "multi-modal multi-correlation learning for audio-visual speech separation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yin22b_interspeech.html": {
    "title": "MIMO-DoAnet: Multi-channel Input and Multiple Outputs DoA Network with Unknown Number of Sound Sources",
    "volume": "main",
    "abstract": "Recent neural network based Direction of Arrival (DoA) estimation algorithms have performed well on unknown number of sound sources scenarios. These algorithms are usually achieved by mapping the multi-channel audio input to the single output (i.e. overall spatial pseudo-spectrum (SPS) of all sources), that is called MISO. However, such MISO algorithms strongly depend on empirical threshold setting and the angle assumption that the angles between the sound sources are greater than a fixed angle. To address these limitations, we propose a novel multi-channel input and multiple outputs DoA network called MIMO-DoAnet. Unlike the general MISO algorithms, MIMO-DoAnet predicts the SPS coding of each sound source with the help of the informative spatial covariance matrix. By doing so, the threshold task of detecting the number of sound sources becomes an easier task of detecting whether there is a sound source in each output, and the serious interaction between sound sources disappears during inference stage. Experimental results show that MIMO-DoAnet achieves relative 18.6% and absolute 13.3%, relative 34.4% and absolute 20.2% F1 score improvement compared with the MISO baseline system in 3, 4 sources scenes. The results also demonstrate MIMO-DoAnet alleviates the threshold setting problem and solves the angle assumption problem effectively",
    "checked": true,
    "id": "c35a34633b45dbc6007fd0c3f6dcd322072b1ddd",
    "semantic_title": "mimo-doanet: multi-channel input and multiple outputs doa network with unknown number of sound sources",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fu22c_interspeech.html": {
    "title": "Iterative Sound Source Localization for Unknown Number of Sources",
    "volume": "main",
    "abstract": "Sound source localization aims to seek the direction of arrival (DOA) of all sound sources from the observed multi-channel audio. For the practical problem of unknown number of sources, existing localization algorithms attempt to predict a likelihood-based coding (i.e., spatial spectrum) and employ a pre-determined threshold to detect the source number and corresponding DOA value. However, these threshold-based algorithms are not stable since they are limited by the careful choice of threshold. To address this problem, we propose an iterative sound source localization approach called ISSL, which can iteratively extract each source's DOA without threshold until the termination criterion is met. Unlike threshold-based algorithms, ISSL designs an active source detector network based on binary classifier to accept residual spatial spectrum and decide whether to stop the iteration. By doing so, our ISSL can deal with an arbitrary number of sources, even more than the number of sources seen during the training stage. The experimental results show that our ISSL achieves significant performance improvements in both DOA estimation and source number detection compared with the existing threshold-based algorithms",
    "checked": true,
    "id": "5de9abc8fabff0008792806a5ee183a54aa17719",
    "semantic_title": "iterative sound source localization for unknown number of sources",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/patterson22_interspeech.html": {
    "title": "Distance-Based Sound Separation",
    "volume": "main",
    "abstract": "We propose the novel task of distance-based sound separation, where sounds are separated based only on their distance from a single microphone. In the context of assisted listening devices, proximity provides a simple criterion for sound selection in noisy environments that would allow the user to focus on sounds relevant to a local conversation. We demonstrate the feasibility of this approach by training a neural network to separate near sounds from far sounds in single channel synthetic reverberant mixtures, relative to a threshold distance defining the boundary between near and far. With a single nearby speaker and four distant speakers, the model improves scale-invariant signal to noise ratio by 4.4 dB for near sounds and 6.8 dB for far sounds",
    "checked": true,
    "id": "9c181fc3361d4c6635cd50ef6fc212b1a29762ee",
    "semantic_title": "distance-based sound separation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22ba_interspeech.html": {
    "title": "VCSE: Time-Domain Visual-Contextual Speaker Extraction Network",
    "volume": "main",
    "abstract": "Speaker extraction seeks to extract the target speech in a multi-talker scenario given an auxiliary reference. Such reference can be auditory, i.e., a pre-recorded speech, visual, i.e., lip movements, or contextual, i.e., phonetic sequence. References in different modalities provide distinct and complementary information that could be fused to form top-down attention on the target speaker. Previous studies have introduced visual and contextual modalities in a single model. In this paper, we propose a two-stage time-domain visual-contextual speaker extraction network named VCSE, which incorporates visual and self-enrolled contextual cues stage by stage to take full advantage of every modality. In the first stage, we pre-extract a target speech with visual cues and estimate the underlying phonetic sequence. In the second stage, we refine the pre-extracted target speech with the self-enrolled contextual cues. Experimental results on the real-world Lip Reading Sentences 3 (LRS3) database demonstrate that our proposed VCSE network consistently outperforms other state-of-the-art baselines",
    "checked": true,
    "id": "b0139ccdc763312099cf968a7ba40e28498e0c80",
    "semantic_title": "vcse: time-domain visual-contextual speaker extraction network",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/aroudi22_interspeech.html": {
    "title": "TRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant Sound Source Separation",
    "volume": "main",
    "abstract": "In recent years, many deep learning techniques for single-channel sound source separation have been proposed using recurrent, convolutional and transformer networks. When multiple microphones are available, spatial diversity between speakers and background noise in addition to spectro-temporal diversity can be exploited by using multi-channel filters for sound source separation. Aiming at end-to-end multi-channel source separation, in this paper we propose a transformer-recurrent-U network (TRUNet), which directly estimates multi-channel filters from multi-channel input spectra. TRUNet consists of a spatial processing network with an attention mechanism across microphone channels aiming at capturing the spatial diversity, and a spectro-temporal processing network aiming at capturing spectral and temporal diversities. In addition to multi-channel filters, we also consider estimating single-channel filters from multi-channel input spectra using TRUNet. We train the network on a large reverberant dataset using a proposed combined compressed mean-squared error loss function, which further improves the sound separation performance. We evaluate the network on a realistic and challenging reverberant dataset, generated from measured room impulse responses of an actual microphone array. The experimental results on realistic reverberant sound source separation show that the proposed TRUNet outperforms state-of-the-art single-channel and multi-channel source separation methods",
    "checked": true,
    "id": "706997d3a838fcac0ebfc52494a853c97656ccf7",
    "semantic_title": "trunet: transformer-recurrent-u network for multi-channel reverberant sound source separation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ge22_interspeech.html": {
    "title": "PercepNet+: A Phase and SNR Aware PercepNet for Real-Time Speech Enhancement",
    "volume": "main",
    "abstract": "PercepNet, a recent extension of the RNNoise, an efficient, high-quality and real-time full-band speech enhancement technique, has shown promising performance in various public deep noise suppression tasks. This paper proposes a new approach, named PercepNet+, to further extend the PercepNet with four significant improvements. First, we introduce a phase-aware structure to leverage the phase information into PercepNet, by adding the complex features and complex subband gains as the deep network input and output respectively. Then, a signal-to-noise ratio (SNR) estimator and an SNR-switched post-processing are specially designed to alleviate the over attenuation (OA) that appears in high SNR conditions of the original PercepNet. Moreover, the GRU layer is replaced by TF-GRU to model both temporal and frequency dependencies. Finally, we propose to integrate the loss of complex subband gain, SNR, pitch filtering strength, and an OA loss in a multi-objective learning manner to further improve the speech enhancement performance. Experimental results show that, the proposed PercepNet+ outperforms the original PercepNet significantly in terms of both PESQ and STOI, without increasing the model size too much",
    "checked": true,
    "id": "23c915ab3fc350b03a167a49b782e1c275adf473",
    "semantic_title": "percepnet+: a phase and snr aware percepnet for real-time speech enhancement",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22c_interspeech.html": {
    "title": "Lightweight Full-band and Sub-band Fusion Network for Real Time Speech Enhancement",
    "volume": "main",
    "abstract": "Recent studies in deep learning based real-time speech enhancement have proven the advantage of sub-band processing in parameter reduction. However, most sub-band based methods utilize the same model for all sub-bands, which limits the upper bound of performance, giving the fact that the spectral patterns in each sub-band are different. In this paper, we take into account this fact and propose a lightweight full-band and sub-band fusion network, where dual-branch based architecture is employed for modeling local and global spectral pattern simultaneously. A simple yet effective sub-band module, the weighted progressive convolutional module, is designed with a small number of parameters, which captures clean features progressively from local perspective. Each sub-band is handled by one module. A novel asymmetric convolutional recurrent network is also proposed to focus on full-band context and extract more robust global features, which is complementary to the sub-band module. We have conducted extensive experiments on both the VoiceBank+Demand and the DNS Challenge datasets, and the experimental results show that our proposed method has achieved superior performance to other state-of-the-art approaches with smaller model size and lower latency",
    "checked": true,
    "id": "4b46ee4cff1ed0ed1bb7f2fd87e8136f63c61d29",
    "semantic_title": "lightweight full-band and sub-band fusion network for real time speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cheng22_interspeech.html": {
    "title": "Cross-Layer Similarity Knowledge Distillation for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement (SE) algorithms based on deep neural networks (DNNs) often encounter challenges of limited hardware resources or strict latency requirements when deployed in real-world scenarios. However, a strong enhancement effect typically requires a large DNN. In this paper, a knowledge distillation framework for SE is proposed to compress the DNN model. We study the strategy of cross-layer connection paths, which fuses multi-level information from the teacher and transfers it to the student. To adapt to the SE task, we propose a frame-level similarity distillation loss. We apply this method to the deep complex convolution recurrent network (DCCRN) and make targeted adjustments. Experimental results show that the proposed method considerably improves the enhancement effect of the compressed DNN and outperforms other distillation methods",
    "checked": true,
    "id": "df7ed73ebde8c3b08b4cf8348b70aae3ec75187f",
    "semantic_title": "cross-layer similarity knowledge distillation for speech enhancement",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xiong22_interspeech.html": {
    "title": "Spectro-Temporal SubNet for Real-Time Monaural Speech Denoising and Dereverberation",
    "volume": "main",
    "abstract": "This paper presents an improved subband neural network applied to joint speech denoising and dereverberation for online single-channel scenarios. Preserving the advantages of subband model (SubNet) that processes each frequency band independently and requires small amount of resources for good generalization, the proposed framework named STSubNet exploits sufficient spectro-temporal receptive fields (STRFs) from speech spectrum via a two-dimensional convolution network cooperating with a bi-directional long short-term memory network across frequency bands, to further improve the neural network discrimination between desired speech component and undesired interference including noise and reverberation. The importance of this STRF extractor is analyzed by evaluating the contribution of individual module to the STSubNet performance for simultaneously denoising and dereverberation. Experimental results show that STSubNet outperforms other subband variants and achieves competitive performance compared to state-of-the-art models on two publicly benchmark test sets",
    "checked": true,
    "id": "2a38c2db630b7e2dd4e7617c8752e5867078afa3",
    "semantic_title": "spectro-temporal subnet for real-time monaural speech denoising and dereverberation",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cao22_interspeech.html": {
    "title": "CMGAN: Conformer-based Metric GAN for Speech Enhancement",
    "volume": "main",
    "abstract": "Recently, convolution-augmented transformer (Conformer) has achieved promising performance in automatic speech recognition (ASR) and time-domain speech enhancement (SE), as it can capture both local and global dependencies in the speech signal. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for SE in the time-frequency (TF) domain. In the generator, we utilize two-stage conformer blocks to aggregate all magnitude and complex spectrogram information by modeling both time and frequency dependencies. The estimation of magnitude and complex spectrogram is decoupled in the decoder stage and then jointly incorporated to reconstruct the enhanced speech. In addition, a metric discriminator is employed to further improve the quality of the enhanced estimated speech by optimizing the generator with respect to a corresponding evaluation score. Quantitative analysis on Voice Bank+DEMAND dataset indicates the capability of CMGAN in outperforming various previous models with a margin, i.e., PESQ of 3.41 and SSNR of 11.10 dB",
    "checked": true,
    "id": "2e1909c8e9f95bd188e3452b2c3eb762633918a0",
    "semantic_title": "cmgan: conformer-based metric gan for speech enhancement",
    "citation_count": 21
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22_interspeech.html": {
    "title": "Model Compression by Iterative Pruning with Knowledge Distillation and Its Application to Speech Enhancement",
    "volume": "main",
    "abstract": "Over the past decade, deep learning has demonstrated its effectiveness and keeps setting new records in a wide variety of tasks. However, good model performance usually leads to a huge amount of parameters and extremely high computational complexity which greatly limit the use cases of deep learning models, particularly in embedded systems. Therefore, model compression is getting more and more attention. In this paper, we propose a compression strategy based on iterative pruning and knowledge distillation. Specifically, in each iteration, we first utilize a pruning criterion to drop the weights which have less impact on performance. Then, the model before pruning is used as a teacher to fine-tune the student which is the model after pruning. After several iterations, we get the final compressed model. The proposed method is verified on gated convolutional recurrent network (GCRN) and long short-term memory (LSTM) for single-channel speech enhancement task. Experimental results show that the proposed compression strategy can dramatically reduce the model size by 40x without significant performance degradation for GCRN",
    "checked": true,
    "id": "f73f68a7b9d01880c21aff8ae1f3a0be2713c178",
    "semantic_title": "model compression by iterative pruning with knowledge distillation and its application to speech enhancement",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22l_interspeech.html": {
    "title": "Single-channel speech enhancement using Graph Fourier Transform",
    "volume": "main",
    "abstract": "This paper presents combination of Graph Fourier Trans- form (GFT) and U-net, proposes a deep neural network (DNN) named G-Unet for single channel speech enhancement. GFT is carried out over speech data for creating inputs of U-net. The GFT outputs are combined with the mask estimated by U- net in time-graph (T-G) domain to reconstruct enhanced speech in time domain by Inverse GFT. The G-Unet outperforms the combination of Short time Fourier Transform (STFT) and mag- nitude estimation U-net in improving speech quality and de- reverberation, and outperforms the combination of STFT and complex U-net in improving speech quality in some cases, which is validated by testing on LibriSpeech and NOISEX92 dataset",
    "checked": true,
    "id": "189b625953f328f925802fc8278c628ab896955d",
    "semantic_title": "single-channel speech enhancement using graph fourier transform",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22c_interspeech.html": {
    "title": "Joint Optimization of the Module and Sign of the Spectral Real Part Based on CRN for Speech Denoising",
    "volume": "main",
    "abstract": "Recently some novel techniques have utilized sophisticated algorithms to correct phase or use phase information by processing real- and image-part respectively or simultaneously in the STFT domain. However, neural networks can not process a complex-valued feature, i.e., the STFT of a noisy speech. Therefore, these methods estimating the STFT of a clean signal can only obtain sub-optimal solutions. To avoid tackling complex-value operations, we formulate that only the real part of 2K-point STFT is utilized as the feature that holds all signal information. Therefore, speech enhancement in the STFT domain turns into a real-valued task. However, it is hard for the network to estimate the correct sign. Consequently, we develop an estimator to predict the real part sign and a decoder to estimate the targeted real part's mask. Then, we devise some experiments to evaluate our model over kinds of metrics. The results indicate that our model outperforms several state-of-the-art (SOTA) models",
    "checked": true,
    "id": "bab3480a9112373178e316214bb7bb994f4b489f",
    "semantic_title": "joint optimization of the module and sign of the spectral real part based on crn for speech denoising",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22m_interspeech.html": {
    "title": "Attentive Recurrent Network for Low-Latency Active Noise Control",
    "volume": "main",
    "abstract": "Processing latency is a critical issue for active noise control (ANC) due to the causality constraint of ANC systems. This paper addresses low-latency ANC in the deep learning framework (i.e. deep ANC). A time-domain method using an attentive recurrent network is employed to perform deep ANC with smaller frame sizes, thus reducing algorithmic latency of deep ANC. In addition, a delay-compensated training strategy is introduced to perform ANC using predicted noise for several milliseconds. Moreover, we utilize a revised overlap-add method during signal resynthesis to avoid the latency introduced due to overlaps between neighboring time frames. Experimental results show that the proposed strategies are effective for achieving low-latency deep ANC. Combining the proposed strategies is capable of yielding zero, even negative, algorithmic latency without significantly affecting ANC performance",
    "checked": true,
    "id": "95c3fdf1c796725335f28a6bb52e8f270a928a1f",
    "semantic_title": "attentive recurrent network for low-latency active noise control",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22e_interspeech.html": {
    "title": "Memory-Efficient Multi-Step Speech Enhancement with Neural ODE",
    "volume": "main",
    "abstract": "Although deep learning-based models proposed in the past years have achieved remarkable results on the speech enhancement tasks, the existing multi-step denoising methods require a memory size proportional to the number of steps during training, which makes it difficult to apply to large models. In this paper, we propose a memory-efficient multi-step speech enhancement method that requires only constant amount of memory for model training. This End-to-End method combines Neural Ordinary Differential Equations (Neural ODEs) with the Memory-efficient Asynchronous Leapfrog Integrator (MALI) for multi-step training. Experiments on the Voice Bank and DEMAND datasets showed that the multi-step method using MALI had better performance than the single-step method, with maximum improvements of 0.16 on PESQ and 0.5% on STOI. In addition to reducing the memory required for model training, this method is also quite competitive with the current state-of-the-art methods",
    "checked": true,
    "id": "7a5c8096b4aa8fc0af3a50ecbeef077c2cf0951c",
    "semantic_title": "memory-efficient multi-step speech enhancement with neural ode",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22f_interspeech.html": {
    "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
    "volume": "main",
    "abstract": "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as Global-Local Dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We have compared the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results have shown that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI",
    "checked": true,
    "id": "a73b0cfad449a22de5b04718696cabf0ac457b9f",
    "semantic_title": "gld-net: improving monaural speech enhancement by learning global and local dependency features with gld block",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22g_interspeech.html": {
    "title": "Improving Visual Speech Enhancement Network by Learning Audio-visual Affinity with Multi-head Attention",
    "volume": "main",
    "abstract": "Audio-visual speech enhancement system is regarded as one of the promising solutions for isolating and enhancing the speech of the desired speaker. Typical methods focus on predicting clean speech spectrum via a naive convolution neural network-based encoder-decoder architecture, and these methods a) are not adequate to use data fully, b) are unable to effectively balance audio-visual features. The proposed model alleviates these drawbacks by a) applying a model that fuses audio and visual features layer by layer in the encoding phase, that feeds fused audio-visual features to each corresponding decoder layer, and more importantly, b) introducing 2-stage multi-head cross attention (MHCA) mechanism to infer audio-visual speech enhancement for balancing the fused audio-visual features and eliminating irrelevant features. This paper proposes an attentional audio-visual multi-layer feature fusion model, in which MHCA units are applied to feature mapping at every layer of the decoder. The proposed model demonstrates the superior performance of the network against the state-of-the-art models. Speech samples are available at: https://XinmengXu.github.io/AVSE/AVCRN.html",
    "checked": true,
    "id": "a6102cc9ccc0007bd0b33826cee00e3da60ec90d",
    "semantic_title": "improving visual speech enhancement network by learning audio-visual affinity with multi-head attention",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22k_interspeech.html": {
    "title": "Speech Enhancement with Fullband-Subband Cross-Attention Network",
    "volume": "main",
    "abstract": "FullSubNet has been shown its promising performance on speech enhancement by utilizing both fullband and subband information. However, the relationship between fullband and subband in FullSubNet is achieved by simply concatenating the output of fullband model and subband units. It only supplements the subband units with a small quantity of global information and has not considered the interaction between fullband and subband. This paper proposes a fullband-subband cross-attention (FSCA) module to interactively fuse the global and local information and applies to FullSubNet. This new framework is called as FS-CANet. Moreover, different from FullSubNet, the proposed FS-CANet optimize the fullband extractor by temporal convolutional network (TCN) blocks to further reduce the model size. Experimental results on DNS Challenge - Interspeech 2021 dataset show that the proposed FS-CANet outperforms other state-of-the-art speech enhancement approaches, and demonstrate the effectiveness of fullband-subband cross-attention",
    "checked": true,
    "id": "c71ae6e93046afe197f18e9e2449d1f359f68d42",
    "semantic_title": "speech enhancement with fullband-subband cross-attention network",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yu22_interspeech.html": {
    "title": "OSSEM: one-shot speaker adaptive speech enhancement using meta learning",
    "volume": "main",
    "abstract": "Although deep learning (DL) has achieved notable progress in speech enhancement (SE), further research is still required for a DL-based SE system to adapt effectively and efficiently to particular speakers. In this study, we propose a novel meta-learning-based speaker-adaptive SE approach (called OSSEM) that aims to achieve SE model adaptation in a one-shot manner. OOSSEM consists of a modified transformer SE network and a speaker specific masking (SSM) network. In practice, the SSM network uses enrolled speaker embeddings extracted using ECAPA-TDNN to adjust input features through masking. To evaluate OSSEM, we design a modified Voice Bank-DEMAND dataset containing the first noisy utterances from speakers in the test set for model adaptation and the remaining utterances for testing performance. Furthermore, we set the constraints to be able to perform the SE process in real time, thereby designing OSSEM as a causal SE system. The experimental results first show that OSSEM can effectively adapt the SE model to a specific speaker using only one of his/her noisy utterances, thereby improving SE results. Meanwhile, OSSEM exhibits competitive performance compared to state-of-the-art causal SE systems",
    "checked": true,
    "id": "44316f44b9134971790580fc043728f604c3ab48",
    "semantic_title": "ossem: one-shot speaker adaptive speech enhancement using meta learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jiang22b_interspeech.html": {
    "title": "Efficient Speech Enhancement with Neural Homomorphic Synthesis",
    "volume": "main",
    "abstract": "Most of the existing deep neural network based speech enhancement methods usually operate on short-time Fourier transform domain or alternatively learned features without employing the speech production model. In this work, we present an efficient speech enhancement algorithm using the speech source-filter model. Concretely, we separate the framed speech into excitation and vocal tract components by homomorphic filtering, adopt two convolutional recurrent networks for estimating the reference magnitude of the separated components, and synthesize the minimum phase signal with the estimated components. Lastly, the enhanced speech is obtained by a post-processing procedure, including using the noisy phase and overlap-addition. Experimental results demonstrated that the proposed method yields a comparable performance with the state-of-the-art complex-valued neural network based method. In addition, we conducted extensive experiments and found that the proposed method is more efficient with a compact model",
    "checked": true,
    "id": "5a325e3ee30d561f837283898bc1704ebcb9f7d9",
    "semantic_title": "efficient speech enhancement with neural homomorphic synthesis",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/thakker22_interspeech.html": {
    "title": "Fast Real-time Personalized Speech Enhancement: End-to-End Enhancement Network (E3Net) and Knowledge Distillation",
    "volume": "main",
    "abstract": "This paper investigates how to improve the runtime speed of personalized speech enhancement (PSE) networks while maintaining the model quality. Our approach includes two aspects: architecture and knowledge distillation (KD). We propose an end-to-end enhancement (E3Net) model architecture, which is 3 faster than a baseline STFT-based model. Besides, we use KD techniques to develop compressed student models without significantly degrading quality. In addition, we investigate using noisy data without reference clean signals for training the student models, where we combine KD with multi-task learning (MTL) using automatic speech recognition (ASR) loss. Our results show that E3Net provides better speech and transcription quality with a lower target speaker over-suppression (TSOS) rate than the baseline model. Furthermore, we show that the KD methods can yield student models that are 2  4 faster than the teacher and provides reasonable quality. Combining KD and MTL improves the ASR and TSOS metrics without degrading the speech quality",
    "checked": true,
    "id": "5dec72021d7c3a392cdff182b8d29918d02e4167",
    "semantic_title": "fast real-time personalized speech enhancement: end-to-end enhancement network (e3net) and knowledge distillation",
    "citation_count": 14
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sato22b_interspeech.html": {
    "title": "Strategies to Improve Robustness of Target Speech Extraction to Enrollment Variations",
    "volume": "main",
    "abstract": "Target speech extraction is a technique to extract the target speaker's voice from mixture signals using a pre-recorded enrollment utterance that characterize the voice characteristics of the target speaker. One major difficulty of target speech extraction lies in handling variability in ``intra-speaker'' characteristics, i.e., characteristics mismatch between target speech and an enrollment utterance. While most conventional approaches focus on improving average performance given a set of enrollment utterances, here we propose to guarantee the worst performance, which we believe is of great practical importance. In this work, we propose an evaluation metric called worst-enrollment source-to-distortion ratio (SDR) to quantitatively measure the robustness towards enrollment variations. We also introduce a novel training scheme that aims at directly optimizing the worst-case performance by focusing on training with difficult enrollment cases where extraction does not perform well. In addition, we investigate the effectiveness of auxiliary speaker identification loss (SI-loss) as another way to improve robustness over enrollments. Experimental validation reveals the effectiveness of both worst-enrollment target training and SI-loss training to improve robustness against enrollment variations, by increasing speaker discriminability",
    "checked": true,
    "id": "102b13be875f48a429ac3fa328740d03e2ace763",
    "semantic_title": "strategies to improve robustness of target speech extraction to enrollment variations",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mehmood22_interspeech.html": {
    "title": "FedNST: Federated Noisy Student Training for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Federated Learning (FL) enables training state-of-the-art Automatic Speech Recognition (ASR) models on user devices (clients) in distributed systems, hence preventing transmission of raw user data to a central server. A key challenge facing practical adoption of FL for ASR is obtaining ground-truth labels on the clients. Existing approaches rely on clients to manually transcribe their speech, which is impractical for obtaining large training corpora. A promising alternative is using semi-/self-supervised learning approaches to leverage unlabelled user data. To this end, we propose FedNST, a novel method for training distributed ASR models using private and unlabelled user data. We explore various facets of FedNST, such as training models with different proportions of labelled and unlabelled data, and evaluate the proposed approach on 1173 simulated clients. Evaluating FedNST on LibriSpeech, where 960 hours of speech data is split equally into server (labelled) and client (unlabelled) data, showed a 22.5\\% relative word error rate reduction (WERR) over a supervised baseline trained only on server data",
    "checked": true,
    "id": "9c46ff9ca2c1739369b8bacb625ed528a688329c",
    "semantic_title": "fednst: federated noisy student training for automatic speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fu22_interspeech.html": {
    "title": "SCaLa: Supervised Contrastive Learning for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "End-to-end Automatic Speech Recognition (ASR) models are usually trained to optimize the loss of the whole token sequence, while neglecting explicit phonemic-granularity supervision. This could result in recognition errors due to similar-phoneme confusion or phoneme reduction. To alleviate this problem, we propose a novel framework based on Supervised Contrastive Learning (SCaLa) to enhance phonemic representation learning for end-to-end ASR systems. Specifically, we extend the self-supervised Masked Contrastive Predictive Coding (MCPC) to a fully-supervised setting, where the supervision is applied in the following way. First, SCaLa masks variable-length encoder features according to phoneme boundaries given phoneme forced-alignment extracted from a pre-trained acoustic model; it then predicts the masked features via contrastive learning. The forced-alignment can provide phoneme labels to mitigate the noise introduced by positive-negative pairs in self-supervised MCPC. Experiments on reading and spontaneous speech datasets show that our proposed approach achieves 2.8 and 1.4 points Character Error Rate (CER) absolute reductions compared to the baseline, respectively",
    "checked": true,
    "id": "b21be836d5082c8be60dd12d6de8953b66c148ab",
    "semantic_title": "scala: supervised contrastive learning for end-to-end speech recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22k_interspeech.html": {
    "title": "NAS-SCAE: Searching Compact Attention-based Encoders For End-to-end Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Recently plenty of attention-based encoders have been proposed for end-to-end (E2E) automatic speech recognition (ASR). Despite the impressive performance, these encoders usually have a large model size and suffer from expensive memory and computation costs. To obtain more compact encoders for E2E ASR, we propose searching compact attention-based encoders using neural architecture search (NAS) in this paper, named NAS-SCAE. NAS-SCAE consists of one search space that contains a set of candidate encoders and one search algorithm responsible for searching the optimal encoder from the search space. On one hand, NAS-SCAE designs a topology-fused search space to integrate different architecture topologies of existing encoders (e.g. Transformer, Conformer) and explore more brand-new architectures. On the other hand, combined with the training pipeline of E2E ASR, NAS-SCAE develops a resource-aware differentiable search algorithm to search compact encoders efficiently and proposes an adjustable search scheme to alleviate the joint optimization problem of the differentiable search algorithm. On four Mandarin and English datasets, NAS-SCAE can effectively reduce the encoder resource consumption with negligible performance drop and achieve at least 2.13x/2.09x parameters/FLOPs reduction than the human-designed baselines",
    "checked": true,
    "id": "b7f2789f44006fe1a6ffce98c117fb1bab9038b7",
    "semantic_title": "nas-scae: searching compact attention-based encoders for end-to-end automatic speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22e_interspeech.html": {
    "title": "Leveraging Acoustic Contextual Representation by Audio-textual Cross-modal Learning for Conversational ASR",
    "volume": "main",
    "abstract": "Leveraging context information is an intuitive idea to improve performance on conversational automatic speech recognition (ASR). Previous works usually adopt recognized hypotheses of historical utterances as preceding context, which may bias the current recognized hypothesis due to the inevitable historical recognition errors. To avoid this problem, we propose an audio-textual cross-modal representation extractor to learn contextual representations directly from preceding speech. Specifically, it consists of two modal-related encoders, extracting high-level latent features from speech or text, and a cross-modal encoder, which aims to learn the correlation between speech and text. For each modal-related encoder, we randomly mask some tokens of its input or the whole input sequence, then we perform a token-missing or modal-missing prediction and a modal-level CTC loss on cross-modal encoder. Thus, the model captures not only the bi-directional context dependencies in a specific modality but also relationships between different modalities. Then, the extractor will be frozen to extract the textual representations of preceding speech during the training of the conversational ASR system through attention mechanism. The effectiveness of the proposed approach is validated on several Mandarin conversation corpora and the highest character error rate (CER) reduction up to 16% is achieved on the MagicData dataset",
    "checked": true,
    "id": "d5ef0a424c30a0f1e98e25564643a95e4a9f964f",
    "semantic_title": "leveraging acoustic contextual representation by audio-textual cross-modal learning for conversational asr",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ma22_interspeech.html": {
    "title": "PM-MMUT: Boosted Phone-mask Data Augmentation using Multi-Modeling Unit Training for Phonetic-Reduction-Robust E2E Speech Recognition",
    "volume": "main",
    "abstract": "Consonant and vowel reduction are often encountered in speech, which might cause performance degradation in automatic speech recognition (ASR). Our recently proposed learning strategy based on masking, Phone Masking Training (PMT), alleviates the impact of such phenomenon in Uyghur ASR. Although PMT achieves remarkably improvements, there still exists room for further gains due to the granularity mismatch between the masking unit of PMT (phoneme) and the modeling unit (word-piece). To boost the performance of PMT, we propose multi-modeling unit training (MMUT) architecture fusion with PMT (PM-MMUT). The idea of MMUT framework is to split the Encoder into two parts including acoustic feature sequences to phoneme-level representation (AF-to-PLR) and phoneme-level representation to word-piece-level representation (PLR-to-WPLR). It allows AF-to-PLR to be optimized by an intermediate phoneme-based CTC loss to learn the rich phoneme-level context information brought by PMT. Experimental results on Uyghur ASR show that the proposed approaches outperform obviously the pure PMT. We also conduct experiments on the 960-hour Librispeech benchmark using ESPnet1, which achieves about 10% relative WER reduction on all the test set without LM fusion comparing with the latest official ESPnet1 pre-trained model",
    "checked": true,
    "id": "3bdca63c01a9006fc8cf55ec15c4b487e83e9540",
    "semantic_title": "pm-mmut: boosted phone-mask data augmentation using multi-modeling unit training for phonetic-reduction-robust e2e speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/audhkhasi22_interspeech.html": {
    "title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Attention layers are an integral part of modern end-to-end automatic speech recognition systems, for instance as part of the Transformer or Conformer architecture. Attention is typically multi-headed, where each head has an independent set of learned parameters and operates on the same input feature sequence. The output of multi-headed attention is a fusion of the outputs from the individual heads. We empirically analyze the diversity between representations produced by the different attention heads and demonstrate that the heads become highly correlated during the course of training. We investigate a few approaches to increasing attention head diversity, including using different attention mechanisms for each head and auxiliary training loss functions to promote head diversity. We show that introducing diversity-promoting auxiliary loss functions during training is a more effective approach, and obtain WER improvements of up to 6% relative on the Librispeech corpus. Finally, we draw a connection between the diversity of attention heads and the similarity of the gradients of head parameters",
    "checked": true,
    "id": "5c74671d756ef2d742b15fdca97fdc68d6cff691",
    "semantic_title": "analysis of self-attention head diversity for conformer-based automatic speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weiran22_interspeech.html": {
    "title": "Improving Rare Word Recognition with LM-aware MWER Training",
    "volume": "main",
    "abstract": "Language models (LMs) significantly improve the recognition accuracy of end-to-end (E2E) models on words rarely seen during training, when used in either the shallow fusion or the rescoring setups. In this work, we introduce LMs in the learning of hybrid autoregressive transducer (HAT) models in the discriminative training framework, to mitigate the training versus inference gap regarding the use of LMs. For the shallow fusion setup, we use LMs during both hypotheses generation and loss computation, and the LM-aware MWER-trained model achieves 10\\% relative improvement over the model trained with standard MWER on voice search test sets containing rare words. For the rescoring setup, we learn a small neural module to generate per-token fusion weights in a data-dependent manner. This model achieves the same rescoring WER as regular MWER-trained model, but without the need for sweeping fusion weights",
    "checked": true,
    "id": "4e01ecf22f3a585252d6e827019120a735c6ab26",
    "semantic_title": "improving rare word recognition with lm-aware mwer training",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zeineldeen22_interspeech.html": {
    "title": "Improving the Training Recipe for a Robust Conformer-based Hybrid Model",
    "volume": "main",
    "abstract": "Speaker adaptation is important to build robust automatic speech recognition (ASR) systems. In this work, we investigate various methods for speaker adaptive training (SAT) based on feature-space approaches for a conformer-based acoustic model (AM) on the Switchboard 300h dataset. We propose a method, called Weighted-Simple-Add, which adds weighted speaker information vectors to the input of the multi-head self-attention module of the conformer AM. Using this method for SAT, we achieve 3.5% and 4.5% relative improvement in terms of WER on the CallHome part of Hub5'00 and Hub5'01 respectively. Moreover, we build on top of our previous work where we proposed a novel and competitive training recipe for a conformer-based hybrid AM. We extend and improve this recipe where we achieve 11% relative improvement in terms of word-error-rate (WER) on Switchboard 300h Hub5'00 dataset. We also make this recipe efficient by reducing the total number of parameters by 34% relative",
    "checked": true,
    "id": "446cc1b387d5fcee0171768d688b637ffffbcead",
    "semantic_title": "improving the training recipe for a robust conformer-based hybrid model",
    "citation_count": 10
  },
  "https://www.isca-speech.org/archive/interspeech_2022/laptev22_interspeech.html": {
    "title": "CTC Variations Through New WFST Topologies",
    "volume": "main",
    "abstract": "This paper presents novel Weighted Finite-State Transducer (WFST) topologies to implement Connectionist Temporal Classification (CTC)-like algorithms for automatic speech recognition. Three new CTC variants are proposed: (1) the \"compact- CTC\", in which direct transitions between units are replaced with  back-off transitions; (2) the \"minimal-CTC\", that only adds blank self-loops when used in WFST-composition; and (3) the \"selfless-CTC\" variants, which disallows self-loop for non-blank units. Compact-CTC allows for 1.5 times smaller WFST decoding graphs and reduces memory consumption by two times when training CTC models with the LF-MMI objective without hurting the recognition accuracy. Minimal-CTC reduces graph size and memory consumption by two and four times for the cost of a small accuracy drop. Using selfless-CTC can improve the accuracy for wide context window models",
    "checked": true,
    "id": "aba1b8b76ae1d9c30d0a566e811b27157671905b",
    "semantic_title": "ctc variations through new wfst topologies",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sustek22_interspeech.html": {
    "title": "Dealing with Unknowns in Continual Learning for End-to-end Automatic Speech Recognition",
    "volume": "main",
    "abstract": "Learning continually from data is a task executed effortlessly by humans but remains to be of significant challenge for machines. Moreover, when encountering unknown test scenarios machines fail to generalize. We propose a mathematically motivated dynamically expanding end-to-end model of independent sequence-to-sequence components trained on different data sets that avoid catastrophically forgetting knowledge acquired from previously seen data while seamlessly integrating knowledge from new data. During inference, the likelihoods of the unknown test scenario are computed using internal model activation distributions. The inference made by each independent component is weighted by the normalized likelihood values to obtain the final decision",
    "checked": true,
    "id": "dea2103e2b666413670b3f5c81a2e3ca318ea2d4",
    "semantic_title": "dealing with unknowns in continual learning for end-to-end automatic speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/miao22c_interspeech.html": {
    "title": "Towards Efficiently Learning Monotonic Alignments for Attention-based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "Inspired by EfficientTTS, a recent proposed speech synthesis model, we propose a new way to train end-to-end speech recognition models with an additional training objective, allowing the models to learn the monotonic alignments effectively and efficiently. The introduced training objective is differential, computationally cheap and most importantly, of no constraint on network structures. Thus, it is quite convenient to be incorporated into any speech recognition model. Through extensive experiments, we observed that the performance of our models significantly outperform baseline models. Specifically, our best performing model achieves WER (Word Error Rate) 3.18% on LibriSpeech test-clean benchmark and 8.41% on test-other. Comparing with a strong baseline obtained by WeNet, the proposed model gets 7.6% relative WER reduction on test-clean and 6.9% on test-other",
    "checked": true,
    "id": "41cc3662ba675e72121885a609e261732ee4c3c9",
    "semantic_title": "towards efficiently learning monotonic alignments for attention-based end-to-end speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22fa_interspeech.html": {
    "title": "On monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training",
    "volume": "main",
    "abstract": "In this paper, we explore an improved framework to train a monoaural neural enhancement model for robust speech recognition. The designed training framework extends the existing mixture invariant training criterion to exploit both unpaired clean speech and real noisy data. It is found that the unpaired clean speech is crucial to improve quality of separated speech from real noisy speech. The proposed method also performs remixing of processed and unprocessed signals to alleviate the processing artifacts. Experiments on the single-channel CHiME-3 real test sets show that the proposed method improves significantly in terms of speech recognition performance over the enhancement system trained either on the mismatched simulated data in a supervised fashion or on the matched real data in an unsupervised fashion. Between 16% and 39% relative WER reduction has been achieved by the proposed system compared to the unprocessed signal using end-to-end and hybrid acoustic models without retraining on distorted data",
    "checked": true,
    "id": "e4456b542db62c4a8f28d9740dbe6780cd911a1f",
    "semantic_title": "on monoaural speech enhancement for automatic recognition of real noisy speech using mixture invariant training",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/handekabil22_interspeech.html": {
    "title": "From Undercomplete to Sparse Overcomplete Autoencoders to Improve LF-MMI based Speech Recognition",
    "volume": "main",
    "abstract": "Starting from a strong Lattice-Free Maximum Mutual Information (LF-MMI) baseline system, we explore different autoencoder configurations to enhance Mel-Frequency Cepstral Coefficients (MFCC) features. Autoencoders are expected to generate new MFCC features that can be used in our LF-MMI based baseline system (with or without retraining) towards speech recognition improvements. Starting from shallow undercomplete autoencoders, and their known equivalence with Principal Component Analysis (PCA), we go to deeper or sparser architectures. In the spirit of kernel-based learning methods, we explore alternatives where the autoencoder first goes overcomplete (i.e., expand the representation space) in a nonlinear way, and then we restrict the autoencoder by means of a sequent bottleneck layer. Finally, as a third solution, we use sparse overcomplete autoencoders where a sparsity constraint is imposed on the higher-dimensional encoding layer. Experimental results are provided on the Augmented Multiparty Interaction (AMI) dataset, where we show that all aforementioned architectures improve speech recognition performance",
    "checked": true,
    "id": "e8f2d42d615257485c50b67f2ac15d256453d481",
    "semantic_title": "from undercomplete to sparse overcomplete autoencoders to improve lf-mmi based speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tanaka22_interspeech.html": {
    "title": "Domain Adversarial Self-Supervised Speech Representation Learning for Improving Unknown Domain Downstream Tasks",
    "volume": "main",
    "abstract": "In this paper, we propose novel self-supervised speech representation learning method that obtains domain invariant representations by using a domain adversarial neural network. Recently, self-supervised representation learning has been actively studied in the speech field. Since self-supervised learning requires large-scale unlabeled data, we need to effectively use data collected from a variety of domains. However, existing methods cannot construct valid representations in unknown domains because they cause overfitting to the domains in the training data. To solve this problem, our proposed method constructs contextual representations that cannot identify the domains from input speech by using domain adversarial neural networks. The domain adversarial training can improve robustness for data in unknown domains because the model trained by our proposed method can construct domain invariant representations. In addition, we investigate multi-task learning of representation construction and domain classification to consider domain information. Experimental results show that our proposed method outperforms the conventional training method of wav2vec 2.0 in unknown domain downstream automatic speech recognition tasks",
    "checked": true,
    "id": "ea9beceb8f8e5c3b45d285006f4fb64008f40481",
    "semantic_title": "domain adversarial self-supervised speech representation learning for improving unknown domain downstream tasks",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/maekaku22_interspeech.html": {
    "title": "Attention Weight Smoothing Using Prior Distributions for Transformer-Based End-to-End ASR",
    "volume": "main",
    "abstract": "Transformer-based encoder-decoder models have so far been widely used for end-to-end automatic speech recognition. However, it has been found that the self-attention weight matrix could be too peaky and biased toward the diagonal component. Such attention weight matrix contains little useful context information, which may result in poor speech recognition performance. Therefore, we propose the following two attention weight smoothing methods based on the hypothesis that an attention weight matrix whose diagonal components are not peaky can capture more context information. One is a method to linearly interpolate the attention weight using a learnable truncated prior distribution. The other uses the attention weight from a previous layer as a prior distribution given that lower-layer weights tend to be less peaky and diagonal. Experiments on LibriSpeech and Wall Street Journal show that the proposed approach achieves 2.9% and 7.9% relative improvement, respectively, over a vanilla Transformer model",
    "checked": true,
    "id": "a9ec648bc620da6c7cb3a1ea24795ecc1b249415",
    "semantic_title": "attention weight smoothing using prior distributions for transformer-based end-to-end asr",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/uchida22_interspeech.html": {
    "title": "Reducing Offensive Replies in Open Domain Dialogue Systems",
    "volume": "main",
    "abstract": "In recent years, a series of open-domain dialogue systems using large-scale language models have been proposed. These dialogue systems are attracting business attention because these do significantly natural and diverse dialogues with humans. However, it has been noted that these dialogue systems reflect gender, race, and other biases inherent in the data and may generate offensive replies or replies that agree with offensive utterances. This study examined a dialogue system that outputs appropriate replies to offensive utterances. Specifically, our system incorporates multiple dialogue models, each of which is specialized to suppress offensive replies in a specific category, then selects the most non-offensive reply from the outputs of the models. We evaluated the utility of our system when suppressing offensive replies of DialoGPT. We confirmed ours reduces the offensive replies to less than 1%, whereas one of the state-of-the-art suppressing methods reduces to 9.8%",
    "checked": true,
    "id": "655a45678f2522d406c7606549bb3cf09683f385",
    "semantic_title": "reducing offensive replies in open domain dialogue systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22b_interspeech.html": {
    "title": "Induce Spoken Dialog Intents via Deep Unsupervised Context Contrastive Clustering",
    "volume": "main",
    "abstract": "Intent detection is one of most critical tasks in spoken language understanding. However, most systems could only identify a predefined set of intents, without covering a ubiquitous space of real-world semantics. Discovering new dialog intents with clustering to explore additional requests is crucial particularly in complex domains like customer support services. Leveraging the strong coherence between the user query utterance and their following contexts in the dialog, we present an effective intent induction approach with fine-tuning and clustering with contrastive learning. In particular, we first transform pretrained LMs into conversational encoders with in-domain dialogs. Then we conduct context-aware contrastive learning to reveal latent intent semantics via the coherence from dialog contexts. After obtaining the initial representations on both views of the query and their contexts, we propose a novel clustering method to iteratively refine the representation by minimizing semantic distances between pairs of utterances or contexts, under the same cluster assignment on the opposite view. The experimental results validate the robustness and versatility of our framework, which also achieves superior performances over competitive baselines without the label supervision",
    "checked": true,
    "id": "02f6ad535c5a3362314be2d8c3bd02aa144f8008",
    "semantic_title": "induce spoken dialog intents via deep unsupervised context contrastive clustering",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nihei22_interspeech.html": {
    "title": "Dialogue Acts Aided Important Utterance Detection Based on Multiparty and Multimodal Information",
    "volume": "main",
    "abstract": "It has been reported that visualization of important utterances in a meeting enables efficient understanding of the meeting. Therefore, creating a model to estimate important utterances and improving its performance is an important issue. Several studies have reported that introducing auxiliary tasks as estimation targets improves the estimation performance of the main task. In this study, we develop estimation models of important utterances using dialogue acts (DAs) as an auxiliary task. The MATRICS corpus of four-party face-to-face meetings was used as the analysis data. A transformer with historical information was used for the model to estimate important utterances, and three types of modal information (text, audio, and video) were used as input data. In addition, audio and video data were separated into information about the speaker and others. As a result, the best model for important utterances was the one that used the speaker's text and audio, as well as others' audio and video data, with the assistance of DAs, with an estimation performance of 0.809 in f-measure. The results also showed that the model performed better than the one that only estimates important utterances, indicating that the assistance of DAs is effective in the estimation of important utterances",
    "checked": true,
    "id": "97097fcac25f6e25b76417660bc3fa834860e22e",
    "semantic_title": "dialogue acts aided important utterance detection based on multiparty and multimodal information",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bekal22_interspeech.html": {
    "title": "Contextual Acoustic Barge-In Classification for Spoken Dialog Systems",
    "volume": "main",
    "abstract": "In this work, we define barge-in verification as a supervised learning task where audio-only information is used to classify user spoken dialogue into true and false barge-ins. Following the success of pre-trained models, we use low-level speech representations from a self-supervised representation learning model for our downstream classification task. Further, we propose a novel technique to infuse lexical information directly into speech representations to improve the domain-specific language information implicitly learned during pre-training. Experiments conducted on spoken dialog data show that our proposed model trained to validate barge-in entirely from speech representations is faster by 38% relative and achieves 4.5% relative F1 score improvement over a baseline LSTM model that uses both audio and Automatic Speech Recognition (ASR) 1-best hypotheses. On top of this, our best proposed model with lexically infused representations along with contextual features provides a fur- ther relative improvement of 5.7% in the F1 score but only 22% faster than the baseline",
    "checked": true,
    "id": "689a5de6a9f4bee7855c7d56db881879d6cd2a06",
    "semantic_title": "contextual acoustic barge-in classification for spoken dialog systems",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22b_interspeech.html": {
    "title": "Calibrate and Refine! A Novel and Agile Framework for ASR Error Robust Intent Detection",
    "volume": "main",
    "abstract": "The past ten years have witnessed the rapid development of text-based intent detection, whose benchmark performances have already been taken to a remarkable level by deep learning techniques. However, automatic speech recognition (ASR) errors are inevitable in real-world applications due to the environment noise, unique speech patterns and etc, leading to sharp performance drop in state-of-the-art text-based intent detection models. Essentially, this phenomenon is caused by the semantic drift brought by ASR errors and most existing works tend to focus on designing new model structures to reduce its impact, which is at the expense of versatility and flexibility. Different from previous one-piece model, in this paper, we propose a novel and agile framework called CR-ID for ASR error robust intent detection with two plug-and-play modules, namely semantic drift calibration module (SDCM) and phonemic refinement module (PRM), which are both model-agnostic and thus could be easily integrated to any existing intent detection models without mod-ifying their structures. Experimental results on SNIPS dataset show that, our proposed CR-ID framework achieves competi-tive performance and outperform all the baseline methods on ASR outputs, which verifies that CR-ID can effectively allevi-ate the semantic drift caused by ASR errors",
    "checked": false,
    "id": "6dbc5ed631b15606f2f62f837ba6b590b29b344e",
    "semantic_title": "calibrate and refine! a novel and agile framework for asr-error robust intent detection",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/feng22c_interspeech.html": {
    "title": "ASR-Robust Natural Language Understanding on ASR-GLUE dataset",
    "volume": "main",
    "abstract": "In recent years, with the increasing demand for voice interface applications, more and more attention has been paid to language understanding in speech systems. These speech-based intelligent systems usually comprise an automatic speech recognition (ASR) component and a natural language understanding (NLU) component which takes the output of the ASR component as input. Despite the rapid development of speech recognition over the past few decades, recognition errors are still inevitable, especially in noisy environments. However, the robustness of natural language understanding (NLU) systems to errors introduced by ASR is under-examined. In this paper, we propose three empirical approaches to improve the robustness of the NLU models. The first one is ASR correction which attempts to make error corrections for the mistranscriptions. The later two methods focus on simulating a noisy training scenario to train more robust NLU models. Extensive experimental results and analyses show that the proposed methods can effectively improve the robustness of NLU models",
    "checked": true,
    "id": "3323846a281fec14aa0be6e31d587f5551871a19",
    "semantic_title": "asr-robust natural language understanding on asr-glue dataset",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dao22_interspeech.html": {
    "title": "From Disfluency Detection to Intent Detection and Slot Filling",
    "volume": "main",
    "abstract": "We present the first empirical study investigating the influence of disfluency detection on downstream tasks of intent detection and slot filling. We perform this study for Vietnamese---a low-resource language that has no previous study as well as no public dataset available for disfluency detection. First, we extend the fluent Vietnamese intent detection and slot filling dataset PhoATIS by manually adding contextual disfluencies and annotating them. Then, we conduct experiments using strong baselines for disfluency detection and joint intent detection and slot filling, which are based on pre-trained language models. We find that: (i) disfluencies produce negative effects on the performances of the downstream intent detection and slot filling tasks, and (ii) in the disfluency context, the pre-trained multilingual language model XLM-R helps produce better intent detection and slot filling performances than the pre-trained monolingual language model PhoBERT, and this is opposite to what generally found in the fluency context",
    "checked": true,
    "id": "d56a8e779df0aaa35f38acb820730c1c5b26bc14",
    "semantic_title": "from disfluency detection to intent detection and slot filling",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22g_interspeech.html": {
    "title": "Audio-Visual Wake Word Spotting in MISP2021 Challenge: Dataset Release and Deep Analysis",
    "volume": "main",
    "abstract": "In this paper, we describe and release publicly the audio-visual wake word spotting (WWS) database in the MISP2021 Challenge, which covers a range of scenarios of audio and video data collected by near-, mid-, and far-field microphone arrays, and cameras, to create a shared and publicly available database for WWS. The database and the code are released, which will be a valuable addition to the community for promoting WWS research using multi-modality information in realistic and complex conditions. Moreover, we investigated the different data augmentation methods for single modalities on an end-to-end WWS network. A set of audio-visual fusion experiments and analysis were conducted to observe the assistance from visual information to acoustic information based on different audio and video field configurations. The results showed that the fusion system generally improves over the single-modality (audio- or video-only) system, especially under complex noisy conditions",
    "checked": true,
    "id": "203ffcadf17de29d4ce06be020f32eb0227a4825",
    "semantic_title": "audio-visual wake word spotting in misp2021 challenge: dataset release and deep analysis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sartzetaki22_interspeech.html": {
    "title": "Extending Compositional Attention Networks for Social Reasoning in Videos",
    "volume": "main",
    "abstract": "We propose a novel deep architecture for the task of reasoning about social interactions in videos. We leverage the multistep reasoning capabilities of Compositional Attention Networks (MAC) [1], and propose a multimodal extension (MAC-X). MAC-X is based on a recurrent cell that performs iterative mid-level fusion of input modalities (visual, auditory, text) over multiple reasoning steps, by use of a temporal attention mechanism. We then combine MAC-X with LSTMs for temporal input processing in an end-to-end architecture. Our ablation studies show that the proposed MAC-X architecture can effectively leverage multimodal input cues using mid-level fusion mechanisms. We apply MAC-X to the task of Social Video Question Answering in the Social IQ dataset and obtain a 2.5% absolute improvement in terms of binary accuracy over the current state-of-the-art. Index Terms: Video Question Answering, Social Reasoning, Compositional Attention Networks, MAC",
    "checked": true,
    "id": "d6cf303910dbf36e0ec2a48bc7294f0efd86cd31",
    "semantic_title": "extending compositional attention networks for social reasoning in videos",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22da_interspeech.html": {
    "title": "TopicKS: Topic-driven Knowledge Selection for Knowledge-grounded Dialogue Generation",
    "volume": "main",
    "abstract": "Knowledge-grounded dialogue generation is proposed to solve the problem of general or meaningless responses in traditional end-to-end dialogue generation methods. It generally includes two sub-modules: knowledge selection and knowledge-aware generation. Most studies consider the topic information for knowledge-aware generation, while ignoring it in knowledge selection. It may cause the topic mismatch between the overall dialogue and the selected knowledge, leading to the inconsistency of the generated response and the context. Therefore, in this study, we propose a Topic-driven Knowledge Selection method (TopicKS) to exploit topic information both in knowledge selection and knowledge-aware generation. Specifically, under the guidance of topic information, TopicKS selects more accurate candidate knowledge for the current turn of dialogue based on context information and historical knowledge information. Then the decoder uses the context information and selected knowledge to generate a higher-quality response under the guidance of topic information. Experiments on the notable benchmark corpus Wizard of Wikipedia (WoW) show that our proposed method not only achieves a significant improvement in terms of selection accuracy rate on knowledge selection, but also outperforms the baseline model in terms of the quality of the generated responses",
    "checked": true,
    "id": "92e1aaa0ed09668dd2cdb47ea27f4ded2e080cfd",
    "semantic_title": "topicks: topic-driven knowledge selection for knowledge-grounded dialogue generation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liesenfeld22_interspeech.html": {
    "title": "Bottom-up discovery of structure and variation in response tokens (backchannels') across diverse languages",
    "volume": "main",
    "abstract": "Response tokens (also known as backchannels, continuers, or feedback) are a frequent feature of human interaction, where they serve to display understanding and streamline turn-taking. We propose a bottom-up method to study responsive behaviour across 16 languages (8 language families). We use sequential context and recurrence of turns formats to identify candidate response tokens in a language-agnostic way across diverse conversational corpora. We then use UMAP clustering directly on speech signals to represent structure and variation. We find that (i) written orthographic annotations underrepresent the attested variation, (ii) distinctions between formats can be gradient rather than discrete, (iii) most languages appear to make available a broad distinction between a minimal nasal format mm' and a fuller yeah'-like format. Charting this aspect of human interaction contributes to our understanding of interactional infrastructure across languages and can inform the design of speech technologies",
    "checked": false,
    "id": "222a040d5dee537d72c693d876810de8727cec94",
    "semantic_title": "bottom-up discovery of structure and variation in response tokens ('backchannels') across diverse languages",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22f_interspeech.html": {
    "title": "Cross-modal Transfer Learning via Multi-grained Alignment for End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "End-to-end spoken language understanding (E2E-SLU) has witnessed impressive improvements through cross-modal (text-to-audio) transfer learning. However, current methods mostly focus on coarse-grained sequence-level text-to-audio knowledge transfer with simple loss, and neglecting the fine-grained temporal alignment between two modalities. In this work, we propose a novel multi-grained cross-modal transfer learning model for E2E-SLU. Specifically, we devise a cross attention module to align the tokens of text with the frame features of speech, encouraging the model to target at the salient acoustic features attended to each token during transferring the semantic information. We also leverage contrastive learning to facilitate cross-modal representation learning in sentence level. Finally, we explore various data augmentation methods to mitigate the deficiency of large amount of labelled data for the training of E2E-SLU. Extensive experiments are conducted on both English and Chinese SLU datasets to verify the effectiveness of our proposed approach. Experimental results and detailed analyses demonstrate the superiority and competitiveness of our model",
    "checked": true,
    "id": "3f13a25359de7c60a04fd99f08004ef2c07c7d9f",
    "semantic_title": "cross-modal transfer learning via multi-grained alignment for end-to-end spoken language understanding",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ochi22_interspeech.html": {
    "title": "Use of Nods Less Synchronized with Turn-Taking and Prosody During Conversations in Adults with Autism",
    "volume": "main",
    "abstract": "Autism spectral disorder (ASD) is a highly prevalent neurodevelopmental disorder characterized by deficits in communication and social interaction. Head-nodding, a kind of visual backchannels, is used to co-construct the conversation and is crucial to smooth social interaction. In the present study, we quantitively analyze how head-nodding relates to speech turn-taking and prosodic change in Japanese conversation. The results showed that nodding was less frequently observed in ASD participants, especially around speakers' turn transitions, whereas it was notable just before and after turn-taking in individuals with typical development (TD). Analysis using 16 sec of long-time sliding segments revealed that synchronization between nod frequency and mean vocal intensity was higher in the TD group than in the ASD group. Classification by a support vector machine (SVM) using these proposed features achieved high performance with an accuracy of 91.1% and an F-measure of 0.942. In addition, the results indicated an optimal way of nodding according to turn-ending and emphasis, which could provide standard responses for reference or feedback in social skill training for people with ASD. Furthermore, the natural timing of nodding implied by the results can also be applied to developing interactive responses in humanoid robots or computer graphic (CG) agents",
    "checked": true,
    "id": "1f055c5003a6be08d4377938f5630776884213b9",
    "semantic_title": "use of nods less synchronized with turn-taking and prosody during conversations in adults with autism",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ivanko22_interspeech.html": {
    "title": "DAVIS: Driver's Audio-Visual Speech recognition",
    "volume": "main",
    "abstract": "DAVIS is a driver's audio-visual assistive system intended to improve accuracy and robustness of speech recognition of the most frequent drivers' requests in natural driving conditions. Since speech recognition in driving condition is highly challenging due to acoustic noises, active head turns, pose variation, distance to recording devices, lightning conditions, etc. We rely on multimodal information and use both automatic lip-reading system for visual stream and ASR for audio stream processing. We have trained audio and video models on own RUSAVIC dataset containing in-the-wild audio and video recordings of 20 drivers. The recognition application comprises a graphical user interface and modules for audio and video signal acquisition, analysis, and recognition. The obtained results demonstrate rather high performance of DAVIS and also the fundamental possibility of recognizing speech commands by using video modality, even in such difficult natural conditions as driving",
    "checked": true,
    "id": "bb46f7d0e40f85dd8aae9d3265b549c0a8a4d8b4",
    "semantic_title": "davis: driver's audio-visual speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vaaras22_interspeech.html": {
    "title": "Analysis of Self-Supervised Learning and Dimensionality Reduction Methods in Clustering-Based Active Learning for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "When domain experts are needed to perform data annotation for complex machine-learning tasks, reducing annotation effort is crucial in order to cut down time and expenses. For cases when there are no annotations available, one approach is to utilize the structure of the feature space for clustering-based active learning (AL) methods. However, these methods are heavily dependent on how the samples are organized in the feature space and what distance metric is used. Unsupervised methods such as contrastive predictive coding (CPC) can potentially be used to learn organized feature spaces, but these methods typically create high-dimensional features which might be challenging for estimating data density. In this paper, we combine CPC and multiple dimensionality reduction methods in search of functioning practices for clustering-based AL. Our experiments for simulating speech emotion recognition system deployment show that both the local and global topology of the feature space can be successfully used for AL, and that CPC can be used to improve clustering-based AL performance over traditional signal features. Additionally, we observe that compressing data dimensionality does not harm AL performance substantially, and that 2-D feature representations achieved similar AL performance as higher-dimensional representations when the number of annotations is not very low",
    "checked": true,
    "id": "434efa2f92e685b73af91f8baa6e3672a8b53a32",
    "semantic_title": "analysis of self-supervised learning and dimensionality reduction methods in clustering-based active learning for speech emotion recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22n_interspeech.html": {
    "title": "Emotion-Shift Aware CRF for Decoding Emotion Sequence in Conversation",
    "volume": "main",
    "abstract": "Emotion recognition in conversation (ERC) is an increasingly important topic as it improves user experiences when adopting speech technology in our daily life. In this work, we propose an emotion-shift aware decoder based on formulation of conditional random field (CRF) to address the perennial issue of poor performances when handling emotion shift in dialogues. We conduct speech emotion recognition experiments on the IEMOCAP and the NNIME and achieve a 74.47% unweighted accuracy, which is the current state-of-the-art performance in the four class emotion recognition on the IEMOCAP. This is also the first work for ERC on the NNIME that obtains an outstanding performance of 61.02\\% weighted accuracy",
    "checked": true,
    "id": "2aacacdb86bec736038cd90d04cfa160f7b6d684",
    "semantic_title": "emotion-shift aware crf for decoding emotion sequence in conversation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/su22_interspeech.html": {
    "title": "Vaccinating SER to Neutralize Adversarial Attacks with Self-Supervised Augmentation Strategy",
    "volume": "main",
    "abstract": "Speech emotion recognition (SER) is being actively developed in multiple real-world application scenarios, and users tend to become intimately connected to these services. However, most existing SER models are vulnerable against a growing diverse set of adversarial attacks. The degraded performances can lead to dreadful user experiences. In this work, we propose a self-supervised augmentation defense (SSAD) strategy to learn a single purify network acts as a general front-end to neutralize adversarial distortions without knowing the types of attack beforehand. We show that our approach can robustly defend against two different gradient-based attacks at various intensities on the well-known IEMOCAP. Further, by examining metrics of protection efficacy and recovery rate, our approach shows a consistent protection behavior to prevent adverse outcomes and is capable to recover samples that are wrongly-predicted before purification",
    "checked": true,
    "id": "5cfc3c9e5e838e84f4295245ccadb0248281ff10",
    "semantic_title": "vaccinating ser to neutralize adversarial attacks with self-supervised augmentation strategy",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/parry22_interspeech.html": {
    "title": "Speech Emotion Recognition in the Wild using Multi-task and Adversarial Learning",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) is an important and challenging task, especially when deploying systems in the wild i.e. on unseen data, as they tend to generalise poorly. One promising approach to improve the generalisation capabilities of SER systems is to incorporate attributes of the speech signal, such as corpus or speaker information, which can be a source of overfitting or confusion for the model. In this paper, we investigate using multi-task learning, where attribute prediction is given as an auxiliary task to the model, and adversarial learning, where the model is explicitly trained to incorrectly predict attributes. We compare two adversarial learning approaches: gradient reversal and an adversarial discriminator. We evaluate these approaches in a cross-corpus training setting using two unseen corpora as test sets. We use four attributes -- corpus, speaker, gender and language -- and evaluate all possible combinations of these attributes. We show that both multi-task learning and adversarial learning improve SER performance in the wild, with the gradient reversal approach being the most consistent across attributes and test sets",
    "checked": true,
    "id": "fbabadab0874da22cfb2bbd8ad4506ec70a755ad",
    "semantic_title": "speech emotion recognition in the wild using multi-task and adversarial learning",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gudmalwar22_interspeech.html": {
    "title": "The Magnitude and Phase based Speech Representation Learning using Autoencoder for Classifying Speech Emotions using Deep Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "Speech Emotion Recognition (SER) from human speech utterances is a task of identifying emotions irrespective of their semantic content. It has an important role in making human-machine interaction natural. Conventional SER approaches emphasize more on magnitude spectrum for feature extraction and ignore phase information. Recent studies reveal that phase information has a significant part in analyzing speech acoustics. This work explores speech representation learning from magnitude and phase information using autoencoder for SER task. We trained the UNET autoencoder using Mel Frequency Cepstral Coefficients (MFCCs) and Modified Group Delay Function (MODGD) for learning representations. The encoder part of the trained UNET autoencoder is used as input to the neural network classifier and fine-tuned it concerning four emotions separately for MFCCs and MODGD. The learned representation for both MFCCs and MODGD are combined and given input to Support Vector Machine (SVM) for classification. The Deep Canonical Correlation Analysis (DCCA) is used to maximize the correlation between magnitude and phase information to improve the conventional SER system's performance. The performance analysis is carried out using the IEMOCAP database. The experimental results show improvement over MFCC features and existing approaches for unimodal SER",
    "checked": true,
    "id": "52bb32037e6d14fd58959160e68e1c0e1f7a6514",
    "semantic_title": "the magnitude and phase based speech representation learning using autoencoder for classifying speech emotions using deep canonical correlation analysis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/goncalves22_interspeech.html": {
    "title": "Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks",
    "volume": "main",
    "abstract": "ISpeech emotion recognition (SER) is a challenging task due to the limited availability of real-world labeled datasets. Since it is easier to find unlabeled data, the use of self-supervised learning (SSL) has become an attractive alternative. This study proposes new pre-text tasks for SSL to improve SER. While our target application is SER, the proposed pre-text tasks includes audiovisual formulations, leveraging the relationship between acoustic and facial features. Our proposed approach introduces three new unimodal and multimodal pre-text tasks that are carefully designed to learn better representations for predicting emotional cues from speech. Task 1 predicts energy variations (high or low) from a speech sequence. Task 2 uses speech features to predict facial activation (high or low) based on facial landmark movements. Task 3 performs a multi-class emotion recognition task on emotional labels obtained from combinations of action units (AUs) detected across a video sequence. We pre-train a network with 60.92 hours of unlabeled data, fine-tuning the model for the downstream SER task. The results on the CREMA-D dataset show that the model pre-trained on the proposed domain-specific pre-text tasks significantly improves the precision (up to 5.1%), recall (up to 4.5%), and F1-scores (up to 4.9%) of our SER system",
    "checked": true,
    "id": "a36e2ef5afe23ff56ddc40ce8c84aa6371a503f9",
    "semantic_title": "improving speech emotion recognition using self-supervised learning with domain-specific audiovisual tasks",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koizumi22b_interspeech.html": {
    "title": "SNRi Target Training for Joint Speech Enhancement and Recognition",
    "volume": "main",
    "abstract": "Speech enhancement (SE) is used as a frontend in speech applications including automatic speech recognition (ASR) and telecommunication. A difficulty in using the SE frontend is that the appropriate noise reduction level differs depending on applications and/or noise characteristics. In this study, we propose \"signal-to-noise ratio improvement (SNRi) target training\"; the SE frontend is trained to output a signal whose SNRi is controlled by an auxiliary scalar input. In joint training with a backend, the target SNRi value is estimated by an auxiliary network. By training all networks to minimize the backend task loss, we can estimate the appropriate noise reduction level for each noisy input in a data-driven scheme. Our experiments showed that the SNRi target training enables control of the output SNRi. In addition, the proposed joint training relatively reduces word error rate by 4.0\\% and 5.7\\% compared to a Conformer-based standard ASR model and conventional SE-ASR joint training model, respectively. Furthermore, by analyzing the predicted target SNRi, we observed the jointly trained network automatically controls the target SNRi according to noise characteristics. Audio demos are available in our demo page (google.github.io/df-conformer/snri_target/)",
    "checked": true,
    "id": "1d8b6bf8b4c5017034b50b6d0c7a0bbb547ca521",
    "semantic_title": "snri target training for joint speech enhancement and recognition",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sanada22_interspeech.html": {
    "title": "Deep Self-Supervised Learning of Speech Denoising from Noisy Speeches",
    "volume": "main",
    "abstract": "In the last few years, unsupervised learning methods have been proposed in speech denoising by taking advantage of Deep Neural Networks (DNNs). The reason is that such unsupervised methods are more practical than the supervised counterparts. In our scenario, we are given a set of noisy speech data, where any two data do not share the same clean data. Our goal is to obtain the denoiser by training a DNN based model. Using the set, we train the model via the following two steps: 1) From the noisy speech data, construct another noisy speech data via our proposed masking technique. 2) Minimize our proposed loss defined from the DNN and the two noisy speech data. We evaluate our method using Gaussian and real-world noises in our numerical experiments. As a result, our method outperforms the state-of-the-art method on average for both noises. In addition, we provide the theoretical explanation of why our method can be efficient if the noise has Gaussian distribution",
    "checked": true,
    "id": "01f3533f42220b3bd4e53258e6deb5f8baa4d23f",
    "semantic_title": "deep self-supervised learning of speech denoising from noisy speeches",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22d_interspeech.html": {
    "title": "NASTAR: Noise Adaptive Speech Enhancement with Target-Conditional Resampling",
    "volume": "main",
    "abstract": "For deep learning-based speech enhancement (SE) systems, the training-test acoustic mismatch can cause notable performance degradation. To address the mismatch issue, numerous noise adaptation strategies have been derived. In this paper, we propose a novel method, called noise adaptive speech enhancement with target-conditional resampling (NASTAR), which reduces mismatches with only one sample (one-shot) of noisy speech in the target environment. NASTAR uses a feedback mechanism to simulate adaptive training data via a noise extractor and a retrieval model. The noise extractor estimates the target noise from the noisy speech, called pseudo-noise. The noise retrieval model retrieves relevant noise samples from a pool of noise signals according to the noisy speech, called relevant-cohort. The pseudo-noise and the relevant-cohort set are jointly sampled and mixed with the source speech corpus to prepare simulated training data for noise adaptation. Experimental results show that NASTAR can effectively use one noisy speech sample to adapt an SE model to a target condition. Moreover, both the noise extractor and the noise retrieval model contribute to model adaptation. To our best knowledge, NASTAR is the first work to perform one-shot noise adaptation through noise extraction and retrieval",
    "checked": true,
    "id": "92acb84c97d5d3418f4857cd1bc7d0b83d20544f",
    "semantic_title": "nastar: noise adaptive speech enhancement with target-conditional resampling",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shchekotov22_interspeech.html": {
    "title": "FFC-SE: Fast Fourier Convolution for Speech Enhancement",
    "volume": "main",
    "abstract": "Fast Fourier convolution (FFC) is the recently proposed neural operator showing promising performance in several computer vision problems. The FFC operator allows employing large receptive field operations within early layers of the neural network. It was shown to be especially helpful for inpainting of periodic structures which are common in audio processing. In this work, we design neural network architectures which adapt FFC for speech enhancement. We hypothesize that a large receptive field allows these networks to produce more coherent phases than vanilla convolutional models, and validate this hypothesis experimentally. We found that neural networks based on Fast Fourier convolution outperform analogous convolutional models and show better or comparable results with other speech enhancement baselines",
    "checked": true,
    "id": "13a704a7a604c0a0a594419e78d7aa7377b594c5",
    "semantic_title": "ffc-se: fast fourier convolution for speech enhancement",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tal22_interspeech.html": {
    "title": "A Systematic Comparison of Phonetic Aware Techniques for Speech Enhancement",
    "volume": "main",
    "abstract": "Speech enhancement has seen great improvement in recent years using end-to-end neural networks. However, most models are agnostic to the spoken phonetic content. Recently, several studies suggested phonetic-aware speech enhancement, mostly using perceptual supervision. Yet, injecting phonetic features during model optimization can take additional forms (e.g., model conditioning). In this paper, we conduct a systematic comparison between different methods of incorporating phonetic information in a speech enhancement model. By conducting a series of controlled experiments, we observe the influence of different phonetic content models as well as various feature-injection techniques on enhancement performance, considering both causal and non-causal models. Specifically, we evaluate three settings for injecting phonetic information, namely: i) feature conditioning; ii) perceptual supervision; and iii) regularization. Phonetic features are obtained using an intermediate layer of either a supervised pre-trained Automatic Speech Recognition (ASR) model or by using a pre-trained Self-Supervised Learning (SSL) model. We further observe the effect of choosing different embedding layers on performance, considering both manual and learned configurations. Results suggest that using a SSL model as phonetic features outperforms the ASR one in most cases. Interestingly, the conditioning setting performs best among the evaluated configurations",
    "checked": true,
    "id": "946b86de6faf22ecfab91f23d032e77a89605382",
    "semantic_title": "a systematic comparison of phonetic aware techniques for speech enhancement",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shin22c_interspeech.html": {
    "title": "Multi-View Attention Transfer for Efficient Speech Enhancement",
    "volume": "main",
    "abstract": "Recent deep learning models have achieved high performance in speech enhancement; however, it is still challenging to obtain a fast and low-complexity model without significant performance degradation. Previous knowledge distillation studies on speech enhancement could not solve this problem because their output distillation methods do not fit the speech enhancement task in some aspects. In this study, we propose multi-view attention transfer (MV-AT), a feature-based distillation, to obtain efficient speech enhancement models in the time domain. Based on the multi-view features extraction model, MV-AT transfers multi-view knowledge of the teacher network to the student network without additional parameters. The experimental results show that the proposed method consistently improved the performance of student models of various sizes on the Valentini and deep noise suppression (DNS) datasets. MANNER-S-8.1GF with our proposed method, a lightweight model for efficient deployment, achieved 15.4x and 4.71x fewer parameters and floating-point operations (FLOPs), respectively, compared to the baseline model with similar performance",
    "checked": true,
    "id": "3067c3f366516653cc5b06614380d7dd835776ad",
    "semantic_title": "multi-view attention transfer for efficient speech enhancement",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/goswami22_interspeech.html": {
    "title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate",
    "volume": "main",
    "abstract": "The mapping of text to speech (TTS) is non-deterministic, letters may be pronounced differently based on context, or phonemes can vary depending on various physiological and stylistic factors like gender, age, accent, emotions, etc. Neural speaker embeddings, trained to identify or verify speakers are typically used to represent and transfer such characteristics from reference speech to synthesized speech. Speech separation on the other hand is the challenging task of separating individual speakers from an overlapping mixed signal of various speakers. Speaker attractors are high-dimensional embedding vectors that pull the time-frequency bins of each speaker's speech towards themselves while repelling those belonging to other speakers. In this work, we explore the possibility of using these powerful speaker attractors for zero-shot speaker adaptation in multi-speaker TTS synthesis and propose speaker attractor text to speech (SATTS). Through various experiments, we show that SATTS can synthesize natural speech from text from an unseen target speaker's reference signal which might have less than ideal recording conditions, i.e. reverberations or mixed with other speakers",
    "checked": true,
    "id": "92ad67927686760529c75b7deb82d09b33dc40d1",
    "semantic_title": "satts: speaker attractor text to speech, learning to speak by learning to separate",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bensimon22_interspeech.html": {
    "title": "Correcting Mispronunciations in Speech using Spectrogram Inpainting",
    "volume": "main",
    "abstract": "Learning a new language involves constantly comparing speech productions with reference productions from the environment. Early in speech acquisition, children make articulatory adjustments to match their caregivers' speech. Grownup learners of a language tweak their speech to match the tutor reference. This paper proposes a method to synthetically generate correct pronunciation feedback given incorrect production. Furthermore, our aim is to generate the corrected production while maintaining the speaker's original voice. The system prompts the user to pronounce a phrase. The speech is recorded, and the samples associated with the inaccurate phoneme are masked with zeros. This waveform serves as an input to a speech generator, implemented as a deep learning inpainting system with a U-net architecture, and trained to output a reconstructed speech. The training set is composed of unimpaired proper speech examples, and the generator is trained to reconstruct the original proper speech. We evaluated the performance of our system on phoneme replacement of minimal pair words of English as well as on children with pronunciation disorders. Results suggest that human listeners slightly prefer our generated speech over a smoothed replacement of the inaccurate phoneme with a production of a different speaker",
    "checked": true,
    "id": "0875c5b1bf0cc6b077a06f959aad4f4eb3910437",
    "semantic_title": "correcting mispronunciations in speech using spectrogram inpainting",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fong22_interspeech.html": {
    "title": "Speech Audio Corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech",
    "volume": "main",
    "abstract": "Correct pronunciation is essential for text-to-speech (TTS) systems in production. Most production systems rely on pronouncing dictionaries to perform grapheme-to-phoneme conversion. Unlike end-to-end TTS, this enables pronunciation correction by manually altering the phoneme sequence, but the necessary dictionaries are labour-intensive to create and only exist in a few high-resourced languages. This work demonstrates that accurate TTS pronunciation control can be achieved without a dictionary. Moreover, we show that such control can be performed without requiring any model retraining or fine-tuning, merely by supplying a single correctly-pronounced reading of a word in a different voice and accent at synthesis time. Experimental results show that our proposed system successfully enables one-off correction of mispronunciations in grapheme-based TTS with maintained synthesis quality. This opens the door to production-level TTS in languages and applications where pronunciation dictionaries are unavailable",
    "checked": true,
    "id": "54ad775698d259810ac6125f9907f887eb17f4d4",
    "semantic_title": "speech audio corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22i_interspeech.html": {
    "title": "End-to-End Binaural Speech Synthesis",
    "volume": "main",
    "abstract": "In this work, we present an end-to-end binaural speech synthesis system that combines a low-bitrate audio codec with a powerful binaural decoder that is capable of accurate speech binauralization while faithfully reconstructing environmental factors like ambient noise or reverb. The network is a modified vector-quantized variational autoencoder, trained with several carefully designed objectives, including an adversarial loss. We evaluate the proposed system on an internal binaural dataset with objective metrics and a perceptual study. Results show that the proposed approach matches the ground truth data more closely than previous methods. In particular, we demonstrate the capability of the adversarial loss in capturing environment effects needed to create an authentic auditory scene",
    "checked": true,
    "id": "166cd4d54cfdb6e1fb577697d28a8946d79fc6f8",
    "semantic_title": "end-to-end binaural speech synthesis",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koch22_interspeech.html": {
    "title": "PoeticTTS - Controllable Poetry Reading for Literary Studies",
    "volume": "main",
    "abstract": "Speech synthesis for poetry is challenging due to specific intonation patterns inherent to poetic speech. In this work, we propose an approach to synthesise poems with almost human like naturalness in order to enable literary scholars to systematically examine hypotheses on the interplay between text, spoken realisation, and the listener's perception of poems. To meet these special requirements for literary studies, we resynthesise poems by cloning prosodic values from a human reference recitation, and afterwards make use of fine-grained prosody control to manipulate the synthetic speech in a human-in-the-loop setting to alter the recitation w.r.t. specific phenomena. We find that finetuning our TTS model on poetry captures poetic intonation patterns to a large extent which is beneficial for prosody cloning and manipulation and verify the success of our approach both in an objective evaluation as well as in human studies",
    "checked": true,
    "id": "aba65437d82167805fcb2597154a22940893a388",
    "semantic_title": "poetictts - controllable poetry reading for literary studies",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/krug22_interspeech.html": {
    "title": "Articulatory Synthesis for Data Augmentation in Phoneme Recognition",
    "volume": "main",
    "abstract": "While numerous studies on automatic speech recognition have been published in recent years describing data augmentation strategies based on time or frequency domain signal processing, few works exist on the artificial extensions of training data sets using purely synthetic speech data. In this work, the German Kiel corpus was augmented with synthetic data generated with the state-of-the-art articulatory synthesizer VocalTractLab. It is shown that the additional synthetic data can lead to a significantly better performance in single-phoneme recognition in certain cases, while at the same time, the performance can also decrease in other cases, depending on the degree of acoustic naturalness of the synthetic phonemes. As a result, this work can potentially guide future studies to improve the quality of articulatory synthesis via the link between synthetic speech production and automatic speech recognition",
    "checked": true,
    "id": "e1da122ba8aa154fc928b6db9c1cc3f48d1fb31c",
    "semantic_title": "articulatory synthesis for data augmentation in phoneme recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22c_interspeech.html": {
    "title": "SF-DST: Few-Shot Self-Feeding Reading Comprehension Dialogue State Tracking with Auxiliary Task",
    "volume": "main",
    "abstract": "Few-shot dialogue state tracking (DST) model tracks user requests in dialogue with reliable accuracy even with a small amount of data. In this paper, we introduce an ontology-free few-shot DST with self-feeding belief state input. The self-feeding belief state input increases the accuracy in multi-turn dialogue by summarizing previous dialogue. Also, we newly developed a slot-gate auxiliary task. This new auxiliary task helps classify whether a slot is mentioned in the dialogue. Our model achieved the best score in a few-shot setting for four domains on multiWOZ 2.0",
    "checked": true,
    "id": "430ef311d350631dc0e97b684b28e92903b698e5",
    "semantic_title": "sf-dst: few-shot self-feeding reading comprehension dialogue state tracking with auxiliary task",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cattan22_interspeech.html": {
    "title": "Benchmarking Transformers-based models on French Spoken Language Understanding tasks",
    "volume": "main",
    "abstract": "In the last five years, the rise of the self-attentional Transformer-based architectures led to state-of-the-art performances over many natural language tasks. Although these approaches are increasingly popular, they require large amounts of data and computational resources. There is still a substantial need for benchmarking methodologies ever upwards on under-resourced languages in data-scarce application conditions. Most pre-trained language models were massively studied using the English language and only a few of them were evaluated on French. In this paper, we propose a unified benchmark, focused on evaluating models quality and their ecological impact on two well-known French spoken language understanding tasks. Especially we benchmark thirteen well-established Transformer-based models on the two available spoken language understanding tasks for French: MEDIA and ATIS-FR. Within this framework, we show that compact models can reach comparable results to bigger ones while their ecological impact is considerably lower. However, this assumption is nuanced and depends on the considered compression method",
    "checked": true,
    "id": "0de905634bd3d42a612ffab7ddd7a814e7e655bb",
    "semantic_title": "benchmarking transformers-based models on french spoken language understanding tasks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/heo22b_interspeech.html": {
    "title": "mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling",
    "volume": "main",
    "abstract": "Zero-shot slot filling has received considerable attention to cope with the problem of limited available data for the target domain. One of the important factors in zero-shot learning is to make the model learn generalized and reliable representations. For this purpose, we present mcBERT, which stands for 'm'omentum 'c'ontrastive learning with BERT, to develop a robust zero-shot slot filling model. mcBERT uses BERT to initialize the two encoders, the query encoder and key encoder, and is trained by applying momentum contrastive learning. Our experimental results on the SNIPS benchmark show that mcBERT substantially outperforms the previous models, recording a new state-of-the-art. Besides, we also show that each component composing mcBERT contributes to the performance improvement",
    "checked": true,
    "id": "0548ce902968c09fc14e2734c834a5d2b33720f7",
    "semantic_title": "mcbert: momentum contrastive learning with bert for zero-shot slot filling",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22y_interspeech.html": {
    "title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "End-to-end spoken language understanding (SLU) systems benefit from pretraining on large corpora, followed by fine-tuning on application-specific data. The resulting models are too large for on-edge applications. For instance, BERT-based systems contain over 110M parameters. Observing the model is over-parameterized, we propose lean transformer structure where the dimension of the attention mechanism is automatically reduced using group sparsity. We propose a variant where the learned attention subspace is transferred to an attention bottleneck layer. In a low-resource setting and without pre-training, the resulting compact SLU model achieves accuracies competitive with pre-trained large models",
    "checked": true,
    "id": "26c309dd934331a024419625af46df07065b0ef7",
    "semantic_title": "bottleneck low-rank transformers for low-resource spoken language understanding",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/raju22_interspeech.html": {
    "title": "On joint training with interfaces for spoken language understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) systems extract both text transcripts and semantics associated with intents and slots from input speech utterances. SLU systems usually consist of (1) an automatic speech recognition (ASR) module (2) an interface module that exposes relevant outputs from ASR, and (3) a natural language understanding (NLU) module. Interfaces in SLU systems carry information on text transcriptions or richer information like neural embeddings from ASR to NLU. In this paper, we study how interfaces affect joint-training for spoken language understanding. Most notably, we obtain the state-of-the-art results on the publicly available 50-hr SLURP [1] dataset. We first leverage large-size pretrained ASR and NLU models that are connected by a text interface, and then jointly train both models via a sequence loss function. For scenarios where pretrained models are not utilized, the best results are obtained through a joint sequence loss training using richer neural interfaces. Finally, we show the overall diminishing impact of leveraging pretrained models with increased training data size",
    "checked": true,
    "id": "eb07badcb7e2e7a25d212e55ca436687ddc49840",
    "semantic_title": "on joint training with interfaces for spoken language understanding",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/garg22_interspeech.html": {
    "title": "Device-Directed Speech Detection: Regularization via Distillation for Weakly-Supervised Models",
    "volume": "main",
    "abstract": "We address the problem of detecting speech directed to a device that does not contain a specific wake-word. Specifically, we focus on audio coming from a touch-based invocation. Mitigating virtual assistants (VAs) activation due to accidental button presses is critical for user experience. While the majority of approaches to false trigger mitigation (FTM) are designed to detect the presence of a target keyword, inferring user intent in absence of keyword is difficult. This also poses a challenge when creating the training/evaluation data for such systems due to inherent ambiguity in the user's data. To this end, we propose a novel FTM approach that uses weakly-labeled training data obtained with a newly introduced data sampling strategy. While this sampling strategy reduces data annotation efforts, the data labels are noisy as the data are not annotated manually. We use these data to train an acoustics-only model for the FTM task by regularizing its loss function via knowledge distillation from an ASR-based (LatticeRNN) model. This improves the model decisions, resulting in 66% gain in accuracy, as measured by equal-error-rate (EER), over the base acoustics-only model. We also show that the ensemble of the LatticeRNN and acoustic-distilled models brings further accuracy improvement of 20%",
    "checked": true,
    "id": "684d44353eaaa6d1c9d9b795e10e39b58171e858",
    "semantic_title": "device-directed speech detection: regularization via distillation for weakly-supervised models",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ogayo22_interspeech.html": {
    "title": "Building African Voices",
    "volume": "main",
    "abstract": "Modern speech synthesis techniques can produce natural-sounding speech given sufficient high-quality data and compute resources. However, such data is not readily available for many languages. This paper focuses on speech synthesis for low-resourced African languages, from corpus creation to sharing and deploying the Text-to-Speech (TTS) systems. We first create a set of general-purpose instructions on building speech synthesis systems with minimum technological resources and subject-matter expertise. Next, we create new datasets and curate datasets from \"found\" data (existing recordings) through a participatory approach while considering accessibility, quality, and breadth. We demonstrate that we can develop synthesizers that generate intelligible speech with 25 minutes of created speech, even when recorded in suboptimal environments. Finally, we release the speech data, code, and trained voices for 12 African languages to support researchers and developers",
    "checked": true,
    "id": "ddedad67de4aa720f7b450605c73e197e9aa912d",
    "semantic_title": "building african voices",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dheram22_interspeech.html": {
    "title": "Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities",
    "volume": "main",
    "abstract": "As for other forms of AI, speech recognition has recently been examined with respect to performance disparities across different user cohorts. One approach to achieve fairness in speech recognition is to (1) identify speaker cohorts that suffer from subpar performance and (2) apply fairness mitigation measures targeting the cohorts so discovered. In this paper, we report on initial findings with both discovery and mitigation of performance disparities using data from a product-scale AI assistant speech recognition system. We compare cohort discovery based on geographic and demographic information to a more scalable method that groups speakers without human labels, using speaker embedding technology. For fairness mitigation, we find that oversampling of underrepresented cohorts, as well as modeling speaker cohort membership by additional input variables, is able to reduce the gap between top- and bottom-performing cohorts, without deteriorating overall recognition accuracy. Index Terms: speech recognition, performance fairness, cohort discovery",
    "checked": true,
    "id": "91e33383baba1259f9a86967cb93873f5b3ebd43",
    "semantic_title": "toward fairness in speech recognition: discovery and mitigation of performance disparities",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chan22b_interspeech.html": {
    "title": "Training and typological bias in ASR performance for world Englishes",
    "volume": "main",
    "abstract": "The use of automatic speech recognition (ASR) has been increasing to promote inclusion and accessibility. Nonetheless, prior work on ASR finds performance gaps conditioned by specific gender and racial groups, revealing systematic biases in modern ASR systems. However, work has focused on native varieties of English, glossing over its impact on a wider range of ASR users, namely second language speakers of English. The present work compares the performance of the transcription system Otter, on 24 varieties of English, 21 of them are non-native varieties. We compare the word and phone error rate (WER/PER) of accent varieties that are claimed to be supported by Otter and those that are unsupported. Results show that English varieties that are supported have lower WERs compared to that of unsupported varieties. However, there are still systematic differences in performance conditioned by linguistic structure in both supported and unsupported Englishes. Specifically, Otter performs better on English varieties from non-tonal first language speakers. We conclude that while inclusion of more varieties of English in the training data set for ASR may promote inclusivity, there may still be biases inherent to the linguistic structure that should not be overlooked",
    "checked": true,
    "id": "626eff7329678b2bb75694f559ecab485f55bf71",
    "semantic_title": "training and typological bias in asr performance for world englishes",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zanonboito22_interspeech.html": {
    "title": "A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems",
    "volume": "main",
    "abstract": "Self-supervised models for speech processing emerged recently as popular foundation blocks in speech processing pipelines. These models are pre-trained on unlabeled audio data and then used in speech processing downstream tasks such as automatic speech recognition (ASR) or speech translation (ST). Since these models are now used in research and industrial systems alike, it becomes necessary to understand the impact caused by some features such as gender distribution within pre-training data. Using French as our investigation language, we train and compare gender-specific wav2vec 2.0 models against models containing different degrees of gender balance in their pre-training data. The comparison is performed by applying these models to two speech-to-text downstream tasks: ASR and ST. Results show the type of downstream integration matters. We observe lower overall performance using gender-specific pre-training before fine-tuning an end-to-end ASR system. However, when self-supervised models are used as feature extractors, the overall ASR and ST results follow more complex patterns in which the balanced pre-trained model does not necessarily lead to the best results. Lastly, our crude 'fairness' metric, the relative performance difference measured between female and male test sets, does not display a strong variation from balanced to gender-specific pre-trained wav2vec 2.0 models",
    "checked": true,
    "id": "f5526c23bda899eb38fb3589507a9a1db28389ed",
    "semantic_title": "a study of gender impact in self-supervised models for speech-to-text systems",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/johnson22_interspeech.html": {
    "title": "Automatic Dialect Density Estimation for African American English",
    "volume": "main",
    "abstract": "In this paper, we explore automatic prediction of dialect density of the African American English (AAE) dialect, where dialect density is defined as the percentage of words in an utterance that contain characteristics of the non-standard dialect. We investigate several acoustic and language modeling features, including the commonly used X-vector representation and ComParE feature set, in addition to information extracted from ASR transcripts of the audio files and prosodic information. To address issues of limited labeled data, we use a weakly supervised model to compress prosodic and X-vector features into more task-relevant representations. An XGBoost model is then used to predict the speaker's dialect density from these features and show which are most significant during inference. We evaluate the utility of these features both alone and in combination for the given task. This work, which does not rely on hand-labeled transcripts, is performed on audio segments from the CORAAL database. We show a significant correlation between our predicted and ground truth dialect density measures for AAE speech in this database and propose this work as a way to automatically explain and mitigate bias in speech technology",
    "checked": true,
    "id": "496a07e882d7511362f419bf4de31367fa20501d",
    "semantic_title": "automatic dialect density estimation for african american english",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kukk22_interspeech.html": {
    "title": "Improving Language Identification of Accented Speech",
    "volume": "main",
    "abstract": "Language identification from speech is a common preprocessing step in many spoken language processing systems. In recent years, this field has seen a fast progress, mostly due to the use of self-supervised models pretrained on multilingual data and the use of large training corpora. This paper shows that for speech with a non-native or regional accent, the accuracy of spoken language identification systems drops dramatically, and that the accuracy of identifying the language is inversely correlated with the strength of the accent. We also show that using the output of a lexicon-free speech recognition system of the particular language helps to improve language identification performance on accented speech by a large margin, without sacrificing accuracy on native speech. We obtain relative error rate reductions ranging from to 35 to 63% over the state-of-the-art model across several non-native speech datasets",
    "checked": true,
    "id": "1b9581a3eb1cd8bd7df74076cbb80f5c6e139485",
    "semantic_title": "improving language identification of accented speech",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/toussaint22_interspeech.html": {
    "title": "Design Guidelines for Inclusive Speaker Verification Evaluation Datasets",
    "volume": "main",
    "abstract": "Speaker verification (SV) provides billions of voice-enabled devices with access control, and ensures the security of voice-driven technologies. As a type of biometrics, it is necessary that SV is unbiased, with consistent and reliable performance across speakers irrespective of their demographic, social and economic attributes. Current SV evaluation practices are insufficient for evaluating bias: they are over-simplified and aggregate users, not representative of usage scenarios encountered in deployment, and consequences of errors are not accounted for. This paper proposes design guidelines for constructing SV evaluation datasets that address these short-comings. We propose a schema for grading the difficulty of utterance pairs, and present an algorithm for generating inclusive SV datasets. We empirically validate our proposed method in a set of experiments on the VoxCeleb1 dataset. Our results confirm that the count of utterance pairs/speaker, and the difficulty grading of utterance pairs have a significant effect on evaluation performance and variability. Our work contributes to the development of SV evaluation practices that are inclusive and fair",
    "checked": true,
    "id": "873a1cab630c7894bfdc6edf4599587d86b1f421",
    "semantic_title": "design guidelines for inclusive speaker verification evaluation datasets",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/trinh22_interspeech.html": {
    "title": "Reducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation",
    "volume": "main",
    "abstract": "We present an approach to reduce the performance disparity between geographic regions without degrading performance on the overall user population for ASR. A popular approach is to fine-tune the model with data from regions where the ASR model has a higher word error rate (WER). However, when the ASR model is adapted to get better performance on these high-WER regions, its parameters wander from the previous optimal values, which can lead to worse performance in other regions. In our proposed method, we utilize the elastic weight consolidation (EWC) regularization loss to identify directions in parameters space along which the ASR weights can vary to improve for high-error regions, while still maintaining performance on the speaker population overall. Our results demonstrate that EWC can reduce the word error rate (WER) in the region with highest WER by 3.2% relative while reducing the overall WER by 1.3% relative. We also evaluate the role of language and acoustic models in ASR fairness and propose a clustering algorithm to identify WER disparities based on geographic region",
    "checked": true,
    "id": "edfb445cfc755d36dfbe9e07c9f4f66339f3a069",
    "semantic_title": "reducing geographic disparities in automatic speech recognition via elastic weight consolidation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kunihara22_interspeech.html": {
    "title": "Gradual Improvements Observed in Learners' Perception and Production of L2 Sounds Through Continuing Shadowing Practices on a Daily Basis",
    "volume": "main",
    "abstract": "Shadowing was proposed originally in psycholinguistics to investigate listeners' process of perceiving speech, and now it is used widely in language education in Japan as a training method for learners to acquire a better skill in perceiving and producing L2 sounds. Recently, a shadowing-based assessment method was proposed to quantify learners' skill in perception and production. By integrating the two methods, this study aims at tracking gradual improvements of learners' skill during shadowing practices. We held a special program of 42-day Shadowing Marathon, where Japanese learners of English participated in shadowing practices every day for six weeks. Four new oral passages were presented daily, each of which was shadowed repeatedly. From the obtained data, we analyzed gradual improvements in learners' perception and production of both segments and prosody through repeating shadowing within a day, and through continuing the shadowing practices over 42 days. The results of analysis showed that, while learners' perception of segments and prosody improved significantly even with no explicit instructions, their production did not show significant improvement in a self-learning condition. The data also showed in what way learners depend on written input and auditory input when they learn perception and production of L2 sounds",
    "checked": true,
    "id": "946e31ed1a67af88c1f404b511684537d1e6c615",
    "semantic_title": "gradual improvements observed in learners' perception and production of l2 sounds through continuing shadowing practices on a daily basis",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kirchhubel22_interspeech.html": {
    "title": "Spoofed speech from the perspective of a forensic phonetician",
    "volume": "main",
    "abstract": "While previous work has uncovered the performances of automatic systems when presented with spoofed speech samples, this work looks deeper into these samples from the perspective of an experienced forensic phonetician. From an evaluation of 300 samples, this paper reveals how similar some of the spoofed speech samples are to genuine human speech samples. One speech synthesis method sticks out' in this respect by producing speech samples that bear a collection of natural speech characteristics. On the other hand, spoofing methods that have been shown to present problems to automatic systems in past work do not present problems to the forensic phonetician. The main contribution of this paper is a descriptive account of different spoofed speech samples, based on the auditory-acoustic evaluation of a forensic phonetician. This is to spark an awareness of the current possibilities of spoofing methods that perhaps have so far been off the radar' for many forensic phoneticians. Such an account can bring great value to the forensic phonetics community as spoofed speech samples could plausibly find their way into forensic casework scenarios",
    "checked": true,
    "id": "62dba3341ff315cd11fead1956b94ba65fd74ff7",
    "semantic_title": "spoofed speech from the perspective of a forensic phonetician",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jeon22_interspeech.html": {
    "title": "Investigating Prosodic Variation in British English Varieties using ProPer",
    "volume": "main",
    "abstract": "This study used ProPer (PROsodic analysis with PERiodic energy), a tool for automatic prosodic analysis, to investigate prosodic variation in four British English varieties. The central question was how to efficiently describe systematic prosodic variation without extensive manual annotations. We analysed ProPer parameters, the magnitude of F0 change between two successive syllables, synchrony (i.e. the general trend of the F0 movement within a syllable), and periodic energy mass. We then carried out audiovisual analysis of the F0 movement. Our overall assessment is that ProPer allows holistic prosodic analysis, significantly reducing researchers' workload in prosodic annotations. The ProPer analysis is optimised for syllable-sized intervals. It requires the examination of a vast amount of acoustic information and possibly the careful design of experimental materials for collecting laboratory speech data",
    "checked": true,
    "id": "1a9f94d0b9bd8ef43ef64f165fd6b304dd15d5df",
    "semantic_title": "investigating prosodic variation in british english varieties using proper",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hwang22_interspeech.html": {
    "title": "Perceived prominence and downstep in Japanese",
    "volume": "main",
    "abstract": "Perceived prominence as a function of fundamental frequency (f0) is examined with special attention given to downstep in Standard Japanese, a lowered f0 of the material following an accented word compared with that following an unaccented word. This lowering does not signal a reduction in prominence, whereas f0 compression triggered by focus is associated with prominence. The purpose of this study is to investigate how much f0 lowering is perceived as equally prominent to understand a perceptually acceptable pitch range of Japanese downstep. A prominence perception test was conducted with varying peak f0s of two successive phrases. Results of the test reveal that there is a particular f0 level of the following phrase to be perceived as equally prominent as the preceding phrase, regardless of the peak f0 of the preceding phrase. This result indicates that f0 differences between the preceding and the following phrases are greater as the f0 of the preceding phrase increases, corroborating the effect of f0 on prominence perception found in the literature. Furthermore, an interesting asymmetry is suggested between production and perception of downstep in Japanese",
    "checked": true,
    "id": "f069f808273902a7c097bbc884d5a79c15074586",
    "semantic_title": "perceived prominence and downstep in japanese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/alicehajic22_interspeech.html": {
    "title": "The discrimination of [zi]-[di] by Japanese listeners and the prospective phonologization of /zi/",
    "volume": "main",
    "abstract": "The sequence /zi/ does not occur in the Japanese language, and in loanwords from English, it is adapted as /di/, e.g., English busy - Japanese /bi.di/. While similar sequences of alveolar obstruent plus high vowel, such as /ti/ or /t/, also used to be avoided in Japanese, they are occurring in recent loanwords. Such formerly impossible sound sequences are likely to have emerged because current Japanese speakers have more contact with English than previous generations: extended exposure to English with its less restrictive phonotactics leads younger speakers to acquire less strong restrictions against the sequences in question in their Japanese. In the present study, we tested whether Japanese listeners are sensitive to the difference between [zi]-[di] in an online AX discrimination task. Our participants ranged in age from 18 to 65. We hypothesized that younger listeners would be better at discriminating [zi]-[di] in non-words as they had more exposure to English, which allows /zi/. Our study could not confirm this hypothesis. Instead, we found large individual variation in performance, and a good discriminability in general, which leads us to expect that the occurrence of /zi/ in Japanese loanwords is imminent",
    "checked": true,
    "id": "311c56eaf24d6f59c129c4a7f51b8255fffdddce",
    "semantic_title": "the discrimination of [zi]-[di] by japanese listeners and the prospective phonologization of /zi/",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/langheinrich22_interspeech.html": {
    "title": "Glottal inverse filtering based on articulatory synthesis and deep learning",
    "volume": "main",
    "abstract": "We propose a new method to estimate the glottal vocal tract excitation from speech signals based on deep learning. To that end, a bidirectional recurrent neural network with long short-term memory units was trained to predict the glottal airflow derivative from the speech signal. Since natural reference data for this task is unobtainable at the required scale, we used the articulatory speech synthesizer VocalTractLab to generate a large dataset containing synchronous connected speech and glottal airflow signals for training. The trained model's performance was objectively evaluated by means of stationary synthetic signals from the OPENGLOT glottal inverse filtering benchmark dataset and by using our dataset of connected synthetic speech. Compared to the state of the art, the proposed model produced a more accurate estimation using OPENGLOT's physically synthesized signals but was less accurate for its computationally simulated signals. However, our model was much more accurate and plausible on the connected speech signals, especially for sounds with mixed excitation (e.g. fricatives) or sounds with pronounced zeros in their transfer function (e.g. nasals). Future work will introduce more variety into the training data (e.g. regarding pitch and phonation) and focus on estimating features of the glottal flow instead of the entire waveform",
    "checked": true,
    "id": "ad75f878384a86238f256f0f41886c733cd5d114",
    "semantic_title": "glottal inverse filtering based on articulatory synthesis and deep learning",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ludusan22_interspeech.html": {
    "title": "Investigating phonetic convergence of laughter in conversation",
    "volume": "main",
    "abstract": "Laughter is one of the most encountered paralinguistic phenomena in conversation. Similarly to other communicative elements, evidence for laughter convergence, in particular for its temporal distribution and its acoustic marking, has been found between interlocutors. We investigate here whether segmental-level convergence effects, previously observed for speech, may be also found in the case of laughter. Using a corpus of dyadic interactions, we evaluate phonetic convergence of the vocalic part of laughs, by means of distances between the formant values. This was carried out for two proposed measures of convergence: global  at the level of the entire conversation, and local  considering consecutive laughs. Our global measure results reveal that interlocutors converge towards the end of the interaction, compared to its beginning, although important individual variation exists. With respect to the local measure, our findings show a lack of phonetic convergence (or divergence) between conversational partners",
    "checked": true,
    "id": "eddd4fa40c0db5d4d9c379399bbbba3ec0db6b82",
    "semantic_title": "investigating phonetic convergence of laughter in conversation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/delvaux22_interspeech.html": {
    "title": "Telling self-defining memories: An acoustic study of natural emotional speech productions",
    "volume": "main",
    "abstract": "Vocal cues in emotion encoding are rarely studied based on real-life, naturalistic emotional speech. In the present study, 20 speakers aged 25-35 were recorded while orally telling 5 successive self-defining autobiographic memories (SDM). By definition, this task is highly emotional, although emotional load and emotion regulation are expected to vary across SDM. Seven acoustic parameters were extracted: MeanF0, MedianFo, StandardDeviationF0, MinF0, MaxF0, Duration and SpeechRate. All SDM were manually transcribed, then their emotional lexicon was analysed using Emotaix. First, speech productions were examined in reference with SDM characteristics (specificity, integrative meaning and affective valence) as determined by 3 independent investigators. Results showed that overall the speech parameters did not change over the time course of the experiment, or as a function of integrative meaning. Specific memories were recounted at a higher speech rate and at greater length than non specific ones. SDM with positive affective valence were shorter and included less variability in fundamental frequency than negative SDM. Second, emotionally-charged (positive vs. negative; high vs. low arousal) vs. emotionally-neutral utterances as to Emotaix classification were compared over all SDM. Only a few significant effects were observed, which led us to discuss the role of emotion regulation in the SDM task",
    "checked": true,
    "id": "f035973b78ba4b6f93af720be9c022908d579ce0",
    "semantic_title": "telling self-defining memories: an acoustic study of natural emotional speech productions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/spinu22_interspeech.html": {
    "title": "Voicing neutralization in Romanian fricatives across different speech styles",
    "volume": "main",
    "abstract": "Romance languages such as Italian or Spanish preserve fricative voicing contrasts in word-final position, while their neutralization has been reported for European Portuguese, but the behavior of Romanian fricatives remains understudied. Previous work with Romanian fricatives suggests a pattern of final devoicing but, due to the specific properties of the corpus analyzed, it is unclear if this is limited to the presence of secondary palatalization and/or the result of morphological conditioning. In this study, we apply speech processing tools to investigate the acoustic characteristics of the voicing contrast in fricatives in contemporary spoken Romanian. We examine a corpus of prepared speech from newscasts and semi-spontaneous TV debates (86 speakers) and compare our results to previous findings from a corpus of controlled experimental speech (31 speakers). Our classification tool employs cepstral coefficients and hidden Markov model (HMM)-defined temporal regions to identify the properties of these segments. Our findings conform to typological predictions regarding partial devoicing in coda position, especially at more posterior places, but we find little support for voicing neutralization in Romanian fricatives more generally. Our study thus documents the properties of Romanian fricatives and contributes to our understanding of the dynamics of contrast maintenance in phonological systems",
    "checked": true,
    "id": "baf1fb4bbea1ec01caf20ceaef1c2923d42344c3",
    "semantic_title": "voicing neutralization in romanian fricatives across different speech styles",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liao22_interspeech.html": {
    "title": "Nasal Coda Loss in the Chengdu Dialect of Mandarin: Evidence from RT-MRI",
    "volume": "main",
    "abstract": "In the Chengdu dialect of Mandarin, the /(V)an/ rime words have been described to have undergone a nasal loss process in the last decades. However, no acoustical or physiological evidence has been provided so far. In this study, we investigate this sound change process by directly looking at the velum gesture in the target words from 4 Chengdu speakers. By means of real-time Magnetic Resonance Imaging (rt-MRI), the velum opening signal was captured along with synchronized and noise suppressed audio. The maximum degree of velum opening was compared between tautosyllabic and heterosyllabic VN sequences for different vowels (N = /n, /). Nasal consonant loss was most evident for tautosyllabic /(V)an/ rime words. This sound change, together with the observed diachronic vowel raising in /(V)an/ rimes, is compatible with other research showing a preference for low vowel raising before nasal consonants. This phonetically motivated oral vowel, which is a consequence of nasal coda loss and vowel raising, would form a new phonological contrast in this dialect e.g., from /pa, pan/ to /pa, p/",
    "checked": true,
    "id": "5549b014ffe6874a4ee7349b4839d4e34588a0a8",
    "semantic_title": "nasal coda loss in the chengdu dialect of mandarin: evidence from rt-mri",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/buech22_interspeech.html": {
    "title": "ema2wav: doing articulation by Praat",
    "volume": "main",
    "abstract": "In this paper, we present ema2wav, a software conversion tool for electromagnetic articulographic data, producing multi-channel WAVE files. The data can be converted either by executing a stand-alone Python script or by using a user-friendly GUI. ema2wav allows the display and extraction of EMA trajectories as well as data smoothing and the computation of derivatives and Euclidean distance between sensors. A great asset of this converter is that it allows the research community to process EMA data in widespread and easy-to-use open-source programs like Praat. It is completely platform-independent and is thus a very promising alternative for e.g., students, teachers and researchers in experimental linguistics who have either limited access to software licenses and/or seek for an easy way to maintain open solutions for their research",
    "checked": true,
    "id": "5555b22ba29b4c481e534471f40e876f0f7538c0",
    "semantic_title": "ema2wav: doing articulation by praat",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rumberg22b_interspeech.html": {
    "title": "Improving Phonetic Transcriptions of Children's Speech by Pronunciation Modelling with Constrained CTC-Decoding",
    "volume": "main",
    "abstract": "Language sample analysis (LSA) is a powerful tool for both therapeutic applications and research of child speech and language development. Nevertheless, it is not routinely used, due to the high cost of manual transcription and analysis. Assistance by automatic speech recognition for children has the potential to enable a wide-spread use of LSA. However, the development of modern speech recognition systems heavily relies on large scale datasets. Therefore, it faces the same obstacle of high cost for transcription as LSA itself. In this paper, we study how cheaply transcribed child speech, i. e., limited to an orthographic transcription, can be improved on a phonetic level by leveraging a CTC based automatic speech recognition model, trained on a small phonetically transcribed dataset. We constrain the CTC decoding by modeling variation of the pronunciation given the orthographic transcription using weighted finite state automata. Our experiments show that the transcription is improved in terms of phone error rate by relative 14% when applying our method. Additionally, we show how the improved transcript can in turn be leveraged to improve the training of a new model",
    "checked": true,
    "id": "bd8261716a5485452f31f0bb7824983308ba3842",
    "semantic_title": "improving phonetic transcriptions of children's speech by pronunciation modelling with constrained ctc-decoding",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/soky22_interspeech.html": {
    "title": "Leveraging Simultaneous Translation for Enhancing Transcription of Low-resource Language via Cross Attention Mechanism",
    "volume": "main",
    "abstract": "This work addresses automatic speech recognition (ASR) of a low-resource language using a translation corpus, which includes the simultaneous translation of the low-resource language. In multi-lingual events such as international meetings and court proceedings, simultaneous interpretation by a human is often available for speeches of low-resource languages. In this setting, we can assume that the content of its back-translation is the same as the transcription of the original speech. Thus, the former is expected to enhance the later process. We formulate this framework as a joint process of ASR and machine translation (MT) and implement it with a combination of cross attention mechanisms of the ASR encoder and the MT encoder. We evaluate the proposed method using the spoken language translation corpus of the Extraordinary Chambers in the Courts of Cambodia (ECCC), achieving a significant improvement in the ASR word error rate (WER) of Khmer by 8.9% relative. The effectiveness is also confirmed in the Fisher-CallHome-Spanish corpus with the reduction of WER in Spanish by 1.7% relative",
    "checked": true,
    "id": "2025f130a947d627fcb3e933e8964e5affe7ae69",
    "semantic_title": "leveraging simultaneous translation for enhancing transcription of low-resource language via cross attention mechanism",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mussakhojayeva22_interspeech.html": {
    "title": "KSC2: An Industrial-Scale Open-Source Kazakh Speech Corpus",
    "volume": "main",
    "abstract": "We present the first industrial-scale open-source Kazakh speech corpus for automatic speech recognition research and development. Our corpus subsumes two previously presented corpora: 1) Kazakh speech corpus (KSC) and 2) Kazakh text-to-speech 2 (KazakhTTS2). We also provide additional data from other sources, including television news, television and radio programs, parliament speeches, and podcasts. Our corpus, which we have named KSC2, contains over a thousand hours of high-quality transcribed data, which is triple the size of KSC. KSC2 was manually transcribed with the help of native Kazakh speakers and validated via preliminary speech recognition experiments on various evaluation sets. Moreover, it contains utterances with Kazakh-Russian code-switching, a conversational practice common among Kazakh speakers. We believe that our corpus will facilitate speech processing research for Kazakh, which is widely considered an under-resourced language. To ensure the reproducibility of experiments, we share the KSC2 corpus, training recipes, and pretrained models",
    "checked": true,
    "id": "e6fb05d5e2d488b15c6fb516d1b66668bf5f15d4",
    "semantic_title": "ksc2: an industrial-scale open-source kazakh speech corpus",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/szalay22_interspeech.html": {
    "title": "Knowledge of accent differences can be used to predict speech recognition",
    "volume": "main",
    "abstract": "If accent differences can predict speech recognition, a smaller dataset systematically representing accent differences might be sufficient and less resource intensive for adapting an automatic speech recognition (ASR) to a novel variety compared to training the ASR on a large, unsystematic dataset. However, it is not known whether ASR errors pattern according to accent differences. Therefore, we tested the performance of Google's General American (GenAm) and Standard Australian English (SAusE) ASR on both dialects using words systematically representing accent differences. Accent differences were quantified using the different number of vowel phonemes, the different phonetic quality of vowels, and differences in rhoticity (i.e., presence/absence of postvocalic /r/). Our results confirm that word recognition is significantly more accurate when ASR dialect matches the speaker dialect compared to the mismatched conditions. Our results reveal that GenAm ASR is less accurate on SAusE speakers due to the higher number of vowel phonemes and to the lack of post-vocalic /r/ in SAusE. Thus, the data need of adapting ASR from GenAm to SAusE might be reduced by using a small dataset focusing on differences in the size of vowel inventory and in rhoticity",
    "checked": true,
    "id": "c701abae7c8adc212629afc4404c68f42047ac75",
    "semantic_title": "knowledge of accent differences can be used to predict speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/scharf22_interspeech.html": {
    "title": "Lombard Effect for Bilingual Speakers in Cantonese and English: importance of spectro-temporal features",
    "volume": "main",
    "abstract": "For a better understanding of the mechanisms underlying speech perception and the contribution of different signal features, computational models of speech recognition have a long tradition in hearing research. Due to the diverse range of situations in which speech needs to be recognized, these models need to be generalizable across many acoustic conditions, speakers, and languages. This contribution examines the importance of different features for speech recognition predictions of plain and Lombard speech for English in comparison to Cantonese in stationary and modulated noise. While Cantonese is a tonal language that encodes information in spectro-temporal features, the Lombard effect is known to be associated with spectral changes in the speech signal. These contrasting properties of tonal languages and the Lombard effect form an interesting basis for the assessment of speech recognition models. Here, an automatic speech recognition-based (ASR) model using spectral or spectro-temporal features is evaluated with empirical data. The results indicate that spectro-temporal features are crucial in order to predict the speaker-specific speech recognition threshold SRT50 in both Cantonese and English as well as to account for the improvement of speech recognition in modulated noise, while effects due to Lombard speech can already be predicted by spectral features",
    "checked": true,
    "id": "a66e72002614ed9ff6e37238c4f650d008d5f744",
    "semantic_title": "lombard effect for bilingual speakers in cantonese and english: importance of spectro-temporal features",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/flechl22_interspeech.html": {
    "title": "End-to-end speech recognition modeling from de-identified data",
    "volume": "main",
    "abstract": "De-identification of data used for automatic speech recognition modeling is a critical component in protecting privacy, especially in the medical domain. However, simply removing all personally identifiable information (PII) from end-to-end model training data leads to a significant performance degradation in particular for the recognition of names, dates, locations, and words from similar categories. We propose and evaluate a two-step method for partially recovering this loss. First, PII is identified, and each occurrence is replaced with a random word sequence of the same category. Then, corresponding audio is produced via text-to-speech or by splicing together matching audio fragments extracted from the corpus. These artificial audio/label pairs, together with speaker turns from the original data without PII, are used to train models. We evaluate the performance of this method on in-house data of medical conversations and observe a recovery of almost the entire performance degradation in the general word error rate while still maintaining a strong diarization performance. Our main focus is the improvement of recall and precision in the recognition of PII-related words. Depending on the PII category, between 50% - 90% of the performance degradation can be recovered using our proposed method",
    "checked": true,
    "id": "28b320060c57a424baac34539f3920398dfecd77",
    "semantic_title": "end-to-end speech recognition modeling from de-identified data",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yadavalli22_interspeech.html": {
    "title": "Multi-Task End-to-End Model for Telugu Dialect and Speech Recognition",
    "volume": "main",
    "abstract": "Conventional Automatic Speech Recognition (ASR) systems are susceptible to dialect variations within a language, thereby adversely affecting the ASR. Therefore, the current practice is to use dialect-specific ASRs. However, dialect-specific information or data is hard to obtain making it difficult to build dialect-specific ASRs. Furthermore, it is cumbersome to maintain multiple dialect-specific ASR systems for each language. We build a unified multi-dialect End-to-End ASR that removes the need for a dialect recognition block and the need to maintain multiple dialect-specific ASRs for three Telugu regional dialects: Telangana, Coastal Andhra, and Rayalaseema. We find that pooling the data and training a multi-dialect ASR benefits the low-resource dialect the most -- an improvement of over 9.71% in relative Word Error Rate (WER). Subsequently, we experiment with multi-task ASRs where the primary task is to transcribe the audio and the secondary task is to predict the dialect. We do this by adding a Dialect ID to the output targets. Such a model outperforms naive multi-dialect ASRs by up to 8.24% in relative WER. Additionally, we test this model on a dialect recognition task and find that it outperforms strong baselines by 6.14% in accuracy",
    "checked": true,
    "id": "ab97a7c27b812fb4c475c3d0f3edaa80f0a04147",
    "semantic_title": "multi-task end-to-end model for telugu dialect and speech recognition",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xie22b_interspeech.html": {
    "title": "DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "Convolutional neural networks (CNN) have improved speech recognition performance greatly by exploiting localized time-frequency patterns. But these patterns are assumed to appear in symmetric and rigid kernels by the conventional CNN operation. It motivates the question: What about asymmetric kernels? In this study, we illustrate adaptive views can discover local features which couple better with attention than fixed views of the input. We replace depthwise CNNs in the Conformer architecture with a deformable counterpart, dubbed this \"Deformer\". By analyzing our best-performing model, we visualize both local receptive fields and global attention maps learned by the Deformer and show increased feature associations on the utterance level. The statistical analysis of learned kernel offsets provides an insight into the change of information in features with the network depth. Finally, replacing only half of the layers in the encoder, the Deformer improves +5.6% relative WER without a LM and +6.4% relative WER with a LM over the Conformer baseline on the WSJ eval92 set",
    "checked": true,
    "id": "aee48b011a22b9e2db56feb499e44e583c2f6d77",
    "semantic_title": "deformer: coupling deformed localized patterns with global context for robust end-to-end speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22_interspeech.html": {
    "title": "Keyword Spotting with Synthetic Data using Heterogeneous Knowledge Distillation",
    "volume": "main",
    "abstract": "It is crucial that Keyword Spotting (KWS) systems learn to understand new classes of user-defined keywords, which however is a challenging task requiring high-quality audio datasets. We propose KWS with Heterogeneous Embedding Knowledge Distillation (HEKD) which uses only synthetic data of unseen keyword classes. In HEKD, a reference model transfers the heterogeneous knowledge on seen classes to the student model for classifying keywords of unseen classes. By mimicking the embedding function of reference model trained on real data via a contrastive learning approach, we show that student model can learn to discriminate unseen keyword classes guided by synthetic data. In addition, we propose to maximize the dispersion of embedding clusters of unseen keywords with approximation guarantees in order to enhance the inter-class variability. Experiments show that HEKD outperforms baseline schemes using few-shot learning and those pre-trained on a large volume of data, demonstrating its effectiveness and efficiency",
    "checked": true,
    "id": "a5a15bd27e334d212818f862846cb508a42ba71f",
    "semantic_title": "keyword spotting with synthetic data using heterogeneous knowledge distillation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deseyssel22_interspeech.html": {
    "title": "Probing phoneme, language and speaker information in unsupervised speech representations",
    "volume": "main",
    "abstract": "Unsupervised models of representations based on Contrastive Predictive Coding (CPC)[1] are primarily used in spoken language modelling in that they encode phonetic information. In this study, we ask what other types of information are present in CPC speech representations. We focus on three categories: phone class, gender and language, and compare monolingual and bilingual models. Using qualitative and quantitative tools, we find that both gender and phone class information are present in both types of models. Language information, however, is very salient in the bilingual model only, suggesting CPC models learn to discriminate languages when trained on multiple languages. Some language information can also be retrieved from monolingual models, but it is more diffused across all features. These patterns hold when analyses are carried on the discrete units from a downstream clustering model. However, although there is no effect of the number of target clusters on phone class and language information, more gender information is encoded with more clusters. Finally, we find that there is some cost to being exposed to two languages on a downstream phoneme discrimination task",
    "checked": true,
    "id": "0f1eb2715913c43d24099013e681c0545cd64c39",
    "semantic_title": "probing phoneme, language and speaker information in unsupervised speech representations",
    "citation_count": 9
  },
  "https://www.isca-speech.org/archive/interspeech_2022/birladeanu22_interspeech.html": {
    "title": "Automatic Detection of Reactive Attachment Disorder Through Turn-Taking Analysis in Clinical Child-Caregiver Sessions",
    "volume": "main",
    "abstract": "To the best of our knowledge, this is the first work aimed at automatic detection of Reactive Attachment Disorder, a psychiatric issue typically affecting children that experienced abuse and neglect. The proposed approach is based on the analysis of turn-taking during clinical sessions and the exper- iments involved 61 children and their caregivers. The results show that it is possible to detect the pathology with accuracy up to 69.2% (F1 Score 68.8%). In addition, the experiments show that the pathology tends to leave different behavioral traces in different activities. This might explain why Reactive Attachment Disorder is difficult to diagnose and tends to re- main undetected. In such a context, methodologies like those proposed in this work can be a valuable support in clinical practice",
    "checked": true,
    "id": "d5dfe4abfe8dcefcd20164e2854b443ec55026b2",
    "semantic_title": "automatic detection of reactive attachment disorder through turn-taking analysis in clinical child-caregiver sessions",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22k_interspeech.html": {
    "title": "Automatic Pronunciation Assessment using Self-Supervised Speech Representation Learning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) approaches such as wav2vec 2.0 and HuBERT models have shown promising results in various downstream tasks in the speech community. In particular, speech representations learned by SSL models have been shown to be effective for encoding various speech-related characteristics. In this context, we propose a novel automatic pronunciation assessment method based on SSL models. First, the proposed method fine-tunes the pre-trained SSL models with connectionist temporal classification to adapt the English pronunciation of English-as-a-second-language (ESL) learners in a data environment. Then, the layer-wise contextual representations are extracted from all across the transformer layers of the SSL models. Finally, the automatic pronunciation score is estimated using bidirectional long short-term memory with the layer-wise contextual representations and the corresponding text. We show that the proposed SSL model-based methods outperform the baselines, in terms of the Pearson correlation coefficient, on datasets of Korean ESL learner children and Speechocean762. Furthermore, we analyze how different representations of transformer layers in the SSL model affect the performance of the pronunciation assessment task",
    "checked": true,
    "id": "5bb169370c6458654031c659f3c1f2837066b0f9",
    "semantic_title": "automatic pronunciation assessment using self-supervised speech representation learning",
    "citation_count": 17
  },
  "https://www.isca-speech.org/archive/interspeech_2022/miller22_interspeech.html": {
    "title": "Exploring Few-Shot Fine-Tuning Strategies for Models of Visually Grounded Speech",
    "volume": "main",
    "abstract": "In this paper, we study models of visually-grounded speech (VGS) in a few-shot setting. Beginning with a model that was pre-trained to associate natural images with speech waveforms describing the images, we probe the model's ability to learn to recognize novel words and their visual referents from a limited number of additional examples. We define new splits for the SpokenCOCO dataset to facilitate few-shot word and object acquisition, explore various few-shot fine-tuning strategies in an effort to mitigate the catastrophic forgetting phenomenon, and identify several techniques that work well in this respect",
    "checked": true,
    "id": "90c89a535eb5750fc0d5198b216d26bfe355b3d0",
    "semantic_title": "exploring few-shot fine-tuning strategies for models of visually grounded speech",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hwang22c_interspeech.html": {
    "title": "Pseudo Label Is Better Than Human Label",
    "volume": "main",
    "abstract": "State-of-the-art automatic speech recognition (ASR) systems are trained with tens of thousands of hours of labeled speech data. Human transcription is expensive and time consuming. Factors such as the quality and consistency of the transcription can greatly affect the performance of the ASR models trained with these data. In this paper, we show that we can train a strong teacher model to produce high quality pseudo labels by utilizing recent self-supervised and semi-supervised learning techniques. Specifically, we use JUST (Joint Unsupervised/Supervised Training) and iterative noisy student teacher training to train a 600 million parameter bi-directional teacher model. This model achieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively better than a baseline. We further show that by using this strong teacher model to generate high-quality pseudo labels for training, we can achieve 13.6% relative WER reduction (5.9% to 5.1%) for a streaming model compared to using human labels",
    "checked": true,
    "id": "5e3ac5c0da335a626cfc296ad9f83ad5400c78c5",
    "semantic_title": "pseudo label is better than human label",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vandermerwe22_interspeech.html": {
    "title": "A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery",
    "volume": "main",
    "abstract": "Latent Dirichlet allocation (LDA) is widely used for unsupervised topic modelling on sets of documents. No temporal information is used in the model. However, there is often a relationship between the corresponding topics of consecutive tokens. In this paper, we present an extension to LDA that uses a Markov chain to model temporal information. We use this new model for acoustic unit discovery from speech. As input tokens, the model takes a discretised encoding of speech from a vector quantised (VQ) neural network with 512 codes. The goal is then to map these 512 VQ codes to 50 phone-like units (topics) in order to more closely resemble true phones. In contrast to the base LDA, which only considers how VQ codes co-occur within utterances (documents), the Markov chain LDA additionally captures how consecutive codes follow one another. This extension leads to an increase in cluster quality and phone segmentation results compared to the base LDA. Compared to a recent vector quantised neural network approach that also learns 50 units, the extended LDA model performs better in phone segmentation but worse in mutual information",
    "checked": true,
    "id": "75c391acf1ee4b581ad92973102a186a1c412e03",
    "semantic_title": "a temporal extension of latent dirichlet allocation for unsupervised acoustic unit discovery",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22_interspeech.html": {
    "title": "PRISM: Pre-trained Indeterminate Speaker Representation Model for Speaker Diarization and Speaker Verification",
    "volume": "main",
    "abstract": "Speaker embedding has been a fundamental feature for speaker-related tasks such as verification, clustering, and diarization. Traditionally, speaker embeddings are represented as fixed vectors in high-dimensional space. This could lead to biased estimations, especially when handling shorter utterances. In this paper we propose to represent a speaker utterance as \"floating\" vector whose state is indeterminate without knowing the context. The state of a speaker representation is jointly determined by itself, other speech from the same speaker, as well as other speakers it is being compared to. The content of the speech also contributes to determining the final state of a speaker representation. We pre-train an indeterminate speaker representation model that estimates the state of an utterance based on the context. The pre-trained model can be fine-tuned for downstream tasks such as speaker verification, speaker clustering, and speaker diarization. Substantial improvements are observed across all downstream tasks",
    "checked": true,
    "id": "d4d3418e6ee67a8c9caa0f13258e010aa9e42db9",
    "semantic_title": "prism: pre-trained indeterminate speaker representation model for speaker diarization and speaker verification",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/qin22_interspeech.html": {
    "title": "Cross-Age Speaker Verification: Learning Age-Invariant Speaker Embeddings",
    "volume": "main",
    "abstract": "Automatic speaker verification has achieved remarkable progress in recent years. However, there is little research on cross-age speaker verification due to insufficient data. In this paper, we mine cross-age test sets based on the VoxCeleb and propose our age-invariant speaker representation learning method. Since the VoxCeleb is collected from the YouTube platform, the dataset consists of cross-age data inherently. However, the meta-data does not contain the speaker age label. Therefore, we adopt the face age estimation method to predict the speaker age value from the associated visual data, then label the audio recording with the estimated age. We construct multiple Cross-Age test sets on VoxCeleb (Vox-CA), which deliberately select the positive trials with large age-gap. Also, the effect of nationality and gender is considered in selecting negative pairs to align with Vox-H cases. The baseline system performance drops from 1.939% EER on the Vox-H test set to 10.419\\% on the Vox-CA20 test set, which indicates how difficult the cross-age scenario is. Consequently, we propose an age-decoupling adversarial learning (ADAL) method to alleviate the negative effect of the age gap and reduce intra-class variance. Our method outperforms the baseline system by over 10% related EER reduction on the Vox-CA20 test set",
    "checked": true,
    "id": "9ad7c2497272d9d08d00791a5a38871f86078626",
    "semantic_title": "cross-age speaker verification: learning age-invariant speaker embeddings",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22j_interspeech.html": {
    "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
    "volume": "main",
    "abstract": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the Alimeeting dataset",
    "checked": true,
    "id": "ecff7c8cc47e861929ad7f1efb371fa5f8996ffa",
    "semantic_title": "online target speaker voice activity detection for speaker diarization",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/brummer22_interspeech.html": {
    "title": "Probabilistic Spherical Discriminant Analysis: An Alternative to PLDA for length-normalized embeddings",
    "volume": "main",
    "abstract": "In speaker recognition, where speech segments are mapped to embeddings on the unit hypersphere, two scoring backends are commonly used, namely cosine scoring or PLDA. Both have advantages and disadvantages, depending on the context. Cosine scoring follows naturally from the spherical geometry, but for PLDA the blessing is mixedlength normalization Gaussianizes the between-speaker distribution, but violates the assumption of a speaker-independent within-speaker distribution. We propose PSDA, an analogue to PLDA that uses Von Mises- Fisher distributions on the hypersphere for both within and between-class distributions. We show how the self-conjugacy of this distribution gives closed-form likelihood-ratio scores, making it a drop-in replacement for PLDA at scoring time. All kinds of trials can be scored, including single-enroll and multienroll verification, as well as more complex likelihood-ratios that could be used in clustering and diarization. Learning is done via an EM-algorithm with closed-form updates. We explain the model and present some first experiments",
    "checked": true,
    "id": "44b7f152dd160aed866fa280528bab2aa120d525",
    "semantic_title": "probabilistic spherical discriminant analysis: an alternative to plda for length-normalized embeddings",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gu22_interspeech.html": {
    "title": "Deep speaker embedding with frame-constrained training strategy for speaker verification",
    "volume": "main",
    "abstract": "Speech signals contain a lot of side information (content,stress, etc.), besides the voiceprint statistics. These sessionvariablilites pose a huge challenge for modeling speaker characteristics. To alleviate this problem, we propose a novel frame-constrained training (FCT) strategy in this paper. It enhances the speaker information in frame-level layers for better embedding extraction. More precisely, a similarity matrix is calculated based on the frame-level features among each batch of the training samples, and a FCT loss is obtained through this similarity matrix. Finally, the speaker embedding network is trained by the combination of the FCT loss and the speaker classification loss. Experiments are performed on the VoxCeleb1 and VOiCES databases. The results demonstrate that the proposed training strategy boosts the system performance",
    "checked": true,
    "id": "4e8e49e0ff33b3839bb1ca80e364af35b4189022",
    "semantic_title": "deep speaker embedding with frame-constrained training strategy for speaker verification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22f_interspeech.html": {
    "title": "Interrelate Training and Searching: A Unified Online Clustering Framework for Speaker Diarization",
    "volume": "main",
    "abstract": "For online speaker diarization, samples arrive incremen tally, and the overall distribution of the samples is invisible. Moreover, in most existing clustering-based methods, the train ing objective of the embedding extractor is not designed spe cially for clustering. To improve online speaker diarization per formance, we propose a unified online clustering framework, which provides an interactive manner between embedding ex tractors and clustering algorithms. Specifically, the framework consists of two highly coupled parts: clustering-guided recur rent training (CGRT) and paths truncated beam search (PTBS). The CGRT introduces the clustering algorithm into the training process of embedding extractors, which could provide not only cluster-aware information for the embedding extractor, but also crucial parameters for the clustering process afterward. And with these parameters, which contain preliminary information of the metric space, the PTBS penalizes the probability score of each cluster, in order to output more accurate clustering re sults in online fashion with low latency. With the above innova tions, our proposed online clustering system achieves 14.48% DER with collar 0.25 at 2.5s latency on the AISHELL-4, while the DER of the offline agglomerative hierarchical clustering is 14.57%",
    "checked": true,
    "id": "50e6ca30d4f0fa787567cf27bdcf3e7e1ea92a2a",
    "semantic_title": "interrelate training and searching: a unified online clustering framework for speaker diarization",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/he22c_interspeech.html": {
    "title": "End-to-End Audio-Visual Neural Speaker Diarization",
    "volume": "main",
    "abstract": "In this paper, we propose a novel end-to-end neural-network-based audio-visual speaker diarization method. Unlike most existing audio-visual methods, our audio-visual model takes audio features (e.g., FBANKs), multi-speaker lip regions of interest (ROIs), and multi-speaker i-vector embbedings as multimodal inputs. And a set of binary classification output layers produces activities of each speaker. With the finely designed end-to-end structure, the proposed method can explicitly handle the overlapping speech and distinguish between speech and non-speech accurately with multi-modal information. I-vectors are the key point to solve the alignment problem caused by visual modality error (e.g., occlusions, off-screen speakers or unreliable detection). Besides, our audio-visual model is robust to the absence of visual modality, where the diarization performance degrades significantly using the visual-only model. Evaluated on the datasets of the first multi-model information based speech processing (MISP) challenge, the proposed method achieved diarization error rates (DERs) of 10.1%/9.5% on development/eval set with reference voice activity detection (VAD) information, while audio-only and video-only system yielded DERs of 27.9%/29.0% and 14.6%/13.1% respectively",
    "checked": true,
    "id": "cdd8119655dbf007f8d4406df997312aff6c92f2",
    "semantic_title": "end-to-end audio-visual neural speaker diarization",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yue22b_interspeech.html": {
    "title": "Online Speaker Diarization with Core Samples Selection",
    "volume": "main",
    "abstract": "We propose a novel online speaker diarization approach based on the VBx algorithm which works well on the offline speaker diarization tasks. To efficiently process long-time recordings, we perform the online diarization in a block-wise manner. First, we devise a core samples updating strategy utilizing time penalty function, which can preserve important historical information with a low memory cost. Then we select clustering samples from core samples by stratified sampling to enhance the variability among samples and retain sufficient speaker identity information, which helps VBx to improve classification accuracy on a small amount of data. Finally, we solve the label ambiguity problem by a global constrained clustering algorithm. We evaluate our system on DIHARD and AMI datasets. The experimental results demonstrate that our online approach achieves superior performance compared with the state-of-the-art",
    "checked": true,
    "id": "bd134b4c036cc22bf12a676854f7370c7e527e60",
    "semantic_title": "online speaker diarization with core samples selection",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22r_interspeech.html": {
    "title": "Robust End-to-end Speaker Diarization with Generic Neural Clustering",
    "volume": "main",
    "abstract": "End-to-end speaker diarization approaches have shown exceptional performance over the traditional modular approaches. To further improve the performance of the end-to-end speaker diarization for real speech recordings, recently works have been proposed which integrate unsupervised clustering algorithms with the end-to-end neural diarization models. However, these methods have a number of drawbacks: 1) The unsupervised clustering algorithms cannot leverage the supervision from the available datasets; 2) The K-means-based unsupervised algorithms that are explored often suffer from the constraint violation problem; 3) There is unavoidable mismatch between the supervised training and the unsupervised inference. In this paper, a robust generic neural clustering approach is proposed that can be integrated with any chunk-level predictor to accomplish a fully supervised end-to-end speaker diarization model. Also, by leveraging the sequence modelling ability of a recurrent neural network, the proposed neural clustering approach can dynamically estimate the number of speakers during inference. Experimental show that when integrating an attractor-based chunk-level predictor, the proposed neural clustering approach can yield better Diarization Error Rate (DER) than the constrained K-means-based clustering approaches under the mismatched conditions",
    "checked": true,
    "id": "f2c1ba8a0490c5dd7da0b09c3c59f3fb544fe30e",
    "semantic_title": "robust end-to-end speaker diarization with generic neural clustering",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22t_interspeech.html": {
    "title": "MSDWild: Multi-modal Speaker Diarization Dataset in the Wild",
    "volume": "main",
    "abstract": "Speaker diarization in real-world acoustic environments is a challenging task of increasing interest from both academia and industry. Although it has been widely accepted that incorporating visual information benefits audio processing tasks such as speech recognition, there is currently no fully released dataset that can be used for benchmarking multi-modal speaker diarization performance in real-world environments. In this paper, we release MSDWild, a benchmark dataset for multi-modal speaker diarization in the wild. The dataset is collected from public videos, covering rich real-world scenarios and languages. All video clips are naturally shot videos without over-editing such as lens switching. Audio and video are both released. In particular, MSDWild has a large portion of the naturally overlapped speech, forming an excellent testbed for cocktail-party problem research. Furthermore, we also conduct baseline experiments on the dataset using audio-only, visual-only, and audio-visual speaker diarization",
    "checked": true,
    "id": "69d5eb9426712c9f0aca76720bbb9355ac51f601",
    "semantic_title": "msdwild: multi-modal speaker diarization dataset in the wild",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tanveer22_interspeech.html": {
    "title": "Unsupervised Speaker Diarization that is Agnostic to Language, Overlap-Aware, and Tuning Free",
    "volume": "main",
    "abstract": "Podcasts are conversational in nature and speaker changes are frequent---requiring speaker diarization for content understanding. We propose an unsupervised technique for speaker diarization without relying on language-specific components. The algorithm is overlap-aware and does not require information about the number of speakers. Our approach shows 79% improvement on purity scores (34% on F-score) against the Google Cloud Platform solution on podcast data",
    "checked": true,
    "id": "2d6c4a067af63b86a62ad09d86ff8f37eed86f0a",
    "semantic_title": "unsupervised speaker diarization that is agnostic to language, overlap-aware, and tuning free",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kinoshita22_interspeech.html": {
    "title": "Utterance-by-utterance overlap-aware neural diarization with Graph-PIT",
    "volume": "main",
    "abstract": "Recent speaker diarization studies showed that integration of end-to-end neural diarization (EEND) and clustering-based diarization is a promising approach for achieving state-of-the-art performance on various tasks. Such an approach first divides an observed signal into fixed-length segments, then performs segment-level local diarization based on an EEND module, and merges the segment-level results via clustering to form a final global diarization result. The segmentation is done to limit the number of speakers in each segment since the current EEND cannot handle many of speakers. In this paper, we argue that such an approach involving the segmentation has several issues; for example, it inevitably faces a dilemma that larger segment sizes increase both the context available for enhancing the performance and the number of speakers for the EEND module to handle. To resolve such a problem, this paper proposes a novel framework that performs diarization without segmentation. However, it can still handle challenging data containing many speakers and a significant amount of overlapping speech. To this end, we leverage a neural network training scheme called Graph-PIT proposed recently for neural source separation. Experiments with simulated active-meeting-like data and CALLHOME data show the superiority of the proposed approach over the conventional methods",
    "checked": true,
    "id": "8500209f5782bc9e89cf082895991f5439637bb9",
    "semantic_title": "utterance-by-utterance overlap-aware neural diarization with graph-pit",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ha_interspeech.html": {
    "title": "Spatial-aware Speaker Diarizaiton for Multi-channel Multi-party Meeting",
    "volume": "main",
    "abstract": "This paper describes a spatial-aware speaker diarization system for the multi-channel multi-party meeting. The diarization system obtains direction information of speaker by microphone array. Speaker-spatial embedding is generated by x-vector and s-vector derived from superdirective beamforming (SDB) which makes the embedding more robust. Specifically, we propose a novel multi-channel sequence-to-sequence neural network architecture named discriminative multi-stream neural network (DMSNet) which consists of attention superdirective beamforming (ASDB) block and Conformer encoder. The proposed ASDB is a self-adapted channel-wise block that extracts the latent spatial features of array audios by modeling interdependencies between channels. We explore DMSNet to address overlapped speech problem on multi-channel audio and achieve 93.53% accuracy on evaluation set. By performing DMSNet based overlapped speech detection (OSD) module, the diarization error rate (DER) of cluster-based diarization system decrease significantly from 13.45% to 7.64%",
    "checked": false,
    "id": "9a6b86edc8e24215e9a47316c70f867d034caf7a",
    "semantic_title": "spatial-aware speaker diarization for multi-channel multi-party meeting",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liang22_interspeech.html": {
    "title": "Selective Pseudo-labeling and Class-wise Discriminative Fusion for Sound Event Detection",
    "volume": "main",
    "abstract": "In recent years, exploring effective sound separation (SSep) techniques to improve overlapping sound event detection (SED) attracts more and more attention. Creating accurate separation signals to avoid the catastrophic error accumulation during SED model training is very important and challenging. In this study, we first propose a novel selective pseudo-labeling approach, termed SPL, to produce high confidence separated target events from blind sound separation outputs. These target events are then used to fine-tune the original SED model that pre-trained on the sound mixtures in a multi-objective learning style. Then, to further leverage the SSep outputs, a class-wise discriminative fusion is proposed to improve the final SED performances, by combining multiple frame-level event predictions of both sound mixtures and their separated signals. All experiments are performed on the public DCASE 2021 Task 4 dataset, and results show that our approaches significantly outperforms the official baseline, the collar-based F 1, PSDS1 and PSDS2 performances are improved from 44.3%, 37.3% and 54.9% to 46.5%, 44.5% and 75.4%, respectively",
    "checked": true,
    "id": "590f4e36ffb41c0afe097b8357596cd2f04affde",
    "semantic_title": "selective pseudo-labeling and class-wise discriminative fusion for sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22d_interspeech.html": {
    "title": "An End-to-End Macaque Voiceprint Verification Method Based on Channel Fusion Mechanism",
    "volume": "main",
    "abstract": "Primates are facing a serious survival crisis. Tracking the range of animal activities and population changes is of great significance for efficient animal protection. Primates are highly alert and inaccessible to humans so that it is difficult to track animals through direct observation, DNA fingerprinting, or marking methods. Primate recognition based on animal calls has the advantages of wide monitoring range, low equipment cost, and good concealment. In this work, we propose an effective macaque speech feature extraction structure, and innovatively propose a feature fusion mechanism to effectively obtain the feature representation of each call. Furthermore, we construct a public open source macaque voiceprint verification dataset. The experimental results show that the proposed method is superior to the existing state-of-the-art human voiceprint verification algorithms with different call durations. The equal error rate (EER) of our macaque voiceprint verification algorithm reaches 6.19%",
    "checked": true,
    "id": "8d4b49e142809bf54a206e27bdee5ac08226518a",
    "semantic_title": "an end-to-end macaque voiceprint verification method based on channel fusion mechanism",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22c_interspeech.html": {
    "title": "Human Sound Classification based on Feature Fusion Method with Air and Bone Conducted Signal",
    "volume": "main",
    "abstract": "The human sound classification task aims at distinguishing different sounds made by human, which can be widely used in medical and health detection area. Different from other sounds in acoustic scene classification task, human sounds can be transmitted either through air or bone conduction. The bone conducted (BC) signal generated by a speaker has strong anti-noise properties and can assist the air conducted (AC) signal to extract additional acoustic features. In this paper, we explore the effect of the BC signal on human sound classification task. Two stream audios combing BC and AC signals are input to a CNN-based model. An attentional feature fusion method suitable for BC and AC signal features is proposed to improve the performance according to the complementarity between the two signal features. Further improvement can be obtained by using a BC signal feature enhancement method. Experiments on an open access and a self-built dataset show that fusing bone conducted signal can achieve 6.2%/17.4% performance improvement over the baseline with only AC signal as input. The results demonstrate the application value of bone conducted signals and the superior performance of the proposed methods",
    "checked": true,
    "id": "c0678040895f0c0656613bada6d243213c4194de",
    "semantic_title": "human sound classification based on feature fusion method with air and bone conducted signal",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22e_interspeech.html": {
    "title": "RaDur: A Reference-aware and Duration-robust Network for Target Sound Detection",
    "volume": "main",
    "abstract": "Target sound detection (TSD) aims to detect the target sound from a mixture audio given the reference information. Previous methods use a conditional network to extract a sound-discriminative embedding from the reference audio, and then use it to detect the target sound from the mixture audio. However, the network performs much differently when using different reference audios (\\text{e.g.} performs poorly for noisy and short-duration reference audios), and tends to make wrong decisions for transient events (\\text{i.e.} shorter than $1$ second). To overcome these problems, in this paper, we present a reference-aware and duration-robust network (RaDur) for TSD. More specifically, in order to make the network more aware of the reference information, we propose an embedding enhancement module to take into account the mixture audio while generating the embedding, and apply the attention pooling to enhance the features of target sound-related frames and weaken the features of noisy frames. In addition, a duration-robust focal loss is proposed to help model different-duration events. To evaluate our method, we build two TSD datasets based on UrbanSound and Audioset. Extensive experiments show the effectiveness of our methods",
    "checked": true,
    "id": "096fd23baa0077520aa9543b47700ee340b03f99",
    "semantic_title": "radur: a reference-aware and duration-robust network for target sound detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tripathi22_interspeech.html": {
    "title": "Temporal Self Attention-Based Residual Network for Environmental Sound Classification",
    "volume": "main",
    "abstract": "Recent years have witnessed a remarkable performance of attention mechanisms for learning representative and prototypical features for tasks such as the classification of distinct sounds and images. Classification of environmental sounds is also an equally challenging task to the classification of speech and music. The presence of semantically irrelevant and silent frames are two major issues that persist in environmental sound classification (ESC). This paper presents a linear self-attention (LSA) mechanism with a learnable memory unit that encodes temporal and spectral characteristics of the spectrogram used while training the deep ESC model. The memory unit can be easily designed using two linear layers followed by a normalization layer. Unlike traditional self-attention mechanisms, the proposed LA mechanism has a linear computational cost. The efficacy of the proposed method is evaluated on two benchmark ESC datasets, viz. ESC-10 and DCASE-2019 Task-1A datasets. The experiments and results show that the model trained with the proposed attention mechanism efficiently learns temporal and spectral information from spectrogram of a signal. The performance of the proposed deep ESC model is comparable or superior to state-of-the-art attention-based deep ESC models",
    "checked": true,
    "id": "84175a6ca1f48ddc45244bb9bd4b2d5566953c87",
    "semantic_title": "temporal self attention-based residual network for environmental sound classification",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22f_interspeech.html": {
    "title": "AudioTagging Done Right: 2nd comparison of deep learning methods for environmental sound classification",
    "volume": "main",
    "abstract": "After its sweeping success in vision and language tasks, pure attention-based neural architectures (e.g. DeiT) are emerging to the top of audio tagging (AT) leaderboards, which seemingly obsoletes traditional convolutional neural networks (CNNs), feed-forward networks or recurrent networks. However, taking a closer look, there is great variability in published research, for instance, performances of models initialized with pretrained weights differ drastically from without pretraining training time for a model varies from hours to weeks, and often, essences are hidden in seemingly trivial details. This urgently calls for a comprehensive study since our 1st comparsion is half-decade old. In this work, we perform extensive experiments on AudioSet~\\cite{gemmeke2017audio} which is the largest weakly-labeled sound event dataset available, we also did analysis based on the data quality and efficiency. We compare a few state-of-the-art baselines on the AT task, and study the performance and efficiency of 2 major categories of neural architectures: CNN variants and attention-based variants. We also closely examine their optimization procedures. Our opensourced experimental results{https://github.com/lijuncheng16/AllaboutAudioSet} provide insights to trade off between performance, efficiency, optimization process, for both practitioners and researchers",
    "checked": true,
    "id": "04cab71e7ff997d32332a633d17abbaa54589764",
    "semantic_title": "audiotagging done right: 2nd comparison of deep learning methods for environmental sound classification",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22i_interspeech.html": {
    "title": "Improving Target Sound Extraction with Timestamp Information",
    "volume": "main",
    "abstract": "Target sound extraction (TSE) aims to extract the sound part of a target sound event class from a mixture audio with multiple sound events. The previous works mainly focus on the problems of weakly-labelled data, jointly learning and new classes, however, no one cares about the onset and offset times of the target sound event, which has been emphasized in the auditory scene analysis. In this paper, we study to utilize such timestamp information to help extract the target sound via a target sound detection network and a target-weighted time-frequency loss function. More specifically, we use the detection result of a target sound detection (TSD) network as the additional information to guide the learning of target sound extraction network. We also find that the result of TSE can further improve the performance of the TSD network, so that a mutual learning framework of the target sound detection and extraction is proposed. In addition, a target-weighted time-frequency loss function is designed to pay more attention to the temporal regions of the target sound during training. Experimental results on the synthesized data generated from the Freesound Datasets show that our proposed method can significantly improve the performance of TSE",
    "checked": true,
    "id": "e44ebc9e293852c6965d685556500566bb6dc1c3",
    "semantic_title": "improving target sound extraction with timestamp information",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22d_interspeech.html": {
    "title": "A Multi-grained based Attention Network for Semi-supervised Sound Event Detection",
    "volume": "main",
    "abstract": "Sound event detection (SED) is an interesting but challenging task due to the scarcity of data and diverse sound events in real life. This paper presents a multi-grained based attention network (MGA-Net) for semi-supervised sound event detection. To obtain the feature representations related to sound events, a residual hybrid convolution (RH-Conv) block is designed to boost the vanilla convolution's ability to extract the time-frequency features. Moreover, a multi-grained attention (MGA) module is designed to learn temporal resolution features from coarse-level to fine-level. With the MGA module, the network could capture the characteristics of target events with short- or long-duration, resulting in more accurately determining the onset and offset of sound events. Furthermore, to effectively boost the performance of the Mean Teacher (MT) method, a spatial shift (SS) module as a data perturbation mechanism is introduced to increase the diversity of data. Experimental results show that the MGA-Net outperforms the published state-of-the-art competitors, achieving 53.27% and 56.96% event-based macro F1 (EB-F1) score, 0.709 and 0.739 polyphonic sound detection score (PSDS) on the validation and public set respectively",
    "checked": true,
    "id": "3a179fecb96aae9abcd19ce1098500732021d814",
    "semantic_title": "a multi-grained based attention network for semi-supervised sound event detection",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22c_interspeech.html": {
    "title": "Temporal coding with magnitude-phase regularization for sound event detection",
    "volume": "main",
    "abstract": "Sound Event Detection (SED) is the challenge of identifying sound events into their temporal boundaries as well as sound category. With recent advances in deep learning, more effective SED techniques are investigated through the annual challenge of Detection and Classification of Acoustic Scenes and Events (DCASE). Most SED systems rely on data-driven learning where a deep neural network is trained to minimize the error between model prediction and the truth. While this framework is generally effective at identifying sound classes present in an audio recording, it results in unreliable estimates of temporal information for identifying sound boundaries. In order to heighten the temporal precision, this paper proposes a novel temporal coding of magnitude and phase for embedding vectors in an intermediate layer. This coding is reflected as a regularization term in the objective function for training the model. The regularization allows magnitude of embedding vectors to increase near event boundaries, which represent the onset and offset points. Simultaneously, each of the boundaries are distinguishable from others using phase difference between two neighboring vectors. This approach results in notable improvement in timing sensitivity compared to a baseline system tested on SED task in the context of DCASE2021 challenge",
    "checked": true,
    "id": "2c502d4f5eeb29bbf282d78d111cab0ed5d4cc00",
    "semantic_title": "temporal coding with magnitude-phase regularization for sound event detection",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shao22_interspeech.html": {
    "title": "RCT: Random consistency training for semi-supervised sound event detection",
    "volume": "main",
    "abstract": "Sound event detection (SED), as a core module of acoustic environmental analysis, suffers from the problem of data deficiency. The integration of semi-supervised learning (SSL) largely mitigates such problem. This paper researches on several core modules of SSL, and introduces a random consistency training (RCT) strategy. First, a hard mixup data augmentation is proposed to account for the additive property of sounds. Second, a random augmentation scheme is applied to stochastically combine different types of data augmentation methods with high flexibility. Third, a self-consistency loss is proposed to be fused with the teacher-student model, aiming at stabilizing the training. Performance-wise, the proposed modules outperform their respective competitors, and as a whole the proposed SED strategies achieve 44.0% and 67.1% in terms of the PSDS_1 and PSDS_2 metrics proposed by the DCASE challenge, which notably outperforms other widely-used alternatives",
    "checked": true,
    "id": "1abffdb65908e8c47a1c694743462a32f58aa13a",
    "semantic_title": "rct: random consistency training for semi-supervised sound event detection",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xin22_interspeech.html": {
    "title": "Audio Pyramid Transformer with Domain Adaption for Weakly Supervised Sound Event Detection and Audio Classification",
    "volume": "main",
    "abstract": "Recently, the Transformer-based model has been applied to sound event detection and audio classification tasks. However, when processing the audio spectrogram on a fine-grained scale, the computational cost is still high even with a hierarchical structure. In this paper, we introduce APT: an audio pyramid transformer with quadtree attention to reduce the computational complexity from quadratic to linear. Besides, most previous methods for weakly supervised sound event detection (WSSED) utilize the multi-instance learning (MIL) mechanism. However, MIL focuses more on the accuracy of bags (clips) rather than the instances (frames), so it tends to localize the most distinct part but not the whole sound event. To solve this problem, we provide a novel perspective that models WSSED as a domain adaption (DA) task, where the weights of the classifier trained on the source (clip) domain are shared to the target (frame) domain to enhance localization performance. Furthermore, we introduce a DAD (domain adaption detection) loss to align the feature distribution between frame and clip domain and make the classifier perceive frame domain information better. Experiments show that our APT achieves new state-of-the-art (SOTA) results on AudioSet, DCASE2017 and Urban-SED datasets. Moreover, our DA-WSSED pipeline significantly outperforms the MIL-based WSSED method",
    "checked": true,
    "id": "9fd67f326cb1f673ebdb53b198596dac101b1f14",
    "semantic_title": "audio pyramid transformer with domain adaption for weakly supervised sound event detection and audio classification",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22aa_interspeech.html": {
    "title": "Active Few-Shot Learning for Sound Event Detection",
    "volume": "main",
    "abstract": "Few-shot learning has shown promising results in sound event detection where the model can learn to recognize novel classes assuming a few labeled examples (typically five) are available at inference time. Most research studies simulate this process by sampling support examples randomly and uniformly from all test data with the target class label. However, in many real-world scenarios, users might not even have five examples at hand or these examples may be from a limited context and not representative, resulting in model performance lower than expected. In this work, we relax these assumptions, and to recover model performance, we propose to use active learning techniques to efficiently sample additional informative support examples at inference time. We developed a novel dataset simulating the long-term temporal characteristics of sound events in real-world environmental soundscapes. Then we ran a series of experiments with this dataset to explore the modeling and sampling choices that arise when combining few-shot learning and active learning, including different training schemes, sampling strategies, models, and temporal windows in sampling",
    "checked": true,
    "id": "355336a907b6c96636a9878de276ac417b47daa2",
    "semantic_title": "active few-shot learning for sound event detection",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ye22b_interspeech.html": {
    "title": "Uncertainty Calibration for Deep Audio Classifiers",
    "volume": "main",
    "abstract": "Although deep Neural Networks (DNNs) have achieved tremendous success in audio classification tasks, their uncertainty calibration are still under-explored. A well-calibrated model should be accurate when it is certain about its prediction and indicate high uncertainty when it is likely to be inaccurate. In this work, we investigate the uncertainty calibration for deep audio classifiers. In particular, we empirically study the performance of popular calibration methods: (i) Monte Carlo dropout, (ii) ensemble, (iii) focal loss, and (iv) spectral-normalized Gaussian process (SNGP), on audio classification datasets. To this end, we evaluate (iiv) for the tasks of environment sound and music genre classification. Results indicate that uncalibrated deep audio classifiers may be over-confident, and SNGP performs the best and is very efficient on the two datasets of this paper",
    "checked": true,
    "id": "f7f3e3b56bbaf2a382a229844e0cc6d1bcb5924b",
    "semantic_title": "uncertainty calibration for deep audio classifiers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hou22c_interspeech.html": {
    "title": "Event-related data conditioning for acoustic event classification",
    "volume": "main",
    "abstract": "Models based on diverse attention mechanisms have recently shined in tasks related to acoustic event classification (AEC). Among them, self-attention is often used in audio-only tasks to help the model recognize different acoustic events. Self-attention relies on the similarity between time frames, and uses global information from the whole segment to highlight specific features within a frame. In real life, information related to acoustic events will attenuate over time, which means the information within some frames around the event deserves more attention than distant time global information that may be unrelated to the event. This paper shows that self-attention may over-enhance certain segments of audio representations, and smooth out the boundaries between events representations and background noises. Hence, this paper proposes an event-related data conditioning (EDC) for AEC. EDC directly works on spectrograms. The idea of EDC is to adaptively select the frame-related attention range based on acoustic features, and gather the event-related local information to represent the frame. Experiments show that: 1) compared with spectrogram-based data augmentation methods and trainable feature weighting and self-attention, EDC outperforms them in both the original-size mode and the augmented mode; 2) EDC effectively gathers event-related local information and enhances boundaries between events and backgrounds, improving the performance of AEC",
    "checked": true,
    "id": "a87e6bd816cdd74318535657dd5f301fc2d52c6e",
    "semantic_title": "event-related data conditioning for acoustic event classification",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22_interspeech.html": {
    "title": "A Multi-Scale Time-Frequency Spectrogram Discriminator for GAN-based Non-Autoregressive TTS",
    "volume": "main",
    "abstract": "The generative adversarial network (GAN) has shown its outstanding capability in improving Non-Autoregressive TTS (NAR-TTS) by adversarially training it with an extra model that discriminates between the real and the generated speech. To maximize the benefits of GAN, it is crucial to find a powerful discriminator that can capture rich distinguishable information. In this paper, we propose a multi-scale time-frequency spectrogram discriminator to help NAR-TTS generate high-fidelity Mel-spectrograms. It treats the spectrogram as a 2D image to exploit the correlation among different components in the time-frequency domain. And a U-Net-based model structure is employed to discriminate at different scales to capture both coarse-grained and fine-grained information. We conduct subjective tests to evaluate the proposed approach. Both multi-scale and time-frequency discriminating bring significant improvement in the naturalness and fidelity. When combining the neural vocoder, it is shown more effective and concise than fine-tuning the vocoder. Finally, we visualize the discriminating maps to compare their difference to verify the effectiveness of multi-scale discriminating",
    "checked": true,
    "id": "24c807a72c3738a4ce58e19ad8de89455fc70153",
    "semantic_title": "a multi-scale time-frequency spectrogram discriminator for gan-based non-autoregressive tts",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yin22_interspeech.html": {
    "title": "RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion",
    "volume": "main",
    "abstract": "This paper proposes a new \"decompose-and-edit\" paradigm for the text-based speech insertion task that facilitates arbitrary-length speech insertion and even full sentence generation. In the proposed paradigm, global and local factors in speech are explicitly decomposed and separately manipulated to achieve high speaker similarity and continuous prosody. Specifically, we proposed to represent the global factors by multiple tokens, which are extracted by cross-attention operation and then injected back by link-attention operation. Due to the rich representation of global factors, we manage to achieve high speaker similarity in a zero-shot manner. In addition, we introduce a prosody smoothing task to make the local prosody factor context-aware and therefore achieve satisfactory prosody continuity. We further achieve high voice quality with an adversarial training stage. In the subjective test, our method achieves state-of-the-art performance in both naturalness and similarity. Audio samples can be found at https://ydcustc.github.io/retrieverTTS-demo/",
    "checked": true,
    "id": "e11ddad54f3ec603170dde67a40812719b9fac00",
    "semantic_title": "retrievertts: modeling decomposed factors for text-based speech insertion",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luong22_interspeech.html": {
    "title": "FlowVocoder: A small Footprint Neural Vocoder based Normalizing Flow for Speech Synthesis",
    "volume": "main",
    "abstract": "Recently, autoregressive neural vocoders have provided remarkable performance in generating high-fidelity speech and have been able to produce synthetic speech in real-time. However, autoregressive neural vocoders such as WaveFlow are capable of modeling waveform signals from mel-spectrogram, the number of parameters of WaveFlow is significant to deploy on edge devices. Though NanoFlow, which has a small number of parameters, is a state-of-the-art autoregressive neural vocoder, the performance of NanoFlow is marginally lower than WaveFlow. Therefore, we propose a new type of autoregressive neural vocoder called FlowVocoder, which has a small memory footprint and is capable of generating high-fidelity audio in real-time. Our proposed model improves the density estimation of flow blocks by utilizing a mixture of Cumulative Distribution Functions (CDF) for bipartite transformation. Hence, the proposed model is capable of modeling waveform signals, while its memory footprint is much smaller than WaveFlow. As shown in experiments, FlowVocoder achieves competitive results with baseline methods in terms of both subjective and objective evaluation, also, it is more suitable for real-time text-to-speech applications",
    "checked": true,
    "id": "6d00691132afa9bf14099cdd59bb9025f6b890c7",
    "semantic_title": "flowvocoder: a small footprint neural vocoder based normalizing flow for speech synthesis",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22c_interspeech.html": {
    "title": "DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders",
    "volume": "main",
    "abstract": "This paper describes DelightfulTTS 2, a new end-to-end architecture for speech synthesis jointly optimizing acoustic model and vocoder modules with phoneme/text and audio data pairs. Current TTS systems usually leverage a cascaded acoustic model and vocoder pipeline with mel-spectrograms as the intermediate representations, which suffer from two limitations: first, the acoustic model and vocoder are separately trained instead of jointly optimized, which incurs cascaded errors; second, the intermediate speech representations (e.g., mel-spectrogram) are predesigned and lose phase information, which are sub-optimal. To solve these problems, in this paper, we develop DelightfulTTS 2, a new end-to-end speech synthesis system with automatically learned speech representations and jointly optimized acoustic model and vocoder. Specifically, 1) We propose a new codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN) to extract intermediate frame-level speech representations (instead of traditional representations like mel-spectrograms) and reconstruct speech waveform. 2) We jointly optimize the acoustic model (based on DelightfulTTS) and the vocoder (the decoder of VQ-GAN), with an auxiliary loss on the acoustic model to predict intermediate speech representations. Experiments show that DelightfulTTS 2 achieves a CMOS gain +0.14 over DelightfulTTS, and more method analyses further verify the effectiveness of the developed system",
    "checked": true,
    "id": "b0a48fcb93c42085ab92289f9216d47e61c34b9f",
    "semantic_title": "delightfultts 2: end-to-end speech synthesis with adversarial vector-quantized auto-encoders",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yuan22_interspeech.html": {
    "title": "AdaVocoder: Adaptive Vocoder for Custom Voice",
    "volume": "main",
    "abstract": "Custom voice is to construct a personal speech synthesis system by adapting the source speech synthesis model to the target model through the target few recordings. The solution to constructing a custom voice is to combine an adaptive acoustic model with a robust vocoder. However, training a robust vocoder usually requires a multi-speaker dataset, which should include various age groups and various timbres, so that the trained vocoder can be used for unseen speakers. Collecting such a multi-speaker dataset is difficult, and the dataset distribution always has a mismatch with the distribution of the target speaker dataset. This paper proposes an adaptive vocoder for custom voice from another novel perspective to solve the above problems. The adaptive vocoder mainly uses a cross-domain consistency loss to solve the overfitting problem encountered by the GAN-based neural vocoder in the transfer learning of few-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN. First, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets, respectively. Then, fine-tune it on the internal dataset VXI-children with few adaptation data. The empirical results show that a high-quality custom voice system can be built by combining a adaptive acoustic model with a adaptive vocoder",
    "checked": true,
    "id": "2f28bab4d2c0b913a12f5f47c591d0e99f0cba94",
    "semantic_title": "adavocoder: adaptive vocoder for custom voice",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22d_interspeech.html": {
    "title": "RefineGAN: Universally Generating Waveform Better than Ground Truth with Highly Accurate Pitch and Intensity Responses",
    "volume": "main",
    "abstract": "Most GAN(Generative Adversarial Network)-based approaches towards high-fidelity waveform generation heavily rely on discriminators to improve their performance. However, GAN methods introduce much uncertainty into the generation process and often result in mismatches of pitch and intensity, which is fatal when it comes to sensitive use cases such as singing voice synthesis(SVS). To address this problem, we propose RefineGAN, a high-fidelity neural vocoder focused on the robustness, pitch and intensity accuracy, and high-speed full-band audio generation. We applyed a pitch-guided refine architecture with a multi-scale spectrogram-based loss function to help stabilize the training process and maintain the robustness of the neural vocoder while using the GAN-based training method. Audio generated using this method shows a better performance in subjective tests when compared with the ground-truth audio. This result shows that the fidelity is even improved during the waveform reconstruction by eliminating defects produced by recording procedures. Moreover, it shows that models trained on a specified type of data can perform on totally unseen language and unseen speaker identically well. Generated sample pairs are provided on https://timedomain-tech.github.io/refinegan/",
    "checked": true,
    "id": "50b4da711035e5ef155f176cb53c19274609b922",
    "semantic_title": "refinegan: universally generating waveform better than ground truth with highly accurate pitch and intensity responses",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/du22b_interspeech.html": {
    "title": "VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature",
    "volume": "main",
    "abstract": "The mainstream neural text-to-speech(TTS) pipeline is a cascade system, including an acoustic model(AM) that predicts acoustic feature from the input transcript and a vocoder that generates waveform according to the given acoustic feature. However, the acoustic feature in current TTS systems is typically mel-spectrogram, which is highly correlated along both time and frequency axes in a complicated way, leading to a great difficulty for the AM to predict. Although high-fidelity audio can be generated by recent neural vocoders from ground-truth(GT) mel-spectrogram, the gap between the GT and the predicted mel-spectrogram from AM degrades the performance of the entire TTS system. In this work, we propose VQTTS, consisting of an AM txt2vec and a vocoder vec2wav, which uses self-supervised vector-quantized(VQ) acoustic feature rather than mel-spectrogram. We redesign both the AM and the vocoder accordingly. In particular, txt2vec basically becomes a classification model instead of a traditional regression model while vec2wav uses an additional feature encoder before HifiGAN generator for smoothing the discontinuous quantized feature. Our experiments show that vec2wav achieves better reconstruction performance than HifiGAN when using self-supervised VQ acoustic feature. Moreover, our entire TTS system VQTTS achieves state-of-the-art performance in terms of naturalness among all current publicly available TTS systems",
    "checked": true,
    "id": "94bcd3cd6557717dd6335b3228d4455ffeabb41b",
    "semantic_title": "vqtts: high-fidelity text-to-speech synthesis with self-supervised vq acoustic feature",
    "citation_count": 30
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mengnan22_interspeech.html": {
    "title": "Improving GAN-based vocoder for fast and high-quality speech synthesis",
    "volume": "main",
    "abstract": "Following tremendous success in the Generative Adversarial Network(GAN), the GAN-based vocoders have recently shown much faster speed in waveform generation. However, the quality of generated speech is slightly inferior, and the real-time factor (RTF) still can't be satisfied in many devices with limited resources. To address the issues, we propose a new GAN-based vocoder model.Firstly, we introduce the Shuffle-Residual Block into the generator to get a lower RTF. Secondly, we propose a Frequency Transformation Block in the discriminator to capture the correlation between different frequency bins in every frame. To the best of our knowledge, our model achieves the lowest RTF of the GAN-based vocoders under the premise of ensuring the speech quality. In our experiments, our model shows a lower RTF with more than 40% improvement and higher speech quality than MB-MelGAN and HiFi-GAN V2",
    "checked": true,
    "id": "c58495bd3e443bc2ffc9b600154f8e19f7ba7d1d",
    "semantic_title": "improving gan-based vocoder for fast and high-quality speech synthesis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yi22_interspeech.html": {
    "title": "SoftSpeech: Unsupervised Duration Model in FastSpeech 2",
    "volume": "main",
    "abstract": "In this paper, we propose a neural Text-To-Speech (TTS) system SoftSpeech, which employs a novel soft length regulated duration attention based decoder. It learns the encoder output mapping to decoder output simultaneously from an unsupervised duration model (Soft-LengthRegulator) without the requirement of external duration information. The Soft-LengthRegulator consists of a Feed-Forward Transformer (FFT) block with Conditional Layer Normalization (CLN), following a learned upsampling layer with multi-head attention and guided multi-head attention constraint, and it is integrated in each decoder layer and achieves accelerated training convergence and better naturalness within FastSpeech 2 framework. Soft Dynamic Time Warping (Soft-DTW) is adopted to align the mismatch spectrogram loss. Moreover, a Fine-Grained style Variational AutoEncoder (VAE) is designed to further improve the naturalness of synthesized speech. The experiments show SoftSpeech outperforms FastSpeech 2 in subjective tests, and can be successfully applied to minority languages with low resources",
    "checked": true,
    "id": "d5509f4cada21ca15be9627ce840233c9c19bd95",
    "semantic_title": "softspeech: unsupervised duration model in fastspeech 2",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22d_interspeech.html": {
    "title": "A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS",
    "volume": "main",
    "abstract": "We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high performance neural TTS synthesis. A vector-quantized, variational autoencoder (VQ-VAE) based feature analyzer is used to encode Mel spectrograms of speech training data by down-sampling progressively in multiple stages into MSMC Representations (MSMCRs) with different time resolutions, and quantizing them with multiple VQ codebooks, respectively. Multi-stage predictors are trained to map the input text sequence to MSMCRs progressively by minimizing a combined loss of the reconstruction Mean Square Error (MSE) and \"triplet loss\". In synthesis, the neural vocoder converts the predicted MSMCRs into final speech waveforms. The proposed approach is trained and tested with an English TTS database of 16 hours by a female speaker. The proposed TTS achieves an MOS score of 4.41, which outperforms the baseline with an MOS of 3.62. Compact versions of the proposed TTS with much less parameters can still preserve high MOS scores. Ablation studies show that both multiple stages and multiple codebooks are effective for achieving high TTS performance",
    "checked": true,
    "id": "d0f8f93bd75c7b4b1536e0747f70681639b1746f",
    "semantic_title": "a multi-stage multi-codebook vq-vae approach to high-performance neural tts",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22q_interspeech.html": {
    "title": "SiD-WaveFlow: A Low-Resource Vocoder Independent of Prior Knowledge",
    "volume": "main",
    "abstract": "Flow-based neural vocoders have demonstrated their effectiveness in generating high-fidelity speech in real-time. However, most flow-based vocoders are computationally heavy models which rely on large amounts of speech for model training. Witnessing the limitations of these vocoders, a new flowbased vocoder, namely Semi-inverse DynamicWaveFlow (SiDWaveFlow), for low-resource speech synthesis is proposed. SiDWaveFlow can generate high-quality speech in real-time with the constraint of limited training data. Specifically, in SiDWaveFlow, a module named Semi-inverse Dynamic Transformation (SiDT) is proposed to improve the synthesis quality as well as the computational efficiency by replacing the affine coupling layers (ACL) used in WaveGlow. In addition, a preemphasis operation is introduced to the training process of SiD-WaveFlow to further improve the quality of the synthesized speech. Experimental results have corroborated that SiDWaveFlow can generate speech with better quality compared with its counterparts. Particularly, the TTS system integrating SiD-WaveFlow vocoder achieves 3.416 and 2.968 mean MOS on CSMSC and LJ Speech data sets, respectively. Besides, SiDWaveFlow converges much faster than WaveGlow at the training stage. Last but not least, SiD-WaveFlow is a lightweight model and can generate speech on edge devices with a much faster inference speed. The source code and demos are available at https://slptongji.github.io/",
    "checked": true,
    "id": "d208bfde714ac334d9f63e92e797d345d10f06a8",
    "semantic_title": "sid-waveflow: a low-resource vocoder independent of prior knowledge",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gorai22_interspeech.html": {
    "title": "Text-to-speech synthesis using spectral modeling based on non-negative autoencoder",
    "volume": "main",
    "abstract": "This paper proposes a statistical parametric speech synthesis system that uses non-negative autoencoder (NAE) for spectral modeling. NAE is a model that extends non-negative matrix factorization (NMF) as neural networks. In the proposed method, we employ latent variables in NAE as acoustic features. Reconstruction of spectral information and estimation of latent variables are simultaneously trained. The non-negativity of latent variables in NAE is expected to contribute to dimensionality reduction such that the fine structure of the spectral envelopes is preserved. Experimental results demonstrates the effectiveness of the proposed framework. We also study multispeaker modeling where each of NAEs corresponds to each single speaker. In addition, a neural source-filter (NSF) model was applied to the waveform generation. When a neural vocoder is trained with natural acoustic features and tested with synthesized features, quality degradation occurs due to the mismatch between training and test data. In order to mitigate the mismatch, this system uses features obtained by reconstructing natural speech using NAE for training. Experimental results show that reconstructed features are similar to synthesized features, and as a result, the quality of the synthesized speech is improved",
    "checked": true,
    "id": "81b3861ea8f489fb0a45d70d72a29c1a503497e8",
    "semantic_title": "text-to-speech synthesis using spectral modeling based on non-negative autoencoder",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kanagawa22_interspeech.html": {
    "title": "Joint Modeling of Multi-Sample and Subband Signals for Fast Neural Vocoding on CPU",
    "volume": "main",
    "abstract": "In this work, we propose a fast and high quality neural vocoder for CPU implementation. The main approaches to realize fast inference via an autoregressive model are 1) a subband-based vocoder and 2) multiple samples prediction. Our previous work demonstrated that the combination worked well up to two samples simultaneous generation without quality degradation. To further increase the number of simultaneous samples while maintaining quality, we focus on the existence of an association between subband signals and multiple samples. Our proposed vocoder jointly models these associations with a multivariate Gaussian. Experimentals show that our proposed four-sample vocoder is 1.47 times faster than the conventional two-sample equivalent. For both the acoustic features extracted from natural speech and those predicted by TTS, the proposed method realizes generation with up to four samples without any significant degradation in naturalness. This vocoder also matched the naturalness comparable of the two-sample conventional method",
    "checked": true,
    "id": "2a148a83bd6c8f8d53e92cd84b69e15f5883e796",
    "semantic_title": "joint modeling of multi-sample and subband signals for fast neural vocoding on cpu",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kaneko22_interspeech.html": {
    "title": "MISRNet: Lightweight Neural Vocoder Using Multi-Input Single Shared Residual Blocks",
    "volume": "main",
    "abstract": "Neural vocoders have recently become popular in text-to-speech synthesis and voice conversion, increasing the demand for efficient neural vocoders. One successful approach is HiFi-GAN, which archives high-fidelity audio synthesis using a relatively small model. This characteristic is obtained using a generator incorporating multi-receptive field fusion (MRF) with multiple branches of residual blocks, allowing the expansion of the description capacity with few-channel convolutions. However, MRF requires the model size to increase with the number of branches. Alternatively, we propose a network called MISRNet, which incorporates a novel module called multi-input single shared residual block (MISR). MISR enlarges the description capacity by enriching the input variation using lightweight convolutions with a kernel size of 1 and, alternatively, reduces the variation of residual blocks from multiple to single. Because the model size of the input convolutions is significantly smaller than that of the residual blocks, MISR reduces the model size compared with that of MRF. Furthermore, we introduce an implementation technique for MISR, where we accelerate the processing speed by adopting tensor reshaping. We experimentally applied our ideas to lightweight variants of HiFi-GAN and iSTFTNet, making the models more lightweight with comparable speech quality and without compromising speed",
    "checked": true,
    "id": "303a1ae0d90bd843635298634fa6471a3f34db29",
    "semantic_title": "misrnet: lightweight neural vocoder using multi-input single shared residual blocks",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/miao22b_interspeech.html": {
    "title": "A compact transformer-based GAN vocoder",
    "volume": "main",
    "abstract": "Recent work has shown that self-attention module in Transformer architecture is an effective way of modeling natural languages and images. In this work, we propose a novel way for audio synthesis using Self-Attention Network (SAN). To the best of our knowledge, there is no successful application of Transformer architecture or SAN in high-fidelity waveform generation tasks. The main challenge in adapting SAN to audio generation tasks lies in its quadratic growth of the computational complexity with respect to the input sequence length, making it impractical with high-resolution audio tasks. To tackle this problem, we apply dilated sliding window to vanilla SAN. This technique enables our model to have large receptive field, linear computational complexity and extremely small footprint. We experimentally show that the proposed model archives smaller model size, while producing audio samples with comparable speech quality in comparison with the best publicly available model. In particular, our small footprint model has only 0.57M parameters and can generate 22.05kHz high-fidelity audio 113 times faster than real-time on a NVIDIA V100 GPU without engineered inference kernels",
    "checked": true,
    "id": "032bb3ad257badaeda15bfd735c08076a01bca34",
    "semantic_title": "a compact transformer-based gan vocoder",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tachibana22_interspeech.html": {
    "title": "Diffusion Generative Vocoder for Fullband Speech Synthesis Based on Weak Third-order SDE Solver",
    "volume": "main",
    "abstract": "Diffusion generative models, which generate data by the time-reverse dynamics of diffusion processes, have attracted much attention recently, and have already been applied in the speech domain such as speech waveform synthesis. Diffusion generative models initially had the disadvantage of slow synthesis, but many fast samplers have been proposed and this disadvantage is being overcome. The authors have also proposed an efficient sampler based on a second-order approximation derived from the It-Taylor series, and have achieved some success. This study further examines the possibility of incorporating third-order terms and experimentally verifies that a vocoder using this method can synthesize high-fidelity fullband (48 kHz) speech signals faster than in real time. It is also shown that the method is applicable to the extension of speech bandwidth from wideband (16 kHz) to fullband (48 kHz)",
    "checked": true,
    "id": "217f00cd3529cbf7b77c2c49f0f7b4ed4a89dd1d",
    "semantic_title": "diffusion generative vocoder for fullband speech synthesis based on weak third-order sde solver",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/variani22_interspeech.html": {
    "title": "On Adaptive Weight Interpolation of the Hybrid Autoregressive Transducer",
    "volume": "main",
    "abstract": "This paper explores rescoring strategies to improve a two-pass speech recognition system when first-pass is a hybrid autoregressive transducer model and second-pass is a neural language model. The main focus is on the scores provided by each of these models, their quantitative analysis, how to improve them and the best way to integrate them with the objective of better recognition accuracy. Several analyses are presented to emphasise the importance of the choice of the integration weights for combining the first-pass and the second-pass scores. A sequence level combination weight estimation model along with four training criteria are proposed which allows adaptive integration of the scores per acoustic sequence. The effectiveness of this algorithm is demonstrated by constructing and analyzing models on the Librispeech data set",
    "checked": true,
    "id": "43e80ee47aff9105b67e18876ecf7ba6a2e612b6",
    "semantic_title": "on adaptive weight interpolation of the hybrid autoregressive transducer",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22_interspeech.html": {
    "title": "Learning to rank with BERT-based confidence models in ASR rescoring",
    "volume": "main",
    "abstract": "We propose a learning-to-rank (LTR) approach to the ASR rescoring problem. The proposed LTR framework has the flexibility of embracing wide varieties of linguistic, semantic, and implicit user feedback signals in rescoring process. BERT-based confidence models (CM) taking account of both acoustic and text information are also proposed to provide features better representing hypothesis quality to the LTR models. We show the knowledge of the entire N-best list is crucial for the confidence and LTR models to achieve best rescoring results. Experimental results on de-identified Alexa data show the proposed LTR framework provides an additional 5.16% relative word error rate reduction (WERR) on top of a neural language model rescored ASR system. On LibriSpeech, a 9.38 % WERR and a 13.63 % WERR are observed on the test-clean and test-other sets, respectively",
    "checked": true,
    "id": "10dfa30e8d021f2ad54f3a3cd1abddb6db33b893",
    "semantic_title": "learning to rank with bert-based confidence models in asr rescoring",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22b_interspeech.html": {
    "title": "VQ-T: RNN Transducers using Vector-Quantized Prediction Network States",
    "volume": "main",
    "abstract": "Beam search, which is the dominant ASR decoding algorithm for end-to-end models, generates tree-structured hypotheses. However, recent studies have shown that decoding with hypothesis merging can achieve a more efficient search with comparable or better performance. But, the full context in recurrent networks is not compatible with hypothesis merging. We propose to use vector-quantized long short-term memory units (VQ-LSTM) in the prediction network of RNN transducers. By training the discrete representation jointly with the ASR network, hypotheses can be actively merged for lattice generation. Our experiments on the Switchboard corpus show that the proposed VQ RNN transducers improve ASR performance over transducers with regular prediction networks while also producing denser lattices with a very low oracle word error rate (WER) for the same beam size. Additional language model rescoring experiments also demonstrate the effectiveness of the proposed lattice generation scheme",
    "checked": true,
    "id": "395240778e8961d1a3eb1491ed0a15b2d0f10138",
    "semantic_title": "vq-t: rnn transducers using vector-quantized prediction network states",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22g_interspeech.html": {
    "title": "WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit",
    "volume": "main",
    "abstract": "Recently, we made available WeNet [1], a production-oriented end-to-end speech recognition toolkit, which introduces a unified two-pass (U2) framework and a built-in runtime to address the streaming and non-streaming decoding modes in a single model. To further improve ASR performance and facilitate various production requirements, in this paper, we present WeNet 2.0 with four important updates. (1) We propose U2++, a unified two-pass framework with bidirectional attention decoders, which includes the future contextual information by a right-to-left attention decoder to improve the representative ability of the shared encoder and the performance during the rescoring stage. (2) We introduce an n-gram based language model and a WFST-based decoder into WeNet 2.0, promoting the use of rich text data in production scenarios. (3) We design a unified contextual biasing framework, which leverages user-specific context (e.g., contact lists) to provide rapid adaptation ability for production and improves ASR accuracy in both with-LM and without-LM scenarios. (4) We design a unified IO to support large-scale data for effective model training. In summary, the brand-new WeNet 2.0 achieves up to 10% relative recognition performance improvement over the original WeNet on various corpora and makes available several important production-oriented features",
    "checked": true,
    "id": "fd84c6d7b4d7ec6b38d7d412b0d796637d3c232b",
    "semantic_title": "wenet 2.0: more productive end-to-end speech recognition toolkit",
    "citation_count": 29
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22j_interspeech.html": {
    "title": "Internal Language Model Estimation Through Explicit Context Vector Learning for Attention-based Encoder-decoder ASR",
    "volume": "main",
    "abstract": "An end-to-end (E2E) ASR model implicitly learns a prior Internal Language Model (ILM) from the training transcripts. To fuse an external LM using Bayes posterior theory, the log-likelihood produced by the ILM has to be accurately estimated and subtracted. In this paper we propose two novel approaches to estimate the ILM based on Listen-Attend-Spell (LAS) framework. The first method is to replace the context vector of the LAS decoder at every time step with a vector that is learned with training transcripts. Furthermore, we propose another method that uses a lightweight feed-forward network to directly map query vector to context vector in a dynamic sense. Since the context vectors are learned by minimizing the perplexities on training transcripts, and their estimation is independent of encoder output, hence the ILMs are accurately learned for both methods. Experiments show that the ILMs achieve the lowest perplexity, indicating the efficacy of the proposed methods. In addition, they also significantly outperform the shallow fusion method, as well as two previously proposed ILM Estimation (ILME) approaches on several datasets",
    "checked": true,
    "id": "9c40cad18c14b26bbd29445d7eccb9f0c40a6ec9",
    "semantic_title": "internal language model estimation through explicit context vector learning for attention-based encoder-decoder asr",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22i_interspeech.html": {
    "title": "Improving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies",
    "volume": "main",
    "abstract": "There is often a trade-off between performance and latency in streaming automatic speech recognition (ASR). Traditional methods such as look-ahead and chunk-based methods, usually require information from future frames to advance recognition accuracy, which incurs inevitable latency even if the computation is fast enough. A causal model that computes without any future frames can avoid this latency, but its performance is significantly worse than traditional methods. In this paper, we propose corresponding revision strategies to improve the causal model. Firstly, we introduce a real-time encoder states revision strategy to modify previous states. Encoder forward computation starts once the data is received and revises the previous encoder states after several frames, which is no need to wait for any right context. Furthermore, a CTC spike position alignment decoding algorithm is designed to reduce time costs brought by the proposed revision strategy. Experiments are all conducted on Librispeech datasets. Fine-tuning on the CTC-based wav2vec2.0 model, our best method can achieve 3.7/9.2 WERs on test-clean/other sets and brings 45% relative improvement for causal models, which is also competitive with the chunk-based methods and the knowledge distillation methods",
    "checked": true,
    "id": "2e028b524419d8042edea841d986a61cf6621e88",
    "semantic_title": "improving streaming end-to-end asr on transformer-based causal models with encoder states revision strategies",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bai22_interspeech.html": {
    "title": "Parameter-Efficient Conformers via Sharing Sparsely-Gated Experts for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "While transformers and their variant conformers show promising performance in speech recognition, the over-parameterized property leads to the much memory cost during training and inference. Some works use cross-layer weight-sharing to reduce the parameters of the model. However, the inevitable loss of capacity affects the model performance. To address this issue, this paper proposes a parameter-efficient conformer via sharing sparsely-gated experts. Specifically, we use sparsely-gated mixture-of-experts (MoE) to extend the capacity of a conformer block without increasing computation. Then, the parameters of the grouped conformer blocks are shared so that the number of parameters is reduced. Next, to ensure the shared blocks with the flexibility of adapting representations at different levels, we design the MoE routers and normalization individually. Moreover, we use knowledge distillation to further improve the performance. Experimental results show that the proposed model achieves competitive performance with 1/3 of the parameters of the encoder, compared with the full-parameter model",
    "checked": true,
    "id": "79b7f6965242af82405ad47647450f6684c0d121",
    "semantic_title": "parameter-efficient conformers via sharing sparsely-gated experts for end-to-end speech recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22n_interspeech.html": {
    "title": "CaTT-KWS: A Multi-stage Customized Keyword Spotting Framework based on Cascaded Transducer-Transformer",
    "volume": "main",
    "abstract": "Customized keyword spotting~(KWS) has great potential to be deployed on edge devices to achieve hands-free user experience. However, in real applications, false alarm (FA) would be a serious problem for spotting dozens or even hundreds of keywords, which drastically affects user experience. To solve this problem, in this paper, we leverage the recent advances in transducer and transformer based acoustic models and propose a new multi-stage customized KWS framework named Cascaded Transducer-Transformer KWS~(CaTT-KWS), which includes a transducer based keyword detector, a frame-level phone predictor based force alignment module and a transformer based decoder. Specifically, the streaming transducer module is used to spot keyword candidates in audio stream. Then force alignment is implemented using the phone posteriors predicted by the phone predictor to finish the first stage keyword verification and refine the time boundaries of keyword. Finally, the transformer decoder further verifies the triggered keyword. Our proposed CaTT-KWS framework reduces FA rate effectively without obviously hurting keyword recognition accuracy. Specifically, we can get impressively 0.13 FA per hour on a challenging dataset, with over 90\\% relative reduction on FA comparing to the transducer based detection model, while keyword recognition accuracy only drops less than 2%",
    "checked": true,
    "id": "69cc5eee99de520210a839447edee7bd83cf0c7c",
    "semantic_title": "catt-kws: a multi-stage customized keyword spotting framework based on cascaded transducer-transformer",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22t_interspeech.html": {
    "title": "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT",
    "volume": "main",
    "abstract": "Self-supervised speech representation learning has shown promising results in various speech processing tasks. However, the pre-trained models, e.g., HuBERT, are storage-intensive Transformers, limiting their scope of applications under low-resource settings. To this end, we propose LightHuBERT, a once-for-all Transformer compression framework, to find the desired architectures automatically by pruning structured parameters. More precisely, we create a Transformer-based supernet that is nested with thousands of weight-sharing subnets and design a two-stage distillation strategy to leverage the contextualized latent representations from HuBERT. Experiments on automatic speech recognition (ASR) and the SUPERB benchmark show the proposed LightHuBERT enables over 10^9 architectures concerning the embedding dimension, attention dimension, head number, feed-forward network ratio, and network depth. LightHuBERT outperforms the original HuBERT on ASR and five SUPERB tasks with the HuBERT size, achieves comparable performance to the teacher model in most tasks with a reduction of 29% parameters, and obtains a 3.5x compression ratio in three SUPERB tasks, e.g., automatic speaker verification, keyword spotting, and intent classification, with a slight accuracy loss. The code and pre-trained models are available at https://github.com/mechanicalsea/lighthubert",
    "checked": true,
    "id": "d77f01d3df9e2f1fd05f5bd51eb283afa691bb27",
    "semantic_title": "lighthubert: lightweight and configurable speech representation learning with once-for-all hidden-unit bert",
    "citation_count": 34
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rathod22_interspeech.html": {
    "title": "Multi-stage Progressive Compression of Conformer Transducer for On-device Speech Recognition",
    "volume": "main",
    "abstract": "The smaller memory bandwidth in smart devices prompts development of smaller Automatic Speech Recognition (ASR) models. To obtain a smaller model, one can employ the model compression techniques. Knowledge distillation (KD) is a popular model compression approach that has shown to achieve smaller model size with relatively lesser degradation in the model performance. In this approach, knowledge is distilled from a trained large size teacher model to a smaller size student model. Also, the transducer based models have recently shown to perform well for on-device streaming ASR task, while the conformer models are efficient in handling long term dependencies. Hence in this work we employ a streaming transducer architecture with conformer as the encoder. We propose a multi-stage progressive approach to compress the conformer transducer model using KD. We progressively update our teacher model with the distilled student model in a multi-stage setup. On standard LibriSpeech dataset, our experimental results have successfully achieved compression rates greater than 60% without significant degradation in the performance compared to the larger teacher model",
    "checked": true,
    "id": "f83d405242b6b024eeb7684dc836ba93e98b22c7",
    "semantic_title": "multi-stage progressive compression of conformer transducer for on-device speech recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weiran22b_interspeech.html": {
    "title": "Streaming Align-Refine for Non-autoregressive Deliberation",
    "volume": "main",
    "abstract": "We propose a streaming non-autoregressive (non-AR) decoding algorithm to deliberate the hypothesis alignment of a streaming RNN-T model. Our algorithm facilitates a simple greedy decoding procedure, and at the same time is capable of producing the decoding result at each frame with limited right context, thus enjoying both high efficiency and low latency. These advantages are achieved by converting the offline Align-Refine algorithm to be streaming-compatible, with a novel transformer decoder architecture that performs local self-attentions for both text and audio, and a time-aligned cross-attention at each layer. Furthermore, we perform discriminative training of our model with the minimum word error rate (MWER) criterion, which has not been done in the non-AR decoding literature. Experiments on voice search datasets and Librispeech show that with reasonable right context, our streaming model performs as well as the offline counterpart, and discriminative training leads to further WER gain when the first-pass model has small capacity",
    "checked": true,
    "id": "3f331faed41f8aa1810f4de71f29b5d3234fea6c",
    "semantic_title": "streaming align-refine for non-autoregressive deliberation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22d_interspeech.html": {
    "title": "Federated Pruning: Improving Neural Network Efficiency with Federated Learning",
    "volume": "main",
    "abstract": "Automatic Speech Recognition models require large amount of speech data for training, and the collection of such data often leads to privacy concerns. Federated learning has been widely used and is considered to be an effective decentralized tech- nique by collaboratively learning a shared prediction model while keeping the data local on different clients devices. How- ever, the limited computation and communication resources on clients devices present practical difficulties for large models. To overcome such challenges, we propose Federated Pruning to train a reduced model under the federated setting, while main- taining similar performance compared to the full model. More- over, the vast amount of clients data can also be leveraged to im- prove the pruning results compared to centralized training. We explore different pruning schemes and provide empirical evi- dence of the effectiveness of our methods",
    "checked": true,
    "id": "03da635ad2b31badf4d6a7e3ccfc713cb48cf0f0",
    "semantic_title": "federated pruning: improving neural network efficiency with federated learning",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ding22b_interspeech.html": {
    "title": "A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes",
    "volume": "main",
    "abstract": "In this paper, we propose a dynamic cascaded encoder Automatic Speech Recognition (ASR) model, which unifies models for different deployment scenarios. Moreover, the model can significantly reduce model size and power consumption without loss of quality. Namely, with the dynamic cascaded encoder model, we explore three techniques to maximally boost the performance of each model size: 1) Use separate decoders for each sub-model while sharing the encoders; 2) Use funnel-pooling to improve the encoder efficiency; 3) Balance the size of causal and non-causal encoders to improve quality and fit deployment constraints. Overall, the proposed large-medium model has 30% smaller size and reduces power consumption by 33%, compared to the baseline cascaded encoder model. The triple-size model that unifies the large, medium, and small models achieves 37% total size reduction with minimal quality loss, while substantially reducing the engineering efforts of having separate models",
    "checked": true,
    "id": "3f28feadf272523190d4c3a451966e39a2cfbf41",
    "semantic_title": "a unified cascaded encoder asr model for dynamic model sizes",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ding22c_interspeech.html": {
    "title": "4-bit Conformer with Native Quantization Aware Training for Speech Recognition",
    "volume": "main",
    "abstract": "Reducing the latency and model size has always been a significant research problem for live Automatic Speech Recognition (ASR) application scenarios. Along this direction, model quantization has become an increasingly popular approach to compress neural networks and reduce computation cost. Most of the existing practical ASR systems apply post-training 8-bit quantization. To achieve a higher compression rate without introducing additional performance regression, in this study, we propose to develop 4-bit ASR models with native quantization aware training, which leverages native integer operations to effectively optimize both training and inference. We conducted two experiments on state-of-the-art Conformer-based ASR models to evaluate our proposed quantization technique. First, we explored the impact of different precisions for both weight and activation quantization on the LibriSpeech dataset, and obtained a lossless 4-bit Conformer model with 7.7x size reduction compared to the float32 model. Following this, we for the first time investigated and revealed the viability of 4-bit quantization on a practical ASR system that is trained with large-scale datasets, and produced a lossless Conformer ASR model with mixed 4-bit and 8-bit weights that has 5x size reduction compared to the float32 model",
    "checked": true,
    "id": "9ee40dc961d319d615bc250ef631aa0c86516ce5",
    "semantic_title": "4-bit conformer with native quantization aware training for speech recognition",
    "citation_count": 11
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22i_interspeech.html": {
    "title": "Self-Distillation Based on High-level Information Supervision for Compressing End-to-End ASR Model",
    "volume": "main",
    "abstract": "Model compression of ASR aims to reduce the model parameters while bringing as little performance degradation as possible. Knowledge Distillation (KD) is an efficient model compression method that transfers the knowledge from a large teacher model to a smaller student model. However, most of the existing KD methods study how to fully utilize the teacher's knowledge without paying attention to the student's own knowledge. In this paper, we explore whether the high-level information of the model itself is helpful for low-level information. We first propose neighboring feature self-distillation (NFSD) approach to distill the knowledge from the adjacent deeper layer to the shallow one, which shows significant performance improvement. Therefore, we further propose attention-based feature self-distillation (AFSD) approach to exploit more high-level information. Specifically, AFSD fuses the knowledge from multiple deep layers with an attention mechanism and distills it to a shallow one. The experimental results on AISHELL-1 dataset show that 7.3% and 8.3% relative character error rate (CER) reduction can be achieved from NFSD and AFSD, respectively. In addition, our proposed two approaches can be easily combined with the general teacher-student knowledge distillation method to achieve 12.4% and 13.4% relative CER reduction compared with the baseline student model, respectively",
    "checked": true,
    "id": "446645106b13bebcf3fb4443316851739ef67973",
    "semantic_title": "self-distillation based on high-level information supervision for compressing end-to-end asr model",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jia22b_interspeech.html": {
    "title": "Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation",
    "volume": "main",
    "abstract": "End-to-end speech-to-speech translation (S2ST) without relying on intermediate text representations is a rapidly emerging frontier of research. Recent works have demonstrated that the performance of such direct S2ST systems is approaching that of conventional cascade S2ST when trained on comparable datasets. However, in practice, the performance of direct S2ST is bounded by the availability of paired S2ST training data. In this work, we explore multiple approaches for leveraging much more widely available unsupervised and weakly-supervised speech and text data to improve the performance of direct S2ST based on Translatotron 2. With our most effective approaches, the average translation quality of direct S2ST on 21 language pairs on the CVSS-C corpus is improved by +13.6 BLEU (or +113% relatively), as compared to the previous state-of-the-art trained without additional data. The improvements on low-resource language are even more significant (+398% relatively on average). Our comparative studies suggest future research directions for S2ST and speech representation learning",
    "checked": true,
    "id": "8abe3d1199f6cf3ee5bee943b4a0b6bb9ca91a20",
    "semantic_title": "leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22_interspeech.html": {
    "title": "A High-Quality and Large-Scale Dataset for English-Vietnamese Speech Translation",
    "volume": "main",
    "abstract": "In this paper, we introduce a high-quality and large-scale benchmark dataset for English-Vietnamese speech translation with 508 audio hours, consisting of 331K triplets of (sentence-lengthed audio, English source transcript sentence, Vietnamese target subtitle sentence). We also conduct empirical experiments using strong baselines and find that the traditional \"Cascaded\" approach still outperforms the modern \"End-to-End\" approach. To the best of our knowledge, this is the first large-scale English-Vietnamese speech translation study. We hope both our publicly available dataset and study can serve as a starting point for future research and applications on English-Vietnamese speech translation",
    "checked": true,
    "id": "5ea290e2b3cf99f4b641a2750fec84c847e97498",
    "semantic_title": "a high-quality and large-scale dataset for english-vietnamese speech translation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22f_interspeech.html": {
    "title": "Investigating Parameter Sharing in Multilingual Speech Translation",
    "volume": "main",
    "abstract": "End-to-end multilingual speech translation (ST) directly models the mapping from the speech in source languages to the text of multiple target languages. While multilingual neural machine translation has been proved effective in modeling the general knowledge with shared parameters and handling inter-task interference with language-specific parameters, it still lacks exploration of when and where parameter sharing matters in multilingual ST. This work offers such a study by proposing a comprehensive analysis on the influence of various heuristically designed sharing strategies. We further investigate the inter-task interference through gradient similarity between different tasks, and improve the parameter sharing strategy in multilingual ST under the guidance of inter-task gradient similarity. Experimental results on the one-to-many MuST-C dataset have shown that the gradient-guided sharing method can significantly improve the translation quality with a comparable or even lower cost in terms of parameter scale",
    "checked": true,
    "id": "8eb0710f9271829193f5eb4cc80ca00c986efc08",
    "semantic_title": "investigating parameter sharing in multilingual speech translation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22h_interspeech.html": {
    "title": "Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational(RAMC) Speech Dataset",
    "volume": "main",
    "abstract": "This paper introduces a high-quality rich annotated Mandarin conversational (RAMC) speech dataset called MagicData-RAMC. The MagicData-RAMC corpus contains 180 hours of conversational speech data recorded from native speakers of Mandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs in MagicData-RAMC are classified into 15 diversified domains and tagged with topic labels, ranging from science and technology to ordinary life. Accurate transcription and precise speaker voice activity timestamps are manually labeled for each sample. Speakers' detailed information is also provided. As a Mandarin speech dataset designed for dialog scenarios with high quality and rich annotations, MagicData-RAMC enriches the data diversity in the Mandarin speech community and allows extensive research on a series of speech-related tasks, including automatic speech recognition, speaker diarization, topic detection, keyword search, text-to-speech, etc. We also conduct several relevant tasks and provide experimental results to help evaluate the dataset",
    "checked": true,
    "id": "d6d9f298f3fdf01afe3c1122932420a8cdb997ff",
    "semantic_title": "open source magicdata-ramc: a rich annotated mandarin conversational(ramc) speech dataset",
    "citation_count": 15
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22j_interspeech.html": {
    "title": "TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline",
    "volume": "main",
    "abstract": "This paper introduces a new corpus of Mandarin-English code-switching speech recognitionTALCS corpus, suitable for training and evaluating code-switching speech recognition systems. TALCS corpus is derived from real online one-to-one English teaching scenes in TAL education group, which contains roughly 587 hours of speech sampled at 16 kHz. To our best knowledge, TALCS corpus is the largest well labeled Mandarin-English code-switching open source automatic speech recognition (ASR) dataset in the world. In this paper, we will introduce the recording procedure in detail, including audio capturing devices and corpus environments. And the TALCS corpus is freely available for download under the permissive license[ https://ai.100tal.com/dataset]. Using TALCS corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet and Wenet. The Mixture Error Rate (MER) performance in the two speech recognition toolkits is compared in TALCS corpus. The experimental results implies that the quality of audio recordings and transcriptions are promising and the baseline system is workable",
    "checked": true,
    "id": "19904ebc01d603b6b7dc1c60ec86f1c7f7055eb3",
    "semantic_title": "talcs: an open-source mandarin-english code-switching corpus and a speech recognition baseline",
    "citation_count": 8
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deng22b_interspeech.html": {
    "title": "Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "Although Transformers have gained success in several speech processing tasks like spoken language understanding (SLU) and speech translation (ST), achieving online processing while keeping competitive performance is still essential for real-world interaction. In this paper, we take the first step on streaming SLU and simultaneous ST using a blockwise streaming Transformer, which is based on contextual block processing and blockwise synchronous beam search. Furthermore, we design an automatic speech recognition (ASR)-based intermediate loss regularization for the streaming SLU task to improve the classification performance further. As for the simultaneous ST task, we propose a cross-lingual encoding method, which employs a CTC branch optimized with target language translations. In addition, the CTC translation output is also used to refine the search space with CTC prefix score, achieving joint CTC/attention simultaneous translation for the first time. Experiments for SLU are conducted on FSC and SLURP corpora, while the ST task is evaluated on Fisher-CallHome Spanish and MuST-C En-De corpora. Experimental results show that the blockwise streaming Transformer achieves competitive results compared to offline models, especially with our proposed methods that further yield a 2.4% accuracy gain on the SLU task and a 4.3 BLEU gain on the ST task over streaming baselines",
    "checked": true,
    "id": "1e4e2aceed87febcc643f1473507c9535ba5c19a",
    "semantic_title": "blockwise streaming transformer for spoken language understanding and simultaneous speech translation",
    "citation_count": 7
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tran22b_interspeech.html": {
    "title": "BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese",
    "volume": "main",
    "abstract": "We present BARTpho with two versions, BARTpho-syllable and BARTpho-word, which are the first public large-scale monolingual sequence-to-sequence models pre-trained for Vietnamese. BARTpho uses the \"large\" architecture and the pre-training scheme of the sequence-to-sequence denoising autoencoder BART, thus it is especially suitable for generative NLP tasks. We conduct experiments to compare our BARTpho with its competitor mBART on a downstream task of Vietnamese text summarization and show that: in both automatic and human evaluations, BARTpho outperforms the strong baseline mBART and improves the state-of-the-art. We further evaluate and compare BARTpho and mBART on the Vietnamese capitalization and punctuation restoration tasks and also find that BARTpho is more effective than mBART on these two tasks. We publicly release BARTpho to facilitate future research and applications of generative Vietnamese NLP tasks",
    "checked": true,
    "id": "a70fc86508bd0133d5d984a4e777abef1934d76c",
    "semantic_title": "bartpho: pre-trained sequence-to-sequence models for vietnamese",
    "citation_count": 13
  },
  "https://www.isca-speech.org/archive/interspeech_2022/markitantov22_interspeech.html": {
    "title": "Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS) Corpus: Multimodal Mask Type Recognition Task",
    "volume": "main",
    "abstract": "In this paper, we present a new multimodal corpus called Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS), which is designed to analyze voice and facial characteristics of persons wearing various masks, as well as to develop automatic systems for bimodal verification and identification of speakers. In particular, we tackle the multimodal mask type recognition task (6 classes). As a result, audio, visual and multimodal systems were developed, which showed UAR of 54.83%, 72.02% and 82.01%, respectively, on the Test set. These performances are the baseline for the BRAVE-MASKS corpus to compare the follow-up approaches with the proposed systems",
    "checked": true,
    "id": "cb1dcdd6334fd498681b23b95f355f1d350eaaec",
    "semantic_title": "biometric russian audio-visual extended masks (brave-masks) corpus: multimodal mask type recognition task",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chien22_interspeech.html": {
    "title": "Bayesian Transformer Using Disentangled Mask Attention",
    "volume": "main",
    "abstract": "Transformer conducts self attention which has achieved state-of-the-art performance in many applications. Multi-head attention in transformer basically gathers the features from individual tokens in input sequence to form the mapping to output sequence. There are twofold weaknesses in transformer. First, due to the natural property that attention mechanism would mix up the features of different tokens in input and output sequences, it is likely that the representation of input tokens contains redundant information. Second, the patterns of attention weights between different heads tend to be similar, the model capacity is bounded. To strengthen the sequential learning, this paper presents a variational disentangled mask attention in transformer where the redundant features are enhanced with semantic information. Latent disentanglement in multi-head attention is learned. The attention weights are filtered by a mask which is optimized by semantic clustering. The proposed attention mechanism is then implemented according to a Bayesian learning for clustered disentanglement. The experiments on machine translation show the merit of the disentangled mask attention",
    "checked": true,
    "id": "4787da07498f9afceba457e36b8223e5c91ef050",
    "semantic_title": "bayesian transformer using disentangled mask attention",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22o_interspeech.html": {
    "title": "Audio-Visual Speech Recognition in MISP2021 Challenge: Dataset Release and Deep Analysis",
    "volume": "main",
    "abstract": "In this paper, we present the updated Audio-Visual Speech Recognition (AVSR) corpus of MISP2021 challenge, a large-scale audio-visual Chinese conversational corpus consisting of 141h audio and video data collected by far/middle/near microphones and far/middle cameras in 34 real-home TV rooms. To our best knowledge, our corpus is the first distant multi-microphone conversational Chinese audio-visual corpus and the first large vocabulary continuous Chinese lip-reading dataset in the adverse home-tv scenario. Moreover, we make a deep analysis of the corpus and conduct a comprehensive ablation study of all audio and video data in the audio-only/video-only/audio-visual systems. Error analysis shows video modality supplement acoustic information degraded by noise to reduce deletion errors and provide discriminative information in overlapping speech to reduce substitution errors. Finally, we also design a set of experiments such as frontend, data augmentation and end-to-end models for providing the direction of potential future work. The corpus and the code are released to promote the research not only in speech area but also for the computer vision area and cross-disciplinary research",
    "checked": true,
    "id": "c48d1ab049b80e0354462f4643073f2cd96ad241",
    "semantic_title": "audio-visual speech recognition in misp2021 challenge: dataset release and deep analysis",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22u_interspeech.html": {
    "title": "From Start to Finish: Latency Reduction Strategies for Incremental Speech Synthesis in Simultaneous Speech-to-Speech Translation",
    "volume": "main",
    "abstract": "Speech-to-speech translation (S2ST) converts input speech to speech in another language. A challenge of delivering S2ST in real time is the accumulated delay between the translation and speech synthesis modules. While recently incremental text-to-speech (iTTS) models have shown large quality improvements, they typically require additional future text inputs to reach optimal performance. In this work, we minimize the initial waiting time of iTTS by adapting the upstream speech translator to generate high-quality pseudo lookahead for the speech synthesizer. After mitigating the initial delay, we demonstrate that the duration of synthesized speech also plays a crucial role on latency. We formalize this as a latency metric and the present a simple yet effective duration-scaling approach for latency reduction. Our approaches consistently reduce latency by 0.2-0.5 second without sacrificing speech translation quality",
    "checked": true,
    "id": "c717be6961788c6cb5f8eb1343915c98cf27eeef",
    "semantic_title": "from start to finish: latency reduction strategies for incremental speech synthesis in simultaneous speech-to-speech translation",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tam22_interspeech.html": {
    "title": "Isochrony-Aware Neural Machine Translation for Automatic Dubbing",
    "volume": "main",
    "abstract": "We introduce the task of isochrony-aware machine translation which aims at generating translations suitable for dubbing. Dubbing of a spoken sentence requires transferring the content as well as the speech-pause structure of the source into the target language to achieve audiovisual coherence. Practically, this implies correctly projecting pauses from the source to the target and ensuring that target speech segments have roughly the same duration of the corresponding source speech segments. In this work, we propose implicit and explicit modeling approaches to integrate isochrony information into neural machine translation. Experiments on EnglishGerman/French language pairs with automatic metrics show that the simplest of the considered approaches works best. Results are confirmed by human evaluations of translations and dubbed videos",
    "checked": true,
    "id": "06f4732e330f7c540e132bf2a0b72db7414033fb",
    "semantic_title": "isochrony-aware neural machine translation for automatic dubbing",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dong22b_interspeech.html": {
    "title": "Leveraging Pseudo-labeled Data to Improve Direct Speech-to-Speech Translation",
    "volume": "main",
    "abstract": "Direct Speech-to-speech translation (S2ST) has drawn more and more attention recently. The task is very challenging due to data scarcity and complex speech-to-speech mapping. In this paper, we report our recent achievements in S2ST. Firstly, we build a S2ST Transformer baseline which outperforms the original Translatotron. Secondly, we utilize the external data by pseudo-labeling and obtain a new state-of-the-art result on the Fisher English-to-Spanish test set. Indeed, we exploit the pseudo data with a combination of popular techniques which are not trivial when applied to S2ST. Moreover, we evaluate our approach on both syntactically similar (Spanish-English) and distant (English-Chinese) language pairs. Our implementation is available at \\url{https://github.com/fengpeng-yue/speech-to-speech-translation}",
    "checked": true,
    "id": "6ea1adab9cfde8bbf1d5d32c0ab9c7b6dc0709bb",
    "semantic_title": "leveraging pseudo-labeled data to improve direct speech-to-speech translation",
    "citation_count": 12
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pan22b_interspeech.html": {
    "title": "A Hybrid Continuity Loss to Reduce Over-Suppression for Time-domain Target Speaker Extraction",
    "volume": "main",
    "abstract": "The speaker extraction algorithm extracts the target speech from a mixture speech containing interference speech and background noise. The extraction process sometimes over-suppresses the extracted target speech, which not only creates artifacts during listening but also harms the performance of downstream automatic speech recognition algorithms. We propose a hybrid continuity loss function for time-domain speaker extraction algorithms to settle the over-suppression problem. On top of the waveform-level loss used for superior signal quality, i.e., SI-SDR, we introduce a multi-resolution delta spectrum loss in the frequency-domain, to ensure the continuity of an extracted speech signal, thus alleviating the over-suppression. We examine the hybrid continuity loss function using a time-domain audio-visual speaker extraction algorithm on the YouTube LRS2-BBC dataset. Experimental results show that the proposed loss function reduces the over-suppression and improves the word error rate of speech recognition on both clean and noisy two-speakers mixtures, without harming the reconstructed speech quality",
    "checked": true,
    "id": "64640a517a47fd9214a97e4bf509bc92a930e941",
    "semantic_title": "a hybrid continuity loss to reduce over-suppression for time-domain target speaker extraction",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/berg22_interspeech.html": {
    "title": "Extending GCC-PHAT using Shift Equivariant Neural Networks",
    "volume": "main",
    "abstract": "Speaker localization using microphone arrays depends on accurate time delay estimation techniques. For decades, methods based on the generalized cross correlation with phase transform (GCC-PHAT) have been widely adopted for this purpose. Recently, the GCC-PHAT has also been used to provide input features to neural networks in order to remove the effects of noise and reverberation, but at the cost of losing theoretical guarantees in noise-free conditions. We propose a novel approach to extending the GCC-PHAT, where the received signals are filtered using a shift equivariant neural network that preserves the timing information contained in the signals. By extensive experiments we show that our model consistently reduces the error of the GCC-PHAT in adverse environments, with guarantees of exact time delay recovery in ideal conditions",
    "checked": true,
    "id": "5aa700ab8fdee037088a2ae711abc9338226d3e7",
    "semantic_title": "extending gcc-phat using shift equivariant neural networks",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tzinis22_interspeech.html": {
    "title": "Heterogeneous Target Speech Separation",
    "volume": "main",
    "abstract": "We introduce a new paradigm for single-channel target source separation where the sources of interest can be distinguished using non-mutually exclusive concepts (e.g., loudness, gender, language, spatial location, etc). Our proposed heterogeneous separation framework can seamlessly leverage datasets with large distribution shifts and learn cross-domain representations under a variety of concepts used as conditioning. Our experiments show that training separation models with heterogeneous conditions facilitates the generalization to new concepts with unseen out-of-domain data while also performing substantially higher than single-domain specialist models. Notably, such training leads to more robust learning of new harder source separation discriminative concepts and can yield improvements over permutation invariant training with oracle source selection. We analyze the intrinsic behavior of source separation training with heterogeneous metadata and propose ways to alleviate emerging problems with challenging separation conditions. We release the collection of preparation recipes for all datasets used to further promote research towards this challenging task",
    "checked": true,
    "id": "c2c8cdc924b69d78526097ff76755939208018c1",
    "semantic_title": "heterogeneous target speech separation",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22w_interspeech.html": {
    "title": "Separate What You Describe: Language-Queried Audio Source Separation",
    "volume": "main",
    "abstract": "In this paper, we introduce the task of language-queried audio source separation (LASS), which aims to separate a target source from an audio mixture based on a natural language query of the target source (e.g., ''a man tells a joke followed by people laughing''). A unique challenge in LASS is associated with the complexity of natural language description and its relation with the audio sources. To address this issue, we proposed LASS-Net, an end-to-end neural network that is learned to jointly process acoustic and linguistic information, and separate the target source that is consistent with the language query from an audio mixture. We evaluate the performance of our proposed system with a dataset created from the AudioCaps dataset. Experimental results show that LASS-Net achieves considerable improvements over baseline methods. Furthermore, we observe that LASS-Net achieves promising generalization results when using diverse human-annotated descriptions as queries, indicating its potential use in real-world scenarios. The separated audio samples and source code are available at https://liuxubo717.github.io/LASS-demopage",
    "checked": true,
    "id": "588c15ef53d38c7c0266eddd235d64df68d6a973",
    "semantic_title": "separate what you describe: language-queried audio source separation",
    "citation_count": 19
  },
  "https://www.isca-speech.org/archive/interspeech_2022/markovic22_interspeech.html": {
    "title": "Implicit Neural Spatial Filtering for Multichannel Source Separation in the Waveform Domain",
    "volume": "main",
    "abstract": "We present a single-stage casual waveform-to-waveform multichannel model that can separate moving sound sources based on their broad spatial locations in a dynamic acoustic scene. We divide the scene into two spatial regions containing, respectively, the target and the interfering sound sources. The model is trained end-to-end and performs spatial processing implicitly, without any components based on traditional processing or use of hand-crafted spatial features. We evaluate the proposed model on a real-world dataset and show that the model matches the performance of an oracle beamformer followed by a state-of-the-art single-channel enhancement network",
    "checked": true,
    "id": "20bea78fc2ce09555b465b80167a80f1faac6782",
    "semantic_title": "implicit neural spatial filtering for multichannel source separation in the waveform domain",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nozaki22_interspeech.html": {
    "title": "End-to-end Speech-to-Punctuated-Text Recognition",
    "volume": "main",
    "abstract": "Conventional automatic speech recognition systems do not produce punctuation marks which are important for the readability of the speech recognition results. They are also needed for subsequent natural language processing tasks such as machine translation. There have been a lot of works on punctuation prediction models that insert punctuation marks into speech recognition results as post-processing. However, these studies do not utilize acoustic information for punctuation prediction and are directly affected by speech recognition errors. In this study, we propose an end-to-end model that takes speech as input and outputs punctuated texts. This model is expected to predict punctuation robustly against speech recognition errors while using acoustic information. We also propose to incorporate an auxiliary loss to train the model using the output of the intermediate layer and unpunctuated texts. Through experiments, we compare the performance of the proposed model to that of a cascaded system. The proposed model achieves higher punctuation prediction accuracy than the cascaded system without sacrificing the speech recognition error rate. It is also demonstrated that the multi-task learning using the intermediate output against the unpunctuated text is effective. Moreover, the proposed model has only about 1/7th of the parameters compared to the cascaded system",
    "checked": true,
    "id": "74b37db4bb4c1f81e110a601d759ab342d90c655",
    "semantic_title": "end-to-end speech-to-punctuated-text recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pupier22_interspeech.html": {
    "title": "End-to-End Dependency Parsing of Spoken French",
    "volume": "main",
    "abstract": "Research efforts in syntactic parsing have focused on written texts. As a result, speech parsing is usually performed on transcriptions, either in unrealistic settings (gold transcriptions) or on predicted transcriptions. Parsing speech from transcriptions, though straightforward to implement using out-of-the-box tools for Automatic Speech Recognition (ASR) and dependency parsing has two important limitations. First, relying on transcriptions will lead to error propagation due to recognition mistakes. Secondly, many acoustic cues that are important for parsing (prosody, pauses, ...) are no longer available in transcriptions. To address these limitations, we introduce wav2tree, an end-to-end dependency parsing model whose only input is the raw signal. Our model builds on a pretrained wav2vec2 encoder with a CTC loss to perform ASR. We extract token segmentation from the CTC layer to construct vector representations for each predicted token. Then, we use these token representations as input to a generic parsing algorithm. The whole model is trained end-to-end with a multitask objective (ASR, parsing) to reduce error propagation. Our experiments on the Orfeo treebank of spoken French show that direct parsing from speech is feasible: wav2tree outperforms a pipeline approach based on wav2vec (for ASR) and flauBERT (for parsing)",
    "checked": true,
    "id": "c98d44da5a0f39b0144c6b73935e2e12066c5ee2",
    "semantic_title": "end-to-end dependency parsing of spoken french",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22_interspeech.html": {
    "title": "Turn-Taking Prediction for Natural Conversational Speech",
    "volume": "main",
    "abstract": "While a streaming voice assistant system has been used in many applications, this system typically focuses on unnatural, one-shot interactions assuming input from a single voice query without hesitation or disfluency. However, a common conversational utterance often involves multiple queries with turn-taking, in addition to disfluencies. These disfluencies include pausing to think, hesitations, word lengthening, filled pauses and repeated phrases. This makes doing speech recognition with conversational speech, including one with multiple queries, a challenging task. To better model the conversational interaction, it is critical to discriminate disfluencies and end of query in order to allow the user to hold the floor for disfluencies while having the system respond as quickly as possible when the user has finished speaking. In this paper, we present a turn-taking predictor built on top of the end-to-end (E2E) speech recognizer. Our best system is obtained by jointly optimizing for ASR task and detecting when the user is paused to think or finished speaking",
    "checked": true,
    "id": "ec89d19b56829daf600e9e205298dc7d3863cc3c",
    "semantic_title": "turn-taking prediction for natural conversational speech",
    "citation_count": 6
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22b_interspeech.html": {
    "title": "Streaming Intended Query Detection using E2E Modeling for Continued Conversation",
    "volume": "main",
    "abstract": "In voice-enabled applications, a predetermined hotword is usually used to activate a device in order to attend to the query. However, speaking queries followed by a hotword each time introduces a cognitive burden in continued conversations. To avoid repeating a hotword, we propose a streaming end-to-end (E2E) intended query detector that identifies the utterances directed towards the device and filters out other utterances not directed towards device. The proposed approach incorporates the intended query detector into the E2E model that already folds different components of the speech recognition pipeline into one neural network. The E2E modeling on speech decoding and intended query detection also allows us to declare a quick intended query detection based on early partial recognition result, which is important to decrease latency and make the system responsive. We demonstrate that the proposed E2E approach yields a 22% relative improvement on equal error rate (EER) for the detection accuracy and 600 ms latency improvement compared with an independent intended query detector. In our experiment, the proposed model detects whether the user is talking to the device with a 8.7% EER within 1.4 seconds of median latency after user starts speaking",
    "checked": true,
    "id": "434205427bffa3cd05ada75a65c088ec737c6542",
    "semantic_title": "streaming intended query detection using e2e modeling for continued conversation",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lehecka22_interspeech.html": {
    "title": "Exploring Capabilities of Monolingual Audio Transformers using Large Datasets in Automatic Speech Recognition of Czech",
    "volume": "main",
    "abstract": "In this paper, we present our progress in pretraining Czech monolingual audio transformers from a large dataset containing more than 80 thousand hours of unlabeled speech, and subsequently fine-tuning the model on automatic speech recognition tasks using a combination of in-domain data and almost 6 thousand hours of out-of-domain transcribed speech. We are presenting a large palette of experiments with various fine-tuning setups evaluated on two public datasets (CommonVoice and VoxPopuli) and one extremely challenging dataset from the MALACH project. Our results show that monolingual Wav2Vec 2.0 models are robust ASR systems, which can take advantage of large labeled and unlabeled datasets and successfully compete with state-of-the-art LVCSR systems. Moreover, Wav2Vec models proved to be good zero-shot learners when no training data are available for the target ASR task",
    "checked": true,
    "id": "32d6ad56eb93a40439fd92148f20822a7f446b6d",
    "semantic_title": "exploring capabilities of monolingual audio transformers using large datasets in automatic speech recognition of czech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schoburgcarrillodemira22_interspeech.html": {
    "title": "SVTS: Scalable Video-to-Speech Synthesis",
    "volume": "main",
    "abstract": "Video-to-speech synthesis (also known as lip-to-speech) refers to the translation of silent lip movements into the corresponding audio. This task has received an increasing amount of attention due to its self-supervised nature (i.e., can be trained without manual labelling) combined with the ever-growing collection of audio-visual data available online. Despite these strong motivations, contemporary video-to-speech works focus mainly on small- to medium-sized corpora with substantial constraints in both vocabulary and setting. In this work, we introduce a scalable video-to-speech framework consisting of two components: a video-to-spectrogram predictor and a pre-trained neural vocoder, which converts the mel-frequency spectrograms into waveform audio. We achieve state-of-the art results for GRID and considerably outperform previous approaches on LRW. More importantly, by focusing on spectrogram prediction using a simple feedforward model, we can efficiently and effectively scale our method to very large and unconstrained datasets: To the best of our knowledge, we are the first to show intelligible results on the challenging LRS3 dataset",
    "checked": true,
    "id": "69b02cc932ab79f49651a6872d31582100be741e",
    "semantic_title": "svts: scalable video-to-speech synthesis",
    "citation_count": 18
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kishiyama22_interspeech.html": {
    "title": "One-step models in pitch perception: Experimental evidence from Japanese",
    "volume": "main",
    "abstract": "Several psycholinguistic and computational models have examined the perception of illusory vowels, where listeners of a language insert an epenthetic vowel to repair illegal consonant clusters, perceiving VCCV as VCVCV. This study investigated whether these top-down effects can be extended to pitch patterns and induce illusory pitches, where a pitch was perceived on the epenthetic vowel. Tokyo and Kinki Japanese are two dialects in Japan with the same phonotactics, but Tokyo and Kinki Japanese regard LLH (low low high) and LHH as illegal tonal patterns, respectively. We controlled an index representing linguistic exposure to the Tokyo pitch pattern and used an AXB discrimination task to investigate whether the pitch patterns influence the perception. We found that Tokyo dialect listeners with the high index, who have long exposure to the Tokyo pitch pattern, perceived \"H\" pitch between L and H, whereas the subjects with the low index did not show any preference. These results indicated that pitch patterns were also used in the perception of illusory pitches and were reproduced in a simulation study",
    "checked": true,
    "id": "5d8909d115b5596c5f1969958633ef4eea99fe0e",
    "semantic_title": "one-step models in pitch perception: experimental evidence from japanese",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/perezramon22_interspeech.html": {
    "title": "Generating iso-accented stimuli for second language research: methodology and a dataset for Spanish-accented English",
    "volume": "main",
    "abstract": "A non-native accent can be conveyed at both the segmental and suprasegmental level. Previous studies have developed techniques to isolate the effect of segmental foreign accent by splicing accented segments from a bilingual speaker into non-accented words produced by the same speaker. The current work addresses the issue of between-segment variability by developing a technique to convert from acoustically-equal accent gradations to perceptually-equal steps. The procedure is used to derive the first corpus of Spanish-accented English composed of lexical tokens each generated with one of five degrees of non-native accent. As an example application, corpus tokens are used to elicit accentedness judgements from four listener cohorts with first languages which differ as to whether they share the native language, the non-native (accented) language of the corpus or have a closer phonological inventory to one or the other. Findings highlight the importance of the relationship between listeners' phonological systems and those of the native and non-native languages of the corpus, especially for vowels, with respect to sensitivity to foreign accent",
    "checked": true,
    "id": "2cceb03b7e5c085b5a18dce7f525746c448f1c17",
    "semantic_title": "generating iso-accented stimuli for second language research: methodology and a dataset for spanish-accented english",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/leemann22_interspeech.html": {
    "title": "Factors affecting the percept of Yanny v. Laurel (or mixed): Insights from a large-scale study on Swiss German listeners",
    "volume": "main",
    "abstract": "In May 2018, Yanny v. Laurel went viral: when listening to the same audio clip, some people claimed to hear only Yanny, others insisted it must be Laurel, and some had a mixed percept. Phoneticians have identified the acoustic features which caused this perceptual ambiguity, but we still know little about the factors affecting individuals' perception of the illusion. We conducted a controlled study with 974 Swiss German listeners, balanced for age, gender, and regional origin. Overall, nearly two thirds heard Yanny, one quarter Laurel, and about 12% had a mixed percept. We found age, gender, and electronic device to play a significant role: younger, female, and laptop-using participants demonstrated higher proportions of Yanny responses. These findings contribute to the growing body of research on polyperceivable words",
    "checked": true,
    "id": "6d60a01c707c41976b90bf9ecf5f86e6a2cabbb4",
    "semantic_title": "factors affecting the percept of yanny v. laurel (or mixed): insights from a large-scale study on swiss german listeners",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22ba_interspeech.html": {
    "title": "Effects of laryngeal manipulations on voice gender perception",
    "volume": "main",
    "abstract": "This study aims to identify laryngeal manipulations that would allow a male to approximate a female-sounding voice, and that can be targeted in voice feminization surgery or therapy. Synthetic voices were generated using a three-dimensional vocal fold model with parametric variations in vocal fold geometry, stiffness, adduction, and subglottal pressure. The vocal tract was kept constant in order to focus on the contribution of laryngeal manipulations. Listening subjects were asked to judge if a voice sounded male or female, or if they were unsure. Results showed the expected large effect of the fundamental frequency (F0) and a moderate effect of spectral shape on gender perception. A mismatch between F0 and spectral shape cues (e.g., low F0 paired with high H1-H2) contributed to ambiguity in gender perception, particularly for voices with F0 in the intermediate range between those of typical adult males and females. Physiologically, the results showed that a female-sounding voice can be produced by decreasing vocal fold thickness and increasing vocal fold transverse stiffness in the coronal plane, changes in which modified both F0 and spectral shape. In contrast, laryngeal manipulations with limited impact on F0 or spectral shape were shown to be less effective in modifying gender perception",
    "checked": true,
    "id": "ccc1368461bae47f510ff0bdeba394be250ac9d7",
    "semantic_title": "effects of laryngeal manipulations on voice gender perception",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22n_interspeech.html": {
    "title": "Why is Korean lenis stop difficult to perceive for L2 Korean learners?",
    "volume": "main",
    "abstract": "This study investigates how French learners, whose native language has only a binary laryngeal contrast, acquire the Korean three-way laryngeal contrast in stops by focusing on cue weighting. We tested how 21 French learners of Korean identify fortis/lenis/aspirated Korean stops over eight monthly sessions. Learners were the most successful at identifying aspirated stops. The identification of lenis stops was the most challenging, with no improvement over the 8 months, whereas the perception of aspirated and fortis stops improved. In order to explain the difficulty with lenis stops, we tested the relative contribution of VOT and f0 to the perception of the contrast on synthesized stimuli. Learners showed different cue-weighting strategies: VOT was used to distinguish between aspirated and fortis/lenis, and f0 was used to differentiate between fortis and lenis, implying a two-way rather than a three-way contrast. Furthermore, the larger weight given to VOT compared to f0 by the French learners can explain the poor identification of lenis stops, whose main contrastive cue in Korean is a lowering of f0 on the following vowel. Based on these findings, the acquisition of the three-way contrast necessitates a reorganization of the cues' relative weight",
    "checked": true,
    "id": "c6d04d82c6ff28711eba8a6c7fd46885c17383c6",
    "semantic_title": "why is korean lenis stop difficult to perceive for l2 korean learners?",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zurita22_interspeech.html": {
    "title": "Lexical stress in Spanish word segmentation",
    "volume": "main",
    "abstract": "We explored the role of lexical stress in word segmentation as speech unfolds in time. We tested participants online with a Mouse Tracking listening experiment using temporarily ambiguous phrase pairs of the form \"PAlo marron\" vs. \"paLOma roja\". These pairs were segmentally ambiguous in the first three syllables but differed in the location of lexical stress. Thus, use of stress cues would allow participants to disambiguate the phrases more quickly. We also manipulated the presence of lexical stress correlates in two conditions (stress natural and stress neutral) and found that lexical stress has an early impact in Spanish word segmentation that can affect how quickly and efficiently the speech signal is processed",
    "checked": true,
    "id": "dbdd97bc2c04d1f2beee09ac767e9828f4f5c84a",
    "semantic_title": "lexical stress in spanish word segmentation",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shin22_interspeech.html": {
    "title": "Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting",
    "volume": "main",
    "abstract": "In this paper, we propose a novel end-to-end user-defined keyword spotting method that utilizes linguistically corresponding patterns between speech and text sequences. Unlike previous approaches requiring speech keyword enrollment, our method compares input queries with an enrolled text keyword sequence. To place the audio and text representations within a common latent space, we adopt an attention-based cross-modal matching approach that is trained in an end-to-end manner with monotonic matching loss and keyword classification loss. We also utilize a de-noising loss for the acoustic embedding network to improve robustness in noisy environments. Additionally, we introduce the LibriPhrase dataset, a new short-phrase dataset based on LibriSpeech for efficiently training keyword spotting models. Our proposed method achieves competitive results on various evaluation sets compared to other single-modal and cross-modal baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/abdullah22_interspeech.html": {
    "title": "Integrating Form and Meaning: A Multi-Task Learning Model for Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "Models of acoustic word embeddings (AWEs) learn to map variable-length spoken word segments onto fixed-dimensionality vector representations such that different acoustic exemplars of the same word are projected nearby in the embedding space. In addition to their speech technology applications, AWE models have been shown to predict human performance on a variety of auditory lexical processing tasks. Current AWE models are based on neural networks and trained in a bottom-up approach that integrates acoustic cues to build up a word representation given an acoustic or symbolic supervision signal. Therefore, these models do not leverage or capture high-level lexical knowledge during the learning process. In this paper, we propose a multi-task learning model that incorporates top-down lexical knowledge into the training procedure of AWEs. Our model learns a mapping between the acoustic input and a lexical representation that encodes high-level information such as word semantics in addition to bottom-up form-based supervision. We experiment with three languages and demonstrate that incorporating lexical knowledge improves the embedding space discriminability and encourages the model to better separate lexical categories",
    "checked": true,
    "id": "1e1c0e21692e8fd870455b799368d7fc0e1a909a",
    "semantic_title": "integrating form and meaning: a multi-task learning model for acoustic word embeddings",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22l_interspeech.html": {
    "title": "Personalized Keyword Spotting through Multi-task Learning",
    "volume": "main",
    "abstract": "Keyword spotting (KWS) plays an essential role in enabling speech-based user interaction on smart devices, and conventional KWS (C-KWS) approaches have concentrated on detecting user-agnostic pre-defined keywords. However, in practice, most user interactions come from target users enrolled in the device which motivates to construct personalized keyword spotting. We design two personalized KWS tasks; (1) Target user Biased KWS (TB-KWS) and (2) Target user Only KWS (TO-KWS). To solve the tasks, we propose personalized keyword spotting through multi-task learning (PK-MTL) that consists of multi-task learning and task-adaptation. First, we introduce applying multi-task learning on keyword spotting and speaker verification to leverage user information to the keyword spotting system. Next, we design task-specific scoring functions to adapt to the personalized KWS tasks thoroughly. We evaluate our framework on conventional and personalized scenarios, and the results show that PK-MTL can dramatically reduce the false alarm rate, especially in various practical scenarios",
    "checked": true,
    "id": "de3f0fc18f253deccca417e2007c0647a603ced2",
    "semantic_title": "personalized keyword spotting through multi-task learning",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/svec22_interspeech.html": {
    "title": "Deep LSTM Spoken Term Detection using Wav2Vec 2.0 Recognizer",
    "volume": "main",
    "abstract": "In recent years, the standard hybrid DNN-HMM speech recognizers are outperformed by the end-to-end speech recognition systems. One of the very promising approaches is the grapheme Wav2Vec 2.0 model, which uses the self-supervised pretraining approach combined with transfer learning of the fine-tuned speech recognizer. Since it lacks the pronunciation vocabulary and language model, the approach is suitable for tasks where obtaining such models is not easy or almost impossible. In this paper, we use the Wav2Vec speech recognizer in the task of spoken term detection over a large set of spoken documents. The method employs a deep LSTM network which maps the recognized hypothesis and the searched term into a shared pronunciation embedding space in which the term occurrences and the assigned scores are easily computed. The paper describes a bootstrapping approach that allows the transfer of the knowledge contained in traditional pronunciation vocabulary of DNN-HMM hybrid ASR into the context of grapheme-based Wav2Vec. The proposed method outperforms the previously published system based on the combination of the DNN-HMM hybrid ASR and phoneme recognizer by a large margin on the MALACH data in both English and Czech languages",
    "checked": true,
    "id": "f324f0153d4f8022d02ae160811043fbe55aae5f",
    "semantic_title": "deep lstm spoken term detection using wav2vec 2.0 recognizer",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jose22_interspeech.html": {
    "title": "Latency Control for Keyword Spotting",
    "volume": "main",
    "abstract": "Conversational agents commonly utilize keyword spotting (KWS) to initiate voice interaction with the user. For user experience and privacy considerations, existing approaches to KWS largely focus on accuracy, which can often come at the expense of introduced latency. To address this tradeoff, we propose a novel approach to control KWS model latency and which generalizes to any loss function without explicit knowledge of the keyword endpoint. Through a single, tunable hyperparameter, our approach enables one to balance detection latency and accuracy for the targeted application. Empirically, we show that our approach gives superior performance under latency constraints when compared to existing methods. Namely, we make a substantial 25 % relative false accepts improvement for a fixed latency target when compared to the baseline state-of-the-art. We also show that when our approach is used in conjunction with a max-pooling loss, we are able to improve relative false accepts by 25 % at a fixed latency when compared to cross entropy loss",
    "checked": true,
    "id": "7c4ec9a20a87976c9180957ec05215d6f9edfc1e",
    "semantic_title": "latency control for keyword spotting",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nayak22_interspeech.html": {
    "title": "Improving Voice Trigger Detection with Metric Learning",
    "volume": "main",
    "abstract": "Voice trigger detection is an important task, which enables activating a voice assistant when a target user speaks a keyword phrase. A detector is typically trained on speech data independent of speaker information and used for the voice trigger detection task. However, such a speaker independent voice trigger detector typically suffers from performance degradation on speech from underrepresented groups, such as accented speakers. In this work, we propose a novel voice trigger detector that can use a small number of utterances from a target speaker to improve detection accuracy. Our proposed model employs an encoder-decoder architecture. While the encoder performs speaker independent voice trigger detection, similar to the conventional detector, the decoder is trained with metric learning and predicts a personalized embedding for each utterance. A personalized voice trigger score is then obtained as a similarity score between the embeddings of enrollment utterances and a test utterance. The personalized embedding allows adapting to target speaker's speech when computing the voice trigger score, hence improving voice trigger detection accuracy. Experimental results show that the proposed approach achieves a 38\\% relative reduction in a false rejection rate (FRR) compared to a baseline speaker independent voice trigger model",
    "checked": true,
    "id": "60eca0ebb2e514866afcdb4e2b363544dc480035",
    "semantic_title": "improving voice trigger detection with metric learning",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/soltau22_interspeech.html": {
    "title": "RNN Transducers for Named Entity Recognition with constraints on alignment for understanding medical conversations",
    "volume": "main",
    "abstract": "Understanding medical conversations requires detecting entities such as Medications, Symptoms, Treatment, Conditions and Diagnosis, which leads to large ontologies with overlapping spans.Popular solutions to Named Entity Recognition (NER) such as conditional random fields, sequence-to-sequence models, or the question-answering framework are not suitable for this task. We address this problem by proposing a new model for NER task -- an RNN transducer, which has hitherto been used only in speech recognition. These models are trained using paired input and output sequences without explicitly specifying the alignment between them, similar to other seq-to-seq models. In NER tasks, however, the alignment between words and labels are available from the human annotations. We propose a fixed alignment model that utilizes the given alignment, while preserving the benefits of RNN-Ts such as modeling output dependencies. We also propose a constrained alignment model where users can specify a relaxation and the model will learn an alignment within the given constraints. In other words, we propose a family of seq-to-seq models which can leverage alignments between input and target sequences when available. Through empirical experiments on a challenging real-world medical NER task with multiple ontologies, we demonstrate that our fixed alignment model outperforms the standard RNN-T model",
    "checked": true,
    "id": "2fec02d05c646b752a3081e6ad90b5ef1124ce19",
    "semantic_title": "rnn transducers for named entity recognition with constraints on alignment for understanding medical conversations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22c_interspeech.html": {
    "title": "Towards Automated Counselling Decision-Making: Remarks on Therapist Action Forecasting on the AnnoMI Dataset",
    "volume": "main",
    "abstract": "Substantial progress has been made in recent years on natural language processing approaches to counselling conversation analysis. However, few studies have investigated therapist action forecasting, which aims to suggest dialogue actions that the therapist can take in the next turn, partly due to generally limited access to counselling dialogue data resulting from privacy-related constraints. In this work, we leverage a recently released public dataset of therapy conversations and experiment with a range of natural language processing techniques to approach the task of therapist action forecasting with language models. We probe various factors that could impact model performance, including data augmentation, dialogue context length, incorporating therapist/client utterance labels in the input, and contrasting high- and low-quality counselling dialogues. With our findings, we hope to provide insights on this task and inspire future efforts in counselling dialogue analysis",
    "checked": true,
    "id": "ed210277a6a58926b9667855d6945447539963ce",
    "semantic_title": "towards automated counselling decision-making: remarks on therapist action forecasting on the annomi dataset",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fara22_interspeech.html": {
    "title": "Speech and the n-Back task as a lens into depression. How combining both may allow us to isolate different core symptoms of depression",
    "volume": "main",
    "abstract": "Embedded in any speech signal is a rich combination of cognitive, neuromuscular and physiological information. This richness makes speech a powerful signal in relation to a range of different health conditions, including major depressive disorders (MDD). One pivotal issue in speech-depression research is the assumption that depressive severity is the dominant measurable effect. However, given the heterogeneous clinical profile of MDD, it may actually be the case that speech alterations are more strongly associated with subsets of key depression symptoms. This paper presents strong evidence in support of this argument. First, we present a novel large, cross-sectional, multi-modal dataset collected at Thymia. We then present a set of machine learning experiments that demonstrate that combining speech with features from an n-Back working memory assessment improves classifier performance when predicting the popular eight-item Patient Health Questionnaire depression scale (PHQ-8). Finally, we present a set of experiments that highlight the association between different speech and n-Back markers at the PHQ-8 item level. Specifically, we observe that somatic and psychomotor symptoms are more strongly associated with n-Back performance scores, whilst the other items: anhedonia, depressed mood, change in appetite, feelings of worthlessness and trouble concentrating are more strongly associated with speech changes",
    "checked": true,
    "id": "9b7dfefdaf65fc2eca04e797d17caa81fbd7a21b",
    "semantic_title": "speech and the n-back task as a lens into depression. how combining both may allow us to isolate different core symptoms of depression",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/romana22_interspeech.html": {
    "title": "Enabling Off-the-Shelf Disfluency Detection and Categorization for Pathological Speech",
    "volume": "main",
    "abstract": "A speech disfluency, such as a filled pause, repetition, or revision, disrupts the typical flow of speech. Disfluency modeling has grown as a research area, as recent work has shown that these disfluencies may help in assessing health conditions. For example, for individuals with cognitive impairment, changes in disfluencies may indicate worsening symptoms. However, work on disfluency modeling has focused heavily on detection and less on categorization. Work that has focused on categorization has suffered with two specific classes: repetitions and revisions. In this paper, we evaluate how BERT (Bidirectional Encoder Representations from Transformers) compares to other models on disfluency detection and categorization. We also propose adding a second fine-tuning task where BERT learns to distance repetitions and revisions from their repairs with triplet loss. We find that BERT and BERT with triplet loss outperform previous work on disfluency detection and categorization, particularly for repetitions and revisions. In this paper we present the first analysis of how these models can be fine-tuned on widely available disfluency data, and then used in an off-the-shelf manner on small corpora of pathological speech",
    "checked": true,
    "id": "ba52b9ad3aa61c9718067c3f21443dc727c5b652",
    "semantic_title": "enabling off-the-shelf disfluency detection and categorization for pathological speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/botelho22_interspeech.html": {
    "title": "Challenges of using longitudinal and cross-domain corpora on studies of pathological speech",
    "volume": "main",
    "abstract": "Several promising works have reported very exciting results in the field of speech in health, however there are still issues to address before deploying such systems into clinical applications. One of such issues is to ensure the generalisability and reliability of results. With this in mind, in this work, we perform a comparative analysis of healthy speech in two scenarios: (1) collected for six different datasets spoken in the same language, and (2) collected across different times in a single longitudinal corpus. We show that feature sets typically used for disease detection from speech (eGeMAPS, ComParE, pause-related features, ECAPA-TDNN embeddings and i-vectors) encode much information about the dataset or about changing recording conditions over time, in longitudinal studies. We support our results with classification results largely above chance level for both scenarios, and through unsupervised clustering experiments, where we observe that data naturally clusters according to dataset",
    "checked": true,
    "id": "01eb682b8d522dc4d4fd2b3cabbc7f908186f938",
    "semantic_title": "challenges of using longitudinal and cross-domain corpora on studies of pathological speech",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22d_interspeech.html": {
    "title": "g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin",
    "volume": "main",
    "abstract": "Polyphone disambiguation is the most crucial task in Mandarin grapheme-to-phoneme (g2p) conversion. Previous studies have approached this problem using pre-trained language models, restricted output, and extra information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we propose a novel approach, called g2pW, which adapts learnable softmax-weights to condition the outputs of BERT with the polyphonic character of interest and its POS tagging. Rather than using the hard mask as in previous works, our experiments show that learning a soft-weighting function for the candidate phonemes benefits performance. In addition, our proposed g2pW does not require extra pre-trained POS tagging models while using POS tags as auxiliary features since we train the POS tagging model simultaneously with the unified encoder. Experimental results show that our g2pW outperforms existing methods on the public CPP dataset. All codes, model weights, and a user-friendly package are publicly available",
    "checked": true,
    "id": "783474c60486cf590ad8685c06f0c0afdeb7a91d",
    "semantic_title": "g2pw: a conditional weighted softmax bert for polyphone disambiguation in mandarin",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22b_interspeech.html": {
    "title": "A Unified Accent Estimation Method Based on Multi-Task Learning for Japanese Text-to-Speech",
    "volume": "main",
    "abstract": "We propose a unified accent estimation method for Japanese text-to-speech (TTS). Unlike the conventional two-stage methods, which separately train two models for predicting accent phrase boundaries and accent nucleus positions, our method merges the two models and jointly optimizes the entire model in a multi-task learning framework. Furthermore, considering the hierarchical linguistic structure of intonation phrases (IPs), accent phrases, and accent nuclei, we generalize the proposed approach to simultaneously model the IP boundaries with accent information. Objective evaluation results reveal that the proposed method achieves an accent estimation accuracy of 80.4%, which is 6.67% higher than the conventional two-stage method. When the proposed method is incorporated into a neural TTS framework, the system achieves a 4.29 mean opinion score with respect to prosody naturalness",
    "checked": true,
    "id": "3d4edbdfb1d99cbf323d521e7842037324fabe9b",
    "semantic_title": "a unified accent estimation method based on multi-task learning for japanese text-to-speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/raitio22_interspeech.html": {
    "title": "Vocal effort modeling in neural TTS for improving the intelligibility of synthetic speech in noise",
    "volume": "main",
    "abstract": "We present a neural text-to-speech (TTS) method that models natural vocal effort variation to improve the intelligibility of synthetic speech in the presence of noise. The method consists of first measuring the spectral tilt of unlabeled conventional speech data, and then conditioning a neural TTS model with normalized spectral tilt among other prosodic factors. Changing the spectral tilt parameter and keeping other prosodic factors unchanged enables effective vocal effort control at synthesis time independent of other prosodic factors. By extrapolation of the spectral tilt values beyond what has been seen in the original data, we can generate speech with high vocal effort levels, thus improving the intelligibility of speech in the presence of masking noise. We evaluate the intelligibility and quality of normal speech and speech with increased vocal effort in the presence of various masking noise conditions, and compare these to well-known speech intelligibility-enhancing algorithms. The evaluations show that the proposed method can improve the intelligibility of synthetic speech with little loss in speech quality",
    "checked": true,
    "id": "028a21d8811bcfa0974e12fc04397852910277f9",
    "semantic_title": "vocal effort modeling in neural tts for improving the intelligibility of synthetic speech in noise",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22d_interspeech.html": {
    "title": "TTS-by-TTS 2: Data-Selective Augmentation for Neural Speech Synthesis Using Ranking Support Vector Machine with Variational Autoencoder",
    "volume": "main",
    "abstract": "Recent advances in synthetic speech quality have enabled us to train text-to-speech (TTS) systems by using synthetic corpora. However, merely increasing the amount of synthetic data is not always advantageous for improving training efficiency. Our aim in this study is to selectively choose synthetic data that are beneficial to the training process. In the proposed method, we first adopt a variational autoencoder whose posterior distribution is utilized to extract latent features representing acoustic similarity between the recorded and synthetic corpora. By using those learned features, we then train a ranking support vector machine (RankSVM) that is well known for effectively ranking relative attributes among binary classes. By setting the recorded and synthetic ones as two opposite classes, RankSVM is used to determine how the synthesized speech is acoustically similar to the recorded data. Then, synthetic TTS data, whose distribution is close to the recorded data, are selected from large-scale synthetic corpora. By using these data for retraining the TTS model, the synthetic quality can be significantly improved. Objective and subjective evaluation results show the superiority of the proposed method over the conventional methods",
    "checked": true,
    "id": "f9682aa7433ca60310bb4a9c73f4c4942fb74632",
    "semantic_title": "tts-by-tts 2: data-selective augmentation for neural speech synthesis using ranking support vector machine with variational autoencoder",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/comini22_interspeech.html": {
    "title": "Low-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation",
    "volume": "main",
    "abstract": "The availability of data in expressive styles across languages is limited, and recording sessions are costly and time consuming. To overcome these issues, we demonstrate how to build low-resource, neural text-to-speech (TTS) voices with only 1 hour of conversational speech, when no other conversational data are available in the same language. Assuming the availability of non-expressive speech data in that language, we propose a 3-step technology: 1) we train an F0-conditioned voice conversion (VC) model as data augmentation technique; 2) we train an F0 predictor to control the conversational flavour of the voice-converted synthetic data; 3) we train a TTS system that consumes the augmented data. We prove that our technology enables F0 controllability, is scalable across speakers and languages and is competitive in terms of naturalness over a state-of-the-art baseline model, another augmented method which does not make use of F0 information",
    "checked": true,
    "id": "7eb632743fae9a844086e8de897f3d021b507a9a",
    "semantic_title": "low-data? no problem: low-resource, language-agnostic conversational text-to-speech via f0-conditioned data augmentation",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ingle22_interspeech.html": {
    "title": "Real-Time Monitoring of Silences in Contact Center Conversations",
    "volume": "main",
    "abstract": "Contact center conversations often contain segments with hold music, automatic-recorded-messages or pure silences, where neither the customer nor the agent is speaking. We refer to these segments as Conversational Silences [1]. These silences when continued beyond an acceptable level can negatively impact im portant contact center KPIs, like average handling time, agent efficiency, etc. and may lead to poor customer experience. As a result, it becomes imperative for contact centers to identify si lences in conversations and define mechanisms to better handle them. In this paper, we propose a cascaded system consisting of an ASR engine, a silence detector block, a text classification layer and a heuristic engine to surface instances in calls where agents are missing the protocols to handle silences. This system is used to trigger alerts to agents in real time thus enabling them to course correct while being on call with the customer. More over, these instances can also be surfaced to their supervisors so as to identify agents who are frequently missing these protocols and thereby design dedicated coaching sessions",
    "checked": true,
    "id": "bc0f04f9403c8401be13cf5f5c4902b4d227f70a",
    "semantic_title": "real-time monitoring of silences in contact center conversations",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zielinski22_interspeech.html": {
    "title": "Humanizing bionic voice: interactive demonstration of aesthetic design and control factors influencing the devices assembly and waveshape engineering",
    "volume": "main",
    "abstract": "Electrolarynx is a speech aid providing voice source for people who have their larynx resected. Bionic voices (BV) extend the device capabilities by combining the biological and technical parts in a functioning communicative system. This interactive demonstration illustrates the challenges in material design and sound engineering in the domain of BV which are aimed at enriching the performance of a conversing dyad in the social context. The factors of control and aesthetics will be introduced by alaryngeal speaker presenting novel prototypes. The participants will experience the concepts in a multimodal demo, try various devices and talk with a laryngectomee",
    "checked": true,
    "id": "c824edd19055346510cf643b8a28fdc163bfc1d5",
    "semantic_title": "humanizing bionic voice: interactive demonstration of aesthetic design and control factors influencing the devices assembly and waveshape engineering",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ronssin22_interspeech.html": {
    "title": "Application for Real-time Personalized Speaker Extraction",
    "volume": "main",
    "abstract": "This short paper demonstrates an audio processing desktop ap plication that allows isolating in real-time the voice of a spe cific speaker from the possibly noisy audio input after a short enrollment phase. The machine learning model embedded in this application suppresses all other sounds than the target voice from the incoming audio stream, including disturbing distractor voices. In the context of a growing need for video-collaboration solutions, personalized speech enhancement enables the use of such technologies in more challenging acoustic environments, i.e., in the presence of near distractor speech. In this situation, classical speech enhancement systems typically fail as they do not filter out any speech, hence the need for personalized meth ods. The presented application is an all-in-one solution for per sonalized speech enhancement: it allows the user to enroll and then to apply the effect seamlessly for one-to-one or one-to many online meetings",
    "checked": true,
    "id": "b230215f6637b5baa2eb96b8a7cae26a2615c224",
    "semantic_title": "application for real-time personalized speaker extraction",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bhattacharya22b_interspeech.html": {
    "title": "Coswara: A website application enabling COVID-19 screening by analysing respiratory sound samples and health symptoms",
    "volume": "main",
    "abstract": "The COVID-19 pandemic has accelerated research on design of alternative, quick and effective COVID-19 diagnosis approaches. In this paper, we describe the Coswara tool, a website application designed to enable COVID-19 detection by analysing respiratory sound samples and health symptoms. A user using this service can log into a website using any device connected to the internet, provide there current health symptom information and record few sound sampled corresponding to breathing, cough, and speech. Within a minute of analysis of this information on a cloud server the website tool will output a COVID-19 probability score to the user. As the COVID-19 pandemic continues to demand massive and scalable population level testing, we hypothesize that the proposed tool provides a potential solution towards this",
    "checked": true,
    "id": "ec3ba010c40d4fd0170f844deaa42f06eb8033b7",
    "semantic_title": "coswara: a website application enabling covid-19 screening by analysing respiratory sound samples and health symptoms",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schafer22_interspeech.html": {
    "title": "CoachLea: an Android Application to Evaluate the Speech Production and Perception of Children with Hearing Loss",
    "volume": "main",
    "abstract": "Hearing loss can affect children's language, speech, and social skills development. Hearing problems can result from impaired auditory feedback due to various reasons such as trauma, a clin ical condition, genetic alterations, and infections, among oth ers. Early treatment is the key to successful hearing and speech rehabilitation if the hearing loss occurs before or during spo ken language acquisition. In this work, we present CoachLea, an Android application to support the clinical evaluation and therapy of the speech production and perception of children with hearing loss. The app includes numerous daily exercises to capture speech and hearing data continuously and longitudi nally using a game-like user interface. Speech exercises include the \"Snail race\", \"Animal Sounds\", and \"Image identification\", whereas the hearing exercise consists of a word identification game based on minimal pairs with speech-in-noise recordings. In the long term, CoachLea aims to be a tool that supports the therapy of children with hearing loss",
    "checked": true,
    "id": "66ba41ac3a25a23ac458354439d7b1d2fbeb0fcb",
    "semantic_title": "coachlea: an android application to evaluate the speech production and perception of children with hearing loss",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/haider22_interspeech.html": {
    "title": "An Automated Mood Diary for Older User's using Ambient Assisted Living Recorded Speech",
    "volume": "main",
    "abstract": "In this paper, we describe a system for recording of mood di aries in the context of an ambient assisted living and intelli gent coaching environment, which ensures privacy by design. The system performs affect recognition in speech features with out recording speech content in any form. We demonstrate re sults of affect recognition models tested on data collected in care-home settings during the SAAM project (Supporting Ac tive Ageing through Multimodal Coaching) using our custom designed audio collection hardware. The proposed system was trained using Bulgarian speech augmented with training data obtained from comparable mood diaries recorded in Scottish English. A degree of transfer learning of Scottish English speech to Bulgarian speech was demonstrated",
    "checked": true,
    "id": "9519f3bfc2c9e27c17ab0bbdb58431b7c7237875",
    "semantic_title": "an automated mood diary for older user's using ambient assisted living recorded speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22_interspeech.html": {
    "title": "Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer Based Infant Cry Recognition",
    "volume": "main",
    "abstract": "Crying is the main way for babies to communicate with the outside world. Analyzing cry enables not only the identification of babies' needs/thoughts they want to express, but also the prediction of potential diseases. In general, it is much more difficult to recognize special needs and emotions from infant cry than adults, because infant cry does not contain any linguistic information and the emotional expression is not as rich as adults.In this work, we focus on the time-frequency characteristics of infant crying signals and propose a differential time-frequency log-Mel spectrogram features based vision transformer (ViT) approach for infant cry recognition (ICR). We first calculate the deltas of log-Mel spectrogram of infant crying sounds over time frames and frequencies, respectively. The log-Mels and deltas are then combined as a 3-D feature representation and fed into the ViT model for cry classification. Experimental results on the CRIED database show the superiority of the proposed system over comparison methods and that the combination of logMels, the time-frame delta and frequency-bin delta achieves the best performance. The proposed method is further validated on a self-recorded dataset",
    "checked": true,
    "id": "e5579160fd1e3c159af427ed3c96dca0289d9226",
    "semantic_title": "differential time-frequency log-mel spectrogram features for vision transformer based infant cry recognition",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fernau22_interspeech.html": {
    "title": "Towards Automated Dialog Personalization using MBTI Personality Indicators",
    "volume": "main",
    "abstract": "As conversational interfaces mature in both capacity and usage, the need to personalize towards specific user characteristics becomes apparent, in order to improve users' acceptance, satisfaction and trust in the conversations. We utilize the concept of Myers-Briggs personality type indicators in order to adapt chatbot behavior. In a user study, we investigate the impact and realization of the so-called ``law of attraction'' by providing users with a chatbot that mirrors their own personality. This entails predicting the personality from the user behavior, in this work chat messages, by utilizing a pre-trained language model rather than composing many resources like lexicons. We conduct a user study with aligned and misaligned personality and analyze the effect on usability. Results show that alignment significantly improves major usability factors such as satisfaction, perceived naturalness, recommendation likelihood, appropriateness and trustworthiness of our interaction. Further, comparing different language models, contrastive learning approaches outperform previous methods. Predicting the thinking vs. feeling and introversion vs. extroversion indicator dichotomies, we achieve 76.14% f1 and 69.11 f1, respectively, with setting a new state-of-the-art performance in the literature for the former. Finally, our work adds transparency to the design of linguistic personality cues, hitherto rarely reported in the literature",
    "checked": true,
    "id": "8df6ab72cdf20636726eb27ef2eb47d296d0a586",
    "semantic_title": "towards automated dialog personalization using mbti personality indicators",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/qian22_interspeech.html": {
    "title": "Word-wise Sparse Attention for Multimodal Sentiment Analysis",
    "volume": "main",
    "abstract": "For multimodal sentiment analysis (MSA), the text-centric approach has been shown to be superior in performance, which adopts powerful text models (e.g., BERT) as backbone and studies how to effectively incorporate non-verbal modalities (i.e., audio and visual) to obtain more refined and expressive word representations. In previous methods, the non-verbal information injected into a word representation only comes from a non-verbal segment corresponding to the time span of the word, ignoring the long-range dependencies across modalities. Meanwhile, these methods utilize the Softmax normalization function-based attention mechanism, which makes it difficult to highlight the important information in non-verbal sequences. To this end, this paper proposes a non-verbal information injection method called Word-wise Sparse Attention (WSA) to capture the cross-modal long-range dependencies. When injecting the non-verbal information into a word, the word is used as the semantic anchor to search for the most relevant non-verbal information from holistic non-verbal sequences. Furthermore, an advanced Multimodal Adaptive Gating (MAG) mechanism is introduced to determine the amount of information injected from non-verbal modalities. We evaluate our method on the two publicly available multimodal sentiment datasets. Experimental results show that the proposed approach improves the baseline model consistently on all metrics",
    "checked": true,
    "id": "de3e70154546a2cbaeaf39e893d0a6c96f111e43",
    "semantic_title": "word-wise sparse attention for multimodal sentiment analysis",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gupta22_interspeech.html": {
    "title": "Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model",
    "volume": "main",
    "abstract": "The estimation of speaker characteristics such as age and height is a challenging task, having numerous applications in voice forensic analysis. In this work, we propose a bi-encoder transformer mixture model for speaker age and height estimation. Considering the wide differences in male and female voice characteristics such as differences in formant and fundamental frequencies, we propose the use of two separate transformer encoders for the extraction of specific voice features in the male and female gender, using wav2vec 2.0 as a common-level feature extractor. This architecture reduces the interference effects during backpropagation and improves the generalizability of the model. We perform our experiments on the TIMIT dataset and significantly outperform the current state-of-the-art results on age estimation. Specifically, we achieve root mean squared error (RMSE) of 5.54 years and 6.49 years for male and female age estimation, respectively. Further experiment to evaluate the relative importance of different phonetic types for our task demonstrate that vowel sounds are the most distinguishing for age estimation",
    "checked": true,
    "id": "9e30727b99e597d219bf33d59032dee2617d4711",
    "semantic_title": "estimation of speaker age and height from speech signal using bi-encoder transformer mixture model",
    "citation_count": 5
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22b_interspeech.html": {
    "title": "Exploring Multi-task Learning Based Gender Recognition and Age Estimation for Class-imbalanced Data",
    "volume": "main",
    "abstract": "Automatic gender recognition and age estimation from speaker's audio is desired by applications in music recommendation, speaker profiling etc. However, its performance degrades greatly with the class-imbalanced data distribution. This paper explores a novel multi-task learning based gender recognition and age estimation system using speaker embedding. We apply the label distribution smoothing referred as LDS and investigate a weight mean squared error focal loss named as w-MSE-FL to reshape the weight assigned to the centralized-distribution samples during training. For a limited dataset, we pretrain a deep convolution neural network stacked with an attentive statistic pooling layer for speaker recognition task on a speaker speech dataset to extract robust speaker embedding feature. Then, we further fine-tune the multi-task learning network for gender recognition and age estimation simultaneously using classifier and regressor on a specific gender and age dataset, respectively. Experimental results verify our proposed system achieves better results on the TIMIT dataset with RMSE of 7.17 and 7.25 years on age estimation for male and female speakers, respectively, while performs an overall gender recognition accuracy of 99.30%",
    "checked": true,
    "id": "e8508340896720a285752f995e597489c360073f",
    "semantic_title": "exploring multi-task learning based gender recognition and age estimation for class-imbalanced data",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22b_interspeech.html": {
    "title": "Audio-Visual Domain Adaptation Feature Fusion for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "Speech emotion recognition has made significant progress in recent years, in which feature representation learning has been paid more attention, but discriminative emotional features extraction has remained unresolved. In this paper, we propose MDSCM - a Multi-attention based Depthwise Separable Convolutional Model for speech emotional feature extraction that can reduce the feature redundancy through separating spatial-wise convolution and channel-wise convolution. MDSCM also enhances the feature discriminability by the multi-attention module that focuses on learning features with more emotional information. In addition, we propose an Audio-Visual Domain Adaptation Learning paradigm (AVDAL) to learn an audio-visual emotion-identity space. A shared audio-visual representation encoder is built to transfer the emotional knowledge learned from the visual domain to complement and enhance the emotional features that only extracted from speech. Domain classifier and emotion classifier are used for encoder training to reduce the mismatching of domain features, and enhance the discriminability of features for emotion recognition. The experimental results on the IEMOCAP dataset demonstrate that our proposed method outperforms other state-of-the-art speech emotion recognition systems, achieving 72.43% on weighted accuracy and 73.22% on unweighted accuracy. The code is available at https://github.com/Janie1996/AV4SER",
    "checked": true,
    "id": "eb154ae6264910533ead28859cb8219252332b7d",
    "semantic_title": "audio-visual domain adaptation feature fusion for speech emotion recognition",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22r_interspeech.html": {
    "title": "Impact of Background Noise and Contribution of Visual Information in Emotion Identification by Native Mandarin Speakers",
    "volume": "main",
    "abstract": "Many studies on emotion processing considered little about the issue of ecological validity and insufficient attention has been drawn to uni-sensory and multisensory emotion perception in challenging environments. The current research explored how adding multi-talker babble noise impacts emotion perception and how visual information affects the results in comparison with the audio alone conditions. Forty native Mandarin participants (21 females and 19 males) were asked to identify the emotion according to the auditory or audiovisual information they received. Results showed that the emotion identification accuracy was significantly lower in noisy conditions than in noiseless ones, whether additional visual information was presented simultaneously or not. In noisy environments, providing multisensory emotional information greatly facilitated recognition performances even when the visual information was less reliable. To conclude, multi-talker babble noise had a corrupting effect on emotion identification, which worked in both unisensory and multisensory settings, and emotion perception is a robust multisensory situation that follows the inverse effectiveness principle",
    "checked": true,
    "id": "96aa0ac0410b4885744dbbe103f5a9fc480dfc7e",
    "semantic_title": "impact of background noise and contribution of visual information in emotion identification by native mandarin speakers",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22q_interspeech.html": {
    "title": "Exploiting Fine-tuning of Self-supervised Learning Models for Improving Bi-modal Sentiment Analysis and Emotion Recognition",
    "volume": "main",
    "abstract": "Speech-based multimodal affective computing has recently attracted significant research attention. Previous experimental results have shown that the audio-only approach exhibits inferior performance than the text-only approach in sentiment analysis and emotion recognition tasks. In this paper, we propose a new strategy to improve the performance of uni-modal and bi-modal affective computing systems via fine-tuning of two pre-trained self-supervised learning models (Text-RoBERTa and Speech-RoBERTa). We fine-tune the models on sentiment analysis and emotion recognition tasks using a shallow architecture, and apply crossmodal attention fusion to the models for further learning and final prediction or classification. We evaluate our proposed method on the CMU-MOSI, CMU-MOSEI and IEMOCAP datasets. The experimental results demonstrate that our approach exhibits superior performance for all benchmarks compared to existing state-of-the-art results, establishing the effectiveness of the proposed method",
    "checked": true,
    "id": "03b828aadf7272076fcd09ca4e0686573d30031a",
    "semantic_title": "exploiting fine-tuning of self-supervised learning models for improving bi-modal sentiment analysis and emotion recognition",
    "citation_count": 3
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tao22_interspeech.html": {
    "title": "Characterizing Therapist's Speaking Style in Relation to Empathy in Psychotherapy",
    "volume": "main",
    "abstract": "In conversation-based psychotherapy, therapists use verbal techniques to help clients express thoughts and feelings, and change behavior. In particular, how well therapists convey empathy is an essential quality index of psychotherapy sessions and is associated with psychotherapy outcome. In this paper, we analyze the prosody of therapist speech and attempt to associate the therapist's speaking style with subjectively perceived empathy. An automatic speech and text processing system is developed to segment long recordings of psychotherapy sessions into pause-delimited utterances with text transcriptions. Data-driven clustering is applied to the utterances from different therapists in multiple sessions. For each cluster, a typological representation of utterance genre is derived based on quantized prosodic feature parameters. Prominent speaking styles of the therapist can be observed and interpreted from salient utterance genres that are correlated with empathy. Using the salient utterance genres, an accuracy of 71% is achieved in classifying psychotherapy sessions into \"high\" and \"low\" empathy level. Analysis of results suggests that empathy level tends to be (1) low if therapists speak long utterances slowly or speak short utterances quickly; and (2) high if therapists talk to clients with a steady tone and volume",
    "checked": true,
    "id": "5bb9c5548c45ce49fe07eddf82f48f68d12a54d3",
    "semantic_title": "characterizing therapist's speaking style in relation to empathy in psychotherapy",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tao22b_interspeech.html": {
    "title": "Hierarchical Attention Network for Evaluating Therapist Empathy in Counseling Session",
    "volume": "main",
    "abstract": "Counseling typically takes the form of spoken conversation between a therapist and a client. The empathy level expressed by the therapist is considered to be an essential quality factor of counseling outcome. This paper proposes a hierarchical recurrent network combined with two-level attention mechanisms to determine the therapist's empathy level solely from the acoustic features of conversational speech in a counseling session. The experimental results show that the proposed model can achieve an accuracy of 72.1% in classifying the therapist's empathy level as being \"high\" or \"low\". It is found that the speech from both the therapist and the client are contributing to predicting the empathy level that is subjectively rated by an expert observer. By analyzing speaker turns assigned with high attention weights, it is observed that 2 to 6 consecutive turns should be considered together to provide useful clues for detecting empathy, and the observer tends to take the whole session into consideration when rating the therapist empathy, instead of relying on a few specific speaker turns",
    "checked": true,
    "id": "1febfebdb5b0959554e2e2bf3996c5875b21d347",
    "semantic_title": "hierarchical attention network for evaluating therapist empathy in counseling session",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22v_interspeech.html": {
    "title": "Context-aware Multimodal Fusion for Emotion Recognition",
    "volume": "main",
    "abstract": "Automatic emotion recognition (AER) is an inherently complex multimodal task that aims to automatically determine the emotional state of a given expression. Recent works have witnessed the benefits of upstream pretrained models in both audio and textual modalities for the AER task. However, efforts are still needed to effectively integrate features across multiple modalities, devoting due considerations to granularity mismatch and asynchrony in time steps. In this work, we first validate the effectiveness of the upstream models in a unimodal setup and empirically find that partial fine-tuning of the pretrained model in the feature space can significantly boost performance. Moreover, we take the context of the current sentence to model a more accurate emotional state. Based on the unimodal setups, we further propose several multimodal fusion methods to combine high-level features from the audio and text modalities. Experiments are carried out on the IEMOCAP dataset in a 4-category classification problem and compared with state-of-the-art methods in recent literature. Results show that the proposed models gave a superior performance of up to 84.45% and 80.36% weighted accuracy scores respectively in Session 5 and 5-fold cross-validation settings",
    "checked": true,
    "id": "a08e8287b2f167905088e9471e17dd089f4bfc6e",
    "semantic_title": "context-aware multimodal fusion for emotion recognition",
    "citation_count": 4
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22z_interspeech.html": {
    "title": "Unsupervised Instance Discriminative Learning for Depression Detection from Speech Signals",
    "volume": "main",
    "abstract": "Major Depressive Disorder (MDD) is a severe illness that affects millions of people, and it is critical to diagnose this disorder as early as possible. Detecting depression from voice signals can be of great help to physicians and can be done without the need of any invasive procedure. Since relevant labelled data are scarce, we propose a modified Instance Discriminative Learning (IDL) method, an unsupervised pre-training technique, to extract augment-invariant and instance-spread-out embeddings. In terms of learning augment-invariant embeddings, various data augmentation methods for speech are investigated, and time-masking is found to provide the best performance. To learn instance-spread-out embeddings, we explore methods for sampling instances for a training batch (distinct speaker-based and random sampling). It is found that the distinct speaker-based sampling provides better performance than the random one, and we hypothesize that this result is because relevant speaker information is preserved in the embedding. Additionally, we propose a novel sampling strategy, Pseudo Instance-based Sampling (PIS), based on clustering algorithms, to enhance spread-out characteristics of the embeddings. Experiments are conducted with DepAudioNet on DAIC-WOZ (English) and CONVERGE (Mandarin) datasets, and statistically significant improvements are observed in the detection of MDD relative to the baseline with no pre-training",
    "checked": true,
    "id": "1b7d347ca1bffa70c56bc901372b0fecee658241",
    "semantic_title": "unsupervised instance discriminative learning for depression detection from speech signals",
    "citation_count": 1
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sardhaei22_interspeech.html": {
    "title": "How do our eyebrows respond to masks and whispering? The case of Persians",
    "volume": "main",
    "abstract": "Whispering is one of the mechanisms of human communication to convey linguistic information. Due to the lack of vocal fold vibration, whispering acoustically differs from the voiced speech in the absence of fundamental frequency which is one of the main prosodic correlates of intonation. This study addresses the importance of facial cues with respect to acoustic cues of intonation. Specifically, we aim to probe how eyebrow velocity and furrowing change when people whisper and wear face masks, also, when they are supposed to produce a prosodic modulation as it is the case in polar questions with rising intonation. To this end, we run an experiment with 10 Persian speakers. The results show the greater mean speed when speakers whisper indicating a compensation effect for the lack of F0 in whispering. We also found a more pronounced movement of both eyebrows when the speakers wear a mask. Finally, our results reveal greater eyebrow motions in questions suggesting the question is a more marked utterance type than a statement. No significant effect of eyebrow furrowing was found. However, eyebrow movements were positively correlated with the eyebrow widening suggesting a mutual link between these two movement types",
    "checked": true,
    "id": "4ff1b66cb9997bc3f2ee78e8425a1a82c162422c",
    "semantic_title": "how do our eyebrows respond to masks and whispering? the case of persians",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baird22_interspeech.html": {
    "title": "State & Trait Measurement from Nonverbal Vocalizations: A Multi-Task Joint Learning Approach",
    "volume": "main",
    "abstract": "Humans infer a wide array of meanings from expressive nonverbal vocalizations, \\eg laughs, cries, and sighs. Thus far, computational research has primarily focused on the coarse classification of vocalizations such as laughs, but that approach overlooks significant variations in the meaning of distinct laughs (e.g., amusement, awkwardness, triumph) and the rich array of more nuanced vocalizations people form. Nonverbal vocalizations are shaped by the emotional state an individual chooses to convey, their wellbeing, and (as with the voice more broadly) their identity-related traits. In the present study, we utilize a large-scale dataset comprising more than 35 hours of densely labeled vocal bursts to model emotionally expressive states and demographic traits from nonverbal vocalizations. We compare a single-task and multi-task deep learning architecture to explore how models can leverage acoustic co-dependencies that may exist between the expression of 10 emotions by vocal bursts and the demographic traits of the speaker. Results show that nonverbal vocalizations can be reliably leveraged to predict emotional expression, age, and country of origin. In a multi-task setting, our experiments show that joint learning of emotional expression and demographic traits appears to yield robust results, primarily beneficial for the classification of a speaker's country of origin",
    "checked": true,
    "id": "a2f9d871319da1216c61a7ae6880338161871bed",
    "semantic_title": "state & trait measurement from nonverbal vocalizations: a multi-task joint learning approach",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saraf22_interspeech.html": {
    "title": "Confidence Measure for Automatic Age Estimation From Speech",
    "volume": "main",
    "abstract": "Age estimation from speech is a problem that has wide applications in call centers, virtual assistants and IoT devices. The estimated age is used for various system decisions related to personalization, parental control, and anomaly detection. Performance of speech based automatic age estimation systems is generally stated in the literature using the Mean Absolute Error (MAE) and Pearson Correlation Coefficient (PC). In real-world applications of these systems, MAE and PC provide little insight into the confidence of a point estimate. An MAE of 5 years on a test set provides only the average error across all estimates in the test set and hence does not provide any information about the confidence of each individual estimate. A confidence score of predicted age is essential to know the trustworthiness of the predictions made by the system. This paper formulates age estimation from speech as a label distribution learning problem to come up with a measure of the confidence related to the point estimate being within a desired range from the ground-truth. It further uses it to analyze the age estimation system under conditions of varying speech quality. We show that the proposed measure of confidence is better than a fixed error-margin",
    "checked": true,
    "id": "8998dcd655512a16f88d2d71e1e1db19a5026cbd",
    "semantic_title": "confidence measure for automatic age estimation from speech",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fasoli22_interspeech.html": {
    "title": "Accelerating Inference and Language Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit Quantization",
    "volume": "main",
    "abstract": "We report on aggressive quantization strategies that greatly accelerate inference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit integer representation for both weights and activations and apply Quantization Aware Training (QAT) to retrain the full model (acoustic encoder and language model) and achieve near-iso-accuracy. We show that customized quantization schemes that are tailored to the local properties of the network are essential to achieve good performance while limiting the computational overhead of QAT. Density ratio Language Model fusion has shown remarkable accuracy gains on RNN-T workloads but it severely increases the computational cost of inference. We show that our quantization strategies enable using large beam widths for hypothesis search while achieving streaming-compatible runtimes and a full model compression ratio of 7.6x compared to the full precision model. Via hardware simulations, we estimate a 3.4x acceleration from FP16 to INT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a Real Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03 test sets, we retain most of the gains associated with LM fusion, improving the average WER by >1.5%",
    "checked": true,
    "id": "17813f4082947ec9e3194398f0e71b0e2d854064",
    "semantic_title": "accelerating inference and language model fusion of recurrent neural network transducers via end-to-end 4-bit quantization",
    "citation_count": 2
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sun22_interspeech.html": {
    "title": "Tree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hou22b_interspeech.html": {
    "title": "Bring dialogue-context into RNN-T for streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weninger22_interspeech.html": {
    "title": "Conformer with dual-mode chunked attention for joint online and offline ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22c_interspeech.html": {
    "title": "Efficient Training of Neural Transducer for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22b_interspeech.html": {
    "title": "Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kuang22_interspeech.html": {
    "title": "Pruned RNN-T for fast, memory-eicient ASR training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22h_interspeech.html": {
    "title": "Deep Sparse Conformer for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22l_interspeech.html": {
    "title": "Chain-based Discriminative Autoencoders for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mahadeokar22_interspeech.html": {
    "title": "Streaming parallel transducer beam search with fast slow cascaded encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22y_interspeech.html": {
    "title": "Self-regularised Minimum Latency Training for Streaming Transformer-based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/albesano22_interspeech.html": {
    "title": "On the Prediction Network Architecture in RNN-T for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shinohara22_interspeech.html": {
    "title": "Minimum latency training of sequence transducers for streaming end-to-end speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/an22_interspeech.html": {
    "title": "CUSIDE: Chunking, Simulating Future Context and Decoding for Streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22j_interspeech.html": {
    "title": "Attention Enhanced Citrinet for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22b_interspeech.html": {
    "title": "Simple and Effective Zero-shot Cross-lingual Phoneme Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22_interspeech.html": {
    "title": "Robust Self-Supervised Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/algayres22_interspeech.html": {
    "title": "Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22e_interspeech.html": {
    "title": "Towards Green ASR: Lossless 4-bit Quantization of a Hybrid TDNN System on the 300-hr Swithboard Corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/qin22b_interspeech.html": {
    "title": "Finer-grained Modeling units-based Meta-Learning for Low-resource Tibetan Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/janbakhshi22_interspeech.html": {
    "title": "Adversarial-Free Speaker Identity-Invariant Representation Learning for Automatic Dysarthric Speech Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22o_interspeech.html": {
    "title": "Automated Detection of Wilson's Disease Based on Improved Mel-frequency Cepstral Coefficients with Signal Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fan22b_interspeech.html": {
    "title": "The effect of backward noise on lexical tone discrimination in Mandarin-speaking amusics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ke22_interspeech.html": {
    "title": "Automatic Selection of Discriminative Features for Dementia Detection in Cantonese-Speaking People",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22q_interspeech.html": {
    "title": "Automated Voice Pathology Discrimination from Continuous Speech Benefits from Analysis by Phonetic Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mallolragolta22_interspeech.html": {
    "title": "Multi-Type Outer Product-Based Fusion of Respiratory Sounds for Detecting COVID-19",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22z_interspeech.html": {
    "title": "Robust Cough Feature Extraction and Classification Method for COVID-19 Cough Detection Based on Vocalization Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/javanmardi22_interspeech.html": {
    "title": "Comparing 1-dimensional and 2-dimensional spectral feature representations in voice pathology detection using machine learning and deep learning classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chatzoudis22_interspeech.html": {
    "title": "Zero-Shot Cross-lingual Aphasia Detection using Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22d_interspeech.html": {
    "title": "Domain-aware Intermediate Pretraining for Dementia Detection with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fougeron22_interspeech.html": {
    "title": "Comparison of 5 methods for the evaluation of intelligibility in mild to moderate French dysarthric speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22b_interspeech.html": {
    "title": "Improving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22b_interspeech.html": {
    "title": "Listen, Adapt, Better WER: Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22d_interspeech.html": {
    "title": "Distilling a Pretrained Language Model to a Multilingual ASR Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sato22_interspeech.html": {
    "title": "Text-Only Domain Adaptation Based on Intermediate CTC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/thienpondt22_interspeech.html": {
    "title": "Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takashima22_interspeech.html": {
    "title": "Updating Only Encoders Prevents Catastrophic Forgetting of End-to-End ASR Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22_interspeech.html": {
    "title": "Improved CNN-Transformer using Broadcasted Residual Learning for Text-Independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jung22_interspeech.html": {
    "title": "Pushing the limits of raw waveform speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22e_interspeech.html": {
    "title": "PHO-LID: A Unified Model Incorporating Acoustic-Phonetic and Phonotactic Information for Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tzudir22_interspeech.html": {
    "title": "Prosodic Information in Dialect Identification of a Tonal Language: The case of Ao",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22o_interspeech.html": {
    "title": "A Multimodal Strategy for Singing Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/daoudi22_interspeech.html": {
    "title": "A comparative study on vowel articulation in Parkinson's disease and multiple system atrophy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ardaillon22_interspeech.html": {
    "title": "Voicing decision based on phonemes classification and spectral moments for whisper-to-speech conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/talkar22_interspeech.html": {
    "title": "Speech Acoustics in Mild Cognitive Impairment and Parkinson's Disease With and Without Concurrent Drawing Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tran22c_interspeech.html": {
    "title": "Investigating the Impact of Speech Compression on the Acoustics of Dysarthric Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/brueggeman22_interspeech.html": {
    "title": "Speaker Trait Enhancement for Cochlear Implant Users: A Case Study for Speaker Emotion Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/reddy22b_interspeech.html": {
    "title": "Optimal thyroplasty implant shape and stiffness for treatment of acute unilateral vocal fold paralysis: Evidence from a canine in vivo phonation model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/babu22_interspeech.html": {
    "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rugayan22_interspeech.html": {
    "title": "Semantically Meaningful Metrics for Norwegian ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/klejch22_interspeech.html": {
    "title": "Deciphering Speech: a Zero-Resource Approach to Cross-Lingual Transfer in ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kumar22c_interspeech.html": {
    "title": "Linguistically Informed Post-processing for ASR Error correction in Sanskrit",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/morshed22_interspeech.html": {
    "title": "Cross-lingual articulatory feature information transfer for speech recognition using recurrent progressive neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/aguirre22_interspeech.html": {
    "title": "Comparison of Models for Detecting Off-Putting Speaking Styles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kawano22_interspeech.html": {
    "title": "Multimodal Persuasive Dialogue Corpus using Teleoperated Android",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shin22b_interspeech.html": {
    "title": "Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/adigwe22_interspeech.html": {
    "title": "Strategies for developing a Conversational Speech Dataset for Text-To-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22f_interspeech.html": {
    "title": "Deep CNN-based Inductive Transfer Learning for Sarcasm Detection in Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mitsui22_interspeech.html": {
    "title": "End-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/afshan22_interspeech.html": {
    "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/afshan22b_interspeech.html": {
    "title": "Learning from human perception to improve automatic speaker verification in style-mismatched conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/martikainen22_interspeech.html": {
    "title": "Exploring audio-based stylistic variation in podcasts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deja22_interspeech.html": {
    "title": "Automatic Evaluation of Speaker Similarity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22c_interspeech.html": {
    "title": "Mix and Match: An Empirical Study on Training Corpus Composition for Polyglot Text-To-Speech (TTS)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takamichi22_interspeech.html": {
    "title": "J-MAC: Japanese multi-speaker audiobook corpus for speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/webber22_interspeech.html": {
    "title": "REYD  The First Yiddish Text-to-Speech Dataset and System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dekorte22_interspeech.html": {
    "title": "Data-augmented cross-lingual synthesis in a teacher-student framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pandey22b_interspeech.html": {
    "title": "Production characteristics of obstruents in WaveNet and older TTS systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lemaguer22_interspeech.html": {
    "title": "Back to the Future: Extending the Blizzard Challenge 2013",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meyer22c_interspeech.html": {
    "title": "BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/maniati22_interspeech.html": {
    "title": "SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22_interspeech.html": {
    "title": "Domain Generalization with Relaxed Instance Frequency-wise Normalization for Multi-device Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rui22_interspeech.html": {
    "title": "Couple learning for semi-supervised sound event detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rajan22_interspeech.html": {
    "title": "Oktoechos Classification in Liturgical Music Using SBU-LSTM/GRU",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/he22_interspeech.html": {
    "title": "SoundDoA: Learn Sound Source Direction of Arrival and Semantics from Sound Raw Waveforms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bergler22_interspeech.html": {
    "title": "ORCA-WHISPER: An Automatic Killer Whale Sound Type Generation Toolkit Using Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22d_interspeech.html": {
    "title": "Convolutional Recurrent Neural Network with Auxiliary Stream for Robust Variable-Length Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bassan22_interspeech.html": {
    "title": "Unsupervised Symbolic Music Segmentation using Ensemble Temporal Prediction Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shirian22_interspeech.html": {
    "title": "Visually-aware Acoustic Event Detection using Heterogeneous Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/singh22_interspeech.html": {
    "title": "A Passive Similarity based CNN Filter Pruning for Efficient Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baade22_interspeech.html": {
    "title": "MAE-AST: Masked Autoencoding Audio Spectrogram Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bayerl22_interspeech.html": {
    "title": "What can Speech and Language Tell us About the Working Alliance in Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/frost22_interspeech.html": {
    "title": "TB or not TB? Acoustic cough analysis for tuberculosis classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/berisha22_interspeech.html": {
    "title": "Are reported accuracies in the clinical speech machine learning literature overoptimistic?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mirheidari22_interspeech.html": {
    "title": "Automatic Detection of Expressed Emotion from Five-Minute Speech Samples: Challenges and Opportunities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mirheidari22b_interspeech.html": {
    "title": "Automatic cognitive assessment: Combining sparse datasets with disparate cognitive scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dang22_interspeech.html": {
    "title": "Exploring Semi-supervised Learning for Audio-based COVID-19 Detection using FixMatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bhattacharya22_interspeech.html": {
    "title": "Analyzing the impact of SARS-CoV-2 variants on respiratory sound signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/braun22_interspeech.html": {
    "title": "Automated Evaluation of Standardized Dementia Screening Tests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pereztoro22_interspeech.html": {
    "title": "Alzheimer's Detection from English to Spanish Using Acoustic and Linguistic Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/su22b_interspeech.html": {
    "title": "Extract and Abstract with BART for Clinical Notes from Doctor-Patient Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lamichhane22_interspeech.html": {
    "title": "Dyadic Interaction Assessment from Free-living Audio for Depression Severity Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nallanthighal22_interspeech.html": {
    "title": "COVID-19 detection based on respiratory sensing from speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luo22c_interspeech.html": {
    "title": "Bifurcation and Reunion: A Loss-Guided Two-Stage Approach for Monaural Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cheng22b_interspeech.html": {
    "title": "A deep complex multi-frame filtering network for stereophonic acoustic echo cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/han22c_interspeech.html": {
    "title": "Speaker- and Phone-aware Convolutional Transformer Network for Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22t_interspeech.html": {
    "title": "Personalized Acoustic Echo Cancellation for Full-duplex Communications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22v_interspeech.html": {
    "title": "LCSM: A Lightweight Complex Spectral Mapping Framework for Stereophonic Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kothapally22_interspeech.html": {
    "title": "Joint Neural AEC and Beamforming with Double-Talk Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/helwani22_interspeech.html": {
    "title": "Clock Skew Robust Acoustic Echo Cancellation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/panchapagesan22_interspeech.html": {
    "title": "A Conformer-based Waveform-domain Neural Acoustic Echo Canceller Optimized for ASR Accuracy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kothapally22b_interspeech.html": {
    "title": "Complex-Valued Time-Frequency Self-Attention for Speech Dereverberation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xue22_interspeech.html": {
    "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22f_interspeech.html": {
    "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22c_interspeech.html": {
    "title": "FlowCPCVC: A Contrastive Predictive Coding Supervised Flow Framework for Any-to-Any Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lei22_interspeech.html": {
    "title": "Glow-WaveGAN 2: High-quality Zero-shot Text-to-speech Synthesis and Any-to-any Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22f_interspeech.html": {
    "title": "AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22d_interspeech.html": {
    "title": "Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22p_interspeech.html": {
    "title": "Streamable Speech Representation Disentanglement and Multi-Level Prosody Modeling for Live One-Shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22d_interspeech.html": {
    "title": "Accent Conversion using Pre-trained Model and Synthesized Data from Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vanrijn22_interspeech.html": {
    "title": "VoiceMe: Personalized voice generation in TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yuan22b_interspeech.html": {
    "title": "DeID-VC: Speaker De-identification via Zero-shot Pseudo Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lian22_interspeech.html": {
    "title": "Towards Improved Zero-shot Voice Conversion with Conditional DSVAE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/du22c_interspeech.html": {
    "title": "Disentanglement of Emotional Style and Speaker Identity for Expressive Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meng22_interspeech.html": {
    "title": "Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/du22_interspeech.html": {
    "title": "A Complementary Joint Training Approach Using Unpaired Speech and Text A Complementary Joint Training Approach Using Unpaired Speech and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gong22_interspeech.html": {
    "title": "Knowledge Transfer and Distillation from Autoregressive to Non-Autoregessive Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deng22_interspeech.html": {
    "title": "Confidence Score Based Conformer Speaker Adaptation for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22b_interspeech.html": {
    "title": "Decoupled Federated Learning for ASR with Non-IID Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22_interspeech.html": {
    "title": "Knowledge Distillation For CTC-based Speech Recognition Via Consistent Acoustic Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cui22b_interspeech.html": {
    "title": "Improving Generalization of Deep Neural Network Acoustic Models with Length Perturbation and N-best Based Label Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22m_interspeech.html": {
    "title": "Supervision-Guided Codebooks for Masked Prediction in Speech Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ren22_interspeech.html": {
    "title": "Speech Pre-training with Acoustic Piece",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22u_interspeech.html": {
    "title": "Censer: Curriculum Semi-supervised Learning for Speech Recognition Based on Self-supervised Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ao22_interspeech.html": {
    "title": "Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sawhney22_interspeech.html": {
    "title": "PISA: PoIncar Saliency-Aware Interpolative Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22w_interspeech.html": {
    "title": "Online Continual Learning of End-to-End Speech Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/moriya22_interspeech.html": {
    "title": "Streaming Target-Speaker ASR with Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jain22_interspeech.html": {
    "title": "SPLICEOUT: A Simple and Efficient Audio Augmentation Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sunder22_interspeech.html": {
    "title": "Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in End-to-End Speech-to-Intent Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ohsugi22_interspeech.html": {
    "title": "Japanese ASR-Robust Pre-trained Language Model with Pseudo-Error Sentences Generated by Grapheme-Phoneme Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dong22_interspeech.html": {
    "title": "Improving Spoken Language Understanding with Cross-Modal Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/avila22_interspeech.html": {
    "title": "Low-bit Shift Network for End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22_interspeech.html": {
    "title": "Meta Auxiliary Learning for Low-resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22o_interspeech.html": {
    "title": "Adversarial Knowledge Distillation For Robust Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ou22_interspeech.html": {
    "title": "Incorporating Dual-Aware with Hierarchical Interactive Memory Networks for Task-Oriented Dialogue",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22w_interspeech.html": {
    "title": "Pay More Attention to History: A Context Modeling Strategy for Conversational Text-to-SQL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22x_interspeech.html": {
    "title": "Small Changes Make Big Differences: Improving Multi-turn Response Selection in Dialogue Systems via Fine-Grained Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dinarelli22_interspeech.html": {
    "title": "Toward Low-Cost End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kapelonis22_interspeech.html": {
    "title": "A Multi-Task BERT Model for Schema-Guided Dialogue State Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22e_interspeech.html": {
    "title": "WavPrompt: Towards Few-Shot Spoken Language Understanding with Frozen Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ogushi22_interspeech.html": {
    "title": "Analysis of praising skills focusing on utterance contents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22fa_interspeech.html": {
    "title": "Speech2Slot: A Limited Generation Framework with Boundary Detection for Slot Filling from Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/koutini22_interspeech.html": {
    "title": "Efficient Training of Audio Transformers with Patchout",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sharma22_interspeech.html": {
    "title": "CNN-based Audio Event Recognition for Automated Violence Classification and Rating for Prime Video Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nam22_interspeech.html": {
    "title": "Frequency Dynamic Convolution: Frequency-Adaptive Pattern Recognition for Sound Event Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mostaani22_interspeech.html": {
    "title": "On Breathing Pattern Information in Synthetic Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22p_interspeech.html": {
    "title": "Interactive Auido-text Representation for Automated Audio Captioning with Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yamamoto22_interspeech.html": {
    "title": "Deformable CNN and Imbalance-Aware Feature Learning for Singing Technique Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/muller22_interspeech.html": {
    "title": "Does Audio Deepfake Detection Generalize?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/muller22b_interspeech.html": {
    "title": "Attacker Attribution of Audio Deepfakes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pierre22_interspeech.html": {
    "title": "Are disentangled representations all you need to build speaker anonymization systems?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/teixeira22_interspeech.html": {
    "title": "Towards End-to-End Private Automatic Speaker Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/amid22_interspeech.html": {
    "title": "Extracting Targeted Training Data from ASR Models, and How to Mitigate It",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22k_interspeech.html": {
    "title": "Detecting Unintended Memorization in Language-Model-Fused ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/taniguchi22_interspeech.html": {
    "title": "Transformer-Based Automatic Speech Recognition with Auxiliary Input of Source Language Text Toward Transcribing Simultaneous Interpretation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gabeur22_interspeech.html": {
    "title": "AVATAR: Unconstrained Audiovisual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22c_interspeech.html": {
    "title": "Word Discovery in Visually Grounded, Self-Supervised Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rose22_interspeech.html": {
    "title": "End-to-End multi-talker audio-visual ASR using an active speaker attention module",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/serdyuk22_interspeech.html": {
    "title": "Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition for Single and Muti-Person Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hong22_interspeech.html": {
    "title": "Visual Context-driven Audio Feature Enhancement for Robust End-to-End Audio-Visual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/harvill22_interspeech.html": {
    "title": "Frame-Level Stutter Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/priyasad22_interspeech.html": {
    "title": "Detecting Heart Failure Through Voice Analysis using Self-Supervised Mode-Based Memory Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ng22_interspeech.html": {
    "title": "Automatic Detection of Speech Sound Disorder in Child Speech Using Posterior-based Speaker Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/woszczyk22_interspeech.html": {
    "title": "Data Augmentation for Dementia Detection in Spoken Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dutta22b_interspeech.html": {
    "title": "Acoustic Representation Learning on Breathing and Speech Signals for COVID-19 Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bayerl22b_interspeech.html": {
    "title": "Detecting Dysfluencies in Stuttering Therapy Using wav2vec 2.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22b_interspeech.html": {
    "title": "HYU Submission for the SASV Challenge 2022: Reforming Speaker Embeddings with Spoofing-Aware Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/heo22_interspeech.html": {
    "title": "Two Methods for Spoofing-Aware Speaker Verification: Multi-Layer Perceptron Score Fusion Model and Integrated Embedding Projector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zeng22_interspeech.html": {
    "title": "Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/alenin22_interspeech.html": {
    "title": "A Subnetwork Approach for Spoofing Aware Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jung22c_interspeech.html": {
    "title": "SASV 2022: The First Spoofing-Aware Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22q_interspeech.html": {
    "title": "Representation Selective Self-distillation and wav2vec 2.0 Feature Exploration for Spoof-aware Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/westhausen22_interspeech.html": {
    "title": "tPLCnet: Real-time Deep Packet Loss Concealment in the Time Domain Using a Short Temporal Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tesch22_interspeech.html": {
    "title": "On the Role of Spatial, Spectral, and Temporal Processing for DNN-based Non-linear Multi-channel Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22e_interspeech.html": {
    "title": "DDS: A new device-degraded speech dataset for speech enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/du22d_interspeech.html": {
    "title": "Direction-Aware Joint Adaptation of Neural Speech Enhancement and Recognition in Real Multiparty Conversational Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bartolewska22_interspeech.html": {
    "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/welker22_interspeech.html": {
    "title": "Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ali22_interspeech.html": {
    "title": "Enhancing Embeddings for Speech Classification in Noisy Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/turetzky22_interspeech.html": {
    "title": "Deep Audio Waveform Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fras22_interspeech.html": {
    "title": "Convolutive Weighted Multichannel Wiener Filter Front-end for Distant Automatic Speech Recognition in Reverberant Multispeaker Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/deoliveira22_interspeech.html": {
    "title": "Efficient Transformer-based Speech Enhancement Using Long Frames and STFT Magnitudes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22x_interspeech.html": {
    "title": "Improving Speech Enhancement through Fine-Grained Speech Characteristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bilinski22_interspeech.html": {
    "title": "Creating New Voices using Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sanchez22_interspeech.html": {
    "title": "Unify and Conquer: How Phonetic Feature Representation Affects Polyglot Text-To-Speech (TTS)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/udagawa22_interspeech.html": {
    "title": "Human-in-the-loop Speaker Adaptation for DNN-based Multi-speaker TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/proszewska22_interspeech.html": {
    "title": "GlowVC: Mel-spectrogram space disentangling model for language-independent text-free voice conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22h_interspeech.html": {
    "title": "One-Shot Speaker Adaptation Based on Initialization by Generative Adversarial Networks for TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/levkovitch22_interspeech.html": {
    "title": "Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22j_interspeech.html": {
    "title": "Advanced Speaker Embedding with Predictive Variance of Gaussian Distribution for Speaker Adaptation in TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kakoulidis22_interspeech.html": {
    "title": "Karaoker: Alignment-free singing voice synthesis with speech training data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/um22_interspeech.html": {
    "title": "ACNN-VC: Utilizing Adaptive Convolution Neural Network for One-Shot Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sadekova22_interspeech.html": {
    "title": "A Unified System for Voice Cloning and Voice Conversion through Diffusion Probabilistic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22o_interspeech.html": {
    "title": "Adversarial Multi-Task Learning for Disentangling Timbre and Pitch in Singing Voice Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/agarwal22_interspeech.html": {
    "title": "Leveraging Symmetrical Convolutional Transformer Networks for Speech to Singing Voice Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/terashima22_interspeech.html": {
    "title": "Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22_interspeech.html": {
    "title": "Deep residual spiking neural network for keyword spotting in low-resource settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baskar22_interspeech.html": {
    "title": "Reducing Domain mismatch in Self-supervised speech pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhen22_interspeech.html": {
    "title": "Sub-8-Bit Quantization Aware Training for 8-Bit Neural Network Accelerator with On-Device Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22l_interspeech.html": {
    "title": "W2V2-Light: A Lightweight Version of Wav2vec 2.0 for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xie22_interspeech.html": {
    "title": "Compute Cost Amortized Transformer for Streaming ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vyas22_interspeech.html": {
    "title": "On-demand compute reduction with stochastic wav2vec 2.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vanderreydt22_interspeech.html": {
    "title": "Transfer Learning from Multi-Lingual Speech Translation Benefits Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22q_interspeech.html": {
    "title": "FeaRLESS: Feature Refinement Loss for Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kitamura22_interspeech.html": {
    "title": "Perceptual Evaluation of Penetrating Voices through a Semantic Differential Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tsukada22_interspeech.html": {
    "title": "Non-native Perception of Japanese Singleton/Geminate Contrasts: Comparison of Mandarin and Mongolian Speakers Differing in Japanese Experience",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/obrien22_interspeech.html": {
    "title": "Evaluating the effects of modified speech on perceptual speaker identification performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22i_interspeech.html": {
    "title": "Mandarin Lombard Grid: a Lombard-grid-like corpus of Standard Chinese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arai22_interspeech.html": {
    "title": "Syllable sequence of /a/+/ta/ can be heard as /atta/ in Japanese with visual or tactile cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22i_interspeech.html": {
    "title": "InQSS: a speech intelligibility and quality assessment model using a multi-task learning network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weise22b_interspeech.html": {
    "title": "Investigating the influence of personality on acoustic-prosodic entrainment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22t_interspeech.html": {
    "title": "Common and differential acoustic representation of interpersonal and tactile iconic perception of Mandarin vowels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/eranovic22_interspeech.html": {
    "title": "Effects of Noise on Speech Perception and Spoken Word Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22aa_interspeech.html": {
    "title": "Acquisition of Two Consecutive Neutral Tones in Mandarin-Speaking Preschoolers: Phonological Representation and Phonetic Realization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/roy22_interspeech.html": {
    "title": "Air tissue boundary segmentation using regional loss in real-time Magnetic Resonance Imaging video for speech production",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gibson22_interspeech.html": {
    "title": "Language-specific interactions of vowel discrimination in noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/antony22_interspeech.html": {
    "title": "An Improved Transformer Transducer Architecture for Hindi-English Code Switched Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kadandale22_interspeech.html": {
    "title": "VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dalhouse22_interspeech.html": {
    "title": "Cross-Lingual Transfer Learning Approach to Phoneme Error Detection via Latent Phonetic Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fukuda22_interspeech.html": {
    "title": "Global RNN Transducer Models For Multi-dialect Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bernhard22_interspeech.html": {
    "title": "Acoustic Stress Detection in Isolated English Words for Computer-Assisted Pronunciation Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pundak22_interspeech.html": {
    "title": "On-the-fly ASR Corrections with Audio Exemplars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/quan22_interspeech.html": {
    "title": "FFM: A Frame Filtering Mechanism To Accelerate Inference Speed For Conformer In Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cui22_interspeech.html": {
    "title": "Two-pass Decoding and Cross-adaptation Based System Combination of End-to-end Conformer and Hybrid TDNN ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ye22_interspeech.html": {
    "title": "Improving Recognition of Out-of-vocabulary Words in E2E Code-switching ASR by Fusing Speech Generation Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22n_interspeech.html": {
    "title": "Mitigating bias against non-native accents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22k_interspeech.html": {
    "title": "A Multi-level Acoustic Feature Extraction Framework for Transformer Based End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22c_interspeech.html": {
    "title": "LAE: Language-Aware Encoder for Monolingual and Multilingual ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pattanayak22_interspeech.html": {
    "title": "Significance of single frequency filter for the development of children's KWS system",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22n_interspeech.html": {
    "title": "A Language Agnostic Multilingual Streaming On-Device ASR System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22m_interspeech.html": {
    "title": "Minimizing Sequential Confusion Error in Speech Command Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schuppler22_interspeech.html": {
    "title": "Homophone Disambiguation Profits from Durational Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zuo22b_interspeech.html": {
    "title": "Speaker-Specific Utterance Ensemble based Transfer Attack on Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sadhu22_interspeech.html": {
    "title": "Complex Frequency Domain Linear Prediction: A Tool to Compute Modulation Spectrum of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/singh22b_interspeech.html": {
    "title": "Spectral Modification Based Data Augmentation For Improving End-to-End ASR For Children's Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/masumura22_interspeech.html": {
    "title": "End-to-End Joint Modeling of Conversation History-Dependent and Independent ASR Systems with Multi-History Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22da_interspeech.html": {
    "title": "Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22d_interspeech.html": {
    "title": "An Anchor-Free Detector for Continuous Speech Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22g_interspeech.html": {
    "title": "Low-complex and Highly-performed Binary Residual Neural Network for Small-footprint Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dinkel22_interspeech.html": {
    "title": "UniKW-AT: Unified Keyword Spotting and Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22n_interspeech.html": {
    "title": "ESSumm: Extractive Speech Summarization from Untranscribed Meeting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/conneau22_interspeech.html": {
    "title": "XTREME-S: Evaluating Cross-lingual Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22r_interspeech.html": {
    "title": "Negative Guided Abstractive Dialogue Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cui22c_interspeech.html": {
    "title": "Exploring representation learning for small-footprint keyword spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xue22d_interspeech.html": {
    "title": "Large-Scale Streaming End-to-End Speech Translation with Neural Transducers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22h_interspeech.html": {
    "title": "Phonetic Embedding for ASR Robustness in Entity Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22f_interspeech.html": {
    "title": "Hierarchical Tagger with Multi-task Learning for Cross-domain Slot Filling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22h_interspeech.html": {
    "title": "Multi-class AUC Optimization for Robust Small-footprint Keyword Spotting with Limited Training Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/martnek22_interspeech.html": {
    "title": "Weak supervision for Question Type Detection with large language models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22n_interspeech.html": {
    "title": "BIT-MI Deep Learning-based Model to Non-intrusive Speech Quality Assessment Challenge in Online Conferencing Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22o_interspeech.html": {
    "title": "MOS Prediction Network for Non-intrusive Speech Quality Assessment in Online Conferencing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shu22_interspeech.html": {
    "title": "Non-intrusive Speech Quality Assessment with a Multi-Task Learning based Subband Adaptive Attention Temporal Convolutional Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hao22_interspeech.html": {
    "title": "Soft-label Learn for No-Intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yi22b_interspeech.html": {
    "title": "ConferencingSpeech 2022 Challenge: Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/elhajal22_interspeech.html": {
    "title": "MOSRA: Joint Mean Opinion Score and Room Acoustics Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22v_interspeech.html": {
    "title": "CCATMos: Convolutional Context-aware Transformer Network for Non-intrusive Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22t_interspeech.html": {
    "title": "Impairment Representation Learning for Speech Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22l_interspeech.html": {
    "title": "Exploring linguistic feature and model combination for speech recognition based automatic AD detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22q_interspeech.html": {
    "title": "ECAPA-TDNN Based Depression Detection from Clinical Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ravi22_interspeech.html": {
    "title": "A Step Towards Preserving Speakers' Identity While Detecting Depression Via Speaker Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rutowski22_interspeech.html": {
    "title": "Toward Corpus Size Requirements for Training and Evaluating Depression Risk Models Using Spoken Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ablimit22_interspeech.html": {
    "title": "Deep Learning Approaches for Detecting Alzheimer's Dementia from Conversational Speech of ILSE Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/seneviratne22_interspeech.html": {
    "title": "Multimodal Depression Severity Score Prediction Using Articulatory Coordination Features and Hierarchical Attention Based Text Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meripo22_interspeech.html": {
    "title": "ASR Error Detection via Audio-Transcript entailment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/karlapati22_interspeech.html": {
    "title": "CopyCat2: A Single Model for Multi-Speaker TTS and Many-to-Many Fine-Grained Prosody Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/makarov22_interspeech.html": {
    "title": "Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nishimura22_interspeech.html": {
    "title": "Acoustic Modeling for End-to-End Empathetic Dialogue Speech Synthesis Using Linguistic and Prosodic Contexts of Dialogue History",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/seshadri22_interspeech.html": {
    "title": "Emphasis Control for Parallel Neural TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/stephenson22_interspeech.html": {
    "title": "BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/omahony22_interspeech.html": {
    "title": "Combining conversational speech with read speech to improve prosody in Text-to-Speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lu22_interspeech.html": {
    "title": "Unsupervised Data Selection via Discrete Speech Representation for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22i_interspeech.html": {
    "title": "CTRL: Continual Representation Learning to Transfer Information of Pre-trained for WAV2VEC 2.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baskar22b_interspeech.html": {
    "title": "Speaker adaptation for Wav2vec2 based dysarthric ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ba_interspeech.html": {
    "title": "Non-Parallel Voice Conversion for ASR Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/qi22_interspeech.html": {
    "title": "Improved Consistency Training for Semi-Supervised Sequence-to-Sequence ASR via Speech Chain Reconstruction and Self-Transcribing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arunkumar22_interspeech.html": {
    "title": "Joint Encoder-Decoder Self-Supervised Pre-training for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zellers22_interspeech.html": {
    "title": "An overview of discourse clicks in Central Swedish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/noguchi22_interspeech.html": {
    "title": "VOT and F0 perturbations for the realization of voicing contrast in Tohoku Japanese",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ridouane22_interspeech.html": {
    "title": "Complex sounds and cross-language influence: The case of ejectives in Omani Mehri",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hutin22_interspeech.html": {
    "title": "When Phonetics Meets Morphology: Intervocalic Voicing Within and Across Words in Romance Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kuang22b_interspeech.html": {
    "title": "The mapping between syntactic and prosodic phrasing in English and Mandarin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/buech22b_interspeech.html": {
    "title": "Pharyngealization in Amazigh: Acoustic and articulatory marking over time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pelloin22_interspeech.html": {
    "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22c_interspeech.html": {
    "title": "Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kumar22_interspeech.html": {
    "title": "Learning Under Label Noise for Robust Spoken Language Understanding systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/le22_interspeech.html": {
    "title": "Deliberation Model for On-Device Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yadav22b_interspeech.html": {
    "title": "Intent classification using pre-trained language agnostic embeddings for low resource languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arora22_interspeech.html": {
    "title": "Two-Pass Low Latency End-to-End Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/close22_interspeech.html": {
    "title": "Non-intrusive Speech Intelligibility Metric Prediction for Hearing Impaired Individuals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tu22_interspeech.html": {
    "title": "Exploiting Hidden Representations from a DNN-based Speech Recogniser for Speech Intelligibility Prediction in Hearing-impaired Listeners",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tu22b_interspeech.html": {
    "title": "Unsupervised Uncertainty Measures of Automatic Speech Recognition for Non-intrusive Speech Intelligibility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/robach22_interspeech.html": {
    "title": "Speech Intelligibility Prediction for Hearing-Impaired Listeners with the LEAP Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cardinale22_interspeech.html": {
    "title": "Predicting Speech Intelligibility using the Spike Acativity Mutual Information Index",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/barker22_interspeech.html": {
    "title": "The 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baas22_interspeech.html": {
    "title": "Voice Conversion Can Improve ASR in Very Low-Resource Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zevallos22_interspeech.html": {
    "title": "Data Augmentation for Low-Resource Quechua ASR Improvement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fatehi22_interspeech.html": {
    "title": "ScoutWav: Two-Step Fine-Tuning on Self-Supervised Automatic Speech Recognition for Low-Resource Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bandarupalli22_interspeech.html": {
    "title": "Semi-supervised Acoustic and Language Modeling for Hindi ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/berrebbi22_interspeech.html": {
    "title": "Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/robinson22_interspeech.html": {
    "title": "When Is TTS Augmentation Through a Pivot Language Useful?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rouhe22_interspeech.html": {
    "title": "Low Resource Comparison of Attention-based and Hybrid ASR Exploiting wav2vec 2.0",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bhanushali22_interspeech.html": {
    "title": "Gram Vaani ASR Challenge on spontaneous telephone speech recordings in regional variations of Hindi",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/manocha22_interspeech.html": {
    "title": "Audio Similarity is Unreliable as a Proxy for Audio Quality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22c_interspeech.html": {
    "title": "Overlapped Frequency-Distributed Network: Frequency-Aware Voice Spoofing Countermeasure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shrem22_interspeech.html": {
    "title": "Formant Estimation and Tracking using Probabilistic Heat-Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/eom22_interspeech.html": {
    "title": "Anti-Spoofing Using Transfer Learning with Variational Information Bottleneck",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/batra22_interspeech.html": {
    "title": "Robust Pitch Estimation Using Multi-Branch CNN-LSTM and 1-Norm LP Residual",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chernyak22_interspeech.html": {
    "title": "DeepFry: Identifying Vocal Fry Using Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wells22_interspeech.html": {
    "title": "Phonetic Analysis of Self-supervised Representations of English Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22p_interspeech.html": {
    "title": "FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dumpala22_interspeech.html": {
    "title": "On Combining Global and Localized Self-Supervised Models of Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dissanayake22_interspeech.html": {
    "title": "Self-supervised Representation Fusion for Speech and Wearable Based Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peyser22_interspeech.html": {
    "title": "Towards Disentangled Speech Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/quintas22_interspeech.html": {
    "title": "Automatic Assessment of Speech Intelligibility using Consonant Similarity for Head and Neck Cancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/neijman22_interspeech.html": {
    "title": "Compensation in Verbal and Nonverbal Communication after Total Laryngectomy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/getman22_interspeech.html": {
    "title": "wav2vec2-based Speech Rating System for Children with Speech Sound Disorder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/triantafyllopoulos22_interspeech.html": {
    "title": "Distinguishing between pre- and post-treatment in the speech of patients with chronic obstructive pulmonary disease",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22k_interspeech.html": {
    "title": "A Study on the Phonetic Inventory Development of Children with Cochlear Implants for 5 Years after Implantation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vitormenezes22_interspeech.html": {
    "title": "Evaluation of different antenna types and positions in a stepped frequency continuous-wave radar-based silent speech interface",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/abderrazek22_interspeech.html": {
    "title": "Validation of the Neuro-Concept Detector framework for the characterization of speech disorders: A comparative study including Dysarthria and Dysphonia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baumann22_interspeech.html": {
    "title": "Nonwords Pronunciation Classification in Language Development Tests for Preschool Children",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/benway22_interspeech.html": {
    "title": "PERCEPT-R: An Open-Access American English Child/Clinical Speech Corpus Specialized for the Audio Classification of //",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cao22b_interspeech.html": {
    "title": "Data Augmentation for End-to-end Silent Speech Recognition for Laryngectomees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kothare22_interspeech.html": {
    "title": "Statistical and clinical utility of multimodal dialogue-based speech and facial metrics for Parkinson's disease assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arco22_interspeech.html": {
    "title": "Evaluation of call centre conversations based on a high-level symbolic representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xu22j_interspeech.html": {
    "title": "Evoc-Learn  High quality simulation of early vocal learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/siddarth22_interspeech.html": {
    "title": "Watch Me Speak: 2D Visualization of Human Mouth during Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lesnichaia22_interspeech.html": {
    "title": "Classification of Accented English Using CNN Model Trained on Amplitude Mel-Spectrograms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kang22c_interspeech.html": {
    "title": "MIM-DG: Mutual information minimization-based domain generalization for speaker verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liang22b_interspeech.html": {
    "title": "Multi-Channel Far-Field Speaker Verification with Large-Scale Ad-hoc Microphone Arrays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lyu22_interspeech.html": {
    "title": "Ant Multilingual Recognition System for OLR 2021 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22b_interspeech.html": {
    "title": "Class-Aware Distribution Alignment based Unsupervised Domain Adaptation for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22m_interspeech.html": {
    "title": "EDITnet: A Lightweight Network for Unsupervised Domain Adaptation in Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22g_interspeech.html": {
    "title": "Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22j_interspeech.html": {
    "title": "Audio Visual Multi-Speaker Tracking with Improved GCF and PMBM Filter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22s_interspeech.html": {
    "title": "The HCCL System for the NIST SRE21",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22c_interspeech.html": {
    "title": "UNet-DenseNet for Robust Far-Field Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shao22b_interspeech.html": {
    "title": "Linguistic-Acoustic Similarity Based Accent Shift for Accent Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shen22b_interspeech.html": {
    "title": "Transducer-based language embedding for spoken language identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ga_interspeech.html": {
    "title": "Oriental Language Recognition (OLR) 2021: Summary and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kang22b_interspeech.html": {
    "title": "Mixup regularization strategies for spoofing countermeasure system",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ghosh22_interspeech.html": {
    "title": "Low-resource Low-footprint Wake-word Detection using Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ding22_interspeech.html": {
    "title": "Personal VAD 2.0: Optimizing Personal Voice Activity Detection for On-Device Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fan22_interspeech.html": {
    "title": "Token-level Speaker Change Detection Using Speaker Difference and Speech Content via Continuous Integrate-and-fire",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rho22_interspeech.html": {
    "title": "NAS-VAD: Neural Architecture Search for Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/larsen22_interspeech.html": {
    "title": "Adversarial Multi-Task Deep Learning for Noise-Robust Voice Activity Detection with Low Algorithmic Delay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xiao22_interspeech.html": {
    "title": "Rainbow Keywords: Efficient Incremental Learning for Online Spoken Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22e_interspeech.html": {
    "title": "Filler Word Detection and Classification: A Dataset and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kanda22_interspeech.html": {
    "title": "Streaming Multi-Talker ASR with Token-Level Serialized Output Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pesoparada22_interspeech.html": {
    "title": "pMCT: Patched Multi-Condition Training for Robust Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/novitasari22_interspeech.html": {
    "title": "Improving ASR Robustness in Noisy Condition Through VAD Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takeda22_interspeech.html": {
    "title": "Empirical Sampling from Latent Utterance-wise Evidence Model for Missing Data ASR based on Neural Encoder-Decoder Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhuang22_interspeech.html": {
    "title": "Coarse-Grained Attention Fusion With Joint Training Framework for Complex Speech Enhancement and End-to-End Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22b_interspeech.html": {
    "title": "DENT-DDSP: Data-efficient noisy speech generator using differentiable digital signal processors for explicit distortion modelling and noise-robust speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wei22c_interspeech.html": {
    "title": "Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gao22d_interspeech.html": {
    "title": "Federated Self-supervised Speech Representations: Are We There Yet?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22x_interspeech.html": {
    "title": "Leveraging Real Conversational Data for Multi-Channel Continuous Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22g_interspeech.html": {
    "title": "End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bando22_interspeech.html": {
    "title": "Weakly-Supervised Neural Full-Rank Spatial Covariance Analysis for a Front-End System of Distant Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/omalley22_interspeech.html": {
    "title": "A universally-deployable ASR frontend for joint acoustic echo cancellation, speech enhancement, and voice separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rajchetupalli22_interspeech.html": {
    "title": "Speaker conditioned acoustic modeling for multi-speaker conversational ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/das22_interspeech.html": {
    "title": "Hear No Evil: Towards Adversarial Robustness of Automatic Speech Recognition via Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22f_interspeech.html": {
    "title": "Tandem Multitask Training of Speaker Diarisation and Speech Recognition for Meeting Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/farooq22_interspeech.html": {
    "title": "Investigating the Impact of Crosslingual Acoustic-Phonetic Similarities on Multilingual Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shen22_interspeech.html": {
    "title": "An Improved Deliberation Network with Text Pre-training for Code-Switching Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22e_interspeech.html": {
    "title": "CyclicAugment: Speech Data Random Augmentation with Cosine Annealing Scheduler for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nie22_interspeech.html": {
    "title": "Prompt-based Re-ranking Language Model for ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22g_interspeech.html": {
    "title": "Avoid Overfitting User Specific Information in Federated Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22g_interspeech.html": {
    "title": "ASR Error Correction with Constrained Decoding on Operation Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pham22_interspeech.html": {
    "title": "Adaptive multilingual speech recognition with pretrained models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/thithuuyen22_interspeech.html": {
    "title": "Vietnamese Capitalization and Punctuation Recovery Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/futami22_interspeech.html": {
    "title": "Non-autoregressive Error Correction for CTC-based ASR with Phone-conditioned Masked LM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22x_interspeech.html": {
    "title": "reducing multilingual context confusion for end-to-end code-switching automatic speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tsunoo22_interspeech.html": {
    "title": "Residual Language Model for End-to-end Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22c_interspeech.html": {
    "title": "An Empirical Study of Language Model Integration for Transducer based Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22s_interspeech.html": {
    "title": "Self-Normalized Importance Sampling for Neural Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fox22_interspeech.html": {
    "title": "Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/udagawa22b_interspeech.html": {
    "title": "Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22e_interspeech.html": {
    "title": "Language-specific Characteristic Assistance for Code-switching Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/irino22_interspeech.html": {
    "title": "Speech intelligibility of simulated hearing loss sounds and its prediction using the Gammachirp Envelope Similarity Index (GESI)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huckvale22_interspeech.html": {
    "title": "ELO-SPHERES intelligibility prediction model for the Clarity Prediction Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22u_interspeech.html": {
    "title": "Listening with Googlears: Low-Latency Neural Multiframe Beamforming and Equalization for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/edozezario22_interspeech.html": {
    "title": "MBI-Net: A Non-Intrusive Multi-Branched Speech Intelligibility Prediction Model for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tan22c_interspeech.html": {
    "title": "A Deep Learning Platform for Language Education Research and Development",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jin22b_interspeech.html": {
    "title": "A VR Interactive 3D Mandarin Pronunciation Teaching Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/strom22_interspeech.html": {
    "title": "Squashed Weight Distribution for Low Bit Quantization of Deep Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hollands22_interspeech.html": {
    "title": "Evaluating the Performance of State-of-the-Art ASR Systems on Non-Native English using Corpora with Extensive Language Background Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/singla22_interspeech.html": {
    "title": "Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/roux22_interspeech.html": {
    "title": "Qualitative Evaluation of Language Model Rescoring in Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/faria22_interspeech.html": {
    "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22p_interspeech.html": {
    "title": "Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoon22_interspeech.html": {
    "title": "Predicting Emotional Intensity in Political Debates via Non-verbal Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeki22d_interspeech.html": {
    "title": "Confusion Detection for Adaptive Conversational Strategies of An Oral Proficiency Assessment Interview Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gent22_interspeech.html": {
    "title": "Deep Learning for Prosody-Based Irony Classification in Spontaneous Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ghosh22c_interspeech.html": {
    "title": "Span Classification with Structured Information for Disfluency Detection in Spoken Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22h_interspeech.html": {
    "title": "Example-based Explanations with Adversarial Attacks for Respiratory Sound Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rennie22_interspeech.html": {
    "title": "Which Model is Best: Comparing Methods and Metrics for Automatic Laughter Detection in a Naturalistic Conversational Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dissen22_interspeech.html": {
    "title": "Self-supervised Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lepage22_interspeech.html": {
    "title": "Label-Efficient Self-Supervised Speaker Verification With Information Maximization and Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kawa22_interspeech.html": {
    "title": "Attack Agnostic Dataset: Towards Generalization and Stabilization of Audio DeepFake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cho22c_interspeech.html": {
    "title": "Non-contrastive self-supervised learning of utterance-level speech representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mohammadamini22_interspeech.html": {
    "title": "Barlow Twins self-supervised learning for robust speaker recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/puffay22_interspeech.html": {
    "title": "Relating the fundamental frequency of speech with EEG using a dilated convolutional network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/defino22_interspeech.html": {
    "title": "Prediction of L2 speech proficiency based on multi-level linguistic features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rana22_interspeech.html": {
    "title": "The effect of increasing acoustic and linguistic complexity on auditory processing: an EEG study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22z_interspeech.html": {
    "title": "Recording and timing vocal responses in online experimentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cordero22_interspeech.html": {
    "title": "Neural correlates of acoustic and semantic cues during speech segmentation in French",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meng22d_interspeech.html": {
    "title": "Evidence of Onset and Sustained Neural Responses to Isolated Phonemes from Intracranial Recordings in a Voice-based Cursor Control Task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mdhaffar22_interspeech.html": {
    "title": "End-to-end model for named entity recognition from speech without paired training data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meeus22_interspeech.html": {
    "title": "Multitask Learning for Low Resource Spoken Language Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jayesh22_interspeech.html": {
    "title": "Transformer Networks for Non-Intrusive Speech Quality Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tamm22_interspeech.html": {
    "title": "Pre-trained Speech Representations as Feature Extractors for Speech Quality Assessment in Online Conferencing Applications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/becerra22_interspeech.html": {
    "title": "Exploring the influence of fine-tuning data on wav2vec 2.0 model for blind speech quality prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22r_interspeech.html": {
    "title": "MAESTRO: Matched Speech Text Representations through Modality Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22b_interspeech.html": {
    "title": "FiLM Conditioning with Enhanced Feature to the Transformer-based End-to-End Noisy Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ristea22_interspeech.html": {
    "title": "SepTr: Separable Transformer for Audio Spectrogram Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/horii22_interspeech.html": {
    "title": "End-to-End Spontaneous Speech Recognition Using Disfluency Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/olivier22_interspeech.html": {
    "title": "Recent improvements of ASR models in the face of adversarial attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shim22_interspeech.html": {
    "title": "Similarity and Content-based Phonetic Self Attention for Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22f_interspeech.html": {
    "title": "Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22c_interspeech.html": {
    "title": "Knowledge distillation for In-memory keyword spotting model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meyer22_interspeech.html": {
    "title": "Automatic Learning of Subword Dependent Model Scales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bittar22_interspeech.html": {
    "title": "Bayesian Recurrent Units and the Forward-Backward Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mei22_interspeech.html": {
    "title": "On Metric Learning for Audio-Text Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hou22_interspeech.html": {
    "title": "CT-SAT: Contextual Transformer for Sequential Audio Tagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22d_interspeech.html": {
    "title": "ADFF: Attention Based Deep Feature Fusion Approach for Music Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lei22b_interspeech.html": {
    "title": "Audio-Visual Scene Classification Based on Multi-modal Graph Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/reddy22_interspeech.html": {
    "title": "MusicNet: Compact Convolutional Neural Network for Real-time Background Music Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22h_interspeech.html": {
    "title": "iCNN-Transformer: An improved CNN-Transformer with Channel-spatial Attention and Keyword Prediction for Automated Audio Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22p_interspeech.html": {
    "title": "ATST: Audio Representation Learning with Teacher-Student Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22v_interspeech.html": {
    "title": "Deep Segment Model for Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sonowal22_interspeech.html": {
    "title": "Novel Augmentation Schemes for Device Robust Acoustic Scene Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22f_interspeech.html": {
    "title": "WideResNet with Joint Representation Learning and Data Augmentation for Cover Song Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/parikh22_interspeech.html": {
    "title": "Impact of Acoustic Event Tagging on Scene Classification in a Multi-Task Learning Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takeuchi22_interspeech.html": {
    "title": "Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pan22_interspeech.html": {
    "title": "Speaker recognition-assisted robust audio deepfake detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22_interspeech.html": {
    "title": "Preventing sensitive-word recognition using self-supervised learning to preserve user-privacy for automatic speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/pia22_interspeech.html": {
    "title": "NESC: Robust Neural End-2-End Speech Coding with GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xue22b_interspeech.html": {
    "title": "Towards Error-Resilient Neural Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jiang22_interspeech.html": {
    "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22x_interspeech.html": {
    "title": "Neural Vocoder is All You Need for Speech Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22y_interspeech.html": {
    "title": "VoiceFixer: A Unified Framework for High-Fidelity Speech Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/stoidis22_interspeech.html": {
    "title": "Generating gender-ambiguous voices for privacy-preserving speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22b_interspeech.html": {
    "title": "Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhan22_interspeech.html": {
    "title": "Exploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual Text-to-Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22e_interspeech.html": {
    "title": "WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22_interspeech.html": {
    "title": "Decoupled Pronunciation and Prosody Modeling in Meta-Learning-based Multilingual Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhuang22b_interspeech.html": {
    "title": "KaraTuner: Towards End-to-End Natural Pitch Correction for Singing Voice in Karaoke",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/xue22c_interspeech.html": {
    "title": "Learn2Sing 2.0: Diffusion and Mutual Information-Based Target Speaker SVS by Learning from Singing Teacher",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guo22e_interspeech.html": {
    "title": "SingAug: Data Augmentation for Singing Voice Synthesis with Cycle-consistent Training Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22d_interspeech.html": {
    "title": "Muskits: an End-to-end Music Processing Toolkit for Singing Voice Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22p_interspeech.html": {
    "title": "Pronunciation Dictionary-Free Multilingual Speech Synthesis by Combining Unsupervised and Supervised Phonetic Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22u_interspeech.html": {
    "title": "Towards high-fidelity singing voice conversion with acoustic reference and contrastive predictive coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22f_interspeech.html": {
    "title": "Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/manghat22_interspeech.html": {
    "title": "Normalization of code-switched text for speech synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chung22_interspeech.html": {
    "title": "Synthesizing Near Native-accented Speech for a Non-native Speaker by Imitating the Pronunciation and Prosody of a Native Speaker",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22da_interspeech.html": {
    "title": "A Hierarchical Speaker Representation Framework for One-shot Singing Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22c_interspeech.html": {
    "title": "Self-Supervised Learning with Multi-Target Contrastive Coding for Non-Native Acoustic Modeling of Mispronunciation Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22_interspeech.html": {
    "title": "L2-GEN: A Neural Phoneme Paraphrasing Approach to L2 Speech Synthesis for Mispronunciation Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dutta22_interspeech.html": {
    "title": "Challenges remain in Building ASR for Spontaneous Preschool Children Speech in Naturalistic Educational Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22p_interspeech.html": {
    "title": "End-to-end Mispronunciation Detection with Simulated Error Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22q_interspeech.html": {
    "title": "BiCAPT: Bidirectional Computer-Assisted Pronunciation Training with Normalizing Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fu22b_interspeech.html": {
    "title": "Using Fluency Representation Learned from Sequential Raw Features for Improving Non-native Fluency Scoring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22l_interspeech.html": {
    "title": "An Alignment Method Leveraging Articulatory Features for Mispronunciation Detection and Diagnosis in L2 English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nidadavolu22_interspeech.html": {
    "title": "RefTextLAS: Reference Text Biased Listen, Attend, and Spell Model For Accurate Reading Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22e_interspeech.html": {
    "title": "CoCA-MDD: A Coupled Cross-Attention based Framework for Streaming Mispronunciation Detection and Diagnosis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22g_interspeech.html": {
    "title": "Spoofing-Aware Speaker Verification by Multi-Level Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kang22_interspeech.html": {
    "title": "End-to-end framework for spoof-aware speaker verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22_interspeech.html": {
    "title": "The CLIPS System for 2022 Spoofing-Aware Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22f_interspeech.html": {
    "title": "Norm-constrained Score-level Ensemble for Spoofing Aware Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22s_interspeech.html": {
    "title": "SASV Based on Pre-trained ASV System and Integrated Scoring Module",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22w_interspeech.html": {
    "title": "Backend Ensemble for Speaker Verification and Spoofing Countermeasure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tan22b_interspeech.html": {
    "title": "NRI-FGSM: An Efficient Transferable Adversarial Attack for Speaker Recognition Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/teng22_interspeech.html": {
    "title": "SA-SASV: An End-to-End Spoof-Aggregated Spoofing-Aware Speaker Verification System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ea_interspeech.html": {
    "title": "The DKU-OPPO System for the 2022 Spoofing-Aware Speaker Verification Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/han22_interspeech.html": {
    "title": "NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeki22b_interspeech.html": {
    "title": "SelfRemaster: Self-Supervised Speech Restoration with Analysis-by-Synthesis Approach Using Channel Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/byun22_interspeech.html": {
    "title": "Optimization of Deep Neural Network (DNN) Speech Coder Using a Multi Time Scale Perceptual Loss Function",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22m_interspeech.html": {
    "title": "Phase Vocoder For Time Stretch Based On Center Frequency Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/siahkoohi22_interspeech.html": {
    "title": "Ultra-Low-Bitrate Speech Coding with Pretrained Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/miao22_interspeech.html": {
    "title": "Analyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/radfar22_interspeech.html": {
    "title": "ConvRNN-T: Convolutional Augmented Recurrent Neural Network Transducers for Streaming Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22f_interspeech.html": {
    "title": "Knowledge Distillation via Module Replacing for Automatic Speech Recognition with Recurrent Neural Network Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22g_interspeech.html": {
    "title": "Memory-Efficient Training of RNN-Transducer with Sampled Softmax",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/do22_interspeech.html": {
    "title": "Multiple-hypothesis RNN-T Loss for Unsupervised Fine-tuning and Self-training of Neural Transducer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sklyar22_interspeech.html": {
    "title": "Separator-Transducer-Segmenter: Streaming Recognition and Segmentation of Multi-party Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wong22_interspeech.html": {
    "title": "Variations of multi-task learning for spoken language assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kunihara22b_interspeech.html": {
    "title": "Detection of Learners' Listening Breakdown with Oral Dictation and Its Use to Model Listening Skill Improvement Exclusively Through Shadowing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/suzuki22b_interspeech.html": {
    "title": "Automatic Prosody Evaluation of L2 English Read Speech in Reference to Accent Dictionary with Transformer Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/banno22_interspeech.html": {
    "title": "View-Specific Assessment of L2 Spoken English",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bai22b_interspeech.html": {
    "title": "The Effects of Implicit and Explicit Feedback in an ASR-based Reading Tutor for Dutch First-graders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22v_interspeech.html": {
    "title": "Improving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sakuma22_interspeech.html": {
    "title": "Response Timing Estimation for Spoken Dialog System using Dialog Act Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jabeen22_interspeech.html": {
    "title": "Hesitations in Urdu/Hindi: Distribution and Properties of Fillers & Silences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/girish22_interspeech.html": {
    "title": "Interpretabilty of Speech Emotion Recognition modelled using Self-Supervised Speech and Text Pre-Trained Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kumar22b_interspeech.html": {
    "title": "Does Utterance entails Intent?: Evaluating Natural Language Inference Based Setup for Few-Shot Intent Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wallbridge22_interspeech.html": {
    "title": "Investigating perception of spoken dialogue acceptability through surprisal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hori22_interspeech.html": {
    "title": "Low-Latency Online Streaming VideoQA Using Audio-Visual Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/stan22_interspeech.html": {
    "title": "The ZevoMOS entry to VoiceMOS Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saeki22c_interspeech.html": {
    "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22b_interspeech.html": {
    "title": "Automatic Mean Opinion Score Estimation with Temporal Modulation Features on Gammatone Filterbank for Speech Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chinen22_interspeech.html": {
    "title": "Using Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22f_interspeech.html": {
    "title": "The VoiceMOS Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tseng22b_interspeech.html": {
    "title": "DDOS: A MOS Prediction Framework utilizing Domain Adaptive Pre-training and Distribution of Opinion Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ammarabbas22_interspeech.html": {
    "title": "Expressive, Variable, and Controllable Duration Modelling in TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nakata22_interspeech.html": {
    "title": "Predicting VQVAE-based Character Acting Style from Quotation-Annotated Text for Audiobook Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22g_interspeech.html": {
    "title": "Adversarial and Sequential Training for Cross-lingual Prosody Transfer TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22j_interspeech.html": {
    "title": "FluentTTS: Text-dependent Fine-grained Style Control for Multi-style TTS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22g_interspeech.html": {
    "title": "Few Shot Cross-Lingual TTS Using Transferable Phoneme Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/finkelstein22_interspeech.html": {
    "title": "Training Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoshioka22_interspeech.html": {
    "title": "Spoken-Text-Style Transfer with Conditional Variational Autoencoder and Content Word Storage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kulkarni22_interspeech.html": {
    "title": "Analysis of expressivity transfer in non-autoregressive end-to-end multispeaker TTS systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rattcliffe22_interspeech.html": {
    "title": "Cross-lingual Style Transfer with Conditional Prior VAE and Style Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zaidi22b_interspeech.html": {
    "title": "Daft-Exprt: Cross-Speaker Prosody Transfer on Any Text for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoon22b_interspeech.html": {
    "title": "Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mukherjee22_interspeech.html": {
    "title": "Text aware Emotional Text-to-speech with BERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mateju22_interspeech.html": {
    "title": "Overlapped Speech Detection in Broadcast Streams Using X-vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/segal22_interspeech.html": {
    "title": "DDKtor: Automatic Diadochokinetic Speech Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meneses22_interspeech.html": {
    "title": "SiDi KWS: A Large-Scale Multilingual Dataset for Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22h_interspeech.html": {
    "title": "Dummy Prototypical Networks for Few-Shot Open-Set Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sarkar22_interspeech.html": {
    "title": "Unsupervised Voice Activity Detection by Modeling Source and System Information using Zero Frequency Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sharon22_interspeech.html": {
    "title": "Multilingual and Multimodal Abuse Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mariotte22_interspeech.html": {
    "title": "Microphone Array Channel Combination Algorithms for Overlapped Speech Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sudo22_interspeech.html": {
    "title": "Streaming Automatic Speech Recognition with Re-blocking Processing Based on Integrated Voice Activity Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fuchs22_interspeech.html": {
    "title": "Unsupervised Word Segmentation using K Nearest Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22d_interspeech.html": {
    "title": "Investigation on the Band Importance of Phase-aware Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sun22b_interspeech.html": {
    "title": "Unsupervised Acoustic-to-Articulatory Inversion with Variable Vocal Tract Anatomy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sun22c_interspeech.html": {
    "title": "Unsupervised Inference of Physiologically Meaningful Articulatory Trajectories with VocalTractLab",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22i_interspeech.html": {
    "title": "Radio2Speech: High Quality Speech Recovery from Radio Frequency Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nabe22_interspeech.html": {
    "title": "Isochronous is beautiful? Syllabic event detection in a neuro-inspired oscillatory model is facilitated by isochrony in speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liebig22_interspeech.html": {
    "title": "An investigation of regression-based prediction of the femininity or masculinity in speech of transgender people",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/parikh22b_interspeech.html": {
    "title": "Acoustic To Articulatory Speech Inversion Using Multi-Resolution Spectro-Temporal Representations Of Speech Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lian22b_interspeech.html": {
    "title": "Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22ea_interspeech.html": {
    "title": "Vocal-Tract Area Functions with Articulatory Reality for Tract Opening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22_interspeech.html": {
    "title": "Coupled Discriminant Subspace Alignment for Cross-database Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/santoso22_interspeech.html": {
    "title": "Performance Improvement of Speech Emotion Recognition by Neutral Speech Detection Using Autoencoder and Intermediate Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22c_interspeech.html": {
    "title": "A Graph Isomorphism Network with Weighted Multiple Aggregators for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/baruah22_interspeech.html": {
    "title": "Speech Emotion Recognition via Generation using an Attention-based Variational Recurrent Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mitra22_interspeech.html": {
    "title": "Speech Emotion: Investigating Model Representations, Multi-Task Learning and Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22e_interspeech.html": {
    "title": "Multiple Enhancements to LSTM for Learning Emotion-Salient Features in Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22k_interspeech.html": {
    "title": "Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22m_interspeech.html": {
    "title": "CTA-RNN: Channel and Temporal-wise Attention RNN leveraging Pre-trained ASR Embeddings for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/velichko22_interspeech.html": {
    "title": "Complex Paralinguistic Analysis of Speech: Predicting Gender, Emotions and Deception in a Hierarchical Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/takashima22b_interspeech.html": {
    "title": "Interactive Co-Learning with Cross-Modal Transformer for Audio-Visual Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kang22d_interspeech.html": {
    "title": "SpeechEQ: Speech Emotion Recognition based on Multi-scale Unified Datasets and Multitask Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22aa_interspeech.html": {
    "title": "Discriminative Feature Representation Based on Cascaded Attention Network with Adversarial Joint Loss for Speech Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/audibert22_interspeech.html": {
    "title": "Intra-speaker phonetic variation in read speech: comparison with inter-speaker variability in a controlled population",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vaessen22_interspeech.html": {
    "title": "Training speaker recognition systems with limited data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lou22_interspeech.html": {
    "title": "A Deep One-Class Learning Method for Replay Attack Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22e_interspeech.html": {
    "title": "A Universal Identity Backdoor Attack against Speaker Verification based on Siamese Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22h_interspeech.html": {
    "title": "A Novel Phoneme-based Modeling for Text-independent Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/han22b_interspeech.html": {
    "title": "Self-Supervised Speaker Verification Using Dynamic Loss-Gate and Label Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shi22c_interspeech.html": {
    "title": "Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22r_interspeech.html": {
    "title": "Acoustic Feature Shuffling Network for Text-independent Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wen22_interspeech.html": {
    "title": "Multi-Path GMM-MobileNet Based on Attack Algorithms and Codecs for Synthetic Speech and Deepfake Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jin22_interspeech.html": {
    "title": "Adversarial Reweighting for Speaker Verification Fairness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chen22s_interspeech.html": {
    "title": "Graph-based Multi-View Fusion and Local Adaptation: Mitigating Within-Household Confusability for Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zuo22_interspeech.html": {
    "title": "Local Context-aware Self-attention for Continuous Sign Language Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/weise22_interspeech.html": {
    "title": "Disentangled Latent Speech Representation for Automatic Pathological Intelligibility Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/song22_interspeech.html": {
    "title": "Improving Hypernasality Estimation with Automatic Speech Recognition in Cleft Palate Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22k_interspeech.html": {
    "title": "Conformer Based Elderly Speech Recognition System for Alzheimer's Disease Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fan22c_interspeech.html": {
    "title": "Revisiting visuo-spatial processing in individuals with congenital amusia",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/aminidigehsara22_interspeech.html": {
    "title": "A user-friendly headset for radar-based silent speech recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/cheng22c_interspeech.html": {
    "title": "A study of production error analysis for Mandarin-speaking Children with Hearing Impairment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huo22_interspeech.html": {
    "title": "Incremental Layer-Wise Self-Supervised Learning for Efficient Unsupervised Speech Domain Adaptation On Device",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/farooq22b_interspeech.html": {
    "title": "Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22c_interspeech.html": {
    "title": "The THUEE System Description for the IARPA OpenASR21 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhong22_interspeech.html": {
    "title": "External Text Based Data Augmentation for Low-Resource Speech Recognition in the Constrained Condition of OpenASR21 Challenge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lonergan22_interspeech.html": {
    "title": "Cross-dialect lexicon optimisation for an endangered language ASR system: the case of Irish",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhu22c_interspeech.html": {
    "title": "Wav2vec-S: Semi-Supervised Pre-Training for Low-Resource ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/schraner22_interspeech.html": {
    "title": "Comparison of Unsupervised Learning and Supervised Learning with Noisy Labels for Low-Resource Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/patel22_interspeech.html": {
    "title": "Using cross-model learnings for the Gram Vaani ASR Challenge 2022",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22aa_interspeech.html": {
    "title": "ASR2K: Speech Recognition for Around 2000 Languages without Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/damania22_interspeech.html": {
    "title": "Combining Simple but Novel Data Augmentation Methods for Improving Conformer ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peterson22_interspeech.html": {
    "title": "OpenASR21: The Second Open Challenge for Automatic Speech Recognition of Low-Resource Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fan22d_interspeech.html": {
    "title": "DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised Learning and Its Application to Children's ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/guillaume22_interspeech.html": {
    "title": "Plugging a neural phoneme recognizer into a simple language model: a workflow for low-resource setting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/choi22e_interspeech.html": {
    "title": "An Evaluation of Three-Stage Voice Conversion Framework for Noisy and Reverberant Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22t_interspeech.html": {
    "title": "An Overview & Analysis of Sequence-to-Sequence Emotional Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/quamer22_interspeech.html": {
    "title": "Zero-Shot Foreign Accent Conversion without a Native Reference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meyer22b_interspeech.html": {
    "title": "Speaker Anonymization with Phonetic Intermediate Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kuhlmann22_interspeech.html": {
    "title": "Investigation into Target Speaking Rate Adaptation for Voice Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/klapsas22_interspeech.html": {
    "title": "Self supervised learning for robust voice cloning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hu22_interspeech.html": {
    "title": "Improving Deliberation by Text-Only and Semi-Supervised Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22e_interspeech.html": {
    "title": "K-Wav2vec 2.0: Automatic Speech Recognition based on Joint Decoding of Graphemes and Syllables",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sriram22_interspeech.html": {
    "title": "Wav2Vec-Aug: Improved self-supervised training with limited data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kocour22_interspeech.html": {
    "title": "Revisiting joint decoding based multi-talker speech recognition with DNN acoustic model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/novak22_interspeech.html": {
    "title": "RNN-T lattice enhancement by grafting of pruned paths",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/komatsu22_interspeech.html": {
    "title": "Better Intermediates Improve CTC Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gessinger22_interspeech.html": {
    "title": "Cross-Cultural Comparison of Gradient Emotion Perception: Human vs. Alexa TTS Voices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kasun22_interspeech.html": {
    "title": "Discriminative Adversarial Learning for Speaker Independent Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/suzuki22_interspeech.html": {
    "title": "Representing 'how you say' with 'what you say': English corpus of focused speech and text reflecting corresponding implications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/salais22_interspeech.html": {
    "title": "Production Strategies of Vocal Attitudes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kirkland22_interspeech.html": {
    "title": "Where's the uh, hesitation? The interplay between filled pause location, speech rate and fundamental frequency in perception of confidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22_interspeech.html": {
    "title": "E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yeh22_interspeech.html": {
    "title": "Autoregressive Co-Training for Learning Discrete Speech Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22e_interspeech.html": {
    "title": "An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lebourdais22_interspeech.html": {
    "title": "Overlapped speech and gender detection with WavLM pre-trained features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/teytaut22_interspeech.html": {
    "title": "A study on constraining Connectionist Temporal Classification for temporal audio alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/siriwardena22_interspeech.html": {
    "title": "Acoustic-to-articulatory Speech Inversion with Multi-task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/maouche22_interspeech.html": {
    "title": "Enhancing Speech Privacy with Slicing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22h_interspeech.html": {
    "title": "An Attention-Based Method for Guiding Attribute-Aligned Speech Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/joshi22_interspeech.html": {
    "title": "Defense against Adversarial Attacks on Hybrid Speech Recognition System using Adversarial Fine-tuning with Denoiser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tseng22_interspeech.html": {
    "title": "Membership Inference Attacks Against Self-supervised Speech Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/shao22c_interspeech.html": {
    "title": "Chunking Defense for Adversarial Attacks on ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/feng22_interspeech.html": {
    "title": "Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/feng22b_interspeech.html": {
    "title": "User-Level Differential Privacy against Attribute Inference Attack of Speech Emotion Recognition on Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/joshi22b_interspeech.html": {
    "title": "AdvEst: Adversarial Perturbation Estimation to Classify and Detect Adversarial Attacks against Speaker Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yoo22_interspeech.html": {
    "title": "Online Learning of Open-set Speaker Identification by Active User-registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/salim22_interspeech.html": {
    "title": "Automatic Speaker Verification System for Dysarthria Patients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/flemotomos22_interspeech.html": {
    "title": "Multimodal Clustering with Role Induced Constraints for Speaker Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/park22d_interspeech.html": {
    "title": "Multi-scale Speaker Diarization with Dynamic Scale Weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chaubey22_interspeech.html": {
    "title": "Improved Relation Networks for End-to-End Speaker Verification and Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rybicka22_interspeech.html": {
    "title": "End-to-End Neural Speaker Diarization with an Iterative Refinement of Non-Autoregressive Attention-based Attractors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/landini22_interspeech.html": {
    "title": "From Simulated Mixtures to Simulated Conversations as Training Data for End-to-End Neural Diarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ide22_interspeech.html": {
    "title": "Can Humans Correct Errors From System? Investigating Error Tendencies in Speaker Identification Using Crowdsourcing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kim22n_interspeech.html": {
    "title": "Light-Weight Speaker Verification with Global Context Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/peng22e_interspeech.html": {
    "title": "Learnable Sparse Filterbank for Speaker Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/sapru22_interspeech.html": {
    "title": "Using Data Augmentation and Consistency Regularization to Improve Semi-supervised Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/mai22_interspeech.html": {
    "title": "Unsupervised domain adaptation for speech recognition with unsupervised error correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/biadsy22_interspeech.html": {
    "title": "A Scalable Model Specialization Framework for Training and Inference using Submodels and its Application to Speech Model Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dieck22_interspeech.html": {
    "title": "Wav2vec behind the Scenes: How end2end Models learn Phonetics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zheng22d_interspeech.html": {
    "title": "Scaling ASR Improves Zero and Few Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nakagome22_interspeech.html": {
    "title": "InterAug: Augmenting Noisy Intermediate Predictions for CTC-based ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/arunkumar22b_interspeech.html": {
    "title": "Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22b_interspeech.html": {
    "title": "Dynamic Sliding Window Modeling for Abstractive Meeting Summarization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saito22_interspeech.html": {
    "title": "STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rumberg22_interspeech.html": {
    "title": "kidsTALC: A Corpus of 3- to 11-year-old German Children's Connected Natural Speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lin22c_interspeech.html": {
    "title": "DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jung22b_interspeech.html": {
    "title": "Asymmetric Proxy Loss for Multi-View Acoustic Word Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chang22f_interspeech.html": {
    "title": "Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22c_interspeech.html": {
    "title": "Building Vietnamese Conversational Smart Home Dataset and Natural Language Understanding Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ghosh22b_interspeech.html": {
    "title": "DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ekstedt22_interspeech.html": {
    "title": "Voice Activity Projection: Self-supervised Learning of Turn-taking Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/popuri22_interspeech.html": {
    "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/huang22l_interspeech.html": {
    "title": "QbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using MLPMixer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/gharbieh22_interspeech.html": {
    "title": "DyConvMixer: Dynamic Convolution Mixer Architecture for Open-Vocabulary Keyword Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/belitz22_interspeech.html": {
    "title": "Challenges in Metadata Creation for Massive Naturalistic Team-Based Audio Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nicmanis22_interspeech.html": {
    "title": "Spoken Dialogue System for Call Centers with Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/draxler22_interspeech.html": {
    "title": "OCTRA  An Innovative Approach to Orthographic Transcription",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/vandevreken22_interspeech.html": {
    "title": "Voice Puppetry with FastPitch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/paul22_interspeech.html": {
    "title": "Improving Data Driven Inverse Text Normalization using Data Augmentation and Machine Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22_interspeech.html": {
    "title": "Native phonotactic interference in L2 vowel processing: Mouse-tracking reveals cognitive conflicts during identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luo22b_interspeech.html": {
    "title": "Mandarin nasal place assimilation revisited: an acoustic study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kaland22_interspeech.html": {
    "title": "Bending the string: intonation contour length as a correlate of macro-rhythm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/hughes22_interspeech.html": {
    "title": "Eliciting and evaluating likelihood ratios for speaker recognition by human listeners under forensically realistic channel-mismatched conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22d_interspeech.html": {
    "title": "Reducing uncertainty at the score-to-LR stage in likelihood ratio-based forensic voice comparison using automatic speaker recognition systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lee22f_interspeech.html": {
    "title": "Durational Patterning at Discourse Boundaries in Relation to Therapist Empathy in Psychotherapy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kadiri22_interspeech.html": {
    "title": "Convolutional Neural Networks for Classification of Voice Qualities from Speech and Neck Surface Accelerometer Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/furukawa22_interspeech.html": {
    "title": "Applying SyntaxProsody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22u_interspeech.html": {
    "title": "Effects of Language Contact on Vowel Nasalization in Wenzhou and Rugao Dialects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/young22_interspeech.html": {
    "title": "A blueprint for using deepfakes in sociolinguistic matched-guise experiments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22e_interspeech.html": {
    "title": "Mandarin Tone Sandhi Realization: Evidence from Large Speech Corpora",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/le22b_interspeech.html": {
    "title": "A Laryngographic Study on the Voice Quality of Northern Vietnamese Tones under the Lombard Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zygis22_interspeech.html": {
    "title": "The Prosody of Cheering in Sport Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22ca_interspeech.html": {
    "title": "Contribution of the glottal flow residual in affect-related voice transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/carne22_interspeech.html": {
    "title": "High level feature fusion in forensic voice comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/begus22_interspeech.html": {
    "title": "Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jun22_interspeech.html": {
    "title": "Paraguayan Guarani: Tritonal pitch accent and Accentual Phrase",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zeng22b_interspeech.html": {
    "title": "Low-resource Accent Classification in Geographically-proximate Settings: A Forensic and Sociophonetics Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/luo22_interspeech.html": {
    "title": "Tiny-Sepformer: A Tiny Time-Domain Transformer Network For Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22_interspeech.html": {
    "title": "Speaker-Aware Mixture of Mixtures Training for Weakly Supervised Speaker Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lutati22_interspeech.html": {
    "title": "SepIt: Approaching a Single Channel Speech Separation Bound",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22d_interspeech.html": {
    "title": "On the Use of Deep Mask Estimation Module for Neural Source Separation Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22b_interspeech.html": {
    "title": "Target Confusion in End-to-end Speaker Extraction: Analysis and Approaches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22d_interspeech.html": {
    "title": "Embedding Recurrent Layers with Dual-Path Strategy in a Variant of Convolutional Network for Speaker-Independent Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22c_interspeech.html": {
    "title": "Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/ivry22_interspeech.html": {
    "title": "Objective Metrics to Evaluate Residual-Echo Suppression During Double-Talk in the Stereophonic Case",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/rixen22_interspeech.html": {
    "title": "QDPN - Quasi-dual-path Network for single-channel Speech Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lu22b_interspeech.html": {
    "title": "Conformer Space Neural Architecture Search for Multi-Task Audio Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kopuklu22_interspeech.html": {
    "title": "ResectNet: An Efficient Architecture for Voice Activity Detection on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22l_interspeech.html": {
    "title": "Gated Convolutional Fusion for Time-Domain Target Speaker Extraction Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wang22p_interspeech.html": {
    "title": "WA-Transformer: Window Attention-based Transformer with Two-stage Strategy for Multi-task Audio Source Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/quan22b_interspeech.html": {
    "title": "Multichannel Speech Separation with Narrow-band Conformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhang22y_interspeech.html": {
    "title": "Separating Long-Form Speech with Group-wise Permutation Invariant Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/paturi22_interspeech.html": {
    "title": "Directed speech separation for automatic speech recognition of long form conversational speech",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chetupalli22_interspeech.html": {
    "title": "Speech Separation for an Unknown Number of Speakers Using Transformers With Encoder-Decoder Attractors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/corey22_interspeech.html": {
    "title": "Cooperative Speech Separation With a Microphone Array and Asynchronous Wearable Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/kilgour22_interspeech.html": {
    "title": "Text-Driven Separation of Arbitrary Sounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/parikh22c_interspeech.html": {
    "title": "An Empirical Analysis on the Vulnerabilities of End-to-End Speech Segregation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22c_interspeech.html": {
    "title": "TaylorBeamformer: Learning All-Neural Beamformer for Multi-Channel Speech Enhancement from Taylor's Approximation Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/iwamoto22_interspeech.html": {
    "title": "How bad are artifacts?: Analyzing the impact of speech enhancement errors on ASR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22_interspeech.html": {
    "title": "Multi-source wideband DOA estimation method by frequency focusing and error weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nossier22_interspeech.html": {
    "title": "Convolutional Recurrent Smart Speech Enhancement Architecture for Hearing Aids",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meng22b_interspeech.html": {
    "title": "Fully Automatic Balance between Directivity Factor and White Noise Gain for Large-scale Microphone Arrays in Diffuse Noise Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/tian22d_interspeech.html": {
    "title": "A Transfer and Multi-Task Learning based Approach for MOS Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/yang22o_interspeech.html": {
    "title": "Fusion of Self-supervised Learned Models for MOS Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/chao22_interspeech.html": {
    "title": "Perceptual Contrast Stretching on Target Feature for Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/geng22_interspeech.html": {
    "title": "A speech enhancement method for long-range speech acquisition task",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lu22c_interspeech.html": {
    "title": "ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zezario22_interspeech.html": {
    "title": "MTI-Net: A Multi-Target Speech Intelligibility Prediction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/bu22_interspeech.html": {
    "title": "Steering vector correction in MVDR beamformer for speech enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/saba22_interspeech.html": {
    "title": "Speech Modification for Intelligibility in Cochlear Implant Listeners: Individual Effects of Vowel- and Consonant-Boosting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/jigang22_interspeech.html": {
    "title": "DCTCN:Deep Complex Temporal Convolutional Network for Long Time Speech Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhao22l_interspeech.html": {
    "title": "Improve Speech Enhancement using Perception-High-Related Time-Frequency Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/fernandez22_interspeech.html": {
    "title": "Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22i_interspeech.html": {
    "title": "Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22h_interspeech.html": {
    "title": "Cross-speaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/wu22e_interspeech.html": {
    "title": "Self-supervised Context-aware Style Representation for Expressive Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/liu22m_interspeech.html": {
    "title": "Integrating Discrete Word-Level Style Variations into Non-Autoregressive Acoustic Models for Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/dai22_interspeech.html": {
    "title": "Automatic Prosody Annotation with Pre-Trained Text-Speech Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/zhou22e_interspeech.html": {
    "title": "Enhancing Word-Level Semantic Representation via Dependency Structure for Expressive Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/lei22c_interspeech.html": {
    "title": "Towards Multi-Scale Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/li22ca_interspeech.html": {
    "title": "Towards Cross-speaker Reading Style Transfer on Audiobook Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/meng22c_interspeech.html": {
    "title": "CALM: Constrastive Cross-modal Speaking Style Modeling for Expressive Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/he22d_interspeech.html": {
    "title": "Improve emotional speech synthesis quality by learning explicit and implicit representations with semi-supervised training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  },
  "https://www.isca-speech.org/archive/interspeech_2022/nguyen22e_interspeech.html": {
    "title": "A Vietnamese-English Neural Machine Translation System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0
  }
}