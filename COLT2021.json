{
  "https://proceedings.mlr.press/v134/abbe21a.html": {
    "title": "Stochastic block model entropy and broadcasting on trees with survey",
    "abstract": "The limit of the entropy in the stochastic block model (SBM) has been characterized in the sparse regime for the special case of disassortative communities [Coja-Oghlan et al. (2017)] and for the classical case of assortative communities but in the dense regime [Deshpande et al. (2016)]. The problem has not been closed in the classical sparse and assortative case. This paper establishes the result in this case for any SNR besides for the interval (1, 3.513). It further gives an approximation to the limit in this window.  The result is obtained by expressing the global SBM entropy as an integral of local tree entropies in a broadcasting on tree model with erasure side-information. The main technical advancement then relies on showing the irrelevance of the boundary in such a model, also studied with variants in [Kanade et al. (2016)], [Mossel et al. (2016)] and [Mossel and Xu (2015)]. In particular, we establish the uniqueness of the BP fixed point in the survey model for any SNR above 3.513 or below 1. This only leaves a narrow region in the plane between SNR and survey strength where the uniqueness of BP conjectured in these papers remains unproved",
    "volume": "main",
    "checked": true,
    "id": "afc3b654daaf3ae7f9f4347ca1819c589b96e70a",
    "citation_count": 4
  },
  "https://proceedings.mlr.press/v134/agrawal21a.html": {
    "title": "Regret Minimization in Heavy-Tailed Bandits",
    "abstract": "We revisit the classic regret-minimization problem in the stochastic multi-armed bandit setting when the arm-distributions are allowed to be heavy-tailed. Regret minimization has been well studied in simpler settings of either bounded support reward distributions or distributions that belong to a single parameter exponential family. We work under the much weaker assumption that the moments of order \\((1+\\epsilon)\\){are} uniformly bounded by a known constant \\(B\\), for some given \\( \\epsilon > 0\\). We propose an optimal algorithm that matches the lower bound exactly in the first-order term. We also give a finite-time bound on its regret. We show that our index concentrates faster than the well-known truncated or trimmed empirical mean estimators for the mean of heavy-tailed distributions. Computing our index can be computationally demanding. To address this, we develop a batch-based algorithm that is optimal up to a multiplicative constant depending on the batch size. We hence provide a controlled trade-off between statistical optimality and computational cost",
    "volume": "main",
    "checked": true,
    "id": "5aa0a667c9e11a1d9251c5a1fb0d0c1991c64faa",
    "citation_count": 5
  },
  "https://proceedings.mlr.press/v134/amir21a.html": {
    "title": "SGD Generalizes Better Than GD (And Regularization Doesn't Help)",
    "abstract": "We give a new separation result between the generalization performance of stochastic gradient descent (SGD) and of full-batch gradient descent (GD) in the fundamental stochastic convex optimization model. While for SGD it is well-known that $O(1/\\epsilon^2)$ iterations suffice for obtaining a solution with $\\epsilon$ excess expected risk, we show that with the same number of steps GD may overfit and emit a solution with $\\Omega(1)$ generalization error. Moreover, we show that in fact $\\Omega(1/\\epsilon^4)$ iterations are necessary for GD to match the generalization performance of SGD, which is also tight due to recent work by Bassily et al. (2020). We further discuss how regularizing the empirical risk minimized by GD essentially does not change the above result, and revisit the concepts of stability, implicit bias and the role of the learning algorithm in generalization",
    "volume": "main",
    "checked": true,
    "id": "2970804b287af94641f20e35ad58e9c46bab28c1",
    "citation_count": 15
  },
  "https://proceedings.mlr.press/v134/anari21a.html": {
    "title": "The Bethe and Sinkhorn Permanents of Low Rank Matrices and Implications for Profile Maximum Likelihood",
    "abstract": "In this paper we consider the problem of computing the likelihood of the profile of a discrete distribution, i.e., the probability of observing the multiset of element frequencies, and computing a profile maximum likelihood (PML) distribution, i.e., a distribution with the maximum profile likelihood. For each problem we provide polynomial time algorithms that given $n$ i.i.d. samples from a discrete distribution, achieve an approximation factor of $\\exp\\left(-O(\\sqrt{n} \\log n) \\right)$, improving upon the previous best-known bound achievable in polynomial time of $\\exp(-O(n^{2/3} \\log n))$ (Charikar, Shiragur and Sidford, 2019). Through the work of Acharya, Das, Orlitsky and Suresh (2016), this implies a polynomial time universal estimator for symmetric properties of discrete distributions in a broader range of error parameter.  To obtain our results on PML we establish new connections between PML and the well-studied Bethe and Sinkhorn approximations to the permanent (Vontobel, 2012 and 2014). It is known that the PML objective is proportional to the permanent of a certain Vandermonde matrix (Vontobel, 2012) with $\\sqrt{n}$ distinct columns, i.e. with non-negative rank at most $\\sqrt{n}$. This allows us to show that the convex approximation to computing PML distributions studied in (Charikar, Shiragur and Sidford, 2019) is governed, in part, by the quality of Sinkhorn approximations to the permanent. We show that both Bethe and Sinkhorn permanents are $\\exp(O(k \\log(N/k)))$ approximations to the permanent of $N \\times N$ matrices with non-negative rank at most $k$. This improves upon the previous known bounds of $\\exp(O(N))$ and combining these insights with careful rounding of the convex relaxation yields our results",
    "volume": "main",
    "checked": true,
    "id": "79a4485fa24f4fb89058d1d556a9364c3a3d535a",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v134/andrade21a.html": {
    "title": "Learning in Matrix Games can be Arbitrarily Complex",
    "abstract": "Many multi-agent systems with strategic interactions have their desired functionality encoded as the Nash equilibrium of a game, e.g. machine learning architectures such as Generative Adversarial Networks. Directly computing a Nash equilibrium of these games is often impractical or impossible in practice, which has led to the development of numerous learning algorithms with the goal of iteratively converging on a Nash equilibrium. Unfortunately, the dynamics generated by the learning process can be very intricate and instances failing to converge become hard to interpret. In this paper we show that, in a strong sense, this dynamic complexity is inherent to games. Specifically, we prove that replicator dynamics, the continuous-time analogue of Multiplicative Weights Update, even when applied in a very restricted class of games–known as finite matrix games–is rich enough to be able to approximate arbitrary dynamical systems. In the context of machine learning, our results are positive in the sense that they show the nearly boundless dynamic modelling capabilities of current machine learning practices, but also negative in implying that these capabilities may come at the cost of interpretability. As a concrete example, we show how replicator dynamics can effectively reproduce the well-known strange attractor of Lonrenz dynamics (the “butterfly effect\") while achieving no regret",
    "volume": "main",
    "checked": true,
    "id": "fc7c1ba7c6f85bdb1adc6024dea673bed2f32ed8",
    "citation_count": 13
  },
  "https://proceedings.mlr.press/v134/ashlagi21a.html": {
    "title": "Functions with average smoothness: structure, algorithms, and learning",
    "abstract": "We initiate a program of average smoothness analysis for efficiently learning real-valued functions on metric spaces. Rather than using the Lipschitz constant as the regularizer, we define a local slope at each point and gauge the function complexity as the average of these values. Since the mean can be dramatically smaller than the maximum, this complexity measure can yield considerably sharper generalization bounds — assuming that these admit a refinement where the Lipschitz constant is replaced by our average of local slopes. In addition to the usual average, we also examine a “weak” average that is more forgiving and yields a much wider function class.  Our first major contribution is to obtain just such distribution-sensitive bounds. This required overcoming a number of technical challenges, perhaps the most formidable of which was bounding the {\\em empirical} covering numbers, which can be much worse-behaved than the ambient ones. Our combinatorial results are accompanied by efficient algorithms for smoothing the labels of the random sample, as well as guarantees that the extension from the sample to the whole space will continue to be, with high probability, smooth on average. Along the way we discover a surprisingly rich combinatorial and analytic structure in the function class we define",
    "volume": "main",
    "checked": true,
    "id": "72fdb23ce5a8062b23ad0cbc3a88925dca98f2bb",
    "citation_count": 5
  },
  "https://proceedings.mlr.press/v134/awasthi21a.html": {
    "title": "Adversarially Robust Low Dimensional Representations",
    "abstract": "Many machine learning systems are vulnerable to small perturbations made to inputs either at test time or at training time. This has received much recent interest on the empirical front due to applications where reliability and security are critical. However, theoretical understanding of algorithms that are robust to adversarial perturbations is limited.  In this work we focus on Principal Component Analysis (PCA), a ubiquitous algorithmic primitive in machine learning.  We formulate a natural robust variant of PCA where the goal is to find a low dimensional subspace to represent the given data with minimum projection error, that is in addition robust to small perturbations measured in $\\ell_q$ norm (say $q=\\infty$). Unlike PCA which is solvable in polynomial time, our formulation is computationally intractable to optimize as it captures a variant of the well-studied sparse PCA objective as a special case. We show the following results: 1. Polynomial time algorithm that is constant factor competitive in the worst-case with respect to the best subspace, in terms of the projection error and the robustness criterion.  2. We show that our algorithmic techniques can also be made robust to adversarial training-time perturbations, in addition to yielding representations that are robust to adversarial perturbations at test time. Specifically, we design algorithms for a strong notion of training-time perturbations, where every point is adversarially perturbed up to a specified amount.  3. We illustrate the broad applicability of our algorithmic techniques in addressing robustness to adversarial perturbations, both at training time and test time. In particular, our adversarially robust PCA primitive leads to computationally efficient and robust algorithms for both unsupervised and supervised learning problems such as clustering and learning adversarially robust classifiers",
    "volume": "main",
    "checked": true,
    "id": "a0b61ee4e308f06b714255c2d5ea21df40fc9a6c",
    "citation_count": 6
  },
  "https://proceedings.mlr.press/v134/azizian21a.html": {
    "title": "The Last-Iterate Convergence Rate of Optimistic Mirror Descent in Stochastic Variational Inequalities",
    "abstract": "In this paper, we analyze the local convergence rate of optimistic mirror descent methods in stochastic variational inequalities, a class of optimization problems with important applications to learning theory and machine learning. Our analysis reveals an intricate relation between the algorithm’s rate of convergence and the local geometry induced by the method’s underlying Bregman function. We quantify this relation by means of the Legendre exponent, a notion that we introduce to measure the growth rate of the Bregman divergence relative to the ambient norm near a solution. We show that this exponent determines both the optimal step-size policy of the algorithm and the optimal rates attained, explaining in this way the differences observed for some popular Bregman functions (Euclidean projection, negative entropy, fractional power, etc.)",
    "volume": "main",
    "checked": true,
    "id": "ee60ff0508499991b94e7e2b261cb70ff835bff8",
    "citation_count": 14
  },
  "https://proceedings.mlr.press/v134/baby21a.html": {
    "title": "Optimal Dynamic Regret in Exp-Concave Online Learning",
    "abstract": "We consider the problem of the Zinkevich (2003)-style dynamic regret minimization in online learning with \\emph{exp-concave} losses. We show that whenever improper learning is allowed, a Strongly Adaptive online learner achieves the dynamic regret of $\\tilde O^*(n^{1/3}C_n^{2/3} \\vee 1)$ where $C_n$ is the \\emph{total variation} (a.k.a. \\emph{path length}) of the an arbitrary sequence of comparators that may not be known to the learner ahead of time. Achieving this rate was highly nontrivial even for square losses in 1D where the best known upper bound was $O(\\sqrt{nC_n} \\vee \\log n)$ (Yuan and Lamperski, 2019). Our new proof techniques make elegant use of the intricate structures of the primal and dual variables imposed by the KKT conditions and could be of independent interest. Finally, we apply our results to the classical statistical problem of \\emph{locally adaptive non-parametric regression} (Mammen, 1991; Donoho and Johnstone, 1998) and obtain a stronger and more flexible algorithm that do not require any statistical assumptions or any hyperparameter tuning",
    "volume": "main",
    "checked": true,
    "id": "97b656520e6351c83128cb00337f8aedc97324dc",
    "citation_count": 14
  },
  "https://proceedings.mlr.press/v134/bandeira21a.html": {
    "title": "Spectral Planting and the Hardness of Refuting Cuts, Colorability, and Communities in Random Graphs",
    "abstract": "We study the problem of efficiently refuting the k-colorability of a graph, or equivalently, certifying a lower bound on its chromatic number. We give formal evidence of average-case computational hardness for this problem in sparse random regular graphs, suggesting that there is no polynomial-time algorithm that improves upon a classical spectral algorithm. Our evidence takes the form of a \"computationally-quiet planting\": we construct a distribution of d-regular graphs that has significantly smaller chromatic number than a typical regular graph drawn uniformly at random, while providing evidence that these two distributions are indistinguishable by a large class of algorithms. We generalize our results to the more general problem of certifying an upper bound on the maximum k-cut.  This quiet planting is achieved by minimizing the effect of the planted structure (e.g. colorings or cuts) on the graph spectrum. Specifically, the planted structure corresponds exactly to eigenvectors of the adjacency matrix. This avoids the pushout effect of random matrix theory, and delays the point at which the planting becomes visible in the spectrum or local statistics. To illustrate this further, we give similar results for a Gaussian analogue of this problem: a quiet version of the spiked model, where we plant an eigenspace rather than adding a generic low-rank perturbation.  Our evidence for computational hardness of distinguishing two distributions is based on three different heuristics: stability of belief propagation, the local statistics hierarchy, and the low-degree likelihood ratio. Of independent interest, our results include general-purpose bounds on the low-degree likelihood ratio for multi-spiked matrix models, and an improved low-degree analysis of the stochastic block model",
    "volume": "main",
    "checked": true,
    "id": "0b9be13a21879574dbcbe87953d4c53607a153c5",
    "citation_count": 16
  },
  "https://proceedings.mlr.press/v134/bassily21a.html": {
    "title": "Non-Euclidean Differentially Private Stochastic Convex Optimization",
    "abstract": "Differentially private (DP) stochastic convex optimization (SCO) is a fundamental problem, where the goal is to approximately minimize the population risk with respect to a convex loss function, given a dataset of i.i.d. samples from a distribution, while satisfying differential privacy with respect to the dataset. Most of the existing works in the literature of private convex optimization focus on the Euclidean (i.e., $\\ell_2$) setting, where the loss is assumed to be Lipschitz (and possibly smooth) w.r.t. the $\\ell_2$ norm over a constraint set with bounded $\\ell_2$ diameter. Algorithms based on noisy stochastic gradient descent (SGD) are known to attain the optimal excess risk in this setting. In this work, we conduct a systematic study of DP-SCO for $\\ell_p$-setups. For $p=1$, under a standard smoothness assumption, we give a new algorithm with nearly optimal excess risk. This result also extends to general polyhedral norms and feasible sets. For $p\\in(1, 2)$, we give two new algorithms, whose central building block is a novel privacy mechanism, which generalizes the Gaussian mechanism. Moreover, we establish a lower bound on the excess risk for this range of $p$, showing a necessary dependence on $\\sqrt{d}$, where $d$ is the dimension of the space. Our lower bound implies a sudden transition of the excess risk at $p=1$, where the dependence on $d$ changes from logarithmic to polynomial, resolving an open question in prior work \\citep{TTZ15a}. For $p\\in (2, \\infty)$, noisy SGD attains optimal excess risk in the low-dimensional regime; in particular, this proves the optimality of noisy SGD for $p=\\infty$. Our work draws upon concepts from the geometry of normed spaces, such as the notions of regularity, uniform convexity, and uniform smoothness",
    "volume": "main",
    "checked": true,
    "id": "89f51453e29ef032e18359f489faf734c2045362",
    "citation_count": 27
  },
  "https://proceedings.mlr.press/v134/bennett21a.html": {
    "title": "Reconstructing weighted voting schemes from partial information about their power indices",
    "abstract": "A number of recent works [Goldberg 2006; O’Donnell and Servedio 2011; De, Diakonikolas, and Servedio 2017; De, Diakonikolas, Feldman, and Servedio 2014] have considered the problem of approximately reconstructing an unknown weighted voting scheme given information about various sorts of “power indices” that characterize the level of control that individual voters have over the final outcome. In the language of theoretical computer science, this is the problem of approximating an unknown linear threshold function (LTF) over ${-1,1}^n$ given some numerical measure (such as the function’s n “Chow parameters,” a.k.a. its degree-1 Fourier coefficients, or the vector of its n Shapley indices) of how much each of the n individual input variables affects the outcome of the function. In this paper we consider the problem of reconstructing an LTF given only partial information about its Chow parameters or Shapley indices; i.e. we are given only the Chow parameters or the Shapley indices corresponding to a subset $S\\subseteq [n]$ of the n input variables. A natural goal in this partial information setting is to find an LTF whose Chow parameters or Shapley indices corresponding to indices in S accurately match the given Chow parameters or Shapley indices of the unknown LTF. We refer to this as the Partial Inverse Power Index Problem. Our main results are a polynomial time algorithm for the ($\\epsilon$-approximate) Chow Parameters Partial Inverse Power Index Problem and a quasi-polynomial time algorithm for the ($\\epsilon$-approximate) Shapley Indices Partial Inverse Power Index Problem",
    "volume": "main",
    "checked": true,
    "id": "48821ef37bc3225d0e55b97f1c12a1282bebe6da",
    "citation_count": 0
  },
  "https://proceedings.mlr.press/v134/berg21a.html": {
    "title": "Deterministic Finite-Memory Bias Estimation",
    "abstract": "In this paper we consider the problem of estimating a Bernoulli parameter using finite memory. Let $X_1,X_2,\\ldots$ be a sequence of independent identically distributed Bernoulli random variables with expectation $\\theta$, where $\\theta \\in [0,1]$. Consider a finite-memory deterministic machine with $S$ states, that updates its state $M_n \\in \\{1,2,\\ldots,S\\}$ at each time according to the rule $M_n = f(M_{n-1},X_n)$, where $f$ is a deterministic time-invariant function. Assume that the machine outputs an estimate at each time point according to some fixed mapping from the state space to the unit interval. The quality of the estimation procedure is measured by the asymptotic risk, which is the long-term average of the instantaneous quadratic risk. The main contribution of this paper is an upper bound on the smallest worst-case asymptotic risk any such machine can attain. This bound coincides with a lower bound derived by Leighton and Rivest, to imply that $\\Theta(1/S)$ is the minimax asymptotic risk for deterministic $S$-state machines. In particular, our result disproves a longstanding $\\Theta(\\log S/S)$ conjecture for this quantity, also posed by Leighton and Rivest",
    "volume": "main",
    "checked": true,
    "id": "89c3a032429097c89c5d909c30b11351682a1ce4",
    "citation_count": 1
  },
  "https://proceedings.mlr.press/v134/besbes21a.html": {
    "title": "Online Learning from Optimal Actions",
    "abstract": "We study the problem of online contextual optimization where, at each period, instead of observing the loss, we observe, after-the-fact, the optimal action an oracle with full knowledge of the objective function would have taken. At each period, the decision-maker has access to a new set of feasible actions to select from and to a new contextual function that affects that period’s loss function. We aim to minimize regret, which is defined as the difference between our losses and the ones incurred by an all-knowing oracle. We obtain the first regret bound for this problem that is logarithmic in the time horizon. Our results are derived through the development and analysis of a novel algorithmic structure that leverages the underlying geometry of the problem",
    "volume": "main",
    "checked": true,
    "id": "1158814268e74119f03830870e8a8ad41ee23d57",
    "citation_count": 0
  },
  "https://proceedings.mlr.press/v134/block21a.html": {
    "title": "Majorizing Measures, Sequential Complexities, and Online Learning",
    "abstract": "We introduce the technique of generic chaining and majorizing measures for controlling sequential Rademacher complexity. We relate majorizing measures to the notion of fractional covering numbers, which we show to be dominated in terms of sequential scale-sensitive dimensions in a horizon-independent way, and, under additional complexity assumptions establish a tight control on worst-case sequential Rademacher complexity in terms of the integral of sequential scale-sensitive dimension. Finally, we establish a tight contraction inequality for worst-case sequential Rademacher complexity.  The above constitutes the resolution of a number of outstanding open problems in extending the classical theory of empirical processes to the sequential case, and, in turn, establishes sharp results for online learning",
    "volume": "main",
    "checked": true,
    "id": "9b97ab6a9f5405d9ff5eb206b8400b3512cccf6a",
    "citation_count": 8
  },
  "https://proceedings.mlr.press/v134/blum21a.html": {
    "title": "Robust learning under clean-label attack",
    "abstract": "We study the problem of robust learning under clean-label data-poisoning attacks, where the attacker injects (an arbitrary set of) \\emph{correctly-labeled} examples to the training set to fool the algorithm into making mistakes on \\emph{specific} test instances at test time. The learning goal is to minimize the attackable rate (the probability mass of attackable test instances), which is more difficult than optimal PAC learning. As we show, any robust algorithm with diminishing attackable rate can achieve the optimal dependence on $\\epsilon$ in its PAC sample complexity, i.e., $O(1/\\epsilon)$. On the other hand, the attackable rate might be large even for some optimal PAC learners, e.g., SVM for linear classifiers. Furthermore, we show that the class of linear hypotheses is not robustly learnable when the data distribution has zero margin and is robustly learnable in the case of positive margin but requires sample complexity exponential in the dimension. For a general hypothesis class with bounded VC dimension, if the attacker is limited to add at most $t=O(1/\\epsilon)$ poison examples, the optimal robust learning sample complexity grows linearly with $t$",
    "volume": "main",
    "checked": true,
    "id": "482e6351510340469db2c021865c7ae4fcafc68c",
    "citation_count": 6
  },
  "https://proceedings.mlr.press/v134/bodin21a.html": {
    "title": "Rank-one matrix estimation: analytic time evolution of gradient descent dynamics",
    "abstract": "We consider a rank-one symmetric matrix corrupted by additive noise. The rank-one matrix is formed by an n-component unknown vector on the sphere of radius $\\sqrt{n}$, and we consider the problem of estimating this vector from the corrupted matrix in the high dimensional limit of $n$ large, by gradient descent for a quadratic cost function on the sphere. Explicit formulas for the whole time evolution of the overlap between the estimator and unknown vector, as well as the cost, are rigorously derived. In the long time limit we recover the well known spectral phase transition, as a function of the signal-to-noise ratio. The explicit formulas also allow to point out interesting transient features of the time evolution. Our analysis technique is based on recent progress in random matrix theory and uses local versions of the semi-circle law",
    "volume": "main",
    "checked": true,
    "id": "44e2e37938fae9ae2e89f3bd09996c55a12b4755",
    "citation_count": 3
  },
  "https://proceedings.mlr.press/v134/branzei21a.html": {
    "title": "Multiplayer Bandit Learning, from Competition to Cooperation",
    "abstract": "The stochastic multi-armed bandit model captures the tradeoff between exploration and exploitation. We study the effects of competition and cooperation on this tradeoff. Suppose there are two arms, one predictable and one risky, and two players, Alice and Bob. In every round, each player pulls an arm, receives the resulting reward, and observes the choice of the other player but not their reward. Alice’s utility is $\\Gamma_A + \\lambda \\Gamma_B$ (and similarly for Bob), where $\\Gamma_A$ is Alice’s total reward and $\\lambda \\in [-1, 1]$ is a cooperation parameter. At $\\lambda = -1$ the players are competing in a zero-sum game, at $\\lambda = 1$, their interests are aligned, and at $\\lambda = 0$, they are neutral: each player’s utility is their own reward. The model is related to the economics literature on strategic experimentation, where usually players observe each other’s rewards.  Suppose the predictable arm has success probability $p$ and the risky arm has prior $\\mu$. If the discount factor is $\\beta$, then the value of $p$ where a single player is indifferent between the arms is the Gittins index $g = g(\\mu,\\beta) > m$, where $m$ is the mean of the risky arm.  Our first result answers, in this setting, a fundamental question posed by Rothschild \\cite{rotschild}. We show that competing and neutral players eventually settle on the same arm (even though it may not be the best arm) in every Nash equilibrium, while this can fail for players with aligned interests.  Moreover, we show that \\emph{competing players} explore \\emph{less} than a single player: there is $p^* \\in (m, g)$ so that for all $p > p^*$, the players stay at the predictable arm. However, the players are not myopic: they still explore for some $p > m$. On the other hand, \\emph{cooperating players} (with $\\lambda =1$) explore \\emph{more} than a single player. We also show that \\emph{neutral players} learn from each other, receiving strictly higher total rewards than they would playing alone, for all $ p\\in (p^*, g)$, where $p^*$ is the threshold above which competing players do not explore",
    "volume": "main",
    "checked": true,
    "id": "bd8045195f30c317b172004aca284248088cfad5",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v134/braverman21a.html": {
    "title": "Near Optimal Distributed Learning of Halfspaces with Two Parties",
    "abstract": "Distributed learning protocols are designed to train on distributed data without gathering it all on a single centralized machine, thus contributing to the efficiency of the system and enhancing its privacy. We study a central problem in distributed learning, called {\\it distributed learning of halfspaces}: let $U \\subseteq \\mathbb{R}^d$ be a known domain of size $n$ and let $h:\\mathbb{R}^d\\to \\mathbb{R}$ be an unknown target affine function.\\footnote{In practice, the domain $U$ is defined implicitly by the representation of $d$-dimensional vectors which is used in the protocol.} A set of examples $\\{(u,b)\\}$ is distributed between several parties, where~$u \\in U$ is a point and $b = \\mathsf{sign}(h(u)) \\in \\{\\pm 1\\}$ is its label. The parties goal is to agree on a classifier~$f: U\\to\\{\\pm 1\\}$ such that~$f(u)=b$ for every input example~$(u,b)$.\nWe design a protocol for the distributed halfspace learning problem in the two-party setting, communicating only $\\tilde O(d\\log n)$ bits. To this end, we introduce a new tool called halfspace containers, that is closely related to bracketing numbers in statistics and to hyperplane cuttings in discrete geometry, and allows for a compressed approximate representation of every halfspace. We complement our upper bound result by an almost matching $\\tilde \\Omega(d\\log n)$ lower bound on the communication complexity of any such protocol\nSince the distributed halfspace learning problem is closely related to the convex set disjointness problem in communication complexity and the problem of distributed linear programming in distributed optimization, we also derive upper and lower bounds of $\\tilde O(d^2\\log n)$ and~$\\tilde{\\Omega}(d\\log n)$ on the communication complexity of both of these basic problems",
    "volume": "main",
    "checked": true,
    "id": "7a7858d3cffeb76fb335516cf20c677e2e098384",
    "citation_count": 2
  },
  "https://proceedings.mlr.press/v134/braverman21b.html": {
    "title": "Near-Optimal Entrywise Sampling of Numerically Sparse Matrices",
    "abstract": "Many real-world data sets are sparse or almost sparse. One method to measure this for a matrix $A\\in \\mathbb{R}^{n\\times n}$ is the \\emph{numerical sparsity}, denoted $\\mathsf{ns}(A)$, defined as the minimum $k\\geq 1$ such that $\\|a\\|_1/\\|a\\|_2 \\leq \\sqrt{k}$ for every row and every column $a$ of $A$. This measure of $a$ is smooth and is clearly only smaller than the number of non-zeros in the row/column $a$.  The seminal work of Achlioptas and McSherry (2007) has put forward the question of approximating an input matrix $A$ by entrywise sampling. More precisely, the goal is to quickly compute a sparse matrix $\\tilde{A}$ satisfying $\\|A - \\tilde{A}\\|_2 \\leq \\epsilon \\|A\\|_2$ (i.e., additive spectral approximation) given an error parameter $\\epsilon>0$. The known schemes sample and rescale a small fraction of entries from $A$.  We propose a scheme that sparsifies an almost-sparse matrix $A$ — it produces a matrix $\\tilde{A}$ with $O(\\epsilon^{-2}\\mathsf{ns}(A) \\cdot n\\ln n)$ non-zero entries with high probability. We also prove that this upper bound on $\\mathsf{nnz}(\\tilde{A})$ is \\emph{tight} up to logarithmic factors. Moreover, our upper bound improves when the spectrum of $A$ decays quickly (roughly replacing $n$ with the stable rank of $A$). Our scheme can be implemented in time $O(\\mathsf{nnz}(A))$ when $\\|A\\|_2$ is given. Previously, a similar upper bound was obtained by Achlioptas et al. (2013), but only for a restricted class of inputs that does not even include symmetric or covariance matrices. Finally, we demonstrate two applications of these sampling techniques, to faster approximate matrix multiplication, and to ridge regression by using sparse preconditioners",
    "volume": "main",
    "checked": true,
    "id": "8a633c9e8894924b2b87411fe45d09e3853d4d19",
    "citation_count": 3
  },
  "https://proceedings.mlr.press/v134/brennan21a.html": {
    "title": "Statistical Query Algorithms and Low Degree Tests Are Almost Equivalent",
    "abstract": "Researchers currently use a number of approaches to predict and substantiate information-computation gaps in high-dimensional statistical estimation problems. A prominent approach is to characterize the limits of restricted models of computation, which on the one hand yields strong computational lower bounds for powerful classes of algorithms and on the other hand helps guide the development of efficient algorithms. In this paper, we study two of the most popular restricted computational models, the statistical query framework and low-degree polynomials, in the context of high-dimensional hypothesis testing. Our main result is that under mild conditions on the testing problem, the two classes of algorithms are essentially equivalent in power. As corollaries, we obtain new statistical query lower bounds for sparse PCA, tensor PCA and several variants of the planted clique problem",
    "volume": "main",
    "checked": false,
    "id": "486d580110a6780012b25c9783299e31f173ecf1",
    "citation_count": 21
  },
  "https://proceedings.mlr.press/v134/bressan21a.html": {
    "title": "Exact Recovery of Clusters in Finite Metric Spaces Using Oracle Queries",
    "abstract": "We investigate the problem of exact cluster recovery using oracle queries.  Previous results show that clusters in Euclidean spaces that are convex and separated with a margin can be reconstructed exactly using only $O(\\log n)$ same-cluster queries, where $n$ is the number of input points.  In this work, we study this problem in the more challenging non-convex setting. We introduce a structural characterization of clusters, called $(\\beta,\\gamma)$-convexity, that can be applied to any finite set of points equipped with a metric (or even a semimetric, as the triangle inequality is not needed).  Using $(\\beta,\\gamma)$-convexity, we can translate natural density properties of clusters (which include, for instance, clusters that are strongly non-convex in $R^d$) into a graph-theoretic notion of convexity.  By exploiting this convexity notion, we design a deterministic algorithm that recovers $(\\beta,\\gamma)$-convex clusters using $O(k^2 \\log n + k^2 (\\frac{6}{\\beta\\gamma})^{dens(X)})$ same-cluster queries, where $k$ is the number of clusters and $dens(X)$ is the density dimension of the semimetric.  We show that an exponential dependence on the density dimension is necessary, and we also show that, if we are allowed to make $O(k^2 + k \\log n)$ additional queries to a \"cluster separation\" oracle, then we can recover clusters that have different and arbitrary scales, even when the scale of each cluster is unknown",
    "volume": "main",
    "checked": true,
    "id": "2a78862ca731737a082221028402bddf32a0d299",
    "citation_count": 9
  },
  "https://proceedings.mlr.press/v134/bubeck21a.html": {
    "title": "A Law of Robustness for Two-Layers Neural Networks",
    "abstract": "We initiate the study of the inherent tradeoffs between the size of a neural network and its robustness, as measured by its Lipschitz constant. We make a precise conjecture that, for any Lipschitz activation function and for most datasets, any two-layers neural network with $k$ neurons that perfectly fit the data must have its Lipschitz constant larger (up to a constant) than $\\sqrt{n/k}$ where $n$ is the number of datapoints. In particular, this conjecture implies that overparametrization is necessary for robustness, since it means that one needs roughly one neuron per datapoint to ensure a $O(1)$-Lipschitz network, while mere data fitting of $d$-dimensional data requires only one neuron per $d$ datapoints. We prove a weaker version of this conjecture when the Lipschitz constant is replaced by an upper bound on it based on the spectral norm of the weight matrix. We also prove the conjecture in the high-dimensional regime $n \\approx d$ (which we also refer to as the undercomplete case, since only $k \\leq d$ is relevant here). Finally we prove the conjecture for polynomial activation functions of degree $p$ when $n \\approx d^p$. We complement these findings with experimental evidence supporting the conjecture",
    "volume": "main",
    "checked": true,
    "id": "8d8923a2593934610b90d75cdaba8d0f9b5f7f60",
    "citation_count": 32
  },
  "https://proceedings.mlr.press/v134/bubeck21b.html": {
    "title": "Cooperative and Stochastic Multi-Player Multi-Armed Bandit: Optimal Regret With Neither Communication Nor Collisions",
    "abstract": "We consider the cooperative multi-player version of the stochastic multi-armed bandit problem. We study the regime where the players cannot communicate but have access to shared randomness. In prior work by the first two authors, a strategy for this regime was constructed for two players and three arms, with regret $\\tilde{O}(\\sqrt{T})$, and with no collisions at all between the players (with very high probability). In this paper we show that these properties (near-optimal regret and no collisions at all) are achievable for any number of players and arms. At a high level, the previous strategy heavily relied on a 2-dimensional geometric intuition that was difficult to generalize in higher dimensions, while here we take a more combinatorial route to build the new strategy",
    "volume": "main",
    "checked": true,
    "id": "6fbd9df16d6b94f3273b778c8830ab12557645d5",
    "citation_count": 13
  },
  "https://proceedings.mlr.press/v134/cabannes21a.html": {
    "title": "Fast Rates for Structured Prediction",
    "abstract": "Discrete supervised learning problems such as classification are often tackled by introducing a continuous surrogate problem akin to regression. Bounding the original error, between estimate and solution, by the surrogate error endows discrete problems with convergence rates already shown for continuous instances. Yet, current approaches do not leverage the fact that discrete problems are essentially predicting a discrete output when continuous problems are predicting a continuous value. In this paper, we tackle this issue for general structured prediction problems, opening the way to “super fast” rates, that is, convergence rates for the excess risk faster than $n^{-1}$, where $n$ is the number of observations, with even exponential rates with the strongest assumptions. We first illustrate it for predictors based on nearest neighbors, generalizing rates known for binary classification to any discrete problem within the framework of structured prediction. We then consider kernel ridge regression where we improve known rates in $n^{-1/4}$ to arbitrarily fast rates, depending on a parameter characterizing the hardness of the problem, thus allowing, under smoothness assumptions, to bypass the curse of dimensionality",
    "volume": "main",
    "checked": false,
    "id": "1f76daa1df02c087cde55ee1ef3b4757e65dbbda",
    "citation_count": 10
  },
  "https://proceedings.mlr.press/v134/carmon21a.html": {
    "title": "Thinking Inside the Ball: Near-Optimal Minimization of the Maximal Loss",
    "abstract": "We characterize the complexity of minimizing $\\max_{i\\in[N]} f_i(x)$ for convex, Lipschitz functions $f_1,\\ldots, f_N$. For non-smooth functions, existing methods require $O(N\\epsilon^{-2})$ queries to a first-order oracle to compute an $\\epsilon$-suboptimal point and $\\widetilde{O}(N\\epsilon^{-1})$ queries if the $f_i$ are $O(1/\\epsilon)$-smooth.  We develop methods with improved complexity bounds of $\\widetilde{O}(N\\epsilon^{-2/3} + \\epsilon^{-8/3})$ in the non-smooth case and $\\widetilde{O}(N\\epsilon^{-2/3} + \\sqrt{N}\\epsilon^{-1})$ in the $O(1/\\epsilon)$-smooth case.  Our methods consist of a recently proposed ball optimization oracle acceleration algorithm (which we refine) and a careful implementation of said oracle for the softmax function.  We also prove an oracle complexity lower bound scaling as $\\Omega(N\\epsilon^{-2/3})$, showing that our dependence on $N$ is optimal up to polylogarithmic factors",
    "volume": "main",
    "checked": true,
    "id": "e73b0eff15d6f0363cf2d79d8aae8b07d6830648",
    "citation_count": 9
  },
  "https://proceedings.mlr.press/v134/casgrain21a.html": {
    "title": "Optimizing Optimizers: Regret-optimal gradient descent algorithms",
    "abstract": "This paper treats the task of designing optimization algorithms as an optimal control problem. Using regret as a metric for an algorithm’s performance, we study the existence, uniqueness and consistency of regret-optimal algorithms. By providing first-order optimality conditions for the control problem, we show that regret-optimal algorithms must satisfy a specific structure in their dynamics which we show is equivalent to performing \\emph{dual-preconditioned gradient descent} on the value function generated by its regret. Using these optimal dynamics, we provide bounds on their rates of convergence to solutions of convex optimization problems. Though closed-form optimal dynamics cannot be obtained in general, we present fast numerical methods for approximating them, generating optimization algorithms which directly optimize their long-term regret. These are benchmarked against commonly used optimization algorithms to demonstrate their effectiveness",
    "volume": "main",
    "checked": true,
    "id": "002cde85488b9206c7095b402dc245d7a10bb09b",
    "citation_count": 1
  },
  "https://proceedings.mlr.press/v134/chatterji21a.html": {
    "title": "When does gradient descent with logistic loss interpolate using deep networks with smoothed ReLU activations?",
    "abstract": "We establish conditions under which gradient descent applied to fixed-width deep networks drives the logistic loss to zero, and prove bounds on the rate of convergence. Our analysis applies for smoothed approximations to the ReLU, such as Swish and the Huberized ReLU, proposed in previous applied work. We provide two sufficient conditions for convergence.  The first is simply a bound on the loss at initialization.  The second is a data separation condition used in prior analyses",
    "volume": "main",
    "checked": true,
    "id": "ae3a77e96c415f9910f124b5cbbf3c6d3e5a7c79",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v134/chen21a.html": {
    "title": "Breaking The Dimension Dependence in Sparse Distribution Estimation under Communication Constraints",
    "abstract": "We consider the problem of estimating a $d$-dimensional $s$-sparse discrete distribution from its samples observed under a $b$-bit communication constraint. The best-known previous result on $\\ell_2$ estimation error for this problem is $O\\left( \\frac{s\\log\\left( {d}/{s}\\right)}{n2^b}\\right)$. Surprisingly, we show that when sample size $n$ exceeds a minimum threshold $n^*(s, d, b)$, we can achieve an $\\ell_2$ estimation error of $O\\left( \\frac{s}{n2^b}\\right)$. This implies that when $n>n^*(s, d, b)$ the convergence rate does not depend on the ambient dimension $d$ and is the same as knowing the support of the distribution beforehand.\nWe next ask the question: ``what is the minimum $n^*(s, d, b)$ that allows dimension-free convergence?'.  To upper bound $n^*(s, d, b)$, we develop novel localization schemes to accurately and efficiently localize the unknown support. For the non-interactive setting, we show that $n^*(s, d, b) = O\\left( \\min \\left( {d^2\\log^2 d}/{2^b}, {s^4\\log^2 d}/{2^b}\\right) \\right)$. Moreover,  we connect the problem with non-adaptive group testing and obtain a polynomial-time estimation scheme when $n = \\tilde{\\Omega}\\left({s^4\\log^4 d}/{2^b}\\right)$. This group testing based scheme is adaptive to the sparsity parameter $s$, and hence can be applied without knowing it. For the interactive setting, we propose a novel tree-based estimation scheme and show that the minimum sample-size needed to achieve dimension-free convergence can be further reduced to $n^*(s, d, b) = \\tilde{O}\\left( {s^2\\log^2 d}/{2^b} \\right)$",
    "volume": "main",
    "checked": true,
    "id": "7a5e06a01966888386d37154cd50eff89deb6c8c",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v134/chen21b.html": {
    "title": "Learning and testing junta distributions with sub cube conditioning",
    "abstract": "We study the problems of learning and testing junta distributions on $\\{-1,1\\}^n$ with respect to the uniform distribution, where a distribution $p$ is a $k$-junta if its probability mass function $p(x)$  depends on a subset of at most $k$ variables. The main contribution is an algorithm for finding relevant coordinates in a $k$-junta distribution with subcube conditioning (Bhattacharyya et al 2018., Canonne et al. 2019). We give two applications: An algorithm for learning $k$-junta distributions with $\\tilde{O}(k/\\epsilon^2) \\log n + O(2^k/\\epsilon^2)$ subcube conditioning queries, and  an algorithm for testing $k$-junta distributions with $\\tilde{O}((k + \\sqrt{n})/\\epsilon^2)$ subcube conditioning queries. All our algorithms are optimal up to poly-logarithmic factors.\nOur results show that subcube conditioning, as a natural model for accessing high-dimensional distributions, enables significant savings in learning and testing junta distributions compared to the standard sampling model. This addresses an open question posed by Aliakbarpour et al. 2016",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://proceedings.mlr.press/v134/chen21c.html": {
    "title": "Black-Box Control for Linear Dynamical Systems",
    "abstract": "We consider the problem of black-box control: the task of controlling an unknown linear time-invariant dynamical system from a single trajectory without a stabilizing controller. Under the assumption that the system is controllable, we give the first {\\it efficient} algorithm that is capable of attaining sublinear regret under the setting of online nonstochastic control. This resolves an open problem since the work of Abbasi-Yadkori and Szepesvari(2011) on the stochastic LQR problem, and in a more challenging setting that allows for adversarial perturbations and adversarially chosen changing convex loss functions. We give finite-time regret bounds for our algorithm on the order of $2^{poly(d)} + \\tilde{O}(poly(d) T^{2/3})$ for general nonstochastic control, and $2^{poly(d)} + \\tilde{O}(poly(d) \\sqrt{T})$ for black-box LQR. To complete the picture, we investigate the complexity of the online black-box control problem and give a matching regret lower bound of $2^{\\Omega(d)}$, showing that the exponential cost is inevitable. This lower bound holds even in the noiseless setting, and applies to any, randomized or deterministic, black-box control method",
    "volume": "main",
    "checked": true,
    "id": "51301b9c3bfd01c840579bf120ee92207bc3907a",
    "citation_count": 43
  },
  "https://proceedings.mlr.press/v134/chen21d.html": {
    "title": "Query complexity of least absolute deviation regression via robust uniform convergence",
    "abstract": "Consider a regression problem where the learner is given a large collection of $d$-dimensional data points, but can only query a small subset of the real-valued labels. How many queries are needed to obtain a $1+\\epsilon$ relative error approximation of the optimum? While this problem has been extensively studied for least squares regression, little is known for other losses. An important example is least absolute deviation regression ($\\ell_1$ regression) which enjoys superior robustness to outliers compared to least squares. We develop a new framework for analyzing importance sampling methods in regression problems, which enables us to show that the query complexity of least absolute deviation regression is $\\Theta(d/\\epsilon^2)$ up to logarithmic factors. We further extend our techniques to show the first bounds on the query complexity for any $\\ell_p$ loss with $p\\in(1,2)$. As a key novelty in our analysis, we introduce the notion of robust uniform convergence, which is a new approximation guarantee for the empirical loss. While it is inspired by uniform convergence in statistical learning, our approach additionally incorporates a correction term to avoid unnecessary variance due to outliers. This can be viewed as a new connection between statistical learning theory and variance reduction techniques in stochastic optimization, which should be of independent interest",
    "volume": "main",
    "checked": true,
    "id": "5a5d1feb07e25fb2f26fac558fc4ceb603e741dc",
    "citation_count": 8
  },
  "https://proceedings.mlr.press/v134/chen21e.html": {
    "title": "Minimax Regret for Stochastic Shortest Path with Adversarial Costs and Known Transition",
    "abstract": "We study the stochastic shortest path problem with adversarial costs and known transition, and show that the minimax regret is $O(\\sqrt{DT_\\star K})$ and $O(\\sqrt{DT_\\star SA K})$ for the full-information setting and the bandit feedback setting respectively, where $D$ is the diameter, $T_\\star$ is the expected hitting time of the optimal policy, $S$ is the number of states, $A$ is the number of actions, and $K$ is the number of episodes. Our results significantly improve upon the recent work of (Rosenberg and Mansour, 2020) which only considers the full-information setting and achieves suboptimal regret. Our work is also the first to consider bandit feedback with adversarial costs.  Our algorithms are built on top of the Online Mirror Descent framework with a variety of new techniques that might be of independent interest, including an improved multi-scale expert algorithm, a reduction from general stochastic shortest path to a special loop-free case, a skewed occupancy measure space, and a novel correction term added to the cost estimators. Interestingly, the last two elements reduce the variance of the learner via positive bias and the variance of the optimal policy via negative bias respectively, and having them simultaneously is critical for obtaining the optimal high-probability bound in the bandit feedback setting",
    "volume": "main",
    "checked": true,
    "id": "6f0b862645fcd324471f92ada8d575119ffe9470",
    "citation_count": 23
  },
  "https://proceedings.mlr.press/v134/chen21f.html": {
    "title": "Impossible Tuning Made Possible: A New Expert Algorithm and Its Applications",
    "abstract": "We resolve the long-standing \"impossible tuning\" issue for the classic expert problem and show that, it is in fact possible to achieve regret $O\\left(\\sqrt{(\\ln d)\\sum_t \\ell_{t,i}^2}\\right)$ simultaneously for all expert $i$ in a $T$-round $d$-expert problem where $\\ell_{t,i}$ is the loss for expert $i$ in round $t$. Our algorithm is based on the Mirror Descent framework with a correction term and a weighted entropy regularizer. While natural, the algorithm has not been studied before and requires a careful analysis. We also generalize the bound to $O\\left(\\sqrt{(\\ln d)\\sum_t (\\ell_{t,i}-m_{t,i})^2}\\right)$ for any prediction vector $m_t$ that the learner receives, and recover or improve many existing results by choosing different $m_t$. Furthermore, we use the same framework to create a master algorithm that combines a set of base algorithms and learns the best one with little overhead. The new guarantee of our master allows us to derive many new results for both the expert problem and more generally Online Linear Optimization",
    "volume": "main",
    "checked": true,
    "id": "7129a8977e6f731b048cef2f5c3e6298855cdaec",
    "citation_count": 19
  },
  "https://proceedings.mlr.press/v134/chewi21a.html": {
    "title": "Optimal dimension dependence of the Metropolis-Adjusted Langevin Algorithm",
    "abstract": "Conventional wisdom in the sampling literature, backed by a popular diffusion scaling limit, suggests that the mixing time of the Metropolis-Adjusted Langevin Algorithm (MALA) scales as O(d^{1/3}), where d is the dimension. However, the diffusion scaling limit requires stringent assumptions on the target distribution and is asymptotic in nature. In contrast, the best known non-asymptotic mixing time bound for MALA on the class of log-smooth and strongly log-concave distributions is O(d). In this work, we establish that the mixing time of MALA on this class of target distributions is \\tilde\\Theta(d^{1/2}) under a warm start.  Our upper bound proof introduces a new technique based on a projection characterization of the Metropolis adjustment which reduces the study of MALA to the well-studied discretization analysis of the Langevin SDE and bypasses direct computation of the acceptance probability",
    "volume": "main",
    "checked": true,
    "id": "9b660e286b29fdf020b61d0ac23a5141c2f815ec",
    "citation_count": 28
  },
  "https://proceedings.mlr.press/v134/cohen21a.html": {
    "title": "Online Markov Decision Processes with Aggregate Bandit Feedback",
    "abstract": "We study a novel variant of online finite-horizon Markov Decision Processes with adversarially changing loss functions and initially unknown dynamics.  In each episode, the learner suffers the loss accumulated along the trajectory realized by the policy chosen for the episode, and observes aggregate bandit feedback: the trajectory is revealed along with the cumulative loss suffered, rather than the individual losses encountered along the trajectory.  Our main result is a computationally efficient algorithm with $O(\\sqrt{K})$ regret for this setting, where K is the number of episodes.  We establish this result via an efficient reduction to a novel bandit learning setting we call Distorted Linear Bandits (DLB), which is a variant of bandit linear optimization where actions chosen by the learner are adversarially distorted before they are committed.  We then develop a computationally-efficient online algorithm for DLB for which we prove an $O(\\sqrt{T})$ regret bound, where T is the number of time steps.  Our algorithm is based on online mirror descent with a self-concordant barrier regularization that employs a novel increasing learning rate schedule",
    "volume": "main",
    "checked": true,
    "id": "b772aa052b7c24877b85a95d9b0b4387ad52cb81",
    "citation_count": 4
  },
  "https://proceedings.mlr.press/v134/cosson21a.html": {
    "title": "Quantifying Variational Approximation for Log-Partition Function",
    "abstract": "Variational methods, such as mean-field (MF) and tree-reweighted (TRW), provide computationally efficient approximations of the log-partition function for generic graphical models but their approximation ratio is generally not quantified. As the primary contribution of this work, we provide an approach to quantify their approximation ratio for any discrete pairwise graphical model with non-negative potentials through a property of the underlying graph structure $G$. Specifically, we argue that (a variant of) TRW produces an estimate within factor $1/\\sqrt{\\kappa(G)}$ where $\\kappa(G) \\in (0,1]$ captures how far $G$ is from tree structure. As a consequence, the approximation ratio is $1$ for trees, $\\sqrt{(d+1)/2}$ for graphs with maximum average degree $d$ and $1+1/(2\\beta)+o_{\\beta\\to \\infty}(1/\\beta)$ for graphs with girth at least $\\beta \\log N$. The quantity $\\kappa(G)$ is the solution of a min-max problem associated with the spanning tree polytope of $G$ that can be evaluated in polynomial time for any graph. We provide a near linear-time variant that achieves an approximation ratio depending on the minimal (across edges) effective resistance of the graph. We connect our results to the graph partition approximation method and thus provide a unified perspective",
    "volume": "main",
    "checked": true,
    "id": "4559c1048e0bb99017791e279043eaf5656cbadb",
    "citation_count": 0
  },
  "https://proceedings.mlr.press/v134/daniely21a.html": {
    "title": "From Local Pseudorandom Generators to Hardness of Learning",
    "abstract": "We prove hardness-of-learning results under a well-studied assumption on the existence of local pseudorandom generators. As we show, this assumption allows us to surpass the current state of the art, and prove hardness of various basic problems, with no hardness results to date.  Our results include: hardness of learning shallow ReLU neural networks under the Gaussian distribution and other distributions; hardness of learning intersections of $\\omega(1)$ halfspaces, DNF formulas with $\\omega(1)$ terms, and ReLU networks with $\\omega(1)$ hidden neurons; hardness of weakly learning deterministic finite automata under the uniform distribution; hardness of weakly learning depth-$3$ Boolean circuits under the uniform distribution, as well as distribution-specific hardness results for learning DNF formulas and intersections of halfspaces. We also establish lower bounds on the complexity of learning intersections of a constant number of halfspaces, and ReLU networks with a constant number of hidden neurons. Moreover, our results imply the hardness of virtually all improper PAC-learning problems (both distribution-free and distribution-specific) that were previously shown hard under other assumptions",
    "volume": "main",
    "checked": true,
    "id": "43420dcdaf30fe4f4d1af520f4da3e62d7899fef",
    "citation_count": 12
  },
  "https://proceedings.mlr.press/v134/daskalakis21a.html": {
    "title": "A Statistical Taylor Theorem and Extrapolation of Truncated Densities",
    "abstract": "We show a statistical version of Taylor’s theorem and apply this result to non-parametric density estimation from truncated samples, which is a classical challenge in Statistics [Woodroofe 1985, Stute 1993]. The single-dimensional version of our theorem has the following implication: \"For any distribution P on [0, 1] with a smooth log-density function, given samples from the conditional distribution of P on [a, a + \\varepsilon] \\subset [0, 1], we can efficiently identify an approximation to P over the whole interval [0, 1], with quality of approximation that improves with the smoothness of P\".  To the best of knowledge, our result is the first in the area of non-parametric density estimation from truncated samples, which works under the hard truncation model, where the samples outside some survival set S are never observed, and applies to multiple dimensions. In contrast, previous works assume single dimensional data where each sample has a different survival set $S$ so that samples from the whole support will ultimately be collected",
    "volume": "main",
    "checked": true,
    "id": "dff0cb7094116360d2781a745e51af8c573ef721",
    "citation_count": 2
  },
  "https://proceedings.mlr.press/v134/de21a.html": {
    "title": "Weak learning convex sets under normal distributions",
    "abstract": "This paper addresses the following natural question: can efficient algorithms weakly learn convex sets under normal distributions? Strong learnability of convex sets under normal distributions is well understood, with near-matching upper and lower bounds given by Klivans et al (2008), but prior to the current work nothing seems to have been known about weak learning. We essentially answer this question, giving near-matching algorithms and lower bounds.  For our positive result, we give a poly(n)-time algorithm that can weakly learn the class of convex sets to advantage $\\Omega(1/\\sqrt{n})$ using only random examples drawn from the background Gaussian distribution. Our algorithm and analysis are based on a new “density increment” result for convex sets, which we prove using tools from isoperimetry.  We also give an information-theoretic lower bound showing that $O(\\log(n)/\\sqrt{n})$ advantage is best possible even for algorithms that are allowed to make poly(n) many membership queries",
    "volume": "main",
    "checked": true,
    "id": "cb1c9273e3303434aabb26860d34bd1599c21112",
    "citation_count": 3
  },
  "https://proceedings.mlr.press/v134/de21b.html": {
    "title": "Learning sparse mixtures of permutations from noisy information",
    "abstract": "We study the problem of learning an unknown mixture of k permutations over n elements, given access to noisy samples drawn from the unknown mixture. We consider a range of different noise models, including natural variants of the “heat kernel” noise framework and the Mallows model. We give an algorithm which, for each of these noise models, learns the unknown mixture to high accuracy under mild assumptions and runs in $n^{O(log k)}$ time. Our approach is based on a new procedure that recovers an unknown mixture of permutations from noisy higher-order marginals",
    "volume": "main",
    "checked": true,
    "id": "b24100debddecc242c5362f010816375ccd202e0",
    "citation_count": 0
  },
  "https://proceedings.mlr.press/v134/derezinski21a.html": {
    "title": "Sparse sketches with small inversion bias",
    "abstract": "For a tall $n\\times d$ matrix $A$ and a random $m\\times n$ sketching matrix $S$, the sketched estimate of the inverse covariance matrix $(A^\\top A)^{-1}$ is typically biased: $E[(\\tilde A^\\top\\tilde A)^{-1}]\\ne(A^\\top A)^{-1}$, where $\\tilde A=SA$. This phenomenon, which we call inversion bias, arises, e.g., in statistics and distributed optimization, when averaging multiple independently constructed estimates of quantities that depend on the inverse covariance. We develop a framework for analyzing inversion bias, based on our proposed concept of an $(\\epsilon,\\delta)$-unbiased estimator for random matrices. We show that when the sketching matrix $S$ is dense and has i.i.d. sub-gaussian entries, then after simple rescaling, the estimator $(\\frac m{m-d}\\tilde A^\\top\\tilde A)^{-1}$ is $(\\epsilon,\\delta)$-unbiased for $(A^\\top A)^{-1}$ with a sketch of size $m=O(d+\\sqrt d/\\epsilon)$. This implies that for $m=O(d)$, the inversion bias of this estimator is $O(1/\\sqrt d)$, which is much smaller than the $\\Theta(1)$ approximation error obtained as a consequence of the subspace embedding guarantee for sub-gaussian sketches. We then propose a new sketching technique, called LEverage Score Sparsified (LESS) embeddings, which uses ideas from both data-oblivious sparse embeddings as well as data-aware leverage-based row sampling methods, to get $\\epsilon$ inversion bias for sketch size $m=O(d\\log d+\\sqrt d/\\epsilon)$ in time $O(\\text{nnz}(A)\\log n+md^2)$, where nnz is the number of non-zeros. The key techniques enabling our analysis include an extension of a classical inequality of Bai and Silverstein for random quadratic forms, which we call the Restricted Bai-Silverstein inequality; and anti-concentration of the Binomial distribution via the Paley-Zygmund inequality, which we use to prove a lower bound showing that leverage score sampling sketches generally do not achieve small inversion bias",
    "volume": "main",
    "checked": true,
    "id": "39333aeef7c79adecf971eb025ac3a0fb9b5d3ac",
    "citation_count": 8
  },
  "https://proceedings.mlr.press/v134/diakonikolas21a.html": {
    "title": "The Sample Complexity of Robust Covariance Testing",
    "abstract": "We study the problem of testing the covariance matrix of a high-dimensional Gaussian in a robust setting, where the input distribution has been corrupted in Huber’s contamination model. Specifically, we are given i.i.d. samples from a distribution of the form $Z = (1-\\epsilon) X + \\epsilon B$, where $X$ is a zero-mean and unknown covariance Gaussian $\\mathcal{N}(0, \\Sigma)$, $B$ is a fixed but unknown noise distribution, and $\\epsilon>0$ is an arbitrarily small constant representing the proportion of contamination.  We want to distinguish between the cases that $\\Sigma$ is the identity matrix versus $\\gamma$-far from the identity in Frobenius norm.  In the absence of contamination, prior work gave a simple tester for this hypothesis testing task that uses $O(d)$ samples. Moreover, this sample upper bound was shown to be best possible, within constant factors. Our main result is that the sample complexity of covariance testing dramatically increases in the contaminated setting. In particular, we prove a sample complexity lower bound of $\\Omega(d^2)$ for $\\epsilon$ an arbitrarily small constant and $\\gamma = 1/2$.  This lower bound is best possible, as $O(d^2)$ samples suffice to even robustly {\\em learn} the covariance. The conceptual implication of our result is that, for the natural setting we consider, robust hypothesis testing is at least as hard as robust estimation",
    "volume": "main",
    "checked": true,
    "id": "69e0d5a46e4b7d1a510639ac0fb91f3897127a33",
    "citation_count": 4
  },
  "https://proceedings.mlr.press/v134/diakonikolas21b.html": {
    "title": "Agnostic Proper Learning of Halfspaces under Gaussian Marginals",
    "abstract": "We study the problem of agnostically learning halfspaces under the Gaussian distribution. Our main result is the {\\em first proper} learning algorithm for this problem whose running time qualitatively matches that of the best known improper agnostic learner. Building on this result, we also obtain the first proper polynomial time approximation scheme (PTAS) for agnostically learning homogeneous halfspaces. Our techniques naturally extend to agnostically learning linear models with respect to other activation functions, yielding the first proper agnostic algorithm for ReLU regression",
    "volume": "main",
    "checked": true,
    "id": "2d119656d1b613cc1e360cc7bab25f2fe144642f",
    "citation_count": 4
  },
  "https://proceedings.mlr.press/v134/diakonikolas21c.html": {
    "title": "The Optimality of Polynomial Regression for Agnostic Learning under Gaussian Marginals in the SQ Model",
    "abstract": "We study the problem of agnostic learning under the Gaussian distribution in the Statistical Query (SQ) model.  We develop a method for finding hard families of examples for a wide range of concept classes by using LP duality.  For Boolean-valued concept classes, we show that the $L^1$-polynomial regression algorithm is essentially best possible among SQ algorithms, and therefore that the SQ complexity of agnostic learning is closely related to the polynomial degree required to approximate any function from the concept class in $L^1$-norm. Using this characterization along with additional analytic tools, we obtain explicit optimal SQ lower bounds for agnostically learning linear threshold functions and the first non-trivial explicit SQ lower bounds for polynomial threshold functions and intersections of halfspaces. We also develop an analogous theory for agnostically learning real-valued functions, and as an application prove near-optimal SQ lower bounds for agnostically learning ReLUs and sigmoids",
    "volume": "main",
    "checked": false,
    "id": "77c044bf4fc8ead6483da56e800fe6ec0048a651",
    "citation_count": 12
  },
  "https://proceedings.mlr.press/v134/diakonikolas21d.html": {
    "title": "Boosting in the Presence of Massart Noise",
    "abstract": "We study the problem of boosting the accuracy of a weak learner in the (distribution-independent) PAC model with Massart noise. In the Massart noise model, the label of each example $x$ is independently misclassified with probability $\\eta(x) \\leq \\eta$, where $\\eta<1/2$. The Massart model lies between the random classification noise model and the agnostic model. Our main positive result is the first computationally efficient boosting algorithm in the presence of Massart noise that achieves misclassification error arbitrarily close to $\\eta$. Prior to our work, no non-trivial booster was known in this setting. Moreover, we show that this error upper bound is best possible for polynomial-time black-box boosters, under standard cryptographic assumptions. Our upper and lower bounds characterize the complexity of boosting in the distribution-independent PAC model with Massart noise. As a simple application of our positive result, we give the first efficient Massart learner for unions of high-dimensional rectangles",
    "volume": "main",
    "checked": true,
    "id": "f42ace1d3dc07449bc0b2c5f60f03f9b60468b99",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v134/diakonikolas21e.html": {
    "title": "Outlier-Robust Learning of Ising Models Under Dobrushin's Condition",
    "abstract": "We study the problem of learning Ising models satisfying Dobrushin’s condition in the outlier-robust setting where a constant fraction of the samples are adversarially corrupted. Our main result is to provide the first computationally efficient robust learning algorithm for this problem with near-optimal error guarantees. Our algorithm can be seen as a special case of an algorithm for robustly learning a distribution from a general exponential family. To prove its correctness for Ising models, we establish new anti-concentration results for degree-2 polynomials of Ising models that may be of independent interest",
    "volume": "main",
    "checked": true,
    "id": "684ffa0e55e7489a0620b43352463abade7f0799",
    "citation_count": 9
  },
  "https://proceedings.mlr.press/v134/ding21a.html": {
    "title": "Random Coordinate Langevin Monte Carlo",
    "abstract": "Langevin Monte Carlo (LMC) is a popular Markov chain Monte Carlo sampling method. One drawback is that it requires the computation of the full gradient at each iteration, an expensive operation if the dimension of the problem is high. We propose a new sampling method: Random Coordinate LMC (RC-LMC). At each iteration, a single coordinate is randomly selected to be updated by a multiple of the partial derivative along this direction plus noise, while all other coordinates remain untouched. We investigate the total complexity of RC-LMC and compare it with the classical LMC for log-concave probability distributions. We show that when the gradient of the log-density is Lipschitz, RC-LMC is less expensive than the classical LMC if the log-density is highly skewed for high dimensional problems. Further, when both the gradient and the Hessian of the log-density are Lipschitz, RC-LMC is always cheaper than the classical LMC, by a factor proportional to the square root of the problem dimension. In the latter case, we use an example to demonstrate that our estimate of complexity is sharp with respect to the dimension",
    "volume": "main",
    "checked": true,
    "id": "63b11f76a1a5090986b48b69c9611e38a89d2442",
    "citation_count": 5
  },
  "https://proceedings.mlr.press/v134/durmus21a.html": {
    "title": "On the Stability of Random Matrix Product with Markovian Noise: Application to Linear Stochastic Approximation and TD Learning",
    "abstract": "This paper studies the exponential stability of random matrix products driven by a general (possibly unbounded) state space Markov chain. It is a cornerstone in the analysis of stochastic algorithms in machine learning (e.g. for parameter tracking in online-learning or reinforcement learning). The existing results impose strong conditions such as uniform boundedness of the matrix-valued functions and uniform ergodicity of the Markov chains.  Our main contribution is an exponential stability result for the p-th moment of random matrix product, provided that (i) the underlying Markov chain satisfies a super-Lyapunov drift condition, (ii) the growth of the matrix-valued functions is controlled by an appropriately defined function (related to the drift condition). Using this result, we give finite-time p-th moment bounds for constant and decreasing stepsize linear stochastic approximation schemes with Markovian noise on general state space. We illustrate these findings for linear value-function estimation in reinforcement learning. We provide finite-time p-th moment bound for various members of temporal difference (TD) family of algorithms",
    "volume": "main",
    "checked": true,
    "id": "5553aadc0a6273b27c7871683a74822e8a899957",
    "citation_count": 9
  },
  "https://proceedings.mlr.press/v134/dwivedi21a.html": {
    "title": "Kernel Thinning",
    "abstract": "We introduce kernel thinning, a new procedure for compressing a distribution $\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given a suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel thinning compresses an $n$-point approximation to $\\mathbb{P}$ into a $\\sqrt{n}$-point approximation with comparable worst-case integration error across the associated reproducing kernel Hilbert space. With high probability, the maximum discrepancy in integration error is $\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported $\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log n})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$.  In contrast, an equal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$ integration error.  Our sub-exponential guarantees resemble the classical quasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply to general distributions on $\\mathbb{R}^d$ and a wide range of common kernels. We use our results to derive explicit non-asymptotic maximum mean discrepancy bounds for Gaussian, Matérn, and B-spline kernels and present two vignettes illustrating the practical benefits of kernel thinning over i.i.d. sampling and standard Markov chain Monte Carlo thinning",
    "volume": "main",
    "checked": true,
    "id": "9827e9f307e9553bfdb7f87bb1e21b2f33e577c2",
    "citation_count": 6
  },
  "https://proceedings.mlr.press/v134/eldan21a.html": {
    "title": "Non-asymptotic approximations of neural networks by Gaussian processes",
    "abstract": "We study the extent to which wide neural networks may be approximated by Gaussian processes, when initialized with random weights. It is a well-established fact that as the width of a network goes to infinity, its law converges to that of a Gaussian process. We make this quantitative by establishing explicit convergence rates for the central limit theorem in an infinite-dimensional functional space, metrized with a natural transportation distance. We identify two regimes of interest; when the activation function is polynomial, its degree determines the rate of convergence, while for non-polynomial activations, the rate is governed by the smoothness of the function",
    "volume": "main",
    "checked": true,
    "id": "0d4e66d40c933e438345133a638ecba5b1b552f5",
    "citation_count": 8
  },
  "https://proceedings.mlr.press/v134/erdogdu21a.html": {
    "title": "On the Convergence of Langevin Monte Carlo: The Interplay between Tail Growth and Smoothness",
    "abstract": "We study sampling from a target distribution $\\nu_* = e^{-f}$ using the unadjusted Langevin Monte Carlo (LMC) algorithm.  For any potential function $f$ whose tails behave like $\\|x\\|^\\alpha$ for ${\\alpha \\in [1,2]}$, and has $\\beta$-Hölder continuous gradient, we prove that $\\widetilde{\\mathcal{O}} \\Big(d^{\\frac{1}{\\beta}+\\frac{1+\\beta}{\\beta}(\\frac{2}{\\alpha}-{1}_{\\{\\alpha \\neq 1\\}})} \\epsilon^{-\\frac{1}{\\beta}}\\Big)$ steps are sufficient to reach the $\\epsilon$-neighborhood of a $d$-dimensional target distribution $\\nu_*$ in KL-divergence.  This bound, in terms of $\\epsilon$ dependency, is not directly influenced by the tail growth rate $\\alpha$ of the potential function as long as its growth is at least linear, and it only relies on the order of smoothness $\\beta$.  One notable consequence of this result is that for potentials with Lipschitz gradient, i.e. $\\beta=1$, the above rate recovers the best known rate $\\widetilde{\\mathcal{O}} (d\\epsilon^{-1})$ which was established for strongly convex potentials in terms of $\\epsilon$ dependency, but we show that the same rate is achievable for a wider class of potentials that are degenerately convex at infinity.  The growth rate $\\alpha$ affects the rate estimate in high dimensions where $d$ is large; furthermore, it recovers the best-known dimension dependency when the tail growth of the potential is quadratic, i.e. $\\alpha = 2$, in the current setup",
    "volume": "main",
    "checked": true,
    "id": "f9b993d6ed9b2b38aa7cd5e0c43da286762a351d",
    "citation_count": 37
  },
  "https://proceedings.mlr.press/v134/esfandiari21a.html": {
    "title": "Adaptivity in Adaptive Submodularity",
    "abstract": "Adaptive sequential decision making is one of the central challenges in machine learning and artificial intelligence. In such problems, the goal is to design an interactive policy that plans for an action to take, from a finite set of $n$ actions, given some partial observations. It has been shown that in many applications such as active learning, robotics, sequential experimental design, and active detection, the utility function satisfies adaptive submodularity, a notion that generalizes the notion of diminishing returns to policies.  In this paper, we revisit the power of adaptivity in maximizing an adaptive monotone submodular function. We propose an efficient semi adaptive policy that with $O(\\log n \\times\\log k)$ adaptive rounds of observations can achieve an almost tight $1-1/e-\\epsilon$ approximation guarantee with respect to an optimal policy that carries out $k$ actions in a fully sequential manner. To complement our results, we also show that it is impossible to achieve a constant factor approximation with $o(\\log n)$ adaptive rounds. We also extend our result to the case of adaptive stochastic minimum cost coverage where the goal is to reach a desired utility $Q$ with the cheapest policy. We first prove the long-standing conjecture by Golovin and Krause and show that the greedy policy achieves the asymptotically tight logarithmic approximation guarantee. We then propose a semi adaptive policy that provides the same guarantee in polylogarithmic adaptive rounds through a similar information-parallelism scheme. Our results shrink the adaptivity gap in adaptive submodular maximization by an exponential factor",
    "volume": "main",
    "checked": true,
    "id": "5bd02fe8f1c590bb849d273e93dbf1799a2056fe",
    "citation_count": 24
  },
  "https://proceedings.mlr.press/v134/even21a.html": {
    "title": "Concentration of Non-Isotropic Random Tensors with Applications to Learning and Empirical Risk Minimization",
    "abstract": "Dimension is an inherent bottleneck to some modern learning tasks, where optimization methods suffer from the size of the data. In this paper, we study non-isotropic distributions of data and develop tools that aim at reducing these dimensional costs by a dependency on an effective dimension rather than the ambient one.  Based on non-asymptotic estimates of the metric entropy of ellipsoids -that prove to generalize to infinite dimensions- and on a chaining argument, our uniform concentration bounds involve an effective dimension instead of the global dimension, improving over existing results.  We show the importance of taking advantage of non-isotropic properties in learning problems with the following applications: i) we improve state-of-the-art results in statistical preconditioning for communication-efficient distributed optimization, ii) we introduce a non-isotropic randomized smoothing for non-smooth optimization. Both applications cover a class of functions that encompasses empirical risk minization (ERM) for linear models",
    "volume": "main",
    "checked": true,
    "id": "10ede2dc08c29066c98a210a4568bd2f0c8dc61e",
    "citation_count": 3
  },
  "https://proceedings.mlr.press/v134/fang21a.html": {
    "title": "Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks",
    "abstract": "This paper proposes a new mean-field framework for over-parameterized deep neural networks (DNNs), which can be used to analyze neural network training.  In this framework, a DNN is represented by probability measures and functions over its features (that is, the function values of the hidden units over the training data) in the continuous limit, instead of the neural network parameters as most existing studies have done. This new representation overcomes the degenerate situation where all the hidden units essentially have only one meaningful hidden unit in each middle layer, leading to a simpler representation of DNNs. Moreover, we construct a non-linear dynamics called neural feature flow, which captures the evolution of an over-parameterized DNN trained by Gradient Descent. We illustrate the framework via the Residual Network (Res-Net) architecture.  It is shown that when the neural feature flow process converges, it reaches a global minimal solution under suitable conditions",
    "volume": "main",
    "checked": true,
    "id": "dd0cbd365304eddec5ee961ced19291e1463bad1",
    "citation_count": 31
  },
  "https://proceedings.mlr.press/v134/feder21a.html": {
    "title": "Sequential prediction under log-loss and misspecification",
    "abstract": "We consider the question of sequential prediction under the log-loss in terms of cumulative regret. Namely, given a hypothesis class of distributions, learner sequentially predicts the (distribution of the) next letter in sequence and its performance is compared to the baseline of the best constant predictor from the hypothesis class. The well-specified case corresponds to an additional assumption that the data-generating distribution belongs to the hypothesis class as well. Here we present results in the more general misspecified case. Due to special properties of the log-loss, the same problem arises in the context of competitive-optimality in density estimation, and model selection.  For the $d$-dimensional Gaussian location hypothesis class, we show that cumulative regrets in the well-specified and misspecified cases asymptotically coincide. In other words, we provide an $o(1)$ characterization of the distribution-free (or PAC) regret in this case – the first such result as far as we know. We recall that the worst-case (or individual-sequence) regret in this case is larger by an additive constant ${d\\over 2} + o(1)$.  Surprisingly, neither the traditional Bayesian estimators, nor the Shtarkov’s normalized maximum likelihood achieve the PAC regret and our estimator requires special “robustification” against heavy-tailed data.  In addition, we show two general results for misspecified regret: the existence and uniqueness of the optimal estimator, and the bound sandwiching the misspecified regret between well-specified regrets with (asymptotically) close hypotheses classes",
    "volume": "main",
    "checked": true,
    "id": "4e881ff878ce5528e72c63a93c0fa8be75d106bf",
    "citation_count": 4
  },
  "https://proceedings.mlr.press/v134/fontaine21a.html": {
    "title": "Convergence rates and approximation results for SGD and its continuous-time counterpart",
    "abstract": "This paper proposes a thorough theoretical analysis of Stochastic Gradient Descent (SGD) with non-increasing step sizes.  First, we show that the recursion defining SGD can be provably approximated by solutions of a time inhomogeneous Stochastic Differential Equation (SDE) using an appropriate coupling. In the specific case of a batch noise we refine our results using recent advances in Stein’s method. Then, motivated by recent analyses of deterministic and stochastic optimization methods by their continuous counterpart, we study the long-time behavior of the continuous processes at hand and establish non-asymptotic bounds. To that purpose, we develop new comparison techniques which are of independent interest. Adapting these techniques to the discrete setting, we show that the same results hold for the corresponding SGD sequences.  In our analysis, we notably improve non-asymptotic bounds in the convex setting for SGD under weaker assumptions than the ones considered in previous works. Finally, we also establish finite-time convergence results under various conditions, including relaxations of the famous Ł{ojasciewicz} inequality, which can be applied to a class of non-convex functions",
    "volume": "main",
    "checked": true,
    "id": "6cf95374e9695fcf5661b03c1a8ccd82368dd1e6",
    "citation_count": 6
  },
  "https://proceedings.mlr.press/v134/foster21a.html": {
    "title": "Instance-Dependent Complexity of Contextual Bandits and Reinforcement Learning: A Disagreement-Based Perspective",
    "abstract": "In the classical multi-armed bandit problem, instance-dependent algorithms attain improved performance on \"easy\" problems with a gap between the best and second-best arm. Are similar guarantees possible for contextual bandits? While positive results are known for certain special cases, there is no general theory characterizing when and how instance-dependent regret bounds for contextual bandits can be achieved for rich, general classes of policies. We introduce a family of complexity measures that are both sufficient and necessary to obtain instance-dependent regret bounds. We then introduce new oracle-efficient algorithms which adapt to the gap whenever possible, while also attaining the minimax rate in the worst case. Finally, we provide structural results that tie together a number of complexity measures previously proposed throughout contextual bandits, reinforcement learning, and active learning and elucidate their role in determining the optimal instance-dependent regret. In a large-scale empirical evaluation, we find that our approach often gives superior results for challenging exploration problems.  Turning our focus to reinforcement learning with function approximation, we develop new oracle-efficient algorithms for reinforcement learning with rich observations that obtain optimal gap-dependent sample complexity",
    "volume": "main",
    "checked": true,
    "id": "643bf18fe4434d6dd8ebc7dbef8361c09d9a34b1",
    "citation_count": 38
  },
  "https://proceedings.mlr.press/v134/fotakis21a.html": {
    "title": "Efficient Algorithms for Learning from Coarse Labels",
    "abstract": "For many learning problems one may not have access to fine grained label information; e.g., an image can be labeled as husky, dog, or even animal depending on the expertise of the annotator.  In this work, we formalize these settings and study the problem of learning from such coarse data.  Instead of observing the actual labels from a set $\\mathcal{Z}$, we observe coarse labels corresponding to a partition of $\\mathcal{Z}$ (or a mixture of partitions).  Our main algorithmic result is that essentially any problem learnable from fine grained labels can also be learned efficiently when the coarse data are sufficiently informative.  We obtain our result through a generic reduction for answering Statistical Queries (SQ) over fine grained labels given only coarse labels.  The number of coarse labels required depends polynomially on the information distortion due to coarsening and the number of fine labels $|\\mathcal{Z}|$.  We also investigate the case of (infinitely many) real valued labels focusing on a central problem in censored and truncated statistics: Gaussian mean estimation from coarse data.  We provide an efficient algorithm when the sets in the partition are convex and establish that the problem is NP-hard even for very simple non-convex sets",
    "volume": "main",
    "checked": true,
    "id": "ba656b19c5800c2e0013c97480d3f66a5fa270f4",
    "citation_count": 3
  },
  "https://proceedings.mlr.press/v134/ganassali21a.html": {
    "title": "Impossibility of Partial Recovery in the Graph Alignment Problem",
    "abstract": "Random graph alignment refers to recovering the underlying vertex correspondence between two random graphs with correlated edges. This can be viewed as an average-case and noisy version of the well-known graph isomorphism problem. For the correlated Erdös-Rényi model, we prove the first impossibility result for partial recovery in the sparse regime (with constant average degree). Our bound is tight in the noiseless case (the graph isomorphism problem) and we conjecture that it is still tight with noise. Our proof technique relies on a careful application of the probabilistic method to build automorphisms between tree components of a subcritical Erdös-Rényi graph",
    "volume": "main",
    "checked": true,
    "id": "321bbdcfa371a3576fb7fcf0fd6de772f30d5eda",
    "citation_count": 8
  },
  "https://proceedings.mlr.press/v134/garber21a.html": {
    "title": "Frank-Wolfe with a Nearest Extreme Point Oracle",
    "abstract": "We consider variants of the classical Frank-Wolfe algorithm for constrained smooth convex minimization, that instead of access to the standard oracle for minimizing a linear function over the feasible set, have access to an oracle that can find an extreme point of the feasible set that is closest in Euclidean distance to a given vector. We first show that for many feasible sets of interest, such an oracle can be implemented with the same complexity as the standard linear optimization oracle. We then show that with such an oracle we can design new Frank-Wolfe variants which enjoy significantly improved complexity bounds in case the set of optimal solutions lies in the convex hull of a subset of extreme points with small diameter (e.g., a low-dimensional face of a polytope). In particular, for many $0\\text{–}1$ polytopes, under quadratic growth and strict complementarity conditions, we obtain the first linearly convergent variant with rate that depends only on the dimension of the optimal face and not on the ambient dimension",
    "volume": "main",
    "checked": true,
    "id": "1770e5a4be60bbc7912ffc7eee93952b0092d196",
    "citation_count": 3
  },
  "https://proceedings.mlr.press/v134/ghazi21a.html": {
    "title": "On Avoiding the Union Bound When Answering Multiple Differentially Private Queries",
    "abstract": "In this work, we study the problem of answering $k$ queries with $(\\epsilon, \\delta)$-differential privacy, where each query has sensitivity one. We give an algorithm for this task that achieves an expected $\\ell_\\infty$ error bound of $O(\\frac{1}{\\epsilon}\\sqrt{k \\log \\frac{1}{\\delta}})$, which is known to be tight (Steinke and Ullman, 2016).  A very recent work by Dagan and Kur (2020) provides a similar result, albeit via a completely different approach. One difference between our work and theirs is that our guarantee holds even when $\\delta < 2^{-\\Omega(k/(\\log k)^8)}$ whereas theirs does not apply in this case. On the other hand, the algorithm of Dagan and Kur (2020) has a remarkable advantage that the $\\ell_{\\infty}$ error bound of $O(\\frac{1}{\\epsilon}\\sqrt{k \\log \\frac{1}{\\delta}})$ holds not only in expectation but always (i.e., with probability one) while we can only get a high probability (or expected) guarantee on the error",
    "volume": "main",
    "checked": true,
    "id": "cad5aa45022e00b0f4fbc65a96684622a536e7ee",
    "citation_count": 5
  },
  "https://proceedings.mlr.press/v134/giannou21a.html": {
    "title": "Survival of the strictest: Stable and unstable equilibria under regularized learning with partial information",
    "abstract": "In this paper, we examine the Nash equilibrium convergence properties of no-regret learning in general N -player games. For concreteness, we focus on the archetypal “follow the regularized leader” (FTRL) family of algorithms, and we consider the full spectrum of uncertainty that the players may encounter – from noisy, oracle-based feedback, to bandit, payoff-based information. In this general context, we establish a comprehensive equivalence between the stability of a Nash equilibrium and its support: a Nash equilibrium is stable and attracting with arbitrarily high probability if and only if it is strict (i.e., each equilibrium strategy has a unique best response). This equivalence extends existing continuous-time versions of the “folk theorem” of evolutionary game theory to a bona fide algorithmic learning setting, and it provides a clear refinement criterion for the prediction of the day-to-day behavior of no-regret learning in games",
    "volume": "main",
    "checked": true,
    "id": "465cd561eed5c567230fcfc41cc443b06484f09e",
    "citation_count": 15
  },
  "https://proceedings.mlr.press/v134/golowich21a.html": {
    "title": "Differentially Private Nonparametric Regression Under a Growth Condition",
    "abstract": "Given a real-valued hypothesis class H, we investigate under what conditions there is a differentially private algorithm which learns an optimal hypothesis from H given i.i.d. data. Inspired by recent results for the related setting of binary classification (Alon et al., 2019; Bun et al., 2020), where it was shown that online learnability of a binary class is necessary and sufficient for its private learnability, Jung et al. (2020) showed that in the setting of regression, online learnability of H is necessary for private learnability. Here online learnability of H is characterized by the finiteness of its eta-sequential fat shattering dimension, sfat_eta(H), for all eta > 0. In terms of sufficient conditions for private learnability, Jung et al. (2020) showed that H is privately learnable if lim_{\\eta -> 0} sfat_eta(H) is finite, which is a fairly restrictive condition. We show that under the relaxed condition liminf_{eta -> 0} eta * sfat_eta(H) = 0, H is privately learnable, thus establishing the first nonparametric private learnability guarantee for classes H with sfat_eta(H) diverging as eta -> 0. Our techniques involve a novel filtering procedure to output stable hypotheses for nonparametric function classes",
    "volume": "main",
    "checked": true,
    "id": "e5da517708f5f309e4bcf763bfb90aa7079ac638",
    "citation_count": 3
  },
  "https://proceedings.mlr.press/v134/gordon21a.html": {
    "title": "Source Identification for Mixtures of Product Distributions",
    "abstract": "We give an algorithm for source identification of a mixture of k product distributions on n bits. This is a fundamental problem in machine learning with many applications. Our algorithm identifies the source parameters of an identifiable mixture, given, as input, approximate values of multilinear moments (derived, for instance, from a sufficiently large sample), using $2^{O(k^2)}n^{O(k)}$ arithmetic operations. Our result is the first explicit bound on the computational complexity of source identification of such mixtures. The running time improves previous results by Feldman, O’Donnell, and Servedio (FOCS 2005) and Chen and Moitra (STOC 2019) that guaranteed only learning the mixture (without parametric identification of the source). Our analysis gives a quantitative version of a qualitative characterization of identifiable sources that is due to Tahmasebi, Motahari, and Maddah-Ali (ISIT 2018)",
    "volume": "main",
    "checked": true,
    "id": "75c0a2759bf77b5f7da88b5310bf75e34dd40431",
    "citation_count": 8
  },
  "https://proceedings.mlr.press/v134/grunwald21a.html": {
    "title": "PAC-Bayes, MAC-Bayes and Conditional Mutual Information: Fast rate bounds that handle general VC classes",
    "abstract": "We give a novel, unified derivation of conditional PAC-Bayesian and mutual information (MI) generalization bounds. We derive conditional MI bounds as an instance, with special choice of prior, of conditional MAC-Bayesian (Mean Approximately Correct) bounds, itself derived from conditional PAC-Bayesian bounds, where ‘conditional’ means that one can use priors conditioned on a joint training and ghost sample.  This allows us to get nontrivial PAC-Bayes and MI-style bounds for general VC classes, something recently shown to be impossible with standard PAC-Bayesian/MI bounds. Second, it allows us to get fast rates of order $O((\\text{KL}/n)^{\\gamma}$ for $\\gamma > 1/2$ if a Bernstein condition holds and for exp-concave losses (with $\\gamma=1$), which is impossible with both standard PAC-Bayes generalization and MI bounds. Our work extends the recent work by Steinke and Zakynthinou (2020) who handle MI with VC but neither PAC-Bayes nor fast rates and Mhammedi et al. (2019) who initiated fast rate PAC-Bayes generalization error bounds but handle neither MI nor general VC classes",
    "volume": "main",
    "checked": true,
    "id": "74c72266179efb4d35dbb03368d4b75938b35dc5",
    "citation_count": 11
  },
  "https://proceedings.mlr.press/v134/guo21a.html": {
    "title": "Generalizing Complex Hypotheses on Product Distributions: Auctions, Prophet Inequalities, and Pandora's Problem",
    "abstract": "This paper explores a theory of generalization for learning problems on product distributions, complementing the existing learning theories in the sense that it does not rely on any complexity measures of the hypothesis classes.  The main contributions are two general sample complexity bounds: (1) $\\tilde{O} \\big( \\frac{nk}{\\epsilon^2} \\big)$ samples are sufficient and necessary for learning an $\\epsilon$-optimal hypothesis in \\emph{any problem} on an $n$-dimensional product distribution, whose marginals have finite supports of sizes at most $k$; (2) $\\tilde{O} \\big( \\frac{n}{\\epsilon^2} \\big)$ samples are sufficient and necessary for any problem on $n$-dimensional product distributions if it satisfies a notion of strong monotonicity from the algorithmic game theory literature.  As applications of these theories, we match the optimal sample complexity for single-parameter revenue maximization (Guo et al., STOC 2019), improve the state-of-the-art for multi-parameter revenue maximization (Gonczarowski and Weinberg, FOCS 2018) and prophet inequality (Correa et al., EC 2019; Rubinstein et al., ITCS 2020), and provide the first and tight sample complexity bound for Pandora’s problem",
    "volume": "main",
    "checked": true,
    "id": "9ee6970eeb7e0cf2f5267871f454a4458ecdb8ad",
    "citation_count": 11
  },
  "https://proceedings.mlr.press/v134/hanneke21a.html": {
    "title": "Online Learning with Simple Predictors and a Combinatorial Characterization of Minimax in 0/1 Games",
    "abstract": "Which classes can be learned properly in the online model? — that is, by an algorithm that on each round uses a predictor from the concept class. While there are simple and natural cases where improper learning is useful and even necessary, it is natural to ask how complex must the improper predictors be in such cases.  Can one always achieve nearly optimal mistake/regret bounds using \"simple\" predictors?  In this work, we give a complete characterization of when this is possible, thus settling an open problem which has been studied since the pioneering works of Angluin (1987) and Littlestone (1988).  More precisely, given any concept class C and any hypothesis class H, we provide nearly tight bounds (up to a log factor) on the optimal mistake bounds for online learning C using predictors from H.  Our bound yields an exponential improvement over the previously best known bound by Chase and Freitag (2020).  As applications, we give constructive proofs showing that (i) in the realizable setting, a near-optimal mistake bound (up to a constant factor) can be attained by a sparse majority-vote of proper predictors, and (ii) in the agnostic setting, a near-optimal regret bound (up to a log factor) can be attained by a randomized proper algorithm.  The latter was proven non-constructively by Rakhlin, Sridharan, and Tewari (2015).  It was also achieved by constructive but improper algorithms proposed by Ben-David, Pal, and Shalev-Shwartz (2009) and Rakhlin, Shamir, and Sridharan (2012).  A technical ingredient of our proof which may be of independent interest is a generalization of the celebrated Minimax Theorem (von Neumann, 1928) for binary zero-sum games with arbitrary action-sets: a simple game which fails to satisfy Minimax is \"Guess the Larger Number\".  In this game, each player picks a natural number and the player who picked the larger number wins.  Equivalently, the payoff matrix of this game is infinite triangular.  We show that this is the only obstruction: if the payoff matrix does not contain triangular submatrices of unbounded sizes then the Minimax theorem is satisfied.  This generalizes von Neumann’s Minimax Theorem by removing requirements of finiteness (or compactness) of the action-sets, and moreover it captures precisely the types of games of interest in online learning: namely, Littlestone games",
    "volume": "main",
    "checked": true,
    "id": "8430b9223396ceb407b9b14dcc7a28c4d5f88e73",
    "citation_count": 4
  },
  "https://proceedings.mlr.press/v134/haochen21a.html": {
    "title": "Shape Matters: Understanding the Implicit Bias of the Noise Covariance",
    "abstract": "The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect for training overparameterized models. Prior theoretical work largely focuses on spherical Gaussian noise, whereas empirical studies demonstrate the phenomenon that parameter-dependent noise — induced by mini-batches or label perturbation — is far more effective than Gaussian noise.  This paper theoretically characterizes this phenomenon on a quadratically-parameterized model introduced by Vaskevicius et al. and Woodworth et al.  We show that in an over-parameterized setting, SGD with label noise recovers the sparse ground-truth with an arbitrary initialization, whereas SGD with Gaussian noise or gradient descent overfits to dense solutions with large norms. Our analysis reveals that parameter-dependent noise introduces a bias towards local minima with smaller noise variance, whereas spherical Gaussian noise does not",
    "volume": "main",
    "checked": true,
    "id": "82b20ed50126e106091dd16aaeb538cbb3bfddb9",
    "citation_count": 44
  },
  "https://proceedings.mlr.press/v134/hopkins21a.html": {
    "title": "Bounded Memory Active Learning through Enriched Queries",
    "abstract": "The explosive growth of easily-accessible unlabeled data has lead to growing interest in \\emph{active learning}, a paradigm in which data-hungry learning algorithms adaptively select informative examples in order to lower prohibitively expensive labeling costs. Unfortunately, in standard worst-case models of learning, the active setting often provides no improvement over non-adaptive algorithms. To combat this, a series of recent works have considered a model in which the learner may ask \\emph{enriched} queries beyond labels. While such models have seen success in drastically lowering label costs, they tend to come at the expense of requiring large amounts of memory. In this work, we study what families of classifiers can be learned in \\emph{bounded memory}. To this end, we introduce a novel streaming-variant of enriched-query active learning along with a natural combinatorial parameter called \\emph{lossless sample compression} that is sufficient for learning not only with bounded memory, but in a query-optimal and computationally efficient manner as well. Finally, we give three fundamental examples of classifier families with small, easy to compute lossless compression schemes when given access to basic enriched queries: axis-aligned rectangles, decision trees, and halfspaces in two dimensions",
    "volume": "main",
    "checked": true,
    "id": "9c282928739008b37759fdc59bed8a9ac87f0614",
    "citation_count": 3
  },
  "https://proceedings.mlr.press/v134/hsieh21a.html": {
    "title": "Adaptive Learning in Continuous Games: Optimal Regret Bounds and Convergence to Nash Equilibrium",
    "abstract": "In game-theoretic learning, several agents are simultaneously following their individual interests, so the environment is non-stationary from each player’s perspective. In this context, the performance of a learning algorithm is often measured by its regret. However, no-regret algorithms are not created equal in terms of game-theoretic guarantees: depending on how they are tuned, some of them may drive the system to an equilibrium, while others could produce cyclic, chaotic, or otherwise divergent trajectories. To account for this, we propose a range of no-regret policies based on optimistic mirror descent, with the following desirable properties: (\\emph{i}) they do not require \\emph{any} prior tuning or knowledge of the game; (\\emph{ii}) they all achieve $\\mathcal{O}(\\sqrt{T})$ regret against arbitrary, adversarial opponents; and (\\emph{iii}) they converge to the best response against convergent opponents. Also, if employed by all players, then (\\emph{iv}) they guarantee $\\mathcal{O}(1)$ \\emph{social} regret; while (\\emph{v}) the induced sequence of play converges to Nash equilibirum with $\\mathcal{O}(1)$ \\emph{individual} regret in all variationally stable games (a class of games that includes all monotone and convex-concave zero-sum games)",
    "volume": "main",
    "checked": true,
    "id": "e6aaac94df717786a467d057cb2157b9d49f0974",
    "citation_count": 26
  },
  "https://proceedings.mlr.press/v134/hsu21a.html": {
    "title": "On the Approximation Power of Two-Layer Networks of Random ReLUs",
    "abstract": "This paper considers the following question: how well can depth-two ReLU networks with randomly initialized bottom-level weights represent smooth functions? We give near-matching upper- and lower-bounds for L2-approximation in terms of the Lipschitz constant, the desired accuracy, and the dimension of the problem, as well as similar results in terms of Sobolev norms. Our positive results employ tools from harmonic analysis and ridgelet representation theory, while our lower-bounds are based on (robust versions of) dimensionality arguments",
    "volume": "main",
    "checked": true,
    "id": "f6b1505b17e2eed435e2e808db94b31cfb30b606",
    "citation_count": 9
  },
  "https://proceedings.mlr.press/v134/hu21a.html": {
    "title": "Fast Rates for the Regret of Offline Reinforcement Learning",
    "abstract": "We study the regret of reinforcement learning from offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted $Q$-iteration (FQI), suggest a $O(1/\\sqrt{n})$ convergence for regret, empirical behavior exhibits much faster convergence. In this paper, we present a finer regret analysis that exactly characterized this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function $Q^*$, the regret of the policy it defines converges at a rate given by the exponentiation of the $Q^*$-estimate’s pointwise convergence rate, thus speeding it up. The level of exponentiation depends on the level of noise in the decision-making problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman residual minimization to establish the correct pointwise convergence guarantees. As specific cases, our results imply $O(1/n)$ regret rates in linear cases and $\\exp(-\\Omega(n))$ regret rates in tabular cases",
    "volume": "main",
    "checked": true,
    "id": "4f6ca03c0ce6fd78a2ebe5caadb9c7605d5f4317",
    "citation_count": 14
  },
  "https://proceedings.mlr.press/v134/huang21a.html": {
    "title": "Streaming k-PCA: Efficient guarantees for Oja's algorithm, beyond rank-one updates",
    "abstract": "We analyze Oja’s algorithm for streaming $k$-PCA, and prove that it achieves performance nearly matching that of an optimal offline algorithm. Given access to a sequence of i.i.d. $d \\times d$ symmetric matrices, we show that Oja’s algorithm can obtain an accurate approximation to the subspace of the top $k$ eigenvectors of their expectation using a number of samples that scales polylogarithmically with $d$. Previously, such a result was only known in the case where the updates have rank one.  Our analysis is based on recently developed matrix concentration tools, which allow us to prove strong bounds on the tails of the random matrices which arise in the course of the algorithm’s execution",
    "volume": "main",
    "checked": true,
    "id": "0a346bcc448dd56a18e5d9c8189aee0010b39aa7",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v134/iliopoulos21a.html": {
    "title": "Group testing and local search: is there a computational-statistical gap?",
    "abstract": "Group testing is a fundamental problem in statistical inference with many real-world applications, including the need for massive group testing during the ongoing COVID-19 pandemic. In this paper we study the task of approximate recovery, in which we tolerate having a small number of incorrectly classified items.  One of the most well-known, optimal, and easy to implement testing procedures is the non-adaptive Bernoulli group testing, where all tests are conducted in parallel, and each item is chosen to be part of any certain test independently with some fixed probability. In this setting, there is an observed gap between the number of tests above which recovery is information theoretically possible, and the number of tests required by the currently best known efficient algorithms to succeed. In this paper we seek to understand whether this computational-statistical gap can be closed. Our main contributions are the following: 1.Often times such gaps are explained by a phase transition in the landscape of the solution space of the problem (an Overlap Gap Property (OGP) phase transition). We provide first moment evidence that, perhaps surprisingly, such a phase transition does not take place throughout the regime for which recovery is information theoretically possible. This fact suggests that the model is in fact amenable to local search algorithms.  2. We prove the complete absence of “bad” local minima for a part of the “hard” regime, a fact which implies an improvement over known theoretical results on the performance of efficient algorithms for approximate recovery without false-negatives.  3. Finally, motivated by the evidence for the abscence for the OGP, we present extensive simulations that strongly suggest that a very simple local algorithm known as Glauber Dynamics does indeed succeed, and can be used to efficiently implement the well-known (theoretically optimal) Smallest Satisfying Set (SSS) estimator. Given that practical algorithms for this task utilize Branch and Bound and Linear Programming relaxation techniques, our finding could potentially be of practical interest",
    "volume": "main",
    "checked": true,
    "id": "0d3fefd50b2f997beda6d9c895a75154e20d3a25",
    "citation_count": 2
  },
  "https://proceedings.mlr.press/v134/ito21a.html": {
    "title": "Parameter-Free Multi-Armed Bandit Algorithms with Hybrid Data-Dependent Regret Bounds",
    "abstract": "This paper presents multi-armed bandit (MAB) algorithms that work well in adversarial environments and that offer improved performance by exploiting inherent structures in such environments, as stochastic generative models, as well as small variations in loss vectors. The fundamental aim of this work is to overcome the limitation of worst-case analyses in MAB contexts. There can be found two basic approaches achieving this purpose: best-of-both-worlds algorithms that work well for both stochastic and adversarial settings, and data-dependent regret bounds that work well depending on certain difficulty indicators w.r.g. loss sequences. One remarkable study w.r.t. the best-of-both-worlds approach deals with the Tsallis-INF algorithm \\citep{zimmert2019optimal}, which achieves nearly optimal regret bounds up to small constants in both settings, though such bounds have remained unproven for a special case of a stochastic setting with multiple optimal arms.  This paper offers two particular contributions: (i) We show that the Tsallis-INF algorithm enjoys a regret bound of a logarithmic order in the number of rounds for stochastic environments, even if the best arm is not unique. (ii) We provide a new algorithm with a new \\textit{hybrid} regret bound that implies logarithmic regret in the stochastic regime and multiple data-dependent regret bounds in the adversarial regime, including bounds dependent on cumulative loss, total variation, and loss-sequence path-length. Both our proposed algorithm and the Tsallis-INF algorithm are based on a follow-the-regularized-leader (FTRL) framework with a time-varying regularizer. The analyses in this paper rely on \\textit{skewed Bregman divergence}, which provides simple expressions of regret bounds for FTRL with a time-varying regularizer",
    "volume": "main",
    "checked": true,
    "id": "8cbadd05e9e01cdf974881b31de2e9c1a3ddc88a",
    "citation_count": 12
  },
  "https://proceedings.mlr.press/v134/jin21a.html": {
    "title": "Double Explore-then-Commit: Asymptotic Optimality and Beyond",
    "abstract": "We study the multi-armed bandit problem with subGaussian rewards. The explore-then-commit (ETC) strategy, which consists of an exploration phase followed by an exploitation phase, is one of the most widely used algorithms in a variety of online decision applications. Nevertheless, it has been shown in \\cite{garivier2016explore} that ETC is suboptimal in the asymptotic sense as the horizon grows, and thus, is worse than fully sequential strategies such as Upper Confidence Bound (UCB). In this paper, we show that a variant of ETC algorithm can actually achieve the asymptotic optimality for multi-armed bandit problems as UCB-type algorithms do and extend it to the batched bandit setting. Specifically, we propose a double explore-then-commit (DETC) algorithm that has two exploration and exploitation phases and proves that DETC achieves the asymptotically optimal regret bound. To our knowledge, DETC is the first non-fully-sequential algorithm that achieves such asymptotic optimality. In addition, we extend DETC to batched bandit problems, where (i) the exploration process is split into a small number of batches and (ii) the round complexity is of central interest. We prove that a batched version of DETC can achieve the asymptotic optimality with only a constant round complexity. This is the first batched bandit algorithm that can attain the optimal asymptotic regret bound and optimal round complexity simultaneously",
    "volume": "main",
    "checked": true,
    "id": "191c72a3d9610e3cd69cffddb33c8bcda526c165",
    "citation_count": 7
  },
  "https://proceedings.mlr.press/v134/jung21a.html": {
    "title": "Moment Multicalibration for Uncertainty Estimation",
    "abstract": "We show how to achieve the notion of \"multicalibration\" from Hebert-Johnson et al. (2018) not just for means, but also for variances and other higher moments.  Informally, this means that we can find regression functions which, given a data point, can make point predictions not just for the expectation of its label, but for higher moments of its label distribution as well—and those predictions match the true distribution quantities when averaged not just over the population as a whole, but also when averaged over an enormous number of finely defined subgroups. It yields a principled way to estimate the uncertainty of predictions on many different subgroups—and to diagnose potential sources of unfairness in the predictive power of features across subgroups. As an application, we show that our moment estimates can be used to derive marginal prediction intervals that are simultaneously valid as averaged over all of the (sufficiently large) subgroups for which moment multicalibration has been obtained",
    "volume": "main",
    "checked": true,
    "id": "e582bdeb9f2614c1f8f26cf917081712af65d65a",
    "citation_count": 35
  },
  "https://proceedings.mlr.press/v134/kacham21a.html": {
    "title": "Reduced-Rank Regression with Operator Norm Error",
    "abstract": "A common data analysis task is the reduced-rank regression problem: $$\\min_{\\textrm{rank-}k  X} \\|AX-B\\|,$$ where $A \\in \\mathbb{R}^{n \\times c}$ and $B \\in \\mathbb{R}^{n \\times d}$ are given large matrices and $\\|\\cdot\\|$ is some norm. Here the unknown matrix $X \\in \\mathbb{R}^{c \\times d}$ is constrained to be of rank $k$ as it results in a significant parameter reduction of the solution when $c$ and $d$ are large. In the case of Frobenius norm error, there is a standard closed form solution to this problem and a fast algorithm to find a $(1+\\varepsilon)$-approximate solution. However, for the important case of operator norm error, no closed form solution is known and the fastest known algorithms take singular value decomposition time. We give the first randomized algorithms for this problem running in time $$(nnz{(A)} + nnz{(B)} + c^2) \\cdot k/\\varepsilon^{1.5} + (n+d)k^2/\\epsilon + c^{\\omega},$$ up to a polylogarithmic factor involving condition numbers, matrix dimensions, and dependence on $1/\\varepsilon$. Here $nnz{(M)}$ denotes the number of non-zero entries of a matrix $M$, and $\\omega$ is the exponent of matrix multiplication. As both (1) spectral low rank approximation ($A = B$) and (2) linear system solving ($n = c$ and $d = 1$) are special cases, our time cannot be improved by more than a $1/\\varepsilon$ factor (up to polylogarithmic factors) without a major breakthrough in linear algebra. Interestingly, known techniques for low rank approximation, such as alternating minimization or sketch-and-solve, provably fail for this problem. Instead, our algorithm uses an existential characterization of a solution, together with Krylov methods, low degree polynomial approximation, and sketching-based preconditioning",
    "volume": "main",
    "checked": true,
    "id": "e2f35b93db34057d3c0dd3e992edf46591aa656a",
    "citation_count": 0
  },
  "https://proceedings.mlr.press/v134/kairouz21a.html": {
    "title": "(Nearly) Dimension Independent Private ERM with AdaGrad Rates\\{via Publicly Estimated Subspaces",
    "abstract": "We revisit the problem of empirical risk minimziation (ERM) with differential privacy. We show that noisy AdaGrad, given appropriate knowledge and conditions on the subspace from which gradients can be drawn, achieves a regret comparable to traditional AdaGrad plus a well-controlled term due to noise. We show a convergence rate of $O(\\tr(G_T)/T)$, where $G_T$ captures the geometry of the gradient subspace. Since $\\tr(G_T)=O(\\sqrt{T})$ we can obtain faster rates for convex and Lipschitz functions, compared to the $O(1/\\sqrt{T})$ rate achieved by known versions of noisy (stochastic) gradient descent with comparable noise variance. In particular, we show that if the gradients lie in a known constant rank subspace, and assuming algorithmic access to an envelope which bounds decaying sensitivity, one can achieve faster convergence to an excess empirical risk of $\\tilde O(1/\\epsilon n)$, where $\\epsilon$ is the privacy budget and $n$ the number of samples. Letting $p$ be the problem dimension, this result implies that, by running noisy Adagrad, we can bypass the DP-SGD bound $\\tilde O(\\sqrt{p}/\\epsilon n)$ in $T=(\\epsilon n)^{2/(1+2\\alpha)}$ iterations, where $\\alpha \\geq 0$ is a parameter controlling gradient norm decay, instead of the rate achieved by SGD of $T=\\epsilon^2n^2$. Our results operate with general convex functions in both constrained and unconstrained minimization.  Along the way, we do a perturbation analysis of noisy AdaGrad, which is of independent interest. Our utility guarantee for the private ERM problem follows as a corollary to the regret guarantee of noisy AdaGrad",
    "volume": "main",
    "checked": false,
    "id": "8ad140c5e2afe33b7faeb1e169c01204435f8a1f",
    "citation_count": 11
  },
  "https://proceedings.mlr.press/v134/kaplan21a.html": {
    "title": "The Sparse Vector Technique, Revisited",
    "abstract": "We revisit one of the most basic and widely applicable techniques in the literature of differential privacy – the sparse vector technique [Dwork et al., STOC 2009]. This simple algorithm privately tests whether the value of a given query on a database is close to what we expect it to be. It allows to ask an unbounded number of queries as long as the answer is close to what we expect, and halts following the first query for which this is not the case.  We suggest an alternative, equally simple, algorithm that can continue testing queries as long as any single individual does not contribute to the answer of too many queries whose answer deviates substantially form what we expect. Our analysis is subtle and some of its ingredients may be more widely applicable.  In some cases our new algorithm allows to privately extract much more information from the database than the original.  We demonstrate this by applying our algorithm to the shifting-heavy-hitters problem: On every time step, each of n users gets a new input, and the task is to privately identify all the current heavy-hitters. That is, on time step i, the goal is to identify all data elements x such that many of the users have x as their current input. We present an algorithm for this problem with improved error guarantees over what can be obtained using existing techniques. Specifically, the error of our algorithm depends on the maximal number of times that a single user holds a heavy-hitter as input, rather than the total number of times in which a heavy-hitter exists",
    "volume": "main",
    "checked": true,
    "id": "6e70f024d81f516536ebc4713022fb3315c1985a",
    "citation_count": 5
  },
  "https://proceedings.mlr.press/v134/kirschner21a.html": {
    "title": "Asymptotically Optimal Information-Directed Sampling",
    "abstract": "We introduce a simple and efficient algorithm for stochastic linear bandits with finitely many actions that is asymptotically optimal and (nearly) worst-case optimal in finite time. The approach is based on the frequentist information-directed sampling (IDS) framework, with a surrogate for the information gain that is informed by the optimization problem that defines the asymptotic lower bound. Our analysis sheds light on how IDS balances the trade-off between regret and information and uncovers a surprising connection between the recently proposed primal-dual methods and the IDS algorithm. We demonstrate empirically that IDS is competitive with UCB in finite-time, and can be significantly better in the asymptotic regime",
    "volume": "main",
    "checked": true,
    "id": "071903c1feec7fb584cd1ada4e9973d5ce67c899",
    "citation_count": 19
  },
  "https://proceedings.mlr.press/v134/kunisky21a.html": {
    "title": "Hypothesis testing with low-degree polynomials in the Morris class of exponential families",
    "abstract": "Analysis of low-degree polynomial algorithms is a powerful, newly-popular method for predicting computational thresholds in hypothesis testing problems. One limitation of current techniques for this analysis is their restriction to Bernoulli and Gaussian distributions. We expand this range of possibilities by performing the low-degree analysis of hypothesis testing for the Morris class of natural exponential families with quadratic variance function, giving a unified treatment of Gaussian, Poisson, gamma (including exponential and chi-squared), binomial (including Bernoulli), negative binomial (including geometric), and generalized hyperbolic secant distributions. We then give several algorithmic applications.  1. In models where a random signal is observed through coordinatewise-independent noise applied in an exponential family, the success or failure of low-degree polynomials is governed by the z-score overlap, the inner product of z-score vectors with respect to the null distribution of two independent copies of the signal.  2. In the same models, testing with low-degree polynomials exhibits channel monotonicity: the above distributions admit a total ordering by computational cost of hypothesis testing, according to a scalar parameter describing how the variance depends on the mean in an exponential family.  3. In a spiked matrix model with a particular non-Gaussian noise distribution, the low-degree prediction is incorrect unless polynomials with arbitrarily large degree in individual matrix entries are permitted. This shows that polynomials summing over self-avoiding walks and variants thereof, as proposed recently by Ding, Hopkins, and Steurer (2020) for spiked matrix models with heavy-tailed noise, are strictly suboptimal for this model. Thus low-degree polynomials appear to offer a tradeoff between robustness and strong performance fine-tuned to specific models. Inspired by this, we suggest that a class of problems requiring \"exploration before inference,\" where an algorithm must first examine the input and then use some intermediate computation to choose a suitable inference subroutine, appears especially difficult for low-degree polynomials",
    "volume": "main",
    "checked": true,
    "id": "9cee36757cbf6e28e5aa5e92816e5dfb3a8bc34c",
    "citation_count": 2
  },
  "https://proceedings.mlr.press/v134/kur21a.html": {
    "title": "On the Minimal Error of Empirical Risk Minimization",
    "abstract": "We study the minimal squared error of the Empirical Risk Minimization (ERM) procedure in the task of regression, both in random and fixed design settings. Our sharp lower bounds shed light on the possibility (or impossibility) of adapting to simplicity of the model generating the data. In the fixed design setting, we show that the error is governed by the global complexity of the entire class. In contrast, in random design, ERM may only adapt to simpler models if the local neighborhoods around the regression function are nearly as complex as the class itself, a somewhat counter-intuitive conclusion. We provide sharp lower bounds for performance of ERM in Donsker and non-Donsker classes. We also discuss our results through the lens of recent studies on interpolation in overparameterized models",
    "volume": "main",
    "checked": true,
    "id": "1a735dedd55763e152295dcff6904cb2d40de951",
    "citation_count": 2
  }
}