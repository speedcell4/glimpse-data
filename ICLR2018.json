{
  "https://openreview.net/forum?id=Hkbd5xZRb": {
    "title": "Spherical CNNs",
    "volume": "oral",
    "abstract": "Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective. In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression",
    "checked": true,
    "id": "ce2845cadc5233ff0a647aa22ae3bbe646258890",
    "semantic_title": "spherical cnns",
    "citation_count": 901,
    "authors": []
  },
  "https://openreview.net/forum?id=rJTutzbA-": {
    "title": "On the insufficiency of existing momentum schemes for Stochastic Optimization",
    "volume": "oral",
    "abstract": "Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Rigorously speaking, fast gradient methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there exist simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG's practical performance gains are a by-product of minibatching. Furthermore, this work provides a viable (and provable) alternative, which, on the same set of problem instances, significantly improves over HB, NAG, and SGD's performance. This algorithm, referred to as Accelerated Stochastic Gradient Descent (ASGD), is a simple to implement stochastic algorithm, based on a relatively less popular variant of Nesterov's Acceleration. Extensive empirical results in this paper show that ASGD has performance gains over HB, NAG, and SGD. The code for implementing the ASGD Algorithm can be found at https://github.com/rahulkidambi/AccSGD",
    "checked": true,
    "id": "a99f6f195d6cbc33a6259206c10d4ab0e167f969",
    "semantic_title": "on the insufficiency of existing momentum schemes for stochastic optimization",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk6kPgZA-": {
    "title": "Certifying Some Distributional Robustness with Principled Adversarial Training",
    "volume": "oral",
    "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches",
    "checked": true,
    "id": "818c52f4ba56cb8cf152ad614f2f4803057a5cfe",
    "semantic_title": "certifying some distributional robustness with principled adversarial training",
    "citation_count": 862,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwZSG-CZ": {
    "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
    "volume": "oral",
    "abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity",
    "checked": true,
    "id": "ef9ddbc35676ce8ffc2a8067044473727839dbac",
    "semantic_title": "breaking the softmax bottleneck: a high-rank rnn language model",
    "citation_count": 369,
    "authors": []
  },
  "https://openreview.net/forum?id=S1JHhv6TW": {
    "title": "Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions",
    "volume": "oral",
    "abstract": "The driving force behind deep networks is their ability to compactly represent rich classes of functions. The primary notion for formally reasoning about this phenomenon is expressive efficiency, which refers to a situation where one network must grow unfeasibly large in order to replicate functions of another. To date, expressive efficiency analyses focused on the architectural feature of depth, showing that deep networks are representationally superior to shallow ones. In this paper we study the expressive efficiency brought forth by connectivity, motivated by the observation that modern networks interconnect their layers in elaborate ways. We focus on dilated convolutional networks, a family of deep models delivering state of the art performance in sequence processing tasks. By introducing and analyzing the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that even a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not. Empirical evaluation demonstrates how the expressive efficiency of connectivity, similarly to that of depth, translates into gains in accuracy. This leads us to believe that expressive efficiency may serve a key role in developing new tools for deep network design",
    "checked": true,
    "id": "9593d6e790065c972668da9ee0ae6614bf63ca18",
    "semantic_title": "boosting dilated convolutional networks with mixed tensor decompositions",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=Hy7fDog0b": {
    "title": "AmbientGAN: Generative models from lossy measurements",
    "volume": "oral",
    "abstract": "Generative models provide a way to model structure in complex distributions and have been shown to be useful for many tasks of practical interest. However, current techniques for training generative models require access to fully-observed samples. In many settings, it is expensive or even impossible to obtain fully-observed samples, but economical to obtain partial, noisy observations. We consider the task of learning an implicit generative model given only lossy measurements of samples from the distribution of interest. We show that the true underlying distribution can be provably recovered even in the presence of per-sample information loss for a class of measurement models. Based on this, we propose a new method of training Generative Adversarial Networks (GANs) which we call AmbientGAN. On three benchmark datasets, and for various measurement models, we demonstrate substantial qualitative and quantitative improvements. Generative models trained with our method can obtain $2$-$4$x higher inception scores than the baselines",
    "checked": true,
    "id": "b959d5655a3b2f92c2c1a8a7896fecafafea979d",
    "semantic_title": "ambientgan: generative models from lossy measurements",
    "citation_count": 201,
    "authors": []
  },
  "https://openreview.net/forum?id=rkRwGg-0Z": {
    "title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs",
    "volume": "oral",
    "abstract": "The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), an interpretation algorithm for analysing individual predictions made by standard LSTMs, without any changes to the underlying model. By decomposing the output of a LSTM, CD captures the contributions of combinations of words or variables to the final prediction of an LSTM. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done",
    "checked": true,
    "id": "cb6866b5fa62ae3cbe21bafd772bcce7d9668dd6",
    "semantic_title": "beyond word importance: contextual decomposition to extract interactions from lstms",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=S1CChZ-CZ": {
    "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning",
    "volume": "oral",
    "abstract": "We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming",
    "checked": true,
    "id": "acbd6e09ad888e68472a5c1cb6ec83176c7969bd",
    "semantic_title": "ask the right questions: active question reformulation with reinforcement learning",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=H1tSsb-AW": {
    "title": "Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines",
    "volume": "oral",
    "abstract": "Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks",
    "checked": true,
    "id": "1bdcf6fe02ed2ff097e5f4ffdc5af159cd9a713a",
    "semantic_title": "variance reduction for policy gradient with action-dependent factorized baselines",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ8vJebC-": {
    "title": "Synthetic and Natural Noise Both Break Neural Machine Translation",
    "volume": "oral",
    "abstract": "Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise",
    "checked": true,
    "id": "765bdcf27ebc1eb03a14f1e47aefa4dda1e03073",
    "semantic_title": "synthetic and natural noise both break neural machine translation",
    "citation_count": 741,
    "authors": []
  },
  "https://openreview.net/forum?id=HkL7n1-0b": {
    "title": "Wasserstein Auto-Encoders",
    "volume": "oral",
    "abstract": "We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality",
    "checked": true,
    "id": "6745c95b88ff9b12401a9ba6f4007f036be591a0",
    "semantic_title": "wasserstein auto-encoders",
    "citation_count": 1055,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk99zCeAb": {
    "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
    "volume": "oral",
    "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset",
    "checked": true,
    "id": "744fe47157477235032f7bb3777800f9f2f45e52",
    "semantic_title": "progressive growing of gans for improved quality, stability, and variation",
    "citation_count": 7339,
    "authors": []
  },
  "https://openreview.net/forum?id=ryQu7f-RZ": {
    "title": "On the Convergence of Adam and Beyond",
    "volume": "oral",
    "abstract": "Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance",
    "checked": true,
    "id": "c983653841b6987d9959318f074a595783838576",
    "semantic_title": "on the convergence of adam and beyond",
    "citation_count": 2494,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGXzmspb": {
    "title": "Training and Inference with Integers in Deep Neural Networks",
    "volume": "oral",
    "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands",
    "checked": true,
    "id": "acdf151b8efc2c6b05662d69f27531afc557dc85",
    "semantic_title": "training and inference with integers in deep neural networks",
    "citation_count": 390,
    "authors": []
  },
  "https://openreview.net/forum?id=BkisuzWRW": {
    "title": "Zero-Shot Visual Imitation",
    "volume": "oral",
    "abstract": "The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/",
    "checked": true,
    "id": "9217c4aa5a95d3209d2071e6889c8dd4b7d9309e",
    "semantic_title": "zero-shot visual imitation",
    "citation_count": 299,
    "authors": []
  },
  "https://openreview.net/forum?id=BJOFETxR-": {
    "title": "Learning to Represent Programs with Graphs",
    "volume": "oral",
    "abstract": "Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk2aImxAb": {
    "title": "Multi-Scale Dense Networks for Resource Efficient Image Classification",
    "volume": "oral",
    "abstract": "In this paper we investigate image classification with computational resource limits at test time. Two such settings are: 1. anytime classification, where the network's prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classification, where a fixed amount of computation is available to classify a set of examples that can be spent unevenly across \"easier\" and \"harder\" inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on convolutional neural networks. We train multiple classifiers with varying resource demands, which we adaptively apply during test time. To maximally re-use computation between the classifiers, we incorporate them as early-exits into a single deep convolutional neural network and inter-connect them with dense connectivity. To facilitate high quality classification early on, we use a two-dimensional multi-scale network architecture that maintains coarse and fine level features all-throughout the network. Experiments on three image-classification tasks demonstrate that our framework substantially improves the existing state-of-the-art in both settings",
    "checked": true,
    "id": "125ccd810f43f1cba83c6681836d000f83d1886d",
    "semantic_title": "multi-scale dense networks for resource efficient image classification",
    "citation_count": 763,
    "authors": []
  },
  "https://openreview.net/forum?id=HkfXMz-Ab": {
    "title": "Neural Sketch Learning for Conditional Program Generation",
    "volume": "oral",
    "abstract": "We study the problem of generating source code in a strongly typed, Java-like programming language, given a label (for example a set of API calls or types) carrying a small amount of information about the code that is desired. The generated programs are expected to respect a `\"realistic\" relationship between programs and labels, as exemplified by a corpus of labeled programs available during training. Two challenges in such *conditional program generation* are that the generated programs must satisfy a rich set of syntactic and semantic constraints, and that source code contains many low-level features that impede learning. We address these problems by training a neural generator not on code but on *program sketches*, or models of program syntax that abstract out names and operations that do not generalize across programs. During generation, we infer a posterior distribution over sketches, then concretize samples from this distribution into type-safe programs using combinatorial techniques. We implement our ideas in a system for generating API-heavy Java code, and show that it can often predict the entire body of a method given just a few API calls or data types that appear in the method",
    "checked": true,
    "id": "d11777ca327c6d91de34d1d2ac50316b905578b2",
    "semantic_title": "neural sketch learning for conditional program generation",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk2u1g-0-": {
    "title": "Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments",
    "volume": "oral",
    "abstract": "Ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest",
    "checked": true,
    "id": "6ac5eb309dd937d801d180c830377e4d551699a2",
    "semantic_title": "continuous adaptation via meta-learning in nonstationary and competitive environments",
    "citation_count": 353,
    "authors": []
  },
  "https://openreview.net/forum?id=HktK4BeCZ": {
    "title": "Learning Deep Mean Field Games for Modeling Large Population Behavior",
    "volume": "oral",
    "abstract": "We consider the problem of representing collective behavior of large populations and predicting the evolution of a population distribution over a discrete state space. A discrete time mean field game (MFG) is motivated as an interpretable model founded on game theory for understanding the aggregate effect of individual actions and predicting the temporal evolution of population distributions. We achieve a synthesis of MFG and Markov decision processes (MDP) by showing that a special MFG is reducible to an MDP. This enables us to broaden the scope of mean field game theory and infer MFG models of large real-world systems via deep inverse reinforcement learning. Our method learns both the reward function and forward dynamics of an MFG from real data, and we report the first empirical test of a mean field game model of a real-world social media population",
    "checked": true,
    "id": "6aad7ac79c221b0b865b694ef9d05d9c91db1650",
    "semantic_title": "learning deep mean field games for modeling large population behavior",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=B1QRgziT-": {
    "title": "Spectral Normalization for Generative Adversarial Networks",
    "volume": "oral",
    "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques",
    "checked": true,
    "id": "84de7d27e2f6160f634a483e8548c499a2cda7fa",
    "semantic_title": "spectral normalization for generative adversarial networks",
    "citation_count": 4433,
    "authors": []
  },
  "https://openreview.net/forum?id=B1gJ1L2aW": {
    "title": "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality",
    "volume": "oral",
    "abstract": "Deep Neural Networks (DNNs) have recently been shown to be vulnerable against adversarial examples, which are carefully crafted instances that can mislead DNNs to make errors during prediction. To better understand such attacks, a characterization is needed of the properties of regions (the so-called `adversarial subspaces') in which adversarial examples lie. We tackle this challenge by characterizing the dimensional properties of adversarial regions, via the use of Local Intrinsic Dimensionality (LID). LID assesses the space-filling capability of the region surrounding a reference example, based on the distance distribution of the example to its neighbors. We first provide explanations about how adversarial perturbation can affect the LID characteristic of adversarial regions, and then show empirically that LID characteristics can facilitate the distinction of adversarial examples generated using state-of-the-art attacks. As a proof-of-concept, we show that a potential application of LID is to distinguish adversarial examples, and the preliminary results show that it can outperform several state-of-the-art detection measures by large margins for five attack strategies considered in this paper across three benchmark datasets. Our analysis of the LID characteristic for adversarial regions not only motivates new directions of effective adversarial defense, but also opens up more challenges for developing new attacks to better understand the vulnerabilities of DNNs",
    "checked": true,
    "id": "a18ada04d93981178234d9c8907fb99ea92fddcb",
    "semantic_title": "characterizing adversarial subspaces using local intrinsic dimensionality",
    "citation_count": 738,
    "authors": []
  },
  "https://openreview.net/forum?id=HJGv1Z-AW": {
    "title": "Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input",
    "volume": "oral",
    "abstract": "The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured",
    "checked": true,
    "id": "c0617490e0ea57b28b89d4ca92e069f538609603",
    "semantic_title": "emergence of linguistic communication from referential games with symbolic and pixel input",
    "citation_count": 216,
    "authors": []
  },
  "https://openreview.net/forum?id=B18WgG-CZ": {
    "title": "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning",
    "volume": "poster",
    "abstract": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner. These representations are typically used as general purpose features for words across a range of NLP problems. However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem. Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations. In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model. We train this model on several data sources with multiple training objectives on over 100 million sentences. Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods. We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations",
    "checked": true,
    "id": "afc2850945a871e72c245818f9bc141bd659b453",
    "semantic_title": "learning general purpose distributed sentence representations via large scale multi-task learning",
    "citation_count": 330,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ_UL-k0b": {
    "title": "Recasting Gradient-Based Meta-Learning as Hierarchical Bayes",
    "volume": "poster",
    "abstract": "Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJNpifWAb": {
    "title": "Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches",
    "volume": "poster",
    "abstract": "Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. We find significant speedups in training neural networks with multiplicative Gaussian perturbations. We show that flipout is effective at regularizing LSTMs, and outperforms previous methods. Flipout also enables us to vectorize evolution strategies: in our experiments, a single GPU with flipout can handle the same throughput as at least 40 CPU cores using existing methods, equivalent to a factor-of-4 cost reduction on Amazon Web Services",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ry1arUgCW": {
    "title": "DORA The Explorer: Directed Outreaching Reinforcement Action-Selection",
    "volume": "poster",
    "abstract": "Exploration is a fundamental aspect of Reinforcement Learning, typically implemented using stochastic action-selection. Exploration, however, can be more efficient if directed toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality. While there are a few model-based solutions to this shortcoming, a model-free approach is still missing. We propose $E$-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using $E$-values improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to efficiently learn continuous MDPs. We demonstrate this by showing that our approach surpasses state of the art performance in the Freeway Atari 2600 game",
    "checked": true,
    "id": "89e9c05ea3b45628beba8134fd5c873dd55003e8",
    "semantic_title": "dora the explorer: directed outreaching reinforcement action-selection",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=SyELrEeAb": {
    "title": "Implicit Causal Models for Genome-wide Association Studies",
    "volume": "poster",
    "abstract": "Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases. In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes? How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships? To address these challenges, we synthesize ideas from causality and modern probabilistic modeling. For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density. For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples. In experiments, we scale Bayesian inference on up to a billion genetic measurements. We achieve state of the art accuracy for identifying causal factors: we significantly outperform the second best result by an absolute difference of 15-45.3%",
    "checked": true,
    "id": "83cffda7d9b47d0927d03fdc574a019487a3d5d8",
    "semantic_title": "implicit causal models for genome-wide association studies",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=HJtEm4p6Z": {
    "title": "Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
    "volume": "poster",
    "abstract": "We present Deep Voice 3, a fully-convolutional attention-based neural text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training an order of magnitude faster. We scale Deep Voice 3 to dataset sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on a single GPU server",
    "checked": true,
    "id": "feca3f41b5b4cc13368d53f3168cc55a2420ec16",
    "semantic_title": "deep voice 3: scaling text-to-speech with convolutional sequence learning",
    "citation_count": 456,
    "authors": []
  },
  "https://openreview.net/forum?id=ryBnUWb0b": {
    "title": "Predicting Floor-Level for 911 Calls with Neural Networks and Smartphone Sensor Data",
    "volume": "poster",
    "abstract": "In cities with tall buildings, emergency responders need an accurate floor level location to find 911 callers quickly. We introduce a system to estimate a victim's floor level via their mobile device's sensor data in a two-step process. First, we train a neural network to determine when a smartphone enters or exits a building via GPS signal changes. Second, we use a barometer equipped smartphone to measure the change in barometric pressure from the entrance of the building to the victim's indoor location. Unlike impractical previous approaches, our system is the first that does not require the use of beacons, prior knowledge of the building infrastructure, or knowledge of user behavior. We demonstrate real-world feasibility through 63 experiments across five different tall buildings throughout New York City where our system predicted the correct floor level with 100% accuracy",
    "checked": true,
    "id": "83f286c386cb5572e572a03bc3297b3cf2e52827",
    "semantic_title": "predicting floor-level for 911 calls with neural networks and smartphone sensor data",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=B1jscMbAW": {
    "title": "Divide and Conquer Networks",
    "volume": "poster",
    "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. Rather than studying this as a black-box discrete regression problem with no assumption whatsoever on the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. This principle creates a powerful inductive bias that we leverage with neural architectures that are defined recursively and dynamically, by learning two scale- invariant atomic operations: how to split a given input into smaller sets, and how to merge two partially solved tasks into a larger partial solution. Our model can be trained in weakly supervised environments, namely by just observing input-output pairs, and in even weaker environments, using a non-differentiable reward signal. Moreover, thanks to the dynamic aspect of our architecture, we can incorporate the computational complexity as a regularization term that can be optimized by backpropagation. We demonstrate the flexibility and efficiency of the Divide- and-Conquer Network on several combinatorial and geometric tasks: convex hull, clustering, knapsack and euclidean TSP. Thanks to the dynamic programming nature of our model, we show significant improvements in terms of generalization error and computational complexity",
    "checked": true,
    "id": "5dd3e28170e4e0694e51f24d3859ff97a2314f54",
    "semantic_title": "divide and conquer networks",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Hko85plCW": {
    "title": "Monotonic Chunkwise Attention",
    "volume": "poster",
    "abstract": "Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of a model using an offline soft attention mechanism. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention-based model",
    "checked": true,
    "id": "f0afdccf2903039d202085a771953a171dfd57b1",
    "semantic_title": "monotonic chunkwise attention",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=HyH9lbZAW": {
    "title": "Variational Message Passing with Structured Inference Networks",
    "volume": "poster",
    "abstract": "Recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret. We propose a variational message-passing algorithm for variational inference in such models. We make three contributions. First, we propose structured inference networks that incorporate the structure of the graphical model in the inference network of variational auto-encoders (VAE). Second, we establish conditions under which such inference networks enable fast amortized inference similar to VAE. Finally, we derive a variational message passing algorithm to perform efficient natural-gradient inference while retaining the efficiency of the amortized inference. By simultaneously enabling structured, amortized, and natural-gradient inference for deep structured models, our method simplifies and generalizes existing methods",
    "checked": true,
    "id": "4a97e7280ac46a28900993d2cdf3db65ecd258ba",
    "semantic_title": "variational message passing with structured inference networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=HkuGJ3kCb": {
    "title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations",
    "volume": "poster",
    "abstract": "Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a {\\em very simple}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations {\\em even stronger}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and text classification) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ33wwxRb": {
    "title": "SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data",
    "volume": "poster",
    "abstract": "Neural networks exhibit good generalization behavior in the over-parameterized regime, where the number of network parameters exceeds the number of observations. Nonetheless, current generalization bounds for neural networks fail to explain this phenomenon. In an attempt to bridge this gap, we study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks. Specifically, we prove convergence rates of SGD to a global minimum and provide generalization guarantees for this global minimum that are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model. This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers",
    "checked": true,
    "id": "c923208fb1e917f55e38b7c9f94f219554f398fa",
    "semantic_title": "sgd learns over-parameterized networks that provably generalize on linearly separable data",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=B1mvVm-C-": {
    "title": "Universal Agent for Disentangling Environments and Tasks",
    "volume": "poster",
    "abstract": "Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task. Hence, both environment and task specific knowledge are entangled into one framework. However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes. Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units. The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task. The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOnmlWC-": {
    "title": "Policy Optimization by Genetic Distillation",
    "volume": "poster",
    "abstract": "Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency",
    "checked": true,
    "id": "dc1d45b529e6149bd49547fbca976a8de9847612",
    "semantic_title": "policy optimization by genetic distillation",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=SJzRZ-WCZ": {
    "title": "Latent Space Oddity: on the Curvature of Deep Generative Models",
    "volume": "poster",
    "abstract": "Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear \"generator\" function that maps latent points into the input space. The nonlinearity of the generator implies that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and we demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalizes to other deep generative models",
    "checked": true,
    "id": "9f696b7156716c978b62a92714e7038a99f7a53c",
    "semantic_title": "latent space oddity: on the curvature of deep generative models",
    "citation_count": 267,
    "authors": []
  },
  "https://openreview.net/forum?id=S1jBcueAb": {
    "title": "Depthwise Separable Convolutions for Neural Machine Translation",
    "volume": "poster",
    "abstract": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves better results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new super-separable convolution operation that further reduces the number of parameters and computational cost of the models",
    "checked": true,
    "id": "07c4fc48ad7b7d1a417b0bb72d0ae2d4efc5aa83",
    "semantic_title": "depthwise separable convolutions for neural machine translation",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=H1BLjgZCb": {
    "title": "Generating Natural Adversarial Examples",
    "volume": "poster",
    "abstract": "Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByrZyglCb": {
    "title": "Robustness of Classifiers to Universal Perturbations: A Geometric Perspective",
    "volume": "poster",
    "abstract": "Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we provide a quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exist shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties",
    "checked": true,
    "id": "03522e27e398f4f5fa5e86cb2ed7e8c86a7e9bf1",
    "semantic_title": "robustness of classifiers to universal perturbations: a geometric perspective",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=H1WgVz-AZ": {
    "title": "Learning Approximate Inference Networks for Structured Prediction",
    "volume": "poster",
    "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use neural network architectures to define energy functions that can capture arbitrary dependencies among parts of structured outputs. Prior work used gradient descent for inference, relaxing the structured output to a set of continuous variables and then optimizing the energy with respect to them. We replace this use of gradient descent with a neural network trained to approximate structured argmax inference. This \"inference network\" outputs continuous values that we treat as the output structure. We develop large-margin training criteria for joint training of the structured energy function and inference network. On multi-label classification we report speed-ups of 10-60x compared to (Belanger et al., 2017) while also improving accuracy. For sequence labeling with simple structured energies, our approach performs comparably to exact inference while being much faster at test time. We then demonstrate improved accuracy by augmenting the energy with a \"label language model\" that scores entire output label sequences, showing it can improve handling of long-distance dependencies in part-of-speech tagging. Finally, we show how inference networks can replace dynamic programming for test-time inference in conditional random fields, suggestive for their general use for fast inference in structured settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl63fZRb": {
    "title": "Parametrized Hierarchical Procedures for Neural Programming",
    "volume": "poster",
    "abstract": "Neural programs are highly accurate and structured policies that perform algorithmic tasks by controlling the behavior of a computation mechanism. Despite the potential to increase the interpretability and the compositionality of the behavior of artificial agents, it remains difficult to learn from demonstrations neural networks that represent computer programs. The main challenges that set algorithmic domains apart from other imitation learning domains are the need for high accuracy, the involvement of specific structures of data, and the extremely limited observability. To address these challenges, we propose to model programs as Parametrized Hierarchical Procedures (PHPs). A PHP is a sequence of conditional operations, using a program counter along with the observation to select between taking an elementary action, invoking another PHP as a sub-procedure, and returning to the caller. We develop an algorithm for training PHPs from a set of supervisor demonstrations, only some of which are annotated with the internal call structure, and apply it to efficient level-wise training of multi-level PHPs. We show in two benchmarks, NanoCraft and long-hand addition, that PHPs can learn neural programs more accurately from smaller amounts of both annotated and unannotated demonstrations",
    "checked": true,
    "id": "f2fe9ceeb9f2d93d917dc491b4ac3c08561c588f",
    "semantic_title": "parametrized hierarchical procedures for neural programming",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy-dQG-Rb": {
    "title": "Neural Speed Reading via Skim-RNN",
    "volume": "poster",
    "abstract": "Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives a significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models. In our experiments, we show that Skim-RNN can achieve significantly reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also shows that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs",
    "checked": true,
    "id": "1e4cfedd79a108d0d04cc498bb146e4dcd4b5f0a",
    "semantic_title": "neural speed reading via skim-rnn",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=ry8dvM-R-": {
    "title": "Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning",
    "volume": "poster",
    "abstract": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network – for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR100 (20 tasks) we obtain cross-stitch performance levels with an 85% average reduction in training time",
    "checked": true,
    "id": "fc9d5be5e3f14b4c4b145b6c4bd96a9182f39fd2",
    "semantic_title": "routing networks: adaptive selection of non-linear functions for multi-task learning",
    "citation_count": 244,
    "authors": []
  },
  "https://openreview.net/forum?id=SyzKd1bCW": {
    "title": "Backpropagation through the Void: Optimizing control variates for black-box gradient estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "682d194235ba3b573889836ba118502e8b525728",
    "semantic_title": "backpropagation through the void: optimizing control variates for black-box gradient estimation",
    "citation_count": 300,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ_wN01C-": {
    "title": "Deep Rewiring: Training very sparse deep networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ccee800244908d2960830967e70ead7dd8266f7a",
    "semantic_title": "deep rewiring: training very sparse deep networks",
    "citation_count": 277,
    "authors": []
  },
  "https://openreview.net/forum?id=H1-nGgWC-": {
    "title": "Gaussian Process Behaviour in Wide Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb350d3b03e9308ccbd131d3d45dd44e383e6227",
    "semantic_title": "gaussian process behaviour in wide deep neural networks",
    "citation_count": 557,
    "authors": []
  },
  "https://openreview.net/forum?id=B1e5ef-C-": {
    "title": "A Compressed Sensing View of Unsupervised Text Embeddings, Bag-of-n-Grams, and LSTMs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "86fa6a6533e1b979eebd05bd6fb336ebf1eb99b9",
    "semantic_title": "a compressed sensing view of unsupervised text embeddings, bag-of-n-grams, and lstms",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=S1J2ZyZ0Z": {
    "title": "Interpretable Counting for Visual Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0605a012aeeee9bef773812a533c4f3cb7fa5a5f",
    "semantic_title": "interpretable counting for visual question answering",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=rylSzl-R-": {
    "title": "On Unifying Deep Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1al7jg0b": {
    "title": "Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0f57fdef846f5d033063c061c24191762e543f18",
    "semantic_title": "overcoming catastrophic interference using conceptor-aided backpropagation",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=HyWrIgW0W": {
    "title": "Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "940912cfc9190f5cc79e3867060e543634b6b22e",
    "semantic_title": "stochastic gradient descent performs variational inference, converges to limit cycles for deep networks",
    "citation_count": 304,
    "authors": []
  },
  "https://openreview.net/forum?id=HyUNwulC-": {
    "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498",
    "semantic_title": "parallelizing linear recurrent neural nets over sequence length",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=HkgNdt26Z": {
    "title": "Distributed Fine-tuning of Language Models on Private Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "947b4752338a4fc44003acce97bccf99be49549a",
    "semantic_title": "distributed fine-tuning of language models on private data",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=rk49Mg-CW": {
    "title": "Stochastic Variational Video Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk9yuql0Z": {
    "title": "Mitigating Adversarial Effects Through Randomization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9a089c56eec68df722b2a5a52727143aacdc2532",
    "semantic_title": "mitigating adversarial effects through randomization",
    "citation_count": 1054,
    "authors": []
  },
  "https://openreview.net/forum?id=B12Js_yRb": {
    "title": "Learning to Count Objects in Natural Images for Visual Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "30a3eee5e9302108416f6234d739373dde68d373",
    "semantic_title": "learning to count objects in natural images for visual question answering",
    "citation_count": 207,
    "authors": []
  },
  "https://openreview.net/forum?id=SyyGPP0TZ": {
    "title": "Regularizing and Optimizing LSTM Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1zlp1bRW": {
    "title": "Large Scale Optimal Transport and Mapping Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1nQvfgA-": {
    "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d35534f3f59631951011539da2fe83f2844ca245",
    "semantic_title": "semantically decomposing the latent spaces of generative adversarial networks",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=Hy6GHpkCW": {
    "title": "A Neural Representation of Sketch Drawings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1p31z-Ab": {
    "title": "Deep contextualized word representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyOK1Sg0W": {
    "title": "Adaptive Quantization of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "accfaca7b884964582843a4cc9bb9dc0f3131528",
    "semantic_title": "adaptive quantization of neural networks",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=r1gs9JgRZ": {
    "title": "Mixed Precision Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e7fd6848cb29ca221a7e17d823e06fb566f1f135",
    "semantic_title": "mixed precision training",
    "citation_count": 1792,
    "authors": []
  },
  "https://openreview.net/forum?id=B1EA-M-0Z": {
    "title": "Deep Neural Networks as Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "075556dd42900a6bc4552a2f2531ba21b9b7b4c0",
    "semantic_title": "deep neural networks as gaussian processes",
    "citation_count": 1091,
    "authors": []
  },
  "https://openreview.net/forum?id=S19dR9x0b": {
    "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJk7Gf-CZ": {
    "title": "Global Optimality Conditions for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "414915126855cab6a30fdcec635d90e793137c3b",
    "semantic_title": "global optimality conditions for deep neural networks",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOfBggRZ": {
    "title": "Detecting Statistical Interactions from Neural Network Weights",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5f85a8eaa7a1a1686f5a2bf721c63e337f03d8eb",
    "semantic_title": "detecting statistical interactions from neural network weights",
    "citation_count": 192,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy21R9JAW": {
    "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "de01f74a899ecc7e3228cddbc743aaf6faf5e55f",
    "semantic_title": "towards better understanding of gradient-based attribution methods for deep neural networks",
    "citation_count": 883,
    "authors": []
  },
  "https://openreview.net/forum?id=BJNRFNlRW": {
    "title": "TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS: A LAGRANGIAN PERSPECTIVE ON GAN",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c837f6bb76af151145df50207992ed3848489bdb",
    "semantic_title": "training generative adversarial networks via primal-dual subgradient methods: a lagrangian perspective on gan",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=rkfOvGbCW": {
    "title": "Memory-based Parameter Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0be49527df4869a0132f5cbc8d4cfa3304ab5843",
    "semantic_title": "memory-based parameter adaptation",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=HyzbhfWRW": {
    "title": "Learn to Pay Attention",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c70218603f0af1be5d063056cbe629e042141a86",
    "semantic_title": "learn to pay attention",
    "citation_count": 439,
    "authors": []
  },
  "https://openreview.net/forum?id=SJi9WOeRb": {
    "title": "Gradient Estimators for Implicit Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkwHObbRZ": {
    "title": "Learning One-hidden-layer Neural Networks with Landscape Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ry6-G_66b": {
    "title": "Active Neural Localization",
    "volume": "poster",
    "abstract": "Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose \"Active Neural Localizer\", a fully differentiable neural network that learns to localize efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to minimize the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine",
    "checked": true,
    "id": "80f9f597992dc4f77911fa6a94a4611f5daafa5d",
    "semantic_title": "active neural localization",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=HkZy-bW0-": {
    "title": "Temporally Efficient Deep Learning with Spikes",
    "volume": "poster",
    "abstract": "The vast majority of natural sensory data is temporally redundant. For instance, video frames or audio samples which are sampled at nearby points in time tend to have similar values. Typically, deep learning algorithms take no advantage of this redundancy to reduce computations. This can be an obscene waste of energy. We present a variant on backpropagation for neural networks in which computation scales with the rate of change of the data - not the rate at which we process the data. We do this by implementing a form of Predictive Coding wherein neurons communicate a combination of their state, and their temporal change in state, and quantize this signal using Sigma-Delta modulation. Intriguingly, this simple communication rule give rise to units that resemble biologically-inspired leaky integrate-and-fire neurons, and to a spike-timing-dependent weight-update similar to Spike-Timing Dependent Plasticity (STDP), a synaptic learning rule observed in the brain. We demonstrate that on MNIST, on a temporal variant of MNIST, and on Youtube-BB, a dataset with videos in the wild, our algorithm performs about as well as a standard deep network trained with backpropagation, despite only communicating discrete values between layers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1HlA-ZAZ": {
    "title": "The Kanerva Machine: A Generative Distributed Memory",
    "volume": "poster",
    "abstract": "We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train",
    "checked": true,
    "id": "5b2c83f41eacdf95e8b38300d2926ac37ea4709e",
    "semantic_title": "the kanerva machine: a generative distributed memory",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=SkZxCk-0Z": {
    "title": "Can Neural Networks Understand Logical Entailment?",
    "volume": "poster",
    "abstract": "We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a ``convolution over possible worlds''. Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks",
    "checked": true,
    "id": "bdea8b6ceabaeb86bd23c2d2585da1ff3858d968",
    "semantic_title": "can neural networks understand logical entailment?",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=SJLlmG-AZ": {
    "title": "Understanding image motion with group representations",
    "volume": "poster",
    "abstract": "Motion is an important signal for agents in dynamic environments, but learning to represent motion from unlabeled video is a difficult and underconstrained problem. We propose a model of motion based on elementary group properties of transformations and use it to train a representation of image motion. While most methods of estimating motion are based on pixel-level constraints, we use these group properties to constrain the abstract representation of motion itself. We demonstrate that a deep neural network trained using this method captures motion in both synthetic 2D sequences and real-world sequences of vehicle motion, without requiring any labels. Networks trained to respect these constraints implicitly identify the image characteristic of motion in different sequence types. In the context of vehicle motion, this method extracts information useful for localization, tracking, and odometry. Our results demonstrate that this representation is useful for learning motion in the general setting where explicit labels are difficult to obtain",
    "checked": true,
    "id": "869ef3835e5da24b4597a757948f1c0e19fa546d",
    "semantic_title": "understanding image motion with group representations",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=S1D8MPxA-": {
    "title": "Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio",
    "volume": "poster",
    "abstract": "Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy. Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources. Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists. In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed. In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm. The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process. Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate. Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet",
    "checked": true,
    "id": "ad3e968a38a5b2d1e72997f978c556a06f625e48",
    "semantic_title": "viterbi-based pruning for sparse matrix with fixed and high index compression ratio",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=SkFAWax0-": {
    "title": "VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop",
    "volume": "poster",
    "abstract": "We present a new neural text to speech (TTS) method that is able to transform text to speech in voices that are sampled in the wild. Unlike other systems, our solution is able to deal with unconstrained voice samples and without requiring aligned phonemes or linguistic features. The network architecture is simpler than those in the existing literature and is based on a novel shifting buffer working memory. The same buffer is used for estimating the attention, computing the output audio, and for updating the buffer itself. The input sentence is encoded using a context-free lookup table that contains one entry per character or phoneme. The speakers are similarly represented by a short vector that can also be fitted to new identities, even with only a few samples. Variability in the generated speech is achieved by priming the buffer prior to generating the audio. Experimental results on several datasets demonstrate convincing capabilities, making TTS accessible to a wider range of applications. In order to promote reproducibility, we release our source code and models",
    "checked": true,
    "id": "7e8f697f19c5d7d95f859e1ad6bcc219a902f52b",
    "semantic_title": "voiceloop: voice fitting and synthesis via a phonological loop",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=BJj6qGbRW": {
    "title": "Few-Shot Learning with Graph Neural Networks",
    "volume": "poster",
    "abstract": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on ‘relational' tasks",
    "checked": true,
    "id": "572a1f77306e160c3893299c18f3ed862fb5f6d9",
    "semantic_title": "few-shot learning with graph neural networks",
    "citation_count": 1239,
    "authors": []
  },
  "https://openreview.net/forum?id=BkXmYfbAZ": {
    "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering",
    "volume": "poster",
    "abstract": "Existing deep multitask learning (MTL) approaches align layers shared between tasks in a parallel ordering. Such an organization significantly constricts the types of shared structure that can be learned. The necessity of parallel ordering for deep MTL is first tested by comparing it with permuted ordering of shared layers. The results indicate that a flexible ordering can enable more effective sharing, thus motivating the development of a soft ordering approach, which learns how shared layers are applied in different ways for different tasks. Deep MTL with soft ordering outperforms parallel ordering methods across a series of domains. These results suggest that the power of deep MTL comes from learning highly general building blocks that can be assembled to meet the demands of each task",
    "checked": true,
    "id": "c4f3375dab1886f37f542d998e61d8c30a927682",
    "semantic_title": "beyond shared hierarchies: deep multitask learning through soft layer ordering",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=BJE-4xW0W": {
    "title": "CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training",
    "volume": "poster",
    "abstract": "We introduce causal implicit generative models (CiGMs): models that allow sampling from not only the true observational but also the true interventional distributions. We show that adversarial training can be used to learn a CiGM, if the generator architecture is structured based on a given causal graph. We consider the application of conditional and interventional sampling of face images with binary feature labels, such as mustache, young. We preserve the dependency structure between the labels with a given causal graph. We devise a two-stage procedure for learning a CiGM over the labels and the image. First we train a CiGM over the binary labels using a Wasserstein GAN where the generator neural network is consistent with the causal graph between the labels. Later, we combine this with a conditional GAN to generate images conditioned on the binary labels. We propose two new conditional GAN architectures: CausalGAN and CausalBEGAN. We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels. The conditional GAN combined with a trained CiGM for the labels is then a CiGM over the labels and the generated image. We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset",
    "checked": true,
    "id": "329f14d68f8e4cbc3ee5109e70c76c6f3b63a19a",
    "semantic_title": "causalgan: learning causal implicit generative models with adversarial training",
    "citation_count": 253,
    "authors": []
  },
  "https://openreview.net/forum?id=ryUlhzWCZ": {
    "title": "TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING & IMITATION LEARNING",
    "volume": "poster",
    "abstract": "In this paper, we propose to combine imitation and reinforcement learning via the idea of reward shaping using an oracle. We study the effectiveness of the near- optimal cost-to-go oracle on the planning horizon and demonstrate that the cost- to-go oracle shortens the learner's planning horizon as function of its accuracy: a globally optimal oracle can shorten the planning horizon to one, leading to a one- step greedy Markov Decision Process which is much easier to optimize, while an oracle that is far away from the optimality requires planning over a longer horizon to achieve near-optimal performance. Hence our new insight bridges the gap and interpolates between imitation learning and reinforcement learning. Motivated by the above mentioned insights, we propose Truncated HORizon Policy Search (THOR), a method that focuses on searching for policies that maximize the total reshaped reward over a finite planning horizon when the oracle is sub-optimal. We experimentally demonstrate that a gradient-based implementation of THOR can achieve superior performance compared to RL baselines and IL baselines even when the oracle is sub-optimal",
    "checked": true,
    "id": "d07fe76aa2ca2337c8aa6001c8cfe296fa824109",
    "semantic_title": "truncated horizon policy search: combining reinforcement learning & imitation learning",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=HkXWCMbRW": {
    "title": "Towards Image Understanding from Deep Compression Without Decoding",
    "volume": "poster",
    "abstract": "Motivated by recent work on deep neural network (DNN)-based image compression methods showing potential improvements in image quality, savings in storage, and bandwidth reduction, we propose to perform image understanding tasks such as classification and segmentation directly on the compressed representations produced by these compression methods. Since the encoders and decoders in DNN-based compression methods are neural networks with feature-maps as internal representations of the images, we directly integrate these with architectures for image understanding. This bypasses decoding of the compressed representation into RGB space and reduces computational cost. Our study shows that accuracies comparable to networks that operate on compressed RGB images can be achieved while reducing the computational complexity up to $2\\times$. Furthermore, we show that synergies are obtained by jointly training compression networks with classification networks on the compressed representations, improving image quality, classification accuracy, and segmentation performance. We find that inference from compressed representations is particularly advantageous compared to inference from compressed RGB images for aggressive compression rates",
    "checked": true,
    "id": "e5604c3f61eb7e8b80bf423f7828d8c1fa0f1d32",
    "semantic_title": "towards image understanding from deep compression without decoding",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Xw62kRZ": {
    "title": "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis",
    "volume": "poster",
    "abstract": "Program synthesis is the task of automatically generating a program consistent with a specification. Recent years have seen proposal of a number of neural approaches for program synthesis, many of which adopt a sequence generation paradigm similar to neural machine translation, in which sequence-to-sequence models are trained to maximize the likelihood of known reference programs. While achieving impressive results, this strategy has two key limitations. First, it ignores Program Aliasing: the fact that many different programs may satisfy a given specification (especially with incomplete specifications such as a few input-output examples). By maximizing the likelihood of only a single reference program, it penalizes many semantically correct programs, which can adversely affect the synthesizer performance. Second, this strategy overlooks the fact that programs have a strict syntax that can be efficiently checked. To address the first limitation, we perform reinforcement learning on top of a supervised model with an objective that explicitly maximizes the likelihood of generating semantically correct programs. For addressing the second limitation, we introduce a training procedure that directly maximizes the probability of generating syntactically correct programs that fulfill the specification. We show that our contributions lead to improved accuracy of the models, especially in cases where the training data is limited",
    "checked": true,
    "id": "dea6aeb514b1969ab879c793d46a0d2eceaa2cbf",
    "semantic_title": "leveraging grammar and reinforcement learning for neural program synthesis",
    "citation_count": 218,
    "authors": []
  },
  "https://openreview.net/forum?id=By4HsfWAZ": {
    "title": "Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge",
    "volume": "poster",
    "abstract": "We consider the use of Deep Learning methods for modeling complex phenomena like those occurring in natural physical processes. With the large amount of data gathered on these phenomena the data intensive paradigm could begin to challenge more traditional approaches elaborated over the years in fields like maths or physics. However, despite considerable successes in a variety of application domains, the machine learning field is not yet ready to handle the level of complexity required by such problems. Using an example application, namely Sea Surface Temperature Prediction, we show how general background knowledge gained from the physics could be used as a guideline for designing efficient Deep Learning models. In order to motivate the approach and to assess its generality we demonstrate a formal link between the solution of a class of differential equations underlying a large family of physical phenomena and the proposed model. Experiments and comparison with series of baselines including a state of the art numerical approach is then provided",
    "checked": true,
    "id": "2581b3e44592b3b3741474c8d6f483a90c29f139",
    "semantic_title": "deep learning for physical processes: incorporating prior scientific knowledge",
    "citation_count": 318,
    "authors": []
  },
  "https://openreview.net/forum?id=B17JTOe0-": {
    "title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization",
    "volume": "poster",
    "abstract": "Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Byt3oJ-0W": {
    "title": "Learning Latent Permutations with Gumbel-Sinkhorn Networks",
    "volume": "poster",
    "abstract": "Permutations and matchings are core building blocks in a variety of latent variable models, as they allow us to align, canonicalize, and sort data. Learning in such models is difficult, however, because exact marginalization over these combinatorial objects is intractable. In response, this paper introduces a collection of new methods for end-to-end learning in such models that approximate discrete maximum-weight matching using the continuous Sinkhorn operator. Sinkhorn iteration is attractive because it functions as a simple, easy-to-implement analog of the softmax operator. With this, we can define the Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method (Jang et al. 2016, Maddison2016 et al. 2016) to distributions over latent matchings. We demonstrate the effectiveness of our method by outperforming competitive baselines on a range of qualitatively different tasks: sorting numbers, solving jigsaw puzzles, and identifying neural signals in worms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJl3yM-Ab": {
    "title": "Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering",
    "volume": "poster",
    "abstract": "Very recently, it comes to be a popular approach for answering open-domain questions by first searching question-related passages, then applying reading comprehension models to extract answers. Existing works usually extract answers from single passages independently, thus not fully make use of the multiple searched passages, especially for the some questions requiring several evidences, which can appear in different passages, to be answered. The above observations raise the problem of evidence aggregation from multiple passages. In this paper, we deal with this problem as answer re-ranking. Specifically, based on the answer candidates generated from the existing state-of-the-art QA model, we propose two different re-ranking methods, strength-based and coverage-based re-rankers, which make use of the aggregated evidences from different passages to help entail the ground-truth answer for the question. Our model achieved state-of-the-arts on three public open-domain QA datasets, Quasar-T, SearchQA and the open-domain version of TriviaQA, with about 8\\% improvement on the former two datasets",
    "checked": true,
    "id": "8e46f7fbb96549cd1b3b0bd226f06a611126b889",
    "semantic_title": "evidence aggregation for answer re-ranking in open-domain question answering",
    "citation_count": 161,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ1Xmf-Rb": {
    "title": "FearNet: Brain-Inspired Model for Incremental Learning",
    "volume": "poster",
    "abstract": "Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks",
    "checked": true,
    "id": "7e77d37e7fbfe90f58e4e96e8198903f364d6402",
    "semantic_title": "fearnet: brain-inspired model for incremental learning",
    "citation_count": 476,
    "authors": []
  },
  "https://openreview.net/forum?id=H15odZ-C-": {
    "title": "Semantic Interpolation in Implicit Models",
    "volume": "poster",
    "abstract": "In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths. Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths, but they are also guaranteed to pass through high-density regions, irrespective of the dimensionality of the latent space. Experiments on standard benchmark image datasets demonstrate clear visual improvements in the quality of the generated samples and exhibit more meaningful interpolation paths",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkHVZWZAZ": {
    "title": "The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning",
    "volume": "poster",
    "abstract": "In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the β-leaveone-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training",
    "checked": true,
    "id": "ee1ac4a86cafa34e4905327f3436fea2d5994fc8",
    "semantic_title": "the reactor: a fast and sample-efficient actor-critic agent for reinforcement learning",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=H135uzZ0-": {
    "title": "Mixed Precision Training of Convolutional Neural Networks using Integer Operations",
    "volume": "poster",
    "abstract": "The state-of-the-art (SOTA) for mixed precision training is dominated by variants of low precision floating point operations, and in particular, FP16 accumulating into FP32 Micikevicius et al. (2017). On the other hand, while a lot of research has also happened in the domain of low and mixed-precision Integer training, these works either present results for non-SOTA networks (for instance only AlexNet for ImageNet-1K), or relatively small datasets (like CIFAR-10). In this work, we train state-of-the-art visual understanding neural networks on the ImageNet-1K dataset, with Integer operations on General Purpose (GP) hardware. In particular, we focus on Integer Fused-Multiply-and-Accumulate (FMA) operations which take two pairs of INT16 operands and accumulate results into an INT32 output.We propose a shared exponent representation of tensors and develop a Dynamic Fixed Point (DFP) scheme suitable for common neural network operations. The nuances of developing an efficient integer convolution kernel is examined, including methods to handle overflow of the INT32 accumulator. We implement CNN training for ResNet-50, GoogLeNet-v1, VGG-16 and AlexNet; and these networks achieve or exceed SOTA accuracy within the same number of iterations as their FP32 counterparts without any change in hyper-parameters and with a 1.8X improvement in end-to-end training throughput. To the best of our knowledge these results represent the first INT16 training results on GP hardware for ImageNet-1K dataset using SOTA CNNs and achieve highest reported accuracy using half precision",
    "checked": true,
    "id": "eae2eba75cc90990ff31d94d1942ffdea7c452b8",
    "semantic_title": "mixed precision training of convolutional neural networks using integer operations",
    "citation_count": 154,
    "authors": []
  },
  "https://openreview.net/forum?id=rJvJXZb0W": {
    "title": "An efficient framework for learning sentence representations",
    "volume": "poster",
    "abstract": "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time",
    "checked": true,
    "id": "ad31866da7f14ae21bd38df0a3b1ffd1a1438122",
    "semantic_title": "an efficient framework for learning sentence representations",
    "citation_count": 543,
    "authors": []
  },
  "https://openreview.net/forum?id=ry80wMW0W": {
    "title": "Hierarchical Subtask Discovery with Non-Negative Matrix Factorization",
    "volume": "poster",
    "abstract": "Hierarchical reinforcement learning methods offer a powerful means of planning flexible behavior in complicated domains. However, learning an appropriate hierarchical decomposition of a domain into subtasks remains a substantial challenge. We present a novel algorithm for subtask discovery, based on the recently introduced multitask linearly-solvable Markov decision process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by representing them as a linear combination of a previously learned basis set of tasks. In this setting, the subtask discovery problem can naturally be posed as finding an optimal low-rank approximation of the set of tasks the agent will face in a domain. We use non-negative matrix factorization to discover this minimal basis set of tasks, and show that the technique learns intuitive decompositions in a variety of domains. Our method has several qualitatively desirable features: it is not limited to learning subtasks with single goal states, instead learning distributed patterns of preferred states; it learns qualitatively different hierarchical decompositions in the same domain depending on the ensemble of tasks the agent will face; and it may be straightforwardly iterated to obtain deeper hierarchical decompositions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ_aoCyRZ": {
    "title": "SpectralNet: Spectral Clustering using Deep Neural Networks",
    "volume": "poster",
    "abstract": "Spectral clustering is a leading and popular technique in unsupervised data analysis. Two of its major limitations are scalability and generalization of the spectral embedding (i.e., out-of-sample-extension). In this paper we introduce a deep learning approach to spectral clustering that overcomes the above shortcomings. Our network, which we call SpectralNet, learns a map that embeds input data points into the eigenspace of their associated graph Laplacian matrix and subsequently clusters them. We train SpectralNet using a procedure that involves constrained stochastic optimization. Stochastic optimization allows it to scale to large datasets, while the constraints, which are implemented using a special purpose output layer, allow us to keep the network output orthogonal. Moreover, the map learned by SpectralNet naturally generalizes the spectral embedding to unseen data points. To further improve the quality of the clustering, we replace the standard pairwise Gaussian affinities with affinities leaned from unlabeled data using a Siamese network. Additional improvement can be achieved by applying the network to code representations produced, e.g., by standard autoencoders. Our end-to-end learning procedure is fully unsupervised. In addition, we apply VC dimension theory to derive a lower bound on the size of SpectralNet. State-of-the-art clustering results are reported for both the MNIST and Reuters datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1sUHgb0Z": {
    "title": "Learning From Noisy Singly-labeled Data",
    "volume": "poster",
    "abstract": "Supervised learning depends on annotated examples, which are taken to be the ground truth. But these labels often come from noisy crowdsourcing platforms, like Amazon Mechanical Turk. Practitioners typically collect multiple labels per example and aggregate the results to mitigate noise (the classic crowdsourcing problem). Given a fixed annotation budget and unlimited unlabeled data, redundant annotation comes at the expense of fewer labeled examples. This raises two fundamental questions: (1) How can we best learn from noisy workers? (2) How should we allocate our labeling budget to maximize the performance of a classifier? We propose a new algorithm for jointly modeling labels and worker quality from noisy crowd-sourced data. The alternating minimization proceeds in rounds, estimating worker quality from disagreement with the current model and then updating the model by optimizing a loss function that accounts for the current estimate of worker quality. Unlike previous approaches, even with only one annotation per example, our algorithm can estimate worker quality. We establish a generalization error bound for models learned with our algorithm and establish theoretically that it's better to label many examples once (vs less multiply) when worker quality exceeds a threshold. Experiments conducted on both ImageNet (with simulated noisy workers) and MS-COCO (using the real crowdsourced labels) confirm our algorithm's benefits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJvvRoe0W": {
    "title": "An image representation based convolutional network for DNA classification",
    "volume": "poster",
    "abstract": "The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that takes an image-representation of primary DNA sequence as its input, and predicts key determinants of chromatin structure. The method is developed such that it is capable of detecting interactions between distal elements in the DNA sequence, which are known to be highly relevant. Our experiments show that the method outperforms several existing methods both in terms of prediction accuracy and training time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1uxsye0Z": {
    "title": "Adaptive Dropout with Rademacher Complexity Regularization",
    "volume": "poster",
    "abstract": "We propose a novel framework to adaptively adjust the dropout rates for the deep neural network based on a Rademacher complexity bound. The state-of-the-art deep learning algorithms impose dropout strategy to prevent feature co-adaptation. However, choosing the dropout rates remains an art of heuristics or relies on empirical grid-search over some hyperparameter space. In this work, we show the network Rademacher complexity is bounded by a function related to the dropout rate vectors and the weight coefficient matrices. Subsequently, we impose this bound as a regularizer and provide a theoretical justified way to trade-off between model complexity and representation power. Therefore, the dropout rates and the empirical loss are unified into the same objective function, which is then optimized using the block coordinate descent algorithm. We discover that the adaptively adjusted dropout rates converge to some interesting distributions that reveal meaningful patterns.Experiments on the task of image and document classification also show our method achieves better performance compared to the state-of the-art dropout algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJQHjzZ0-": {
    "title": "Quantitatively Evaluating GANs With Divergences Proposed for Training",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) have been extremely effective in approximating complex distributions of high-dimensional, input data samples, and substantial progress has been made in understanding and improving GAN performance in terms of both theory and application. However, we currently lack quantitative methods for model assessment. Because of this, while many GAN variants being proposed, we have relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics and, interestingly, the test-time metrics do not favour networks that use the same training-time criterion. We also compare the proposed metrics to human perceptual scores",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryup8-WCW": {
    "title": "Measuring the Intrinsic Dimension of Objective Landscapes",
    "volume": "poster",
    "abstract": "Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk7KsfW0-": {
    "title": "Lifelong Learning with Dynamically Expandable Networks",
    "volume": "poster",
    "abstract": "We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efficiently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets in lifelong learning scenarios on multiple public datasets, on which it not only significantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch model with substantially fewer number of parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1vuO-bCW": {
    "title": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning",
    "volume": "poster",
    "abstract": "Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states",
    "checked": true,
    "id": "ec4df801c640169e18d8da1bdc65a0b6fc2d3d94",
    "semantic_title": "leave no trace: learning to reset for safe and autonomous reinforcement learning",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy8XvGb0-": {
    "title": "Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models",
    "volume": "poster",
    "abstract": "Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal \"realism\" constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1mCp-ZRZ": {
    "title": "Action-dependent Control Variates for Policy Optimization via Stein Identity",
    "volume": "poster",
    "abstract": "Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more flexible and general action-dependent baseline functions. Empirical studies show that our method essentially improves the sample efficiency of the state-of-the-art policy gradient approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1MczcgR-": {
    "title": "Understanding Short-Horizon Bias in Stochastic Meta-Optimization",
    "volume": "poster",
    "abstract": "Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias. We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons. We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area. We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkLhaGZRW": {
    "title": "Improving GAN Training via Binarized Representation Entropy (BRE) Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f074217d51ffb0da3b9716af4adae56215de488",
    "semantic_title": "improving gan training via binarized representation entropy (bre) regularization",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=rkZvSe-RZ": {
    "title": "Ensemble Adversarial Training: Attacks and Defenses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Syhr6pxCW": {
    "title": "PixelNN: Example-based Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "731f0d42b1679aa6c89693ff62d41f74f2519124",
    "semantic_title": "pixelnn: example-based image synthesis",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ0hF1Z0b": {
    "title": "Learning Differentially Private Recurrent Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed46493d568030b42f0154d9e5bf39bbd07962b3",
    "semantic_title": "learning differentially private recurrent language models",
    "citation_count": 1279,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Lc-Gb0Z": {
    "title": "Deep Learning as a Mixed Convex-Combinatorial Optimization Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkn7CBaTW": {
    "title": "Learning how to explain neural networks: PatternNet and PatternAttribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkQkBnJAb": {
    "title": "Improving GANs Using Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyZI0GWCZ": {
    "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJhIM0xAW": {
    "title": "Learning a neural response metric for retinal prosthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "440b22ccdc7990c65d948856f9fbf8ca762745ab",
    "semantic_title": "learning a neural response metric for retinal prosthesis",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SkT5Yg-RZ": {
    "title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8499a250422a3c66357367c8d5fa504de5424c59",
    "semantic_title": "intrinsic motivation and automatic curricula via asymmetric self-play",
    "citation_count": 337,
    "authors": []
  },
  "https://openreview.net/forum?id=rydeCEhs-": {
    "title": "SMASH: One-Shot Model Architecture Search through HyperNetworks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJWLfGWRb": {
    "title": "Matrix capsules with EM routing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "603caed9430283db6c7f43169555c8d18e97a281",
    "semantic_title": "matrix capsules with em routing",
    "citation_count": 1050,
    "authors": []
  },
  "https://openreview.net/forum?id=HyRVBzap-": {
    "title": "Cascade Adversarial Machine Learning Regularized with a Unified Embedding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJiHXGWAZ": {
    "title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkgOLb-0W": {
    "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1cbf097ce436f7304a1984f4a29ab41f75ebfe3",
    "semantic_title": "neural language modeling by jointly learning syntax and lexicon",
    "citation_count": 178,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ZvaaeAZ": {
    "title": "WRPN: Wide Reduced-Precision Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkMvEOlAb": {
    "title": "Learning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJyEH91A-": {
    "title": "Learning Wasserstein Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryTp3f-0-": {
    "title": "Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7dd8fc595afdd2097b43a5af9a8d9f5e97a65ec1",
    "semantic_title": "reinforcement learning on web interfaces using workflow-guided exploration",
    "citation_count": 215,
    "authors": []
  },
  "https://openreview.net/forum?id=HkAClQgA-": {
    "title": "A Deep Reinforced Model for Abstractive Summarization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
    "semantic_title": "a deep reinforced model for abstractive summarization",
    "citation_count": 1556,
    "authors": []
  },
  "https://openreview.net/forum?id=By-7dz-AZ": {
    "title": "A Framework for the Quantitative Evaluation of Disentangled Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByKWUeWA-": {
    "title": "GANITE: Estimation of Individualized Treatment Effects using Generative Adversarial Nets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ1nzBeA-": {
    "title": "Multi-Task Learning for Document Ranking and Query Suggestion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1aIuk-RW": {
    "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c342c71cb23199f112d0bc644fcce56a7306bf94",
    "semantic_title": "active learning for convolutional neural networks: a core-set approach",
    "citation_count": 1968,
    "authors": []
  },
  "https://openreview.net/forum?id=S1v4N2l0-": {
    "title": "Unsupervised Representation Learning by Predicting Image Rotations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aab368284210c1bb917ec2d31b84588e3d2d7eb4",
    "semantic_title": "unsupervised representation learning by predicting image rotations",
    "citation_count": 3283,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkc-TeZ0W": {
    "title": "A Hierarchical Model for Device Placement",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "22ba3276c8797fddeb5d9db083ed1010da182549",
    "semantic_title": "a hierarchical model for device placement",
    "citation_count": 151,
    "authors": []
  },
  "https://openreview.net/forum?id=SyJS-OgR-": {
    "title": "Multi-level Residual Networks from Dynamical Systems View",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cd958525291ee1ab856d23aa93cb95c86d87ccbe",
    "semantic_title": "multi-level residual networks from dynamical systems view",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=r1ZdKJ-0W": {
    "title": "Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1dHXnH6-": {
    "title": "Natural Language Inference over Interaction Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyydRMZC-": {
    "title": "Spatially Transformed Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d3c071dbbb4520ed5875f7e064a9da87240534db",
    "semantic_title": "spatially transformed adversarial examples",
    "citation_count": 522,
    "authors": []
  },
  "https://openreview.net/forum?id=S18Su--CW": {
    "title": "Thermometer Encoding: One Hot Way To Resist Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8b9127bee0f7d109da2672ba06d0f39a5a60335a",
    "semantic_title": "thermometer encoding: one hot way to resist adversarial examples",
    "citation_count": 613,
    "authors": []
  },
  "https://openreview.net/forum?id=SkhQHMW0W": {
    "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "92495abbac86394cb759bec15a763dbf49a8e590",
    "semantic_title": "deep gradient compression: reducing the communication bandwidth for distributed training",
    "citation_count": 1407,
    "authors": []
  },
  "https://openreview.net/forum?id=BkN_r2lR-": {
    "title": "Identifying Analogies Across Domains",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "411500f7aaa0f3c00d492b78b24b33da0fd0d58d",
    "semantic_title": "identifying analogies across domains",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJQVZW0b": {
    "title": "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ebc3bd9cd67193b3a8f84d43d5b4377107c680dc",
    "semantic_title": "hierarchical and interpretable skill acquisition in multi-task reinforcement learning",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=rJm7VfZA-": {
    "title": "Learning Parametric Closed-Loop Policies for Markov Potential Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkpoTaxA-": {
    "title": "Self-ensembling for visual domain adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rywHCPkAW": {
    "title": "Noisy Networks For Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkmu5b0a-": {
    "title": "MGAN: Training Generative Adversarial Nets with Multiple Generators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B13njo1R-": {
    "title": "Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1XolQbRW": {
    "title": "Model compression via distillation and quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1IDRdeCW": {
    "title": "The High-Dimensional Geometry of Binary Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJaP_-xAb": {
    "title": "Deep Learning with Logged Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryiAv2xAZ": {
    "title": "Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkeqO7x0-": {
    "title": "Unsupervised Cipher Cracking Using Discrete GANs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HktRlUlAZ": {
    "title": "Polar Transformer Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1X0mzZCW": {
    "title": "Fidelity-Weighted Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1cZsf-RW": {
    "title": "WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1ae1lZRb": {
    "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1J_rgWRW": {
    "title": "Understanding Deep Neural Networks with Rectified Linear Units",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1vuQG-CW": {
    "title": "HexaConv",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyrCWeWCb": {
    "title": "Trust-PCL: An Off-Policy Trust Region Method for Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1NYjfbR-": {
    "title": "Generative networks as inverse problems with Scattering transforms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJsjkMb0Z": {
    "title": "i-RevNet: Deep Invertible Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJySbbAZ": {
    "title": "Training GANs with Optimism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Yy1BxCZ": {
    "title": "Don't Decay the Learning Rate, Increase the Batch Size",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryH20GbRW": {
    "title": "Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1DmUzWAW": {
    "title": "A Simple Neural Attentive Meta-Learner",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJGWO9k0Z": {
    "title": "Critical Percolation as a Framework to Analyze the Training of Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkTEFfZRb": {
    "title": "Attacking Binarized Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkHDoG-Cb": {
    "title": "Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk6fD5yCb": {
    "title": "Espresso: Efficient Forward Propagation for Binary Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1WRibb0Z": {
    "title": "Expressive power of recurrent neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyfHgI6aW": {
    "title": "Memory Augmented Control Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Ddp1-Rb": {
    "title": "mixup: Beyond Empirical Risk Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkPLzgZAZ": {
    "title": "Modular Continual Learning in a Unified Visual Environment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkxF5RgC-": {
    "title": "Sparse Persistent RNNs: Squeezing Large Recurrent Networks On-Chip",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJXMpikCZ": {
    "title": "Graph Attention Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1hcZZ-AW": {
    "title": "N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkLyJl-0-": {
    "title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByBAl2eAZ": {
    "title": "Parameter Space Noise for Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwBEMWCZ": {
    "title": "Skip Connections Eliminate Singularities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkA-IE06W": {
    "title": "When is a Convolutional Filter Easy to Learn?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ8c3f-0b": {
    "title": "Auto-Encoding Sequential Monte Carlo",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJA7xfb0b": {
    "title": "Sobolev GAN",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJJLHbb0-": {
    "title": "Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJcKhk-Ab": {
    "title": "Can recurrent neural networks warp time?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rytstxWAW": {
    "title": "FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Skz_WfbCZ": {
    "title": "A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk6WhagRW": {
    "title": "Emergent Communication through Negotiation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJehNfW0-": {
    "title": "Do GANs learn the distribution? Some Theory and Empirics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r11Q2SlRW": {
    "title": "Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJk59JZ0b": {
    "title": "Guide Actor-Critic for Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1l4eQW0Z": {
    "title": "Kernel Implicit Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJJinbWRZ": {
    "title": "Model-Ensemble Trust-Region Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1DWPP1A-": {
    "title": "Unsupervised Learning of Goal Spaces for Intrinsically Motivated Goal Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJUYGxbCW": {
    "title": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJYFzMZC-": {
    "title": "Simulating Action Dynamics with Neural Process Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ry_WPG-A-": {
    "title": "On the Information Bottleneck Theory of Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1SnX5xCb": {
    "title": "Deep Sensing: Active Sensing using Multi-directional Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1Gi6LeRZ": {
    "title": "Learning from Between-class Examples for Deep Sound Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJGZq6g0-": {
    "title": "Emergent Communication in a Multi-Modal, Multi-Step Referential Game",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyZoi-WRb": {
    "title": "Debiasing Evidence Approximations: On Importance-weighted Autoencoders and Jackknife Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkpiPMbA-": {
    "title": "Decision Boundary Analysis of Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJwelMbR-": {
    "title": "Divide-and-Conquer Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Skj8Kag0Z": {
    "title": "Stabilizing Adversarial Nets with Prediction Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkUHlMZ0b": {
    "title": "Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk8XMWgRb": {
    "title": "Not-So-Random Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkO3uTkAZ": {
    "title": "Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJJ23bW0b": {
    "title": "Initialization matters: Orthogonal Predictive State Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyoDInJ0-": {
    "title": "Reinforcement Learning Algorithm Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1dh6Ax0Z": {
    "title": "TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1VVsebAZ": {
    "title": "Synthesizing realistic neural population activity patterns using Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1T2hmZAb": {
    "title": "Deep Complex Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJRZzFlRb": {
    "title": "Compressing Word Embeddings via Deep Compositional Code Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1q-TM-AW": {
    "title": "A DIRT-T Approach to Unsupervised Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rywDjg-RW": {
    "title": "Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy2ogebAW": {
    "title": "Unsupervised Neural Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkQqq0gRb": {
    "title": "Variational Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1uR4GZRZ": {
    "title": "Stochastic Activation Pruning for Robust Adversarial Defense",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJ-C6JbRW": {
    "title": "Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkVqXOxCb": {
    "title": "Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJNMYceCW": {
    "title": "Residual Loss Prediction: Reinforcement Learning With No Incremental Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1VjBebR-": {
    "title": "The Role of Minimal Complexity Functions in Unsupervised Learning of Semantic Mappings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkTS8lZAb": {
    "title": "Boundary Seeking GANs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BydLzGb0Z": {
    "title": "Twin Networks: Matching the Future for Sequence Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rk6cfpRjZ": {
    "title": "Learning Intrinsic Sparse Structures within Long Short-Term Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyiAuyb0b": {
    "title": "TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJx9GQb0-": {
    "title": "Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Dy---0Z": {
    "title": "Distributed Prioritized Experience Replay",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1cWzoxA-": {
    "title": "Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SysEexbRb": {
    "title": "Critical Points of Linear Neural Networks: Analytical Forms and Landscape Properties",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1HhRfWRZ": {
    "title": "Learning Awareness Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1kG7GZAW": {
    "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled Observations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkrC3GbRW": {
    "title": "Learning a Generative Model for Validity in Complex Discrete Structures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H196sainb": {
    "title": "Word translation without parallel data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BywyFQlAW": {
    "title": "Minimax Curriculum Learning: Machine Teaching with Desirable Difficulties and Scheduled Diversity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1VGkIxRZ": {
    "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJCXZQbAZ": {
    "title": "Hierarchical Density Order Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkNGsseC-": {
    "title": "On the Expressive Power of Overlapping Architectures of Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkSDMA36Z": {
    "title": "A New Method of Region Embedding for Text Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJa9iHgAZ": {
    "title": "Residual Connections Encourage Iterative Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BydjJte0-": {
    "title": "Towards Reverse-Engineering Black-Box Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1Dh8Tg0-": {
    "title": "Fix your classifier: the marginal value of training the last weight layer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1nZ1weCZ": {
    "title": "Learning to Multi-Task by Active Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyyP33gAZ": {
    "title": "Activation Maximization Generative Adversarial Nets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyqShMZRb": {
    "title": "Syntax-Directed Variational Autoencoder for Structured Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByQpn1ZA-": {
    "title": "Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1QgVti6Z": {
    "title": "Empirical Risk Landscape Analysis for Understanding Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1hYRMbCW": {
    "title": "On the regularization of Wasserstein GANs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1ANxQW0b": {
    "title": "Maximum a Posteriori Policy Optimisation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJzIBfZAb": {
    "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJewuJWCZ": {
    "title": "Learning to Teach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyProzZAW": {
    "title": "The power of deeper networks for expressing natural functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkrsAzWAb": {
    "title": "Online Learning Rate Adaptation with Hypergradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkUp6GZRW": {
    "title": "Boosting the Actor with Dual Critic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkN2Il-RZ": {
    "title": "SCAN: Learning Hierarchical Compositional Visual Concepts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Skw0n-W0Z": {
    "title": "Temporal Difference Models: Model-Free Deep RL for Model-Based Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyMvJrdaW": {
    "title": "Decoupling the Layers in Residual Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByS1VpgRZ": {
    "title": "cGANs with Projection Discriminator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJlMAAeC-": {
    "title": "Improving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryazCMbR-": {
    "title": "Communication Algorithms via Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyX0IeWAW": {
    "title": "META LEARNING SHARED HIERARCHIES",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyYe6k-CW": {
    "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1Euwz-Rb": {
    "title": "Compositional Attention Networks for Machine Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkJ3ibb0-": {
    "title": "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SywXXwJAb": {
    "title": "Deep Learning and Quantum Entanglement: Fundamental Connections with Implications to Network Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Y8hhg0b": {
    "title": "Learning Sparse Neural Networks through L_0 Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1n8LexRZ": {
    "title": "Generalizing Hamiltonian Monte Carlo with Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyjC5yWCW": {
    "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJQDjk-0b": {
    "title": "Unbiased Online Recurrent Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkabRiQpb": {
    "title": "Consequentialist conditional cooperation in social dilemmas with imperfect information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkwVAXyCW": {
    "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hyg0vbWC-": {
    "title": "Generating Wikipedia by Summarizing Long Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJyVzQ-C-": {
    "title": "Fraternal Dropout",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BySRH6CpW": {
    "title": "Learning Discrete Weights Using the Local Reparameterization Trick",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1wEFyWCW": {
    "title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sy0GnUxCb": {
    "title": "Emergent Complexity via Multi-Agent Competition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkFqf0lAZ": {
    "title": "Memory Architectures in Recurrent Neural Network Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJIoJWZCZ": {
    "title": "Adversarial Dropout Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk9zbyZCZ": {
    "title": "Neural Map: Structured Memory for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByOExmWAb": {
    "title": "MaskGAN: Better Text Generation via Filling in the _______",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk9Xc_lR-": {
    "title": "On the Discrimination-Generalization Tradeoff in GANs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1UOm4gA-": {
    "title": "Interactive Grounded Language Acquisition and Generalization in a 2D World",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Syg-YfWCW": {
    "title": "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Skp1ESxRZ": {
    "title": "Towards Synthesizing Complex Programs From Input-Output Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkrSv0lA-": {
    "title": "Loss-aware Weight Quantization of Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJzgZ3JCW": {
    "title": "Efficient Sparse-Winograd Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1lUOzWCW": {
    "title": "Demystifying MMD GANs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJC2SzZCW": {
    "title": "Sensitivity and Generalization in Neural Networks: an Empirical Study",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B14TlG-RW": {
    "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyZipzbCb": {
    "title": "Distributed Distributional Deterministic Policy Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bk8ZcAxR-": {
    "title": "Eigenoption Discovery through the Deep Successor Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1q7n9gAb": {
    "title": "The Implicit Bias of Gradient Descent on Separable Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyJ7ClWCb": {
    "title": "Countering Adversarial Images using Input Transformations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByRWCqvT-": {
    "title": "Learning to cluster in order to transfer across domains and tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bys4ob-Rb": {
    "title": "Certified Defenses against Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkcQFMZRb": {
    "title": "Variational image compression with a scale hyperprior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJQRKzbA-": {
    "title": "Hierarchical Representations for Efficient Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rytNfI1AZ": {
    "title": "Training wide residual networks for deployment using a single bit for each weight",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk0wHx-RW": {
    "title": "Learning Sparse Latent Representations with the Deep Copula Information Bottleneck",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1iuQjxCZ": {
    "title": "On the importance of single directions for generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJcSzz-CZ": {
    "title": "Meta-Learning for Semi-Supervised Few-Shot Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1zriGeCZ": {
    "title": "Hyperparameter optimization: a spectral approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Dx7fbCW": {
    "title": "Generalizing Across Domains via Cross-Gradient Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByJHuTgA-": {
    "title": "On the State of the Art of Evaluation in Neural Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rypT3fb0b": {
    "title": "LEARNING TO SHARE: SIMULTANEOUS PARAMETER TYING AND SPARSIFICATION IN DEEP LEARNING",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1vEXaxA-": {
    "title": "Emergent Translation in Multi-Agent Communication",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ry018WZAZ": {
    "title": "Deep Active Learning for Named Entity Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1l8BtlCb": {
    "title": "Non-Autoregressive Neural Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HktJec1RZ": {
    "title": "Towards Neural Phrase-based Machine Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJIgi_eCZ": {
    "title": "FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkYTTf-AZ": {
    "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByJIWUnpW": {
    "title": "Automatically Inferring Data Quality for Spatiotemporal Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hksj2WWAW": {
    "title": "Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByeqORgAW": {
    "title": "Proximal Backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1meywxRW": {
    "title": "DCN+: Mixed Objective And Deep Residual Coattention for Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk3ddfWRW": {
    "title": "Imitation Learning from Visual Data with Multiple Intentions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ94fqApW": {
    "title": "Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1Yp-j1Cb": {
    "title": "An Online Learning Approach to Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkZB1XbRZ": {
    "title": "Scalable Private Learning with PATE",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SygwwGbRW": {
    "title": "Semi-parametric topological memory for navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyRnez-RW": {
    "title": "Multi-Mention Learning for Reading Comprehension with Neural Cascades",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJuWrGW0Z": {
    "title": "Dynamic Neural Program Embeddings for Program Repair",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJWechg0Z": {
    "title": "Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkr1UDeC-": {
    "title": "Large scale distributed neural network training through online distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ry-TW-WAb": {
    "title": "Variational Network Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rk07ZXZRb": {
    "title": "Learning an Embedding Space for Transferable Robot Skills",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Skdvd2xAZ": {
    "title": "A Scalable Laplace Approximation for Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1sqHMZCb": {
    "title": "NerveNet: Learning Structured Policy with Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkhlb8lCZ": {
    "title": "Wavelet Pooling for Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkCsm6lRb": {
    "title": "Generative Models of Visually Grounded Imagination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rknt2Be0-": {
    "title": "Compositional Obverter Communication Learning from Raw Visual Input",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HyMTkQZAb": {
    "title": "Kronecker-factored Curvature Approximations for Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkUR_y-RZ": {
    "title": "SEARNN: Training RNNs with global-local losses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryRh0bb0Z": {
    "title": "Multi-View Data Generation Without View Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk5elxbRW": {
    "title": "Smooth Loss Functions for Deep Top-k Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rkHywl-A-": {
    "title": "Learning Robust Rewards with Adverserial Inverse Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJij4yg0Z": {
    "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}